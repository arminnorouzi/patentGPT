<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005476A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005476</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17363702</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>93</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>94</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>2015</main-group><subgroup>223</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>2015</main-group><subgroup>227</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEM AND METHOD FOR SPEECH PROCESSING BASED ON RESPONSE CONTENT</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Bank of America Corporation</orgname><address><city>Charlotte</city><state>NC</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Noorizadeh</last-name><first-name>Emad</first-name><address><city>Plano</city><state>TX</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A system for determining intent in a voice signal receives a first voice signal that indicates to perform a task. The system sends a first response that comprises a hyperlink associated with a particular webpage used to perform the task. The system receives a second voice signal that indicates whether to access the hyperlink. The system determines intent of the second voice signal by comparing keywords of the second voice signal with keywords of the first response. The system activates the hyperlink in response to determining that the keywords of the second voice signal correspond to the keywords of the first response.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="217.17mm" wi="109.56mm" file="US20230005476A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="228.18mm" wi="148.42mm" orientation="landscape" file="US20230005476A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="221.91mm" wi="111.59mm" file="US20230005476A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present disclosure relates generally to data processing, and more specifically to a system and method for speech processing based on response content.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">It is technically challenging to determine intent from speech in a conversational messaging environment, such as a messaging environment that uses a conversation agent or &#x201c;chatbot.&#x201d; This can lead to incorrect responses which waste computing resources and networking bandwidth. Current audio signal processing technologies are not configured to provide a reliable and efficient solution for detecting intent from speech.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0004" num="0003">Current conversation agents are not configured to provide a reliable and efficient solution to detect intent from speech. This disclosure contemplates systems and methods for implementing a conversation agent configured to detect intent from speech within the context of the last response shown to a user. In other words, the conversation agent detects the intent of a second speech within the context of the last response generated in response to receiving a first speech. The term &#x201c;speech&#x201d; is interchangeably referred to herein as a voice signal or an audio signal.</p><p id="p-0005" num="0004">For example, assume that the conversation agent receives a first speech (i.e., first voice signal) from a user. The user may ask the conversation agent to perform a task. For example, the first speech may include &#x201c;I want to login to my account&#x201d; or &#x201c;login to my account.&#x201d; The conversation agent generates a first response to the first speech. For example, the first response may include a hyperlink that is linked to a particular web site to perform the task. The hyperlink is interchangeably referred to herein as a call-to-action link. In this example, the hyperlink may be represented by text that includes &#x201c;access account.&#x201d;</p><p id="p-0006" num="0005">In some cases, the user may access the call-to-action link by clicking on the call-to-action link. In such cases, the conversation agent loads the website so that the user may login and access their account.</p><p id="p-0007" num="0006">In other cases, the user may utter a second speech (i.e., second voice signal) that indicates whether to activate the call-to-action link. For example, the second speech may include &#x201c;Yes, access the account.&#x201d; The conversation agent is configured to determine the intent of the second speech within the context of the first response. For example, the conversation agent may determine whether the second speech contains keywords that correspond to or match keywords in the call-to-action link. In another example, the conversation agent may determine whether utterances in the second speech correspond to or match utterances in the call-to-action link. In another example, the conversation agent may determine whether intent associated with the call-to-action link corresponds to or match the intent of the second speech.</p><p id="p-0008" num="0007">The conversation agent is configured to determine the intent of the second speech within the context of the first response based on a granularity level of the first response. In some cases, the first response may not be annotated with text conveying intent of the first response. In such cases, the conversation agent determines that the granularity level of the first response indicates that the first response is a broad response.</p><p id="p-0009" num="0008">In some cases, the first response may be annotated with text conveying the intent of the first response. For example, the first response may include annotations describing the intent of the first response. In such cases, the conversation agent determines that the granularity level of the first response indicates that the first response is an annotated response.</p><p id="p-0010" num="0009">If the conversation agent determines that the first response is a broad response, the conversation agent may perform keyword and/or utterance analysis to predict the intent of the second speech. For example, the conversation agent may compare keywords and/or utterances of the call-to-action link with keywords and/or utterances of the second speech, respectively.</p><p id="p-0011" num="0010">If the conversation agent determines that the first response is an annotated response, the conversation agent may perform keyword, utterance, and intent analysis to predict the intent of the second speech. For example, the conversation agent may compare keywords, utterances, and intent of the call-to-action link with keywords, utterances, and intent of the second speech, respectively.</p><p id="p-0012" num="0011">In this manner, the disclosed system may reduce the computational, processing, and memory resources, in cases where the first response is a broad response, because since the broad response is not annotated with text conveying the intent of the first response, the conversation agent may only perform keyword and/or utterance analysis. Thus, the disclosed system may reduce the computational, processing, and memory resources that would otherwise be spent using the current conversation agent and speech processing technologies.</p><p id="p-0013" num="0012">In one embodiment, a system for determining intent associated with a voice signal comprises a processor and a memory. The processor receives a first voice signal, where the first voice signal indicates to perform a task. The processor communicates a first response to the first voice signal, where the first response comprises a hyperlink associated with a particular webpage used to perform the task. The processor extracts a first set of keywords from the first response. The processor receives a second voice signal, where the second voice signal indicates access the hyperlink. The processor extracts a second set of keywords from the second voice signal. The processor determine a level of granularity of the first response, where determining the level of granularity of the first response comprises determining whether or not the first response is annotated with text conveying a first intent. In response to determining that the level of granularity of the first response indicates that the first response is not annotated with text conveying the first intent, the processor compares the first set of keywords with the second set of keywords. The processor determines whether the first set of keywords corresponds to the second set of keywords. In response to determining that the first set of keywords corresponds to the second set of keywords, the processor activates the hyperlink to launch the particular webpage to perform the task. The memory is operably coupled with the processor. The memory is operable to store the first voice signal, the second voice signal, and the first response.</p><p id="p-0014" num="0013">The disclosed system provides several practical applications and technical advantages, which include: 1) technology that predicts intent of speech (i.e., second voice signal) with respect to the context of the last response shown to the user (i.e., a first response), where the first response is generated in response to receiving a first speech; 2) technology that predicts the intent of the second speech based on keyword and/or utterance analysis between the keywords and/or utterances in the second speech and the last response, respectively, in response to determining that the last response is not annotated with text conveying intent of the last response; 3) technology that activates a call-to-action link in the first response, in response to determining that keywords and/or utterances in the second speech correspond to keywords and/or utterances in the last response, respectively; 4) technology that detects the intent of the second speech based on keyword, utterance, and intent analysis between the second speech and the last response, in response to determining that the last response is annotated with text conveying the intent of the last response; 5) technology that determines the intent of the second speech by extracting a first set of features from the last response representing a first intent, extracting a second set of features from the second voice signal, comparing the first set of features with the second set of features, and in response to determining that the first set of features corresponds to the first set of features, determining that the intent of the second speech is the first intent; and 6) technology that activates a call-to-action link in the first response, in response to determining that keywords, utterances, and/or the intent of the second speech correspond to keywords, utterances, and/or the intent in the last response, respectively.</p><p id="p-0015" num="0014">As such, the disclosed system may improve the current speech signal processing, conversation agent, and machine-to-human conversation technologies, for example, by predicting intent in an speech with respect the context of the last response show to the user. Accordingly, the disclosed system may be integrated into a practical application of reducing computational complexity of detecting intent in different speeches. For example, by determining whether or not the last response is annotated with text conveying intent of the last response, the disclosed system may determine whether or not to perform intent analysis.</p><p id="p-0016" num="0015">If the last response is not annotated with text conveying intent of the last response, the disclosed system may exclude the intent analysis, and perform keyword and/or utterance analysis. In this case, by excluding the intent analysis, the disclosed system may reduce computational complexity in detecting the intent of the second speech, and thus, reduce the processing and memory resources that would otherwise be spent using the current conversation agent and speech processing technologies.</p><p id="p-0017" num="0016">If the last response is annotated with text conveying intent of the last response, the disclosed system may perform the intent analyses in addition to or instead of the keyword and utterance analysis. For example, the disclosed system may perform the intent analysis by extracting a first set of features from the last response, extracting a second set of features from the second speech, and comparing the first set of features with the second set of features. In this case, by implementing the intent analysis, the disclosed system may increase the accuracy of predicting the intent of the second speech. For example, in some cases, the keywords and utterances of the last response may not correspond to the keywords and utterances of the second speech. Thus, in such cases, relying solely on the keyword and utterance analysis may result in predicting an inaccurate intent of the second speech within the context of the last response. Therefore, in cases that the last response is annotated with text conveying intent of the last response, the disclosed system increases the accuracy in predicting the intent of the second speech by implementing the intent analysis.</p><p id="p-0018" num="0017">The disclosed system may further be integrated into a practical application of recognizing, interpreting, and processing voice signals, e.g., received from a user, and allowing the user of a system that generates textual information (e.g., keywords) to interact with a voice or speech processing system that is configured to receive voice signals as input. Thus, the disclosed system may transform the received voice signal to textual information, e.g., using voice signal processing, natural language processing, etc.</p><p id="p-0019" num="0018">The disclosed system may further be integrated into an additional practical application of creating an improved machine-to-human conversation system that &#x201c;understands&#x201d; the intent of speech signals with respect to the context of one or more last responses. For example, the improved machine-to-human conversation system determines whether the user is continuing the same conversation, e.g., in response to determining that the intent of the second speech corresponds to the intent of the one or more last responses. In another example, the improved machine-to-human conversation system determines whether the user is starting a new conversation, e.g., in response to determining that the intent of the second speech does not correspond to the intent of the one or more last responses. In another example, the improved machine-to-human conversation system determines whether the user is asking to revise, update, or override a call-to-action link provided in one of the previous responses.</p><p id="p-0020" num="0019">Similarly, the disclosed system may further be integrated into an additional practical application of creating an improved machine-to-machine conversation (and/or communication) system by implementing a first machine that is configured to predict the intent of a second audio signal, text, or code with respect to the context of one or more previous responses received from a second machine in form of audio signal, text, or code.</p><p id="p-0021" num="0020">The disclosed system may further be integrated into an additional practical application of creating an improved conversation agent that is capable of producing a more seamless conversation with a user based on detecting intent in speech received from the user with respect to the context of one or more previous responses.</p><p id="p-0022" num="0021">The disclosed system may be integrated into an additional practical application of improving underlying operations of computing devices tasked to process a task requested by a user, and perform the task. This, in turn, provides additional practical applications, including ease of use, fewer resources needed, faster implementation and response, and more accurate communication with the conversation agents. For example, using the current speech signal processing and conversation agent technologies, the user may have to have a longer conversation with the conversation agent to convey a particular task to the conversation agent, and because the conversation agent is not configured to &#x201c;understand&#x201d; and predict intent from user's speech in the context of one or more previous responses, the conversation agent spends more processor and memory resources compared to the disclosed system.</p><p id="p-0023" num="0022">Certain embodiments of this disclosure may include some, all, or none of these advantages. These advantages and other features will be more clearly understood from the following detailed description taken in conjunction with the accompanying drawings and claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0024" num="0023">For a more complete understanding of this disclosure, reference is now made to the following brief description, taken in connection with the accompanying drawings and detailed description, wherein like reference numerals represent like parts.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an embodiment of a system configured for intent detection in speech; and</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example flowchart of a method for intent detection in speech.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0027" num="0026">As described above, previous technologies fail to provide efficient and reliable solutions for intent detection in speech. This disclosure provides various systems and methods for intent detection in speech within the content of the last response shown to a user. In one embodiment, system <b>100</b> and method <b>200</b> for intent detection in speech within the content of the last response shown to a user are described in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, respectively.</p><heading id="h-0006" level="1">Example System for Intent Detection in Speech</heading><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates one embodiment of a system <b>100</b> that is configured for detecting intent <b>130</b> in speech, e.g., intent <b>130</b><i>b </i>from a second voice signal <b>104</b><i>b</i>, based on a level of granularity <b>132</b> associated with a response <b>106</b>, e.g., the last response <b>106</b><i>a </i>generated in response to receiving a first voice signal <b>104</b><i>a</i>. In one embodiment, the system <b>100</b> comprises a server <b>140</b>. In some embodiments, system <b>100</b> further comprises a network <b>110</b> and a computing device <b>120</b>. Server <b>140</b> comprises a processor <b>142</b> in signal communication with a memory <b>148</b>. Memory <b>148</b> stores software instructions <b>150</b> that when executed by the processor <b>142</b>, cause the processor <b>142</b> to perform one or more functions described herein. For example, when the software instructions <b>150</b> are executed, the processor <b>142</b> executes a conversation agent <b>144</b> to determine intent <b>130</b> from a voice signal <b>104</b> in the context of the last response <b>106</b> based on a level of granularity of the last response <b>106</b>. In other embodiments, system <b>100</b> may not have all of the components listed and/or may have other elements instead of, or in addition to, those listed above.</p><heading id="h-0007" level="2">System Components</heading><p id="p-0029" num="0028">Network <b>110</b> may be any suitable type of wireless and/or wired network, including, but not limited to, all or a portion of the Internet, an Intranet, a private network, a public network, a peer-to-peer network, the public switched telephone network, a cellular network, a local area network (LAN), a metropolitan area network (MAN), a wide area network (WAN), and a satellite network. The network <b>110</b> may be configured to support any suitable type of communication protocol as would be appreciated by one of ordinary skill in the art.</p><p id="p-0030" num="0029">Computing device <b>120</b> is generally any device that is configured to process data and interact with users <b>102</b>. Examples of the computing device <b>120</b> include, but are not limited to, a personal computer, a desktop computer, a workstation, a server, a laptop, a tablet computer, a mobile phone (such as a smartphone), etc. The computing device <b>120</b> may include a user interface, such as a display, a microphone, keypad, or other appropriate terminal equipment usable by user <b>102</b>. The computing device <b>120</b> may include a hardware processor, memory, and/or circuitry configured to perform any of the functions or actions of the computing device <b>120</b> described herein. For example, a software application designed using software code may be stored in the memory and executed by the processor to perform the functions of the computing device <b>120</b>.</p><p id="p-0031" num="0030">The computing device <b>120</b> is associated with or includes a speaker <b>122</b>, a microphone <b>124</b>, and a conversational interface module <b>126</b>. The user <b>102</b> can interact or converse with the conversational interface module <b>126</b> using the microphone <b>124</b>. The conversational interface module <b>126</b> is configured to respond to the user <b>102</b> using the speaker <b>122</b>.</p><p id="p-0032" num="0031">The conversational interface module <b>126</b> may be implemented by a software and/or a hardware processor. The conversational interface module <b>126</b> may be executed by the processor of the computing device <b>120</b> executing software instruction stored in the memory of the computing device <b>120</b>. The conversational interface module <b>126</b> is in signal communication with the server <b>140</b>, processor <b>142</b>, and conversation agent <b>144</b>, via the network <b>110</b>. The processor <b>142</b> may deploy or install the conversational interface module <b>126</b> on the computing device <b>120</b> in order to interact with the user <b>102</b>. For example, the user <b>102</b> may converse with the conversational interface module <b>126</b> indicating that the user <b>102</b> is asking the conversation agent <b>144</b> to perform a task <b>112</b>.</p><p id="p-0033" num="0032">As the user <b>102</b> speaks to the microphone <b>124</b>, the conversational interface module <b>126</b> captures voice signals <b>104</b>, e.g., voice signal <b>104</b><i>a </i>and voice signal <b>104</b><i>b</i>, that include the speech of the user <b>102</b>. The conversational interface module <b>126</b> forwards the captured voice signal <b>104</b> to the conversation agent <b>144</b>. The conversation agent <b>144</b> processes the received voice signal <b>104</b> and generates a response <b>106</b>. For example, the conversation agent <b>144</b> receives a first voice signal <b>104</b><i>a</i>, processes the first voice signal <b>104</b><i>a</i>, and generates a first response <b>106</b><i>a </i>to the first voice signal <b>104</b><i>a</i>. The conversation agent <b>144</b> communicates the first response <b>106</b><i>a </i>to the computing device <b>120</b>. In another example, the conversation agent <b>144</b> receives a second voice signal <b>104</b><i>b</i>, processes the second voice signal <b>104</b><i>b</i>, and generates a second response <b>106</b><i>b </i>to the second voice signal <b>104</b><i>b. </i></p><p id="p-0034" num="0033">The conversation agent <b>144</b> communicates the second response <b>106</b><i>b </i>to the computing device <b>120</b>. The responses <b>106</b> may include audio signals, text, or any other type of data. Upon receiving a response <b>106</b>, the conversational interface module <b>126</b> may utter the response <b>106</b> from the microphone <b>124</b> and/or display the text within the response <b>106</b> (e.g., hyperlink <b>108</b> or call-to-action link <b>108</b>) on a display screen of the computing device <b>120</b>.</p><p id="p-0035" num="0034">The processes of analyzing the voice signals <b>104</b> and generating respective responses <b>106</b> are described further below in conjunction with the operational flow of the system <b>100</b> and method <b>200</b> described in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><heading id="h-0008" level="2">Server</heading><p id="p-0036" num="0035">Server <b>140</b> is generally a server or any other device configured to process data and communicate with computing devices (e.g., computing device <b>120</b>), databases, etc., via the network <b>110</b>. The server <b>140</b> is generally configured to oversee the operations of the conversation agent <b>144</b>, as described further below in conjunction with the operational flow of system <b>100</b> and method <b>200</b> described in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0037" num="0036">Processor <b>142</b> comprises one or more processors operably coupled to the memory <b>148</b>. The processor <b>142</b> is any electronic circuitry, including, but not limited to, state machines, one or more central processing unit (CPU) chips, logic units, cores (e.g., a multi-core processor), field-programmable gate array (FPGAs), application-specific integrated circuits (ASICs), or digital signal processors (DSPs). The processor <b>142</b> may be a programmable logic device, a microcontroller, a microprocessor, or any suitable combination of the preceding. The one or more processors are configured to process data and may be implemented in hardware or software. For example, the processor <b>142</b> may be 8-bit, 16-bit, 32-bit, 64-bit, or of any other suitable architecture. The processor <b>142</b> may include an arithmetic logic unit (ALU) for performing arithmetic and logic operations, processor <b>142</b> registers the supply operands to the ALU and store the results of ALU operations, and a control unit that fetches instructions from memory and executes them by directing the coordinated operations of the ALU, registers and other components. The one or more processors are configured to implement various instructions. For example, the one or more processors are configured to execute instructions (e.g., software instructions <b>150</b>) to implement the conversation agent <b>144</b>. In this way, processor <b>142</b> may be a special-purpose computer designed to implement the functions disclosed herein. In an embodiment, the processor <b>142</b> is implemented using logic units, FPGAs, ASICs, DSPs, or any other suitable hardware. The processor <b>142</b> is configured to operate as described in <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>2</b></figref>. For example, the processor <b>142</b> may be configured to perform one or more steps of method <b>200</b> as described in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0038" num="0037">Network interface <b>146</b> is configured to enable wired and/or wireless communications (e.g., via network <b>110</b>). The network interface <b>146</b> is configured to communicate data between the server <b>140</b> and other devices (e.g., computing device <b>120</b>), databases, systems, or domains. For example, the network interface <b>146</b> may comprise a WIFI interface, a local area network (LAN) interface, a wide area network (WAN) interface, a modem, a switch, or a router. The processor <b>142</b> is configured to send and receive data using the network interface <b>146</b>. The network interface <b>146</b> may be configured to use any suitable type of communication protocol as would be appreciated by one of ordinary skill in the art.</p><p id="p-0039" num="0038">Memory <b>148</b> may be volatile or non-volatile and may comprise a read-only memory (ROM), random-access memory (RAM), ternary content-addressable memory (TCAM), dynamic random-access memory (DRAM), and static random-access memory (SRAM). Memory <b>148</b> may be implemented using one or more disks, tape drives, solid-state drives, and/or the like. Memory <b>148</b> is operable to store the software instructions <b>150</b>, machine learning algorithm <b>152</b>, training dataset <b>154</b>, voice signals <b>104</b> a and <b>104</b><i>b</i>, responses <b>106</b><i>a </i>and <b>106</b><i>b</i>, certainty level <b>162</b>, and/or any other data or instructions. The software instructions <b>150</b> may comprise any suitable set of instructions, logic, rules, or code operable to execute the processor <b>142</b>.</p><heading id="h-0009" level="2">Conversation Agent</heading><p id="p-0040" num="0039">Conversation agent <b>144</b> may be implemented by the processor <b>142</b> executing the software instructions <b>150</b>, and is generally configured to determine intent <b>130</b> from a voice signal <b>104</b> based on a granularity level <b>132</b> associated with the last response <b>106</b> shown to the user <b>102</b>.</p><p id="p-0041" num="0040">In one embodiment, the conversation agent <b>144</b> may be implemented by a machine learning algorithm <b>152</b>. For example, the machine learning algorithm <b>152</b> may comprise support vector machine, neural network, random forest, k-means clustering, etc. The machine learning algorithm <b>152</b> may be implemented by a plurality of neural network (NN) layers, Convolutional NN (CNN) layers, Long-Short-Term-Memory (LSTM) layers, Bi-directional LSTM layers, Recurrent NN (RNN) layers, and the like. In another example, the machine learning algorithm <b>152</b> may be implemented by Natural Language Processing (NLP). In another example, the machine learning algorithm <b>152</b> may be implemented by analog signal processing, digital signal processing, speech signal processing, signal quantization, signal frequency sampling, speech transcription, audio-to-text converter, among others. The operation of the conversation agent <b>144</b> is described further below in conjunction with the operational flow of the system <b>100</b> and method <b>200</b> described in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0042" num="0041">In one embodiment, the conversation agent <b>144</b> may be trained to detect intent <b>130</b> from a voice signal <b>104</b> by using the training dataset <b>154</b>. The training dataset <b>154</b> may comprise a plurality of voice signals <b>104</b> comprising utterances <b>114</b>, each labeled with a different intent <b>130</b>. For example, the training dataset <b>154</b> may include a first set of utterances <b>114</b> that is labeled with a first intent <b>130</b>, a second set of utterances <b>114</b> that is labeled with a second intent <b>130</b>, and so on.</p><p id="p-0043" num="0042">In the training process, the conversation agent <b>144</b> is given a first set of utterances <b>114</b> from the training dataset <b>154</b>, each labeled with a different intent <b>130</b>, and is asked to determine correlations and associations between the first set of utterances <b>114</b> and their corresponding intent <b>130</b>. In this process, the conversation agent <b>144</b> may implement the machine learning algorithm <b>152</b> to extract a first set of features from the first set of utterances <b>114</b>, and determine that the extracted first set of features are associated with the corresponding intent <b>130</b>. The first set of features may include one or more keywords representing contextual data including the intent of the first set of utterances <b>114</b>.</p><p id="p-0044" num="0043">In the testing process, the conversation agent <b>144</b> is given a second set of utterances <b>114</b> from the training dataset <b>154</b> that are not labeled with intent <b>130</b>, and is asked to predict the intent <b>130</b> of the second set of utterances <b>114</b>. In this process, the conversation agent <b>144</b> may implement the machine learning algorithm <b>152</b> to extract a second set of features from the second set of utterances <b>114</b>. The second set of features may include one or more keywords representing contextual data, including the intent of the second set of utterances <b>114</b>.</p><p id="p-0045" num="0044">The conversation agent <b>144</b> compares the second set of features (not labeled with intent) with the first set of features (labeled with a particular intent <b>130</b>). For example, the conversation agent <b>144</b> may compare a first vector representing the first set of features with a second vector representing the second set of features, such as by calculating a dot product, Euclidian distance between the first vector and the second vector, etc. If the conversation agent <b>144</b> determines that the Euclidian distance between the first vector and the second vector is less than a threshold distance (e.g., less than 1%, 2%, etc.), the conversation agent <b>144</b> determines that the second set of features corresponds to the first set of features. If the conversation agent <b>144</b> determines that the second set of features corresponds to the first set of features, the conversation agent <b>144</b> determines that the second set of features is also associated with particular intent <b>130</b> that the first set of features is associated with, and that the intent of the second set of utterances <b>114</b> is the particular intent <b>130</b>. In this manner, the conversation agent <b>144</b> may be trained and tested to detect intent <b>130</b> from voice signals <b>104</b>.</p><heading id="h-0010" level="2">Operational Flow</heading><heading id="h-0011" level="2">Receiving a First Voice Signal</heading><p id="p-0046" num="0045">The operational flow of the system <b>100</b> begins when the conversation agent <b>144</b> receives the first voice signal <b>104</b><i>a</i>. For example, the conversation agent <b>144</b> receives the first voice signal <b>104</b><i>a </i>when the user <b>102</b> utters speech to the microphone <b>124</b>, and the conversational interface module <b>126</b> transmits the first voice signal <b>104</b> to the conversation agent <b>144</b>. The first voice signal <b>104</b><i>a </i>may indicate to perform a task <b>112</b><i>a. </i></p><p id="p-0047" num="0046">The conversation agent <b>144</b> processes the first voice signal <b>104</b><i>a </i>by implementing the machine learning algorithm <b>152</b> to detect the task <b>112</b><i>a </i>and intent <b>130</b><i>a </i>associated with the first voice signal <b>104</b><i>a</i>, similar to that described above. In this process, the conversation agent <b>144</b> extracts one or more utterances <b>114</b><i>a </i>from the first voice signal <b>104</b><i>a </i>by signal quantization, speech transcription, audio-to-text convertor, and/or any audio signal processing.</p><p id="p-0048" num="0047">The conversation agent <b>144</b> determines one or more keywords <b>116</b><i>a </i>from the first voice signals <b>104</b> a using any type of text analysis, such as word segmentation, sentence segmentation, word tokenization, sentence tokenization, and/or the like. The conversation agent <b>144</b> may implement the machine learning algorithm <b>152</b> to determine the intent <b>130</b><i>a </i>associated with the first voice signal <b>104</b><i>a</i>. The intent <b>130</b><i>a </i>may be represented by one or more keywords including and/or related to keywords <b>116</b><i>a</i>. In this process, the conversation agent <b>144</b> may extract features <b>118</b><i>a </i>from the first voice signal <b>104</b><i>a</i>, utterances <b>114</b><i>a</i>, and/or keywords <b>116</b><i>a</i>. The extracted features <b>118</b><i>a </i>may be represented by a vector <b>128</b><i>a </i>that comprises a set of numerical values. In this manner, the conversation agent <b>144</b> may determine the intent <b>130</b><i>a </i>associated with the first voice signal <b>104</b><i>a. </i></p><heading id="h-0012" level="2">Generating a First Response to the First Voice Signal</heading><p id="p-0049" num="0048">The conversation agent <b>144</b> generates the first response <b>106</b><i>a </i>to the first voice signal <b>104</b><i>a</i>. In one example, the first response <b>106</b><i>a </i>may include a call-to-action link <b>108</b>. The call-to-action link <b>108</b> may be a hyperlink that directs the user <b>102</b> to a website to perform the task <b>112</b><i>a. </i></p><p id="p-0050" num="0049">For example, assume that the first voice signal <b>104</b><i>a </i>comprises &#x201c;I want to access my account.&#x201d; The conversation agent <b>144</b> generates the first response <b>106</b><i>a </i>to the first voice signal <b>104</b><i>a</i>. In this example, the first response <b>106</b><i>a </i>may include the call-to-action link <b>108</b> that is a hyperlink to a particular webpage so that the user <b>102</b> may log in and access their account. Upon activation of the call-to-action link <b>108</b>, the conversation agent <b>144</b> launches the particular webpage that is used to perform the task <b>112</b><i>a</i>, such that the particular webpage is loaded and displayed on a web browser on the computing device <b>120</b>. In this example, the call-to-action link <b>108</b> may include &#x201c;access account&#x201d; hyperlink that directs the user <b>102</b> to a particular webpage so that the user <b>102</b> may log in and access their account.</p><heading id="h-0013" level="2">Determining a Granularity Level of the First Response</heading><p id="p-0051" num="0050">The conversation agent <b>144</b> determines a granularity level <b>132</b> associate with the first response <b>106</b><i>a</i>. In one embodiment, the conversation agent <b>144</b> may determine the granularity level <b>132</b> associated with the first response <b>106</b><i>a </i>by determining whether or not the first response <b>106</b><i>a </i>is annotated with text conveying intent <b>156</b>.</p><p id="p-0052" num="0051">In some cases, the first response <b>106</b><i>a </i>may include a broad response <b>106</b><i>a</i>-<b>1</b>. The broad response <b>106</b><i>a</i>-<b>1</b> may not be annotated with or include text indicating or describing intent of the broad response <b>106</b><i>a</i>-<b>1</b>. In such cases, the conversation agent <b>144</b> determines that the granularity level <b>132</b><i>a </i>of the broad response <b>106</b><i>a</i>-<b>1</b> is less than a threshold level, e.g., less than 5 out of 10.</p><p id="p-0053" num="0052">In some cases, the first response <b>106</b><i>a </i>may include an annotated response <b>106</b><i>a</i>-<b>2</b>. The annotated response <b>106</b><i>a</i>-<b>2</b> may be annotated with text <b>138</b> indicating or describing intent <b>156</b> associated with the annotated response <b>106</b><i>a</i>-<b>2</b>. For example, the text <b>138</b> may include text describing the intent <b>156</b>. In the example where the call-to-action link <b>108</b> is &#x201c;access account,&#x201d; the text <b>138</b> may include &#x201c;This link is to view and access an account.&#x201d; In such cases, the conversation agent <b>144</b> determines that the granularity level <b>132</b><i>b </i>of the annotated response <b>106</b><i>a</i>-<b>2</b> is more than a threshold level, e.g., more than 5 out of 10. The text <b>138</b> may or may not accompany an annotated response <b>106</b><i>a</i>-<b>2</b> and thus may or may not be shown to the user <b>102</b> when the conversation agent <b>144</b> sends the annotated response <b>106</b><i>a</i>-<b>2</b> to the computing device <b>120</b>.</p><p id="p-0054" num="0053">The conversation agent <b>144</b> uses the granularity level <b>132</b> associated with the first response <b>106</b><i>a </i>to detect intent <b>130</b><i>b </i>associated with the second voice signal <b>104</b><i>b </i>within the context of the first response <b>106</b><i>a</i>, as described below. Continuing the example above, upon generating the first response <b>106</b><i>a</i>, the conversation agent <b>144</b> sends the first response <b>106</b><i>a </i>to the computing device <b>120</b> to be displayed on a screen of the computing device <b>120</b>.</p><heading id="h-0014" level="2">Determining Intent of a Second Voice Signal Based on a Granularity Level of the First Response</heading><p id="p-0055" num="0054">Upon receiving the first response <b>106</b><i>a</i>, the user <b>102</b> may utter speech providing the second voice signal <b>104</b><i>b</i>. The conversation agent <b>144</b> receives the second voice signal <b>104</b><i>b </i>via the conversational interface module <b>126</b>, similar to that described above with respect to receiving the first voice signal <b>104</b><i>a</i>. For example, the second voice signal <b>104</b><i>b </i>may indicate whether or not to activate the call-to-action link <b>108</b> in the first response <b>106</b><i>a</i>, i.e., whether or not to perform the task <b>112</b><i>b</i>. The second voice signal <b>104</b><i>b </i>may indicate to activate, update, or revise the call-to-action link <b>108</b>.</p><p id="p-0056" num="0055">The conversation agent <b>144</b> inputs the second voice signal <b>104</b><i>b </i>to the machine learning algorithm <b>152</b> to extract utterances <b>114</b><i>b</i>, keywords <b>116</b><i>b</i>, and features <b>118</b><i>b</i>, similar to that described above with respect to extracting utterances <b>114</b><i>a</i>, keywords <b>116</b><i>a</i>, and features <b>118</b><i>a </i>from the first voice signal <b>104</b><i>a</i>, respectively. The features <b>118</b><i>b </i>are represented by the vector <b>128</b><i>b </i>that comprises a set of numerical values. The features <b>118</b><i>b </i>may include one or more keywords including and/or related to keywords <b>116</b><i>b. </i></p><p id="p-0057" num="0056">The conversation agent <b>144</b> detects the intent <b>130</b><i>b </i>based on the granularity level <b>132</b> associated with the first response <b>106</b><i>a</i>, as described below. In some cases, the conversation agent <b>144</b> may detect the intent <b>130</b><i>b </i>within the context of one or more previous responses <b>106</b> generated before the voice signal <b>104</b><i>b. </i></p><p id="p-0058" num="0000">Determining the Intent of the Second Voice Signal within a Context of a Broad Last Response</p><p id="p-0059" num="0057">In a first example, assume that the first response <b>106</b><i>a </i>is a broad response <b>106</b><i>a</i>-<b>1</b> whose granularity level <b>132</b><i>a </i>is determined to be less than a threshold level, similar to that described above. In this example, the conversation agent <b>144</b> determines that the granularity level <b>132</b><i>a </i>indicates that the first response <b>106</b><i>a</i>-<b>1</b> is not annotated with text conveying intent of the first response <b>106</b><i>a</i>. Thus, the conversation agent <b>144</b> may perform keyword and/or utterance analysis to determine the intent <b>130</b><i>b </i>of the second voice signal <b>104</b><i>b. </i></p><p id="p-0060" num="0058">In this process, the conversation agent <b>144</b> may parse the broad response <b>106</b><i>a</i>-<b>1</b> to detect the utterances <b>134</b><i>a </i>and keywords <b>136</b><i>a</i>. The conversation agent <b>144</b> may then compare the keywords <b>116</b><i>b </i>with keywords <b>136</b><i>a</i>. The conversation agent <b>144</b> may determine whether the keywords <b>116</b><i>b </i>correspond to the keywords <b>136</b><i>a. </i></p><p id="p-0061" num="0059">In one embodiment, determining whether the keywords <b>116</b><i>b </i>correspond to the keywords <b>136</b><i>a </i>comprises determining a percentage of keywords <b>116</b><i>b </i>that corresponds to counterpart keywords <b>136</b><i>a</i>. For example, if the conversation agent <b>144</b> determines that more than 90%, 95%, etc. of the keywords <b>116</b><i>b </i>corresponds to counterpart keywords <b>136</b><i>a</i>, the conversation agent <b>144</b> determines that the keywords <b>116</b><i>b </i>correspond to the keywords <b>136</b><i>a. </i></p><p id="p-0062" num="0060">In response to determining that the keywords <b>116</b><i>b </i>correspond to the keywords <b>136</b><i>a</i>, the conversation agent <b>144</b> activates the call-to-action link <b>108</b>. For example, the conversation agent <b>144</b> may activate the call-to-action link <b>108</b> by loading a webpage that the call-to-action link <b>108</b> is linked to. The conversation agent <b>144</b> may send a second response <b>106</b><i>b </i>indicating that the call-to-action link <b>108</b> is activated.</p><p id="p-0063" num="0061">If the conversation agent <b>144</b> determines that the keywords <b>116</b><i>b </i>correspond to the keywords <b>136</b><i>a</i>, the conversation agent <b>144</b> may determine that the second voice signal <b>104</b><i>b </i>is related to the broad response <b>106</b><i>a</i>-<b>1</b>. In other words, the conversation agent <b>144</b> determines that the user <b>102</b> is continuing the same conversation.</p><p id="p-0064" num="0062">If the conversation agent <b>144</b> determines that the keywords <b>116</b><i>b </i>do not correspond to the keywords <b>136</b><i>a</i>, the conversation agent <b>144</b> determines that the second voice signal <b>104</b><i>b </i>is not related to the broad response <b>106</b><i>a</i>-<b>1</b>. In other words, the conversation agent <b>144</b> may determine that the user <b>102</b> is starting a new conversation.</p><p id="p-0065" num="0063">In one embodiment, if the conversation agent <b>144</b> determines that the keywords <b>116</b><i>b </i>do not correspond to the keywords <b>136</b><i>a</i>, the conversation agent <b>144</b> may send a second response <b>106</b><i>b </i>comprising a second call-to-action link whose keywords correspond to the keywords <b>116</b><i>b. </i></p><p id="p-0066" num="0064">In one embodiment, if the conversation agent <b>144</b> determines that the keywords <b>116</b><i>b </i>do not correspond to the keywords <b>136</b><i>a</i>, the conversation agent <b>144</b> may send a second response <b>106</b><i>b </i>that is a broad response <b>106</b> that is annotated with text conveying intent.</p><p id="p-0067" num="0065">In one embodiment, the conversation agent <b>144</b> may compare the utterances <b>114</b><i>b </i>with utterances <b>134</b><i>a </i>to determine whether the utterances <b>114</b><i>b </i>correspond to the utterances <b>134</b><i>a</i>, similar to that described above with respect to comparing the keywords <b>116</b><i>b </i>with keywords <b>136</b><i>a. </i></p><p id="p-0068" num="0066">If the conversation agent <b>144</b> determines that the utterances <b>114</b><i>b </i>correspond to the utterances <b>134</b><i>a</i>, the conversation agent <b>144</b> activates the call-to-action link <b>108</b>. Otherwise, the conversation agent <b>144</b> may not activate the call-to-action link <b>108</b>.</p><p id="p-0069" num="0000">Determining the Intent of the Second Voice Signal within a Context of an Annotated Last Response</p><p id="p-0070" num="0067">In a second example, assume that the first response <b>106</b><i>a </i>is an annotated response <b>106</b><i>a</i>-<b>2</b> whose granularity level <b>132</b><i>b </i>is determined to be more than a threshold level, similar to that described above. In this example, the conversation agent <b>144</b> determines that the granularity level <b>132</b><i>b </i>indicates that the first response <b>106</b><i>a </i>is annotated with text <b>138</b> indicating or describing the intent <b>156</b>. Thus, the conversation agent <b>144</b> may perform keyword, utterance, and intent comparing and analysis to predict or determine the intent <b>130</b><i>b </i>of the second voice signal <b>104</b><i>b. </i></p><p id="p-0071" num="0068">In this process, the conversation agent <b>144</b> may parse the annotated response <b>106</b><i>a</i>-<b>2</b> to detect keywords <b>136</b><i>b </i>and utterances <b>134</b><i>b</i>. The conversation agent <b>144</b> may also extract features <b>158</b> from the annotated response <b>106</b><i>a</i>-<b>2</b>, where the features <b>158</b> represent the intent <b>156</b>. The features <b>158</b> may be represented by a vector <b>160</b> that comprises a set of numerical values.</p><p id="p-0072" num="0069">With respect to keyword analysis, the conversation agent <b>144</b> may compare the keywords <b>136</b><i>b </i>with keywords <b>116</b><i>b</i>. The conversation agent <b>144</b> determines whether the keywords <b>136</b><i>b </i>correspond to the keywords <b>116</b><i>b</i>. In one embodiment, the conversation agent <b>144</b> determines that the keywords <b>136</b><i>b </i>correspond to the keywords <b>116</b><i>b </i>if more than a threshold percentage of the keywords <b>136</b><i>b </i>(e.g., more than 90%, 95, etc.) correspond to counterpart keywords <b>116</b><i>b </i>or within a threshold range, e.g., within &#xb1;5%, &#xb1;10%, etc. of the counterpart keywords <b>116</b><i>b</i>. In response to determining that the keywords <b>136</b><i>b </i>correspond to the keywords <b>116</b><i>b</i>, the conversation agent <b>144</b> activates the call-to-action link <b>108</b>, similar to that described above. The conversation agent <b>144</b> may send a second response <b>106</b><i>b </i>indicating that the call-to-action link <b>108</b> is activated. In one embodiment, if the conversation agent <b>144</b> determines that the keywords <b>116</b><i>b </i>do not correspond to the keywords <b>136</b><i>b</i>, the conversation agent <b>144</b> may send a second response <b>106</b><i>b </i>comprising a second call-to-action link whose keywords correspond to the keywords <b>116</b><i>b. </i></p><p id="p-0073" num="0070">With respect to utterance analysis, the conversation agent <b>144</b> may compare the utterances <b>114</b><i>b </i>with utterances <b>134</b><i>b </i>to determine whether the utterances <b>114</b><i>b </i>correspond to the utterances <b>134</b><i>b</i>, similar to that described above with respect to comparing the keywords <b>116</b><i>b </i>with keywords <b>136</b><i>b. </i></p><p id="p-0074" num="0071">With respect to intent analysis, the conversation agent <b>144</b> compare the features <b>158</b> with features <b>118</b><i>b</i>. In this process, the conversation agent <b>144</b> compares the vector <b>160</b> that represents the features <b>158</b> with the vector <b>128</b><i>b </i>that represents the features <b>118</b><i>b</i>. The conversation agent <b>144</b> determines whether features <b>158</b> correspond to features <b>118</b><i>b. </i></p><p id="p-0075" num="0072">To this end, the conversation agent <b>144</b> compares each numerical value of the vector <b>160</b> with a counterpart numerical value of the vector <b>128</b><i>b</i>. The conversation agent <b>144</b> determines whether each numerical value of the vector <b>160</b> corresponds to the counterpart numerical value of the vector <b>128</b><i>b</i>. For example, the conversation agent <b>144</b> determines that the features <b>118</b><i>b </i>correspond to the features <b>158</b> if more than a threshold percentage of the numerical values of the vector <b>160</b> (e.g., more than 90%, 95%, etc.) correspond to the counterpart numerical value of the vector <b>128</b><i>b</i>. In another example, the conversation agent <b>144</b> may determine that the features <b>118</b><i>b </i>correspond to the features <b>158</b> if more than a threshold percentage of the numerical values of the vector <b>160</b> (e.g., more than 90%, 95%, etc.) are within a threshold range (e.g., &#xb1;5%, &#xb1;10%, etc.) of the counterpart numerical value of the vector <b>128</b><i>b. </i></p><p id="p-0076" num="0073">If the conversation agent <b>144</b> determines that the features <b>118</b><i>b </i>correspond to the features <b>158</b>, the conversation agent <b>144</b> determines that the intent <b>156</b> corresponds to the intent <b>130</b><i>b</i>. If the conversation agent <b>144</b> determines that the intent <b>156</b> corresponds to the intent <b>130</b><i>b</i>, the conversation agent <b>144</b> may determine that the second voice signal <b>104</b><i>b </i>is related to the annotated response <b>106</b><i>a</i>-<b>2</b>. In other words, the conversation agent <b>144</b> determines that the user <b>102</b> is continuing the same conversation. For example, assuming that the second voice signal <b>104</b><i>b </i>indicates a confirmation that the call-to-action link <b>108</b> is accurate, the conversation agent <b>144</b> may proceed to activate the call-to-action link <b>108</b>.</p><p id="p-0077" num="0074">If the conversation agent <b>144</b> determines that the intent <b>156</b> does not correspond to the intent <b>130</b><i>b</i>, the conversation agent <b>144</b> determines that the second voice signal <b>104</b><i>b </i>is not related to the annotated response <b>106</b><i>a</i>-<b>2</b>. For example, the conversation agent <b>144</b> may determine that the call-to-action link <b>108</b> is not accurate, and send a second response <b>106</b><i>b </i>that includes a second call-to-action link <b>108</b> whose intent and/or features correspond to the intent <b>130</b><i>b </i>and/or features <b>118</b><i>b</i>, respectively. In other words, if the conversation agent <b>144</b> determines that the intent <b>156</b> does not correspond to the intent <b>130</b><i>b</i>, the conversation agent <b>144</b> may determine that the user <b>102</b> is starting a new conversation.</p><p id="p-0078" num="0075">The conversation agent <b>144</b> may determine whether the features <b>158</b> correspond to features <b>118</b><i>b </i>in other method, described below. For example, the conversation agent <b>144</b> may perform a dot product between the vectors <b>160</b> and <b>128</b><i>b</i>. If the dot product between the vectors <b>160</b> and <b>128</b><i>b </i>is less than a threshold percentage, e.g., 1%, 2%, etc., the conversation agent <b>144</b> determines that the features <b>158</b> correspond to the features <b>118</b><i>b</i>. In another example, the conversation agent <b>144</b> may calculate a Euclidian distance between the vectors <b>160</b> and <b>128</b><i>b</i>. If the Euclidian distance between the vectors <b>160</b> and <b>128</b><i>b </i>is less than a threshold percentage, e.g., 1%, 2%, etc. the conversation agent <b>144</b> determines that the features <b>158</b> correspond to the features <b>118</b><i>b. </i></p><p id="p-0079" num="0076">In one embodiment, the conversation agent <b>144</b> may determine a certainty level <b>162</b> of activating the call-to-action link <b>108</b> based on the granularity level <b>132</b> associated with the first response <b>106</b><i>a</i>. For example, in response to determining that the granularity level <b>132</b> of the first response <b>106</b><i>a </i>indicates that the first response <b>106</b><i>a </i>is not annotated with text conveying intent of the first response <b>106</b><i>a</i>, the conversation agent <b>144</b> determines that the certainty level <b>162</b> of activating the call-to-action link <b>108</b> is at a first level, e.g., less than 5 out of 10. In another example, in response to determining that the granularity level <b>132</b> of the first response <b>106</b><i>a </i>indicates that the first response <b>106</b><i>a </i>is annotated with text conveying intent of the first response <b>106</b><i>a</i>, the conversation agent <b>144</b> determines that the certainty level <b>162</b> of activating the call-to-action link <b>108</b> is at a second level, e.g., more than 5 out of 10.</p><heading id="h-0015" level="1">Example Method for Intent Detection in Speech</heading><p id="p-0080" num="0077"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example flowchart of a method <b>200</b> for detecting intent <b>130</b> in speech, e.g., intent <b>130</b><i>b </i>from a second voice signal <b>104</b><i>b</i>, based on a level of granularity <b>132</b> associated with a response <b>106</b>, e.g., the last response <b>106</b><i>a</i>. Modifications, additions, or omissions may be made to method <b>200</b>. Method <b>200</b> may include more, fewer, or other steps. For example, steps may be performed in parallel or in any suitable order. While at times discussed as the system <b>100</b>, processor <b>142</b>, conversation agent <b>144</b>, or components of any of thereof performing steps, any suitable system or components of the system may perform one or more steps of the method <b>200</b>. For example, one or more steps of method <b>200</b> may be implemented, at least in part, in the form of software instructions <b>150</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, stored on non-transitory, tangible, machine-readable media (e.g., memory <b>148</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) that when run by one or more processors (e.g., processor <b>142</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) may cause the one or more processors to perform steps <b>202</b>-<b>222</b>.</p><p id="p-0081" num="0078">The method <b>200</b> begins where the conversation agent <b>144</b> receives a first voice signal <b>104</b><i>a</i>, where the first voice signal <b>104</b> indicates to perform a task <b>112</b><i>a</i>. For example, the conversation agent <b>144</b> receives the first voice signal <b>104</b><i>a </i>when the user <b>102</b> speaks to the microphone <b>124</b> providing the first voice signal <b>104</b><i>a</i>, similar to that described in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The conversation agent <b>144</b> determines the intent <b>130</b><i>a </i>of the first voice signal <b>104</b><i>a </i>by feeding the first voice signal <b>104</b><i>a </i>to the machine learning algorithm <b>152</b>, similar to that described in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0082" num="0079">At step <b>204</b>, the conversation agent <b>144</b> communicates a first response <b>106</b><i>a </i>to the first voice signal <b>104</b><i>a</i>, where the first response <b>106</b><i>a </i>comprises a hyperlink (e.g., the call-to-action link <b>108</b>) to perform the task <b>112</b><i>a</i>. For example, the conversation agent <b>144</b> generates the first response <b>106</b><i>a </i>based on the detected intent <b>130</b><i>a</i>, similar to that described in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0083" num="0080">At step <b>206</b>, the conversation agent <b>144</b> extracts a first set of keywords <b>136</b> from the first response <b>106</b><i>a</i>. For example, the conversation agent <b>144</b> extracts the first set of keywords <b>136</b> by parsing the first response <b>106</b><i>a</i>. Assuming that the first response <b>104</b><i>a </i>is a broad response <b>106</b><i>a</i>-<b>1</b>, the conversation agent <b>144</b> extracts keywords <b>136</b><i>a</i>. Likewise, assuming that the first response <b>104</b><i>a </i>is an annotated response <b>106</b><i>a</i>-<b>2</b>, the conversation agent <b>144</b> extracts keywords <b>136</b><i>b. </i></p><p id="p-0084" num="0081">At step <b>208</b>, the conversation agent <b>144</b> receives a second voice signal <b>104</b><i>b</i>, where the second voice signal <b>104</b><i>b </i>indicates whether to access and/or activate the hyperlink (e.g., the call-to-action link <b>108</b>). For example, the second voice signal <b>104</b><i>b </i>may indicate whether to access the call-to-action link <b>108</b>, and upon activation of the call-to-action link <b>108</b>, the user <b>102</b> can perform the task <b>112</b><i>a </i>indicated in the first voice signal <b>104</b><i>a. </i></p><p id="p-0085" num="0082">At step <b>210</b>, the conversation agent <b>144</b> extracts a second set of keywords <b>116</b><i>b </i>from the second voice signal <b>104</b><i>b</i>. For example, the conversation agent <b>144</b> extracts the second set of keywords <b>116</b><i>b </i>by feeding the second voice signal <b>104</b><i>b </i>to the machine learning algorithm <b>152</b>, similar to that described in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0086" num="0083">At step <b>212</b>, the conversation agent <b>144</b> determines a granularity level <b>132</b> of the first response <b>106</b><i>a</i>, e.g. by parsing the first response <b>106</b><i>a</i>. For example, the conversation agent <b>144</b> may determine whether the first response <b>106</b><i>a </i>is a broad response <b>106</b><i>a</i>-<b>1</b> or an annotated response <b>106</b><i>a</i>-<b>2</b>. The conversation agent <b>144</b> determines that the granularity level <b>132</b> of the first response <b>106</b><i>a </i>indicates that the first response <b>106</b><i>a </i>is a broad response <b>106</b><i>a</i>-<b>1</b> if the first response <b>106</b><i>a </i>is not annotated with text conveying intent. The conversation agent <b>144</b> determines that the granularity level <b>132</b> of the first response <b>106</b><i>a </i>indicates that the first response <b>106</b><i>a </i>is an annotated response <b>106</b><i>a</i>-<b>2</b> if the first response <b>106</b><i>a </i>is annotated with intent <b>156</b>. Method <b>200</b> is directed to a use case where the first response <b>106</b><i>a </i>is a broad response <b>106</b><i>a</i>-<b>1</b>, as described below. The other use case where the first response <b>106</b><i>a </i>is an annotated response <b>106</b><i>a</i>-<b>2</b> is described in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0087" num="0084">At step <b>214</b>, the conversation agent <b>144</b> determines that the granularity level <b>132</b><i>a </i>of the first response <b>104</b><i>a </i>indicates that the first response <b>104</b><i>a </i>is not annotated with text conveying intent.</p><p id="p-0088" num="0085">At step <b>216</b>, the conversation agent <b>144</b> compares the first set of keywords <b>136</b><i>a </i>with the second set of keywords <b>116</b><i>b</i>. For example, the conversation agent <b>144</b> may compare each keyword <b>136</b><i>a </i>with each keyword <b>116</b><i>b </i>by executing a parsing algorithm.</p><p id="p-0089" num="0086">At step <b>218</b>, the conversation agent <b>144</b> determines whether the first set of keywords <b>136</b><i>a </i>corresponds to the second set of keywords <b>116</b><i>b</i>. For example, the conversation agent <b>144</b> may determine that the first set of keywords <b>136</b><i>a </i>corresponds to the second set of keywords <b>116</b><i>b </i>if more than a threshold percentage of keywords <b>136</b><i>a </i>(e.g., more than 90%, 95, etc.) correspond to the counterpart keywords <b>116</b><i>b. </i></p><p id="p-0090" num="0087">If the conversation agent <b>144</b> determines that the first set of keywords <b>136</b><i>a </i>correspond to the second set of keywords <b>116</b><i>b</i>, method <b>200</b> proceed to step <b>222</b>. Otherwise, method <b>200</b> proceeds to step <b>220</b>.</p><p id="p-0091" num="0088">At step <b>220</b>, the conversation agent <b>144</b> does not activate the call-to-action link <b>108</b>. The conversation agent <b>144</b> may also send a second response <b>106</b><i>b </i>asking the user <b>102</b> to confirm, revise, or update the call-to-action link <b>108</b>, task <b>112</b><i>b</i>, and/or task <b>112</b><i>a</i>. For example, the conversation agent <b>144</b> may send a second response <b>106</b><i>b </i>that comprises a second call-to-action link whose keywords correspond to the keywords <b>116</b><i>b</i>, similar to that described in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0092" num="0089">At step <b>222</b>, the conversation agent <b>144</b> activates the call-to-action link <b>108</b> to launch a particular webpage that is used to perform the task, similar to that described in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0093" num="0090">While several embodiments have been provided in the present disclosure, it should be understood that the disclosed systems and methods might be embodied in many other specific forms without departing from the spirit or scope of the present disclosure. The present examples are to be considered as illustrative and not restrictive, and the intention is not to be limited to the details given herein. For example, the various elements or components may be combined or integrated with another system or certain features may be omitted, or not implemented.</p><p id="p-0094" num="0091">In addition, techniques, systems, subsystems, and methods described and illustrated in the various embodiments as discrete or separate may be combined or integrated with other systems, modules, techniques, or methods without departing from the scope of the present disclosure. Other items shown or discussed as coupled or directly coupled or communicating with each other may be indirectly coupled or communicating through some interface, device, or intermediate component whether electrically, mechanically, or otherwise. Other examples of changes, substitutions, and alterations are ascertainable by one skilled in the art and could be made without departing from the spirit and scope disclosed herein.</p><p id="p-0095" num="0092">To aid the Patent Office, and any readers of any patent issued on this application in interpreting the claims appended hereto, applicants note that they do not intend any of the appended claims to invoke 35 U.S.C. &#xa7; 112(f) as it exists on the date of filing hereof unless the words &#x201c;means for&#x201d; or &#x201c;step for&#x201d; are explicitly used in the particular claim.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system for determining intent associated with a voice signal, comprising:<claim-text>a processor configured to:<claim-text>receive a first voice signal, wherein the first voice signal indicates to perform a task;</claim-text><claim-text>communicate a first response to the first voice signal, wherein the first response comprises a hyperlink associated with a particular webpage that is used to perform the task;</claim-text><claim-text>extract a first set of keywords from the first response;</claim-text><claim-text>receive a second voice signal, wherein the second voice signal indicates whether to access the hyperlink;</claim-text><claim-text>extract a second set of keywords from the second voice signal;</claim-text><claim-text>determine a level of granularity of the first response, wherein determining the level of granularity of the first response comprises determining whether or not the first response is annotated with text conveying a first intent;</claim-text><claim-text>in response to determining that the level of granularity of the first response indicates that the first response is not annotated with the text:<claim-text>compare the first set of keywords with the second set of keywords;</claim-text><claim-text>determine whether the first set of keywords corresponds to the second set of keywords; and</claim-text><claim-text>in response to determining that the first set of keywords corresponds to the second set of keywords, activate the hyperlink to launch the particular webpage to perform the task; and</claim-text></claim-text></claim-text><claim-text>a memory, operably coupled with the processor, and operable to store the first voice signal, the second voice signal, and the first response.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor is further configured to:<claim-text>in response to determining that the level of granularity of the first response indicates that the first response is annotated with the text:<claim-text>extract a first set of features from the first response, wherein the first set of features represents the first intent;</claim-text><claim-text>extract a second set of features from the second voice signal, wherein the second set of features represents a second intent associated with the second voice signal;</claim-text><claim-text>compare the first set of features with the second set of features;</claim-text><claim-text>determine whether the first set of features corresponds to the second set of features; and</claim-text><claim-text>in response to determining that the first set of features corresponds to the second set of features, activate the hyperlink to launch the particular webpage to perform the task.</claim-text></claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein:<claim-text>the first set of features is represented by a first vector comprising a first set of numerical values,</claim-text><claim-text>the second set of features is represented by a second vector comparing a second set of numerical values, and</claim-text><claim-text>determining whether the first set of features corresponds to the second set of features comprises:<claim-text>comparing each numerical value of the first vector with a counterpart numerical value of the second vector;</claim-text><claim-text>determining whether each numerical value of the first vector corresponds to the counterpart numerical value of the second vector; and</claim-text><claim-text>in response to determining that more than a threshold percentage of the first set of numerical values corresponds to the second set of numerical values, determining that the first set of features corresponds to the second set of features.</claim-text></claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining whether the first set of keywords corresponds to the second set of keywords, comprises:<claim-text>determining a percentage of keywords of the first set of keywords that matches to counterpart keywords of the second set of keywords;</claim-text><claim-text>determining whether the percentage of keywords of the first set of keywords exceeds a threshold percentage; and</claim-text><claim-text>in response to determining that the percentage of keywords of the first set of keywords exceeds the threshold percentage, determining that the first set of keywords corresponds to the second set of keywords.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor is further configured to, in response to determining that the first set of keywords does not correspond to the second set of keywords, communicate a second response comprising a second hyperlink whose keywords correspond to the second set of keywords.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the processor is further configured to, in response to determining that the first set of features does not correspond to the second set of features, communicate a third response comprising a second hyperlink whose features correspond to the second set of features.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the task comprises accessing an account.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A method for determining intent associated with a voice signal, comprising:<claim-text>receiving a first voice signal, wherein the first voice signal indicates to perform a task;</claim-text><claim-text>communicating a first response to the first voice signal, wherein the first response comprises a hyperlink associated with a particular webpage used to perform the task;</claim-text><claim-text>extracting a first set of keywords from the first response;</claim-text><claim-text>receiving a second voice signal, wherein the second voice signal indicates whether to access the hyperlink;</claim-text><claim-text>extracting a second set of keywords from the second voice signal;</claim-text><claim-text>determining a level of granularity of the first response, wherein determining the level of granularity of the first response comprises determining whether or not the first response is annotated with text conveying a first intent;</claim-text><claim-text>in response to determining that the level of granularity of the first response indicates that the first response is not annotated with the text:<claim-text>comparing the first set of keywords with the second set of keywords;</claim-text><claim-text>determining whether the first set of keywords corresponds to the second set of keywords; and</claim-text><claim-text>in response to determining that the first set of keywords corresponds to the second set of keywords, activating the hyperlink to launch the particular webpage to perform the task.</claim-text></claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:<claim-text>in response to determining that the level of granularity of the first response indicates that the first response is annotated with the text:<claim-text>extracting a first set of features from the first response, wherein the first set of features represents the first intent;</claim-text><claim-text>extracting a second set of features from the second voice signal, wherein the second set of features represents a second intent associated with the second voice signal;</claim-text><claim-text>comparing the first set of features with the second set of features;</claim-text><claim-text>determining whether the first set of features corresponds to the second set of features; and</claim-text><claim-text>in response to determining that the first set of features corresponds to the second set of features, activating the hyperlink to launch the particular webpage to perform the task.</claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein:<claim-text>the first set of features is represented by a first vector comprising a first set of numerical values,</claim-text><claim-text>the second set of features is represented by a second vector comparing a second set of numerical values, and</claim-text><claim-text>determining whether the first set of features corresponds to the second set of features comprises:<claim-text>comparing each numerical value of the first vector with a counterpart numerical value of the second vector;</claim-text><claim-text>determining whether each numerical value of the first vector corresponds to the counterpart numerical value of the second vector; and</claim-text><claim-text>in response to determining that more than a threshold percentage of the first set of numerical values corresponds to the second set of numerical values, determining that the first set of features corresponds to the second set of features.</claim-text></claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein determining whether the first set of keywords corresponds to the second set of keywords, comprises:<claim-text>determining a percentage of keywords of the first set of keywords that matches to counterpart keywords of the second set of keywords;</claim-text><claim-text>determining whether the percentage of keywords of the first set of keywords exceeds a threshold percentage; and</claim-text><claim-text>in response to determining that the percentage of keywords of the first set of keywords exceeds the threshold percentage, determining that the first set of keywords corresponds to the second set of keywords.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising, in response to determining that the first set of keywords does not correspond to the second set of keywords, communicating a second response comprising a second hyperlink whose keywords correspond to the second set of keywords.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising, in response to determining that the first set of features does not correspond to the second set of features, communicating a third response comprising a second hyperlink whose features correspond to the second set of features.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the task comprises accessing an account.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A computer program comprising executable instructions stored in a non-transitory computer-readable medium that when executed by a processor causes the processor to:<claim-text>receive a first voice signal, wherein the first voice signal indicates to perform a task;</claim-text><claim-text>communicate a first response to the first voice signal, wherein the first response comprises a hyperlink associated with a particular webpage used to perform the task;</claim-text><claim-text>extract a first set of keywords from the first response;</claim-text><claim-text>receive a second voice signal, wherein the second voice signal indicates whether to access the hyperlink;</claim-text><claim-text>extract a second set of keywords from the second voice signal;</claim-text><claim-text>determine a level of granularity of the first response, wherein determining the level of granularity of the first response comprises determining whether or not the first response is annotated with text conveying a first intent;</claim-text><claim-text>in response to determining that the level of granularity of the first response indicates that the first response is not annotated with the text:<claim-text>compare the first set of keywords with the second set of keywords;</claim-text><claim-text>determine whether the first set of keywords corresponds to the second set of keywords; and</claim-text><claim-text>in response to determining that the first set of keywords corresponds to the second set of keywords, activate the hyperlink to launch the particular webpage to perform the task.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computer program of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions when executed by the processor, further cause the processor to:<claim-text>in response to determining that the level of granularity of the first response indicates that the first response is annotated with the text:<claim-text>extract a first set of features from the first response, wherein the first set of features represents the first intent;</claim-text><claim-text>extract a second set of features from the second voice signal, wherein the second set of features represents a second intent associated with the second voice signal;</claim-text><claim-text>compare the first set of features with the second set of features;</claim-text><claim-text>determine whether the first set of features corresponds to the second set of features; and</claim-text><claim-text>in response to determining that the first set of features corresponds to the second set of features, activate the hyperlink to launch the particular webpage to perform the task.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The computer program of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein:<claim-text>the first set of features is represented by a first vector comprising a first set of numerical values,</claim-text><claim-text>the second set of features is represented by a second vector comparing a second set of numerical values, and</claim-text><claim-text>determining whether the first set of features corresponds to the second set of features comprises:<claim-text>comparing each numerical value of the first vector with a counterpart numerical value of the second vector;</claim-text><claim-text>determining whether each numerical value of the first vector corresponds to the counterpart numerical value of the second vector; and</claim-text><claim-text>in response to determining that more than a threshold percentage of the first set of numerical values corresponds to the second set of numerical values, determining that the first set of features corresponds to the second set of features.</claim-text></claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computer program of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein determining whether the first set of keywords corresponds to the second set of keywords, comprises:<claim-text>determining a percentage of keywords of the first set of keywords that matches to counterpart keywords of the second set of keywords;</claim-text><claim-text>determining whether the percentage of keywords of the first set of keywords exceeds a threshold percentage; and</claim-text><claim-text>in response to determining that the percentage of keywords of the first set of keywords exceeds the threshold percentage, determining that the first set of keywords corresponds to the second set of keywords.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The computer program of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions when executed by the processor, further cause the processor to, in response to determining that the first set of keywords does not correspond to the second set of keywords, communicate a second response comprising a second hyperlink whose keywords correspond to the second set of keywords.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computer program of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the instructions when executed by the processor, further cause the processor to, in response to determining that the first set of features does not correspond to the second set of features, communicate a third response comprising a second hyperlink whose features correspond to the second set of features.</claim-text></claim></claims></us-patent-application>