<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004430A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004430</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17856968</doc-number><date>20220702</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>48</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>4893</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">ESTIMATION OF POWER PROFILES FOR NEURAL NETWORK MODELS RUNNING ON AI ACCELERATORS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only"><addressbook><orgname>Intel Corporation</orgname><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Richmond</last-name><first-name>Richard</first-name><address><city>Belfast</city><country>GB</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Luk</last-name><first-name>Eric</first-name><address><city>Dublin</city><country>IE</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Zeng</last-name><first-name>Lingdan</first-name><address><city>Chandler</city><state>AZ</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Hacking</last-name><first-name>Lance</first-name><address><city>Spanish Fork</city><state>UT</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Palla</last-name><first-name>Alessandro</first-name><address><city>Pisa</city><country>IT</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Elmalaki</last-name><first-name>Mohamed</first-name><address><city>Gilbert</city><state>AZ</state><country>US</country></address></addressbook></inventor><inventor sequence="06" designation="us-only"><addressbook><last-name>Almalih</last-name><first-name>Sara</first-name><address><city>Scottsdale</city><state>AZ</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Technology for estimating neural network (NN) power profiles includes obtaining a plurality of workloads for a compiled NN model, the plurality of workloads determined for a hardware execution device, determining a hardware efficiency factor for the compiled NN model, and generating, based on the hardware efficiency factor, a power profile for the compiled NN model on one or more of a per-layer basis or a per-workload basis. The hardware efficiency factor can be determined on based on a hardware efficiency measurement and a hardware utilization measurement, and can be determined on a per-workload basis. A configuration file can be provided for generating the power profile, and an output visualization of the power profile can be generated. Further, feedback information can be generated to perform one or more of selecting a hardware device, optimizing a breakdown of workloads, optimizing a scheduling of tasks, or confirming a hardware device design.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="77.89mm" wi="158.75mm" file="US20230004430A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="236.64mm" wi="130.81mm" orientation="landscape" file="US20230004430A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="173.74mm" wi="103.80mm" file="US20230004430A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="246.89mm" wi="165.35mm" orientation="landscape" file="US20230004430A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="246.63mm" wi="161.21mm" orientation="landscape" file="US20230004430A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="243.42mm" wi="146.64mm" file="US20230004430A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="234.70mm" wi="158.75mm" orientation="landscape" file="US20230004430A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="246.21mm" wi="143.34mm" file="US20230004430A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="227.58mm" wi="159.34mm" orientation="landscape" file="US20230004430A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">Embodiments relate generally to computing systems. More particularly, embodiments relate to power profile estimation for neural networks on AI accelerators.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Neural networks are diverse in terms of their topology, computational intensity, and memory bandwidth requirements. For any given artificial intelligence (AI) accelerator architecture, the performance or overall throughput for any given network is a function of all three elements (e.g., topology, computational intensity, and memory bandwidth requirements). Within a network, the computational intensity can vary from layer to layer depending on the tensor dimensions, kernel size and other factors which determine how efficiently the operations map to underlying hardware architecture. Power consumption can vary greatly with computational intensity, leading to spikes in power consumption for significant periods even when the average power over a network is moderate. With current power modeling tools and performance simulators, however, only average power estimations are possible.</p><p id="p-0004" num="0003">Previous power modeling solutions tend to be populated with data from well-known power and performance benchmarks for long-established devices, such as central processing units (CPUs) and graphics processing units (GPUs). Generally, for such devices the prior architecture has only small deltas versus the next generation. By contrast, for AI hardware, architectures evolve quickly to keep apace the development of new network topologies and operations. Consequently, building an accurate power model for a next generation AI accelerator is challenging and time consuming.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0005" num="0004">The various advantages of the embodiments will become apparent to one skilled in the art by reading the following specification and appended claims, and by referencing the following drawings, in which:</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>1</b></figref> provides a block diagram illustrating an example of a neural network power profile estimation system according to one or more embodiments;</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>2</b></figref> provides a diagram of sample code illustrating generation of an example power configuration file according to one or more embodiments;</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>D</figref> provide examples of visualization graphs according to one or more embodiments;</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>4</b></figref> provides a flow chart illustrating an example method of generating a power profile for a neural network according to one or more embodiments;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram illustrating an example computing system for neural network power profile estimation according to one or more embodiments;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram illustrating an example semiconductor apparatus according to one or more embodiments;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram illustrating an example processor according to one or more embodiments; and</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a block diagram illustrating an example of a multiprocessor-based computing system according to one or more embodiments.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0014" num="0013">An improved computing system as described herein provides technology for estimating power profiles for neural networks on current or future architectures for peak power consumption as well as average power. The technology provides valuable information that can inform design decisions for networks, hardware architectures or micro-architectures, and system-on-chip (SoC) power delivery and management. The technology also enables per layer and/or per workload power estimation, enabling fine-grained power profiling&#x2014;which current models and simulators cannot achieve.</p><p id="p-0015" num="0014">The improved technology as described herein provides for measuring or estimating power consumption in neural networks on a workload by workload basis or a layer by layer basis, and further enhanced by taking hardware efficiency into consideration. For example, by utilizing neural network compiler and network performance simulation, which model the hardware architecture at an abstract level, the power consumption per workload or per layer is profiled in the neural network at an early stage in the architecture development. Estimating the power consumption on a workload by workload or layer by layer basis enables fine power optimization on AI accelerator devices. This allows a power profile to be constructed providing a fine-grained power estimation&#x2014;down to a per-workload or per-layer basis&#x2014;within a short compute run time. Such power profiling also gives insights which are valuable for SoC power delivery and power management planning. Additionally, when well calibrated against power estimations derived by annotation of switching activity&#x2014;e.g., from register-transfer level (RTL) simulation&#x2014;onto an implementation of a given design (e.g., using Synopsys PrimePower or similar tool), the system can be used to quickly provide power profiles for many neural networks with a quick turn-around time. The information obtained from such an exercise can be used to construct complex scenarios which would not otherwise be possible without an unfeasible amount of RTL simulation. This new system also enables non-experts in power estimation to easily create power estimation profiles for arbitrary neural networks.</p><p id="p-0016" num="0015">Power consumption in a digital circuit has two major elements: leakage power and dynamic power. Leakage depends on the technology process and the cell Vt (threshold voltage) type mix of the design. For example, a technology process library used to implement the design consists of standard cells which implement logic gates or sequential elements. Typically a library will have low (LVT), standard (SVT) and high (HVT) Vt versions of each cell. The LVT cells will be faster but will exhibit higher leakage current. Therefore in any implementation of a design there can be a trade-off between performance (speed) and power (leakage). Leakage can be modelled as a constant which scales approximately linearly with voltage (within a certain range) and non-linearly with temperature. Dynamic power can be further divided into two elements, the idle (or static) power and the application power. The idle power term models the power consumed by the design while idle, this is power consumed by the clock tree and other structural elements which is invariant with the amount of work being done (computational intensity). The application power is the portion of dynamic power which varies with work being done (and therefore switching activity) in the circuit for a constant voltage and frequency.</p><p id="p-0017" num="0016">Power estimation tools (such as, e.g., Synopsys PrimePower) can estimate the average power consumed by a technology implementation of a design by annotating switching activity captured from a simulation of the design to its netlist. A detailed breakdown of the power consumed at each level of hierarchy may be attained. The resulting power estimation will be for a given simulation (e.g. a particular application or workload), a given process, voltage and temperature (PVT) corner and a given frequency of operation. Process refers to the statistical distribution of fabricated chips and , for example, there can be slow, typical or fast process corners. A particular chip will be binned as either slow, typical or fast. However, the voltage and temperature it operates at may be dynamic variables for any operation or use case. Factoring out the leakage power, the dynamic power P<sub>dyn </sub>may then be used with the known voltage (V) and frequency (f) to give the dynamic capacitance (C<sub>dyn</sub>) of the design for the activity (A) of the particular application or workload as follows:</p><p id="p-0018" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>C</i><sub>dyn</sub><i>=P</i><sub>dyn</sub>/(<i>A*V</i><sup>2</sup><i>*f</i>) &#x2003;&#x2003;EQ (1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0019" num="0017">Cdyn therefore provides a constant which can be used to predict the power for different workloads (changing the activity A), different frequencies and/or voltages. As further described herein, the technology includes providing for power estimation with the additional accuracy provided with per layer per workload estimates enhanced with estimated hardware efficiency for the particular hardware device (e.g., AI accelerator). For example, this can include breaking an AI inference use case down into tens or hundreds of layers/workloads, and provides more accurate per layer/workload power estimation by taking into consideration the hardware efficiency as well as C<sub>dyn</sub>, application ratio (AppRatio), frequency, voltage, and leakage. The peak power of a function or hardware can be characterized as power for an application ratio of 100%. Other workloads (or applications) with a power profile lower than the peak can be defined as having an AppRatio less than 100%. Accordingly, the technology enables identification of which layers/workloads in a neural network are more power constrained.</p><p id="p-0020" num="0018"><figref idref="DRAWINGS">FIG. <b>1</b></figref> provides a block diagram illustrating an example of neural network power profile estimation system <b>100</b> according to one or more embodiments, with reference to components and features described herein including but not limited to the figures and associated description. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the system <b>100</b> includes a neural network (NN) model <b>110</b>, a neural network compiler <b>120</b>, a blob estimator <b>130</b>, a performance simulator <b>140</b>, a power simulator <b>150</b>, and an output unit <b>160</b>. In embodiments, the system <b>100</b> also includes one or more hardware device(s) <b>170</b> (e.g., hardware accelerator devices) under test. For example, hardware devices <b>170</b> under test can include a first device (HW Dev_A) <b>172</b>, a second device (HW Dev_B) <b>174</b>, and/or a third device (HW Dev_C) <b>176</b>. A fewer or greater number of devices can be included in the hardware devices <b>170</b>. The hardware devices <b>170</b> can include various types of AI accelerators such as, e.g., a vision processing unit (VPU). It will be understood that in embodiments one or more of the hardware devices <b>170</b> under test may not be an actual physical hardware device but, instead, can represent design criteria and parameters for existing or proposed hardware device architecture/designs.</p><p id="p-0021" num="0019">The neural network model <b>110</b> is a pre-trained model, which in embodiments can be received (obtained) by the system <b>100</b> from an AI framework. The NN model <b>110</b> can be developed using an AI framework from a variety of sources, including, for example, TensorFlow, ONNX Runtime, PyTorch, Caffe, OpenVino, etc. The NN model <b>110</b> typically includes information and data regarding the model architecture/topology (i.e., graph), including tensor volumes, nodes, operators, weights and biases. Each node in a model graph represents an operation (e.g. mathematical, logical operator etc.) which is evaluated at runtime. The NN model <b>110</b> is provided as input to the NN compiler <b>120</b>.</p><p id="p-0022" num="0020">The neural network compiler <b>120</b> compiles the NN model <b>110</b> into executable code to be run on selected hardware (e.g., one of the hardware devices <b>170</b>). The NN compiler <b>120</b> decomposes a network model into workloads which form a blob <b>125</b> that is adapted for execution on the selected hardware device <b>170</b> (e.g., an AI accelerator). As part of this process, the NN compiler <b>120</b> can decompose a layer or a full workload into many sub-workloads. For example, the neural network compiler <b>120</b> breaks the input tensor at a layer into a series of workloads which map efficiently to the hardware device <b>170</b> that is to execute the NN model <b>110</b>, where each layer of the neural network model will have an input tensor, which is generally the output tensor of the previously layer. A tensor is a multi-dimensional array of activations, where activations are individual data-points of a tensor. Additionally, the neural network compiler <b>120</b> performs task assignment, e.g. computation, direct memory access (DMA) to control data movement in and out of the hardware device <b>170</b>, etc.; and schedules each task on a timeline. The compiled results are stored in a blob <b>125</b>, which is a time graph with tasks placed in series or in parallel depending on the scheduling strategies and dependencies at each layer. The blob <b>125</b> describes each workload and the order dependencies between them.</p><p id="p-0023" num="0021">For example, typically the NN compiler <b>120</b> optimizes the executable blob <b>125</b> for a specific type of hardware device, such as, e.g., a specific type of AI accelerator (e.g., HW Dev_A <b>172</b>). Thus, for example, if a different type of hardware device (e.g., selection of a new or different type of hardware device, such as HW Dev_B <b>174</b>) is to be used for execution, the NN compiler <b>120</b> typically needs to be modified or changed to a version that is specifically designed to produce results for that type of hardware device.</p><p id="p-0024" num="0022">The blob estimator <b>130</b>, in conjunction with the performance simulator <b>140</b> and the power simulator <b>150</b>, provides for calculating the power from a series of sub-workloads (as generated by the NN compiler <b>120</b>) that form a fine-grained basis of the power estimation and, in addition, estimating the total energy by aggregating the energy from all sub-workloads. The blob estimator <b>130</b> extracts the tasks from the blob <b>125</b> and invokes the performance simulator <b>140</b> to provide performance metrics for execution of the blob <b>125</b> (representing the neural network model <b>110</b>) on the selected hardware device <b>170</b>. The performance metrics include the cost such as, e.g., the number of clock cycles required, or number of frames per second (e.g., frames relating to frames of an input sequence or video data), etc., the hardware utilization, and hardware efficiency. Hardware efficiency measures the relative turn-on time of the hardware, while hardware utilization measures how efficient the hardware is being utilized as the circuitry is being turned on. For example, one of the fundamental mathematical operations performed repetitively in neural networks is convolution and convolution de-composes into multiplying multiple pairs of values and accumulating their results. Hardware multiply-accumulate (MAC) arrays have often been used to support these computations, and AI accelerators often have large arrays of MACs. Thus, for MAC arrays hardware efficiency measures the relative turn-on time of the MAC array, while hardware utilization measures how efficient the MAC array is being utilized as the circuitry is being turned on.</p><p id="p-0025" num="0023">For example, based on the breakdown of the workloads for the selected hardware device <b>170</b>, the performance simulator <b>140</b> estimates (e.g., calculates) the hardware utilization (e.g., hardware utilization factor) and hardware efficiency (e.g., hardware efficiency factor). In embodiments, the hardware utilization factor and/or the hardware efficiency factor are determined on a per workload basis. This enables a determination of the power for each workload of which the neural network layer is composed, which then allows accurately estimating the total energy required for that layer. The hardware utilization and hardware efficiency are passed to the power simulator <b>150</b>.</p><p id="p-0026" num="0024">The power simulator <b>150</b> provides both per layer and average power estimations for internal and external AI hardware devices based on performance metrics from the performance simulator <b>140</b>. The power estimation is provided for different nodes of a power model&#x2014;each of which has a different C<sub>dyn </sub>constant. A node refers to hardware structure or operation that is active when performing the computations required to process the layers of a neural network. For example, a MAC array can be a node, or the on-chip SRAM memory used to store the data being processed can be a node. The power model would model the activity and power of nodes separately with certain operations being more or less costly for certain nodes, depending on whether, for example, that operation required more computation or more memory bandwidth, etc. These C<sub>dyn </sub>constants are populated with values based on well understood, high confidence power estimations for specific workloads, each of which is modeled differently in the performance and power simulator.</p><p id="p-0027" num="0025">The power simulator <b>150</b> receives the hardware utilization and hardware efficiency, as determined by the performance simulator <b>140</b>, and calculates the power for the given workload using the estimated hardware utilization and efficiency as a proxy for activity (A). In embodiments, the power simulator <b>150</b> provides flexible power configuration files to define various power nodes, where a power node models the power consumed in a design block or blocks on an AI accelerator. For example, a power node can have lower-level nodes (e.g., child nodes) depending on the accelerator design topology.</p><p id="p-0028" num="0026">In embodiments, equations for calculating average power of a power node are defined as follows:</p><p id="p-0029" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i><sub>dyn</sub>=HW<sub>util</sub>*AR*<i>C</i><sub>dyn</sub><i>*V</i><sup>2</sup><i>*F </i>&#x2003;&#x2003;EQ (2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0030" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i><sub>lkg</sub><i>=P</i><sub>ref_lkg</sub><i>*f</i>(<i>V,T</i>) &#x2003;&#x2003;EQ (3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0031" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i><sub>lkg</sub><i>=C</i><sub>idle</sub><i>*V</i><sup>2</sup><i>*F </i>&#x2003;&#x2003;EQ (4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0032" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i><sub>node</sub><i>=P</i><sub>dyn</sub><i>+P</i><sub>lkg</sub><i>+P</i><sub>idle </sub>&#x2003;&#x2003;EQ (5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0033" num="0027">where the parameters and variables are identified as follows:</p><p id="p-0034" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="49pt" align="left"/><colspec colname="2" colwidth="154pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry>Name</entry><entry>Description</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>F</entry><entry>Power node running frequency</entry></row><row><entry/><entry>V</entry><entry>Voltage</entry></row><row><entry/><entry>AR</entry><entry>Workload application ratio</entry></row><row><entry/><entry>HW<sub>util</sub></entry><entry>Hardware utilization</entry></row><row><entry/><entry>HW<sub>eff</sub></entry><entry>Hardware efficiency</entry></row><row><entry/><entry>C<sub>dyn</sub></entry><entry>Transistor switching capacitance</entry></row><row><entry/><entry>C<sub>idle</sub></entry><entry>Idle capacitance</entry></row><row><entry/><entry>T</entry><entry>Junction temperature</entry></row><row><entry/><entry>P<sub>dyn</sub></entry><entry>Dynamic power</entry></row><row><entry/><entry>P<sub>lkg</sub></entry><entry>Leakage power</entry></row><row><entry/><entry>P<sub>ref</sub>_lkg</entry><entry>Leakage power in typical condition (85C, TT)</entry></row><row><entry/><entry>P<sub>idle</sub></entry><entry>Idle power</entry></row><row><entry/><entry>P<sub>node</sub></entry><entry>Total power of a node</entry></row><row><entry/><entry>f(V,T)</entry><entry>function of Voltage (V) and temperature (T).</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0035" num="0028">Thus, the reference leakage is scaled by a factor f(V,T) that is a function of Voltage (V) and temperature (T). This is generally a non-linear scale factor which is empirically characterized for a particular silicon process. Typically f(V, T) can be implemented by a 2 dimensional look-up table indexed by V and T. Not only is power estimated with C<sub>dyn </sub>and leakages, the power simulator <b>150</b> also factors in hardware utilization and hardware efficiency for each workload to increase its accuracy and is able to run many fine-grained workloads and generate per workload power numbers instead of an average power in a short run time. In embodiments, equations for estimating power per workload are defined as follows (with reference to the above-listed parameters and variables):</p><p id="p-0036" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i><sub>dyn</sub>(WL)=HW<sub>eff</sub>(WL)*HW<sub>util</sub>(WL)*AR*<i>C</i><sub>dyn</sub><i>*V</i><sup>2</sup><i>* F </i>&#x2003;&#x2003;EQ (6)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0037" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i><sub>lkg</sub><i>=P</i><sub>ref_lkg</sub><i>*f</i>(<i>V,T</i>) &#x2003;&#x2003;EQ (7)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0038" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i><sub>idle</sub><i>=C</i><sub>idle</sub><i>*V</i><sup>2</sup><i>*F </i>&#x2003;&#x2003;EQ (8)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0039" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i><sub>node</sub><i>=P</i><sub>dyn</sub>(WL)+<i>P</i><sub>lkg</sub><i>+P</i><sub>idle </sub>&#x2003;&#x2003;EQ (9)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0040" num="0029">where P<sub>dyn</sub>(WL) is the estimated dynamic power for the given workload, HW<sub>eff</sub>(WL) is the determined hardware efficiency for the given workload, and HW<sub>util</sub>(WL) is the determined hardware utilization for the given workload. In embodiments a hardware efficiency factor is determined based on the hardware efficiency measurement and the hardware utilization measurement. In some embodiments similar equations can be defined with alternative parameters and variables. For example, the equations can use a single hardware efficiency factor that is based on the hardware efficiency measurement, on the hardware efficiency measurement and the hardware utilization measurement, or on another hardware efficiency metric. Per these or similar power equations, the power simulator <b>150</b> generates power estimates for all nodes with detailed power breakdowns per node, providing a greatly detailed power profile per workload. By splitting each network layer into multiple workloads, the hardware utilization and hardware efficiency are more readily assessed, and the power estimation is more fine-grained. Therefore, the estimated power is more accurate with the least run time.</p><p id="p-0041" num="0030">In one example, a NN model <b>110</b> has a layer, res3a branchl, which belongs to the model ResNet-50. This example layer has a workload of tensor shape [28, 28, 512] (undivided workload). In the example, the NN compiler <b>120</b> breaks that workload into 4 tiles of [28, 7, 512]. Subsequently, the NN compiler <b>120</b> splits each tile into 7 sub-workloads of [4, 4, 512] (Group A) and 7 sub-workloads of [4, 3, 512] (Group B). The sub-workloads for an example tile are illustrated in the following Table:</p><p id="p-0042" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="56pt" align="center"/><colspec colname="2" colwidth="91pt" align="center"/><colspec colname="3" colwidth="70pt" align="left"/><thead><row><entry namest="1" nameend="3" rowsep="1">TABLE 1</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row><row><entry>Group A</entry><entry>Group B</entry><entry/></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>[4, 4, 512] [4, 4, 512] [4, 4, 512] [4, 4, 512] [4, 4, 512] [4, 4, 512] [4, 4, 512]</entry><entry>[4, 3, 512] [4, 3, 512] [4, 3, 512] [4, 3, 512] [4, 3, 512] [4, 3, 512] [4, 3, 512]</entry><entry><chemistry id="CHEM-US-00001" num="00001"><img id="EMI-C00001" he="21.84mm" wi="4.23mm" file="US20230004430A1-20230105-C00001.TIF" alt="embedded image" img-content="table" img-format="tif"/></chemistry></entry></row><row><entry> </entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="147pt" align="center"/><colspec colname="2" colwidth="70pt" align="left"/><tbody valign="top"><row><entry><chemistry id="CHEM-US-00002" num="00002"><img id="EMI-C00002" he="4.57mm" wi="39.03mm" file="US20230004430A1-20230105-C00002.TIF" alt="embedded image" img-content="table" img-format="tif"/></chemistry></entry><entry/></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0043" num="0031">Using a conventional average power approach over the undivided workload of tensor shape [28, 28, 512], the average power calculated from the conventional method for this example, with no hardware efficiency factor, is 166.3 mW. After the workload is broken into tiles and each tile is further sub-divided into 7 sub-workloads of [4, 4, 512] (Group A) and 7 sub-workloads of [4, 3, 512] (Group A) (as illustrated in Table 1), the hardware efficiency factor in this example is determined to be 87.5%, and the power number for the workload is estimated as 146.1 mW. Accordingly, as illustrated by this example, using the techniques as described herein results in a more accurate power estimation.</p><p id="p-0044" num="0032">Moreover, the power simulator <b>150</b> provides configuration files that enable users to define voltage and frequency maps for various power nodes. For example, discrete voltage-frequency (V-F) points and exponential V-F curves are supported. The power simulator <b>150</b> further provides built-in equations to calculate dynamic power, leakage power and idle power of the power nodes. Thus, for each power node, users can define &#x201c;frequency&#x201d;, &#x201c;number of instances&#x201d;, &#x201c;C<sub>dyn</sub>&#x201d;, &#x201c;idle C<sub>dyn</sub>&#x201d;, and other parameters in the power configuration, and the power simulator <b>150</b> automatically links &#x201c;frequency&#x201d;, &#x201c;number of instances&#x201d;, &#x201c;utilization&#x201d; and &#x201c;read/write bandwidth&#x201d; to performance metrics derived from the performance simulator <b>140</b> to generate a power profile.</p><p id="p-0045" num="0033"><figref idref="DRAWINGS">FIG. <b>2</b></figref> provides a diagram of sample code illustrating generation of an example power configuration file <b>200</b> according to one or more embodiments, with reference to components and features described herein including but not limited to the figures and associated description. The code in <figref idref="DRAWINGS">FIG. <b>2</b></figref> provides an example of generic power nodes, the definition of leakage and C<sub>dyn </sub>constants for those nodes and how they can be hierarchically composed (with child nodes).</p><p id="p-0046" num="0034">Returning now to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in embodiments the blob estimator <b>130</b>, the performance simulator <b>140</b> and the power simulator <b>150</b> are in data communication with each other such that data, commands, queries, etc. can be passed between and among each of these units. In embodiments, the blob estimator <b>130</b>, the performance simulator <b>140</b> and the power simulator <b>150</b> are arranged in a hierarchical manner such that data, commands, queries, etc. can be passed, e.g., between the blob estimator <b>130</b> and the performance simulator <b>140</b>, and also between the performance simulator <b>140</b> and the power simulator <b>150</b>. Other embodiments can include a variety of arrangements of these components, including the sharing of functionality among the components or incorporating functionality of one component into another.</p><p id="p-0047" num="0035">The output unit <b>160</b> receives performance and power estimates from the blob estimator <b>130</b>, e.g., as provided via the performance simulator <b>140</b> and the power simulator <b>150</b>). As an example, the power per workload, once estimated, can be written to a profile file which describes the performance (the time a given workload took to execute) and the power consumed for each workload. In some embodiments, the performance and power estimates are provided to the output unit <b>160</b> via a function (not shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) to organize or format the data in a form that can be more readily sorted, analyzed and/or presented with available tools. As one example, data from the blob estimator <b>130</b> can be converted into serialized data using JSON (Javascript object notation). In embodiments, the output unit <b>160</b> also receives high-level information about the blob <b>125</b> from the NN compiler <b>120</b>. For example, the information about blob <b>125</b> includes information such as workloads, parameters or configuration of each workload (operation, input tensor, output tensor) and dependencies. The workloads and their dependencies together can represent the schedule, the order in which the workloads must be dispatched and executed.</p><p id="p-0048" num="0036">Based on the received information, the output unit <b>160</b> determines metrics and provides fine-grained results, e.g., a performance profile&#x2014;including, e.g., key performance index (KPI) measurements, and/or a power profile&#x2014;including, e.g., power measurements. The performance profile provides, e.g., workload performance with separate timelines for different types of tasks and/or instances of computation engines. The power profile provides, e.g., estimated power consumption for workloads across a similar timeline.</p><p id="p-0049" num="0037">In embodiments, the output unit <b>160</b> includes a visualizer <b>162</b>. The visualizer <b>162</b> generates visual fine-grained output&#x2014;for example, in the form of a graph&#x2014;of the performance profile and/or the power profile. <figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>D</figref> provide examples of visualization graphs according to one or more embodiments, with reference to components and features described herein including but not limited to the figures and associated description. Turning to <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, shown is an example of a performance profile graph <b>300</b>, where the performance profile is shown in terms of execution time, with separate timelines for different types of tasks and/or instances of computation engines. An expanded view <b>325</b> of a portion of the example performance profile graph is shown in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>. The timeline is represented in terms of milliseconds (MS). For example, the top row of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> shows the performance for a controller task (e.g., a CPU interrogating the schedule and dispatching workloads for execution on the computational units, or DMA tasks). The second row shows the performance for Direct Memory Access (DMA) tasks moving data from external memory into the accelerator's internal memory array (DDR refers to dual data rate dynamic random access memory, which in examples serves as main computing system memory). The bottom two rows show performance for two computational tasks for the neural network executed in parallel on the AI accelerator under test, where each block indicates a separate workload.</p><p id="p-0050" num="0038">Turning now to <figref idref="DRAWINGS">FIG. <b>3</b>C</figref>, shown is an example of a power profile graph <b>350</b>, where the power profile is shown in terms of a reference time sequence (similar to the reference time sequence of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>). An expanded view <b>375</b> of a portion of the example power profile graph is shown in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>. The graph <b>350</b> displays individual timelines for estimated power for DMA processing (e.g., moving data in and out of the AI accelerator); data processing unit (DPU) processing (e.g., processing, by the AI accelerator, of tensor data, including convolutions); and static power (e.g., no processing occurs, but the clock signal is generated which consumes power). Average power is shown above the graph. The graph <b>350</b> also displays a line showing the total estimated power (e.g., sum of DMA power, DPU power and static power). The visualizer <b>162</b> can also display average power across the timeline. As illustrated in the example graph <b>350</b>, the estimated power consumption can vary widely as the NN executes. Further, various peaks in power consumption can be identified. For example, in embodiments the peaks are used to identify when the peak power exceeds a power limitation or threshold. As shown in comparing Ms. <b>3</b>A-<b>3</b>B with <figref idref="DRAWINGS">FIGS. <b>3</b>C-<b>3</b>D</figref>, there are correlations between the workloads in <figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>B</figref> and the power profile in <figref idref="DRAWINGS">FIGS. <b>3</b>C-<b>3</b>D</figref>. For example, where there are concurrent tasks in <figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>B</figref> that are corresponding peaks in the power profile in <figref idref="DRAWINGS">FIGS. <b>3</b>C-<b>3</b>D</figref>.</p><p id="p-0051" num="0039">Returning now to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, In embodiments, the output unit <b>160</b> includes a feedback unit <b>164</b>. The feedback unit <b>164</b> generates performance and power feedback information, based on the per-layer or per-workload power estimates, to be used in one or more various ways. In embodiments, the feedback unit <b>164</b> provides information to identify those device(s) under test (e.g., one of the devices <b>170</b>) that perform better in terms of power profiling and/or performance profiling that other devices (e.g., another of the devices <b>170</b>). For example, such feedback information is used to select a better-performing device for implementing the neural network for inference. In embodiments, the feedback unit <b>164</b> provides information to identify changes in the neural network model <b>110</b> and/or the NN compiler <b>120</b>. For example, such feedback information is used to optimize the breakdown of workloads by the compiler into tasks that reduce power spikes or average power consumed. As another example, such feedback information is used to optimize scheduling of tasks by the compiler to be performed by the hardware device. In embodiments, the feedback unit <b>164</b> provides information to confirm the design of a device <b>170</b>. For example, such feedback information is used to confirm that any power spikes fall within acceptable parameters (such as, e.g., below a threshold power level).</p><p id="p-0052" num="0040">Some or all components in the system <b>100</b> can be implemented using one or more of a central processing unit (CPU), a graphics processing unit (GPU), an artificial intelligence (AI) accelerator, a field programmable gate array (FPGA) accelerator, an application specific integrated circuit (ASIC), and/or via a processor with software, or in a combination of a processor with software and an FPGA or ASIC. More particularly, components of the system <b>100</b> can be implemented in one or more modules as a set of program or logic instructions stored in a machine- or computer-readable storage medium such as random access memory (RAM), read only memory (ROM), programmable ROM (PROM), firmware, flash memory, etc., in hardware, or any combination thereof. For example, hardware implementations can include configurable logic, fixed-functionality logic, or any combination thereof. Examples of configurable logic include suitably configured programmable logic arrays (PLAs), FPGAs, complex programmable logic devices (CPLDs), and general purpose microprocessors. Examples of fixed-functionality logic include suitably configured ASICs, combinational logic circuits, and sequential logic circuits. The configurable or fixed-functionality logic can be implemented with complementary metal oxide semiconductor (CMOS) logic circuits, transistor-transistor logic (TTL) logic circuits, or other circuits.</p><p id="p-0053" num="0041">For example, computer program code to carry out operations by the system <b>100</b> can be written in any combination of one or more programming languages, including an object oriented programming language such as JAVA, SMALLTALK, C++ or the like and conventional procedural programming languages, such as the &#x201c;C&#x201d; programming language or similar programming languages. Additionally, program or logic instructions might include assembler instructions, instruction set architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, state-setting data, configuration data for integrated circuitry, state information that personalizes electronic circuitry and/or other structural components that are native to hardware (e.g., host processor, central processing unit/CPU, microcontroller, etc.).</p><p id="p-0054" num="0042"><figref idref="DRAWINGS">FIG. <b>4</b></figref> provides a flow chart illustrating an example method <b>400</b> of generating a power profile for a neural network according to one or more embodiments, with reference to components and features described herein including but not limited to the figures and associated description. The method <b>400</b> can generally be implemented in the system <b>100</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>, already discussed) and/or via components of the system <b>100</b>, such as, e.g., the blob estimator <b>130</b>, the performance simulator <b>140</b> and/or the power simulator <b>150</b>. More particularly, the method <b>400</b> can be implemented as one or more modules as a set of logic instructions stored in a machine- or computer-readable storage medium such as RAM, ROM, PROM, firmware, flash memory, etc., in hardware, or any combination thereof. For example, hardware implementations can include configurable logic, fixed-functionality logic, or any combination thereof. Examples of configurable logic include suitably configured PLAs, FPGAs, CPLDs, and general purpose microprocessors. Examples of fixed-functionality logic include suitably configured ASICs, combinational logic circuits, and sequential logic circuits. The configurable or fixed-functionality logic can be implemented with CMOS logic circuits, TTL logic circuits, or other circuits.</p><p id="p-0055" num="0043">For example, computer program code to carry out operations shown in the method <b>400</b> can be written in any combination of one or more programming languages, including an object oriented programming language such as JAVA, SMALLTALK, C++ or the like and conventional procedural programming languages, such as the &#x201c;C&#x201d; programming language or similar programming languages. Additionally, program or logic instructions might include assembler instructions, instruction set architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, state-setting data, configuration data for integrated circuitry, state information that personalizes electronic circuitry and/or other structural components that are native to hardware (e.g., host processor, central processing unit/CPU, microcontroller, etc.).</p><p id="p-0056" num="0044">Illustrated processing block <b>410</b> provides for obtaining a plurality of workloads for a compiled neural network (NN) model, where at block <b>410</b><i>a </i>the plurality of workloads are determined for a hardware execution device. Illustrated processing block <b>420</b> provides for determining a hardware efficiency factor for the compiled NN model. In some embodiments, at block <b>420</b><i>a </i>the hardware efficiency factor is determined on based on a hardware efficiency measurement and a hardware utilization measurement. In some embodiments, at block <b>420</b><i>b </i>the hardware efficiency factor is determined on a per-workload basis. Illustrated processing block <b>430</b> provides for generating, based on the hardware efficiency factor, a power profile for the compiled NN model on one or more of a per-layer basis or a per-workload basis.</p><p id="p-0057" num="0045">In some embodiments, the method <b>400</b> includes, at processing block <b>440</b>, providing a configuration file to be used for generating the power profile. In some embodiments, the method <b>400</b> includes, at processing block <b>450</b>, generating an output visualization of the power profile. In some embodiments, the visualization includes a power profile graph. In some embodiments, a performance profile graph is also generated. In some embodiments, the method <b>400</b> includes, at processing block <b>460</b>, generating feedback information to perform one or more of selecting a hardware device, optimizing a breakdown of workloads, optimizing a scheduling of tasks, or confirming a hardware device design.</p><p id="p-0058" num="0046">Embodiments of each of the above systems, devices, components and/or methods, including the system <b>100</b> (or any components thereof), the process <b>400</b>, and/or any other system components, can be implemented in hardware, software, or any suitable combination thereof. For example, hardware implementations can include configurable logic, fixed-functionality logic, or any combination thereof. Examples of configurable logic include suitably configured PLAs, FPGAs, CPLDs, and general purpose microprocessors. Examples of fixed-functionality logic include suitably configured ASICs, combinational logic circuits, and sequential logic circuits. The configurable or fixed-functionality logic can be implemented with CMOS logic circuits, TTL logic circuits, or other circuits. For example, embodiments of each of the above systems, devices, components and/or methods can be implemented via the system <b>10</b> (<figref idref="DRAWINGS">FIG. <b>5</b></figref>, discussed further below), the semiconductor apparatus <b>30</b> (<figref idref="DRAWINGS">FIG. <b>6</b></figref>, discussed further below), the processor <b>40</b> (<figref idref="DRAWINGS">FIG. <b>7</b></figref>, discussed further below), and/or the computing system <b>60</b> (<figref idref="DRAWINGS">FIG. <b>8</b></figref>, discussed further below).</p><p id="p-0059" num="0047">Alternatively, or additionally, all or portions of the foregoing systems and/or devices and/or components and/or methods can be implemented in one or more modules as a set of program or logic instructions stored in a machine- or computer-readable storage medium such as RAM, ROM, PROM, firmware, flash memory, etc., to be executed by a processor or computing device. For example, computer program code to carry out the operations of the components can be written in any combination of one or more operating system (OS) applicable/appropriate programming languages, including an object-oriented programming language such as PYTHON, PERL, JAVA, SMALLTALK, C++, C# or the like and conventional procedural programming languages, such as the &#x201c;C&#x201d; programming language or similar programming languages.</p><p id="p-0060" num="0048"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a block diagram illustrating an example computing system <b>10</b> for estimating power profiles for neural networks according to one or more embodiments, with reference to components and features described herein including but not limited to the figures and associated description. The system <b>10</b> can generally be part of an electronic device/platform having computing and/or communications functionality (e.g., server, cloud infrastructure controller, database controller, notebook computer, desktop computer, personal digital assistant/PDA, tablet computer, convertible tablet, smart phone, etc.), imaging functionality (e.g., camera, camcorder), media playing functionality (e.g., smart television/TV), wearable functionality (e.g., watch, eyewear, headwear, footwear, jewelry), vehicular functionality (e.g., car, truck, motorcycle), robotic functionality (e.g., autonomous robot), Internet of Things (IoT) functionality, etc., or any combination thereof. In the illustrated example, the system <b>10</b> can include a host processor <b>12</b> (e.g., central processing unit/CPU) having an integrated memory controller (WIC) <b>14</b> that can be coupled to system memory <b>20</b>. The host processor <b>12</b> can include any type of processing device, such as, e.g., microcontroller, microprocessor, RISC processor, ASIC, etc., along with associated processing modules or circuitry. The system memory <b>20</b> can include any non-transitory machine- or computer-readable storage medium such as RAM, ROM, PROM, EEPROM, firmware, flash memory, etc., configurable logic such as, for example, PLAs, FPGAs, CPLDs, fixed-functionality hardware logic using circuit technology such as, for example, ASIC, CMOS or TTL technology, or any combination thereof suitable for storing instructions <b>28</b>.</p><p id="p-0061" num="0049">The system <b>10</b> can also include an input/output (I/O) subsystem <b>16</b>. The I/O subsystem <b>16</b> can communicate with for example, one or more input/output (I/O) devices <b>17</b>, a network controller <b>24</b> (e.g., wired and/or wireless NIC), and storage <b>22</b>. The storage <b>22</b> can be comprised of any appropriate non-transitory machine- or computer-readable memory type (e.g., flash memory, DRAM, SRAM (static random access memory), solid state drive (SSD), hard disk drive (HDD), optical disk, etc.). The storage <b>22</b> can include mass storage. In some embodiments, the host processor <b>12</b> and/ or the I/O subsystem <b>16</b> can communicate with the storage <b>22</b> (all or portions thereof) via a network controller <b>24</b>. In some embodiments, the system <b>10</b> can also include a graphics processor <b>26</b> (e.g., a graphics processing unit/GPU and/or an AI accelerator <b>27</b>. In an embodiment, the system <b>10</b> can also include a vision processing unit (VPU), not shown.</p><p id="p-0062" num="0050">The host processor <b>12</b> and the I/O subsystem <b>16</b> can be implemented together on a semiconductor die as a system on chip (SoC) <b>11</b>, shown encased in a solid line. The SoC <b>11</b> can therefore operate as a computing apparatus for estimating power profiles for neural networks. In some embodiments, the SoC <b>11</b> can also include one or more of the system memory <b>20</b>, the network controller <b>24</b>, and/or the graphics processor <b>26</b> (shown encased in dotted lines). In some embodiments, the SoC <b>11</b> can also include other components of the system <b>10</b>.</p><p id="p-0063" num="0051">The host processor <b>12</b> and/or the I/O subsystem <b>16</b> can execute program instructions <b>28</b> retrieved from the system memory <b>20</b> and/or the storage <b>22</b> to perform one or more aspects of process <b>400</b> as described herein with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The system <b>10</b> can implement one or more aspects or components of the system <b>100</b> as described herein with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The system <b>10</b> is therefore considered to be performance-enhanced at least to the extent that the technology provides the ability to measure or estimate power consumption in neural networks on a workload by workload basis or a layer by layer basis, and further enhanced by taking hardware efficiency into consideration.</p><p id="p-0064" num="0052">Computer program code to carry out the processes described above can be written in any combination of one or more programming languages, including an object-oriented programming language such as JAVA, JAVASCRIPT, PYTHON, SMALLTALK, C++ or the like and/or conventional procedural programming languages, such as the &#x201c;C&#x201d; programming language or similar programming languages, and implemented as program instructions <b>28</b>. Additionally, program instructions <b>28</b> can include assembler instructions, instruction set architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, state-setting data, configuration data for integrated circuitry, state information that personalizes electronic circuitry and/or other structural components that are native to hardware (e.g., host processor, central processing unit/CPU, microcontroller, microprocessor, etc.).</p><p id="p-0065" num="0053">I/O devices <b>17</b> can include one or more of input devices, such as a touch-screen, keyboard, mouse, cursor-control device, touch-screen, microphone, digital camera, video recorder, camcorder, biometric scanners and/or sensors; input devices can be used to enter information and interact with system <b>10</b> and/or with other devices. The I/O devices <b>17</b> can also include one or more of output devices, such as a display (e.g., touch screen, liquid crystal display/LCD, light emitting diode/LED display, plasma panels, etc.), speakers and/or other visual or audio output devices. The input and/or output devices can be used, e.g., to provide a user interface.</p><p id="p-0066" num="0054"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a block diagram illustrating an example semiconductor apparatus <b>30</b> for estimating power profiles for neural networks according to one or more embodiments, with reference to components and features described herein including but not limited to the figures and associated description. The semiconductor apparatus <b>30</b> can be implemented, e.g., as a chip, die, or other semiconductor package. The semiconductor apparatus <b>30</b> can include one or more substrates <b>32</b> comprised of, e.g., silicon, sapphire, gallium arsenide, etc. The semiconductor apparatus <b>30</b> can also include logic <b>34</b> comprised of, e.g., transistor array(s) and other integrated circuit (IC) components) coupled to the substrate(s) <b>32</b>. The logic <b>34</b> can be implemented at least partly in configurable logic or fixed-functionality logic hardware. The logic <b>34</b> can implement the system on chip (SoC) <b>11</b> described above with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>. The logic <b>34</b> can implement one or more aspects of the processes described above, including process <b>400</b>. The logic <b>34</b> can implement one or more aspects or components of the system <b>100</b> as described herein with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The apparatus <b>30</b> is therefore considered to be performance-enhanced at least to the extent that the technology provides the ability to measure or estimate power consumption in neural networks on a workload by workload basis or a layer by layer basis, and further enhanced by taking hardware efficiency into consideration.</p><p id="p-0067" num="0055">The semiconductor apparatus <b>30</b> can be constructed using any appropriate semiconductor manufacturing processes or techniques. For example, the logic <b>34</b> can include transistor channel regions that are positioned (e.g., embedded) within the substrate(s) <b>32</b>. Thus, the interface between the logic <b>34</b> and the substrate(s) <b>32</b> may not be an abrupt junction. The logic <b>34</b> can also be considered to include an epitaxial layer that is grown on an initial wafer of the substrate(s) <b>34</b>.</p><p id="p-0068" num="0056"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram illustrating an example processor core <b>40</b> according to one or more embodiments, with reference to components and features described herein including but not limited to the figures and associated description. The processor core <b>40</b> can be the core for any type of processor, such as a micro-processor, an embedded processor, a digital signal processor (DSP), a network processor, a graphics processing unit (GPU), or other device to execute code. Although only one processor core <b>40</b> is illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a processing element can alternatively include more than one of the processor core <b>40</b> illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. The processor core <b>40</b> can be a single-threaded core or, for at least one embodiment, the processor core <b>40</b> can be multithreaded in that it can include more than one hardware thread context (or &#x201c;logical processor&#x201d;) per core.</p><p id="p-0069" num="0057"><figref idref="DRAWINGS">FIG. <b>7</b></figref> also illustrates a memory <b>41</b> coupled to the processor core <b>40</b>. The memory <b>41</b> can be any of a wide variety of memories (including various layers of memory hierarchy) as are known or otherwise available to those of skill in the art. The memory <b>41</b> can include one or more code <b>42</b> instruction(s) to be executed by the processor core <b>40</b>. The code <b>42</b> can implement one or more aspects of the process <b>400</b> described above. The processor core <b>40</b> can implement one or more aspects or components of the system <b>100</b> as described herein with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The processor core <b>40</b> can follow a program sequence of instructions indicated by the code <b>42</b>. Each instruction can enter a front end portion <b>43</b> and be processed by one or more decoders <b>44</b>. The decoder <b>44</b> can generate as its output a micro operation such as a fixed width micro operation in a predefined format, or can generate other instructions, microinstructions, or control signals which reflect the original code instruction. The illustrated front end portion <b>43</b> also includes register renaming logic <b>46</b> and scheduling logic <b>48</b>, which generally allocate resources and queue the operation corresponding to the convert instruction for execution.</p><p id="p-0070" num="0058">The processor core <b>40</b> is shown including execution logic <b>50</b> having a set of execution units <b>55</b>-<b>1</b> through <b>55</b>-N. Some embodiments can include a number of execution units dedicated to specific functions or sets of functions. Other embodiments can include only one execution unit or one execution unit that can perform a particular function. The illustrated execution logic <b>50</b> performs the operations specified by code instructions.</p><p id="p-0071" num="0059">After completion of execution of the operations specified by the code instructions, back end logic <b>58</b> retires the instructions of code <b>42</b>. In one embodiment, the processor core <b>40</b> allows out of order execution but requires in order retirement of instructions. Retirement logic <b>59</b> can take a variety of forms as known to those of skill in the art (e.g., re-order buffers or the like). In this manner, the processor core <b>40</b> is transformed during execution of the code <b>42</b>, at least in terms of the output generated by the decoder, the hardware registers and tables utilized by the register renaming logic <b>46</b>, and any registers (not shown) modified by the execution logic <b>50</b>.</p><p id="p-0072" num="0060">Although not illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a processing element can include other elements on chip with the processor core <b>40</b>. For example, a processing element can include memory control logic along with the processor core <b>40</b>. The processing element can include I/O control logic and/or can include I/O control logic integrated with memory control logic. The processing element can also include one or more caches.</p><p id="p-0073" num="0061"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a block diagram illustrating an example of a multi-processor based computing system <b>60</b> according to one or more embodiments, with reference to components and features described herein including but not limited to the figures and associated description. The multiprocessor system <b>60</b> includes a first processing element <b>70</b> and a second processing element <b>80</b>. While two processing elements <b>70</b> and <b>80</b> are shown, it is to be understood that an embodiment of the system <b>60</b> can also include only one such processing element.</p><p id="p-0074" num="0062">The system <b>60</b> is illustrated as a point-to-point interconnect system, wherein the first processing element <b>70</b> and the second processing element <b>80</b> are coupled via a point-to-point interconnect <b>71</b>. It should be understood that any or all of the interconnects illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref> can be implemented as a multi-drop bus rather than point-to-point interconnect.</p><p id="p-0075" num="0063">As shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, each of the processing elements <b>70</b> and <b>80</b> can be multicore processors, including first and second processor cores (i.e., processor cores <b>74</b><i>a </i>and <b>74</b><i>b </i>and processor cores <b>84</b><i>a </i>and <b>84</b><i>b </i>). Such cores <b>74</b><i>a </i>, <b>74</b><i>b </i>, <b>84</b><i>a </i>, <b>84</b><i>b </i>can be configured to execute instruction code in a manner similar to that discussed above in connection with <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0076" num="0064">Each processing element <b>70</b>, <b>80</b> can include at least one shared cache <b>99</b><i>a </i>, <b>99</b><i>b </i>. The shared cache <b>99</b><i>a </i>, <b>99</b><i>b </i>can store data (e.g., instructions) that are utilized by one or more components of the processor, such as the cores <b>74</b><i>a </i>, <b>74</b><i>b </i>and <b>84</b><i>a </i>, <b>84</b><i>b </i>, respectively. For example, the shared cache <b>99</b><i>a </i>, <b>99</b><i>b </i>can locally cache data stored in a memory <b>62</b>, <b>63</b> for faster access by components of the processor. In one or more embodiments, the shared cache <b>99</b><i>a </i>, <b>99</b><i>b </i>can include one or more mid-level caches, such as level 2 (L2), level 3 (L3), level 4 (L4), or other levels of cache, a last level cache (LLC), and/or combinations thereof.</p><p id="p-0077" num="0065">While shown with only two processing elements <b>70</b>, <b>80</b>, it is to be understood that the scope of the embodiments is not so limited. In other embodiments, one or more additional processing elements can be present in a given processor. Alternatively, one or more of the processing elements <b>70</b>, <b>80</b> can be an element other than a processor, such as an accelerator or a field programmable gate array. For example, additional processing element(s) can include additional processors(s) that are the same as a first processor <b>70</b>, additional processor(s) that are heterogeneous or asymmetric to processor a first processor <b>70</b>, accelerators (such as, e.g., graphics accelerators or digital signal processing (DSP) units), field programmable gate arrays, or any other processing element. There can be a variety of differences between the processing elements <b>70</b>, <b>80</b> in terms of a spectrum of metrics of merit including architectural, micro architectural, thermal, power consumption characteristics, and the like. These differences can effectively manifest themselves as asymmetry and heterogeneity amongst the processing elements <b>70</b>, <b>80</b>. For at least one embodiment, the various processing elements <b>70</b>, <b>80</b> can reside in the same die package.</p><p id="p-0078" num="0066">The first processing element <b>70</b> can further include memory controller logic (MC) <b>72</b> and point-to-point (P-P) interfaces <b>76</b> and <b>78</b>. Similarly, the second processing element <b>80</b> can include a MC <b>82</b> and P-P interfaces <b>86</b> and <b>88</b>. As shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, MC's <b>72</b> and <b>82</b> couple the processors to respective memories, namely a memory <b>62</b> and a memory <b>63</b>, which can be portions of main memory locally attached to the respective processors. While the MC <b>72</b> and <b>82</b> is illustrated as integrated into the processing elements <b>70</b>, <b>80</b>, for alternative embodiments the MC logic can be discrete logic outside the processing elements <b>70</b>, <b>80</b> rather than integrated therein.</p><p id="p-0079" num="0067">The first processing element <b>70</b> and the second processing element <b>80</b> can be coupled to an I/O subsystem <b>90</b> via P-P interconnects <b>76</b> and <b>86</b>, respectively. As shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the I/O subsystem <b>90</b> includes P-P interfaces <b>94</b> and <b>98</b>. Furthermore, the I/O subsystem <b>90</b> includes an interface <b>92</b> to couple I/O subsystem <b>90</b> with a high performance graphics engine <b>64</b>. In one embodiment, a bus <b>73</b> can be used to couple the graphics engine <b>64</b> to the I/O subsystem <b>90</b>. Alternately, a point-to-point interconnect can couple these components.</p><p id="p-0080" num="0068">In turn, the I/O subsystem <b>90</b> can be coupled to a first bus <b>65</b> via an interface <b>96</b>. In one embodiment, the first bus <b>65</b> can be a Peripheral Component Interconnect (PCI) bus, or a bus such as a PCI Express bus or another third generation I/O interconnect bus, although the scope of the embodiments are not so limited.</p><p id="p-0081" num="0069">As shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, various I/O devices <b>65</b><i>a </i>(e.g., biometric scanners, speakers, cameras, and/or sensors) can be coupled to the first bus <b>65</b>, along with a bus bridge <b>66</b> which can couple the first bus <b>65</b> to a second bus <b>67</b>. In one embodiment, the second bus <b>67</b> can be a low pin count (LPC) bus. Various devices can be coupled to the second bus <b>67</b> including, for example, a keyboard/mouse <b>67</b><i>a </i>, communication device(s) <b>67</b><i>b </i>, and a data storage unit <b>68</b> such as a disk drive or other mass storage device which can include code <b>69</b>, in one embodiment. The illustrated code <b>69</b> can implement one or more aspects of the processes described above, including the process <b>400</b>. The illustrated code <b>69</b> can be similar to the code <b>42</b> (<figref idref="DRAWINGS">FIG. <b>7</b></figref>), already discussed. Further, an audio I/O <b>67</b><i>c </i>can be coupled to second bus <b>67</b> and a battery <b>61</b> can supply power to the computing system <b>60</b>. The system <b>60</b> can implement one or more aspects or components of the system <b>100</b> as described herein with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0082" num="0070">Note that other embodiments are contemplated. For example, instead of the point-to-point architecture of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, a system can implement a multi-drop bus or another such communication topology. Also, the elements of <figref idref="DRAWINGS">FIG. <b>8</b></figref> can alternatively be partitioned using more or fewer integrated chips than shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><heading id="h-0005" level="1">ADDITIONAL NOTES AND EXAMPLES</heading><p id="p-0083" num="0071">Example 1 includes a performance-enhanced computing system comprising a processor, and memory coupled to the processor, the memory to store instructions which, when executed by the processor, cause the computing system to obtain a plurality of workloads for a compiled neural network (NN) model, the plurality of workloads determined for a hardware execution device, determine a hardware efficiency factor for the compiled NN model, and generate, based on the hardware efficiency factor, a power profile for the compiled NN model on one or more of a per-layer basis or a per-workload basis.</p><p id="p-0084" num="0072">Example 2 includes the computing system of Example 1, wherein the hardware efficiency factor is determined on based on a hardware efficiency measurement and a hardware utilization measurement.</p><p id="p-0085" num="0073">Example 3 includes the computing system of Example 1, wherein the hardware efficiency factor is determined on a per-workload basis.</p><p id="p-0086" num="0074">Example 4 includes the computing system of Example 1, wherein the instructions, when executed, further cause the computing system to provide a configuration file to be used for generating the power profile.</p><p id="p-0087" num="0075">Example 5 includes the computing system of Example 1, wherein the instructions, when executed, further cause the computing system to generate an output visualization of the power profile.</p><p id="p-0088" num="0076">Example 6 includes the computing system of any one of Examples 1 to 5, wherein the instructions, when executed, further cause the computing system to generate feedback information to perform one or more of selecting a hardware device, optimizing a breakdown of workloads, optimizing a scheduling of tasks, or confirming a hardware device design.</p><p id="p-0089" num="0077">Example 7 includes a semiconductor apparatus comprising one or more substrates, and logic coupled to the one or more substrates, wherein the logic is implemented at least partly in one or more of configurable logic or fixed-functionality hardware logic, the logic to obtain a plurality of workloads for a compiled neural network (NN) model, the plurality of workloads determined for a hardware execution device, determine a hardware efficiency factor for the compiled NN model, and generate, based on the hardware efficiency factor, a power profile for the compiled NN model on one or more of a per-layer basis or a per-workload basis.</p><p id="p-0090" num="0078">Example 8 includes the apparatus of Example 7, wherein the hardware efficiency factor is determined on based on a hardware efficiency measurement and a hardware utilization measurement.</p><p id="p-0091" num="0079">Example 9 includes the apparatus of Example 7, wherein the hardware efficiency factor is determined on a per-workload basis.</p><p id="p-0092" num="0080">Example 10 includes the apparatus of Example 7, wherein the logic is further to provide a configuration file to be used for generating the power profile.</p><p id="p-0093" num="0081">Example 11 includes the apparatus of Example 7, wherein the logic is further to generate an output visualization of the power profile.</p><p id="p-0094" num="0082">Example 12 includes the apparatus of any one of Examples 7 to 11, wherein the logic is further to generate feedback information to perform one or more of selecting a hardware device, optimizing a breakdown of workloads, optimizing a scheduling of tasks, or confirming a hardware device design.</p><p id="p-0095" num="0083">Example 13 includes the apparatus of any one of Examples 7 to 12, wherein the logic coupled to the one or more substrates includes transistor channel regions that are positioned within the one or more substrates.</p><p id="p-0096" num="0084">Example 14 includes at least one computer readable storage medium comprising a set of instructions which, when executed by a computing system, cause the computing system to obtain a plurality of workloads for a compiled neural network (NN) model, the plurality of workloads determined for a hardware execution device, determine a hardware efficiency factor for the compiled NN model, and generate, based on the hardware efficiency factor, a power profile for the compiled NN model on one or more of a per-layer basis or a per-workload basis.</p><p id="p-0097" num="0085">Example 15 includes the at least one computer readable storage medium of Example 14, wherein the hardware efficiency factor is determined on based on a hardware efficiency measurement and a hardware utilization measurement.</p><p id="p-0098" num="0086">Example 16 includes the at least one computer readable storage medium of Example 14, wherein the hardware efficiency factor is determined on a per-workload basis.</p><p id="p-0099" num="0087">Example 17 includes the at least one computer readable storage medium of Example 14, wherein the instructions, when executed, further cause the computing system to provide a configuration file to be used for generating the power profile.</p><p id="p-0100" num="0088">Example 18 includes the at least one computer readable storage medium of Example 14, wherein the instructions, when executed, further cause the computing system to generate an output visualization of the power profile.</p><p id="p-0101" num="0089">Example 19 includes the at least one computer readable storage medium of any one of Examples 14 to 18, wherein the instructions, when executed, further cause the computing system to generate feedback information to perform one or more of selecting a hardware device, optimizing a breakdown of workloads, optimizing a scheduling of tasks, or confirming a hardware device design.</p><p id="p-0102" num="0090">Example 20 includes a method comprising obtaining a plurality of workloads for a compiled neural network (NN) model, the plurality of workloads determined for a hardware execution device, determining a hardware efficiency factor for the compiled NN model, and generating, based on the hardware efficiency factor, a power profile for the compiled NN model on one or more of a per-layer basis or a per-workload basis.</p><p id="p-0103" num="0091">Example 21 includes the method of Example 20, wherein the hardware efficiency factor is determined on based on a hardware efficiency measurement and a hardware utilization measurement.</p><p id="p-0104" num="0092">Example 22 includes the method of Example 20, wherein the hardware efficiency factor is determined on a per-workload basis.</p><p id="p-0105" num="0093">Example 23 includes the method of Example 20, further comprising providing a configuration file to be used for generating the power profile.</p><p id="p-0106" num="0094">Example 24 includes the method of Example 20, further comprising generating an output visualization of the power profile.</p><p id="p-0107" num="0095">Example 25 includes the method of any one of Examples 20 to 24, further comprising generating feedback information to perform one or more of selecting a hardware device, optimizing a breakdown of workloads, optimizing a scheduling of tasks, or confirming a hardware device design.</p><p id="p-0108" num="0096">Example 26 includes an apparatus comprising means for performing the method of any one of Examples 20 to 25.</p><p id="p-0109" num="0097">Embodiments are applicable for use with all types of semiconductor integrated circuit (&#x201c;IC&#x201d;) chips. Examples of these IC chips include but are not limited to processors, controllers, chipset components, programmable logic arrays (PLAs), memory chips, network chips, systems on chip (SoCs), SSD/NAND controller ASICs, and the like. In addition, in some of the drawings, signal conductor lines are represented with lines. Some may be different, to indicate more constituent signal paths, have a number label, to indicate a number of constituent signal paths, and/or have arrows at one or more ends, to indicate primary information flow direction. This, however, should not be construed in a limiting manner. Rather, such added detail may be used in connection with one or more exemplary embodiments to facilitate easier understanding of a circuit. Any represented signal lines, whether or not having additional information, may actually comprise one or more signals that may travel in multiple directions and may be implemented with any suitable type of signal scheme, e.g., digital or analog lines implemented with differential pairs, optical fiber lines, and/or single-ended lines.</p><p id="p-0110" num="0098">Example sizes/models/values/ranges may have been given, although embodiments are not limited to the same. As manufacturing techniques (e.g., photolithography) mature over time, it is expected that devices of smaller size could be manufactured. In addition, well known power/ground connections to IC chips and other components may or may not be shown within the figures, for simplicity of illustration and discussion, and so as not to obscure certain aspects of the embodiments. Further, arrangements may be shown in block diagram form in order to avoid obscuring embodiments, and also in view of the fact that specifics with respect to implementation of such block diagram arrangements are highly dependent upon the platform within which the embodiment is to be implemented, i.e., such specifics should be well within purview of one skilled in the art. Where specific details (e.g., circuits) are set forth in order to describe example embodiments, it should be apparent to one skilled in the art that embodiments can be practiced without, or with variation of, these specific details. The description is thus to be regarded as illustrative instead of limiting.</p><p id="p-0111" num="0099">The term &#x201c;coupled&#x201d; may be used herein to refer to any type of relationship, direct or indirect, between the components in question, and may apply to electrical, mechanical, fluid, optical, electromagnetic, electromechanical or other connections, including logical connections via intermediate components (e.g., device A may be coupled to device C via device B). In addition, the terms &#x201c;first&#x201d;, &#x201c;second&#x201d;, etc. may be used herein only to facilitate discussion, and carry no particular temporal or chronological significance unless otherwise indicated. As used in this application and in the claims, a list of items joined by the term &#x201c;one or more of&#x201d; may mean any combination of the listed terms. For example, the phrases &#x201c;one or more of A, B or C&#x201d; may mean A, B, C; A and B; A and C; B and C; or A, B and C.</p><p id="p-0112" num="0100">Those skilled in the art will appreciate from the foregoing description that the broad techniques of the embodiments can be implemented in a variety of forms. Therefore, while the embodiments have been described in connection with particular examples thereof, the true scope of the embodiments should not be so limited since other modifications will become apparent to the skilled practitioner upon a study of the drawings, specification, and following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-chemistry idref="CHEM-US-00001" cdx-file="US20230004430A1-20230105-C00001.CDX" mol-file="US20230004430A1-20230105-C00001.MOL"/><us-chemistry idref="CHEM-US-00002" cdx-file="US20230004430A1-20230105-C00002.CDX" mol-file="US20230004430A1-20230105-C00002.MOL"/><us-claim-statement>We claim:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computing system comprising:<claim-text>a processor; and</claim-text><claim-text>memory coupled to the processor, the memory to store instructions which, when executed by the processor, cause the computing system to:<claim-text>obtain a plurality of workloads for a compiled neural network (NN) model, the plurality of workloads determined for a hardware execution device;</claim-text><claim-text>determine a hardware efficiency factor for the compiled NN model; and</claim-text><claim-text>generate, based on the hardware efficiency factor, a power profile for the compiled NN model on one or more of a per-layer basis or a per-workload basis.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the hardware efficiency factor is determined on based on a hardware efficiency measurement and a hardware utilization measurement.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the hardware efficiency factor is determined on a per-workload basis.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the instructions, when executed, further cause the computing system to provide a configuration file to be used for generating the power profile.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the instructions, when executed, further cause the computing system to generate an output visualization of the power profile.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the instructions, when executed, further cause the computing system to generate feedback information to perform one or more of selecting a hardware device, optimizing a breakdown of workloads, optimizing a scheduling of tasks, or confirming a hardware device design.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A semiconductor apparatus comprising:<claim-text>one or more substrates; and</claim-text><claim-text>logic coupled to the one or more substrates, wherein the logic is implemented at least partly in one or more of configurable logic or fixed-functionality hardware logic, the logic to:<claim-text>obtain a plurality of workloads for a compiled neural network (NN) model, the plurality of workloads determined for a hardware execution device;</claim-text><claim-text>determine a hardware efficiency factor for the compiled NN model; and</claim-text><claim-text>generate, based on the hardware efficiency factor, a power profile for the compiled NN model on one or more of a per-layer basis or a per-workload basis.</claim-text></claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the hardware efficiency factor is determined on based on a hardware efficiency measurement and a hardware utilization measurement.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the hardware efficiency factor is determined on a per-workload basis.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the logic is further to provide a configuration file to be used for generating the power profile.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the logic is further to generate an output visualization of the power profile.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the logic is further to generate feedback information to perform one or more of selecting a hardware device, optimizing a breakdown of workloads, optimizing a scheduling of tasks, or confirming a hardware device design.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the logic coupled to the one or more substrates includes transistor channel regions that are positioned within the one or more substrates.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. At least one computer readable storage medium comprising a set of instructions which, when executed by a computing system, cause the computing system to:<claim-text>obtain a plurality of workloads for a compiled neural network (NN) model, the plurality of workloads determined for a hardware execution device;</claim-text><claim-text>determine a hardware efficiency factor for the compiled NN model; and</claim-text><claim-text>generate, based on the hardware efficiency factor, a power profile for the compiled NN model on one or more of a per-layer basis or a per-workload basis.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The at least one computer readable storage medium of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the hardware efficiency factor is determined on based on a hardware efficiency measurement and a hardware utilization measurement.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The at least one computer readable storage medium of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the hardware efficiency factor is determined on a per-workload basis.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The at least one computer readable storage medium of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the instructions, when executed, further cause the computing system to provide a configuration file to be used for generating the power profile.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The at least one computer readable storage medium of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the instructions, when executed, further cause the computing system to generate an output visualization of the power profile.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The at least one computer readable storage medium of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the instructions, when executed, further cause the computing system to generate feedback information to perform one or more of selecting a hardware device, optimizing a breakdown of workloads, optimizing a scheduling of tasks, or confirming a hardware device design.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A method comprising:<claim-text>obtaining a plurality of workloads for a compiled neural network (NN) model, the plurality of workloads determined for a hardware execution device;</claim-text><claim-text>determining a hardware efficiency factor for the compiled NN model; and</claim-text><claim-text>generating, based on the hardware efficiency factor, a power profile for the compiled NN model on one or more of a per-layer basis or a per-workload basis.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The method of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the hardware efficiency factor is determined on based on a hardware efficiency measurement and a hardware utilization measurement.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The method of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the hardware efficiency factor is determined on a per-workload basis.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The method of <claim-ref idref="CLM-00020">claim 20</claim-ref>, further comprising providing a configuration file to be used for generating the power profile.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The method of <claim-ref idref="CLM-00020">claim 20</claim-ref>, further comprising generating an output visualization of the power profile.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The method of <claim-ref idref="CLM-00020">claim 20</claim-ref>, further comprising generating feedback information to perform one or more of selecting a hardware device, optimizing a breakdown of workloads, optimizing a scheduling of tasks, or confirming a hardware device design.</claim-text></claim></claims></us-patent-application>