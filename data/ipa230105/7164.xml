<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007165A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007165</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17943966</doc-number><date>20220913</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2016-124538</doc-number><date>20160623</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20210101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>03</class><subclass>B</subclass><main-group>17</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20210101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>03</class><subclass>B</subclass><main-group>17</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20210101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>03</class><subclass>B</subclass><main-group>17</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23216</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232945</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>03</class><subclass>B</subclass><main-group>17</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>03</class><subclass>B</subclass><main-group>17</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232127</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23299</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23293</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23218</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>03</class><subclass>B</subclass><main-group>17</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232933</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23245</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">MOBILE TERMINAL</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17714696</doc-number><date>20220406</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11490004</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17943966</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17135958</doc-number><date>20201228</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11323611</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17714696</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16311132</doc-number><date>20181218</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10911661</doc-number></document-id></parent-grant-document><parent-pct-document><document-id><country>WO</country><doc-number>PCT/JP2017/020460</doc-number><date>20170601</date></document-id></parent-pct-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17135958</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>MAXELL, LTD.</orgname><address><city>Kyoto</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Nishimura</last-name><first-name>Ryuji</first-name><address><city>Kyoto</city><country>JP</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A mobile terminal in which an imaging unit is provided on a front surface side and a display unit and a sensor are provided on a back surface side includes a signal processing unit which, from a signal output by the imaging unit, generates information to be displayed by the display unit, and a control unit into which a signal from the sensor is input and which controls the signal processing unit. A display state of the display unit based on the control performed by the control unit includes a first display state for display in a first display region if no detection signal is being input from the sensor, and a second display state for display in a second display region if a detection signal is being input from the sensor.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="103.89mm" wi="158.75mm" file="US20230007165A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="243.25mm" wi="159.60mm" orientation="landscape" file="US20230007165A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="210.82mm" wi="162.22mm" file="US20230007165A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="206.93mm" wi="154.09mm" file="US20230007165A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="231.31mm" wi="135.13mm" file="US20230007165A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="231.31mm" wi="135.13mm" file="US20230007165A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="129.96mm" wi="154.09mm" file="US20230007165A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="242.32mm" wi="158.83mm" orientation="landscape" file="US20230007165A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="243.67mm" wi="162.31mm" file="US20230007165A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="240.88mm" wi="141.99mm" file="US20230007165A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><p id="p-0002" num="0001">This application is a Continuation of U.S. application Ser. No. 17/714,696, filed Apr. 6, 2022, which is a Continuation of U.S. application Ser. No. 17/135,958, filed Dec. 28, 2020, now U.S. Pat. No. 11,323,611, issued May 3, 2022, which is a Continuation of U.S. application Ser. No. 16/311,132, filed Dec. 18, 2018, now U.S. Pat. No. 10,911,661, issued Feb. 2, 2021, which is the U.S. National Phase under 35 U.S.C. &#xa7; 371 of International Application No. PCT/JP2017/020460, filed Jun. 1, 2017, which in turn claims the benefit of Japanese Application No. 2016-124538, filed Jun. 23, 2016, the entire contents of each are hereby incorporated by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present invention relates to a mobile terminal having an imaging unit and a display.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0004" num="0003">A mobile terminal of the related art, having an imaging unit (a camera) generally has a communication function, so that the mobile terminal is capable of transmitting images photographed by the camera to the outside through a telephone line or a network and is easy to share the images among users. Such a mobile terminal usually has a display such as a liquid crystal on a back surface, and a user holds the terminal when performing photographing by the camera, and can perform photographing while monitoring the images displayed on the display from a certain distance away during the photographing.</p><p id="p-0005" num="0004">For example, Patent Documents 1 and 2 disclose such mobile terminals. On the other hand, in addition to such a display, an imaging apparatus such as a digital camera is an apparatus having an electronic viewfinder such that a user can observe the photographed images by looking into the electronic viewfinder. For example, Patent Document 3 discloses the electronic viewfinder.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Document</heading><p id="p-0006" num="0005">Patent Document 1: JP 2001-136499 A</p><p id="p-0007" num="0006">Patent Document 2: JP 2006-157171 A</p><p id="p-0008" num="0007">Patent Document 3: JP 2002-10112 A</p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><heading id="h-0006" level="1">Problems to be Solved by the Invention</heading><p id="p-0009" num="0008">However, the mobile terminals disclosed in Patent Documents 1 and 2 have a problem that external light is incident on a display in bright environments such as outdoors, making it difficult for the images on the display to be viewed. Further, the electronic viewfinder disclosed in Patent Document 3 is easy to shield light and easy to monitor images even in bright environments such as outdoors, whereas apparatuses having the electronic viewfinder has a problem that the apparatuses are difficult to be downsized and thin and in addition to the display, cost of electronic components of the electronic viewfinder is increased.</p><p id="p-0010" num="0009">Accordingly, an object of the present invention is to control the display of a mobile terminal such that a viewer including a finder portion can be used for the mobile terminal.</p><heading id="h-0007" level="1">Solutions to Problems</heading><p id="p-0011" num="0010">In order to solve the problems, the present invention adopts, for example, the configurations described in the claims. An example thereof is a mobile terminal in which an imaging unit is provided on a front surface side and a display unit and a sensor are provided on a back surface side, including: a signal processing unit which, from a signal output by the imaging unit, generates information to be displayed by the display unit; and a control unit into which a signal from the sensor is input and which controls the signal processing unit,</p><p id="p-0012" num="0011">in which a display state of the display unit based on the control performed by the control unit includes a first display state for display in a first display region if no detection signal is being input from the sensor, and a second display state for display in a second display region if a detection signal is being input from the sensor.</p><heading id="h-0008" level="1">Effects of the Invention</heading><p id="p-0013" num="0012">According to the present invention, it is possible to perform display suitable for a viewer including a finder portion, so that the viewer may be used for the mobile terminal.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating an example of a configuration of a mobile terminal of Embodiment 1.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating an example of a structure of a display.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating an example of an external appearance of the mobile terminal of Embodiment 1.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating an example of a viewer of Embodiment 1.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating an example of a structure of a finder portion of the viewer.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> is a diagram illustrating an example of a display in a case where the viewer is not mounted.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>6</b>B</figref> is a diagram illustrating an example of a display in a case where the viewer of Embodiment 1 is mounted.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is a diagram illustrating an example of a focus setting region frame in a case where the viewer is not mounted.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> is a diagram illustrating an example of a focus setting region frame in a case where the viewer of Embodiment 1 is mounted.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram illustrating another example of the viewer of Embodiment 1.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram illustrating an example of a configuration of a mobile terminal of Embodiment 2.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating an example of an external appearance of the mobile terminal of Embodiment 2.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram illustrating an example of a viewer of Embodiment 2.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>12</b>A</figref> is a diagram illustrating an example of a display of a photographing mode of Embodiment 2.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>12</b>B</figref> is a diagram illustrating an example of a display in a reproducing mode of Embodiment 2.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">MODE FOR CARRYING OUT THE INVENTION</heading><p id="p-0029" num="0028">Hereinafter, embodiments of the present invention will be described with reference to the drawings.</p><heading id="h-0011" level="1">Embodiment 1</heading><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating an example of a configuration of a mobile terminal <b>1</b> according to Embodiment 1. A lens unit <b>2</b><i>a </i>and an imaging unit <b>3</b><i>a </i>are a rear camera disposed on the front surface of the mobile terminal <b>1</b> and are generally used for ordinary photographing such as landscape. A lens unit <b>2</b><i>b </i>and an imaging unit <b>3</b><i>b </i>are a front camera disposed on the back surface of the mobile terminal <b>1</b>, and are used for so-called self-photographing.</p><p id="p-0031" num="0030">Each of the lens units <b>2</b><i>a </i>and <b>2</b><i>b </i>is configured with a plurality of lenses including a focus lens, and a control circuit <b>6</b> controls a position of the focus lens to perform a focusing operation. The imaging units <b>3</b><i>a </i>and <b>3</b><i>b </i>include an imaging sensor configured with a CMOS, a CCD, or the like. Photoelectric conversion elements as an imaging sensor are two-dimensionally disposed on an imaging surface of the imaging units <b>3</b><i>a </i>and <b>3</b><i>b, </i>and an optical image of a subject, which is input through the lens units <b>2</b><i>a </i>and <b>2</b><i>b </i>and formed on the imaging surface, is photoelectrically converted into an imaging signal.</p><p id="p-0032" num="0031">The imaging units <b>3</b><i>a </i>and <b>3</b><i>b </i>include an AD conversion circuit that converts an analog signal into a digital signal, and output a digitized imaging signal. Further, the imaging units <b>3</b><i>a </i>and <b>3</b><i>b </i>may include an imaging sensor in which pixels for a phase difference AF (autofocus) are disposed, so that it is intended to speed up the AF. The imaging units <b>3</b><i>a </i>and <b>3</b><i>b </i>may include a memory, so that an interval until the next imaging is shortened.</p><p id="p-0033" num="0032">Incidentally, in a case where the imaging units <b>3</b><i>a </i>and <b>3</b><i>b </i>do not include the AD conversion circuit, the AD conversion circuit may be provided outside the imaging units <b>3</b><i>a </i>and <b>3</b><i>b. </i>The control circuit <b>6</b> controls the imaging units <b>3</b><i>a </i>and <b>3</b><i>b, </i>and the lens units <b>2</b><i>a </i>and <b>2</b><i>b </i>according to an operation mode of the mobile terminal <b>1</b>, in addition to the focusing operation.</p><p id="p-0034" num="0033">An interface circuit <b>4</b> outputs either or both of input signals from the imaging units <b>3</b><i>a </i>and <b>3</b><i>b </i>to an image/audio signal processing circuit <b>5</b> according to the operation mode of the mobile terminal <b>1</b>. The control circuit <b>6</b> also controls this operation mode. The image/audio signal processing circuit <b>5</b> performs various image signal processing such as filtering, amplification according to sensitivity setting, white balance correction, and the like on the input signal from the interface circuit <b>4</b>.</p><p id="p-0035" num="0034">The image/audio signal processing circuit <b>5</b> generates moving image data or still image data as image data for display or image data for recording from the signal subjected to image signal processing, according to the operation mode. Here, the image data for display may be scaled up or down as an image. The image data for display is output to a display <b>8</b>, and the image data for recording is output to an encoding/decoding circuit <b>15</b>.</p><p id="p-0036" num="0035">The image/audio signal processing circuit <b>5</b> may input the encoded or decoded image data from the encoding/decoding circuit <b>15</b> to be output to a recording/reproducing circuit <b>10</b>, or may generate the image data for display to be output to the display <b>8</b>. Under the control of the control circuit <b>6</b>, the image/audio signal processing circuit <b>5</b> may generate image data of texts and graphics to be output to the display <b>8</b>, or may adjust the brightness of the display <b>8</b>.</p><p id="p-0037" num="0036">Further, in addition to a circuit for performing image signal processing, the image/audio signal processing circuit <b>5</b> also includes a circuit for performing audio signal processing, and performs predetermined audio signal processing such as encoding on an input signal from a microphone <b>34</b>. Then, in a case where an encoded audio signal is input, the audio signal is decoded to be output to a speaker <b>32</b>. The microphone <b>34</b> and the speaker <b>32</b> may be used for speech communication, and the audio signal may be transmitted to and received from a telephone line by a wireless communication unit <b>14</b> through the image/audio signal processing circuit <b>5</b> and the control circuit <b>6</b>.</p><p id="p-0038" num="0037">Incidentally, the image/audio signal processing circuit <b>5</b> may be an LSI integrated into one chip with another circuit, or may be one independent LSI. Then, there may be provided a circuit including two systems of circuits for processing the imaging units <b>3</b><i>a </i>and <b>3</b><i>b </i>depending on the performance of the imaging units <b>3</b><i>a </i>and <b>3</b><i>b, </i>and capable of simultaneously performing two systems of signal processing. Further, a memory <b>12</b> that is a DRAM, a flash memory, or the like may be used as a temporary buffer memory.</p><p id="p-0039" num="0038">The encoding/decoding circuit <b>15</b> encodes moving image data or still image data, or decodes the encoded data. Instead of the image/audio signal processing circuit <b>5</b>, an encoding/decoding circuit <b>15</b> may encode or decode the audio signal. The recording/reproducing circuit <b>10</b> writes data input from the image/audio signal processing circuit <b>5</b> into a recording medium <b>11</b>, or reads out the data stored in the recording medium <b>11</b> to be output to the image/audio signal processing circuit <b>5</b>. Incidentally, data to be read from or written to may be any encoded data.</p><p id="p-0040" num="0039">The control circuit <b>6</b> inputs information from each unit of a posture detection unit <b>7</b>, an operation input unit <b>9</b>, a position information input unit <b>13</b>, and a proximity sensor <b>31</b>, and controls each unit of the lens units <b>2</b><i>a </i>and <b>2</b><i>b, </i>the imaging units <b>3</b><i>a </i>and <b>3</b><i>b, </i>and the interface circuit <b>4</b> to input and output information with the image/audio signal processing circuit <b>5</b> and the wireless communication unit <b>14</b>. Further, the control circuit <b>6</b> may control the image/audio signal processing circuit <b>5</b> and the wireless communication unit <b>14</b>, or may control the display <b>8</b>, the encoding/decoding circuit <b>15</b>, and the recording/reproducing circuit <b>10</b>, through the image/audio signal processing circuit <b>5</b>.</p><p id="p-0041" num="0040">Further, the control circuit <b>6</b> may directly control the display <b>8</b>, the encoding/decoding circuit <b>15</b>, and the recording/reproducing circuit <b>10</b> by using a control line (not illustrated). Incidentally, the control circuit <b>6</b> may be a processor, or may execute a program stored in a memory (not illustrated) or a memory incorporated in the processor.</p><p id="p-0042" num="0041">The wireless communication unit <b>14</b> is a circuit that communicates with the Internet or the like by wireless communication. Wireless communication may be a telephone line, a wireless LAN, or the like. The position information input unit <b>13</b> is a circuit that acquires the position information of the mobile terminal <b>1</b> by a GPS or wireless communication to output the acquired position information to the control circuit <b>6</b>. The posture detection unit <b>7</b> is a circuit that detects gravity by an acceleration sensor or detects rotation by an angular velocity sensor to output the detected information to the control circuit <b>6</b>.</p><p id="p-0043" num="0042">The proximity sensor <b>31</b> is a sensor that detects an object approaching the mobile terminal <b>1</b> and is a sensor that mainly detects a face to be in close to the mobile terminal <b>1</b> for speech communication. Since the proximity sensor <b>31</b> of the present embodiment also detects approach other than the approach of the face, it is preferable that the proximity sensor <b>31</b> is an infrared type proximity sensor or the like for detecting the approach of an object other than a human body.</p><p id="p-0044" num="0043">The operation input unit <b>9</b> receives an operation input from a user. Specifically, as described later, the operation input unit includes a touch panel, a power button, and a shutter. However, the operation input unit is not limited thereto and may include various dedicated buttons. The display <b>8</b> is, for example, a display panel such as a liquid crystal panel or an organic EL panel. It is preferable that the display <b>8</b> and the touch panel of the operation input unit <b>9</b> are integrated. In the following description, the touch panel of the operation input unit <b>9</b> is also referred to as the display <b>8</b> unless otherwise mentioned.</p><p id="p-0045" num="0044">Incidentally, the mobile terminal <b>1</b> may be a general smartphone, a tablet, or the like.</p><p id="p-0046" num="0045">The operation mode of the mobile terminal <b>1</b> includes at least a photographing mode and a reproducing mode. First, operations in the photographing mode will be described. The control circuit <b>6</b> detects an activation instruction of the photographing mode by an operation on the operation input unit <b>9</b> (touch panel) and the like, and performs control according to the photographing mode. The photographing mode further includes a still image photographing mode and a moving image photographing mode.</p><p id="p-0047" num="0046">In either case of still image photographing or moving image photographing, the imaging units <b>3</b><i>a </i>and <b>3</b><i>b </i>output imaging signals at predetermined intervals, and the imaging signals are subjected to predetermined image signal processing in the image/audio signal processing circuit <b>5</b> through the interface circuit <b>4</b>, so that the image data for display is generated. The image data for display is displayed on the display <b>8</b> in real time.</p><p id="p-0048" num="0047">At this time, depending on which of the imaging unit <b>3</b><i>a </i>and the imaging unit <b>3</b><i>b </i>is used, the AF or a diaphragm value, and the like, the control circuit <b>6</b> controls the lens units <b>2</b><i>a </i>and <b>2</b><i>b, </i>the imaging units <b>3</b><i>a </i>and <b>3</b><i>b, </i>the interface circuit <b>4</b>, the image/audio signal processing circuit <b>5</b>, and the like. Incidentally, in a case where it is set to detect two states of a half depression and a full depression on the depression of the shutter, once the state of the half depression is detected, the control circuit <b>6</b> may perform control of the AF.</p><p id="p-0049" num="0048">In the still image photographing mode, once the depression of the shutter or the full depression is detected, the control circuit <b>6</b> controls each unit according to the photographing conditions such as a shutter speed. The imaging signals of a still image imaged by the imaging units <b>3</b><i>a </i>and <b>3</b><i>b </i>are subjected to predetermined still image signal processing by the image/audio signal processing circuit <b>5</b>, and are then encoded by the encoding/decoding circuit <b>15</b>.</p><p id="p-0050" num="0049">Here, the memory <b>12</b> may be used as a buffer memory when still image signal processing and encoding are performed. Further, the encoding may be, for example, any encoding into JPEG data, or encoding of MPEG data into still images. The encoded still image data is output to the recording/reproducing circuit <b>10</b> by the image/audio signal processing circuit <b>5</b>, and is recorded on the recording medium <b>11</b> by the recording/reproducing circuit <b>10</b>. Incidentally, without the use of the encoding/decoding circuit <b>15</b>, recoding may be performed in a high-quality RAW format.</p><p id="p-0051" num="0050">In the moving image photographing mode, once the depression of the shutter or the full depression is detected, the control circuit <b>6</b> controls each unit according to the photographing conditions. In the case of the moving image photographing mode, even though the depression of the shutter is released, the control circuit <b>6</b> continues control of the photographing. The imaging signals of a moving image imaged by the imaging units <b>3</b><i>a </i>and <b>3</b><i>b </i>are subjected to predetermined moving image signal processing by the image/audio signal processing circuit <b>5</b>, and are then encoded by the encoding/decoding circuit <b>15</b>.</p><p id="p-0052" num="0051">The encoding may be, for example, any encoding into MPEG data such as H.264 or H.265, or encoding into other moving image formats. The encoded moving image data is output to the recording/reproducing circuit <b>10</b> by the image/audio signal processing circuit <b>5</b> and is recorded on the recording medium <b>11</b> by the recording/reproducing circuit <b>10</b>.</p><p id="p-0053" num="0052">Once the depression of the shutter during photographing of the moving image is detected again, the control circuit <b>6</b> terminates control of the photographing. Incidentally, the image/audio signal processing circuit <b>5</b> generates image data for display during photographing and also after photographing of the moving image, and the image data for display is displayed on the display <b>8</b> in real time.</p><p id="p-0054" num="0053">Next, operations in the reproducing mode will be described. The control circuit <b>6</b> detects an activation instruction of the reproducing mode on an operation of the operation input unit <b>9</b> (touch panel) or the like, and performs control according to the reproducing mode. The recording/reproducing circuit <b>10</b> reads out designated data from the data recorded on the recording medium <b>11</b>, the read-out data is decoded by the encoding/decoding circuit <b>15</b> through the image/audio signal processing circuit <b>5</b>, and the decoded data is output to the image/audio signal processing circuit <b>5</b>. The image/audio signal processing circuit <b>5</b> generates image data for display from the decoded data, and the image data for display is displayed on the display <b>8</b>.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating an example of a cross-sectional structure of the display <b>8</b> including a liquid crystal panel <b>22</b> and a touch panel <b>21</b>. The display <b>8</b> is formed by disposing the touch panel <b>21</b> on an upper surface of the liquid crystal panel <b>22</b> to integrate them, disposing a protective cover <b>20</b> such as glass on the upper surface of the touch panel <b>21</b>, and disposing a backlight <b>23</b> on a lower surface of the liquid crystal panel <b>22</b>.</p><p id="p-0056" num="0055">The touch panel <b>21</b> uses, for example, an electrostatic capacitance type, and transparent electrodes are disposed in a two-dimensional orthogonal manner. Once a human finger or a touch pen is in contact with or approaches a display screen of the display <b>8</b>, electrostatic capacitance between the finger or the touch pen and the transparent electrodes is changed, so that a position in the horizontal and vertical directions on the contacted screen is detected. Incidentally, display and operations of the display <b>8</b> including the touch panel <b>21</b> will be described later.</p><p id="p-0057" num="0056">In this structure, since the electrostatic capacitance is detected through the protective cover <b>20</b>, the touch panel <b>21</b> is disposed in close contact with the protective cover <b>20</b> to obtain high detection accuracy. Further, since a surface of the protective cover <b>20</b> is subjected to gloss finishing, high visibility of the display image of the liquid crystal panel <b>22</b> is obtained and a slip of the finger and the touch pen is improved. However, once the surface of the protective cover <b>20</b> is subjected to glossy finishing, a reflection amount of light from the outside is increased.</p><p id="p-0058" num="0057">The backlight <b>23</b> emits illumination light from the back surface of the liquid crystal panel <b>22</b> to display an image. In a case where it is not necessary to display the image, the backlight <b>23</b> is controlled to be turned off, in order to reduce power consumption. The backlight <b>23</b> may be turned off over the whole of the display <b>8</b>, and the backlight <b>23</b> is divided into a plurality of portions to be controlled so that a portion of the display <b>8</b> may be turned off.</p><p id="p-0059" num="0058">Incidentally, instead of the liquid crystal panel <b>22</b> and the backlight <b>23</b>, a panel such as an organic EL may be used. In the display panel using a light-emitting element such as an organic EL, the backlight is unnecessary. Further, in addition to the electrostatic capacitance type, the touch panel <b>21</b> may use a pressure sensitive type or the like.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating an example of the external appearance of the mobile terminal <b>1</b>, <figref idref="DRAWINGS">FIG. <b>3</b>(<i>a</i>)</figref> is a diagram of the mobile terminal <b>1</b> as seen from a back surface side, <figref idref="DRAWINGS">FIG. <b>3</b>(<i>b</i>)</figref> is a diagram of the mobile terminal <b>1</b> as seen from a side surface side, and <figref idref="DRAWINGS">FIG. <b>3</b>(<i>c</i>)</figref> is a diagram of the mobile terminal <b>1</b> as seen from a front surface side. The display <b>8</b> integrated with the touch panel <b>21</b> is disposed on the back surface of the mobile terminal <b>1</b>.</p><p id="p-0061" num="0060">As illustrated in <figref idref="DRAWINGS">FIGS. <b>3</b>(<i>a</i>) and <b>3</b>(<i>c</i>)</figref>, the lens unit <b>2</b><i>a </i>of the rear camera is disposed on the front surface, and the lens unit <b>2</b><i>b </i>of the front camera is disposed on the back surface. <figref idref="DRAWINGS">FIGS. <b>3</b>(<i>a</i>) and <b>3</b>(<i>c</i>)</figref> illustrate that the lens units <b>2</b><i>a </i>and <b>2</b><i>b </i>is fixed on the front surface and the back surface, respectively, but may be replaced with a lens of an attachable or detachable type having a different focal length. A power switch <b>33</b> is a switch operated when activating and terminating the mobile terminal <b>1</b>, and may also serve as a switch for returning to a power saving mode in the mobile terminal <b>1</b> having a power saving mode.</p><p id="p-0062" num="0061">Since the speaker <b>32</b> and the microphone <b>34</b> are for speech communication, for example, the speaker and the microphone are disposed to be in close to the ears and the mouth of a caller in speech communication. The proximity sensor <b>31</b> is disposed to be in close to the speaker <b>32</b>, for example, in order to detect the approach of a face in speech communication, and the infrared type proximity sensor has a light emitting portion and a light receiving portion for near infrared light. The proximity sensor <b>31</b> is disposed on the same surface as that of the display <b>8</b>, that is, on the back surface, in order to detect that the viewer is mounted as described later. The speaker <b>32</b> and the microphone <b>34</b> are also disposed on the back surface.</p><p id="p-0063" num="0062">A shutter <b>35</b> is a switch for instructing to perform photographing through the lens unit <b>2</b><i>a </i>and the shutter <b>35</b> may coexist with a shutter executed by touching a portion of the touch panel <b>21</b>, or may be eliminated. The example of <figref idref="DRAWINGS">FIG. <b>3</b>(<i>b</i>)</figref> is an example of the disposition of the power switch <b>33</b> and the shutter <b>35</b>, without limiting to this disposition.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating an example of a viewer <b>40</b> mounted on the mobile terminal <b>1</b>. The viewer <b>40</b> has a structure in which the viewer is attachable to or detachable from the back surface of the mobile terminal <b>1</b>. <figref idref="DRAWINGS">FIG. <b>4</b>(<i>a</i>)</figref> is a diagram of the viewer <b>40</b> as seen from a back surface side, <figref idref="DRAWINGS">FIG. <b>4</b>(<i>b</i>)</figref> is a diagram of the viewer <b>40</b> as seen from a side surface side, and <figref idref="DRAWINGS">FIG. <b>4</b>(<i>c</i>)</figref> is a diagram of the viewer <b>40</b> as seen from a front surface side. Here, the back surface, the side surface, and the front surface are the same surfaces as those of the mobile terminal <b>1</b> illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, but are not surfaces based on the structure of the viewer <b>40</b>.</p><p id="p-0065" num="0064">The viewer <b>40</b> includes a finder portion <b>42</b><i>a. </i>As illustrated in <figref idref="DRAWINGS">FIGS. <b>4</b>(<i>a</i>) and <b>4</b>(<i>b</i>)</figref>, the finder portion <b>42</b><i>a </i>has a rectangular tube shape, that is, a shape in which four side surfaces are surrounded by substantially rectangular light shielding plates to shield light from the side surfaces. As illustrated in <figref idref="DRAWINGS">FIGS. <b>4</b>(<i>a</i>) and <b>4</b>(<i>c</i>)</figref>, an opening portion <b>41</b> of the finder portion <b>42</b><i>a </i>is an opening portion through which light is passed from the front surface side to the back surface side of the viewer <b>40</b>, and the structure of the opening portion <b>41</b> will be described later with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>. A cavity portion <b>43</b> is a cavity through which light, a human finger or a touch pen is passed between the front surface side and the back surface side of the viewer <b>40</b>.</p><p id="p-0066" num="0065">The whole of the viewer <b>40</b> has a shape in which the viewer is mounted covering portions of the back surface and the side surface of the mobile terminal <b>1</b>, and is formed of a material such as colored polycarbonate as a light shielding material. The viewer <b>40</b> and the mobile terminal <b>1</b> may have a shape in which the viewer and the mobile terminal are pinched between fingers to maintain a mounted state, and each of the viewer <b>40</b> and the mobile terminal <b>1</b> may have a concave portion and convex portion (not illustrated) so that the concave portion is fitted to the convex portion to maintain a mounted state.</p><p id="p-0067" num="0066">Further, the viewer <b>40</b> has a shape in which the proximity sensor <b>31</b> detects the viewer in a state where the viewer <b>40</b> is mounted to the mobile terminal <b>1</b>. Therefore, at the time of mounting, the opening portion <b>41</b> and the cavity portion <b>43</b> are not located at positions facing the proximity sensor <b>31</b>, and a portion of the viewer <b>40</b> excluding the opening portion <b>41</b> and the cavity portion <b>43</b> is located facing the proximity sensor <b>31</b>.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a cross-sectional view illustrating an example of the finder portion <b>42</b><i>a. </i>A cover glass <b>50</b> and lens <b>51</b> are fitted to the opening portion <b>41</b> of the finder portion <b>42</b><i>a, </i>and the side surfaces of the finder portion <b>42</b><i>a </i>serve as supporting members of the cover glass and lens. In a case where the display <b>8</b> is directly viewed without the viewer <b>40</b>, it is performed to normally view the display <b>8</b> at a distance of about 30 cm or more from the display <b>8</b>, whereas in a case where the viewer <b>40</b> is used, it is performed to view the display <b>8</b> in a state of looking into the finder portion <b>42</b><i>a, </i>so that the image displayed on the display <b>8</b> is enlarged by the lens <b>51</b> to adjust focus on the image.</p><p id="p-0069" num="0068">The cover glass <b>50</b> protects the lens <b>51</b>. It is preferable that the finder portion <b>42</b><i>a </i>extends from an eyepiece position to the cover glass <b>50</b>, and it is preferable to have a shape in which the cover glass <b>50</b> is shielded from external light by the side surfaces of the finder portion <b>42</b><i>a </i>and a face of a person looking into the finder portion.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> is a diagram illustrating an example of a display in a case where the viewer <b>40</b> is not mounted, and <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> is a diagram illustrating an example of a display in a case where the viewer <b>40</b> is mounted. The display on the display <b>8</b> is changed depending on whether or not the viewer <b>40</b> is mounted, that is, whether or not the viewer <b>40</b> is used. In the case of the viewer <b>40</b> being not used of <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, it is performed to directly view the display <b>8</b>.</p><p id="p-0071" num="0070">In <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, a subject <b>67</b><i>a </i>is a display of a subject imaged by the imaging unit <b>3</b><i>a. </i>An imaging region <b>66</b><i>a </i>is preferably a display within a range imaged by the imaging unit <b>3</b><i>a, </i>but may be a display within a range of a portion obtained by clipping the periphery. Further, the display of the subject <b>67</b><i>a </i>and the imaging region <b>66</b><i>a </i>may be reduced and displayed according to a ratio between the number of pixels of the imaging units <b>3</b><i>a </i>and <b>3</b><i>b </i>and the number of pixels of the display <b>8</b>.</p><p id="p-0072" num="0071">Setting buttons <b>60</b> to <b>64</b> are a display of various setting buttons, and a shutter button <b>65</b> is a display of a button corresponding to a shutter. The setting button <b>60</b> is a button for switching between the front camera and the rear camera, the setting button <b>61</b> is a button for changing an ISO sensitivity, the setting button <b>62</b> is a button for changing a diaphragm value, the setting button <b>63</b> is a button for changing a shutter speed, and the setting button <b>64</b> is a button for setting a flash.</p><p id="p-0073" num="0072">These setting buttons <b>60</b> to <b>64</b>, which are operation menus for setting the camera, are displayed on the display <b>8</b>. Once it is detected that a position corresponding to a display is touched on the touch panel <b>21</b> of the operation input unit <b>9</b>, the setting is changed by the processing of the control circuit <b>6</b>. For example, each time the setting button <b>60</b> is touched, the rear camera and the front camera are switched.</p><p id="p-0074" num="0073">Further, in a case where the setting button <b>61</b> is touched, a selectable ISO sensitivity is displayed by horizontal scrolling, and in a case where a selection is made by touch, the ISO sensitivity is changed. The diaphragm value of the setting button <b>62</b> and the shutter speed of the setting button <b>63</b> also operate in the same manner as the setting button <b>61</b>. Each time the setting button <b>64</b> is touched, the setting button <b>64</b> is set to ON (forced light emission), OFF (light emission prohibition), or automatic light emission (light emission control according to brightness).</p><p id="p-0075" num="0074">Incidentally, these setting buttons <b>60</b> to <b>64</b> are examples of operation menus, and menus for changing an optional setting and setting buttons such as changing a mode to a photographing mode including moving image photographing, or a reproducing mode, a focusing mode, and white balance may be displayed. The shutter button <b>65</b> is the same operation as the shutter <b>35</b> and may be combined with the shutter <b>35</b>.</p><p id="p-0076" num="0075">Further, when the image imaged by the imaging unit <b>3</b><i>b </i>as the front camera is displayed on the display <b>8</b>, the image is displayed as a mirror image, and the other images are normally displayed as normal images. Then, the display and operations of <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> may be a general display and operations of the smartphone.</p><p id="p-0077" num="0076">In the display of <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>, the same displays as those in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> are denoted by the same reference numerals, and the descriptions thereof will not be repeated. A region <b>68</b> is a region corresponding to the cavity portion <b>43</b>, and even in a state where the viewer <b>40</b> is mounted, the region <b>68</b> can be viewed directly and can be touched through the cavity portion <b>43</b>. The display of the setting buttons <b>60</b> to <b>64</b> and the shutter button <b>65</b> in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> is illustrated as an example of the same displays as those in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, but may be reduced and displayed, as compared with the display of <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, to fit in the region <b>68</b>.</p><p id="p-0078" num="0077">In a case where the viewer <b>40</b> is mounted, the setting button <b>60</b> for switching between the rear camera and the front camera may not be displayed using only the rear camera, or the setting button <b>60</b> may be grayed out not to be selectable. Further, in a case where it is detected that the viewer <b>40</b> is mounted, the setting buttons <b>60</b> to <b>64</b> may be changed to different operation menus.</p><p id="p-0079" num="0078">A subject <b>67</b><i>b </i>and an imaging region <b>66</b><i>b </i>correspond to the subject <b>67</b><i>a </i>and the imaging region <b>66</b><i>a, </i>respectively, and the same image data is used and displayed as an image display by one operation of the shutter <b>35</b> or the shutter button <b>65</b>. Here, in a case where the subject <b>67</b><i>a </i>and the imaging region <b>66</b><i>a </i>are displayed at equal magnification, the subject <b>67</b><i>b </i>and the imaging region <b>66</b><i>b </i>are reduced and displayed, and in a case where the subject <b>67</b><i>a </i>and the imaging region <b>66</b><i>a </i>are reduced and displayed, the subject <b>67</b><i>b </i>and the imaging region <b>66</b><i>b </i>are reduced and displayed to be smaller, as compared with the reduced display of the subject <b>67</b><i>a </i>and the imaging region <b>66</b><i>a. </i>This reduction is performed by the image/audio signal processing circuit <b>5</b>.</p><p id="p-0080" num="0079">It is preferable that a size of the imaging region <b>66</b><i>b </i>is large enough to sufficiently confirm the image displayed in the imaging region <b>66</b><i>b </i>from corner to corner, due to enlargement by the lens <b>51</b>. For example, a magnification setting button is also displayed on the operation menus of the setting buttons <b>60</b> to <b>64</b>, and a reduction magnification for this size may be set by a touch operation. Further, a distance from the display <b>8</b> to the lens <b>51</b>, a distance from the lens <b>51</b> to an eye looking into the lens, a focal length of the lens <b>51</b>, and the like are preset, and a size and resolution and the like of a screen of the display are acquired, so that the reduction magnification may be calculated.</p><p id="p-0081" num="0080">The periphery display of the imaging region <b>66</b><i>b, </i>that is, a hatched portion of a region <b>69</b>, may be displayed with a reduced brightness or black. For this display, the liquid crystal panel <b>22</b> may be controlled, or the backlight <b>23</b> may be controlled.</p><p id="p-0082" num="0081">The mounting of the viewer <b>40</b> is detected by the proximity sensor <b>31</b>. For example, once the proximity sensor <b>31</b> detects a proximity object to be within a range of a preset distance longer than a preset time in a state where the photographing mode is set, the control circuit <b>6</b> may perform control of changing a display to another display illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>. That is, the control circuit <b>6</b> may perform control of reducing the imaging region <b>66</b><i>b </i>including the subject <b>67</b><i>b </i>and displaying the region <b>69</b> with a reduced display brightness.</p><p id="p-0083" num="0082">Further, the removal of the viewer <b>40</b> is also detected by the proximity sensor <b>31</b>. For example, once the proximity sensor <b>31</b> does not detect a proximity object to be within a range of a preset distance longer than a preset time in a state where the photographing mode is set, the control circuit <b>6</b> may perform control of changing a display to the display illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>. Incidentally, the setting button for switching is also displayed on the operation menus of the setting buttons <b>60</b> to <b>64</b>, and may control switching between the display illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> and the display illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> by a touch operation.</p><p id="p-0084" num="0083">When a still image or a moving image is photographed, there is a case where bright surroundings make it difficult for the screen of the display <b>8</b> to be viewed. In such a case, since the viewer <b>40</b> is mounted to improve a light shielding property, it is easy to confirm the image of the subject <b>67</b><i>b </i>even in the bright surroundings.</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is a diagram illustrating an example of a display including a focus setting region frame <b>70</b><i>a </i>in a case where the viewer <b>40</b> is not mounted. In the display of <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, the same displays as those in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> are denoted by the same reference numerals, and the descriptions thereof will not be repeated. In the display illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, for example, once the touch panel <b>21</b> detects that an optional position in the display of the imaging region <b>66</b><i>a </i>is touched, the control circuit <b>6</b> displays the focus setting region frame <b>70</b><i>a </i>by focusing on a distance to an imaging target corresponding to the touched position.</p><p id="p-0086" num="0085"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> is a diagram illustrating an example of a display including a focus setting region frame <b>70</b><i>b </i>in a case where the viewer <b>40</b> is mounted. In the display of <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, the same displays as those in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> are denoted by the same reference numerals, and the descriptions thereof will not be repeated. A touch pad region <b>71</b> is touched through the region <b>68</b> corresponding to the cavity portion <b>43</b> and the control circuit <b>6</b> performs control of moving a position of the focus setting region frame <b>70</b><i>b </i>in the imaging region <b>66</b><i>b </i>according to a touched position in the region <b>68</b>.</p><p id="p-0087" num="0086">In this way, it is possible to move the position of the focus setting region frame <b>70</b><i>b </i>in the display of the imaging region <b>66</b><i>b, </i>even though the cover glass <b>50</b> cannot be touched due to the mounting of the viewer <b>40</b>. Further, icons and the like corresponding to the setting buttons <b>60</b> to <b>64</b> illustrated in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref> are displayed in the display of the imaging region <b>66</b><i>b, </i>and the control circuit <b>6</b> may perform control such that icons and the like are selectable according to the position of the focus setting region frame <b>70</b><i>b. </i></p><p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram illustrating another example of the viewer <b>40</b>. <figref idref="DRAWINGS">FIG. <b>8</b>(<i>a</i>)</figref> is a diagram of the viewer <b>40</b> as seen from the back surface side, <figref idref="DRAWINGS">FIG. <b>8</b>(<i>b</i>)</figref> is a diagram of the viewer <b>40</b> as seen from the side surface side, and <figref idref="DRAWINGS">FIG. <b>8</b>(<i>c</i>)</figref> is a diagram of the viewer <b>40</b> as seen from the front surface side. In <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the same structures as those in <figref idref="DRAWINGS">FIG. <b>4</b></figref> are denoted by the same reference numerals, and the descriptions thereof will not be repeated. <figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a different shape of the finder portion <b>42</b><i>b, </i>as compared with <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0089" num="0088">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, each of the side surfaces of the finder portion <b>42</b><i>a </i>is substantially rectangular, but in the example illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, each of the side surfaces of the finder portion <b>42</b><i>b </i>is substantially trapezoidal. That is, as the opening portion <b>41</b> approaches the display <b>8</b>, the opening portion becomes wider. In this way, in the case of looking into the finder portion <b>42</b><i>b, </i>an inside of the finder portion <b>42</b><i>b, </i>that is, a visible region of an inside surface on an opening portion <b>41</b> side is reduced, and the display regions of the imaging region <b>66</b><i>b </i>illustrated in <figref idref="DRAWINGS">FIGS. <b>6</b>B and <b>7</b>B</figref> are widened, so that there are a case where the number of pixels in the display region is easy to be increased. Further, since the side surface of the finder portion <b>42</b><i>b </i>is substantially trapezoidal, a physical structural strength also is enhanced.</p><p id="p-0090" num="0089">As described above, according to the present embodiment, in the mobile terminal <b>1</b> which has the display <b>8</b> such as a liquid crystal panel but does not have the electronic viewfinder, the same functions and effects as the viewfinder are obtained only by mounting the viewer <b>40</b>. Here, the viewer <b>40</b> does not include display components unlike the electronic viewfinder, so that a simple structure and a low cost are obtained.</p><p id="p-0091" num="0090">Further, since it is possible to eliminate, through image processing of the mobile terminal <b>1</b>, a portion in which the display of the imaging region <b>66</b><i>a </i>cannot be confirmed resulting from the mounting of the viewer <b>40</b>, the viewer <b>40</b> can be substantially used. Then, since it is possible to automatically switch a display when the viewer <b>40</b> is mounted or not mounted, it is possible to provide a user-friendly mobile terminal. Further, even though the viewer <b>40</b> is mounted, it is possible to move a manual focus setting region frame.</p><heading id="h-0012" level="1">Embodiment 2</heading><p id="p-0092" num="0091">In Embodiment 1, the example of one rear camera has been described, but in Embodiment 2, an example of two rear cameras will be described. <figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram illustrating an example of the configuration of the mobile terminal <b>1</b> of Embodiment 2. In <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the same structures as those in <figref idref="DRAWINGS">FIG. <b>1</b></figref> are denoted by the same reference numerals and the descriptions thereof will not be repeated. In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, two lens units <b>2</b><i>a </i>and <b>2</b><i>b </i>corresponding to the front camera and the rear camera, and two imaging units <b>3</b><i>a </i>and <b>3</b><i>b </i>are provided, whereas in the example of <figref idref="DRAWINGS">FIG. <b>9</b></figref>, a lens unit <b>2</b><i>c </i>and an imaging unit <b>3</b><i>c </i>are further provided.</p><p id="p-0093" num="0092">The lens unit <b>2</b><i>c </i>and the imaging unit <b>3</b><i>c </i>are combined with the lens unit <b>2</b><i>a </i>and the imaging unit <b>3</b><i>a, </i>so that the front camera can cope with stereo imaging or the like. For example, the imaging unit <b>3</b><i>a </i>and the imaging unit <b>3</b><i>b </i>are controlled to perform imaging at the same timing, and the interface circuit <b>4</b> outputs both input signals from the imaging units <b>3</b><i>a </i>and <b>3</b><i>c </i>to the image/audio signal processing circuit <b>5</b>. Further, the image/audio signal processing circuit <b>5</b> generates image data for display based on both input signals and outputs the image data to the display <b>8</b>. This display will be described later.</p><p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating an example of the external appearance of the mobile terminal <b>1</b> of Embodiment 2. <figref idref="DRAWINGS">FIG. <b>10</b>(<i>a</i>)</figref> is a diagram of the mobile terminal <b>1</b> as seen from the back surface side, <figref idref="DRAWINGS">FIG. <b>10</b>(<i>b</i>)</figref> is a diagram of the mobile terminal <b>1</b> as seen from the side surface side, and <figref idref="DRAWINGS">FIG. <b>10</b>(<i>c</i>)</figref> is a diagram of the mobile terminal <b>1</b> as seen from the front surface side. In <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the same structures as those in <figref idref="DRAWINGS">FIG. <b>3</b></figref> are denoted by the same reference numerals, and the descriptions thereof will not be repeated. In <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the lens unit <b>2</b><i>a </i>of the rear camera is provided, but in the example of <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the lens unit <b>2</b><i>a </i>and the lens unit <b>2</b><i>c </i>of the rear camera are provided.</p><p id="p-0095" num="0094">The lens unit <b>2</b><i>a </i>and the lens unit <b>2</b><i>c </i>are disposed at a predetermined interval in a longitudinal direction on the front surface of the mobile terminal <b>1</b>. Incidentally, disposition in the longitudinal direction is for performing photographing by using the mobile terminal <b>1</b> sideways. Although disposition is not limited thereto, it is preferable to dispose the lens units side by side in a long side direction of the display <b>8</b>.</p><p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram illustrating an example of a viewer <b>80</b> of Embodiment 2. The viewer <b>40</b> has a structure in which the viewer is attachable to or detachable from the back surface of the mobile terminal <b>1</b>. Incidentally, it is preferable that the viewer is mounted on the mobile terminal <b>1</b> illustrated in <figref idref="DRAWINGS">FIGS. <b>9</b> and <b>10</b></figref> of the present embodiment, and the viewer is mounted on the mobile terminal <b>1</b> illustrated in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>3</b></figref> of Embodiment 1, so that the viewer may be used for reproducing and displaying a 3D image.</p><p id="p-0097" num="0096">The viewer <b>80</b>, which is mounted on the mobile terminal <b>1</b> illustrated in <figref idref="DRAWINGS">FIGS. <b>9</b> and <b>10</b></figref> of the present embodiment, is used for 3D photographing by which images corresponding to a right eye and a left eye are simultaneously photographed by the two imaging units of the imaging unit <b>3</b><i>a </i>and the imaging unit <b>3</b><i>c. </i>Further, in reproducing, this viewer <b>80</b> is used for viewing 3D images. <figref idref="DRAWINGS">FIG. <b>11</b>(<i>a</i>)</figref> is a diagram of the viewer <b>80</b> as seen from the back surface side, <figref idref="DRAWINGS">FIG. <b>11</b>(<i>b</i>)</figref> is a diagram of the viewer <b>80</b> as seen from the side surface side, and <figref idref="DRAWINGS">FIG. <b>11</b>(<i>c</i>)</figref> is a diagram of the viewer <b>80</b> as seen from the front surface side.</p><p id="p-0098" num="0097">As illustrated in <figref idref="DRAWINGS">FIGS. <b>11</b>(<i>a</i>) and <b>11</b>(<i>b</i>)</figref>, each of the finder portions <b>82</b><i>a </i>and <b>82</b><i>c </i>has a rectangular tube shape, that is, a shape in which four side surfaces are surrounded by substantially rectangular light shielding plates to shield light from the side surfaces. Further, each of opening portions <b>81</b><i>a </i>and <b>81</b><i>c </i>of the finder portions <b>82</b><i>a </i>and <b>82</b><i>c </i>is an opening portion through which light is passed from the front surface side to the back surface side of the viewer <b>80</b>. A cavity portion <b>83</b> is a cavity through which light, a human finger or a touch pen is passed between the front surface side and the back surface side of the viewer <b>80</b>.</p><p id="p-0099" num="0098">The finder portion <b>82</b><i>a </i>and the finder portion <b>82</b><i>c </i>are disposed at a predetermined interval in the longitudinal direction on the back surface of the viewer <b>80</b>. This corresponds to the disposition of the lens units <b>2</b><i>a </i>and <b>2</b><i>c </i>illustrated in <figref idref="DRAWINGS">FIG. <b>10</b>(<i>c</i>)</figref>, and when photographing is performed by using the mobile terminal <b>1</b> sideways, each of the finder portion <b>82</b><i>a </i>and the finder portion <b>82</b><i>c </i>is disposed to be viewed by a left eye and a right eye. Incidentally, in order to increase area of the opening portions <b>81</b><i>a </i>and <b>81</b><i>c, </i>it is preferable to dispose the finder portions side by side in the long side direction of the display <b>8</b>. The structure of each of the finder portions <b>82</b><i>a </i>and <b>82</b><i>c </i>is the same as described with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0100" num="0099">In <figref idref="DRAWINGS">FIGS. <b>11</b>(<i>a</i>) and <b>11</b>(<i>c</i>)</figref>, there is illustrated an example in which, when photographing is performed by using the mobile terminal <b>1</b> sideways, the cavity portion <b>83</b> is disposed at a position where the touch panel <b>21</b> is easy to be operated by a thumb. However, the cavity portion is limited thereto, and may be disposed anywhere as long as a position is easy to be operated other than the finder portions <b>82</b><i>a </i>and <b>82</b><i>c. </i>However, the opening portions <b>81</b><i>a </i>and <b>81</b><i>c </i>and the cavity portion <b>83</b> are preferably disposed such that a portion of the viewer <b>80</b> other than the opening portions <b>81</b><i>a </i>and <b>81</b><i>c </i>and the cavity portion <b>83</b> is detected by the proximity sensor <b>31</b> at the time of mounting.</p><p id="p-0101" num="0100"><figref idref="DRAWINGS">FIG. <b>12</b>A</figref> is a diagram illustrating an example of a display of the photographing mode in a case where the viewer <b>80</b> is mounted. On the display <b>8</b>, imaging regions <b>91</b><i>a </i>and <b>91</b><i>c, </i>the setting buttons <b>61</b> to <b>63</b>, and the shutter button <b>65</b> are displayed, but the setting buttons <b>61</b> to <b>63</b> and the shutter button <b>65</b> are the same as described with reference to <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>. Then, the setting buttons <b>61</b> to <b>63</b> and the shutter button <b>65</b> are displayed in a region <b>90</b> corresponding to the cavity portion <b>83</b>.</p><p id="p-0102" num="0101">Each of the imaging region <b>91</b><i>a </i>and the imaging region <b>91</b><i>c </i>is a display of the range imaged by the imaging unit <b>3</b><i>a </i>and the imaging unit <b>3</b><i>c. </i>When the viewer <b>80</b> is mounted, the imaging regions are displayed at positions where the display of the imaging region <b>91</b><i>a </i>is viewed through the opening portion <b>81</b><i>a, </i>and the display of the imaging region <b>91</b><i>c </i>is viewed through the opening portion <b>81</b><i>c. </i>Although a subject <b>94</b><i>a </i>and a subject <b>94</b><i>c </i>are the same objects, the images are imaged by the imaging unit <b>3</b><i>a </i>and the imaging unit <b>3</b><i>c, </i>so that the images are displayed from positions having different angles.</p><p id="p-0103" num="0102">Then, the image data, which has been imaged and obtained by the imaging unit <b>3</b><i>a, </i>is displayed as the subject <b>94</b><i>a </i>in the imaging region <b>91</b><i>a, </i>so that the display of the subject <b>94</b><i>a </i>is viewed through the finder portion <b>82</b><i>a </i>by the left eye, and the image data, which has been imaged and obtained by the imaging unit <b>3</b><i>c, </i>is displayed as the subject <b>94</b><i>c </i>in the imaging region <b>91</b><i>c, </i>so that the display of the subject <b>94</b><i>a </i>is viewed through the finder portion <b>82</b><i>c </i>by the right eye. In this way, the image data can be viewed as 3D images.</p><p id="p-0104" num="0103">Incidentally, in a case where the viewer <b>80</b> is not mounted in the photographing mode, the image data cannot be viewed as 3D images, so that there may be provided a display including the imaging region <b>66</b><i>a </i>illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, instead of two displays of the imaging region <b>91</b><i>a </i>and the imaging region <b>91</b><i>c. </i>In order to display the imaging region <b>66</b><i>a, </i>image data, which has been imaged and obtained by either the imaging unit <b>3</b><i>a </i>or the imaging unit <b>3</b><i>c, </i>may be displayed.</p><p id="p-0105" num="0104">The display of each of the imaging region <b>91</b><i>a </i>and the imaging region <b>91</b><i>c </i>is smaller than the display of the imaging region <b>66</b><i>a </i>and the display of each of the subject <b>94</b><i>a </i>and the subject <b>94</b><i>c </i>may be reduced to be smaller than the display of the subject <b>67</b><i>a. </i>The display size of the imaging region <b>91</b><i>a </i>and the imaging region <b>91</b><i>c </i>may be the same size. Further, the mounting of the viewer <b>80</b> may be detected by the proximity sensor <b>31</b> and the display screen may be switched.</p><p id="p-0106" num="0105"><figref idref="DRAWINGS">FIG. <b>12</b>B</figref> is a diagram illustrating an example of the display of the reproducing mode in a case where the viewer <b>80</b> is mounted. In the display of <figref idref="DRAWINGS">FIG. <b>12</b>B</figref>, the same displays as those in <figref idref="DRAWINGS">FIG. <b>12</b>A</figref> are denoted by the same reference numerals, and the descriptions thereof will not be repeated. However, the setting buttons <b>61</b> to <b>63</b> may be operation menus for reproduction. Alternatively, the setting buttons may be a display of setting contents at the time of photographing an image to be reproduced. A selection button <b>95</b> for a recorded image, which has been displayed instead of the shutter button <b>65</b>, is a display for input used for selecting an image to be reproduced.</p><p id="p-0107" num="0106">The display regions <b>93</b><i>a </i>and <b>93</b><i>c </i>are displays of images to be reproduced, and in a case where imaging is performed in the display state illustrated in <figref idref="DRAWINGS">FIG. <b>12</b>A</figref>, subjects <b>94</b><i>d </i>and <b>94</b><i>e </i>are displayed in the same way as the subjects <b>94</b><i>a </i>and <b>94</b><i>c </i>illustrated in <figref idref="DRAWINGS">FIG. <b>12</b>A</figref>. Further, the display regions <b>93</b><i>a </i>and <b>93</b><i>c </i>are displayed at the same positions as the imaging regions <b>91</b><i>a </i>and <b>91</b><i>c. </i>In this way, the image data can be seen as 3D images in the same way as described with reference to <figref idref="DRAWINGS">FIG. <b>12</b>A</figref>.</p><p id="p-0108" num="0107">The imaging regions <b>92</b><i>a </i>and <b>92</b><i>c </i>are displays of images imaged by the imaging units <b>3</b><i>a </i>and <b>3</b><i>c. </i>Also, in the reproducing mode, imaging is performed by the imaging units <b>3</b><i>a </i>and <b>3</b><i>c, </i>and the imaged image is combined with a portion of the display regions <b>93</b><i>a </i>and <b>93</b><i>c </i>in real time to be displayed. In a case where the viewer <b>80</b> is used at the time of reproduction, light is shielded by the finder portions <b>82</b><i>a </i>and <b>82</b><i>c, </i>so that the surroundings are not viewed. However, the imaging regions <b>92</b><i>a </i>and <b>92</b><i>c </i>are superimposed and displayed on the display regions <b>93</b><i>a </i>and <b>93</b><i>c, </i>so that it is possible to confirm the surrounding situations while viewing 3D reproduction images. This synthesis and superimposition may be performed by the image/audio signal processing circuit <b>5</b>.</p><p id="p-0109" num="0108">Incidentally, the display of the imaging regions <b>92</b><i>a </i>and <b>92</b><i>c </i>in the reproducing mode may be switched on or off by a setting button or the like (not illustrated). Further, in the photographing mode, a reproduction image may be superimposed and displayed on a portion of the imaging regions <b>91</b><i>a </i>and <b>91</b><i>c. </i></p><p id="p-0110" num="0109">Since a positional relationship between the lens units <b>2</b><i>a </i>and <b>2</b><i>c, </i>the mobile terminal <b>1</b>, and the viewer <b>80</b> mounted on the mobile terminal <b>1</b> is fixed, in the photographing mode, the viewer <b>80</b> is in a state of looking into a direction in which the lens units <b>2</b><i>a </i>and <b>2</b><i>c </i>are directed. Also, in the reproducing mode, as in this state, the display images of the display regions <b>93</b><i>a </i>and <b>93</b><i>c </i>may be changed according to the direction in which the lens units <b>2</b><i>a </i>and <b>2</b><i>c </i>are directed.</p><p id="p-0111" num="0110">Here, the direction in which the lens units <b>2</b><i>a </i>and <b>2</b><i>c </i>are directed is also a direction of the viewer <b>80</b> and the mobile terminal <b>1</b>, a posture of the mobile terminal <b>1</b> is detected by the posture detection unit <b>7</b>, and a viewing direction of the viewer <b>80</b> mounted on the mobile terminal <b>1</b> is specified. Then, for example, out of the image data obtained by imaging at a super wide angle, that is, the image data recorded in a photographing direction, an image is cut out according to the direction in which the viewer <b>80</b> is directed, and the cut out image may be displayed in the display regions <b>93</b><i>a </i>and <b>93</b><i>c. </i></p><p id="p-0112" num="0111">The image data obtained by imaging at a super wide angle may be obtained by imaging by the mobile terminal <b>1</b> with the lens units <b>2</b><i>a </i>and <b>2</b><i>c </i>using super wide angle lenses, or the images obtained by imaging by other apparatuses may be taken in. Further, for example, it is possible to reproduce image data of so-called full sky type images having a field angle of 360 degrees, and to view the images in 3D in a direction corresponding to the direction in which the viewer <b>80</b> is directed while observing the images using the viewer <b>80</b>, so that the user can enjoy images with realistic feelings.</p><p id="p-0113" num="0112">Incidentally, also in this case, as described with reference to <figref idref="DRAWINGS">FIG. <b>12</b>B</figref>, since the images imaged by the imaging units <b>3</b><i>a </i>and <b>3</b><i>c </i>are superimposed and displayed on the reproduction image, it is possible to confirm the surrounding situations and to enhance safety. Further, the images are imaged by one of the imaging unit <b>3</b><i>a </i>and the imaging unit <b>3</b><i>c </i>of the mobile terminal <b>1</b> illustrated in <figref idref="DRAWINGS">FIGS. <b>9</b> and <b>10</b></figref> of the present embodiment, so that a display may be performed as illustrated in <figref idref="DRAWINGS">FIGS. <b>6</b>A, <b>6</b>B, <b>7</b>A, and <b>7</b>B</figref> of Embodiment 1. The viewer <b>40</b> illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref> and the viewer <b>80</b> illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref> may be selectively used depending on whether either or both of the imaging unit <b>3</b><i>a </i>and the imaging unit <b>3</b><i>b </i>is used.</p><p id="p-0114" num="0113">According to Embodiment 2, as described above, it is possible to view an imaging target in 3D images in photographing of the 3D images and also view the 3D images in the reproduction of the 3D images by the viewer <b>80</b>. In particular, it is not necessary to provide two electronic viewfinders, and it is possible to use the viewer <b>80</b> with a simple structure and at a low cost.</p><p id="p-0115" num="0114">Incidentally, in Embodiments 1 and 2, the mobile terminal <b>1</b> includes a general-purpose machine such as a tablet terminal or smartphone having an imaging unit. However, in addition to these, the present invention is also applicable to an imaging apparatus including a dedicated machine for mainly imaging, such as a digital camera or a video camera.</p><p id="p-0116" num="0115">According to the present invention, a portion of the display can be used in the same manner as the electronic viewfinder by mounting the viewer on the mobile terminal having the imaging unit. Therefore, it is easy to monitor images even in bright outdoors. Further, the display screen may be automatically switched between a case where the viewer is used and a case where the viewer is not used. Thereby, it is possible to provide a user-friendly mobile terminal and an imaging apparatus.</p><heading id="h-0013" level="1">REFERENCE SIGNS LIST</heading><p id="p-0117" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0116"><b>1</b> Mobile terminal</li>    <li id="ul0001-0002" num="0117"><b>2</b><i>a, </i><b>2</b><i>b, </i><b>2</b><i>c </i>Lens unit</li>    <li id="ul0001-0003" num="0118"><b>3</b><i>a, </i><b>3</b><i>b, </i><b>3</b><i>c </i>Imaging unit</li>    <li id="ul0001-0004" num="0119"><b>4</b> Interface circuit</li>    <li id="ul0001-0005" num="0120"><b>8</b> Display</li>    <li id="ul0001-0006" num="0121"><b>31</b> Proximity sensor</li>    <li id="ul0001-0007" num="0122"><b>40</b>, <b>80</b> Viewer</li>    <li id="ul0001-0008" num="0123"><b>42</b><i>a, </i><b>42</b><i>b, </i><b>82</b><i>a, </i><b>82</b><i>c </i>Finder portion</li>    <li id="ul0001-0009" num="0124"><b>43</b>, <b>83</b> Cavity portion</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-01-20" num="01-20"><claim-text><b>1</b>-<b>20</b>. (canceled)</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. A mobile terminal having a first side and a second side opposite to the first side, the mobile terminal comprising:<claim-text>a first camera included on the first side of the mobile terminal, the first camera configured to generate first image information including still image data and moving image data;</claim-text><claim-text>a second camera included on the second side of the mobile terminal, the second camera configured to generate second image information including still image data and moving image data;</claim-text><claim-text>a third camera included on the second side of the mobile terminal, the third camera configured to generate third image information, wherein the second camera is disposed at a predetermined interval from the third camera;</claim-text><claim-text>an encoder encoding the still image data included in the first image information and the second image information using a first type of compression scheme, and encoding the moving image data included in the first image information and the second image information using a second type of compression scheme;</claim-text><claim-text>a storage for storing the encoded still image data and the encoded moving image data;</claim-text><claim-text>a wireless communication circuit for connecting the mobile terminal to the Internet;</claim-text><claim-text>a GPS signal receiver for acquiring position information of the mobile terminal;</claim-text><claim-text>an interface circuit receiving the first image information from the first camera, the second image information from the second camera and the third image information from the third camera;</claim-text><claim-text>a proximity sensor generating proximity information;</claim-text><claim-text>a sensor to detect rotation of the mobile terminal;</claim-text><claim-text>a touch panel;</claim-text><claim-text>a display integrated with the touch panel,<claim-text>wherein the display is configured to switch between at least a first state and a second state based on at least the proximity information, and</claim-text><claim-text>wherein the display includes a first display region, a second display region, and a third display region, the third display region displaying at least one setting button for switching between the first camera and the second camera;</claim-text></claim-text><claim-text>a signal processor which receives the first image information, the second image information, or the third image information via the interface circuit, and generates displaying information to be displayed by the display based on the first image information, the second image information, or the third image information,<claim-text>wherein the signal processor performs image signal processing on the first image information, the second image information, or the third image information to perform a white balance correction; and</claim-text></claim-text><claim-text>a controller that receives the position information from the GPS signal receiver, wherein the controller controls at least the first camera, the second camera, the third camera, the wireless communication circuit, the display, and the signal processor,</claim-text><claim-text>wherein the display displays a focus setting region frame in response to a touch detected by the touch panel in the first display region, and the mobile terminal is configured to change a display position of the focus setting region frame based on detection of an additional touch in the first display region,</claim-text><claim-text>wherein the display displays the second displaying information, generated by the signal processor based on the second image information, in the first display region, and the third displaying information, generated by the signal processor based on the third image information, in the second display region, and</claim-text><claim-text>wherein the display displays the at least one setting button in the third display region, and the touch panel detects a touch to configure the displayed setting button.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The mobile terminal of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the third camera is used to generate an image with a super wide angle that is less than or equal to 360 degrees.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The mobile terminal of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the display displays the third displaying information which is clipped from the third image information by the signal processor.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The mobile terminal of <claim-ref idref="CLM-00021">claim 21</claim-ref>,<claim-text>wherein the first type of compression scheme is JPEG, the second type of compression scheme is MPEG, and</claim-text><claim-text>wherein the storage stores the still image data in the second image information, the still image data in the second image information, or the still image data in the third image information in JPEG format or RAW format, and the moving image data in the first image information, the moving image data in the second image information, or the moving image data in the third image information in MPEG format or RAW format.</claim-text></claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The mobile terminal of <claim-ref idref="CLM-00021">claim 21</claim-ref>,<claim-text>wherein the second type of compression scheme is H.264 or H.265, and</claim-text><claim-text>wherein the storage stores the moving image data in H.264, H.265, or RAW format.</claim-text></claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The mobile terminal of <claim-ref idref="CLM-00021">claim 21</claim-ref>, further comprising:<claim-text>a first lens associated with the first camera;</claim-text><claim-text>a second lens associated with the second camera;</claim-text><claim-text>a third lens associated with the third camera; and</claim-text><claim-text>an acceleration sensor.</claim-text></claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The mobile terminal of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the sensor to detect the rotation of the mobile terminal is an angular velocity sensor.</claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The mobile terminal of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the mobile terminal is configured to detect presence of a face of a user.</claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. The mobile terminal of <claim-ref idref="CLM-00021">claim 21</claim-ref>,<claim-text>wherein the second camera generates the second image information during a same time period as the third camera generates the third image information, and</claim-text><claim-text>wherein the signal processor receives the second image information and the third image information from the interface circuit, and performs image 21-processing on the second image information and the third image information such that the display displays image data based on the image processing performed on both the second image information and the third image information.</claim-text></claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The mobile terminal of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the mobile terminal detects whether an object different from a face is within a predetermined distance from the mobile terminal.</claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. A method for displaying information on a display of a mobile terminal having a first side and a second side opposite to the first side, the mobile terminal comprising:<claim-text>a first camera included on the first side of the mobile terminal, the first camera configured to generate first image information including still image data and moving image data;</claim-text><claim-text>a second camera included on the second side of the mobile terminal, the second camera configured to generate second image information including still image data and moving image data;</claim-text><claim-text>a third camera included on the second side of the mobile terminal, the third camera configured to generate third image information, wherein the second camera is disposed at a predetermined interval from the third camera;</claim-text><claim-text>an encoder encoding the still image data included in the first image information and the second image information using a first type of compression scheme, and encoding the moving image data included in the first image information and the second image information using a second type of compression scheme;</claim-text><claim-text>a storage for storing the encoded still image data and the encoded moving image data;</claim-text><claim-text>a wireless communication circuit for connecting the mobile terminal to the Internet;</claim-text><claim-text>a GPS signal receiver for acquiring position information of the mobile terminal;</claim-text><claim-text>an interface circuit receiving the first image information from the first camera, the second image information from the second camera and the third image information from the third camera;</claim-text><claim-text>a proximity sensor generating proximity information;</claim-text><claim-text>a sensor to detect rotation of the mobile terminal;</claim-text><claim-text>a touch panel;</claim-text><claim-text>a display integrated with the touch panel,<claim-text>wherein the display is configured to switch between at least a first state and a second state based on at least the proximity information, and</claim-text><claim-text>wherein the display includes a first display region, a second display region, and a third display region, the third display region displaying at least one setting button for switching between the first camera and the second camera;</claim-text></claim-text><claim-text>a signal processor which receives the first image information, the second image information, or the third image information via the interface circuit, and generates displaying information to be displayed by the display based on the first image information, the second image information, or the third image information,<claim-text>wherein the signal processor performs image signal processing on the first image information, the second image information, or the third image information to perform a white balance correction; and</claim-text></claim-text><claim-text>a controller that receives the position information from the GPS signal receiver, wherein the controller controls at least the first camera, the second camera, the third camera, the wireless communication circuit, the display, and the signal processor,</claim-text><claim-text>the method for displaying information on a display of a mobile terminal comprising:</claim-text><claim-text>displaying a focus setting region frame in response to a touch detected by the touch panel in the first display region, and changing a display position of the focus setting region frame based on detection of an additional touch in the first display region,</claim-text><claim-text>displaying the second displaying information, generated by the signal processor based on the second image information, in the first display region, and the third displaying information, generated by the signal processor based on the third image information, in the second display region, and</claim-text><claim-text>displaying the at least one setting button in the third display region, and detecting a touch to configure the displayed setting button.</claim-text></claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein the third camera is used to generate an image with a super wide angle that is less than or equal to 360 degrees.</claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. The method of <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein the third displaying information is clipped from the third image information by the signal processor.</claim-text></claim><claim id="CLM-00034" num="00034"><claim-text><b>34</b>. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref>,<claim-text>wherein the first type of compression scheme is JPEG, the second type of compression scheme is MPEG, and</claim-text><claim-text>wherein the storage stores the still image data in the second image information, the still image data in the second image information, or the still image data in the third image information in JPEG format or RAW format, and the moving image data in the first image information, the moving image data in the second image information, or the moving image data in the third image information in MPEG format or RAW format.</claim-text></claim-text></claim><claim id="CLM-00035" num="00035"><claim-text><b>35</b>. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref>,<claim-text>wherein the second type of compression scheme is H.264 or H.265, and</claim-text><claim-text>wherein the storage stores the moving image data in H.264, H.265, or RAW format.</claim-text></claim-text></claim><claim id="CLM-00036" num="00036"><claim-text><b>36</b>. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein the sensor to detect the rotation of the mobile terminal is an angular velocity sensor.</claim-text></claim><claim id="CLM-00037" num="00037"><claim-text><b>37</b>. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein the mobile terminal is configured to detect presence of a face of a user.</claim-text></claim><claim id="CLM-00038" num="00038"><claim-text><b>38</b>. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref>,<claim-text>wherein the second camera generates the second image information during a same time period as the third camera generates the third image information, and</claim-text><claim-text>wherein the signal processor receives the second image information and the third image information from the interface circuit, and performs image processing on the second image information and the third image information such that the display displays image data based on the image processing performed on both the second image information and the third image information.</claim-text></claim-text></claim><claim id="CLM-00039" num="00039"><claim-text><b>39</b>. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein the mobile terminal detects whether an object different from a face is within a predetermined distance from the mobile terminal.</claim-text></claim></claims></us-patent-application>