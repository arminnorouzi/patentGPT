<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004757A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004757</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17854722</doc-number><date>20220630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>DE</country><doc-number>10 2021 207 008.6</doc-number><date>20210705</date></priority-claim><priority-claim sequence="02" kind="national"><country>DE</country><doc-number>10 2021 207 246.1</doc-number><date>20210708</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>41</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>48</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>13</main-group><subgroup>931</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>931</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6262</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>022</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>41</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>4802</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>13</main-group><subgroup>931</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>931</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e79">DEVICE, MEMORY MEDIUM, COMPUTER PROGRAM AND COMPUTER-IMPLEMENTED METHOD FOR VALIDATING A DATA-BASED MODEL</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>Robert Bosch GmbH</orgname><address><city>Stuttgart</city><country>DE</country></address></addressbook><residence><country>DE</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Runge</last-name><first-name>Armin</first-name><address><city>Pittsburgh</city><state>PA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Weiss</last-name><first-name>Christian</first-name><address><city>Leonberg</city><country>DE</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Hakobyan</last-name><first-name>Gor</first-name><address><city>Schoenaich</city><country>DE</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Leidich</last-name><first-name>Stefan</first-name><address><city>Rutesheim</city><country>DE</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A device, a memory medium, a computer program, and a computer-implemented method for validating a data-based model for classifying an object into a class for an object type or a function type for a driver assistance system of a vehicle. The classification is determined as a function of a digital signal using the data-based model. A reference classification for the object is determined as a function of the digital signal, using a reference model. It is checked, as a function of the classification and the reference classification, whether or not the classification of the data-based model for the object is correct, and the data-based model is validated or not validated, depending on whether or not the classification is correct. The classification and the reference classification are determined for a set of digital signals that are associated with different distances between the object and a reference point.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="81.70mm" wi="122.26mm" file="US20230004757A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="105.07mm" wi="125.05mm" file="US20230004757A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="222.76mm" wi="87.55mm" orientation="landscape" file="US20230004757A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="226.57mm" wi="134.54mm" file="US20230004757A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="228.35mm" wi="147.66mm" file="US20230004757A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="211.92mm" wi="137.50mm" file="US20230004757A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD</heading><p id="p-0002" num="0001">The present invention relates to a device, a memory medium, a computer program, and a computer-implemented method for validating a data-based model.</p><heading id="h-0002" level="1">BACKGROUND INFORMATION</heading><p id="p-0003" num="0002">Driver assistance systems such as an emergency braking assistant and adaptive cruise control/cruise control systems may be implemented using video sensors and/or radar sensors. The objects encoded in data of these sensors may be recognized with the aid of object recognition and classified with the aid of object type recognition.</p><p id="p-0004" num="0003">Data-based models may be used for the object type recognition. Use of a data-based model in safety-critical applications requires a validation of the data-based model and the creation of a representative data set, for example for the validation or a training of the data-based model.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">A method and a device according to the present invention allow a validation of the data-based model and the creation of a representative data set for same.</p><p id="p-0006" num="0005">According to an example embodiment of the present invention, a computer-implemented method for validating a data-based model for classifying an object, in particular into a class for an object type or a function type for a driver assistance system of a vehicle, provides that the classification is determined as a function of a digital signal, in particular a digital image, in particular a radar spectrum or a LIDAR spectrum or a segment of one of these spectra, using the data-based model, a reference classification for the object being determined as a function of the digital signal, using a reference model, it being checked, as a function of the classification and the reference classification, whether or not the classification of the data-based model for the object is correct, and the data-based model being validated or not validated, depending on whether or not the classification of the data-based model for the object is correct, the classification and the reference classification preferably being determined for a set of digital signals that are associated with different distances between the object and a reference point, in particular the vehicle or a sensor for detecting the set, for each digital signal from the set a measure of confidence, in particular a distance of the object from the reference point, being determined, and the data-based model being validated when the classification of the data-based model for the object in the digital signals is correct, whose measure of confidence meets a condition, in particular the condition that the distance is within a reference distance from the reference point. With increasing confidence, for example with decreasing distance, the reference model reliably recognizes the correct object type. For the validation of data-based models, a demonstration is to be made that the intended function is fulfilled. The method allows a statistical argument concerning various actual situations. Via the validation it is either demonstrated that the data-based model has not resulted in a wrong decision or its decisions were even better than those of the reference model, or it is established that this is not the case. The use of the measure of confidence allows a particularly reliable validation.</p><p id="p-0007" num="0006">According to an example embodiment of the present invention, it may be provided that the set of digital signals and the reference classification are stored in association with one another when the measure of confidence meets the condition, in particular the distance is within the reference distance, and the classification deviates from the reference classification, and the digital signal is otherwise discarded and/or not stored. In this way a misclassification is recognized, and a data set that is particularly well suited for a training is created with little complexity.</p><p id="p-0008" num="0007">For the set, a value pair that includes a first value and a second value is preferably determined, the first value indicating a distance within which the reference classification for the object is correct, the second value indicating either a distance within which the classification of the data-based model for the object is correct, or a spacing of this distance from the reference distance. The data-based model is to correctly classify the object at least within the same distance as the reference model. The first value and the second value include the information necessary for this purpose, and are also variables that are easily evaluatable in the validation.</p><p id="p-0009" num="0008">According to an example embodiment of the present invention, for the value pair, a memory location in a memory is preferably determined, a value that is stored at this memory location being changed as a function of the values of the value pair. Instead of storing the first value and the second value themselves, only one value is stored. This is a particularly efficient way of storing cumulative information concerning the variables that are particularly easily evaluatable for the validation.</p><p id="p-0010" num="0009">The data-based model is preferably validated as a function of the value that is stored at this memory location.</p><p id="p-0011" num="0010">According to an example embodiment of the present invention, for a plurality of sets of digital signals, their classifications and their reference classifications are preferably determined, and it is checked whether or not the classification of the data-based model for the object is correct. The digital signals represent sequences of individual recordings that result at different distances upon an approach toward an object. The recordings may be radar, LIDAR, or video recordings or their spectra. The recordings may also be signals that are derived from same. Spectra are one option, but point clouds or other derived signals may also be used. The classifications of the plurality of sets of digital signals represent classifications for a large number of such approaches. A statistically relevant quantity of various situations is thus ensured for reliably validating the data-based model.</p><p id="p-0012" num="0011">It may be provided that for each set from the plurality of sets, a value pair that includes a first value and a second value for the particular set is determined, for each set a memory location for the value pair determined for this set being determined, and a value stored at this memory location being changed as a function of the values of the value pair. A statistically relevant quantity of results with which the data-based model is validated is thus provided.</p><p id="p-0013" num="0012">It may be provided that for each digital signal, a position is detected and/or stored in particular using a system for satellite navigation, the distance being determined as a function of the position. Region-specific relevant data may thus be determined.</p><p id="p-0014" num="0013">It may be provided that when the validation of the data-based model fails, the data-based model is retrained or trained with different data, and/or some other data-based model is used.</p><p id="p-0015" num="0014">It may be provided that when the validation of the data-based model is successful, the data-based model is used in a system for classifying objects, in particular in the driver assistance system.</p><p id="p-0016" num="0015">According to an example embodiment of the present invention, a device for validating a data-based model for classifying an object includes at least one processor and at least one memory that are designed to carry out the method.</p><p id="p-0017" num="0016">According to an example embodiment of the present invention, a computer program may be provided that includes machine-readable instructions, the method running when the machine-readable instructions are executed by a computer.</p><p id="p-0018" num="0017">According to an example embodiment of the present invention, a memory medium, in particular a permanent memory medium on which the computer program is stored, may be provided.</p><p id="p-0019" num="0018">Further advantageous specific embodiments of the present invention result from the description below and the figures.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a device according to an example embodiment of the present invention.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a schematic illustration of an object type recognition, according to an example embodiment of the present invention.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>3</b></figref> (which includes <figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref>) shows an example of the method according to the present invention.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an array with entries for the validation, according to an example embodiment of the present invention.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an example approach toward an object.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION OF EXAMPLE EMBODIMENTS</heading><p id="p-0025" num="0024">A device <b>100</b> for validating a data-based model is schematically illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The data-based model is designed for classifying an object. Device <b>100</b> includes at least one processor <b>102</b> and at least one memory <b>104</b>. Device <b>100</b> optionally includes at least one sensor <b>106</b> and a system <b>108</b> for satellite navigation.</p><p id="p-0026" num="0025">In the example, the at least one memory <b>104</b> includes a working memory and a permanent memory. In the example, the working memory allows faster access compared to the permanent memory.</p><p id="p-0027" num="0026">In the example, the at least one sensor <b>106</b> includes a radar sensor. The radar sensor emits high-frequency signals and receives reflections of static objects and moving objects. The signals are received with the aid of antennas of the radar sensor, changed into electrical signals by an electronics system, and converted into digital signals with the aid of analog-digital converters. The time signals are transferred into the frequency space with the aid of primary signal processing such as FFT.</p><p id="p-0028" num="0027">In the example, the at least one processor <b>102</b> and the at least one memory <b>104</b> are connected via a data link. The at least one sensor <b>106</b> and/or system <b>108</b> are/is connected to a data link for communicating with the at least one processor <b>102</b>. The at least one sensor <b>106</b> and/or the system may be connected to this data link from outside device <b>100</b>, or may have a design that is integrated into device <b>100</b>.</p><p id="p-0029" num="0028">The at least one processor <b>102</b> and the at least one memory <b>104</b> are designed to carry out an object recognition, an object type recognition, and the method or steps therein described below.</p><p id="p-0030" num="0029">A schematic illustration of the object type recognition is depicted in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0031" num="0030">In the example, device <b>100</b> is situated in a vehicle <b>200</b>. A spectrum <b>202</b> of the signal received from at least one radar sensor <b>106</b> is provided for the object type recognition. In the example, the object type recognition is carried out on a segment <b>204</b> of spectrum <b>202</b>, using a data-based model <b>206</b>. In the example, data-based model <b>206</b> includes an artificial neural network designed as a convolutional neural network, for example.</p><p id="p-0032" num="0031">In the example, segment <b>204</b> includes an object <b>208</b>. In the example, an object type <b>210</b> of object <b>208</b> is determined using data-based model <b>206</b>.</p><p id="p-0033" num="0032">The object recognition, within the meaning of which an object is present or not present in segment <b>204</b>, may take place in various ways. For example, a threshold value detector may be used. A distance of a recognized object from the sensor is recognizable via propagation time measurements or phase shifts, for example.</p><p id="p-0034" num="0033">In the example, the object recognition and the object type recognition are used for driver assistance.</p><p id="p-0035" num="0034">The quality of the object type recognition is essential for the quality of the driver assistance. The object type recognition may be designed to recognize the following object types: passenger car, bicycle, pedestrian, manhole cover. The passenger car, bicycle, and pedestrian object types may be assigned to a class &#x201c;may not be driven over.&#x201d; The manhole cover object type may be assigned to a class &#x201c;may be driven over.&#x201d; The object type recognition may also be designed to recognize other object types. Other classes may also be provided. For example, a class is provided for each object type.</p><p id="p-0036" num="0035">The quality of the object type recognition is measured, for example, by the correctness of the object type recognition over a distance from the object to be recognized. The greater the distance for a correct recognition, the better is the quality, for example, since the driving behavior may thus be adapted early to a recognized situation.</p><p id="p-0037" num="0036">The object type recognition may be technically implemented in various ways. In the example, data-based model <b>206</b> is designed as an artificial neural network. Data-based model <b>206</b> to be validated may also be part of a hybrid model. In this case, &#x201c;hybrid model&#x201d; refers to a combination of conventional signal processing and data-based model <b>206</b>. A conventional signal processing concept or some other data-based model that is already established may be used as a reference model for a validation of data-based model <b>206</b>.</p><p id="p-0038" num="0037">The reference model may also be a hybrid model, i.e., a combination of conventional signal processing and at least one data-based model.</p><p id="p-0039" num="0038">The validation of data-based model <b>206</b> may take place by comparing the results achieved using data-based model <b>206</b> to the results achieved using the reference model. The reference model preferably has a certain demonstrable classification quality.</p><p id="p-0040" num="0039">An example of a sequence of the method is described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref> (which includes <figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref>). The method makes use of the fact that the quality of classification at a close range is greater than that at a farther distance. An actual object type of detected objects is not subject to a temporal change. It is thus possible within the scope of an approach, i.e., travel of vehicle <b>200</b> toward the real object, to utilize changes that occur in the recognized object types in order to determine the quality and to identify data that are relevant for a training. The recordings which at a large distance result in a classification result that deviates from the classification result at the close range are relevant. The close range is, for example, a distance of the real object from vehicle <b>200</b> or from at least one sensor <b>106</b> that is between 3 meters and 30 meters. Beyond this distance, it may be assumed that the object type predicted by the reference model is correct based on the convergence property of the reference model.</p><p id="p-0041" num="0040">For cost reasons, the method according to the present invention is designed in such a way that a necessary data memory having a quick access time, in the example the working memory, is small.</p><p id="p-0042" num="0041">The method starts, for example, with a first-time recognition of an object by the object detector.</p><p id="p-0043" num="0042">Sensor data of a sensor are recorded in a step <b>302</b>. In the example, the sensor is a radar sensor. The function obtains new sensor data in this way. A spectrum is determined using the sensor data. In the example, a frame that includes the spectrum is determined.</p><p id="p-0044" num="0043">A step <b>304</b> is subsequently carried out.</p><p id="p-0045" num="0044">The object is recognized in step <b>304</b>. For example, the object in the spectrum is recognized.</p><p id="p-0046" num="0045">An instantaneous segment of the spectrum is determined in step <b>304</b>. In the example, the segment is a detail from the spectrum that includes the object. The instantaneous segment is stored in a variable S_akt. In the example, a frame that includes the instantaneous segment is stored in variable S_akt.</p><p id="p-0047" num="0046">An instantaneous distance is estimated in step <b>304</b>. In the example, the distance is the distance of the sensor from the object. The instantaneous distance is stored in a variable d_akt.</p><p id="p-0048" num="0047">A step <b>306</b> is subsequently carried out.</p><p id="p-0049" num="0048">Instantaneous segment S_akt is classified on the one hand using a data-based model <b>206</b>, and on the other hand using a reference model, in step <b>306</b>.</p><p id="p-0050" num="0049">A classification result of the reference model is stored in a variable for an instantaneous object type OT_akt_base. A classification result of the data-based model to be validated is stored in a variable for an instantaneous object type OT_akt_val. The reference model may include an established object recognition algorithm. The data-based model may include an algorithm to be validated.</p><p id="p-0051" num="0050">A step <b>308</b> is subsequently carried out.</p><p id="p-0052" num="0051">Variables that are used in the further process are initialized in step <b>308</b>.</p><p id="p-0053" num="0052">For the data-based model, a variable for a relevant object type OT_rel_val, a variable for a relevant segment S_rel_val, and a variable for a relevant distance d_rel_val are initialized.</p><p id="p-0054" num="0053">For the reference model, a variable for a relevant object type OT_rel_base, a variable for a relevant segment S_rel_base, and a variable for a relevant distance d_rel_base are initialized. In addition, a variable &#x201c;entries&#x201d; for a number of entries is initialized. In the example, the variables are assigned and stored as follows:</p><p id="p-0055" num="0054">OT_rel_base=OT_akt_base</p><p id="p-0056" num="0055">OT_rel_val=OT_akt_val</p><p id="p-0057" num="0056">S_rel_base=S_akt</p><p id="p-0058" num="0057">S_rel_val=S_akt</p><p id="p-0059" num="0058">d_rel_base=d_akt</p><p id="p-0060" num="0059">d_rel_val=d_akt</p><p id="p-0061" num="0060">entries=0</p><p id="p-0062" num="0061">A step <b>310</b> is subsequently carried out.</p><p id="p-0063" num="0062">Step <b>310</b> represents a beginning of a main loop.</p><p id="p-0064" num="0063">The following variables are assigned and stored as follows in step <b>310</b>:</p><p id="p-0065" num="0064">OT_old_base=OT_akt_base</p><p id="p-0066" num="0065">OT_old_val=OT_akt_val</p><p id="p-0067" num="0066">S_old_base=S_akt</p><p id="p-0068" num="0067">S_old_val=S_akt</p><p id="p-0069" num="0068">d_old_base=d_akt</p><p id="p-0070" num="0069">d_old_val=d_akt</p><p id="p-0071" num="0070">For the data-based model, the instantaneous object type is stored in a variable OT_old_val, the instantaneous sequence is stored in a variable S_old_val, and the instantaneous distance is stored in a variable d_old_val.</p><p id="p-0072" num="0071">For the reference model, the instantaneous object type is stored in a variable OT_old_base, the instantaneous sequence is stored in a variable S_old_base, and the instantaneous distance is stored in a variable d_old_base.</p><p id="p-0073" num="0072">A step <b>312</b> is subsequently carried out.</p><p id="p-0074" num="0073">Sensor data of the sensor are recorded in a step <b>312</b>. The function obtains new sensor data in this way. A spectrum is determined using the sensor data. In the example, a frame that includes the spectrum is determined.</p><p id="p-0075" num="0074">An instantaneous segment of the spectrum is determined in step <b>314</b>. In the example, the segment is a detail from the spectrum that includes the object. The instantaneous segment is stored in a variable S_akt. In the example, a frame that includes the instantaneous segment is stored in variable S_akt.</p><p id="p-0076" num="0075">An instantaneous distance is estimated in step <b>314</b>. In the example, the distance is the distance of the sensor from the object. The instantaneous distance is stored in variable d_akt.</p><p id="p-0077" num="0076">A step <b>316</b> is subsequently carried out.</p><p id="p-0078" num="0077">Instantaneous segment S_akt is classified in step <b>316</b>, using the reference model. A classification result of the reference model is stored in the variable for instantaneous object type OT_akt_base.</p><p id="p-0079" num="0078">A step <b>318</b> is subsequently carried out.</p><p id="p-0080" num="0079">For the reference model it is checked in step <b>318</b> whether or not the instantaneous object type and the buffered object type match. In the example, it is checked whether object type OT_akt_base!=object type OT_old_base.</p><p id="p-0081" num="0080">If the object types do not match, a step <b>320</b> is carried out. Otherwise, a step <b>322</b> is carried out.</p><p id="p-0082" num="0081">A change between the object types may be recognized via the comparison between the object types.</p><p id="p-0083" num="0082">The buffered data are stored as relevant data in step <b>320</b>, i.e., when a change takes place. In the example, the variables are assigned and buffered as follows:</p><p id="p-0084" num="0083">OT_rel_base=OT_old_base</p><p id="p-0085" num="0084">S_rel_base=S_old_base</p><p id="p-0086" num="0085">d_rel_base=d_old_base.</p><p id="p-0087" num="0086">If the object types are the same, the previously buffered relevant data are retained. The relevant data are preferably stored in the working memory, for example a volatile memory.</p><p id="p-0088" num="0087">Instantaneous segment S_akt is classified in step <b>322</b>, using data-based model <b>206</b>.</p><p id="p-0089" num="0088">A classification result of the data-based model to be validated is stored in the variable for instantaneous object type OT_akt_val.</p><p id="p-0090" num="0089">A step <b>324</b> is subsequently carried out.</p><p id="p-0091" num="0090">For the data-based model to be validated, it is checked in step <b>324</b> whether or not the instantaneous object type and the buffered object type match. In the example, it is checked whether object type OT_akt_val!=object type OT_old_val.</p><p id="p-0092" num="0091">If the object types do not match, a step <b>326</b> is carried out. Otherwise, a step <b>328</b> is carried out.</p><p id="p-0093" num="0092">A change between the object types may be recognized via the comparison between the object types.</p><p id="p-0094" num="0093">The buffered data are stored as relevant data in step <b>326</b>, i.e., when a change takes place. In the example, the variables are assigned and stored as follows:</p><p id="p-0095" num="0094">OT_rel_val=OT_old_val</p><p id="p-0096" num="0095">S_rel_val=S_old_val</p><p id="p-0097" num="0096">d_rel_val=d_old_val.</p><p id="p-0098" num="0097">If the object types are the same, the previously stored relevant data are retained.</p><p id="p-0099" num="0098">A comparison is made between the instantaneous distance from the object and a threshold value in step <b>328</b>. For example, it is checked whether d_akt&#x3c;SHORTDIST, where SHORTDIST is a stored constant. In the example, constant SHORTDIST is a value that represents a distance between 3 m and 30 m. In the example, it is checked whether the object is situated in the close range. In the close range, the object recognition using the reference model, i.e., using the established algorithm, is deemed reliable. If the close range is not reached, the main loop is executed anew, beginning with step <b>310</b>.</p><p id="p-0100" num="0099">It may be provided that in step <b>328</b> it is also checked whether a predefined time has elapsed since the most recent execution of an update of the relevant data. If this is not the case, in the example the main loop is executed anew, beginning with step <b>310</b>, regardless of whether or not the close range is reached.</p><p id="p-0101" num="0100">For example, an instantaneous time is determined, and a difference between the instantaneous time and the most recent point in time at which the update took place is determined, in step <b>328</b>. The instantaneous time is determined using a function time_now( ) for example. The most recent point in time at which the update took place is stored in a variable lastUpdate, for example. This variable is initialized, for example, in a first iteration using zero.</p><p id="p-0102" num="0101">In the example, a step <b>330</b> is carried out when the close range is reached and the difference is greater than a threshold value. The threshold value is a constant RETRIGGER, for example. Constant RETRIGGER may be a time from a range between 10 ms and 1 s. Otherwise, step <b>310</b> is carried out in the example.</p><p id="p-0103" num="0102">The most recent point in time at which the update took place is set to the value of the instantaneous time in step <b>330</b>. In the example, lastUpdate=time now( ) is set.</p><p id="p-0104" num="0103">When the close range is reached and the actual object type has thus been identified, it may be provided to store the relevant data of the algorithm to be validated and/or to insert an entry into a validation array.</p><p id="p-0105" num="0104">An example of a procedure for storing the relevant data of the algorithm to be validated is referred to below as corner case detection.</p><p id="p-0106" num="0105">An example of a procedure for inserting an entry into the validation array is referred to below as validation.</p><p id="p-0107" num="0106">In the example, both processes run in parallel, and are explained in greater detail below. After step <b>330</b>, in the example step <b>332</b> is carried out at the start of the corner case detection, and step <b>336</b> is carried out at the start of the validation.</p><p id="p-0108" num="0107">Corner Case Detection:</p><p id="p-0109" num="0108">It is basically assumed that the segment stored in S_rel_val is relevant. It is assumed that the algorithm to be validated likewise has a higher classification quality for fairly short distances. However, it is not a necessary condition that the algorithm already delivers a reliable classification when the close range, in the example d_akt&#x3c;SHORTDIST, has been reached.</p><p id="p-0110" num="0109">It is checked in step <b>332</b> whether or not the object type, which has been recognized using the reference model, matches the object type from the relevant data for data-based model <b>206</b> to be validated. For example, it is checked whether OT_rel_val!=OT_akt_base. If both object types agree, the main loop is executed, beginning with step <b>310</b>. Otherwise, a step <b>334</b> is carried out. It is thus ensured that no segments are stored for which data-based model <b>206</b> to be validated has already classified the correct object type in a previous iteration, but has then changed to an incorrect object type.</p><p id="p-0111" num="0110">The relevant data are stored in step <b>334</b>. The relevant data are preferably stored in the permanent memory in step <b>334</b>.</p><p id="p-0112" num="0111">If the object types differ, a relevant segment has been identified, and the corresponding relevant data are stored for later use.</p><p id="p-0113" num="0112">In the example, after step <b>334</b> the main loop is executed, beginning with step <b>310</b>.</p><p id="p-0114" num="0113">It may be provided that in a parallel task, not illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the data stored in the permanent memory are transferred into a computer infrastructure as soon as a sufficient quantity of relevant data is present.</p><p id="p-0115" num="0114">The arrival of new data from the permanent memory in the computer infrastructure may initiate a training operation.</p><p id="p-0116" num="0115">In the example, a new data-based model <b>206</b> is determined in the training operation. It may be provided that this model is compiled to form new firmware and is provided to the sensor via over-the-air firmware, for example. It may be provided to activate the new firmware in the sensor, reinitialize the variables, and start the method anew.</p><p id="p-0117" num="0116">Validation:</p><p id="p-0118" num="0117">In the example, the validation is used to establish whether or not data-based model <b>206</b> is suitable for its intended purpose.</p><p id="p-0119" num="0118">Situations in which data-based model <b>206</b> to be validated has not been able to classify the correct result play a special role in the validation.</p><p id="p-0120" num="0119">It is checked in a step <b>336</b> whether or not this situation is present. In the example, it is checked whether OT_akt_val!=OT_akt_base. If this situation is present, a step <b>338</b> is carried out. Otherwise, a step <b>340</b> is carried out.</p><p id="p-0121" num="0120">In the example, a ccc value for OTC_val is set to 0 in step <b>338</b>, since d_rel_val in this case belongs to the most recent change of the object type, but not to the correct object type. Step <b>340</b> is subsequently carried out.</p><p id="p-0122" num="0121">An important metric for the quality of an object recognition algorithm is a distance at or above which it has been continuously possible to correctly classify the object. This is referred to as continuous correct classification (ccc). In the example, the ccc value is determined using a function ccc(&#x22c5;). For the reference model, the ccc value is determined using a function ccc(OTC_base). For data-based model <b>206</b> to be validated, the ccc value is determined using a function ccc(OTC_val). In the example, the reference model has a demonstrably sufficient classification quality. For data-based model <b>206</b> to be validated, in the example it is to be shown that in all relevant situations and below a distance limit DIST REL, the model has a ccc value that is at least as great as the reference model.</p><p id="p-0123" num="0122">To allow this demonstration to be made, the ccc value of the reference model and difference &#x394;ccc between the ccc value for the reference model and the ccc value of data-based model <b>206</b> to be validated are stored in a two-dimensional array. This array may be represented as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0124" num="0123">Difference &#x394;ccc in meters is illustrated across an x axis. A range from &#x2212;200 meters to +200 meters is illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. A plurality of ranges are defined on the x axis. A range has an extension <b>402</b>, referred to below as BIN_SIZE, in the x direction.</p><p id="p-0125" num="0124">The ccc value for the reference model is illustrated in meters across a y axis. A range from 0 meters to 200 meters is illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In the example, the close range is reached at a boundary <b>404</b> at a distance DIST_REL that is less than or equal to 150 meters, for example.</p><p id="p-0126" num="0125">To allow the array to be efficiently stored, the ccc values are assigned to individual BINs. Each BIN has a size of BIN_SIZE. Accordingly, the array has a size of (200*2/BIN_SIZE)&#xd7;(200/BIN_SIZE), for example, and an entry for ccc(OTC_base), ccc(OTC_val) results in an increment of the array at the following (x, y) position of the array:</p><p id="p-0127" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mo>(</mo>  <mrow>   <mrow>    <mo>&#x230a;</mo>    <mfrac>     <mrow>      <mrow>       <mi>ccc</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mi>OTC</mi>        <mo>&#x2062;</mo>        <mi>_base</mi>       </mrow>       <mo>)</mo>      </mrow>      <mo>-</mo>      <mrow>       <mi>ccc</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mi>OTC</mi>        <mo>&#x2062;</mo>        <mo>_</mo>        <mo>&#x2062;</mo>        <mi>val</mi>       </mrow>       <mo>)</mo>      </mrow>     </mrow>     <mi>BIN_SIZE</mi>    </mfrac>    <mo>&#x230b;</mo>   </mrow>   <mo>,</mo>   <mrow>    <mo>&#x230a;</mo>    <mfrac>     <mrow>      <mi>ccc</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>OTC</mi>       <mo>&#x2062;</mo>       <mi>_base</mi>      </mrow>      <mo>)</mo>     </mrow>     <mrow>      <mo>(</mo>      <mi>BIN_SIZE</mi>      <mo>)</mo>     </mrow>    </mfrac>    <mo>&#x230b;</mo>   </mrow>  </mrow>  <mo>)</mo> </mrow></math></maths></p><p id="p-0128" num="0126">Since for distances less than distance <b>404</b>, data-based model <b>206</b> to be validated must have at least the classification quality of the established reference model, all entries in a range of 0&#x3c;x &#x3c;200 and 0&#x3c;y&#x3c;DIST REL are to be smaller than a threshold, preferably 0. This is a range at the lower right side in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Entries in this range mean that the difference between ccc(OTC_base) and ccc(OTC_val) was positive, and data-based model <b>206</b> to be validated thus has a poorer ccc value. ccc(OTC_base) and ccc(OTC_val) represent a value pair. The value pair includes a first value ccc(OTC_base), which represents a distance within which the reference classification for the object is correct. The value pair includes a second value ccc(OTC_val), which represents a distance within which the classification of data-based model <b>206</b> for the object is correct. The difference ccc(OTC_base)&#x2212;ccc(OTC_val) represents a spacing of this distance from the reference distance.</p><p id="p-0129" num="0127">In contrast, entries in which &#x2212;200&#x3c;x&#x2264;0 and <b>0</b>&#x3c;y&#x3c;DIST_REL are to assume high values. This is a range at the lower left side in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Entries in this range mean that data-based model <b>206</b> to be validated has a higher ccc value than the established reference model.</p><p id="p-0130" num="0128">For distances greater than or equal to DIST_REL, a higher classification quality of data-based model <b>206</b> to be validated is likewise desirable, but not absolutely necessary.</p><p id="p-0131" num="0129">The following variables are determined in each case for the relevant data in step <b>340</b>:</p><p id="p-0132" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>bin_ _rel_base&#x2212;d_rel_val<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0133" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>bin_val=floor (delta/BIN_SIZE)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0134" num="0130">A step <b>342</b> is subsequently carried out.</p><p id="p-0135" num="0131">The array is updated in step <b>342</b>. For example, a function ccc_matrix(bin_bas, bin_val)++ is executed. This function increments the entries in the array by one at the locations that are defined by bin_bas and bin_val. In this way, a value stored at this memory location is changed as a function of the values of the value pair.</p><p id="p-0136" num="0132">In addition, in the example a number of entries in the array are counted. In the example, the variable &#x201c;entries&#x201d; is incremented by one: entries++</p><p id="p-0137" num="0133">A step <b>344</b> is subsequently carried out.</p><p id="p-0138" num="0134">It is checked in step <b>344</b> whether the number of entries in the array exceeds a threshold value. In the example, it is checked whether the variable &#x201c;entries&#x201d; &#x3e;MAX_ENTRIES. If the number of entries exceeds the threshold value, a step <b>346</b> is carried out. Otherwise, the validation is ended.</p><p id="p-0139" num="0135">The array thus generated when MAX_ENTRIES is exceeded is transferred into the computer infrastructure in step <b>346</b>.</p><p id="p-0140" num="0136">The array is an efficient representation of the ccc values. It may be provided that data-based model <b>206</b> is validated using the array.</p><p id="p-0141" num="0137">The validation subsequently ends.</p><p id="p-0142" num="0138">It may be provided that when the validation of data-based model <b>206</b> fails, data-based model <b>206</b> is retrained or trained with different data, and/or some other data-based model is used.</p><p id="p-0143" num="0139">It may be provided that when the validation of data-based model <b>206</b> is successful, data-based model <b>206</b> is used in a system for classifying objects, in particular in the driver assistance system.</p><p id="p-0144" num="0140">It may be provided that this method is carried out by multiple vehicles. It may be provided that data-based model <b>206</b> is validated using the arrays of these vehicles.</p><p id="p-0145" num="0141">These arrays are used, for example, for a statistical validation of data-based model <b>206</b>.</p><p id="p-0146" num="0142">An example approach toward an object is schematically illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. The x axis shows a distance from the object in negative values. The object type is plotted on the y axis. In the example, this is an object having the arbitrarily selected object type class <b>3</b>.</p><p id="p-0147" num="0143">An object type that is predicted with the aid of the established reference model is illustrated as a triangle for various distances. An object type that is predicted with the aid of data-based model <b>206</b> to be validated is illustrated as a circle for various distances.</p><p id="p-0148" num="0144">In this example, the reference model continuously correctly classifies the object starting at a distance of 8 m. Data-based model <b>206</b> to be validated already continuously correctly classifies the object starting at 10 m.</p><p id="p-0149" num="0145">In this example, when the close range is reached, at a distance of 8 meters in the example, the correct object type is identified and the most recent misclassification by the data-based model to be validated for a distance of 11 m is transferred. In this example, ccc(OTC_base)=8 and ccc(OTC_val) =10. This results in an incrementation of the validation array at (8, &#x2212;2).</p><p id="p-0150" num="0146">An identification of data for which data-based model <b>206</b> to be validated has classified an incorrect object type may be provided. These data are data, for example, which the established reference model has classified differently. These data are of particular importance for a training of data-based model <b>206</b>, for example a neural network for the classification, since they reveal a weak point of the object recognition in the particular instantaneous state.</p><p id="p-0151" num="0147">Instead of using distance d_akt and threshold value SHORTDIST to decide that data-based model <b>206</b> to be validated has correctly classified the object, additionally or alternatively a measure of confidence of the object recognition by the reference model may be used. This measure of confidence is, for example, provided by the reference model, and may be based, for example, on a duration of a stable classification by the reference model.</p><p id="p-0152" num="0148">It may also be provided to determine a ccc distance for the reference model and data-based model <b>206</b> to be validated for an increasing distance from the object. If an object, which was situated in a range in which a continuously correct classification was possible, subsequently moves out of this range, for example the ccc distance at or above which the ccc is no longer possible is determined. The above-described procedure is followed for this purpose. In this case, the corner case detection may likewise be carried out.</p><p id="p-0153" num="0149">Instead of using the distance from the classified object as a measure for a reliable classification result of the established reference model, some other measure of confidence may be used. For example, the classification result of the established reference model may be regarded as reliable when, for a certain time period greater than a threshold value, for example t_stable, a stable, i.e., unchanging, classification result was present. Thus, follow-up trips without a close approach may also be used for the validation, regardless of the distance from the object.</p><p id="p-0154" num="0150">The example of object classification is based on segments of spectra. Instead of segments of spectra, the object classification may also be based on other input variables. For example, the procedure may also be used for a location-based object recognition algorithm that replaces or supplements the object classification that is based on the segments of spectra. In this case, the corresponding data, i.e., the positions instead of the spectra, are stored as relevant data.</p><p id="p-0155" num="0151">In the example described above, for the corner case detection it is checked whether the object types, in the example OT_rel_val and OT_akt_base, are different in order to ensure that no data that have resulted in the correct classification are stored in the permanent memory. Instead, it may be provided when there is a disparity in the recognized object types, in the example of OT_akt_base and OT_akt_val, it is not the object type, in the example OT_rel_val, that was previously recognized by the reference model that is stored in the permanent memory, but, rather, the object type, in the example OT_akt_val, that is recognized at that moment by the reference model. This is advantageous, since data-based model <b>206</b> in this case also delivers an incorrect classification result for the instantaneous segment.</p><p id="p-0156" num="0152">It may be provided that particular GPS positions of the data detection are stored in addition to the stated data. These GPS positions may be provided by the vehicle via a bus system. With the aid of the GPS positions, data are provided with which it is possible to train data-based model <b>206</b> on a region-specific basis.</p><p id="p-0157" num="0153">A comparison of the object types may be replaced by other functions such as an intervention of an automatic emergency brake or an automatic emergency evasive maneuver. This means that a response of the function to the particular recognized object type is used.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230004757A1-20230105-M00001.NB"><img id="EMI-M00001" he="6.01mm" wi="76.20mm" file="US20230004757A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-01-13" num="01-13"><claim-text><b>1</b>-<b>13</b> (canceled)</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A computer-implemented method for validating a data-based model for classifying an object into a class for an object type or a function type for a driver assistance system of a vehicle, the method comprising the following steps:<claim-text>determining the classification as a function of a digital signal, using the data-based model, the digital signal being a digital image or a radar spectrum or a LIDAR spectrum or a segment of a radar spectrum or a segment of a LIDAR spectrum;</claim-text><claim-text>determining, using the data-based model, a reference classification for the object as a function of the digital signal;</claim-text><claim-text>checking, using a reference model, as a function of the classification and the reference classification, whether or not the classification of the data-based model for the object is correct; and</claim-text><claim-text>validating or not validating the data-based model, depending on whether or not the classification of the data-based model for the object is correct;</claim-text><claim-text>wherein the classification and the reference classification are determined for a set of digital signals that are associated with different distances between the object and a reference point, and wherein for each digital signal from the set, a measure of confidence is determined, and the data-based model being validated when the classification of the data-based model for the object in the digital signals is correct, whose measure of confidence meets a condition, wherein the measure of confidence is a distance of the object from the reference point, and wherein the condition is that the distance is within a reference distance from the reference point.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method as recited in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the reference point is the vehicle or a sensor for detecting the set of digital signals.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method as recited in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the set of digital signals and the reference classifications are stored in association with one another when the measure of confidence meets the condition and the classification deviates from the reference classification, and the digital signals are otherwise discarded and/or not stored.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method as recited in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein for the set, a value pair that includes a first value and a second value is determined, the first value indicating a distance within which the reference classification for the object is correct, and the second value indicating a distance within which the classification of the data-based model for the object is correct, or a spacing of the distance from the reference distance.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method as recited in <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein for the value pair, a memory location in a memory is determined, a value that is stored at the determined memory location being changed as a function of the values of the value pair.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method as recited in <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the data-based model is validated as a function of the value that is stored at the determined memory location.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method as recited in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein, for a plurality of sets of digital signals, their classifications and their reference classifications are determined, and it is checked whether or not the classification of the data-based model for the object is correct.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The method as recited in <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein for each set from the plurality of sets, a value pair that includes a first value and a second value for the set is determined, for each set, a memory location for the value pair determined for the set is determined, and a value stored at the determined memory location is changed as a function of the values of the value pair.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The method as recited in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein for each digital signal, a position is detected and/or stored using a system for satellite navigation, the distance being determined as a function of the position.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The method as recited in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein when the validation of the data-based model fails: (i) the data-based model is retrained or trained with different data, and/or (ii) a different data-based model is used.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The method as recited in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein when the validation of the data-based model is successful, the data-based model is used in a system for classifying objects in the driver assistance system.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. A device for validating a data-based model for classifying an object, the device comprising:<claim-text>at least one processor; and</claim-text><claim-text>at least one memory;</claim-text><claim-text>wherein the device is configured to:<claim-text>determine a classification of the object as a function of a digital signal, using the data-based model, the digital signal being a digital image or a radar spectrum or a LIDAR spectrum or a segment of a radar spectrum or a segment of a LIDAR spectrum;</claim-text><claim-text>determine, using the data-based model, a reference classification for the object as a function of the digital signal;</claim-text><claim-text>check, using a reference model, as a function of the classification and the reference classification, whether or not the classification of the data-based model for the object is correct; and</claim-text><claim-text>validate or not validate the data-based model, depending on whether or not the classification of the data-based model for the object is correct;</claim-text><claim-text>wherein the classification and the reference classification are determined for a set of digital signals that are associated with different distances between the object and a reference point, and wherein for each digital signal from the set, a measure of confidence is determined, and the data-based model being validated when the classification of the data-based model for the object in the digital signals is correct, whose measure of confidence meets a condition,</claim-text></claim-text><claim-text>wherein the measure of confidence is a distance of the object from the reference point, and wherein the condition is that the distance is within a reference distance from the reference point.</claim-text></claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. A non-transitory memory medium on which is stored a computer program for validating a data-based model for classifying an object into a class for an object type or a function type for a driver assistance system of a vehicle, the computer program, when executed by a computer, causing the computer to perform the following steps:<claim-text>determining the classification as a function of a digital signal, using the data-based model, the digital signal being a digital image or a radar spectrum or a LIDAR spectrum or a segment of a radar spectrum or a segment of a LIDAR spectrum;</claim-text><claim-text>determining, using the data-based model, a reference classification for the object as a function of the digital signal;</claim-text><claim-text>checking, using a reference model, as a function of the classification and the reference classification, whether or not the classification of the data-based model for the object is correct; and</claim-text><claim-text>validating or not validating the data-based model, depending on whether or not the classification of the data-based model for the object is correct;</claim-text><claim-text>wherein the classification and the reference classification are determined for a set of digital signals that are associated with different distances between the object and a reference point, and wherein for each digital signal from the set, a measure of confidence is determined, and the data-based model being validated when the classification of the data-based model for the object in the digital signals is correct, whose measure of confidence meets a condition, wherein the measure of confidence is a distance of the object from the reference point, and wherein the condition is that the distance is within a reference distance from the reference point.</claim-text></claim-text></claim></claims></us-patent-application>