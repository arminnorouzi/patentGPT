<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000357A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000357</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17779492</doc-number><date>20201124</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>1455</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>N</subclass><main-group>21</main-group><subgroup>31</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>0075</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>1455</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>N</subclass><main-group>21</main-group><subgroup>31</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">VIRTUAL HYPERSPECTRAL IMAGING OF BIOLOGICAL TISSUE FOR BLOOD HEMOGLOBIN ANALYSIS</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>62945808</doc-number><date>20191209</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>62945816</doc-number><date>20191209</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Purdue Research Foundation</orgname><address><city>West Lafayette</city><state>IN</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Kim</last-name><first-name>Young L.</first-name><address><city>West Lafayette</city><state>IN</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Haque</last-name><first-name>Md Munirul</first-name><address><city>Fishers</city><state>IN</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Visbal-Onufrak</last-name><first-name>Michelle Amaris</first-name><address><city>Las Piedras</city><state>PR</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Park</last-name><first-name>Sang Mok</first-name><address><city>West Lafayette</city><state>IN</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Purdue Research Foundation</orgname><role>02</role><address><city>West Lafayette</city><state>IN</state><country>US</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/US2020/062016</doc-number><date>20201124</date></document-id><us-371c12-date><date>20220524</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A system for generating hyperspectral imaging data for measuring biochemical compositions is disclosed which includes a spectral imaging device adapted to acquire one or more hyperspectral linescan images, an optical imaging device with a red-green-blue (RGB) sensor adapted to acquire an RGB dataset, a processor adapted to co-locate a plurality of pixels in the RGB dataset vs. a corresponding plurality of pixels of the one or more hyperspectral linescan datasets, establish a transformation matrix utilizing the plurality of co-located pixels, apply the transformation matrix to the RGB dataset to thereby generate the hyperspectral dataset, and analyze the generated hyperspectral image dataset to determine the biochemical compositions.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="77.72mm" wi="158.75mm" file="US20230000357A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="189.74mm" wi="123.36mm" orientation="landscape" file="US20230000357A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="243.33mm" wi="148.08mm" file="US20230000357A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="246.72mm" wi="128.95mm" file="US20230000357A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="249.60mm" wi="127.42mm" file="US20230000357A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="177.21mm" wi="124.54mm" orientation="landscape" file="US20230000357A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="235.03mm" wi="146.64mm" orientation="landscape" file="US20230000357A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="187.79mm" wi="130.05mm" orientation="landscape" file="US20230000357A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="223.60mm" wi="147.24mm" orientation="landscape" file="US20230000357A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="119.63mm" wi="112.18mm" orientation="landscape" file="US20230000357A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="119.38mm" wi="113.71mm" orientation="landscape" file="US20230000357A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="230.80mm" wi="118.53mm" orientation="landscape" file="US20230000357A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="236.47mm" wi="152.32mm" orientation="landscape" file="US20230000357A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="118.11mm" wi="117.35mm" orientation="landscape" file="US20230000357A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="210.90mm" wi="107.02mm" orientation="landscape" file="US20230000357A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The present patent application is related to and claims the priority benefit of U.S. Provisional Patent Application Ser. No. 62/945,808 filed Dec. 9, 2019, titled &#x201c;VIRTUAL HYPERSPECTRAL IMAGING OF BIOLOGICAL TISSUE FOR BLOOD HEMOGLOBIN ANALYSIS&#x201d;; and U.S. Provisional Patent Application Ser. No. 62/945,816, filed Dec. 9, 2019, titled &#x201c;HYPERSPECTRAL IMAGE CONSTRUCTION OF BIOLOGICAL TISSUE FOR BLOOD HEMOGLOBIN ANALYSIS USING A SMARTPHONE&#x201d; the contents of each of which are hereby incorporated by reference in its entirety into the present disclosure.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?federal-research-statement description="Federal Research Statement" end="lead"?><heading id="h-0002" level="1">STATEMENT REGARDING GOVERNMENT FUNDING</heading><p id="p-0003" num="0002">This invention was made with government support under R21TW010620 awarded by the National Institutes of Health and 7200AA18CA00019 awarded by the US Agency for International Development. The government has certain rights in the invention.</p><?federal-research-statement description="Federal Research Statement" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0003" level="1">TECHNICAL FIELD</heading><p id="p-0004" num="0003">The present disclosure generally relates to generating a hyperspectral imaging dataset, recovering hyperspectral information from RGB values, analyzing blood, and in particular, to a system and method of analyzing biological tissue for blood hemoglobin analysis.</p><heading id="h-0004" level="1">BACKGROUND</heading><p id="p-0005" num="0004">This section introduces aspects that may help facilitate a better understanding of the disclosure. Accordingly, these statements are to be read in this light and are not to be understood as admissions about what is or is not prior art.</p><p id="p-0006" num="0005">Blood hemoglobin (Hgb) tests are routinely ordered as an initial screening of the amount of red blood cells (or hemoglobin) in the blood as part of a general health test for a subject. Blood Hgb tests are extensively performed for a variety of patient care needs, such as anemia detection as a cause of other underlying diseases, hemorrhage detection after traumatic injury, assessment of hematologic disorders, and for transfusion initiation. There are several biological assays for measuring blood Hgb content in grams per deciliter (i.e. g dL<sup>&#x2212;1</sup>) from blood drawn via traditional needle-based methods. Portable point-of-care hematology analyzers using blood draws (e.g. Abbott i-STAT and HemoCue) are also commercially available. However, all these tests require expensive and environment-sensitive analytical cartridges with short shelf lives, as well as unaffordable for both resource-limited and homecare settings. In addition, repeated blood Hgb measurements using these invasive tests can cause iatrogenic complications such as blood loss.</p><p id="p-0007" num="0006">Unlike measuring oxygen saturation with pulse oximetry, noninvasive measurements of a total Hgb concentration in the blood are not straightforward. A few noninvasive Hgb testing devices (e.g. MASIMO and ORSENSE) have recently become available that are currently undergoing clinical studies for immediate reading and continuous monitoring of blood Hgb levels in different clinical settings. Aside from the relatively high cost associated with operating and maintaining the equipment, the medical community agrees that the broad limits of agreement between these devices and central laboratory tests pose a significant challenge in making clinical decision, thus generating skepticism in clinical adaptation. Several smartphone-based anemia detection technologies (e.g., HEMOGLOBE, EYENAEMIA, AND HEMAAPP) have also made progress, however, most of these mobile applications are intended for initial screening or risk stratification of severe anemia and are not developed for measuring exact Hgb content in the unit of g dL<sup>&#x2212;1</sup>.</p><p id="p-0008" num="0007">Therefore, there is an unmet need for a novel technology that can provide non-invasive Hgb measurements that can be relied for accuracy without the complications associated with expensive laboratory equipment.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0009" num="0008">A system for generating hyperspectral imaging data for measuring biochemical compositions is disclosed. The system includes a spectral imaging device adapted to acquire one or more hyperspectral linescan images from one or more regions of interest of a subject, thereby generating one or more hyperspectral linescan datasets. The system further includes an optical imaging device with a red-green-blue (RGB) sensor adapted to acquire an RGB image from the region of interest of the subject, thereby generating an RGB dataset. The system further includes a processor which is adapted to co-locate a plurality of pixels in the RGB dataset vs. a corresponding plurality of pixels of the one or more hyperspectral linescan datasets. The processor is further adapted to establish a transformation matrix utilizing the plurality of co-located pixels, the transformation matrix adapted to convert the RGB dataset into a hyperspectral dataset of the region of interest. Additionally, the processor is adapted to apply the transformation matrix to the RGB dataset to thereby generate the hyperspectral dataset for the region of interest. Furthermore, the processor is adapted to analyze the generated hyperspectral image dataset to determine the biochemical compositions.</p><p id="p-0010" num="0009">According to one embodiment, in the system of the present disclosure each of the plurality of co-located pixels from the RGB dataset is associated with a 3&#xd7;1 RGB value matrix.</p><p id="p-0011" num="0010">According to one embodiment, in the system of the present disclosure each of the co-located plurality of pixels from the hyperspectral linescan dataset is associated with an N&#xd7;1 spectrum matrix, where N represents discretized spectra between a lower bound and an upper bound.</p><p id="p-0012" num="0011">According to one embodiment, in the system of the present disclosure the lower and upper bounds are determined by the spectral range of RGB sensors.</p><p id="p-0013" num="0012">According to one embodiment, in the system of the present disclosure the spectral range of sensors are between 400 nm and 800 nm.</p><p id="p-0014" num="0013">According to one embodiment, in the system of the present disclosure the transformation matrix is an inverse of the RGB response function matrix of the RGB sensor.</p><p id="p-0015" num="0014">According to one embodiment, in the system of the present disclosure the inverse of the transformation matrix is determined numerically by using RGB and spectral data from a subset of the collocated plurality of pixels.</p><p id="p-0016" num="0015">According to one embodiment, in the system of the present disclosure the region of interest includes inner eyelid.</p><p id="p-0017" num="0016">According to one embodiment, in the system of the present disclosure the biochemical compositions includes blood hemoglobin.</p><p id="p-0018" num="0017">According to one embodiment, in the system of the present disclosure the biochemical compositions are determined using spectral analysis.</p><p id="p-0019" num="0018">According to one embodiment, in the system of the present disclosure the spectral analysis includes a partial least square regression statistical modeling technique to first build a model from a training set of a first hyperspectral dataset vs. the biochemical compositions and then apply the model to a second dataset from the generated hyperspectral image dataset.</p><p id="p-0020" num="0019">A method for generating hyperspectral imaging data for measuring biochemical compositions is also disclosed. The method includes obtaining one or more hyperspectral linescan images using a spectral imaging device from one or more region of interest of a subject, thereby generating one or more hyperspectral linescan datasets. The method also includes obtaining an RGB image from the region of interest using an optical imaging device with a red-green-blue (RGB) sensor, thereby generating an RGB dataset. The method further includes co-locating a plurality of pixels in the RGB dataset vs. a corresponding plurality of pixels of the one or more hyperspectral linescan datasets. Additionally, the method includes establishing a transformation matrix utilizing the plurality of co-located pixels, the transformation matrix adapted to convert the RGB dataset into a hyperspectral dataset of the region of interest. Furthermore, the method includes applying the transformation matrix to the RGB dataset to thereby generate the hyperspectral dataset for the region of interest. The method also includes analyzing the generated hyperspectral image dataset to determine the biochemical compositions.</p><p id="p-0021" num="0020">According to one embodiment, in the method of the present disclosure each of the plurality of co-located pixels from the RGB dataset is associated with a 3&#xd7;1 RGB value matrix.</p><p id="p-0022" num="0021">According to one embodiment, in the method of the present disclosure each of the co-located plurality of pixels from the hyperspectral linescan dataset is associated with an N&#xd7;1 spectrum matrix, where N represents discretized spectra between a lower bound and an upper bound.</p><p id="p-0023" num="0022">According to one embodiment, in the method of the present disclosure the lower and upper bounds are determined by the spectral range of RGB sensors.</p><p id="p-0024" num="0023">According to one embodiment, in the method of the present disclosure the spectral range of sensors are between 400 nm and 800 nm.</p><p id="p-0025" num="0024">According to one embodiment, in the method of the present disclosure the transformation matrix is an inverse of the RGB response function matrix of the RGB sensor.</p><p id="p-0026" num="0025">According to one embodiment, in the method of the present disclosure the inverse of the transformation matrix is determined numerically by using RGB and spectral data from a subset of the co-located plurality of pixels.</p><p id="p-0027" num="0026">According to one embodiment, in the method of the present disclosure the region of interest includes inner eyelid.</p><p id="p-0028" num="0027">According to one embodiment, in the method of the present disclosure the biochemical compositions are determined using spectral analysis.</p><p id="p-0029" num="0028">According to one embodiment, in the method of the present disclosure the spectral analysis includes a partial least square regression statistical modeling technique to first build a model from a training set of a first hyperspectral dataset vs. the biochemical compositions and then apply the model to a second dataset from the generated hyperspectral image dataset.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>1</b><i>a </i></figref>is a simplified block diagram depicting the major blocks of the system of the present disclosure.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b><i>b </i></figref>a flowchart of steps of an algorithm of the present disclosure is provided.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>1</b><i>c </i></figref>a flowchart of steps of an algorithm of the present disclosure is provided.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>1</b><i>c </i></figref>a flowchart of steps of an algorithm of the present disclosure is provided.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>1</b><i>d </i></figref>a flowchart of steps of an algorithm of the present disclosure is provided.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIGS. <b>2</b><i>a </i>and <b>2</b><i>b </i></figref>provide a photograph of a hyper-spectrographic setup capable of providing a hyperspectral image data for a subarea (or a line) as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref><i>b. </i></p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>2</b><i>c </i></figref>is a photograph of the microvessel-mimicking phantom.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIGS. <b>2</b><i>d </i>and <b>2</b><i>e </i></figref>are hyperspectral linescans are shown outlined in the RGB image of the microvessel phantom of <figref idref="DRAWINGS">FIG. <b>2</b></figref><i>c. </i></p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>2</b><i>f </i></figref>spectra are shown corresponding to the average intensity along the distance outline in <figref idref="DRAWINGS">FIG. <b>2</b><i>e </i></figref>for two different Hgb concentrations.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>2</b><i>g </i></figref>is a photograph of the inner eyelid, with a frame of pixels shown thereon.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>2</b><i>h </i></figref>is a graph of relative sensitivity (%) vs wavelength in (nm) for red, green, and blue spectral responses of a camera.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a graph of normalized intensity vs. wavelength in nm of full width at half maximum (FWHM) of a HeNe laser.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>4</b></figref> are graphs of blood hemoglobin (HGb) in g dL<sup>&#x2212;1 </sup>vs. wavelength in nm showing comparison between a hyperspectral dataset (acquired by the image-guided hyperspectral system) and the algorithm-reconstructed hyperspectral datasets (based on an RGB image) according to the algorithm of the present disclosure for various levels of blood Hgb levels.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>5</b><i>a </i></figref>is a schematic of an inner eyelid showing a first linescan (line <b>1</b>) used for training a model and a second line scan (line <b>2</b>) for testing the model.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIGS. <b>5</b><i>b </i>and <b>5</b><i>c </i></figref>are blood hemoglobin (HGb) in g dL-1 vs. wavelength in nm (<figref idref="DRAWINGS">FIG. <b>5</b><i>b</i></figref>) and intensity vs. wavelength in nm for an obtained hyperspectral dataset vs. a calculated hyperspectral dataset (i.e., line <b>1</b> vs. line <b>2</b> of <figref idref="DRAWINGS">FIG. <b>5</b><i>a</i></figref>) according to the algorithm of the present disclosure.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is histogram which summarizes the blood Hgb values of a total of 153 individuals that were used for spectroscopic and blood Hgb reconstruction measurements using the algorithm of the present disclosure.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a collection of graphs of a linear correlation between the computed blood Hgb content and the laboratory blood Hgb levels and differences in blood hemoglobin in g dL<sup>&#x2212;1 </sup>for one subset of the population of individuals (138) used as training data as well as a second population of individuals (15) used as testing data.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0047" num="0046">The patent or application file contains at least one drawing executed in color. Copies of this patent or patent application publication with color drawing(s) will be provided by the Office upon request and payment of the necessary fee.</p><p id="p-0048" num="0047">For the purposes of promoting an understanding of the principles of the present disclosure, reference will now be made to the embodiments illustrated in the drawings, and specific language will be used to describe the same. It will nevertheless be understood that no limitation of the scope of this disclosure is thereby intended.</p><p id="p-0049" num="0048">In the present disclosure, the term &#x201c;about&#x201d; can allow for a degree of variability in a value or range, for example, within 10%, within 5%, or within 1% of a stated value or of a stated limit of a range.</p><p id="p-0050" num="0049">In the present disclosure, the term &#x201c;substantially&#x201d; can allow for a degree of variability in a value or range, for example, within 90%, within 95%, or within 99% of a stated value or of a stated limit of a range.</p><p id="p-0051" num="0050">For noninvasive blood Hgb measurements, it is important to rely on an appropriate anatomical sensing site where the underlying microvasculature is exposed on the skin surface without being affected by confounding factors of skin pigmentation and light absorption of molecules (e.g. melanin) in tissue. Commonly used clinical examination sites of pallor or microcirculation, such as the conjunctiva, the nailbed, the palm, and the sublingual region, provide a clue for an examination site selection. Specially, the palpebral conjunctiva (i.e. inner eyelid) can serve as an ideal site for peripheral access, because the microvasculature is easily visible and melanocytes are absent. The easy accessibility of the inner eyelid allows for reflectance spectroscopy and digital photography to be tested for anemia assessments.</p><p id="p-0052" num="0051">To this end, the present disclosure advantageously applies techniques typically used in astrophysics to the inside of the eyelid to ascertain hyperspectral imaging data of tissue which can then be used to measure effective blood hemoglobin content. Referring to <figref idref="DRAWINGS">FIGS. <b>1</b><i>a</i>, <b>1</b><i>b</i>, <b>1</b><i>c</i></figref>, and <b>1</b><i>d</i>, a block diagram and flowcharts depicting the system and method of the present disclosure are provided. Referring to <figref idref="DRAWINGS">FIG. <b>1</b><i>a</i></figref>, a simplified block diagram is shown depicting the major blocks of the system <b>10</b> of the present disclosure. The system <b>10</b> includes a hyperspectral imaging system that can associate wavelength from a plurality of pixels. It should be appreciated that hyperspectral imaging of a large area represents complex imaging equipment and complex processes; however, producing a linescan, as discussed below, is significantly more straightforward and simplified. A linescan, however, cannot generate the information on the entire area of interest needed to calculate the target output which is a measure of hemoglobin. Referring to <figref idref="DRAWINGS">FIG. <b>1</b><i>a</i></figref>, the system <b>10</b> includes a linescan imaging apparatus <b>12</b> capable of generating linescans of a subarea of an area of interest, and a color (RGB) imaging apparatus <b>14</b> capable of generating a red-green-blue (RGB) image of the entire area of interest. The output of these two imaging apparatuses are combined by a processing system (not shown) but partially represented as a summer <b>16</b> which produces a matrix of intensity as a function of the position (x, y) and the wavelength of light &#x3bb; (also known as a hypercube or a hyperspectral image) without using a conventional hyperspectral imaging system. Using a hyperspectral dataset and an RGB dataset from the subarea (or a line), a transformation (or extrapolation) algorithm (referred to herein as a virtual hyperspectral imaging (VHI) algorithm depicted across <figref idref="DRAWINGS">FIGS. <b>1</b><i>b</i>, <b>1</b><i>c</i>, and <b>1</b><i>d</i></figref>) is used to construct a hyperspectral image for the larger area of interest, all of which is represented by the summer <b>16</b>. The aforementioned transformation algorithm is then applied to the RGB dataset at pixels outside the subarea (or line) to generate a hyperspectral image dataset for the entire area by the processing system (not shown), as represented by the block <b>18</b>.</p><p id="p-0053" num="0052">Referring to <figref idref="DRAWINGS">FIG. <b>1</b><i>b</i></figref>, the specifics of VHI algorithm is provided, as provided herein. First, a method <b>100</b> is described performed by the processing system (not shown). The Method <b>100</b> begins at the initial block <b>102</b> (identified as &#x201c;Begin&#x201d;). The method <b>100</b> then proceeds to a block <b>104</b> wherein the method <b>100</b> obtains a hyperspectral linescan data from the linescan imaging apparatus <b>12</b> (see <figref idref="DRAWINGS">FIG. <b>1</b><i>a</i></figref>) of a subarea (or line) of interest. The output of the linescan is provided as I(x<sub>sub</sub>, y<sub>sub</sub>, &#x3bb;) for each pixel in the subarea (or line), where &#x3bb; is the wavelength output of the pixel having location x and y. The method <b>100</b> next move to a block <b>106</b> where an RGB image from the area of interest is obtained from the RGB imaging apparatus <b>14</b>. The data obtained includes three image intensity sets: one for Red (R(x<sub>full</sub>, y<sub>full</sub>)), one for Green (G(x<sub>full</sub>, y<sub>full</sub>)), and one for Blue (B(x<sub>full</sub>, y<sub>full</sub>)). Next the method <b>100</b> proceeds to a block <b>108</b> where the image intensity sets of the block <b>106</b> are divided into the subarea (or line) and everywhere else. The subarea image intensity dataset includes: R(x<sub>sub</sub>, y<sub>sub</sub>), G(x<sub>sub</sub>, y<sub>sub</sub>), and B(x<sub>sub</sub>, y<sub>sub</sub>); and R(x<sub>out</sub>, y<sub>out</sub>), G(x<sub>out</sub>, y<sub>out</sub>), and B(x<sub>out</sub>, y<sub>out</sub>), where sub out. Next, the method <b>100</b> proceeds to a block <b>110</b> where the hyperspectral datasets from the block <b>104</b> matched to the RGB image intensity data of the subarea (or line) from the block <b>108</b>. In particular, the method <b>100</b> makes the following correspondences: R(x<sub>sub</sub>, y<sub>sub</sub>)&#x2190;&#x2192;I(x<sub>sub</sub>, y<sub>sub</sub>, &#x3bb;), G(x<sub>sub</sub>, y<sub>sub</sub>)&#x2190;&#x2192;I(x<sub>sub</sub>, y<sub>sub</sub>, &#x3bb;), and B(x<sub>sub</sub>, y<sub>sub</sub>)&#x2190;&#x2192;I(x<sub>sub</sub>, y<sub>sub</sub>, &#x3bb;). The method <b>100</b> then obtains the RGB spectral response functions (also known as RGB spectral sensitivity functions) of the RGB imaging apparatus <b>14</b> (see <figref idref="DRAWINGS">FIG. <b>1</b><i>a</i></figref>), as shown in a block <b>112</b>. The RGB spectral response functions of the RGB imaging apparatus is data that is provided by the manufacturer of the RGB imaging apparatus (sensors and cameras) <b>14</b> (see <figref idref="DRAWINGS">FIG. <b>1</b><i>a</i></figref>). The datasets in the blocks <b>112</b> and <b>110</b> are combined as provided by the summer <b>114</b>, in order to generate a transformation matrix (M) in a block <b>116</b>, according to a procedure provided in <figref idref="DRAWINGS">FIG. <b>1</b><i>d</i></figref>, discussed below. Once the transformation matrix is generated in the block <b>116</b>, the method <b>100</b> applies the transformation matrix to the image intensity data of the area of interest as provided in a block <b>118</b>. This operation is defined by the operation: M&#xb7;R(x<sub>full</sub>, y<sub>full</sub>), M&#xb7;G(x<sub>full</sub>, y<sub>full</sub>), and M&#xb7;B(x<sub>full</sub>, y<sub>full</sub>). The operation in the block <b>118</b> results in the hyperspectral data for the area of interest I(x<sub>full</sub>, y<sub>full</sub>, &#x3bb;) as provided in a block <b>120</b>.</p><p id="p-0054" num="0053">Referring to <figref idref="DRAWINGS">FIG. <b>1</b><i>c</i></figref>, a method <b>130</b> is shown that uses the hyperspectral data of the area of interest (i.e., the output of the method <b>100</b> as provided in the block <b>120</b> (see <figref idref="DRAWINGS">FIG. <b>1</b><i>b</i></figref>)) and applies that output to obtain an estimate of hemoglobin. The method <b>130</b> starts in a block <b>132</b> (identified as &#x201c;Begin&#x201d;). The method then proceeds to a block <b>134</b> where the output of the block <b>120</b> (see <figref idref="DRAWINGS">FIG. <b>1</b><i>b</i></figref>) is obtained. Next an area of the eyelid is isolated (x<sub>eyelid</sub>, y<sub>eyelid</sub>) as provided in a block <b>136</b> and the hyperspectral data from this area is separated from the larger area of interest. This output is shown in a block <b>138</b> as I(x<sub>eyelid</sub>, y<sub>eyelid</sub>, &#x3bb;). Next the method <b>130</b> computes the hemoglobin concentration based on the isolated hyperspectral data output of the block <b>138</b>, as provided in a block <b>140</b>. The method <b>130</b> then proceeds to a block <b>142</b> where the estimate of the hemoglobin concentration (gdL<sup>&#x2212;1</sup>) is provided as the output of the method <b>130</b>.</p><p id="p-0055" num="0054">Referring to <figref idref="DRAWINGS">FIG. <b>1</b><i>c</i></figref>, a method <b>150</b> according to the present disclosure is provided to depict how the transformation matrix (M), see block <b>116</b> of <figref idref="DRAWINGS">FIG. <b>1</b><i>b</i></figref>, is generated. The method <b>150</b> begins at a block <b>152</b> (identified as &#x201c;Begin&#x201d;). The steps of the method <b>150</b> are described below.</p><p id="p-0056" num="0055">Referring to <figref idref="DRAWINGS">FIGS. <b>2</b><i>a </i>and <b>2</b><i>b</i></figref>, a photograph of a hyper-spectrographic setup is shown capable of providing a hyperspectral image data for a subarea (or a line). The system shown in <figref idref="DRAWINGS">FIG. <b>2</b><i>a </i></figref>includes a dual-channel system in which one detection arm is coupled with a hyperspectral linescanning system (see linsescan imaging apparatus <b>12</b> of <figref idref="DRAWINGS">FIG. <b>1</b><i>a</i></figref>) and the other detection arm is coupled with an imaging camera with an RGB imaging sensor (see RGB imaging apparatus of <figref idref="DRAWINGS">FIG. <b>1</b><i>b</i></figref>). The setup shown in <figref idref="DRAWINGS">FIG. <b>2</b><i>a </i></figref>is inspired by an astronomical hyperspectral imaging system, which is used for imaging the inner eyelid. A subject sits in front of the system, facing a telecentric lens (or a lens), places the chin on the chinrest, and pulls down the eyelid for imaging when instructed. The system is adapted to instantaneously acquire a hyperspectral line in the center of the inner eyelid. The RGB image also shows the exact location where the hyperspectral linescanning is performed (shown as a translucent white rectangle with a physical height of about 6.4 mm). The hyperspectral linescan dataset contains spatial (y), which serves as a subarea and wavelength (&#x3bb;) information. When the averaged spectrum corresponds to the average intensity along the spatial y axis for each &#x3bb; value, the characteristic absorption of hemoglobin (Hgb) is clearly visible. As a result, a two-dimensional hyperspectral graph of y vs. &#x3bb; is generated, shown adjacent the photograph of an example inner eyelid, provided in <figref idref="DRAWINGS">FIG. <b>2</b><i>b</i></figref>, for the hyperspectral linescan dataset. This two-dimensional graph can then be represented as a graph of wavelength in nm vs. intensity, also shown in <figref idref="DRAWINGS">FIG. <b>2</b><i>b </i></figref>and aligned with the two-dimensional hyperspectral graph of y vs. &#x3bb;.</p><p id="p-0057" num="0056">Referring back to <figref idref="DRAWINGS">FIG. <b>2</b><i>a</i></figref>, this custom-built dual-channel system shown uses a dual-channel spectrograph (Shelyak Instruments) that has two detection arms to allows for simultaneous acquisitions of hyperspectral and RGB image data along the line. To provide broadband white-light illumination to the inner eyelid, a white-light LED ring (Neopixel RGBW 24 LED ring, Adafruit Industries) is attached to a telecentric lens (0.5&#xd7;, Edmund Optics) via a custom-built 3-D printed ring holder to fit the lens circumference. Other lenses can be used, however, a telecentric lens offers the ability to eliminate parallax error and to provide constant magnification. Telecentric imaging is also beneficial for biological tissue, including resolution enhancement by diffuse light suppression, large constant transverse field of view, consistent magnification over the axial direction, and long working distance. The intensity of the LED ring is controlled with a microcontroller (Arduino UNO). The image-guided hyperspectral linescanning system has two data acquisition ports: hyperspectral line-scanning port mounted with a mono CCD camera (e.g., PointGrey Grasshopper3 5.0 MP Mono, FUR Integrated Imaging Solutions Inc.) and image port mounted with a 3-color CCD (e.g., PointGrey Grasshopper3 5.0 MP Color). For hyperspectral linescanning (length=6.4 mm), the telecentric lens collects light scattered from the inner eyelid, which passes through the slit of the spectrograph and disperses with a diffraction grating. The diffraction grating in the dual-channel spectrograph was selected to cover the visible wavelength range of 400-700 nm and the slitwidth is 23 &#x3bc;m inside the system, resulting in a spectral resolution of &#x394;&#x3bb;=1 nm as shown in <figref idref="DRAWINGS">FIG. <b>2</b><i>b</i></figref>. For RGB imaging, the light from the inner eyelid is reflected via another mirror toward the imaging port to generate a field of view (14 mm&#xd7;12 mm) with a spatial resolution of &#x2dc;150 &#x3bc;m. It should be noted that by changing the imaging lens, the field of view can easily be increased for other applications. The custom-built dual-channel system rests on a base with two interlocked x-y-z positioning bases that serve to move the imaging system to locate the eyelid image centered within the rectangular region of interest (ROI), also referred to herein as the area of interest, guide. A LabVIEW Virtual Instrument (National Instruments Corporation) was generated to synchronize data acquisition, LED light control, and background room light subtraction. A series of tests were conducted using tissue-mimicking phantoms, see <figref idref="DRAWINGS">FIGS. <b>2</b><i>c</i>, <b>2</b><i>d</i>, <b>2</b><i>e</i>, and <b>2</b><i>f</i></figref>. In <figref idref="DRAWINGS">FIG. <b>2</b><i>c </i></figref>a photograph of the microvessel-mimicking phantom is presented. Hgb-filled tubings are fixed at the bottom of the glass petri dish and are submerged in the optical scattering suspension. In <figref idref="DRAWINGS">FIGS. <b>2</b><i>d </i>and <b>2</b><i>e</i></figref>, a hyperspectral linescan is outlined in the RGB image of the microvessel phantom. The microvessels are positioned perpendicular to the linescan. In <figref idref="DRAWINGS">FIG. <b>2</b><i>f</i></figref>, each spectrum shown corresponds to the average intensity along the distance outline in <figref idref="DRAWINGS">FIG. <b>2</b><i>e</i></figref>. The microvessel with a higher Hgb concentration (5.0 g dL-1) has a lower reflection intensity for the wavelengths between 450 and 550 nm than the lower Hgb concentration (3.0 g dL-1). During eyelid imaging, the subject is asked to sit down facing the imaging system and to place their head in a chinrest. Once the eyelid is correctly focused and positioned within the ROI rectangle, we proceed with data acquisition, while reminding the individual the instructions not to move or close their eyes until the completion of imaging session. Measurements of a reference reflectance standard (SRT-99-050, Labsphere, Inc.) are also conducted with the hyperspectral line-scanning system to correct for the system response (both illumination and detection).</p><p id="p-0058" num="0057">As discussed above, a single hyperspectral linescan dataset, however, does not have sufficient information to reliably use for hemoglobin data extraction from the entire inner eyelid. Therefore, what is needed is additional hyperspectral data for the entire inner eyelid that can be used for averaging and other statistical operations. Such additional data can be formed from additional linescans or based on extrapolation of one or more linescans, see <figref idref="DRAWINGS">FIGS. <b>1</b><i>a</i>, <b>1</b><i>b</i>, and <b>1</b><i>c</i></figref>. According to one embodiment of the present disclosure, a sufficient number of hyperspectral linescan datasets can be progressively scanned and generated and then stitched together to form an ensemble of a portion of the inner eyelid. This is a typical approach of conventional hyperspectral imaging systems. Such a process is cumbersome and very slow for hyperspectral data acquisition, since it will require accounting for slight movement of the subject during each linescan capture. Alternatively, the linescan dataset can be used as a baseline and the same extrapolated using an RGB image whereby the RGB image is converted into a hyperspectral graph based on the single or multiple hyperspectral linescan datasets, as discussed above and with reference to the present disclosure. For example, in the latter approach, one or more hyperspectral linescan datasets or hyperspectral data for the entire area can be generated. In other words, the hyperspectral data and the RGB image from the line is used to construct a transformation matrix (M, see block <b>116</b> in <figref idref="DRAWINGS">FIG. <b>1</b><i>b</i></figref>) that mathematically predicts a hyperspectrum from the RGB data at a pixel location outside the line. One hyperspectral linescan dataset and its corresponding RGB dataset are sufficient to construct the transformation matrix. By applying this transformation matrix to all of the pixel locations outside the line, a hyperspectral imaging dataset is generated for the entire area (see block <b>120</b> in <figref idref="DRAWINGS">FIG. <b>1</b><i>b</i></figref>). This extrapolation approach is referred to herein as the virtual hyperspectral imaging (VHI) algorithm/system. The VHI approach advantageously requires only one raw hyperspectral linescan dataset and an RGB image at the minimum, preferably produced at the same time to avoid subject movement, simplifying the imaging requirements, significantly. The custom-built dual channel system shown in <figref idref="DRAWINGS">FIG. <b>2</b><i>a</i></figref>, allows simultaneous acquisitions of hyperspectral line-scanning and RGB imaging. It should be noted that the hyperspectral image(s) and the RGB image can be produced at different times in close proximity to one another, as long as any variations due to movement of the eyelid is considered. With the VHI approach, a hyperspectral dataset for the entire eyelid can be generated in order to form a more accurate correlation to hemoglobin with only one or more hyperspectral linescan datasets and an RGB image.</p><p id="p-0059" num="0058">The spectroscopic and VHI blood Hgb measurements systems and methods of the present disclosure are not affected by variations in the illumination and detection of the imaging systems as well as the background ambient room light as follows: The measured spectral intensity I<sub>m</sub>(&#x3bb;) reflected from the inner eyelid in a given location of (x, y) is expressed as a function of the wavelength &#x3bb;:</p><p id="p-0060" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>I</i><sub>m</sub>(&#x3bb;)=<i>L</i>(&#x3bb;)<i>C</i>(&#x3bb;)<i>D</i>(&#x3bb;)<i>r</i>(&#x3bb;)&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0061" num="0000">where L (&#x3bb;) is the spectral shape of the illumination light source,<br/>C(&#x3bb;) is the spectral response of all optical components in the imaging system (e.g. lenses and diffraction grating),<br/>D(&#x3bb;) is the spectral response of the detector (e.g. mono imaging sensor or RGB imaging sensor in the image-guided hyperspectral linescanning system), and<br/>r(&#x3bb;) is the true spectral intensity reflected from the inner eyelid. First, to compensate for the system response (i.e. L(&#x3bb;)C(&#x3bb;)D(&#x3bb;)), we use the reference reflectance standards that have a reflectivity of 99% in the visible range. I<sub>m</sub>(&#x3bb;) is normalized by the reflectance measurement I<sub>reference</sub>(&#x3bb;) of the diffuse reflectance standard in which r<sub>reference</sub>(&#x3bb;)=0.99 in the visible range</p><p id="p-0062" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>r</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mi>&#x3bb;</mi>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mfrac>      <mrow>       <msub>        <mi>I</mi>        <mi>m</mi>       </msub>       <mo>(</mo>       <mi>&#x3bb;</mi>       <mo>)</mo>      </mrow>      <mrow>       <msub>        <mi>I</mi>        <mi>reference</mi>       </msub>       <mo>(</mo>       <mi>&#x3bb;</mi>       <mo>)</mo>      </mrow>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0063" num="0000">Second, to remove the ambient stray and background light I<sub>background</sub>(&#x3bb;), two measurements are acquired with the external light source (i.e., white-light LED ring illuminator of the custom-built dual imaging system) on and off. The measurements are repeated without the sample while the illumination is kept on. Finally, r(&#x3bb;) is calculated by subtracting I<sub>background</sub>(&#x3bb;) from each measurement such that:</p><p id="p-0064" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>r</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mi>&#x3bb;</mi>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mfrac>      <mrow>       <mrow>        <msub>         <mi>I</mi>         <mi>m</mi>        </msub>        <mo>(</mo>        <mi>&#x3bb;</mi>        <mo>)</mo>       </mrow>       <mo>-</mo>       <mrow>        <msub>         <mi>I</mi>         <mi>backgroud</mi>        </msub>        <mo>(</mo>        <mi>&#x3bb;</mi>        <mo>)</mo>       </mrow>      </mrow>      <mrow>       <mrow>        <msub>         <mi>I</mi>         <mi>reference</mi>        </msub>        <mo>(</mo>        <mi>&#x3bb;</mi>        <mo>)</mo>       </mrow>       <mo>-</mo>       <mrow>        <msub>         <mi>I</mi>         <mi>backgroud</mi>        </msub>        <mo>(</mo>        <mi>&#x3bb;</mi>        <mo>)</mo>       </mrow>      </mrow>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0065" num="0000">This systematic and rigorous data acquisition procedure serves as the foundation for developing a reliable VHI transformation matrix and a universal blood Hgb computation algorithm. It should be noted that the built-in data acquisition step to factor out the contributions of room light conditions provides a unique advantage to generate this reliable blood Hgb calculation.</p><p id="p-0066" num="0059">To better understand this approach, reference is made to <figref idref="DRAWINGS">FIG. <b>2</b><i>g </i></figref>which is a photograph of the inner eyelid, with a frame of pixels shown thereon. The frame is an RGB 2-dimensional frame and includes pixels in the X-direction and the Y-direction. The corner pixel is shown as P<sub>1</sub>. The first row of pixels includes pixels P<sub>11</sub>, P<sub>12</sub>, P<sub>13</sub>, P<sub>14</sub>, . . . P<sub>1l</sub>, . . . and P<sub>1q</sub>. The second row includes pixels P<sub>21</sub>, P<sub>22</sub>, P<sub>23</sub>, P<sub>24</sub>, . . . P<sub>2l</sub>, . . . and P<sub>2q</sub>, until the last row which includes pixels P<sub>m1</sub>, P<sub>m2</sub>, P<sub>m3</sub>, P<sub>m4</sub>, . . . P<sub>ml</sub>, . . . and P<sub>mq</sub>. The first column of pixels includes pixels P<sub>11</sub>, P<sub>21</sub>, P<sub>31</sub>, P<sub>41</sub>, . . . P<sub>m1</sub>. The second column includes pixels P<sub>12</sub>, P<sub>22</sub>, P<sub>32</sub>, P<sub>42</sub>, . . . P<sub>m2</sub>. The last column includes pixels P<sub>1q</sub>, P<sub>2q</sub>, P<sub>3q</sub>, P<sub>4q</sub>, . . . P<sub>mq</sub>. The l<sup>th </sup>column which happens to be the column which is coincident with the column with the hyperspectral data (linescan) includes pixels P<sub>11</sub>, P<sub>21</sub>, P<sub>31</sub>, P<sub>41</sub>, . . . P<sub>m1 </sub>(note the second index in each pixel is lowercase L (i.e., &#x201c;1&#x201d; and not one (&#x201c;1&#x201d;)). This column provides the aforementioned limited hyperspectral data. Concentrating on the l<sup>th </sup>column, each pixel (i.e., P<sub>11</sub>, P<sub>21</sub>, P<sub>31</sub>, P<sub>41</sub>, . . . P<sub>m1</sub>) having an RGB intensity can be paired to the corresponding hyperspectral pixel with a corresponding wavelength data (i.e., &#x3bb;<sub>1</sub>, &#x3bb;<sub>2</sub>, &#x3bb;<sub>3</sub>, &#x3bb;<sub>4</sub>, . . . &#x3bb;<sub>m</sub>) where &#x3bb; represents a discretized wavelength between a first wavelength (e.g., 450 nm) and a second wavelength (e.g., 679 nm). That is each hyperspectral pixel is represented by a spectrum bounded between the lower and upper bounds. For a discretized spectrum, the number of wavelengths is identified as N. Therefore, for pixel P<sub>11</sub>, one can correlate the RGB intensity of the pixel P<sub>11 </sub>to the spectrum obtained from the hyperspectral imaging of the same pixel (obtained at preferably the same time with different cameras). A transformation matrix can be derived from this correlation that can then be applied to other pixels and their associated RGB intensities in order to derive corresponding spectra of those other pixels.</p><p id="p-0067" num="0060">In the case of VHI, a mathematical reconstruction of the full spectral information from an RGB image taken by a conventional camera (i.e. three-color information from Red, Green, and Blue channels) is generated, according to the present disclosure. Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the spectral resolution (&#x394;&#x3bb;=1 nm) of the system by measuring full width at half maximum (FWHM) of a HeNe laser is provided. The mathematical relationship between the full spectrum and the RGB intensity is described as</p><p id="p-0068" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>x=Sr+e</i>&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0069" num="0000">where x is a vector corresponding to the reflection intensity in each R, G, and B channel,<br/>S is a matrix of the RGB spectral response functions of the three-color sensor,<br/>r is a vector of the spectral intensity reflected from the inner eyelid, and<br/>e is a vector of the system noise. In our case, the hyperspectral reconstruction from the RGB signal is an inverse problem such that the number of actual measurements (i.e. three-color information) is less than the dimensionality of the full spectrum with &#x3bb;=&#x3bb;<sub>1</sub>, &#x3bb;<sub>2</sub>, . . . , &#x3bb;<sub>N</sub>. Given the relatively limited sample size, we took advantage of fixed-design linear regression with polynomial features to reliably reconstruct the full spectral information r(&#x3bb;<sub>1</sub>, &#x3bb;<sub>2</sub>, . . . , &#x3bb;<sub>N</sub>) from the RGB signals x(R, G, B) of the three-color RGB sensor, as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>; wherein a comparison between the original hyperspectral dataset (acquired by the image-guided hyperspectral system) and the VHI-reconstructed hyperspectral datasets (based on an RGB image) are provided for various levels of blood Hgb levels. The differences in the wavelength range between 450 and 575 nm are generally higher, because the distinct Hgb absorption is present in this range. To better demonstrate the construction of the transformation matrix, reference is made to <figref idref="DRAWINGS">FIG. <b>1</b><i>d</i></figref>. First, the method <b>150</b> describes the measured RGB intensity as provided in a block <b>154</b>:</p><p id="p-0070" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>x</i><sub>3&#xd7;1</sub><i>=S</i><sub>3xN</sub><i>r</i><sub>N&#xd7;1</sub><i>+e</i><sub>3&#xd7;1</sub>&#x2003;&#x2003;(5-1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0071" num="0000">where x is a 3&#xd7;1 vector corresponding to the reflection intensity in each R, G, and B channel (e.g., pixel P<sub>11</sub>, is identified by x<sub>3&#xd7;1 </sub>which is a 3&#xd7;1 matrix with each row associated with an RGB channel output, i.e., the first row represents the R value, the second row represents the G value, and the third row represents the B value),<br/>S is a 3&#xd7;N matrix of the RGB spectral response functions of the 3-color sensor, i.e. built-in camera (S represent the discretized versions of the spectra for each RGB channel, as shown in <figref idref="DRAWINGS">FIG. <b>2</b><i>h</i></figref>, in the form of a matrix, i.e., the first row of S is the spectrum for relative intensity of the R channel output of the sensor over discretized range bounded between a lower and upper range, the second row of S is the spectrum for relative intensity of the G channel output of the sensor over discretized range bounded between the lower and upper range, the third row of S is the spectrum for relative intensity of the B channel output of the sensor over discretized range bounded between the lower and upper range),<br/>r is an N&#xd7;1 vector that has the spectral reflection intensity (that is r is the spectrum over discretized range bounded between the lower and upper range of the pixel from the hyperspectral image)&#x2014;in our case, r(&#x3bb;=&#x3bb;<sub>1</sub>, &#x3bb;<sub>2</sub>, . . . , &#x3bb;<sub>N</sub>) is discretized from 450 nm to 679 nm with a spectral interval of 1 nm, and<br/>e is a 3&#xd7;1 vector of the system noise with zero mean. The hyperspectral reconstruction from the RGB signal is to obtain [S<sub>3&#xd7;N</sub>]<sup>&#x2212;1</sup>. However, this inverse calculation is an underdetermined problem since N&#x3e;3.</p><p id="p-0072" num="0061">To solve this underdetermined problem, we formulate fixed-design linear regression with polynomial features of the three-color information to infer the spectral information r from the RGB signals x. We take advantage of multiple collections of the hyperspectral reflection dataset (acquired by the image-guided hyperspectral line-scanning system) and the RGB dataset (acquired by the RGB camera), respectively. X<sub>3&#xd7;m </sub>and R<sub>N&#xd7;m </sub>are formed by adding x<sub>3&#xd7;1 </sub>and r<sub>N&#xd7;1 </sub>from m different measurements. Referring to <figref idref="DRAWINGS">FIG. <b>2</b><i>g</i></figref>, while the aforementioned underdetermined problem as initially described with respect to pixel P<sub>11</sub>, there are m pixels in the l<sup>th </sup>column of the frame. Therefore, other pixels in equation (5-1) can be used to numerically solve for [S<sub>3&#xd7;N</sub>]<sup>&#x2212;1 </sup>in an alternative manner, as provided in a block <b>156</b> of <figref idref="DRAWINGS">FIG. <b>1</b><i>d</i></figref>. The relationship in Equation (5-1) is described as:</p><p id="p-0073" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i><sub>3&#xd7;m</sub><i>=S</i><sub>3&#xd7;N</sub><i>R</i><sub>N&#xd7;m</sub>&#x2003;&#x2003;(5-2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0074" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>which can be expressed as:<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0075" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>R</i><sub>N&#xd7;m</sub><i>=T</i><sub>N&#xd7;3</sub><i>X</i><sub>3&#xd7;m</sub>&#x2003;&#x2003;(5-3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0076" num="0000">where the transformation (or extrapolation) matrix is T<sub>N&#xd7;3</sub>=[S<sub>3&#xd7;N</sub>]<sup>&#x2212;1</sup>, as provided in a block <b>158</b> in <figref idref="DRAWINGS">FIG. <b>1</b><i>d</i></figref>. If Equation (5-3) is solved for the unknown T<sub>N&#xd7;3</sub>, then T<sub>N&#xd7;3 </sub>can be used to convert the RGB dataset into the hyperspectral reflection dataset, as provided in a block <b>160</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref><i>d. </i></p><p id="p-0077" num="0062">Next, each three-color sensor model in different cameras has unique RGB spectral responses with spectral overlaps among the R, G, and B channels (also known as the sensitivity function of the camera), as discussed above with reference to <figref idref="DRAWINGS">FIG. <b>2</b><i>h</i></figref>. To effectively incorporate the RGB spectral response of the camera, we expand X<sub>3&#xd7;m </sub>to {circumflex over (X)}<sub>p&#xd7;m </sub>for maximizing the accuracy of the hyperspectral reconstruction such that:</p><p id="p-0078" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>R</i><sub>N&#xd7;m</sub><i>={circumflex over (T)}</i><sub>N&#xd7;p</sub><i>{circumflex over (X)}</i><sub>p&#xd7;m</sub>&#x2003;&#x2003;(5-4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0079" num="0000">here {circumflex over (X)}<sub>p&#xd7;m </sub>can be expressed explicitly such that:</p><p id="p-0080" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mover>       <mi>X</mi>       <mo>^</mo>      </mover>      <mrow>       <mi>p</mi>       <mo>&#xd7;</mo>       <mi>m</mi>      </mrow>     </msub>     <mo>=</mo>     <msup>      <mrow>       <mo>[</mo>       <mtable>        <mtr>         <mtd>          <msub>           <mi>R</mi>           <mn>1</mn>          </msub>         </mtd>         <mtd>          <msub>           <mi>G</mi>           <mn>1</mn>          </msub>         </mtd>         <mtd>          <msub>           <mi>B</mi>           <mn>1</mn>          </msub>         </mtd>         <mtd>          <mo>&#x2026;</mo>         </mtd>         <mtd>          <msubsup>           <mi>R</mi>           <mn>1</mn>           <mi>i</mi>          </msubsup>         </mtd>         <mtd>          <msubsup>           <mi>G</mi>           <mn>1</mn>           <mi>i</mi>          </msubsup>         </mtd>         <mtd>          <msubsup>           <mi>B</mi>           <mn>1</mn>           <mi>i</mi>          </msubsup>         </mtd>         <mtd>          <mrow>           <msub>            <mi>R</mi>            <mn>1</mn>           </msub>           <mo>&#x2062;</mo>           <msub>            <mi>G</mi>            <mn>1</mn>           </msub>          </mrow>         </mtd>         <mtd>          <mrow>           <msub>            <mi>G</mi>            <mn>1</mn>           </msub>           <mo>&#x2062;</mo>           <msub>            <mi>B</mi>            <mn>1</mn>           </msub>          </mrow>         </mtd>         <mtd>          <mrow>           <msub>            <mi>B</mi>            <mn>1</mn>           </msub>           <mo>&#x2062;</mo>           <msub>            <mi>R</mi>            <mn>1</mn>           </msub>          </mrow>         </mtd>         <mtd>          <mo>&#x2026;</mo>         </mtd>         <mtd>          <msup>           <mrow>            <mo>(</mo>            <mrow>             <msub>              <mi>R</mi>              <mn>1</mn>             </msub>             <mo>&#x2062;</mo>             <msub>              <mi>G</mi>              <mn>1</mn>             </msub>            </mrow>            <mo>)</mo>           </mrow>           <mi>j</mi>          </msup>         </mtd>         <mtd>          <msup>           <mrow>            <mo>(</mo>            <mrow>             <msub>              <mi>G</mi>              <mn>1</mn>             </msub>             <mo>&#x2062;</mo>             <msub>              <mi>B</mi>              <mn>1</mn>             </msub>            </mrow>            <mo>)</mo>           </mrow>           <mi>j</mi>          </msup>         </mtd>         <mtd>          <msup>           <mrow>            <mo>(</mo>            <mrow>             <msub>              <mi>B</mi>              <mn>1</mn>             </msub>             <mo>&#x2062;</mo>             <msub>              <mi>R</mi>              <mn>1</mn>             </msub>            </mrow>            <mo>)</mo>           </mrow>           <mi>j</mi>          </msup>         </mtd>         <mtd>          <mrow>           <msub>            <mi>R</mi>            <mn>1</mn>           </msub>           <mo>&#x2062;</mo>           <msub>            <mi>G</mi>            <mn>1</mn>           </msub>           <mo>&#x2062;</mo>           <msub>            <mi>B</mi>            <mn>1</mn>           </msub>          </mrow>         </mtd>         <mtd>          <mo>&#x2026;</mo>         </mtd>         <mtd>          <msup>           <mrow>            <mo>(</mo>            <mrow>             <msub>              <mi>R</mi>              <mn>1</mn>             </msub>             <mo>&#x2062;</mo>             <msub>              <mi>G</mi>              <mn>1</mn>             </msub>             <mo>&#x2062;</mo>             <msub>              <mi>B</mi>              <mn>1</mn>             </msub>            </mrow>            <mo>)</mo>           </mrow>           <mi>j</mi>          </msup>         </mtd>        </mtr>        <mtr>         <mtd>          <mo>&#x22ee;</mo>         </mtd>         <mtd>          <mo>&#x22ee;</mo>         </mtd>         <mtd>          <mo>&#x22ee;</mo>         </mtd>         <mtd>          <mo>&#x22ee;</mo>         </mtd>         <mtd>          <mo>&#x22ee;</mo>         </mtd>         <mtd>          <mo>&#x22ee;</mo>         </mtd>         <mtd>          <mo>&#x22ee;</mo>         </mtd>         <mtd>          <mo>&#x22ee;</mo>         </mtd>         <mtd>          <mo>&#x22ee;</mo>         </mtd>         <mtd>          <mo>&#x22ee;</mo>         </mtd>         <mtd>          <mo>&#x22ee;</mo>         </mtd>         <mtd>          <mo>&#x22ee;</mo>         </mtd>         <mtd>          <mo>&#x22ee;</mo>         </mtd>         <mtd>          <mo>&#x22ee;</mo>         </mtd>         <mtd>          <mo>&#x22ee;</mo>         </mtd>         <mtd>          <mo>&#x22ee;</mo>         </mtd>         <mtd>          <mo>&#x22ee;</mo>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>R</mi>           <mi>m</mi>          </msub>         </mtd>         <mtd>          <msub>           <mi>G</mi>           <mi>m</mi>          </msub>         </mtd>         <mtd>          <msub>           <mi>B</mi>           <mi>m</mi>          </msub>         </mtd>         <mtd>          <mo>&#x2026;</mo>         </mtd>         <mtd>          <msubsup>           <mi>R</mi>           <mi>m</mi>           <mi>i</mi>          </msubsup>         </mtd>         <mtd>          <msubsup>           <mi>G</mi>           <mi>m</mi>           <mi>i</mi>          </msubsup>         </mtd>         <mtd>          <msubsup>           <mi>B</mi>           <mi>m</mi>           <mi>i</mi>          </msubsup>         </mtd>         <mtd>          <mrow>           <msub>            <mi>R</mi>            <mi>m</mi>           </msub>           <mo>&#x2062;</mo>           <msub>            <mi>G</mi>            <mi>m</mi>           </msub>          </mrow>         </mtd>         <mtd>          <mrow>           <msub>            <mi>G</mi>            <mi>m</mi>           </msub>           <mo>&#x2062;</mo>           <msub>            <mi>B</mi>            <mi>m</mi>           </msub>          </mrow>         </mtd>         <mtd>          <mrow>           <msub>            <mi>B</mi>            <mi>m</mi>           </msub>           <mo>&#x2062;</mo>           <msub>            <mi>R</mi>            <mi>m</mi>           </msub>          </mrow>         </mtd>         <mtd>          <mo>&#x2026;</mo>         </mtd>         <mtd>          <msup>           <mrow>            <mo>(</mo>            <mrow>             <msub>              <mi>R</mi>              <mi>m</mi>             </msub>             <mo>&#x2062;</mo>             <msub>              <mi>G</mi>              <mi>m</mi>             </msub>            </mrow>            <mo>)</mo>           </mrow>           <mi>j</mi>          </msup>         </mtd>         <mtd>          <msup>           <mrow>            <mo>(</mo>            <mrow>             <msub>              <mi>G</mi>              <mi>m</mi>             </msub>             <mo>&#x2062;</mo>             <msub>              <mi>B</mi>              <mi>m</mi>             </msub>            </mrow>            <mo>)</mo>           </mrow>           <mi>r</mi>          </msup>         </mtd>         <mtd>          <msup>           <mrow>            <mo>(</mo>            <mrow>             <msub>              <mi>B</mi>              <mi>m</mi>             </msub>             <mo>&#x2062;</mo>             <msub>              <mi>R</mi>              <mi>m</mi>             </msub>            </mrow>            <mo>)</mo>           </mrow>           <mi>j</mi>          </msup>         </mtd>         <mtd>          <mrow>           <msub>            <mi>R</mi>            <mi>m</mi>           </msub>           <mo>&#x2062;</mo>           <msub>            <mi>G</mi>            <mi>m</mi>           </msub>           <mo>&#x2062;</mo>           <msub>            <mi>B</mi>            <mi>m</mi>           </msub>          </mrow>         </mtd>         <mtd>          <mo>&#x2026;</mo>         </mtd>         <mtd>          <msup>           <mrow>            <mo>(</mo>            <mrow>             <msub>              <mi>R</mi>              <mi>m</mi>             </msub>             <mo>&#x2062;</mo>             <msub>              <mi>G</mi>              <mi>m</mi>             </msub>             <mo>&#x2062;</mo>             <msub>              <mi>B</mi>              <mi>m</mi>             </msub>            </mrow>            <mo>)</mo>           </mrow>           <mi>j</mi>          </msup>         </mtd>        </mtr>       </mtable>       <mo>]</mo>      </mrow>      <mi>T</mi>     </msup>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mn>5</mn>      <mo>-</mo>      <mn>5</mn>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0081" num="0000">where the exact powers of i and j of the single and cross terms are uniquely determined for a specific three-color sensor model, by checking the error between the reconstructed hyperspectral data and the original data.</p><p id="p-0082" num="0063">Next, the inverse of the expanded transformation matrix {circumflex over (T)} in Equation (5-4) can be considered to be the minimum-norm-residual solution to R=TX. Typically, this inverse problem is to solve a least-squares problem. We take use of QR decomposition, in particular the QR solver. After QR factorization is applied to {circumflex over (X)}, {circumflex over (T)} is estimated by minimizing the sum of the squares of the elements of R-{circumflex over (T)}{circumflex over (X)} and is selected such that the number of nonzero entries in T is minimized. Overall, the computation of the transformation (extrapolation) matrix establishes VHI, eliminating a need of bulky dispersion hardware components (e.g. spectrometer, spectrograph, mechanical filter wheel, or liquid crystal tunable filter).</p><p id="p-0083" num="0064">We validated the performance of the RGB-assisted VHI as shown <figref idref="DRAWINGS">FIGS. <b>5</b><i>a</i>, <b>5</b><i>b</i>, and <b>5</b><i>c</i></figref>. The line #<b>1</b> (shown in <figref idref="DRAWINGS">FIG. <b>5</b><i>a</i></figref>) was used to construct the transformation matrix that generates hyperspectral data from RGB data. The transformation matrix was applied to the RGB data of Line #<b>2</b> to generate a hyperspectral linescan of Line #<b>2</b> (see <figref idref="DRAWINGS">FIGS. <b>5</b><i>a</i>, <b>5</b><i>b</i>, and <b>5</b><i>c</i></figref>). This extrapolated data is compared with a measured hyperspectral linescan of Line #<b>2</b> by moving the systems. The reconstructed data is in excellent agreement with the original data. When the hyperspectral data were averaged over the line scanning direction into a spectrum, the reconstructed spectrum is also in excellent agreement with the original one.</p><p id="p-0084" num="0065">In order to make comparison with clinical data, reference is made to <figref idref="DRAWINGS">FIG. <b>6</b></figref> which summarizes the blood Hgb values of a total of 153 individuals that were used for spectroscopic and VHI blood Hgb measurements. The study covers a wide range of Hgb values from 3.3 to 19.2 g dL<sup>&#x2212;1</sup>. We conducted a clinical study within the facilities overseen by the accepted authorities. We enrolled patients who were referred for complete blood count (CBC) tests. For all individuals enrolled in the study, we collected hyperspectral data and RGB images from the palpebral conjunctiva (i.e. inner eyelid) using an image-guided hyperspectral line-scanning system. As the &#x2018;gold standard&#x2019; clinical laboratory measurements, blood Hgb levels were measured in an Accredited Clinical Laboratory using a commercial hematology analyzer (BECKMAN COULTER AcT 5diff auto, BECKMAN COULTER, INC.). For developing the blood Hgb quantification algorithm, we randomly selected 138 individuals (78 females and 60 males) to use as a preliminary (training) dataset. The average Hgb level is 12.65 g dL<sup>&#x2212;1 </sup>with a standard deviation (SD) of 3.11 g dL<sup>&#x2212;1 </sup>and the average age is 37.78 years with SD of 16.38 years. As a new masked (testing) dataset, we employed the rest of 15 individuals (12 females and 3 males) not included in the preliminary dataset. The average Hgb level is 11.06 g dL<sup>&#x2212;1 </sup>with SD of 3.62 g dL<sup>&#x2212;1</sup>. The average age is 39.13 years with SD of 17.30 years.</p><p id="p-0085" num="0066">We now describe the partial least square regression (PLSR) to estimate a blood Hgb level from the hyperspectral information averaged from the entire inner eyelid. We built a model for computing blood Hgb content from the hyperspectral reflection data averaged over the inner eyelid. Analytical model-based Hgb prediction methods are often used, because Hgb has distinct spectral signatures (e.g. Soret and Q bands) in the visible range. However, such model-based approaches often require a priori information on all possible light absorbers in tissue for reliable Hgb quantification. Thus, we made use of PLSR, which can be used to model relationships among measured variables (i.e. predictors) and response variables (i.e. outcomes). Because PLSR transforms high-dimensional measured variables onto a reduced space of latent variables, it is highly beneficial to examine the significance of individual measured variables by eliminating insignificant variables. While PLSR is based on the extraction of principal components, it incorporates variations of both predictor and outcome variables simultaneously, enhancing the prediction performance. Similar to principal component analysis, it is critical to determine an optimal number of components in PLSR, as a greater number of components better captures variations in the predictor and outcome variables, thus lowering the prediction error. The determination of an optimal number of principal components in ten-fold cross-validation of partial least squares regression (PLSR) is thus performed. In particular, as the number of partial least squares (PLS) components increases, the percentage variance in the true Hgb values (outcome variable) increases, while the mean squared prediction error has minimal values for 18 components. These numbers of PLS components contribute to appropriate representation of variations in the spectroscopic and laboratory blood Hgb values simultaneously, thus making its prediction errors lower. As a result, 18 PLS components are selected and used in the Hgb prediction model. We can select an optimal number of components using cross-validation in a conservative manner as follows: The original dataset was randomly grouped into sub-datasets with the same sample size. One sub-dataset was not used for training the model and was retained as a validation dataset for testing the model. After this process was repeated, the different validations were averaged. The main advantage of such a cross-validation is that all of the datasets were incorporated to determine the optimal number of principal components, given the limited number of individuals. Although the use of PLSR often avoids overfitting when the number of predictors is larger than the sample size, it is also important to evaluate the ability for predicting Hgb levels from a completely new dataset after the model is established properly. Thus, we defined the two datasets for training and testing the blood Hgb model without reutilization of data from the same individuals.</p><p id="p-0086" num="0067">Based on the aforementioned information, a hyperspectral/imaging data processing and statistical analysis is now provided. For data processing and algorithm development, we computed the hyperspectral and RGB data and developed the blood Hgb prediction model and the VHI algorithm using MATLAB (MATLAB R2018b, The MathWorks, Inc.). For statistical analyses, we evaluated multiple linear regression, linear correlations, and intra-class correlations using STATA (STATA 14.2, STATACORP LLC). We conducted Bland-Altman analyses to compare the blood Hgb measurements as non-parametric methods. The bias is defined by the mean of the differences between the hyperspectral (or VHI) and central laboratory blood Hgb measurements (d=y<sup>VHI</sup>&#x2212;y<sup>central</sup>):</p><p id="p-0087" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>Bias</mi>     <mo>=</mo>     <mrow>      <mover>       <mi>d</mi>       <mo>_</mo>      </mover>      <mo>=</mo>      <mrow>       <mfrac>        <mn>1</mn>        <mi>n</mi>       </mfrac>       <mo>&#x2062;</mo>       <mrow>        <msubsup>         <mo>&#x2211;</mo>         <mrow>          <mi>k</mi>          <mo>=</mo>          <mn>1</mn>         </mrow>         <mi>n</mi>        </msubsup>        <mrow>         <msub>          <mi>d</mi>          <mi>k</mi>         </msub>         <mo>.</mo>        </mrow>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>6</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0088" num="0000">The 95% limits of agreement (LOA) is defined by a 95% prediction interval of the standard deviation:</p><p id="p-0089" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>L</mi>      <mo>&#x2062;</mo>      <mi>O</mi>      <mo>&#x2062;</mo>      <mi>A</mi>     </mrow>     <mo>=</mo>     <mrow>      <mover>       <mi>d</mi>       <mo>_</mo>      </mover>      <mo>&#xb1;</mo>      <mrow>       <mn>1.96</mn>       <mrow>        <msqrt>         <mrow>          <mrow>           <mfrac>            <mn>1</mn>            <mrow>             <mi>n</mi>             <mo>-</mo>             <mn>1</mn>            </mrow>           </mfrac>           <msubsup>            <mo>&#x2211;</mo>            <mrow>             <mi>k</mi>             <mo>=</mo>             <mn>1</mn>            </mrow>            <mi>n</mi>           </msubsup>          </mrow>          <mo>=</mo>          <mrow>           <mo>(</mo>           <mrow>            <msub>             <mi>d</mi>             <mi>k</mi>            </msub>            <mo>-</mo>            <mover>             <mi>d</mi>             <mo>_</mo>            </mover>           </mrow>           <mo>)</mo>          </mrow>         </mrow>        </msqrt>        <mo>.</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>7</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0090" num="0000">Several patients have multiple disorders. Types of cancer include Kaposi sarcoma, breast cancer, skin cancer, and Hodgkin's lymphoma. SD means standard deviation.</p><p id="p-0091" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 1</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Patient characteristics</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="119pt" align="left"/><colspec colname="2" colwidth="84pt" align="center"/><tbody valign="top"><row><entry/><entry>Disorder</entry><entry>Number of patients</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="119pt" align="left"/><colspec colname="2" colwidth="84pt" align="char" char="."/><tbody valign="top"><row><entry/><entry>Cancer</entry><entry>45</entry></row><row><entry/><entry>HIV</entry><entry>46</entry></row><row><entry/><entry>Tuberculosis</entry><entry>8</entry></row><row><entry/><entry>Sickle cell disease</entry><entry>19</entry></row><row><entry/><entry>Acute kidney failure</entry><entry>1</entry></row><row><entry/><entry>Heart failure</entry><entry>4</entry></row><row><entry/><entry>Malaria</entry><entry>1</entry></row><row><entry/><entry>Anemia</entry><entry>3</entry></row><row><entry/><entry>Immune thrombocytopenic purpura</entry><entry>1</entry></row><row><entry/><entry>No major disease</entry><entry>25</entry></row><row><entry/><entry>Dataset</entry><entry>Average Hgb (g dL<sup>&#x2212;1</sup>)</entry></row><row><entry/><entry>Preliminary (training) dataset (n = 138)</entry><entry>12.65 (SD = 3.11)</entry></row><row><entry/><entry>Masked (testing) dataset (n = 15)</entry><entry>11.06 (SD = 3.62)</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0092" num="0068">Using the system shown in <figref idref="DRAWINGS">FIG. <b>2</b><i>a </i></figref>which includes a hyperspectral imaging system with an integrated and cooperative RGB camera, we conducted imaging sessions which results are provided in <figref idref="DRAWINGS">FIG. <b>6</b></figref> and referred to above. In particular, the guiding camera allows us to pinpoint the exact location of one or more hyperspectral line-scanning in the inner eyelid, see <figref idref="DRAWINGS">FIG. <b>2</b><i>b</i></figref>. The subject sits and places his/her chin on the chinrest and pulls down the eyelid for imaging when instructed. The white-light LED is illuminated on the inner eyelid, ensuring minimal light exposure to the eye. The guiding image panel shows a guide line corresponding to the location of the spectrograph slit, which is positioned vertically to acquire a hyperspectral line-scan dataset over the entire inner eyelid from top to bottom, see <figref idref="DRAWINGS">FIGS. <b>2</b><i>c</i>-<b>2</b><i>f</i></figref>. The image-guided hyperspectral line-scanning system acquires a snapshot of spatial-spectral information only for three seconds. To factor out the ambient room light, two measurements are conducted with the white-light LED on and off. To compensate for the system spectral response, the reflectance standard is used as a reference measurement.</p><p id="p-0093" num="0069">A spectrum reflected from the inner eyelid directly acquired by the image-guided hyperspectral line-scanning system allows us to build a blood Hgb extraction model for predicting actual blood Hgb content. First, we constructed a prediction model of blood Hgb levels using the preliminary (training) dataset of 138 individuals, using PLSR. In our case, a reflection spectrum r(&#x3bb;) has multicollinearity due to the large number of wavelengths (&#x3bb;=&#x3bb;<sub>1</sub>, &#x3bb;<sub>2</sub>, . . . , &#x3bb;<sub>N</sub>) and only a handful of the underlying latent variables are responsible for capturing the most variations in the predictor variables. Using ten-fold cross-validation, we determined 18 principal components as an optimal number of PLSR components for the blood Hgb prediction model. The results are shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. In <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a linear correlation between the computed blood Hgb content and the laboratory blood Hgb levels (i.e. the gold standard) is provided which shows an excellent R<sup>2 </sup>value of 0.95 for the preliminary dataset. When the Bland-Altman analysis is used to compare the two blood Hgb measurements, the 95% limits of agreement (LOA) exclude three out of 138 (2.17% outside LOA) with bias of 0 g dL<sup>&#x2212;1</sup>. Second, we applied the same Hgb prediction model to the separate testing dataset of 15 individuals. <figref idref="DRAWINGS">FIG. <b>7</b></figref> shows that LOA includes 15 computed blood Hgb values with bias of 0.01 g dL<sup>&#x2212;1</sup>. In addition, an excellent R<sup>2 </sup>value of 0.95 for the testing dataset supports the prediction model. Both preliminary and testing results clearly support the underlying idea that the full hyperspectral information of the inner eyelid can be used for accurately and precisely extracting actual Hgb count in the blood, noninvasively.</p><p id="p-0094" num="0070">In further description of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, to strengthen the validation of the blood Hgb prediction, two separate preliminary and masked testing datasets are used without reuse of individuals; a subset of 138 individuals is randomly selected as the preliminary dataset and the remaining 15 individuals is used to test the blood Hgb model. The Bland-Altman analyses compare the computed blood Hgb measurements with the laboratory blood Hgb test results, showing 95% limits of agreement (LOA) and bias in each system. With reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, spectroscopic blood Hgb measurements using the image-guided hyperspectral line-scanning system show the excellent performance with narrow LOA of [&#x2212;1.31, 1.31 g dL<sup>&#x2212;1</sup>] with bias of 0 g dL<sup>&#x2212;1 </sup>for the preliminary dataset. The testing dataset also supports a consistent yet low error in the blood Hgb measurements for the testing dataset. Only three out of 138 data points fall outside LOA for the preliminary dataset and none out of 15 for the testing dataset, indicating a consistent yet low error in the blood Hgb measurements.</p><p id="p-0095" num="0071">Those having ordinary skill in the art will recognize that numerous modifications can be made to the specific implementations described above. The implementations should not be limited to the particular limitations described. Other implementations may be possible.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230000357A1-20230105-M00001.NB"><img id="EMI-M00001" he="6.35mm" wi="76.20mm" file="US20230000357A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230000357A1-20230105-M00002.NB"><img id="EMI-M00002" he="6.35mm" wi="76.20mm" file="US20230000357A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230000357A1-20230105-M00003.NB"><img id="EMI-M00003" he="10.24mm" wi="155.53mm" file="US20230000357A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230000357A1-20230105-M00004.NB"><img id="EMI-M00004" he="5.67mm" wi="76.20mm" file="US20230000357A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230000357A1-20230105-M00005.NB"><img id="EMI-M00005" he="6.35mm" wi="76.20mm" file="US20230000357A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system for generating hyperspectral imaging data for measuring biochemical compositions, comprising:<claim-text>a spectral imaging device adapted to acquire one or more hyperspectral linescan images from one or more regions of interest of a subject, thereby generating one or more hyperspectral linescan datasets;</claim-text><claim-text>an optical imaging device with a red-green-blue (RGB) sensor adapted to acquire an RGB image from the region of interest of the subject, thereby generating an RGB dataset;</claim-text><claim-text>a processor adapted to:<claim-text>co-locate a plurality of pixels in the RGB dataset vs. a corresponding plurality of pixels of the one or more hyperspectral linescan datasets,</claim-text><claim-text>establish a transformation matrix utilizing the plurality of co-located pixels, the transformation matrix adapted to convert the RGB dataset into a hyperspectral dataset of the region of interest,</claim-text><claim-text>apply the transformation matrix to the RGB dataset to thereby generate the hyperspectral dataset for the region of interest, and</claim-text><claim-text>analyze the generated hyperspectral image dataset to determine the biochemical compositions.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the plurality of co-located pixels from the RGB dataset is associated with a 3&#xd7;1 RGB value matrix.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein each of the co-located plurality of pixels from the hyperspectral linescan dataset is associated with an N&#xd7;1 spectrum matrix, where N represents discretized spectra between a lower bound and an upper bound.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the lower and upper bounds are determined by the spectral range of RGB sensors.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the transformation matrix is an inverse of the RGB response function matrix of the RGB sensor.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the inverse of the transformation matrix is determined numerically by using RGB and spectral data from a subset of the collocated plurality of pixels.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the region of interest includes inner eyelid.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the biochemical compositions includes blood hemoglobin.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the biochemical compositions are determined using spectral analysis.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the spectral analysis includes a partial least square regression statistical modeling technique to first build a model from a training set of a first hyperspectral dataset vs. the biochemical compositions and then apply the model to a second dataset from the generated hyperspectral image dataset.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A method for generating hyperspectral imaging data for measuring biochemical compositions, comprising:<claim-text>obtaining one or more hyperspectral linescan images using a spectral imaging device from one or more region of interest of a subject, thereby generating one or more hyperspectral linescan datasets;</claim-text><claim-text>obtaining an RGB image from the region of interest using an optical imaging device with a red-green-blue (RGB) sensor, thereby generating an RGB dataset;</claim-text><claim-text>co-locating a plurality of pixels in the RGB dataset vs. a corresponding plurality of pixels of the one or more hyperspectral linescan datasets;</claim-text><claim-text>establishing a transformation matrix utilizing the plurality of co-located pixels, the transformation matrix adapted to convert the RGB dataset into a hyperspectral dataset of the region of interest;</claim-text><claim-text>applying the transformation matrix to the RGB dataset to thereby generate the hyperspectral dataset for the region of interest; and</claim-text><claim-text>analyzing the generated hyperspectral image dataset to determine the biochemical compositions.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein each of the plurality of co-located pixels from the RGB dataset is associated with a 3&#xd7;1 RGB value matrix.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein each of the co-located plurality of pixels from the hyperspectral linescan dataset is associated with an N&#xd7;1 spectrum matrix, where N represents discretized spectra between a lower bound and an upper bound.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the lower and upper bounds are determined by the spectral range of sensors.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the transformation matrix is an inverse of the RGB response matrix of the RGB sensor.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the inverse of the transformation matrix is determined numerically by using RGB and spectral data from a subset of the co-located plurality of pixels.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the region of interest includes inner eyelid.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the biochemical compositions includes blood hemoglobin.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the biochemical compositions are determined using spectral analysis.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the spectral modeling includes a partial least square regression statistical modeling technique to first build a model from a training set of a first hyperspectral dataset vs. the biochemical compositions and then apply the model to a second dataset from the generated hyperspectral image dataset.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. A method for generating hyperspectral imaging data for measuring biochemical compositions, comprising:<claim-text>obtaining one or more hyperspectral linescan images using a spectral imaging device from one or more region of interest of a subject, thereby generating one or more hyperspectral linescan datasets;</claim-text><claim-text>obtaining an RGB image from the region of interest using an optical imaging device with a red-green-blue (RGB) sensor, thereby generating an RGB dataset;</claim-text><claim-text>co-locating a plurality of pixels in the RGB dataset vs. a corresponding plurality of pixels of the one or more hyperspectral linescan datasets;</claim-text><claim-text>converting the RGB dataset into a hyperspectral dataset of the region of interest; and</claim-text><claim-text>analyzing the generated hyperspectral image dataset to determine the biochemical compositions.</claim-text></claim-text></claim></claims></us-patent-application>