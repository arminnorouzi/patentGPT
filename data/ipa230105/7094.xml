<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007095A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007095</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17546603</doc-number><date>20211209</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>34</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">METHODS AND APPARATUS FOR COMMUNICATING VECTOR DATA</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63218468</doc-number><date>20210705</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>HUAWEI TECHNOLOGIES CO., LTD.</orgname><address><city>Shenzhen</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant><us-applicant sequence="01" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>The Governing Council of the University of Toronto</orgname><address><city>Toronto</city><country>CA</country></address></addressbook><residence><country>CA</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>ADIKARI</last-name><first-name>Tharindu</first-name><address><city>Toronto</city><country>CA</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>DRAPER</last-name><first-name>Stark C.</first-name><address><city>Toronto</city><country>CA</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>LAM</last-name><first-name>Jason T S</first-name><address><city>Markham</city><country>CA</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>HU</last-name><first-name>Zhenhua</first-name><address><city>Toronto</city><country>CA</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>HUAWEI TECHNOLOGIES CO., LTD.</orgname><role>03</role><address><city>SHENZHEN</city><country>CN</country></address></addressbook></assignee><assignee><addressbook><orgname>The Governing Council of the University of Toronto</orgname><role>03</role><address><city>Toronto</city><country>CA</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method of communicating time correlated vector data within a network includes reading, by a transmitting node, a first vector data including a plurality of elements, selecting, by the transmitting node, a subset of elements of the plurality of elements based on a criteria and sending, by the transmitting node, the subset of elements to a receiving node. The receiving node receives the subset of elements and estimates a plurality of elements not included in the subset of elements based on a previously received subset of element based on a second vector data. The first vector data and the second vector data are part of a time series of vectors.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="173.91mm" wi="129.71mm" file="US20230007095A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="170.18mm" wi="163.49mm" orientation="landscape" file="US20230007095A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="142.92mm" wi="163.49mm" orientation="landscape" file="US20230007095A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="230.89mm" wi="131.74mm" file="US20230007095A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="218.10mm" wi="138.77mm" orientation="landscape" file="US20230007095A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="176.02mm" wi="119.97mm" file="US20230007095A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="233.00mm" wi="157.31mm" file="US20230007095A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="207.43mm" wi="171.45mm" orientation="landscape" file="US20230007095A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims the benefit of and priority to U.S. Provisional Application Ser. No. 63/218,468 filed Jul. 5, 2021, the entire contents of which are hereby incorporated by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure pertains to the field of machine learning technologies, such as but not necessarily limited to the training of machine learning models, and in particular to a method and apparatus for distributed training machine learning models where large amounts of update vector data are communicated.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Machine learning includes classes of computer implemented solutions and applications where computers are trained and optimized to perform tasks without being explicitly programmed to do so. Machine learning models may be advantageously used for problems where it may be challenging for a human to create the needed algorithms manually. Examples of applications that benefit from machine learning solutions include self-driving vehicles, language translation, fraud detection, weather forecasting, and others.</p><p id="p-0005" num="0004">Machine learning models are trained using sample datasets selected for a particular application. For example, a character recognition model may be trained using a database of handwriting samples. Training data includes input data and information indicating the correct solution to the problem and is used to train and improve the machine learning model until it produces sufficiently accurate results. Training can involve very large datasets and require significant time to produce and train a sufficiently accurate model. Solutions involving decentralized, distributed computers and multiple computer nodes connected through computer networks have been used to decrease training time.</p><p id="p-0006" num="0005">Decentralized optimization using multiple computer nodes, where update training vectors are exchanged among nodes, has become the norm for training machine learning models on large datasets. With the need to train bigger models on ever-growing datasets, scalability of communications between computer nodes has become a concern. A potential solution to growing dataset size is to increase the number of nodes, however communication amongst nodes can become a processing bottleneck and communication time can account for a significant portion of the overall machine learning model training time.</p><p id="p-0007" num="0006">Therefore, there is a need for a method and apparatus for optimizing computer node updates by minimizing the size of update vector transmissions that obviates or mitigates one or more limitations of the prior art, for example by reducing communication overhead, while minimizing any impact on the convergence rate, and by reducing the amount of time and bandwidth required to communicate update vector transmissions.</p><p id="p-0008" num="0007">This background information is provided to reveal information believed by the applicant to be of possible relevance to the present disclosure. No admission is necessarily intended, nor should be construed, that any of the preceding information constitutes prior art against the present disclosure.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0009" num="0008">An object of embodiments of the present disclosure is to provide a method and apparatus for compressing a time series of vector data for transmission in a network such as computer network. Embodiments use vector data where there is a temporal correlation between consecutive vector data of the time series. The time series of vector data may also refer to time correlated vector data.</p><p id="p-0010" num="0009">Embodiments may be used in applications where machine learning models are trained using a plurality of computer nodes connected by a computer network, which may be configured in a master-worker arrangement where one master computer node coordinates update vector calculations performed by one or more worker computer nodes.</p><p id="p-0011" num="0010">Embodiments may use error-feedback to improve compression rates without decreasing the convergence rate while training a machine learning model.</p><p id="p-0012" num="0011">In accordance with embodiments of the present disclosure, there is provided a method of communicating vector data within a network. The method includes obtaining, by a transmitting node, a first vector data including a plurality of elements. Then, selecting, by the transmitting node, a subset of elements of the plurality of elements and sending, by the transmitting node, the subset of elements to a receiving node. Also, estimating, by the transmitting node, a plurality of elements not included in the subset of elements based on a previously transmitted subset of element based on a second vector data, and forming, by the transmitting node, a reconstructed vector data including the subset of elements and the estimated plurality of elements not included in the subset of elements.</p><p id="p-0013" num="0012">In further embodiments, the estimating the plurality of elements not included in the subset of elements includes updating, when one of the subset of elements is transmitted, a predicted value of the one of the subset of elements and resetting a counter, and setting, when one of the subset of elements is not transmitted, one of the plurality of elements not included in the subset of elements with the predicted value, and incrementing the counter.</p><p id="p-0014" num="0013">In further embodiments, the subset of elements is selected based on a criteria, and the criteria is an absolute value of each of the plurality of elements of the first vector data.</p><p id="p-0015" num="0014">In further embodiments, the first vector data and the second vector data are update vectors as part of a machine learning model training process.</p><p id="p-0016" num="0015">In further embodiments, the first vector data and the second vector data are part of a time series of vectors.</p><p id="p-0017" num="0016">In further embodiments, the first vector data is obtained by combining an initial vector data with a weighted difference of the reconstructed vector data and the first vector data.</p><p id="p-0018" num="0017">In accordance with embodiments of the present disclosure, there is provided a network node for transmitting vector data over a network connection. The network node includes a processor and a non-transient memory for storing instructions which when executed by the processor cause the network node to read a first vector data including a plurality of elements, select a subset of elements of the plurality of elements, and send the subset of elements to a receiving node. Also, to estimate a plurality of elements not included in the subset of elements based on a previously transmitted subset of elements based on a second vector data, and to form a reconstructed vector data including the subset of elements and the estimated plurality of elements not included in the subset of elements.</p><p id="p-0019" num="0018">In further embodiments, the estimating the plurality of elements not included in the subset of elements includes updating, when one of the subset of elements is transmitted, a predicted value of the one of the subset of elements and resetting a counter, and setting, when one of the subset of elements is not transmitted, one of the plurality of elements not included in the subset of elements with the predicted value, and incrementing the counter.</p><p id="p-0020" num="0019">In further embodiments, the subset of elements is selected based on a criteria, and the criteria is an absolute value of each of the plurality of elements of the first vector data.</p><p id="p-0021" num="0020">In further embodiments, the first vector data and the second vector data are update vectors as part of a machine learning model training process.</p><p id="p-0022" num="0021">In further embodiments, the first vector data is obtained by combining an initial vector data with a weighted difference of the reconstructed vector data and the first vector data.</p><p id="p-0023" num="0022">In accordance with embodiments of the present disclosure, there is provided a network node for receiving vector data over a network connection. The network node includes a processor and a non-transient memory for storing instructions which when executed by the processor cause the network node to receive, from a transmitting node, a subset of elements of a first vector data. Furthermore, to estimate a plurality of elements not included in the subset of elements based on a previously received subset of element based on a second vector data, the first vector data and the second vector data being part of a time series of vectors, the subset of elements selected by the transmitting node. Also, to form a reconstructed vector data including the subset of elements and the estimated plurality of elements not included in the subset of elements.</p><p id="p-0024" num="0023">In further embodiments, the estimating the plurality of elements not included in the subset of elements includes updating, when one of the subset of elements is received, a predicted value of the one of the subset of elements and clearing a counter, and setting, when one of the subset of elements is not received, one of the plurality of elements not included in the subset of elements with the predicted value, and incrementing the counter.</p><p id="p-0025" num="0024">In further embodiments, the subset of elements is selected based on a criteria, and the criteria is an absolute value of each of the plurality of elements of the first vector data.</p><p id="p-0026" num="0025">In further embodiments, the first vector data and the second vector data are update vectors as part of a machine learning model training process.</p><p id="p-0027" num="0026">In further embodiments, the first vector data is obtained by combining an initial vector data with a weighted difference of the reconstructed vector data and the first vector data.</p><p id="p-0028" num="0027">In accordance with embodiments of the present disclosure, there is provided a method of communicating vector data within a network. The method includes receiving, from a transmitting node, a subset of elements of a first vector data, and estimating a plurality of elements not included in the subset of elements based on a previously received subset of element based on a second vector data. The first vector data and the second vector data are part of a time series of vectors. The subset of elements selected by the transmitting node. Also, forming a reconstructed vector data including the subset of elements and the estimated plurality of elements not included in the subset of elements.</p><p id="p-0029" num="0028">In further embodiments, the estimating the plurality of elements not included in the subset of elements includes updating, when one of the subset of elements is received, a predicted value of the one of the subset of elements and resetting a counter, and setting, when one of the subset of elements is not received, one of the plurality of elements not included in the subset of elements with the predicted value, and incrementing the counter.</p><p id="p-0030" num="0029">In further embodiments, the subset of elements is selected based on a criteria, and the criteria is an absolute value of each of the plurality of elements of the first vector data.</p><p id="p-0031" num="0030">In further embodiments, the first vector data and the second vector data are update vectors as part of a machine learning model training process.</p><p id="p-0032" num="0031">In further embodiments, the first vector data is obtained by combining an initial vector data with a weighted difference of the reconstructed vector data and the first vector data.</p><p id="p-0033" num="0032">In accordance with embodiments of the present disclosure, there is provided a method of communicating vector data within a network. The method includes obtaining, by a transmitting node, a first vector data including a plurality of elements, compressing, by the transmitting node, the first vector data to produce a compressed vector data, and sending, by the transmitting node, the compressed vector data to a receiving node. Also, estimating, by the transmitting node, a reconstructed vector from the compressed vector data and a second vector data, the first vector data and the second vector data being a part of a time correlated series of vector data, the second vector data being earlier in time than the first vector data. Furthermore, receiving, by the receiving node, from a transmitting node, the compressed vector data, and estimating, by the receiving node, the reconstructed vector.</p><p id="p-0034" num="0033">Embodiments have been described above in conjunctions with aspects of the present disclosure upon which they can be implemented. Those skilled in the art will appreciate that embodiments may be implemented in conjunction with the aspect with which they are described, but may also be implemented with other embodiments of that aspect. When embodiments are mutually exclusive, or are otherwise incompatible with each other, it will be apparent to those skilled in the art. Some embodiments may be described in relation to one aspect, but may also be applicable to other aspects, as will be apparent to those of skill in the art.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0035" num="0034">Further features and advantages of the present disclosure will become apparent from the following detailed description, taken in combination with the appended drawings, in which:</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a network with a master computer node and a plurality of worker computer nodes in a star topology, according to an embodiment.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a schematic diagram of a computing device that may be used to implement master computer node and worker computer nodes, according to embodiments.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a method of training and utilizing a machine learning model and algorithm, according to an embodiment.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a method of a worker node performing momentum, quantization, error-feedback, and encoding, and of a master performing decoding, and possible post-processing, according to an embodiment.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrated the operation of an encoder, according to an embodiment.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a method that may be used by predictors according to an embodiment.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a generalized method for transmitting vector data between computer nodes, according to an embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0043" num="0042">It will be noted that throughout the appended drawings, like features are identified by like reference numerals.</p><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0044" num="0043">Embodiments of the present disclosure relate to methods, systems, and apparatus for compressing a time series of vector data for transmission in a computer network. Embodiments use time series vector data where there is a temporal correlation between consecutive vector data in the time series.</p><p id="p-0045" num="0044">Embodiments may use error-feedback to improve compression rates without decreasing the convergence rate during a process to train a machine learning model.</p><p id="p-0046" num="0045">Embodiments may be used in applications where machine learning models are trained using a plurality of computer nodes connected by a computer network, which may be configured in a master-worker arrangement where one master computer node coordinates update vector calculations performed by one or more worker computer nodes.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a decentralized machine learning training architecture <b>100</b> using a master-worker model. A master <b>102</b> and eight workers (labelled <b>104</b><i>a </i>through <b>104</b><i>h, </i>which may be referred to collectively as <b>104</b> which may refer to workers individually or collectively) can be configured in a master-worker topology. Each worker <b>104</b> computes an update vector and sends it to the master <b>102</b>. The master <b>102</b> computes the average of all update vectors received from the workers <b>104</b> and broadcasts the computed average back to workers <b>104</b>. Each worker <b>104</b> then uses the average to update their learning model. These steps may be executed iteratively until a convergence criteria is met.</p><p id="p-0048" num="0047">Though <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a network in a star topology, embodiments may use any type of topology where there exist direct or indirect computer network connections <b>106</b> between master node <b>102</b> and worker nodes <b>104</b>. Master node <b>102</b> and worker <b>104</b> nodes may be co-located or be distributed geographically. Computer network connections such as <b>106</b> may be implemented by a combination of hardware and software and include one or more networking technologies as are known in the art. Networking technologies include wired and wireless protocols such as WiFi (IEEE 802.11) and Ethernet protocols such as Gigabit Ethernet (GbE) and 10 Gigabit Ethernet (10GbE). Physical layer connections of connection <b>106</b> may use twisted pair cables or fibre optic cables as well as other physical layer connections as are known in the art. Physical layer connections of connection <b>106</b> may also include routers, switches, and bridges as required to make connections between master <b>102</b> and workers <b>104</b>. Though <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a star network topology, embodiments are not limited to any particular topology and may be implemented in networks with topologies such as ring, mesh, tree, bus, any others as are known in the art.</p><p id="p-0049" num="0048">Stochastic gradient descent (SDG) is an algorithm for training a wide range of models in machine learning and for training artificial neural networks. SGD is an iterative method for optimizing an objective function with suitable smoothness properties and may be seen as a stochastic approximation of gradient descent optimization. SGD replaces the actual gradient calculated from the entire data set with an estimate gradient calculated from a randomly selected subset of the data. In high-dimensional optimization problems SGD reduces the computational burden and achieves faster iterations at the expense of a lower convergence rate. Other algorithms may also be used in embodiments such as the ADAM algorithm, an optimization algorithm for stochastic gradient descent for training deep learning models, which combines momentum ideas with adaptive step size.</p><p id="p-0050" num="0049">A variation of the SGD algorithm is the momentum-SGD which is an iterative algorithm where all workers <b>104</b> collectively optimize a machine learning model while the master <b>102</b> facilitates synchronization. Each worker <b>104</b> computes an update vector and sends it to the master <b>102</b>. The master <b>102</b> computes the average of all update vectors received from the workers <b>104</b> and broadcasts the average back to the workers <b>104</b>. Each worker <b>104</b> then uses the average to update the learning model. These steps are executed iteratively until a convergence criteria is met. Successive update vectors transmitted between the master <b>102</b> and each worker <b>104</b> may be viewed as a plurality of time series of vector data, which in this embodiment may be iterative optimization parameters of the machine learning model. As used herein, &#x201c;time series&#x201d; refers to iterations of data occurring at consecutive times. In the cases, each iteration may be spaced equally apart in time, while in other cases, each iteration may be spaced at varying or random times from each other. In embodiments, each time iteration may depend on processing or communication time so that the time between iterations will vary within an expected range of values. In embodiments, each iteration occurs subsequent or previous to another while the time between samples places no limitations on embodiments. Examples of time series data includes update vectors used to train machine learning models, video processing, analysing stock market data, and processing astronomical and meteorological data.</p><p id="p-0051" num="0050">Vector data transmitted from n workers <b>104</b> to the master <b>102</b> may be viewed as n separate time series of vector data. Vector data transmitted from master <b>102</b> to each of the n workers <b>104</b> may be viewed as another n separate time series of vector data. By using a momentum-SGD algorithm, the update vector smooths the stochastic gradient over the iterations for each time series of vector data. The momentum-SGD algorithm applies an exponentially weighted low-pass filter (LPF) to gradients across time iterations of the update vector which filters out high-frequency components and preserves low-frequency ones and reduces the variation in the resulting update vectors in consecutive iterations. Embodiments may also be optimized using different filter variations, such as filters that implement a combination of low-pass and band-pass characteristics. This causes each entry in the update vector to change slowly over the time iterations. Embodiments use this temporal correlation between elements of update vectors when compressing the update vectors transmitted between master <b>102</b> and workers <b>104</b>.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram of a computing device <b>200</b> that may be used to implement master computer node <b>102</b> and worker computer nodes <b>104</b> according to embodiments. Computing device <b>200</b> that may perform any or all of operations of the methods and features explicitly or implicitly described herein, according to different embodiments of the present disclosure. As shown, the device includes a processor <b>210</b>, such as a Central Processing Unit (CPU) or specialized processors such as a Graphics Processing Unit (GPU), Vector Processing Unit (VPU), or other such processor unit, memory <b>220</b>, non-transitory mass storage <b>230</b>, I/O interface <b>240</b>, network interface <b>250</b>, and a transceiver <b>260</b>, all of which are communicatively coupled via bi-directional bus <b>270</b>. According to certain embodiments, any or all of the depicted elements may be utilized, or a subset of the elements. Further, the computing device <b>200</b> may contain multiple instances of certain elements, such as multiple processors, memories, or transceivers. Also, elements of the hardware device may be directly coupled to other elements without the bi-directional bus. Additionally, or alternatively to a processor and memory, other electronics, such as integrated circuits, may be employed for performing the required logical operations.</p><p id="p-0053" num="0052">The memory <b>220</b> may include any type of non-transitory memory such as static random access memory (SRAM), dynamic random access memory (DRAM), synchronous DRAM (SDRAM), read-only memory (ROM), any combination of such, or the like. The mass storage element <b>230</b> may include any type of non-transitory storage device, such as a solid state drive, hard disk drive, a magnetic disk drive, an optical disk drive, USB drive, or any computer program product configured to store data and machine executable program code. According to certain embodiments, the memory <b>220</b> or mass storage <b>230</b> may have recorded thereon statements and instructions executable by the processor <b>210</b> for performing any of the aforementioned method operations described above.</p><p id="p-0054" num="0053">Computing device <b>200</b> may also include one or more optional components and modules such as video adapter <b>270</b> coupled to a display <b>275</b> for providing output, and I/O interface <b>240</b> coupled to I/O devices <b>245</b> for providing input and output interfaces.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a method of training and utilizing a machine learning model <b>314</b> and algorithm, according to an embodiment. Machine learning model <b>302</b> includes computer statements and instruction stored in memory <b>220</b> or mass storage <b>230</b> that may be read and executed by the processor <b>210</b> for performing methods described herein. Data <b>308</b> is collected from any number of sources and pre-processed to produce a training dataset <b>310</b>. Pre-processing may include ensuring that data <b>308</b> is complete and in one or more common formats to be input into the machine learning model <b>302</b>. Missing data may be added, ignored, or replaced with estimates, noisy data that results in outliers may be removed or smoothed with other data values. Inconsistent data and data that is in error may be corrected or compensated for. In the case that data <b>308</b> comes from a plurality of sources the data may have to be converted into a common format, have data from multiple sources combined, or have data divided or split as required for input to the machine learning model <b>320</b>. The training dataset <b>310</b> may also be tagged to indicate to the machine learning model a type of data, a class of data, a subject of the data, the correctness of the data, etc. to allow the machine learning model <b>302</b> to &#x201c;learn&#x201d; from the training dataset <b>310</b>.</p><p id="p-0056" num="0055">In embodiments, the machine learning model <b>302</b> includes one or more machine learning algorithms that may be broadly classified as decision trees, support vector machines, regression, clustering, and other machine learning algorithms as is known in the art. Machine learning model <b>302</b> may also include an evaluation module <b>306</b> to evaluate the results of algorithm <b>304</b> which, in the case of supervised learning applications, may be done by comparing the tags of the training dataset <b>310</b> to the classification results produced by algorithm <b>304</b> in response to the training data <b>310</b>. Training dataset <b>310</b> is used to tune and configure algorithm <b>304</b> which may include tuning parameters of the algorithm <b>304</b>. Once tuned, machine learning model <b>302</b> may be tested or verified using testing dataset <b>312</b>. The machine learning model <b>302</b> may be tested for accuracy, speed, and other parameters such as the number of false negative or false positive results, as required to qualify machine learning model <b>302</b> for use on production data <b>316</b>. Once qualified, production data <b>316</b> may be input to the model <b>314</b>, which implements machine learning model <b>302</b>, to produce prediction results <b>318</b>.</p><p id="p-0057" num="0056">With reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, embodiments compress update vectors between two computer nodes, such as master <b>102</b> and any of workers <b>104</b>, by communicating a subset of elements of the update vectors. In embodiments, parts of elements, for example, only the most significant elements may be included in the subset, with most significant being determined using a criteria such as the absolute value of each vector element. Elements that are not communicated are predicted, or estimated, at the computer node receiving the update and may also be predicted by the sending computer node. For example, a worker <b>104</b> may transmit the most significant elements of an update vector to the master <b>102</b>. The master <b>102</b> receives the most significant elements and predicts the remaining elements that were not communicated by taking advantage of the temporal correlation between vector elements of prior values of the same vector element previously received or predicted. Similarly, the sending worker <b>104</b> may predict the remaining elements that were not communicated using the same prediction method as the master <b>102</b> allowing both the sending worker <b>104</b> and the receiving master <b>102</b> to obtain the same reconstructed vector data.</p><p id="p-0058" num="0057">With reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, in embodiments, g<sub>t</sub><sup>i </sup>is a stochastic gradient calculated by a worker <b>104</b> where i indicates the worker that calculated the stochastic gradient vector and t indicates the iteration (time) of the time series of the vector. For example, g<sub>5</sub><sup>2 </sup>would refer to the vector, g, calculated by worker 2 (reference <b>104</b><i>b </i>in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) at time=5.</p><p id="p-0059" num="0058">Parameter &#x3b2; <b>403</b>, 0&#x2264;&#x3b2;&#x3c;1, is used to control the low pass filter effects of the momentum-SGD algorithm and in practice may be set close to 1. In embodiments, this may be 0.9 or 0.99. The gradient vector, g<sub>t</sub><sup>i </sup><b>402</b>, is used to produce update vector, v<sub>t</sub><sup>i </sup><b>404</b>, where v<sub>t</sub><sup>i</sup>=&#x3b2;v<sub>t&#x2212;1</sub><sup>i</sup>+(1&#x2212;&#x3b2;)g<sub>t</sub><sup>i</sup>. Using a value of &#x3b2; close to 1 ensures that values of v<sub>t</sub><sup>i </sup>are determined mainly by the previous value of v, that is v<sub>t&#x2212;1</sub><sup>i</sup>.</p><p id="p-0060" num="0059">In embodiments, switch EF <b>424</b> may be open and vector r<sub>t</sub><sup>i</sup>=v<sub>t</sub><sup>i</sup>. In embodiments with error feedback, switch EF <b>424</b> is closed and vector</p><p id="p-0061" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <msubsup>    <mi>r</mi>    <mi>t</mi>    <mi>i</mi>   </msubsup>   <mo>=</mo>   <mrow>    <msubsup>     <mi>v</mi>     <mi>t</mi>     <mi>i</mi>    </msubsup>    <mo>+</mo>    <mrow>     <mfrac>      <msub>       <mi>&#x3b7;</mi>       <mrow>        <mi>t</mi>        <mo>-</mo>        <mn>1</mn>       </mrow>      </msub>      <msub>       <mi>&#x3b7;</mi>       <mi>t</mi>      </msub>     </mfrac>     <mo>&#x2062;</mo>     <msubsup>      <mi>e</mi>      <mrow>       <mi>t</mi>       <mo>-</mo>       <mn>1</mn>      </mrow>      <mi>i</mi>     </msubsup>    </mrow>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0062" num="0000">where n<sub>t </sub>is a learning rate, and e<sub>t</sub><sup>i </sup>is an error vector, indicating a difference between the vector, r<sub>t</sub><sup>i </sup><b>406</b>, and the reconstructed or predicted vector, {tilde over (r)}<sub>t</sub><sup>i </sup><b>428</b>.</p><p id="p-0063" num="0060">In embodiments, a quantizer, Q <b>414</b>, is used to compress the vector, r<sub>t</sub><sup>i </sup><b>406</b>, to produce a sparse vector, {circumflex over (r)}<sub>t</sub><sup>i </sup><b>408</b>. Vector {circumflex over (r)}<sub>t</sub><sup>i </sup><b>408</b> is given by the equation {circumflex over (r)}<sub>t</sub><sup>i</sup>=Q(r<sub>t</sub><sup>i</sup>). In embodiments, the Q <b>414</b> operator produces sparse vector, {circumflex over (r)}<sub>t</sub><sup>i </sup><b>408</b>, by setting all elements in vector r<sub>t</sub><sup>i </sup><b>406</b> to zero except for the K elements with the largest absolute value magnitudes. Alternatively, other quantizers may be used that select or omit vector elements based on other criteria.</p><p id="p-0064" num="0061">In embodiments, encoder, &#x3b5; <b>416</b>, is used to produce a bit stream <b>426</b> that is transmitted to the master <b>102</b> by encoding the non-zero locations in {circumflex over (r)}<sub>t</sub><sup>i </sup><b>408</b> and the corresponding values.</p><p id="p-0065" num="0062">Master <b>102</b> received bitstream <b>426</b> at decoder, D <b>418</b> and recreates the sparse vector, {circumflex over (r)}<sub>t</sub><sup>i </sup><b>410</b>, that was transmitted by worker <b>104</b>. A prediction system, P <b>420</b>, may then be used to predict one of more of the zero value elements of vector, r<sub>t</sub><sup>i </sup><b>406</b> that were not included in sparse vector, {circumflex over (r)}<sub>t</sub><sup>i </sup><b>410</b>, and not included in received bitstream <b>426</b>. Predicted vector elements may be used to create a reconstructed vector, {tilde over (r)}<sub>t</sub><sup>i</sup>, using the equation {tilde over (r)}<sub>t</sub><sup>i</sup>=P({circumflex over (r)}<sub>t</sub><sup>i</sup>), and are combined with received vector elements to produce vector {tilde over (r)}<sub>t</sub><sup>i</sup>. An example of a prediction method is illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> and described below.</p><p id="p-0066" num="0063">In embodiments, worker node <b>104</b> may also use its own prediction system, P <b>422</b>, to apply the same predicted vector elements to the sparse vector to obtain a vector {tilde over (r)}<sub>t</sub><sup>i </sup><b>428</b>, that is the same vector {tilde over (r)}<sub>t</sub><sup>i </sup><b>412</b> as used by master <b>102</b>.</p><p id="p-0067" num="0064">In embodiments, workers <b>104</b><i>a </i>through <b>104</b><i>h </i>may calculate stochastic gradient vectors, g<sub>t</sub><sup>1</sup>, g<sub>t</sub><sup>2</sup>, . . . , g<sub>t</sub><sup>n</sup>, where in the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, n=8, and transmit updates vectors, v<sub>t</sub><sup>1</sup>, v<sub>t</sub><sup>2</sup>, . . . , v<sub>t</sub><sup>n</sup>, to master <b>102</b>. The master then computes an average of {tilde over (r)}<sub>t</sub><sup>i </sup>across all workers (all i). Finally, master <b>102</b> broadcasts the average back to the workers <b>104</b>. All workers <b>104</b> then update their parameter vector,</p><p id="p-0068" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>w</mi>    <mrow>     <mi>t</mi>     <mo>+</mo>     <mn>1</mn>    </mrow>   </msub>   <mo>=</mo>   <mrow>    <msub>     <mi>w</mi>     <mi>t</mi>    </msub>    <mo>-</mo>    <mrow>     <msub>      <mi>&#x3b7;</mi>      <mi>t</mi>     </msub>     <mo>&#x2062;</mo>     <mfrac>      <mn>1</mn>      <mi>n</mi>     </mfrac>     <mo>&#x2062;</mo>     <mrow>      <msub>       <mo>&#x2211;</mo>       <mrow>        <mi>i</mi>        <mo>&#x2208;</mo>        <mrow>         <mo>[</mo>         <mi>n</mi>         <mo>]</mo>        </mrow>       </mrow>      </msub>      <msubsup>       <mover accent="true">        <mi>r</mi>        <mi>&#x2dc;</mi>       </mover>       <mi>t</mi>       <mi>i</mi>      </msubsup>     </mrow>    </mrow>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0069" num="0000">used in training the machine learning model.</p><p id="p-0070" num="0065">Though the embodiment of <figref idref="DRAWINGS">FIG. <b>4</b></figref> is described from the point of view of a worker <b>104</b> transmitting an update vector <b>404</b> to a master <b>102</b>, the same method may also be applied to a master <b>104</b> transmitting an update vector <b>404</b> to any of a plurality of workers <b>104</b>. More generally, the method of <figref idref="DRAWINGS">FIG. <b>4</b></figref> may be used to transmit any suitable vector data from one computer node to another computer node.</p><p id="p-0071" num="0066">In embodiments, the number of elements in the vectors will vary depending on the application and the value of K may also be varied to obtain a compression factor that yields acceptable results.</p><p id="p-0072" num="0067">As shall be appreciated on a more generic level, quantizer <b>414</b> may be any compression method that may be used on a time correlated series of vector data. Furthermore, the prediction systems <b>420</b> and <b>422</b> may include any number of methods, designed jointly with quantizer <b>414</b>, in order to produce a more efficiently compressed bit stream <b>426</b>, consisting of fewer bits. The decoder <b>418</b> and prediction system <b>420</b> can act on the bit stream <b>426</b> to produce the predicted vector <b>412</b>.</p><p id="p-0073" num="0068">Referring again to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, embodiments may use error-feedback where a switch EF <b>424</b> is in the closed position to feedback the error between the update vector, r<sub>t</sub><sup>i </sup><b>406</b>, and the predicted vector, {tilde over (r)}<sub>t</sub><sup>i </sup><b>412</b>, as received by the master <b>102</b>. Predictor, P <b>422</b>, operates on the sparse vector, {circumflex over (r)}<sub>t</sub><sup>i </sup><b>408</b>, to create an error vector with predicted values, {tilde over (r)}<sub>t</sub><sup>i </sup><b>428</b>, which has identical values to predicted vector, {tilde over (r)}<sub>t</sub><sup>i </sup><b>412</b>, calculated by the master <b>102</b>. Vector <b>428</b> is subtracted from update vector, r<sub>t</sub><sup>i </sup><b>406</b> to produce error vector, e<sub>t</sub><sup>i </sup><b>430</b>, where e<sub>t</sub><sup>i</sup>=r<sub>t</sub><sup>i</sup>&#x2212;{tilde over (r)}<sub>t</sub><sup>i </sup><b>430</b>, may then be combined with update vector, v<sub>t</sub><sup>i </sup><b>404</b>. In embodiments, both functions, z<sup>&#x2212;1 </sup><b>432</b> and <b>434</b>, may be unit delays.</p><p id="p-0074" num="0069">In embodiments, predictors, P <b>422</b> at worker <b>104</b> and P <b>420</b> at master <b>102</b>, may be used to predict or estimate any of the vector elements that are not in the top K most significant elements. Since both computer nodes, master <b>102</b> and worker <b>104</b>, use the same predictor, both sides have access to the same data.</p><p id="p-0075" num="0070">The operation of encoder <b>416</b> is illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref> according to an embodiment. In this non limiting example, v<sub>t</sub>, v<sub>t+1</sub>, and v<sub>t+2</sub>, are consecutive update vectors with five elements, numbered <b>0</b>, <b>1</b>, . . . , <b>4</b>. Quantizer, Q <b>414</b>, operates on v<sub>t</sub>, v<sub>t+1</sub>, and v<sub>t+2 </sub>to produce corresponding sparse vectors, {circumflex over (r)}<sub>t</sub>, {circumflex over (r)}<sub>t+1</sub>, and {circumflex over (r)}<sub>+2</sub>. The value K in this example is 2. At time t, v<sub>t=(&#x2212;</sub>0.4, 2.2, 5.2, 1.3, &#x2212;2.5) <b>502</b>. The two elements with the largest absolute value magnitude are element 2, 5.2, and element 4, &#x2212;2.5. Therefore, sparse vector {circumflex over (r)}<sub>t</sub>=(0, 0, 5.2, 0, &#x2212;2.5) <b>503</b> since the K largest values are passed through while all other values are set to zero. Similarly, at time t+1, v<sub>t+1</sub>=(&#x2212;0.6, 2.6, 4.1,1.5, &#x2212;2.2) <b>505</b>. The two elements with the largest absolute value magnitude are element 3, 4.1, and element 2, 2.6. Therefore, sparse vector {circumflex over (r)}<sub>t+1</sub>=(0,2.6, 4.1,0,0) <b>506</b> since the K largest values are passed through while all other elements are set to zero. Similarly, at time t+2, v<sub>t+2</sub>=(3.4, &#x2212;0.5, 1.1, &#x2212;2.8, 0.9) <b>508</b>. The two elements with the largest absolute value magnitude are element 0, 3.4, and element 3, &#x2212;2.8. Therefore, sparse vector {circumflex over (r)}<sub>t+2</sub>=(0,2.6,4.1,0,0) <b>509</b> since the K largest values are passed through while all other elements are set to zero.</p><p id="p-0076" num="0071"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a method that may be used by predictors, P, according to an embodiment. Predictors include memory to allow for the storage of predicted values, &#x3c1;<sub>i</sub>, and a counter, &#x3c4;<sub>i</sub>, a vector that indicates the number of times an estimated value was used for a vector element of {tilde over (r)}<sub>t</sub><sup>i</sup>. Initially, t=0, in step <b>502</b>, the state vectors, &#x3c1;<sub>i</sub>, and counter, &#x3c4;<sub>i </sub>are initialized with all elements set to zero, i.e., &#x3c1;<sub>i</sub>=0 and &#x3c4;<sub>i</sub>=0. In step <b>512</b>, a sparse vector input, {circumflex over (r)}<sub>t</sub><sup>i </sup><b>408</b> or <b>410</b>, is received. Predicted vector {tilde over (r)}<sub>t</sub><sup>i </sup><b>412</b> is stored locally after having first been initialized to zero for each element. Each element, k, of the of {tilde over (r)}<sub>t</sub><sup>i </sup><b>412</b> is processed. In step <b>506</b>, a check is made of {circumflex over (r)}<sub>t</sub><sup>i</sup>[k] to determine if a zero value or no value has been received, i.e., is {circumflex over (r)}<sub>t</sub><sup>i</sup>[k]&#x2260;0. If there is no value or a zero value of {circumflex over (r)}<sub>t</sub><sup>i </sup>[k] then, in step <b>508</b>, a predicted value, &#x3c1;<sub>i</sub>[k], is assigned to {tilde over (r)}<sub>t</sub><sup>i</sup>[k], and in step <b>510</b> the counter, &#x3c4;<sub>i</sub>[k], is incremented by 1 to indicate that no value had been received for {circumflex over (r)}<sub>t</sub><sup>i</sup>[k] at time t. If a value for {circumflex over (r)}<sub>t</sub><sup>i</sup>[k] has been received then, in step <b>512</b>, that value is used in the predicted vector, {tilde over (r)}<sub>t</sub><sup>i</sup>[k]={circumflex over (r)}<sub>t</sub><sup>i</sup>[k]. In step <b>514</b>, the estimated value, &#x3c1;<sub>i</sub>[k], is updated for future use using the formula,</p><p id="p-0077" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mrow>  <mrow>   <mrow>    <msub>     <mi>p</mi>     <mi>i</mi>    </msub>    <mo>[</mo>    <mi>k</mi>    <mo>]</mo>   </mrow>   <mo>=</mo>   <mrow>    <mrow>     <msub>      <mi>p</mi>      <mi>i</mi>     </msub>     <mo>[</mo>     <mi>k</mi>     <mo>]</mo>    </mrow>    <mo>+</mo>    <mfrac>     <mrow>      <msubsup>       <mover>        <mi>r</mi>        <mo>^</mo>       </mover>       <mi>t</mi>       <mi>i</mi>      </msubsup>      <mo>[</mo>      <mi>k</mi>      <mo>]</mo>     </mrow>     <mrow>      <mrow>       <msub>        <mi>&#x3c4;</mi>        <mi>i</mi>       </msub>       <mo>[</mo>       <mi>k</mi>       <mo>]</mo>      </mrow>      <mo>+</mo>      <mn>1</mn>     </mrow>    </mfrac>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0078" num="0000">and in step <b>516</b>, the counter, &#x3c4;<sub>i</sub>[k], is reset to zero. In step <b>518</b>, the next vector element is analyzed until all vector elements have been processed and a complete predicted vector, {tilde over (r)}<sub>t</sub><sup>i</sup>, is produced.</p><p id="p-0079" num="0072">With reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, at time t, the vector elements of v<sub>t </sub><b>502</b> not included in sparse vector sparse vector, {circumflex over (r)}<sub>t </sub><b>503</b>, are elements <b>0</b>, <b>1</b>, and <b>3</b>. These elements are predicted by predictors <b>420</b>, or <b>422</b>, or both <b>420</b> and <b>422</b> as described above, using an algorithm such as the predictor method of <figref idref="DRAWINGS">FIG. <b>6</b></figref>. At time t, both the stored predicted values, &#x3c4;<sub>i</sub>, and the counter, &#x3c4;<sub>i</sub>, have values of zero so the predicted values are also zero and the predicted vector, {tilde over (r)}<sub>t</sub><sup>i </sup><b>504</b>, are equal to the sparse vector, {circumflex over (r)}<sub>t </sub><b>503</b>. At time t+1, vector element <b>4</b> is not part of sparse vector, {circumflex over (r)}<sub>t </sub><b>506</b>, however since a value for element <b>4</b> of sparse vector <b>503</b> was received at time t, element <b>4</b> may be predicted to yield a value of &#x2212;2.5, based on the time correlation of consecutive sparse vectors <b>503</b> and <b>506</b>. Finally, at time t+2, vector elements <b>1</b>, <b>2</b>, and <b>4</b> are not part of sparse vector, {circumflex over (r)}<sub>t </sub><b>509</b>, however since values for these elements were received previously, they may be predicted to yield values of 1.3, 9.3, and &#x2212;2.5, based on the time correlation of consecutive sparse vectors <b>503</b>, <b>506</b> and <b>510</b>.</p><p id="p-0080" num="0073"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a generalized method <b>700</b> for transmitting vector data between computer nodes, such as from a transmitting node <b>702</b> to a receiving node <b>704</b>, according to an embodiment. Transmitting computer node <b>702</b> obtains vector data <b>706</b> including a plurality of vector elements. Vector data <b>706</b> may be obtained by a momentum-SGD method where variations in vector elements over time have minimal changes between iterations or that consecutive vector elements are sufficiently correlated in time. The transmitting node <b>702</b> and the receiving node <b>704</b> communicate or are configured with a criteria, such as a value, K, which indicates the number of most significant vector elements of vector data <b>706</b> are to be transmitted. Transmitting node <b>702</b> selects the K-th most significant vector elements from vector data <b>706</b> to create Tx sparse vector data <b>708</b>. Tx sparse vector data <b>708</b> includes K vector elements with the remaining vector elements set to zero. Tx sparse vector data <b>708</b> may be encoded for transmission using a variety of methods including transmitting their position within vector data <b>706</b> and their value. Once encoded, the transmitting node <b>702</b> transmits the encoded sparse vector data <b>708</b> over a communications link to receiving node <b>704</b>. Receiving node <b>704</b> decodes the encoded sparse vector data <b>708</b> to obtain Rx sparse vector data <b>710</b> which includes the K most significant elements of vector data <b>706</b>. Independently, receiving node <b>704</b> or both receiving node <b>704</b> and transmitting node <b>702</b> make estimations of the non-transmitted elements <b>712</b> and <b>714</b> of vector data <b>706</b>. In embodiments, the estimation of non-transmitted elements may use methods that utilize the time correlation of vector elements between consecutive vectors, such as the method of <figref idref="DRAWINGS">FIG. <b>6</b></figref>. Transmitting node <b>702</b> obtains reconstructed vector <b>715</b> and receiving node <b>704</b> obtains reconstructed vector <b>713</b>. Both reconstructed vectors <b>713</b> and <b>715</b> include the K most significant vector elements of vector data <b>702</b> combined with estimations of the non-transmitted elements independent calculated by receiving node <b>704</b> and transmitting node <b>702</b>.</p><p id="p-0081" num="0074">In accordance with embodiments of the present disclosure, there is provided a method of communicating time correlated vector data within a network. The method includes reading, by a transmitting node, a first vector data including a plurality of elements. The transmitting node selects a subset of elements of the plurality of elements based on a criteria and sends the subset of elements to a receiving node. A receiving node receives the subset of elements and estimates a plurality of elements not included in the subset of elements based on a previously received subset of element based on a second vector data. The first vector data and the second vector data are part of a time series of vectors.</p><p id="p-0082" num="0075">In a further embodiment, the estimating the plurality of elements not included in the subset of elements includes updating, when one of the subset of elements is received, a predicted value of the one of the subset of elements and clearing a counter, and setting, when one of the subset of elements is not received, one of the plurality of elements not included in the subset of elements with the predicted value, and incrementing the counter.</p><p id="p-0083" num="0076">In a further embodiment, the criteria is an absolute value of each of the plurality of elements of the first vector data.</p><p id="p-0084" num="0077">In a further embodiment, the first vector data and the second vector data are update vectors as part of a machine learning model training process.</p><p id="p-0085" num="0078">Further embodiments include estimating, by the transmitting node, the plurality of elements not included in the subset of elements based on the previously received subset of element based on the second vector data. The transmitting node forms a reconstructed vector data including the subset of elements and the estimated plurality of elements not included in the subset of elements.</p><p id="p-0086" num="0079">In a further embodiment, the first vector data is obtained by combining an initial vector data with a weighted difference of the reconstructed vector data and the first vector data.</p><p id="p-0087" num="0080">In accordance with embodiments of the present disclosure, there is provided a network node for transmitting vector data over a network connection. The network node includes a processor and a non-transient memory for storing instructions which when executed by the processor cause the network node to read a first vector data including a plurality of elements, select a subset of elements of the plurality of elements based on a criteria, and send the subset of elements to a receiving node. The instructions further cause the network node to estimate a plurality of elements not included in the subset of elements based on a previously transmitted subset of elements based on a second vector data, and form a reconstructed vector data including the subset of elements and the estimated plurality of elements not included in the subset of elements.</p><p id="p-0088" num="0081">In a further embodiment, the estimating the plurality of elements not included in the subset of elements includes updating, when one of the subset of elements is received, a predicted value of the one of the subset of elements and clearing a counter, and setting, when one of the subset of elements is not received, one of the plurality of elements not included in the subset of elements with the predicted value, and incrementing the counter.</p><p id="p-0089" num="0082">In further embodiments, the criteria is an absolute value of each of the plurality of elements of the first vector data.</p><p id="p-0090" num="0083">In further embodiments, the first vector data and the second vector data are update vectors as part of a machine learning model training process.</p><p id="p-0091" num="0084">In further embodiments, the first vector data is obtained by combining an initial vector data with a weighted difference of the reconstructed vector data and the first vector data.</p><p id="p-0092" num="0085">In accordance with embodiments of the present disclosure, there is provided a network node for receiving vector data over a network connection. The network node includes a processor and a non-transient memory for storing instructions which when executed by the processor cause the network node to receive, from a transmitting node, a subset of elements of a first vector data and estimating a plurality of elements not included in the subset of elements based on a previously received subset of element based on a second vector data. The first vector data and the second vector data are part of a time series of vectors and the subset of elements selected by the transmitting node is based on a criteria. The receiving node also forms a reconstructed vector data including the subset of elements and the estimated plurality of elements not included in the subset of elements.</p><p id="p-0093" num="0086">In further embodiments, the estimating the plurality of elements not included in the subset of elements includes updating, when one of the subset of elements is received, a predicted value of the one of the subset of elements and clearing a counter, and setting, when one of the subset of elements is not received, one of the plurality of elements not included in the subset of elements with the predicted value, and incrementing the counter.</p><p id="p-0094" num="0087">In further embodiments, the criteria is an absolute value of each of the plurality of elements of the first vector data.</p><p id="p-0095" num="0088">In further embodiments, the first vector data and the second vector data are update vectors as part of a machine learning model training process.</p><p id="p-0096" num="0089">In further embodiments, the first vector data is obtained by combining an initial vector data with a weighted difference of the reconstructed vector data and the first vector data.</p><p id="p-0097" num="0090">Acts associated with the method described herein can be implemented as coded instructions in a computer program product. In other words, the computer program product is a computer-readable medium upon which software code is recorded to execute the method when the computer program product is loaded into memory and executed on the microprocessor of the wireless communication device.</p><p id="p-0098" num="0091">Further, each operation of the method may be executed on any computing device, such as a personal computer, server, PDA, or the like and pursuant to one or more, or a part of one or more, program elements, modules or objects generated from any programming language, such as C++, Java, or the like. In addition, each operation, or a file or object or the like implementing each said operation, may be executed by special purpose hardware or a circuit module designed for that purpose.</p><p id="p-0099" num="0092">Through the descriptions of the preceding embodiments, the present disclosure may be implemented by using hardware or by using software and a necessary universal hardware platform. Based on such understandings, the technical solution of the present disclosure may be embodied in the form of a software product. The software product may be stored in a non-volatile or non-transitory storage medium, which can be a compact disk read-only memory (CD-ROM), USB flash disk, or a removable hard disk. The software product includes a number of instructions that enable a computer device (personal computer, server, or network device) to execute the methods provided in the embodiments of the present disclosure. For example, such an execution may correspond to a simulation of the logical operations as described herein. The software product may additionally or alternatively include number of instructions that enable a computer device to execute operations for configuring or programming a digital logic apparatus in accordance with embodiments of the present disclosure.</p><p id="p-0100" num="0093">It will be appreciated that, although specific embodiments of the technology have been described herein for purposes of illustration, various modifications may be made without departing from the scope of the technology. The specification and drawings are, accordingly, to be regarded simply as an illustration of the disclosure as defined by the appended claims, and are contemplated to cover any and all modifications, variations, combinations or equivalents that fall within the scope of the present disclosure. In particular, it is within the scope of the technology to provide a computer program product or program element, or a program storage or memory device such as a magnetic or optical wire, tape or disc, or the like, for storing signals readable by a machine, for controlling the operation of a computer according to the method of the technology and/or to structure some or all of its components in accordance with the system of the technology.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230007095A1-20230105-M00001.NB"><img id="EMI-M00001" he="5.67mm" wi="76.20mm" file="US20230007095A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230007095A1-20230105-M00002.NB"><img id="EMI-M00002" he="5.67mm" wi="76.20mm" file="US20230007095A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230007095A1-20230105-M00003.NB"><img id="EMI-M00003" he="6.35mm" wi="76.20mm" file="US20230007095A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of communicating vector data within a network, the method comprising:<claim-text>obtaining, by a transmitting node, a first vector data including a plurality of elements;</claim-text><claim-text>selecting, by the transmitting node, a subset of elements of the plurality of elements;</claim-text><claim-text>sending, by the transmitting node, the subset of elements to a receiving node;</claim-text><claim-text>estimating, by the transmitting node, a plurality of elements not included in the subset of elements based on a previously transmitted subset of element; and</claim-text><claim-text>forming, by the transmitting node, a reconstructed vector data including the subset of elements and the estimated plurality of elements not included in the subset of elements.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the estimating the plurality of elements not included in the subset of elements comprises:<claim-text>updating, when one of the subset of elements is transmitted, a predicted value of the one of the subset of elements and resetting a counter; and</claim-text><claim-text>setting, when one of the subset of elements is not transmitted, one of the plurality of elements not included in the subset of elements with the predicted value, and incrementing the counter.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the subset of elements is selected based on a criteria, and the criteria is an absolute value of each of the plurality of elements of the first vector data.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the estimating is further based on a second vector data, and the first vector data and the second vector data are update vectors as part of a machine learning model training process.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein, the first vector data and the second vector data being part of a time series of vectors.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the first vector data is obtained by combining an initial vector data with a weighted difference of the reconstructed vector data and the first vector data.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref> wherein the subset of elements is selected based on a criteria, and the criteria is an absolute value of each of the plurality of elements of the first vector data.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method <claim-ref idref="CLM-00002">claim 2</claim-ref> wherein the estimating is further based on a second vector data, and the first vector data and the second vector data are update vectors as part of a machine learning model training process.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A network node for transmitting vector data over a network connection, the network node comprising:<claim-text>a processor and a non-transient memory for storing instructions which when executed by the processor cause the network node to:<claim-text>obtain a first vector data including a plurality of elements;</claim-text><claim-text>select a subset of elements of the plurality of elements;</claim-text><claim-text>send the subset of elements to a receiving node; and</claim-text><claim-text>estimate a plurality of elements not included in the subset of elements based on a previously transmitted subset of elements; and</claim-text><claim-text>forming a reconstructed vector data including the subset of elements and the estimated plurality of elements not included in the subset of elements.</claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The network node of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein the estimating the plurality of elements not included in the subset of elements comprises:<claim-text>updating, when one of the subset of elements is transmitted, a predicted value of the one of the subset of elements and resetting a counter; and</claim-text><claim-text>setting, when one of the subset of elements is not transmitted, one of the plurality of elements not included in the subset of elements with the predicted value, and incrementing the counter.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The network node of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein the subset of elements is selected based on a criteria, and the criteria is an absolute value of each of the plurality of elements of the first vector data.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The network node <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein the estimating is further based on a second vector data, and the first vector data and the second vector data are update vectors as part of a machine learning model training process.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The network node of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein the first vector data is obtained by combining an initial vector data with a weighted difference of the reconstructed vector data and the first vector data.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The network node of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein the first vector data is obtained by combining an initial vector data with a weighted difference of the reconstructed vector data and the first vector data.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The network node of <claim-ref idref="CLM-00010">claim 10</claim-ref> wherein the subset of elements is selected based on a criteria, and the criteria is an absolute value of each of the plurality of elements of the first vector data.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A network node for receiving vector data over a network connection, the network node comprising:<claim-text>a processor and a non-transient memory for storing instructions which when executed by the processor cause the network node to:<claim-text>receive, from a transmitting node, a subset of elements of a first vector data;</claim-text><claim-text>estimate a plurality of elements not included in the subset of elements based on a previously received subset of element based on a second vector data, the first vector data and the second vector data being part of a time series of vectors, the subset of elements selected by the transmitting node; and</claim-text><claim-text>form a reconstructed vector data including the subset of elements and the estimated plurality of elements not included in the subset of elements.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The network node of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the estimating the plurality of elements not included in the subset of elements comprises:<claim-text>updating, when one of the subset of elements is received, a predicted value of the one of the subset of elements and resetting a counter; and</claim-text><claim-text>setting, when one of the subset of elements is not received, one of the plurality of elements not included in the subset of elements with the predicted value, and incrementing the counter.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The network node of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the subset of elements is selected based on a criteria, and the criteria is an absolute value of each of the plurality of elements of the first vector data.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The network node <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the first vector data and the second vector data are update vectors as part of a machine learning model training process.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The network node of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the first vector data is obtained by combining an initial vector data with a weighted difference of the reconstructed vector data and the first vector data.</claim-text></claim></claims></us-patent-application>