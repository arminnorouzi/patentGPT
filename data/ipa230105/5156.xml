<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005157A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005157</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17791155</doc-number><date>20201210</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2020-002735</doc-number><date>20200110</date></priority-claim><priority-claim sequence="02" kind="national"><country>JP</country><doc-number>2020-201982</doc-number><date>20201204</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>13</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>13</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30164</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10061</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e79">PATTERN-EDGE DETECTION METHOD, PATTERN-EDGE DETECTION APPARATUS, AND STORAGE MEDIUM STORING PROGRAM FOR CAUSING A COMPUTER TO PERFORM PATTERN-EDGE DETECTION</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>TASMIT, INC.</orgname><address><city>Yokohama</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>OKAMOTO</last-name><first-name>Yosuke</first-name><address><city>Yokohama</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/046010</doc-number><date>20201210</date></document-id><us-371c12-date><date>20220706</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present invention relates to a method of detecting an edge (or a contour line) of a pattern, which is formed on a workpiece (e.g., a wafer or a mask) for use in manufacturing of semiconductor, from an image generated by a scanning electron microscope. The pattern-edge detection method includes: generating an objective image of a target pattern formed on a workpiece; generating a feature vector representing features of each pixel constituting the objective image; inputting the feature vector to a model constructed by machine learning; outputting, from the model, a determination result indicating whether the pixel having the feature vector is an edge pixel or a non-edge pixel; and connecting a plurality of pixels, each having a feature vector that has obtained a determination result indicating an edge pixel, with a line to generate a virtual edge.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="178.14mm" wi="143.09mm" file="US20230005157A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="196.34mm" wi="145.12mm" file="US20230005157A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="229.79mm" wi="93.56mm" file="US20230005157A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="225.98mm" wi="139.11mm" file="US20230005157A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="107.44mm" wi="136.91mm" file="US20230005157A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="220.30mm" wi="126.15mm" file="US20230005157A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="229.02mm" wi="112.18mm" file="US20230005157A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="223.77mm" wi="131.83mm" file="US20230005157A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="184.83mm" wi="140.55mm" file="US20230005157A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="229.36mm" wi="124.71mm" file="US20230005157A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="205.66mm" wi="114.89mm" file="US20230005157A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="174.50mm" wi="130.73mm" file="US20230005157A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="227.08mm" wi="96.18mm" file="US20230005157A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="101.01mm" wi="92.29mm" file="US20230005157A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="176.36mm" wi="118.11mm" orientation="landscape" file="US20230005157A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="149.52mm" wi="117.77mm" file="US20230005157A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="153.59mm" wi="117.86mm" file="US20230005157A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="211.16mm" wi="114.38mm" file="US20230005157A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="173.57mm" wi="123.95mm" file="US20230005157A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="171.37mm" wi="96.94mm" file="US20230005157A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present invention relates to a method and an apparatus for detecting an edge (or a contour line) of a pattern, which is formed on a workpiece (e.g., a wafer or a mask) for use in manufacturing of semiconductor, from an image generated by a scanning electron microscope. The present invention further relates to a program for causing a computer to perform such pattern-edge detection.</p><p id="p-0003" num="0002">The present invention further relates to a method and an apparatus for creating an edge detection model by machine learning.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0004" num="0003">Conventionally, an edge (or a contour line) of a pattern formed on a workpiece, such as a wafer, is detected as follows. First, an image of the pattern on the workpiece is generated by a scanning electron microscope. Next, a CAD pattern is generated from design data (also referred to as CAD data) for the pattern, and the CAD pattern is superimposed on the pattern on the image. The CAD pattern is a virtual pattern created based on the design information (position, length, size, etc.) of the pattern included in the design data.</p><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a schematic diagram showing a CAD pattern <b>505</b> superimposed on a pattern <b>501</b> on an image <b>500</b>. As shown in <figref idref="DRAWINGS">FIG. <b>25</b></figref>, a computer generates multiple search lines <b>507</b> each extending in a normal direction with respect to an edge of the CAD pattern <b>505</b>, and creates brightness profiles of the image <b>500</b> along these search lines <b>507</b>, respectively. In <figref idref="DRAWINGS">FIG. <b>25</b></figref>, only a part of the multiple search lines <b>507</b> is depicted in order to simplify the drawing.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a diagram showing the brightness profile along the search line shown in <figref idref="DRAWINGS">FIG. <b>25</b></figref>. Vertical axis of <figref idref="DRAWINGS">FIG. <b>26</b></figref> represents brightness value, and horizontal axis represents position on the search line <b>507</b>. The computer detects an edge point <b>510</b> where the brightness value on the brightness profile is equal to a threshold value. The computer repeats the same operation to determine multiple edge points on the brightness profiles along all search lines <b>507</b>. A line connecting the multiple edge points is determined to be the edge of the pattern <b>501</b> on the image <b>500</b>.</p><p id="p-0007" num="0006">However, in examples shown in <figref idref="DRAWINGS">FIGS. <b>27</b> to <b>29</b></figref>, a pattern edge on the image <b>500</b> may not be correctly determined (detected). Specifically, in the example shown in <figref idref="DRAWINGS">FIG. <b>27</b></figref>, a part of the edge of the pattern <b>501</b> is missing, and the edge of the pattern <b>501</b> does not exist on a search line <b>507</b> perpendicular to the CAD pattern <b>505</b>. In the example shown in <figref idref="DRAWINGS">FIG. <b>28</b></figref>, the edge of the CAD pattern <b>505</b> is far from the edge of the pattern <b>501</b> on the image, and as a result, the edge of the pattern <b>501</b> does not exist on a search line <b>507</b>. In the example shown in <figref idref="DRAWINGS">FIG. <b>29</b></figref>, an edge of a pattern <b>510</b>, which does not exist in the CAD pattern <b>505</b>, cannot be detected by the conventional method using the search lines <b>507</b>.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIGS. <b>27</b> to <b>29</b></figref> show examples of pattern defects, and it is important to detect the edges of such defective patterns. However, the real pattern may deviate from the design data, and the conventional method using the design data may not be able to correctly detect the edge of the defective pattern.</p><p id="p-0009" num="0008">On the other hand, a technique for detecting an edge of a pattern formed on a workpiece, such as a wafer, using a model created by machine learning has been developed. In this technique, an edge detection model (trained model) determines whether or not each pixel of an image of a pattern constitutes a pattern edge.</p><p id="p-0010" num="0009">The edge detection model is created by the machine learning (for example, deep learning, decision tree learning, etc.) using training data prepared in advance. The training data includes a pattern image generated by a scanning electron microscope and correct answer data of each pixel constituting the pattern image. The correct answer data is information that identifies each pixel as either a pixel that constitutes an edge of a pattern or a pixel that does not constitute an edge. The machine learning using such training data is performed so as to optimize parameters (weighting factors, etc.) constituting the edge detection model.</p><p id="p-0011" num="0010">However, the pattern edge used for the training data has deviations. Moreover, a boundary line between the edge and a non-edge region on the image is unclear. The edge detection model created using such training data may fail to detect edges or may erroneously detect edges. Creating an accurate edge detection model requires a large amount of training data for the machine learning, and as a result, the machine learning takes a very long time.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Literature</heading><p id="p-0012" num="0011">Patent document 1: Japanese laid-open patent publication No. 2003-178314</p><p id="p-0013" num="0012">Patent document 2: Japanese laid-open patent publication No. 2013-98267</p><p id="p-0014" num="0013">Patent document 3: Japanese laid-open patent publication No. 2020-140518</p><heading id="h-0005" level="1">SUMMARY OF INVENTION</heading><heading id="h-0006" level="1">Technical Problem</heading><p id="p-0015" num="0014">The present invention provides a pattern-edge detection method and a pattern-edge detection apparatus capable of detecting an edge (or a contour line) of a pattern on an image without using pattern design data.</p><p id="p-0016" num="0015">The present invention further provides a method and an apparatus capable of creating an accurate edge detection model without requiring a long time on machine learning.</p><heading id="h-0007" level="1">Solution to Problem</heading><p id="p-0017" num="0016">In an embodiment, there is provided a pattern-edge detection method comprising: generating an objective image of a target pattern formed on a workpiece; generating a feature vector representing features of each pixel constituting the objective image; inputting the feature vector to a model constructed by machine learning; outputting, from the model, a determination result indicating whether the pixel having the feature vector is an edge pixel or a non-edge pixel; and connecting a plurality of pixels, each having a feature vector that has obtained a determination result indicating an edge pixel, with a line to generate a virtual edge.</p><p id="p-0018" num="0017">In an embodiment, the model is a decision tree.</p><p id="p-0019" num="0018">In an embodiment, the pattern-edge detection method further comprises: selecting training patterns from design data; generating training images of real patterns that have been produced based on the training patterns, respectively; detecting edges of the real patterns on the training images; classifying reference pixels constituting the training images into first reference pixels each constituting an edge and second reference pixels that do not constitute an edge; generating first feature vectors of the first reference pixels and second feature vectors of the second reference pixels; and constructing the model by the machine learning using training data including the first feature vectors and the second feature vectors.</p><p id="p-0020" num="0019">In an embodiment, the training patterns include a plurality of patterns having at least an edge extending in a first direction, an edge extending in a second direction perpendicular to the first direction, a corner edge, and a terminal edge.</p><p id="p-0021" num="0020">In an embodiment, the real patterns are patterns formed on the workpiece.</p><p id="p-0022" num="0021">In an embodiment, selecting the training patterns from the design data comprises: displaying, on a display screen, a design diagram including a plurality of patterns drawn based on the design data; and displaying, on the display screen, the training patterns selected from the plurality of patterns included in the design diagram or an area where the training patterns are located in a visually emphasized manner.</p><p id="p-0023" num="0022">In an embodiment, the pattern-edge detection method further comprises: generating multiple brightness profiles of the objective image along multiple search lines each extending in a normal direction with respect to the virtual edge; determining multiple edge points based on the multiple brightness profiles; and generating a renewed edge by connecting the multiple edge points with a line.</p><p id="p-0024" num="0023">In an embodiment, the pattern-edge detection method further comprises: generating, from the design data, a CAD pattern corresponding to the target pattern; and measuring a distance from an edge of the CAD pattern to the renewed edge.</p><p id="p-0025" num="0024">In an embodiment, there is provided a pattern-edge detection apparatus comprising: an image generating device configured to generate an objective image of a target pattern formed on a workpiece; and an arithmetic system coupled to the image generating device, the arithmetic system being configured to: generate a feature vector representing features of each pixel constituting the objective image; input the feature vector to a model constructed by machine learning; output, from the model, a determination result indicating whether the pixel having the feature vector is an edge pixel or a non-edge pixel; and connect a plurality of pixels, each having a feature vector that has obtained a determination result indicating an edge pixel, with a line to generate a virtual edge.</p><p id="p-0026" num="0025">In an embodiment, the model is a decision tree.</p><p id="p-0027" num="0026">In an embodiment, the arithmetic system is configured to: select training patterns from design data; generate training images of real patterns that have been produced based on the training patterns, respectively; detect edges of the real patterns on the training images; classify reference pixels constituting the training images into first reference pixels each constituting an edge and second reference pixels that do not constitute an edge; generate first feature vectors of the first reference pixels and second feature vectors of the second reference pixels; and construct the model by the machine learning using training data including the first feature vectors and the second feature vectors.</p><p id="p-0028" num="0027">In an embodiment, the training patterns include a plurality of patterns having at least an edge extending in a first direction, an edge extending in a second direction perpendicular to the first direction, a corner edge, and a terminal edge.</p><p id="p-0029" num="0028">In an embodiment, the real patterns are patterns formed on the workpiece.</p><p id="p-0030" num="0029">In an embodiment, the arithmetic system includes a display screen, and the arithmetic system is configured to: display, on the display screen, a design diagram including a plurality of patterns drawn based on the design data; and display, on the display screen, the training patterns selected from the plurality of patterns included in the design diagram or an area where the training patterns are located in a visually emphasized manner.</p><p id="p-0031" num="0030">In an embodiment, the arithmetic system is configured to: generate multiple brightness profiles of the objective image along multiple search lines each extending in a normal direction with respect to the virtual edge; determine multiple edge points based on the multiple brightness profiles; and generate a renewed edge by connecting the multiple edge points with a line.</p><p id="p-0032" num="0031">In an embodiment, the arithmetic system is configured to: generate, from the design data, a CAD pattern corresponding to the target pattern; and measure a distance from an edge of the CAD pattern to the renewed edge.</p><p id="p-0033" num="0032">In an embodiment, there is provided a computer-readable storage medium storing a program therein for causing a computer to: instruct a scanning electron microscope to generate an objective image of a target pattern formed on a workpiece; generate a feature vector representing features of each pixel constituting the objective image; input the feature vector to a model constructed by machine learning; output, from the model, a determination result indicating whether the pixel having the feature vector is an edge pixel or a non-edge pixel; and connect a plurality of pixels, each having a feature vector that has obtained a determination result indicating an edge pixel, with a line to generate a virtual edge.</p><p id="p-0034" num="0033">In an embodiment, the model is a decision tree.</p><p id="p-0035" num="0034">In an embodiment, the program is further configured to cause the computer to: select training patterns from design data; instruct the scanning electron microscope to generate training images of real patterns that have been produced based on the training patterns, respectively; detect edges of the real patterns on the training images; classify reference pixels constituting the training images into first reference pixels each constituting an edge and second reference pixels that do not constitute an edge; generate first feature vectors of the first reference pixels and second feature vectors of the second reference pixels; and construct the model by the machine learning using training data including the first feature vectors and the second feature vectors.</p><p id="p-0036" num="0035">In an embodiment, the training patterns include a plurality of patterns having at least an edge extending in a first direction, an edge extending in a second direction perpendicular to the first direction, a corner edge, and a terminal edge.</p><p id="p-0037" num="0036">In an embodiment, the real patterns are patterns formed on the workpiece.</p><p id="p-0038" num="0037">In an embodiment, selecting the training patterns from the design data comprises: displaying, on a display screen, a design diagram including a plurality of patterns drawn based on the design data; and displaying, on the display screen, the training patterns selected from the plurality of patterns included in the design diagram or an area where the training patterns are located in a visually emphasized manner.</p><p id="p-0039" num="0038">In an embodiment, the program is further configured to cause the computer to: generate multiple brightness profiles of the objective image along multiple search lines each extending in a normal direction with respect to the virtual edge; determine multiple edge points based on the multiple brightness profiles; and generate a renewed edge by connecting the multiple edge points with a line.</p><p id="p-0040" num="0039">In an embodiment, the program is further configured to cause the computer to: generate, from the design data, a CAD pattern corresponding to the target pattern; and measure a distance from an edge of the CAD pattern to the renewed edge.</p><p id="p-0041" num="0040">In an embodiment, there is provided a method of producing an edge detection model for detecting an edge of a pattern on an image, comprising: generating a training image of a workpiece having a pattern formed thereon with a scanning electron microscope; detecting an edge of the pattern on the training image; calculating feature vectors of pixels constituting the training image; dividing a target area in the training image into an edge region, a near-edge region, and a non-edge region; producing training data including feature vectors of first pixels in the edge region, feature vectors of second pixels in the near-edge region, and feature vectors of third pixels in the non-edge region; and producing the edge detection model by machine learning using the training data.</p><p id="p-0042" num="0041">In an embodiment, where the number of first pixels is denoted by A, and a sum of the number of second pixels and the number of third pixels is denoted by B, a value (A/B) obtained by dividing the number A by the number B is a predetermined numerical value.</p><p id="p-0043" num="0042">In an embodiment, the value (A/B) obtained by dividing the number A by the number B is in a range of 0.6 to 1.5.</p><p id="p-0044" num="0043">In an embodiment, the non-edge region is located away from the edge region by a predetermined number of pixels, and the near-edge region is located between the edge region and the non-edge region.</p><p id="p-0045" num="0044">In an embodiment, dividing the target region in the training image into the edge region, the near-edge region, and the non-edge region comprises dividing the target region in the training image into an edge region, an exclusion region, a near-edge region, and a non-edge region; the exclusion region is adjacent to the edge region and is located between the edge region and the near-edge region; and the training data does not include feature vectors of pixels in the exclusion region.</p><p id="p-0046" num="0045">In an embodiment, the target region includes a first region including a first edge, a second region including a second edge perpendicular to the first edge, and a third region including a corner edge and a terminal edge.</p><p id="p-0047" num="0046">In an embodiment, the number of pixels in the first region, the number of pixels in the second region, and the number of pixels in the third region are in a predetermined ratio.</p><p id="p-0048" num="0047">In an embodiment, there is provided a model generating apparatus for producing an edge detection model for detecting an edge of a pattern on an image, comprising: a memory storing a program for producing the edge detection model; and an arithmetic device configured to perform arithmetic operations according to instructions included in the program, wherein the model generating apparatus is configured to: obtain, from a scanning electron microscope, a training image of a workpiece having a pattern formed thereon; detect an edge of the pattern on the training image; calculate feature vectors of pixels constituting the training image; divide a target area in the training image into an edge region, a near-edge region, and a non-edge region; produce training data including feature vectors of first pixels in the edge region, feature vectors of second pixels in the near-edge region, and feature vectors of third pixels in the non-edge region; and produce the edge detection model by machine learning using the training data.</p><p id="p-0049" num="0048">In an embodiment, where the number of first pixels is denoted by A, and a sum of the number of second pixels and the number of third pixels is denoted by B, a value (A/B) obtained by dividing the number A by the number B is a predetermined numerical value.</p><p id="p-0050" num="0049">In an embodiment, the value (A/B) obtained by dividing the number A by the number B is in a range of 0.6 to 1.5.</p><p id="p-0051" num="0050">In an embodiment, the non-edge region is located away from the edge region by a predetermined number of pixels, and the near-edge region is located between the edge region and the non-edge region.</p><p id="p-0052" num="0051">In an embodiment, the model generating apparatus is configured to divide the target region in the training image into an edge region, an exclusion region, a near-edge region, and a non-edge region; the exclusion region is adjacent to the edge region and is located between the edge region and the near-edge region; and the training data does not include feature vectors of pixels in the exclusion region.</p><p id="p-0053" num="0052">In an embodiment, the target region includes a first region including a first edge, a second region including a second edge perpendicular to the first edge, and a third region including a corner edge and a terminal edge.</p><p id="p-0054" num="0053">In an embodiment, the number of pixels in the first region, the number of pixels in the second region, and the number of pixels in the third region are in a predetermined ratio.</p><p id="p-0055" num="0054">In an embodiment, there is provided a computer-readable storage medium storing a program therein for causing a computer to: obtain, from a scanning electron microscope, a training image of a workpiece having a pattern formed thereon; detect an edge of the pattern on the training image; calculate feature vectors of pixels constituting the training image; divide a target area in the training image into an edge region, a near-edge region, and a non-edge region; produce training data including feature vectors of first pixels in the edge region, feature vectors of second pixels in the near-edge region, and feature vectors of third pixels in the non-edge region; and produce an edge detection model by machine learning using the training data.</p><heading id="h-0008" level="1">Advantageous Effects of Invention</heading><p id="p-0056" num="0055">According to the present invention, the edge is detected using the model created by the machine learning, instead of pattern design data. Specifically, the virtual edge is generated based on the determination result output from the model. This virtual edge is expected to have a shape very close to the edge of the pattern appearing on the image.</p><p id="p-0057" num="0056">According to the present invention, the training data including pixels in the edge region, pixels in the near-edge region, and pixels in the non-edge region is used for the machine learning. In particular, the training data includes the pixels in the near-edge region which are considered difficult to be determined. Therefore, the edge detection model created by the machine learning can accurately determine whether or not a given pixel constitutes an edge.</p><p id="p-0058" num="0057">Further, according to the present invention, the pixels in the exclusion region are not used for the machine learning. The pixels in this exclusion region may be edge pixels or non-edge pixels. In other words, the pixels in the exclusion region are uncertain pixels. By excluding feature vectors of such uncertain pixels from the training data, the machine learning of the edge detection model can be completed in a shorter time.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram showing an embodiment of a pattern-edge detection apparatus.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram showing an objective image.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram showing a virtual edge.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram showing an embodiment of a model constituted of a decision tree.</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram showing an example of a determination result when a certain feature vector is input to a plurality of decision trees shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram showing another example of a determination result when another feature vector is input to the plurality of decision trees shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram showing multiple search lines each extending in a normal direction with respect to a virtual edge.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram showing an example of a brightness profile along one of the search lines shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic diagram showing an example of a training pattern used for producing training data.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a schematic diagram showing an example of a design diagram displayed on a display screen.</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram showing an embodiment in which a frame indicating an area where a selected training pattern is located is displayed on a display screen.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram showing an embodiment in which the selected training pattern is displayed in a manner visually different from other patterns.</p><p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a schematic diagram showing training data including a first feature vector of a first reference pixel constituting an edge and a second feature vector of a second reference pixel that does not constitute an edge.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a part of a flowchart showing the operation of the pattern-edge detection apparatus.</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a remaining part of the flowchart showing the operation of the pattern-edge detection apparatus.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a diagram illustrating an embodiment of measuring a distance from an edge of a CAD pattern to a renewed edge.</p><p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a schematic diagram showing an embodiment of a pattern-edge detection apparatus.</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram showing an example of an image of a workpiece on which a pattern is formed.</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a diagram showing a detected edge.</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a diagram in which the detected edge shown in <figref idref="DRAWINGS">FIG. <b>19</b></figref> is superimposed on the image shown in <figref idref="DRAWINGS">FIG. <b>18</b></figref>.</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a diagram illustrating an embodiment of calculating a feature of a pixel.</p><p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a diagram explaining an operation of dividing a target area in an image into an edge region, a near-edge region, and a non-edge region.</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a diagram explaining another embodiment of creating an edge detection model.</p><p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a diagram showing an example of a target area including a plurality of areas set in an image.</p><p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a schematic diagram showing a CAD pattern superimposed on a pattern on an image.</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a diagram showing a brightness profile along a search line shown in <figref idref="DRAWINGS">FIG. <b>25</b></figref>.</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a diagram showing an example of a defective pattern.</p><p id="p-0086" num="0085"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is a diagram showing another example of a defective pattern.</p><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a diagram showing still another example of a defective pattern.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0088" num="0087">Hereinafter, embodiments of the present invention will be described with reference to the drawings.</p><p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram showing an embodiment of a pattern-edge detection apparatus. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the pattern-edge detection apparatus includes a scanning electron microscope <b>1</b> and an arithmetic system <b>3</b>. The scanning electron microscope <b>1</b> is an example of an image generating device that generates an image of a workpiece. Examples of workpiece include wafer and mask involved in manufacturing of a semiconductor. In the embodiments described below, a wafer is adopted as an example of the workpiece, but the present invention is not limited to the following embodiments. A pattern is an interconnect pattern of an electronic device formed on the workpiece.</p><p id="p-0090" num="0089">The scanning electron microscope <b>1</b> is coupled to the arithmetic system <b>3</b>, and operations of the scanning electron microscope <b>1</b> are controlled by the arithmetic system <b>3</b>. The calculation system <b>3</b> includes a memory <b>6</b> in which a database <b>5</b> and programs are stored, a processor <b>7</b> configured to perform arithmetic operations according to instructions included in the programs, and a display screen <b>10</b> configured to display an image and a GUI (graphical user interface). The memory <b>6</b> includes a main memory, such as RAM, and an auxiliary memory, such as a hard disk drive (HDD) or a solid state drive (SSD). Examples of the processor <b>7</b> include a CPU (central processing unit) and a GPU (graphic processing unit). However, the specific configuration of the arithmetic system <b>3</b> is not limited to these examples.</p><p id="p-0091" num="0090">The arithmetic system <b>3</b> further includes an input device <b>12</b> including a mouse <b>12</b><i>a </i>and a keyboard <b>12</b><i>b. </i>A user can manipulate the GUI appearing on the display screen <b>10</b> by using the mouse <b>12</b><i>a </i>and/or the keyboard <b>12</b><i>b. </i>The input device <b>12</b> including the mouse <b>12</b><i>a </i>and the keyboard <b>12</b><i>b </i>is an example, and the present invention is not limited to the input device <b>12</b> of the present embodiment.</p><p id="p-0092" num="0091">The arithmetic system <b>3</b> includes at least one computer. For example, the arithmetic system <b>3</b> may be an edge server coupled to the scanning electron microscope <b>1</b> by a communication line, or may be a cloud server coupled to the scanning electron microscope <b>1</b> by a communication network, such as the Internet or a local area network. The arithmetic system <b>3</b> may be a combination of a plurality of servers. For example, the arithmetic system <b>3</b> may be a combination of an edge server and a cloud server coupled to each other by a communication network, such as the Internet or a local area network, or a combination of a plurality of servers which are not connected by a communication network.</p><p id="p-0093" num="0092">The scanning electron microscope <b>1</b> includes an electron gun <b>15</b> configured to emit an electron beam composed of primary electrons (charged particles), a converging lens <b>16</b> configured to converge the electron beam emitted by the electron gun <b>15</b>, an X deflector <b>17</b> configured to deflect the electron beam in an X direction, a Y deflector <b>18</b> configured to deflect the electron beam in a Y direction, and an objective lens <b>20</b> configured to focus the electron beam on a wafer W which is an example of a workpiece.</p><p id="p-0094" num="0093">The converging lens <b>16</b> and the objective lens <b>20</b> are coupled to a lens controller <b>21</b>, so that operations of the converging lens <b>16</b> and the objective lens <b>20</b> are controlled by the lens controller <b>21</b>. The lens controller <b>21</b> is coupled to the arithmetic system <b>3</b>. The X deflector <b>17</b> and the Y deflector <b>18</b> are coupled to a deflection controller <b>22</b>, so that deflecting operations of the X deflector <b>17</b> and the Y deflector <b>18</b> are controlled by the deflection controller <b>22</b>. The deflection controller <b>22</b> is also coupled to the arithmetic system <b>3</b> as well. A secondary-electron detector <b>25</b> and a backscattered-electron detector <b>26</b> are coupled to an image acquisition device <b>28</b>. The image acquisition device <b>28</b> is configured to convert output signals of the secondary-electron detector <b>25</b> and the backscattered-electron detector <b>26</b> into image(s). The image acquisition device <b>28</b> is also coupled to the arithmetic system <b>3</b> as well.</p><p id="p-0095" num="0094">A stage <b>31</b>, which is arranged in a chamber <b>30</b>, is coupled to a stage controller <b>32</b>, so that a position of the stage <b>31</b> is controlled by the stage controller <b>32</b>. The stage controller <b>32</b> is coupled to the arithmetic system <b>3</b>. A transporting device <b>34</b> for placing the wafer W onto the stage <b>31</b> in the chamber <b>30</b> is also coupled to the arithmetic system <b>3</b>.</p><p id="p-0096" num="0095">The electron beam emitted by the electron gun <b>15</b> is converged by the converging lens <b>16</b> and then focused by the objective lens <b>20</b> on the surface of the wafer W, while the electron beam is deflected by the X deflector <b>17</b> and the Y deflector <b>18</b>. When the wafer W is irradiated with the primary electrons of the electron beam, the secondary electrons and the backscattered electrons are emitted from the wafer W. The secondary electrons are detected by the secondary-electron detector <b>25</b>, and the backscattered electrons are detected by the backscattered-electron detector <b>26</b>. Signals of the detected secondary electrons and signals of the detected backscattered electron are input to the image acquisition device <b>28</b> and converted into image(s). The image is transmitted to the arithmetic system <b>3</b>.</p><p id="p-0097" num="0096">A design data for a pattern formed on the wafer W is stored in advance in the memory <b>6</b>. The pattern on the wafer W is produced based on the design data. The design data for the pattern includes pattern design information, such as coordinates of vertices of the pattern, position, shape, and size of the pattern, and the number of the layer to which the pattern belongs. The database <b>5</b> is constructed in the memory <b>6</b>. The pattern design data is stored in the database <b>5</b> in advance. The arithmetic system <b>3</b> can read out the design data from the database <b>5</b> stored in the memory <b>6</b>. The design data is also called CAD data. CAD is an abbreviation for computer-aided design.</p><p id="p-0098" num="0097">Next, a method of detecting an edge (or a contour line) of the pattern on the image will be described. First, the scanning electron microscope <b>1</b> generates a plurality of images of a plurality of patterns formed on the wafer W. The arithmetic system <b>3</b> acquires an objective image, which is one of the plurality of images, from the scanning electron microscope <b>1</b>. <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram showing the objective image. A target pattern <b>51</b> having an edge to be detected appears on the objective image <b>50</b>. This target pattern <b>51</b> is a pattern formed on the wafer W.</p><p id="p-0099" num="0098">The arithmetic system <b>3</b> generates a feature vector representing a plurality of features of each pixel of the objective image <b>50</b>. The feature vector is a multidimensional vector including a plurality of features of each pixel. The features are numerical values representing a characteristic of each pixel. In the present embodiment, the plurality of features of each pixel are differences between a brightness value of that pixel and brightness values of other pixels. The brightness value is, in one example, a discrete numerical value ranging from <b>0</b> to <b>255</b> according to a gray scale. In this embodiment, the other pixels are adjacent pixels. In one embodiment, the other pixels may be non-adjacent pixels.</p><p id="p-0100" num="0099">The arithmetic system <b>3</b> is configured to apply a differential filter to the objective image <b>50</b> to generate the feature vector including the plurality of features. Specifically, the arithmetic system <b>3</b> calculates differences between a brightness value of a pixel and brightness values of pixels existing around that pixel. These calculated differences constitute the plurality of features included in one feature vector.</p><p id="p-0101" num="0100">For example, when a brightness value of a pixel P<b>1</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> is 100 and brightness values of a plurality of pixels existing around the pixel P<b>1</b> are 200, 150, 100, 50, the calculated differences are &#x2212;100, &#x2212;50, 0, 50. Therefore, the feature vector of the pixel P<b>1</b> in this example is expressed as (&#x2212;100, &#x2212;50, 0, 50). In another example, when a brightness value of a pixel P<b>2</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> is 10, and brightness values of a plurality of pixels existing around the pixel P<b>2</b> are 20, 15, 10, and 10, the calculated differences are &#x2212;10, &#x2212;5, 0, 0. Therefore, the feature vector of the pixel P<b>2</b> in this example is expressed as (&#x2212;10, &#x2212;5, 0, 0).</p><p id="p-0102" num="0101">In the present embodiment, the number of features included in the feature vector is four, but the present invention is not limited to this embodiment. The feature vector may contain feature(s) less than four or greater than four.</p><p id="p-0103" num="0102">The arithmetic system <b>3</b> inputs the plurality of features constituting the feature vector into a model constructed according to the machine learning, and outputs from the model a determination result indicating an edge pixel or a non-edge pixel. This model is a trained model or tuned model that has been produced according to the machine learning using training data. The training data includes feature vectors of a plurality of pixels and correct answer data of these feature vectors. The correct answer data includes information that identifies a pixel having a certain feature vector as either a pixel that constitutes an edge of a pattern or a pixel that does not constitute an edge. Each of the feature vectors of the plurality of pixels included in the training data is associated with (or connected to) the correct answer data.</p><p id="p-0104" num="0103">The model created by the machine learning using such training data can determine whether an unknown pixel is an edge pixel or a non-edge pixel from a feature vector of the unknown pixel. Specifically, the feature vector of the unknown pixel is input to the model, and the model outputs the determination result indicating an edge pixel or a non-edge pixel.</p><p id="p-0105" num="0104">The arithmetic system <b>3</b> selects a plurality of pixels each having a feature vector that has obtained a determination result indicating an edge pixel, and connects the plurality of selected pixels with a line to generate a virtual edge. <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram showing the virtual edge. The arithmetic system <b>3</b> forms a virtual edge <b>55</b> by connecting a plurality of pixels PX, each having a feature vector that has obtained a determination result indicating an edge pixel, with a line. The virtual edge <b>55</b> is expected to have a shape close to the edge of the target pattern <b>51</b> (see <figref idref="DRAWINGS">FIG. <b>2</b></figref>) on the wafer W.</p><p id="p-0106" num="0105">In this embodiment, a decision tree is used as the model. The decision tree is a model (trained model or tuned model) constructed according to a random forest algorithm, which is an example of machine learning algorithm.</p><p id="p-0107" num="0106"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram showing an embodiment of the model constituted of the decision tree. As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a model <b>60</b> includes a plurality of decision trees <b>60</b>A, <b>60</b>B, <b>60</b>C. The feature vector of each pixel is input to each of these decision trees <b>60</b>A, <b>60</b>B, <b>60</b>C. The plurality of decision trees <b>60</b>A, <b>60</b>B, and <b>60</b>C determine whether the pixel having the feature vector is an edge pixel or a non-edge pixel according to the algorithm of each decision tree. In the example shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the model <b>60</b> includes three decision trees <b>60</b>A, <b>60</b>B, and <b>60</b>C, but the number of decision trees is not particularly limited. In one embodiment, the model <b>60</b> may include only one decision tree.</p><p id="p-0108" num="0107"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram showing an example of the determination result when the feature vector (&#x2212;100, &#x2212;50, 0, 50) is input to the plurality of decision trees <b>60</b>A, <b>60</b>B, <b>60</b>C shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The feature vector (&#x2212;100, &#x2212;50, 0, 50) is input to each of the three decision trees <b>60</b>A, <b>60</b>B, <b>60</b>C. The first decision tree <b>60</b>A and the second decision tree <b>60</b>B determine that the pixel having the feature vector (&#x2212;100, &#x2212;50, 0, 50) is an edge pixel, and the third decision tree <b>60</b>C determines that the pixel having the feature vector (&#x2212;100, &#x2212;50, 0, 50) is a non-edge pixel.</p><p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram showing an example of the determination result when the feature vector (-<b>10</b>, -<b>5</b>, <b>0</b>, <b>0</b>) is input to the plurality of decision trees <b>60</b>A, <b>60</b>B, <b>60</b>C shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The feature vector (&#x2212;10, &#x2212;5, 0, 0) is input to each of the three decision trees <b>60</b>A, <b>60</b>B, <b>60</b>C. All the decision trees <b>60</b>A, <b>60</b>B, <b>60</b>C determine that the pixel having the feature vector (&#x2212;10, &#x2212;5, 0, 0) is a non-edge pixel.</p><p id="p-0110" num="0109">There are as many determination results as the number of decision trees <b>60</b>A, <b>60</b>B, <b>60</b>C, and the determination results may differ depending on the decision trees <b>60</b>A, <b>60</b>B, <b>60</b>C. The arithmetic system <b>3</b> adopts the determination result indicating an edge pixel or the determination result indicating a non-edge pixel, whichever has a larger number. In the example shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, two of the three decision trees <b>60</b>A, <b>60</b>B, and <b>60</b>C output the determination result indicating an edge pixel, while the other one outputs the determination result indicating a non-edge pixel. In this case, the arithmetic system <b>3</b> adopts the determination results with the larger number, and determines that the pixel having the input feature vector (&#x2212;100, &#x2212;50, 0, 50) is an edge pixel. In the example shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, all the decision trees <b>60</b>A, <b>60</b>B, <b>60</b>C output the determination result indicating a non-edge pixel. In this case, the arithmetic system <b>3</b> determines that the pixel having the input feature vector (&#x2212;10, &#x2212;5, 0, 0) is a non-edge pixel.</p><p id="p-0111" num="0110">The decision tree has the advantage that the machine learning can be completed faster than other models, such as neural network. For example, the machine learning for constructing a plurality of decision trees using training data is completed in about one to five minutes. Therefore, using the model <b>60</b> of the decision trees can shorten a time from the start of the machine learning to the generation of the virtual edge <b>55</b>.</p><p id="p-0112" num="0111">In general, an edge shape of a pattern will be slightly different from wafer to wafer, even if the pattern is produced from the same design data. A model that has been created using images of patterns on one wafer may fail to detect an edge of a pattern on other wafer. According to this embodiment, the real pattern used for producing the training data and the target pattern <b>51</b> whose virtual edge <b>55</b> is to be generated are formed on the same wafer (workpiece) W. Specifically, the machine learning for the model <b>60</b> in the learning phase and the generation of the virtual edge <b>55</b> in the edge detection phase are performed using images of the same wafer (workpiece) W. Therefore, the arithmetic system <b>3</b> can generate the virtual edge <b>55</b> of the target pattern <b>51</b> with high accuracy by using the model <b>60</b> constructed by the machine learning using the training data.</p><p id="p-0113" num="0112">In the present embodiment, the plurality of decision trees are used as the model <b>60</b> constructed by the machine learning, while the present invention is not limited to the present embodiment. In one embodiment, the model <b>60</b> constructed by the machine learning may be a model constituted of a support vector machine or a neural network. In the case where the model <b>60</b> is a neural network, the feature vector is input to an input layer of the neural network, and the determination result is output from an output layer of the neural network. Deep learning is suitable for the machine learning for the neural network.</p><p id="p-0114" num="0113">The virtual edge <b>55</b> shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> is expected to have a shape very close to the edge of the target pattern <b>51</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In one embodiment, the arithmetic system <b>3</b> may further perform a process of detecting the edge of the target pattern <b>51</b> based on the virtual edge <b>55</b>. The edge detection for the target pattern <b>51</b> is performed in the same manner as the conventional edge detection method described with reference to <figref idref="DRAWINGS">FIGS. <b>25</b> and <b>26</b></figref>. However, the virtual edge <b>55</b> is used instead of the CAD pattern. Specifically, as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the arithmetic system <b>3</b> generates multiple brightness profiles of the objective image <b>50</b> along multiple search lines <b>65</b> each extending in the normal direction with respect to the virtual edge <b>55</b>. The arithmetic system <b>3</b> determines multiple edge points EP based on the brightness profiles, and connects the multiple edge points EP with a line to generate a renewed edge <b>67</b>.</p><p id="p-0115" num="0114"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram showing an example of the brightness profile along one of the search lines <b>65</b> shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. The arithmetic system <b>3</b> determines the edge point EP whose brightness value on the brightness profile is equal to a threshold value. As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the arithmetic system <b>3</b> determines multiple edge points EP on the brightness profiles along the multiple search lines <b>65</b>, connects these edge points EP with a line to generate the renewed edge <b>67</b>, and draw the renewed edge <b>67</b> on the objective image <b>50</b>. The renewed edge <b>67</b> is expected to have a shape very close to the actual edge of the target pattern <b>51</b> (see <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0116" num="0115">Next, the training data used for the machine learning for constructing the model <b>60</b> will be described. As described above, the training data is produced from images of a plurality of real patterns on the wafer W having the target pattern <b>51</b> formed thereon whose virtual edge <b>55</b> is to be generated. The arithmetic system <b>3</b> selects a plurality of training patterns from the design data. The design data (also referred to as CAD data) is design data for the patterns formed on the wafer W.</p><p id="p-0117" num="0116">In order to improve the edge determination accuracy of the model <b>60</b>, it is desirable that the training data is produced from images of patterns having various edge shapes. From this point of view, the training patterns used to produce the training data include, as shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, patterns PT<b>1</b>, PT<b>2</b>, PT<b>3</b> having edges E<b>1</b> extending in a first direction, edges E<b>2</b> extending in a second direction perpendicular to the first direction, corner edge(s) E<b>3</b>, and terminal edges E<b>4</b>. The arithmetic system <b>3</b> extracts (selects), from the design data, the plurality of training patterns PT<b>1</b>, PT<b>2</b>, and PT<b>3</b> having edges E<b>1</b> to E<b>4</b> having such various shapes.</p><p id="p-0118" num="0117">The arithmetic system <b>3</b> is configured to display a design diagram drawn based on the design data on the display screen <b>10</b> (see <figref idref="DRAWINGS">FIG. <b>1</b></figref>). <figref idref="DRAWINGS">FIG. <b>10</b></figref> is a schematic diagram showing an example of a design diagram <b>75</b> displayed on the display screen <b>10</b>. The design diagram <b>75</b> includes various patterns drawn based on the design data. The user can visually check the design diagram <b>75</b> on the display screen <b>10</b> and can select the patterns PT<b>1</b>, PT<b>2</b>, and PT<b>3</b> having edges extending in multiple directions as shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. More Specifically, the user operates the input device <b>12</b> including the mouse <b>12</b><i>a </i>and the keyboard <b>12</b><i>b </i>shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, and can select training patterns PT<b>1</b>, PT<b>2</b>, and PT<b>3</b> from the plurality of patterns on the design diagram <b>75</b>, as shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0119" num="0118">The user can operate the input device <b>12</b> to delete or change a part of the training patterns PT<b>1</b>, PT<b>2</b>, PT<b>3</b>, or add another pattern on the design diagram <b>75</b> to the training patterns PT<b>1</b>, PT<b>2</b>, PT<b>3</b>.</p><p id="p-0120" num="0119">The arithmetic system <b>3</b> displays the selected training patterns PT<b>1</b>, PT<b>2</b>, PT<b>3</b>, or an area in which these patterns PT<b>1</b>, PT<b>2</b>, PT<b>3</b> are located in a visually emphasized manner. For example, as shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a frame <b>80</b> indicating an area in which the selected training patterns PT<b>1</b>, PT<b>2</b>, and PT<b>3</b> are located may be displayed on the display screen <b>10</b>, or as shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the selected training patterns PT<b>1</b>, PT<b>2</b>, PT<b>3</b> themselves may be displayed in a manner visually different from other patterns. In the example shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the selected training patterns PT<b>1</b>, PT<b>2</b>, PT are displayed with thicker lines than other patterns. In another example, the selected training patterns PT<b>1</b>, PT<b>2</b>, PT<b>3</b> may be displayed in color(s) different from other patterns. The user can visually check the plurality of training patterns PT<b>1</b>, PT<b>2</b>, and PT<b>3</b> on the display screen <b>10</b>.</p><p id="p-0121" num="0120">The scanning electron microscope <b>1</b> generates a plurality of training images of a plurality of real patterns that have been produced based on the plurality of selected training patterns, respectively. At this time, the scanning electron microscope <b>1</b> may generate an image of the target pattern <b>51</b> whose virtual edge <b>55</b> is to be generated. The arithmetic system <b>3</b> obtains the plurality of training images from the scanning electron microscope <b>1</b> and stores them in the memory <b>6</b>.</p><p id="p-0122" num="0121">Next, the arithmetic system <b>3</b> detects edges of real patterns on the plurality of training images. This edge detection is performed according to the conventional edge detection method described with reference to <figref idref="DRAWINGS">FIGS. <b>25</b> and <b>26</b></figref>. Specifically, the arithmetic system <b>3</b> generates, from the design data, a plurality of CAD patterns corresponding to the plurality of training patterns, respectively. The arithmetic system <b>3</b> may apply a corner rounding process to each CAD pattern to form rounded corner edges. Next, the arithmetic system <b>3</b> superimposes these CAD patterns on the plurality of patterns on the training images. The arithmetic system <b>3</b> generates multiple search lines each extending in the normal direction with respect to the edge of each CAD pattern, and produces multiple brightness profiles of the image along the search lines. The arithmetic system <b>3</b> determines an edge point where a brightness value on one brightness profile is equal to a threshold value. Further, the arithmetic system <b>3</b> repeats the same operation to determine multiple edge points on the brightness profiles along all the search lines. The arithmetic system <b>3</b> connects the multiple edge points with a line to determine an edge of the real pattern on the training image constituted of the line connecting the multiple edge points. In this way, the edge of the real pattern on the training image is detected (determined).</p><p id="p-0123" num="0122">As a result of the edge detection of the real pattern on the training image, the arithmetic system <b>3</b> can label each reference pixel constituting the training image with an edge pixel or a non-edge pixel. Specifically, the arithmetic system <b>3</b> classifies reference pixels constituting the plurality of training images into first reference pixels each constituting an edge and second reference pixels that do not constitute an edge.</p><p id="p-0124" num="0123">The arithmetic system <b>3</b> generates first feature vectors of the first reference pixels and second feature vectors of the second reference pixels. As described above, each feature vector is a multidimensional vector including a plurality of features of each reference pixel. The arithmetic system <b>3</b> produces training data including the first feature vectors, the second feature vectors, and correct answer data of these feature vectors. The correct answer data is information that identifies each pixel having a certain feature vector as either a pixel that constitutes an edge of a pattern or a pixel that does not constitute an edge. Each of the first feature vector and the second feature vector included in the training data is connected with (associated with) the correct answer data.</p><p id="p-0125" num="0124"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a schematic diagram showing training data <b>70</b> including the first feature vectors of the first reference pixels each constituting an edge and the second feature vectors of the second reference pixels that do not constitute an edge. The first feature vectors are labeled with correct data representing edge pixel, and the second feature vectors are labeled with correct data representing non-edge pixel. The arithmetic system <b>3</b> constructs the model <b>60</b> by the machine learning using the training data <b>70</b>.</p><p id="p-0126" num="0125">In this embodiment, the model <b>60</b> includes a plurality of decision trees <b>60</b>A, <b>60</b>B, <b>60</b>C. The arithmetic system <b>3</b> creates a plurality of data groups <b>70</b>A, <b>70</b>B, <b>70</b>C each including a plurality of first feature vectors and a plurality of second vectors randomly extracted from the training data <b>70</b>, and constructs the decision trees <b>60</b>A, <b>60</b>B, <b>60</b>C using these data groups <b>70</b>A, <b>70</b>B, <b>70</b>C. More Specifically, the arithmetic system <b>3</b> uses the data group <b>70</b>A to determine model parameters of the decision tree <b>60</b>A. In the same manner, the arithmetic system <b>3</b> uses the data group <b>70</b>B to determine model parameters of the decision tree <b>60</b>B and uses the data group <b>70</b>C to determine model parameters of the decision tree <b>60</b>C.</p><p id="p-0127" num="0126">The arithmetic system <b>3</b> uses the training data <b>70</b> to verify the model <b>60</b> including the plurality of decision trees <b>60</b>A, <b>60</b>B, <b>60</b>C having the model parameters determined as described above. Specifically, the arithmetic system <b>3</b> inputs the first feature vector included in the training data <b>70</b> to the model <b>60</b>, outputs the determination result from the model <b>60</b>, and verifies whether the determination result indicates an edge pixel. Similarly, the arithmetic system <b>3</b> inputs the second feature vector included in the training data <b>70</b> to the model <b>60</b>, outputs the determination result from the model <b>60</b>, and verifies whether the determination result indicates a non-edge pixel. The arithmetic system <b>3</b> repeatedly performs such verification to thereby obtain a plurality of determination results, and calculates a determination accuracy which is a rate of the plurality of determination results that coincide with the correct answer data.</p><p id="p-0128" num="0127">If the determination accuracy is equal to or higher than a set value, the arithmetic system <b>3</b> performs the generation of the virtual edge <b>55</b> using the model <b>60</b> as described above. If the determination accuracy is smaller than the set value, the arithmetic system <b>3</b> produces the training data again and performs the machine learning for the model again. In one embodiment, the arithmetic system <b>3</b> may not use the model <b>60</b> when the determination accuracy is smaller than the set value and may detect the edge of the target pattern <b>51</b> according to the conventional edge detection method described with reference to <figref idref="DRAWINGS">FIGS. <b>25</b> and <b>26</b></figref>.</p><p id="p-0129" num="0128"><figref idref="DRAWINGS">FIGS. <b>14</b> and <b>15</b></figref> are flowcharts showing the operation of the pattern-edge detection apparatus described above.</p><p id="p-0130" num="0129">In step 1, the arithmetic system <b>3</b> selects (extracts) the plurality of training patterns from the design data. As shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the plurality of selected training patterns include patterns having the edge E<b>1</b> extending in the first direction, the edge E<b>2</b> extending in the second direction perpendicular to the first direction, the corner edge E<b>3</b>, and the terminal edge E<b>4</b>.</p><p id="p-0131" num="0130">In step 2, the scanning electron microscope <b>1</b> generates a plurality of training images of real patterns that have been produced based on the plurality of selected training patterns, respectively. At this time, the scanning electron microscope <b>1</b> may generate an image of the target pattern <b>51</b> whose virtual edge is to be generated.</p><p id="p-0132" num="0131">In step 3, the arithmetic system <b>3</b> detects edges of the real patterns on the plurality of training images. This edge detection is performed according to the conventional edge detection method described with reference to <figref idref="DRAWINGS">FIGS. <b>25</b> and <b>26</b></figref>.</p><p id="p-0133" num="0132">In step 4, the arithmetic system <b>3</b> classifies reference pixels constituting the plurality of training images into the first reference pixels each constituting an edge and the second reference pixels that do not constitute an edge.</p><p id="p-0134" num="0133">In step 5, the arithmetic system <b>3</b> generates the plurality of first feature vectors of the plurality of first reference pixels and the plurality of second feature vectors of the plurality of second reference pixels. Specifically, the arithmetic system <b>3</b> generates a feature vector representing a plurality of features of each reference pixel.</p><p id="p-0135" num="0134">In step 6, the arithmetic system <b>3</b> produces the training data <b>70</b> including the plurality of first feature vectors, the plurality of second feature vectors, and the correct answer data of these feature vectors (see <figref idref="DRAWINGS">FIG. <b>13</b></figref>).</p><p id="p-0136" num="0135">In step 7, the arithmetic system <b>3</b> performs the machine learning using the training data <b>70</b> to construct the model <b>60</b>. More Specifically, the arithmetic system <b>3</b> adjusts the model parameters such that when a certain feature vector is input to the model <b>60</b>, the model <b>60</b> outputs a correct determination result.</p><p id="p-0137" num="0136">In step 8, the arithmetic system <b>3</b> verifies the determination accuracy of the model <b>60</b> using the training data <b>70</b>. Specifically, the arithmetic system <b>3</b> inputs a plurality of feature vectors included in the training data <b>70</b> into the model <b>60</b> one by one, and outputs a plurality of determination results from the model <b>60</b>. The calculation system <b>3</b> calculates the determination accuracy, which is the rate of the plurality of determination results that coincide with the correct answer data.</p><p id="p-0138" num="0137">In step 9, the arithmetic system <b>3</b> compares the determination accuracy with the set value. If the determination accuracy is smaller than the set value, the operation flow returns to the step 6. In one embodiment, when the determination accuracy is smaller than the set value, the operation flow may not return to the step 6, and the arithmetic system <b>3</b> may detect the edge of the target pattern <b>51</b> according to the conventional edge detection method described with reference to <figref idref="DRAWINGS">FIGS. <b>25</b> and <b>26</b></figref>.</p><p id="p-0139" num="0138">If the determination accuracy is equal to or higher than the set value in the step 9, the arithmetic system <b>3</b> generates a virtual edge using the model <b>60</b> in step 10, as shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref>. Specifically, the arithmetic system <b>3</b> generates a feature vector representing a plurality of features of each pixel of the objective image <b>50</b>, inputs the feature vector into the model <b>60</b>, and outputs from the model <b>60</b> a determination result indicating an edge pixel or a non-edge pixel. The arithmetic system <b>3</b> generates the virtual edge by connecting a plurality of pixels, each having feature vector that has obtained a determination result indicating an edge pixel, with a line.</p><p id="p-0140" num="0139">In step 11, the arithmetic system <b>3</b> performs the edge detection using the virtual edge as a reference edge according to the conventional edge detection method, and generates a renewed edge. Specifically, as shown in <figref idref="DRAWINGS">FIGS. <b>7</b> and <b>8</b></figref>, the arithmetic system <b>3</b> generates multiple search lines <b>65</b> each extending in the normal direction with respect to the virtual edge <b>55</b>, produces multiple brightness profiles of the objective image <b>50</b> along these search lines <b>65</b>, determines multiple edge points EP based on the multiple brightness profiles, and connects the multiple edge points EP with a line to generate the renewed edge <b>67</b>.</p><p id="p-0141" num="0140">In step 12, the arithmetic system <b>3</b> checks how far the renewed edge <b>67</b> generated in the step 11 is located apart from an edge of a CAD pattern. Specifically, as shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref>, the arithmetic system <b>3</b> generates from the design data a CAD pattern <b>75</b> corresponding to the target pattern <b>51</b>, superimposes the CAD pattern <b>75</b> onto the target pattern <b>51</b> on the objective image <b>50</b>, and measures a distance from an edge of the CAD pattern <b>75</b> to the renewed edge <b>67</b> of the target pattern <b>51</b> at a plurality of measurement points. The plurality of measurement points are arranged on the edge of the CAD pattern. According to this step 12, it is possible to know how much the renewed edge <b>67</b> deviates from the design data (or how much the renewed edge <b>67</b> is close to the design data).</p><p id="p-0142" num="0141">The arithmetic system <b>3</b> including at least one computer operates according to the instructions contained in the program electrically stored in the memory <b>6</b>. Specifically, the arithmetic system <b>3</b> performs the steps of: instructing the scanning electronic microscope <b>1</b> to generate the objective image <b>50</b> of the target pattern <b>51</b> formed on the workpiece; generating a feature vector representing a plurality of features of each pixel of the objective image <b>50</b>; inputting the feature vector into the model <b>60</b> constructed by the machine learning; outputting from the model <b>60</b> a determination result indicating whether the pixel having the feature vector is an edge pixel or a non-edge pixel; and connecting a plurality of pixels, each having a feature vector that has obtained the determination result indicating an edge pixel, with a line to generate the virtual edge <b>55</b>.</p><p id="p-0143" num="0142">The program for causing the arithmetic system <b>3</b> to perform these steps is stored in a computer-readable storage medium which is a non-transitory tangible medium, and is provided to the arithmetic system <b>3</b> via the storage medium. Alternatively, the program may be input to the arithmetic system <b>3</b> via a communication network, such as the Internet or a local area network.</p><p id="p-0144" num="0143">The pattern edge used for the training data has deviations. Moreover, a boundary line between the edge and a non-edge region on the image is unclear. The model (hereinafter referred to as edge detection model) that has been produced using such training data may fail to detect an edge or may erroneously detect an edge. Creating an accurate model requires a large amount of training data for the machine learning, and as a result, the machine learning takes a very long time.</p><p id="p-0145" num="0144">Thus, embodiments described below provide a method and an apparatus capable of producing an accurate edge detection model without requiring a long time in the machine learning. <figref idref="DRAWINGS">FIG. <b>17</b></figref> is a schematic diagram showing another embodiment of the pattern-edge detection apparatus. Configurations and operations of the present embodiment, which will not be particularly described, are the same as those of the embodiments described with reference to <figref idref="DRAWINGS">FIGS. <b>1</b> to <b>16</b></figref>, and the duplicated descriptions thereof will be omitted.</p><p id="p-0146" num="0145">The arithmetic system <b>3</b> includes a model generating apparatus <b>80</b> configured to generate an edge detection model for detecting an edge of a pattern formed on a workpiece W. The image acquisition device <b>28</b> is coupled to the model generating apparatus <b>80</b>.</p><p id="p-0147" num="0146">The model generating apparatus <b>80</b> is composed of at least one computer. The model generating apparatus <b>80</b> includes a memory <b>80</b><i>a </i>storing programs therein, and a processor <b>80</b><i>b </i>configured to perform arithmetic operations according to instructions included in the programs. The memory <b>80</b><i>a </i>includes a main memory, such as a RAM, and an auxiliary memory, such as a hard disk drive (HDD) or a solid state drive (SSD). Examples of the processor <b>80</b><i>b </i>include a CPU (central processing unit) and a GPU (graphic processing unit). However, the specific configuration of the model generating apparatus <b>80</b> is not limited to these examples. The memory <b>80</b><i>a </i>may be integrated with the memory <b>6</b>, and the processor <b>80</b><i>b </i>may be integrated with the processor <b>7</b>.</p><p id="p-0148" num="0147">The model generating apparatus <b>80</b> is configured to produce, by the machine learning, an edge detection model for detecting an edge of a pattern on an image sent from the image acquisition device <b>28</b>. The producing of the edge detection model will be described below.</p><p id="p-0149" num="0148">First, the workpiece W on which pattern is formed is prepared. The scanning electron microscope <b>1</b> generates a training image of the workpiece W, and the model generating apparatus <b>80</b> obtains the training image from the scanning electron microscope <b>1</b>. <figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram showing an example of the training image of the workpiece W on which the pattern is formed. In the example shown in <figref idref="DRAWINGS">FIG. <b>18</b></figref>, a plurality of patterns appear in the training image. These patterns are training patterns used for the training data. The model generating apparatus <b>80</b> detects an edge of a pattern on the training image. A known image processing technique, such as Sobel filter or Canny method, is used to detect the edge. Alternatively, the edge detection may be performed according to the conventional edge detection method described with reference to <figref idref="DRAWINGS">FIGS. <b>25</b> and <b>26</b></figref>. In one embodiment, the user may manually correct the detected edge. Further, in one embodiment, the edge may be drawn by the user.</p><p id="p-0150" num="0149"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a diagram showing the detected edge. As shown in <figref idref="DRAWINGS">FIG. <b>19</b></figref>, the detected edge is expressed by line. The model generating apparatus <b>80</b> superimposes the detected edge on the training image of the workpiece W. <figref idref="DRAWINGS">FIG. <b>20</b></figref> is a diagram in which the detected edge shown in <figref idref="DRAWINGS">FIG. <b>19</b></figref> is superimposed on the training image shown in <figref idref="DRAWINGS">FIG. <b>18</b></figref>.</p><p id="p-0151" num="0150">Next, the model generating apparatus <b>80</b> generates a feature vector representing a plurality of features of each pixel constituting the training image shown in <figref idref="DRAWINGS">FIG. <b>18</b></figref>. The feature vector is a multidimensional vector including a plurality of features of each pixel. The features are numerical values representing a characteristic of each pixel. In the present embodiment, the plurality of features of each pixel are differences between a brightness value of that pixel and brightness values of other pixels. The brightness value is, in one example, a discrete numerical value ranging from 0 to 255 according to a gray scale. In this embodiment, the other pixels are adjacent pixels. In one embodiment, the other pixels may be non-adjacent pixels.</p><p id="p-0152" num="0151">An embodiment for calculating features of a pixel will be described with reference to <figref idref="DRAWINGS">FIG. <b>21</b></figref>. As shown in <figref idref="DRAWINGS">FIG. <b>21</b></figref>, the model generating apparatus <b>80</b> applies a differential filter to the training image and calculates the features of each pixel. More Specifically, the model generating apparatus <b>80</b> differentiates brightness values of pixels constituting the training image along multiple directions, and calculates, for each pixel, a feature constituted of a difference in the brightness value between two pixels arranged in each direction.</p><p id="p-0153" num="0152">In the example shown in <figref idref="DRAWINGS">FIG. <b>21</b></figref>, the model generating apparatus <b>80</b> differentiates the brightness values of the pixels along the directions of 0 degrees, 45 degrees, 90 degrees, and 135 degrees. Specifically, the model generating apparatus <b>80</b> calculates a difference in brightness value between pixels arranged in the 0-degree direction, a difference in brightness value between pixels arranged in the 45-degree direction, a difference in brightness value between pixels arranged in the 90-degree direction, and a difference in brightness value between pixels arranged in the 135-degree direction. Therefore, the features constituted of four numerical values are determined for each pixel. For example, features of a pixel denoted by a reference numeral P<b>1</b> in <figref idref="DRAWINGS">FIG. <b>21</b></figref> are represented by a feature vector composed of 200, 50, 0, &#x2212;50. It should be noted that angles of differentiation, the number of angles, and the number of features per pixel are not limited to this embodiment.</p><p id="p-0154" num="0153">Next, as shown in <figref idref="DRAWINGS">FIG. <b>22</b></figref>, the model generating apparatus <b>80</b> divides a target region <b>100</b> in the training image into an edge region R<b>1</b>, a near-edge region R<b>2</b>, and a non-edge region R<b>3</b>. The target area <b>100</b> shown in <figref idref="DRAWINGS">FIG. <b>22</b></figref> is a part of the training image of the workpiece W shown in <figref idref="DRAWINGS">FIG. <b>18</b></figref>. More Specifically, the target region <b>100</b> shown in <figref idref="DRAWINGS">FIG. <b>22</b></figref> is a region including the edge of the pattern on the training image shown in <figref idref="DRAWINGS">FIG. <b>18</b></figref>. The edge region R<b>1</b> is a region including pixels constituting the detected edge shown in <figref idref="DRAWINGS">FIG. <b>19</b></figref>. A width of the edge region R<b>1</b> is constant. For example, the width of the edge region R<b>1</b> may be a width corresponding to one pixel, or may be a width corresponding to a predetermined numbers (for example, three) of pixels.</p><p id="p-0155" num="0154">The non-edge region R<b>3</b> is located away from the edge region R<b>1</b> by a predetermined number of pixels. The near-edge region R<b>2</b> is located between the edge region R<b>1</b> and the non-edge region R<b>3</b>. Specifically, the near-edge region R<b>2</b> is adjacent to the edge region R<b>1</b> and the non-edge region R<b>3</b>, and extends along the edge region R<b>1</b> and the non-edge region R<b>3</b>. A width of the near-edge region R<b>2</b> is constant. In one embodiment, the width of the near-edge region R<b>2</b> is larger than the width of the edge region R<b>1</b>. Typically, the edge region R<b>1</b> is smaller than the near-edge region R<b>2</b>, and the near-edge region R<b>2</b> is smaller than the non-edge region R<b>3</b>.</p><p id="p-0156" num="0155">The model generating apparatus <b>80</b> produces training data including a plurality of feature vectors of a plurality of pixels in the edge region R<b>1</b>, a plurality of feature vectors of a plurality of pixels in the near-edge region R<b>2</b>, and a plurality of feature vectors of a plurality of pixels in the non-edge region R<b>3</b>. The model generating apparatus <b>80</b> produces an edge detection model by the machine learning using the training data. Examples of the edge detection model include decision tree and neural network. Examples of the machine learning include decision tree learning and deep learning.</p><p id="p-0157" num="0156">The training data includes correct answer data (or correct label) for each pixel. This correct answer data is information that identifies each pixel as either a pixel that constitutes an edge of a pattern or a pixel that does not constitute an edge. The pixels in the edge region R<b>1</b> are pixels that constitute an edge, and the pixels in the near-edge region R<b>2</b> and the non-edge region R<b>3</b> are pixels that do not constitute an edge. The machine learning optimizes parameters (e.g., weighting factors) of the edge detection model such that, when a feature vector is input to the edge detection model, the edge detection model can correctly determine whether a pixel with the input feature vector is an edge pixel or a non-edge pixel. The edge detection model created by the machine learning in this way can determine whether a pixel is an edge pixel or a non-edge pixel based on the feature vector of that pixel.</p><p id="p-0158" num="0157">According to this embodiment, the machine learning uses the training data that inevitably includes pixels in the edge region R<b>1</b>, pixels in the near-edge region R<b>2</b>, and pixels in the non-edge region R<b>3</b>. In particular, the pixels in the near-edge region R<b>2</b>, which are considered difficult to be determined, are included in the training data. Therefore, the edge detection model created by the machine learning can accurately determine whether or not a given pixel constitutes an edge.</p><p id="p-0159" num="0158">If the number of pixels in the non-edge region R<b>3</b> included in the training data is much larger than the number of pixels in the edge region R<b>1</b> included in the training data, the algorithm of the edge detection model produced using such training data is biased toward non-edge pixel detection. As a result, the edge detection model may not correctly determine that the input pixel is an edge pixel. Therefore, in order to improve the edge detection accuracy of the edge detection model, it is preferable that pixels used in the machine learning for the edge detection model evenly include edge pixels (i.e., pixels in the edge region R<b>1</b>) and non-edge pixels (i.e., pixels in the near-edge region R<b>2</b> and the non-edge region R<b>3</b>).</p><p id="p-0160" num="0159">From this point of view, where the number of pixels in the edge region R<b>1</b> is denoted by A, and the sum of the number of pixels in the near-edge region R<b>2</b> and the number of pixels in the non-edge region R<b>3</b> is denoted by B, a value (A/B) obtained by dividing the number A by the number B is a predetermined numerical value. The value (A/B) obtained by dividing the number A by the number B is in a range of 0.6 to 1.5. In order to improve the edge detection accuracy of the edge detection model, in one embodiment, the number A of pixels in the edge region R<b>1</b> included in the training data is equal to the sum B of the number of pixels in the near-edge region R<b>2</b> and the number of pixels in the edge region R<b>3</b> included in the training data.</p><p id="p-0161" num="0160">The feature vectors of the pixels in the near-edge region R<b>2</b> have values between the feature vectors of the pixels in the edge region R<b>1</b> and the feature vectors of the pixels in the non-edge region R<b>3</b>. Therefore, it is difficult to accurately determine whether a pixel in the near-edge region R<b>2</b> is an edge pixel or a non-edge pixel. From another point of view, the edge detection model with high edge-detection accuracy can be generated by using training data including many feature vectors of pixels in the near-edge region R<b>2</b>. Therefore, in one embodiment, the number of pixels in the near-edge region R<b>2</b> included in the training data is larger than the number of pixels in the non-edge region R<b>3</b> included in the training data.</p><p id="p-0162" num="0161">The arithmetic system <b>3</b> detects an edge on an objective image of the workpiece W using the edge detection model created by the machine learning as follows. The scanning electron microscope <b>1</b> generates an objective image of the workpiece W. The arithmetic system <b>3</b> receives the objective image of the workpiece W from the scanning electron microscope <b>1</b> and calculates a feature vector of a pixel constituting the objective image of the workpiece W. The arithmetic system <b>3</b> inputs the feature vector to the edge detection model, and outputs from the edge detection model a determination result indicating whether the pixel having the feature vector is an edge pixel or a non-edge pixel. The arithmetic system <b>3</b> then connects a plurality of pixels, each having a feature vector that has obtained a determination result indicating an edge pixel, with a line to generate an edge.</p><p id="p-0163" num="0162">The model generating apparatus <b>80</b> including at least one computer operates according to the instructions contained in the programs electrically stored in the memory <b>80</b><i>a. </i>Specifically, the model generating apparatus <b>80</b> performs the steps of: obtaining a training image of the workpiece W having a pattern formed thereon from the scanning electron microscope <b>1</b>; detecting an edge of the pattern on the training image; calculating feature vectors of pixels that constitute the training image; dividing a target area in the training image into the edge region R<b>1</b>, the near-edge region R<b>2</b>, and the non-edge region R<b>3</b>; producing training data including feature vectors of pixels in the edge region R<b>1</b>, feature vectors of pixels in the near-edge region R<b>2</b>, and feature vectors of pixels in the non-edge region R<b>3</b>; and producing the edge detection model by the machine learning using the training data.</p><p id="p-0164" num="0163">The program for causing the model generating apparatus <b>80</b> to perform these steps is stored in a computer-readable storage medium which is a non-transitory tangible object, and is provided to the model generating apparatus <b>80</b> via the storage medium. Alternatively, the program may be input to the model generating apparatus <b>80</b> via a communication network, such as the Internet or a local area network.</p><p id="p-0165" num="0164">Next, another embodiment for producing an edge detection model will be described with reference to <figref idref="DRAWINGS">FIG. <b>23</b></figref>. Processes of the present embodiment, which will not be particularly described, are the same as those of the above-described embodiments described with reference to <figref idref="DRAWINGS">FIGS. <b>17</b> to <b>22</b></figref>, and the repetitive descriptions thereof will be omitted.</p><p id="p-0166" num="0165">As shown in <figref idref="DRAWINGS">FIG. <b>23</b></figref>, in the present embodiment, the model generating apparatus <b>80</b> is configured to divide the target region <b>100</b> in the training image into the edge region R<b>1</b>, the near-edge region R<b>2</b>, the non-edge region R<b>3</b>, and an exclusion region R<b>4</b>. The exclusion region R<b>4</b> is adjacent to the edge region R<b>1</b> and is located between the edge region R<b>1</b> and the near-edge region R<b>2</b>. A width of the exclusion region R<b>4</b> shown in <figref idref="DRAWINGS">FIG. <b>23</b></figref> is constant. In one embodiment, the width of the exclusion region R<b>4</b> is narrower than the width of the near-edge region R<b>2</b>. Further, in one embodiment, the width of the exclusion region R<b>4</b> is the same as the width of the edge region R<b>1</b> or narrower than the width of the edge region R<b>1</b>.</p><p id="p-0167" num="0166">The training data does not include feature vectors of pixels in the exclusion region R<b>4</b>. Specifically, the training data includes feature vectors of pixels in the edge region R<b>1</b>, feature vectors of pixels in the near-edge region R<b>2</b>, and feature vectors of pixels in the non-edge region R<b>3</b>, but does not include feature vectors of pixels in the exclusion region R<b>4</b>. Therefore, the pixels in the exclusion region R<b>4</b> are not used for the machine learning.</p><p id="p-0168" num="0167">The exclusion region R<b>4</b> is located next to the edge region R<b>1</b>, and the feature vector of each pixel in the exclusion region R<b>4</b> is almost the same as the feature vector of each pixel in the edge region R<b>1</b>. Therefore, the pixels in the exclusion region R<b>4</b> may be edge pixels or non-edge pixels. In other words, the pixels in the exclusion region R<b>4</b> are uncertain pixels. Including such uncertain pixels in the training data requires the machine learning to continue until the edge detection model satisfies a desired percentage of correct answers. As a result, the machine learning takes a long time to complete. According to this embodiment, since the feature vectors of the pixels in the exclusion region R<b>4</b> are excluded from the training data, the machine learning of the edge detection model can be completed in a shorter time.</p><p id="p-0169" num="0168">In order to further improve the edge detection accuracy of the edge detection model, in one embodiment, the target region <b>100</b> includes a plurality of regions including various pattern edges in the training image. This is because sharpness of edges of patterns on the training image can vary depending on directions in which the edges extend.</p><p id="p-0170" num="0169"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a diagram showing an example of a target region <b>100</b> including a plurality of regions set in the training image. As shown in <figref idref="DRAWINGS">FIG. <b>24</b></figref>, the target region <b>100</b> includes a first region T<b>1</b> including the first edge E<b>1</b> of the pattern in the image of the workpiece W, a second region T<b>2</b> including the second edge E<b>2</b> perpendicular to the first edge E<b>1</b>, and third regions T<b>3</b> including the corner edge E<b>3</b> and the terminal edge E<b>4</b> of the pattern. The training data includes feature vectors of pixels in the plurality of regions T<b>1</b>, T<b>2</b>, T<b>3</b> including the edges E<b>1</b>, E<b>2</b>, E<b>3</b>, E<b>4</b> extending in different directions. The machine learning using such training data can improve the accuracy of detecting edges extending in various directions.</p><p id="p-0171" num="0170">In order to further improve the accuracy of detecting edges extending in multiple directions, in one embodiment, the number of pixels in the first region T<b>1</b>, the number of pixels in the second region T<b>2</b>, and the number of pixels in the third region T<b>3</b> are in a predetermined ratio. Where the number of pixels in the first region T<b>1</b> is denoted by S<b>1</b>, the number of pixels in the second region T<b>2</b> is denoted by S<b>2</b>, and the number of pixels in the third region T<b>3</b> is denoted by S<b>3</b>, a relationship between S<b>1</b>, S<b>2</b>, and S<b>3</b> is expressed by the following equation.</p><p id="p-0172" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>S</i>1=<i>m&#xd7;S</i>2=<i>n&#xd7;S</i>3<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0173" num="0171">where, m is in a range of 0.9 to 1.1 and n is in a range of 0.01 to 0.1.</p><p id="p-0174" num="0172">The previous description of embodiments is provided to enable a person skilled in the art to make and use the present invention. Moreover, various modifications to these embodiments will be readily apparent to those skilled in the art, and the generic principles and specific examples defined herein may be applied to other embodiments. Therefore, the present invention is not intended to be limited to the embodiments described herein but is to be accorded the widest scope as defined by limitation of the claims.</p><heading id="h-0011" level="1">INDUSTRIAL APPLICABILITY</heading><p id="p-0175" num="0173">The present invention is applicable to a method and an apparatus for detecting an edge (or a contour line) of a pattern, which is formed on a workpiece (e.g., a wafer or a mask) for use in manufacturing of semiconductor, from an image generated by a scanning electron microscope. The present invention is further applicable to a program for causing a computer to perform such pattern-edge detection.</p><p id="p-0176" num="0174">The present invention is further applicable to a method and an apparatus for creating an edge detection model by machine learning.</p><heading id="h-0012" level="1">REFERENCE SIGNS LIST</heading><p id="p-0177" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0175"><b>1</b> scanning electron microscope</li>    <li id="ul0001-0002" num="0176"><b>3</b> arithmetic system</li>    <li id="ul0001-0003" num="0177"><b>5</b> database</li>    <li id="ul0001-0004" num="0178"><b>6</b> memory</li>    <li id="ul0001-0005" num="0179"><b>7</b> processor</li>    <li id="ul0001-0006" num="0180"><b>10</b> display screen</li>    <li id="ul0001-0007" num="0181"><b>12</b> input device</li>    <li id="ul0001-0008" num="0182"><b>15</b> electron gun</li>    <li id="ul0001-0009" num="0183"><b>16</b> converging lens</li>    <li id="ul0001-0010" num="0184">X deflector</li>    <li id="ul0001-0011" num="0185"><b>18</b> Y deflector</li>    <li id="ul0001-0012" num="0186"><b>20</b> objective lens</li>    <li id="ul0001-0013" num="0187"><b>21</b> lens controller</li>    <li id="ul0001-0014" num="0188"><b>22</b> deflection controller</li>    <li id="ul0001-0015" num="0189"><b>25</b> secondary-electron detector</li>    <li id="ul0001-0016" num="0190"><b>26</b> backscattered-electron detector</li>    <li id="ul0001-0017" num="0191"><b>28</b> image acquisition device</li>    <li id="ul0001-0018" num="0192"><b>30</b> chamber</li>    <li id="ul0001-0019" num="0193"><b>31</b> stage</li>    <li id="ul0001-0020" num="0194"><b>32</b> stage controller</li>    <li id="ul0001-0021" num="0195"><b>34</b> transporting device</li>    <li id="ul0001-0022" num="0196"><b>50</b> objective image</li>    <li id="ul0001-0023" num="0197"><b>51</b> target pattern</li>    <li id="ul0001-0024" num="0198"><b>55</b> virtual edge</li>    <li id="ul0001-0025" num="0199"><b>60</b> model</li>    <li id="ul0001-0026" num="0200"><b>60</b>A, <b>60</b>B, <b>60</b>C decision tree</li>    <li id="ul0001-0027" num="0201"><b>65</b> search line</li>    <li id="ul0001-0028" num="0202"><b>67</b> renewed edge</li>    <li id="ul0001-0029" num="0203"><b>70</b>training data</li>    <li id="ul0001-0030" num="0204"><b>70</b>A. <b>70</b>B, <b>70</b>C data group</li>    <li id="ul0001-0031" num="0205"><b>75</b> design diagram</li>    <li id="ul0001-0032" num="0206"><b>80</b> model generating apparatus</li>    <li id="ul0001-0033" num="0207"><b>100</b> target region</li>    <li id="ul0001-0034" num="0208">W wafer (workpiece)</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A pattern-edge detection method comprising:<claim-text>generating an objective image of a target pattern formed on a workpiece;</claim-text><claim-text>generating a feature vector representing features of each pixel constituting the objective image;</claim-text><claim-text>inputting the feature vector to a model constructed by machine learning;</claim-text><claim-text>outputting, from the model, a determination result indicating whether the pixel having the feature vector is an edge pixel or a non-edge pixel; and</claim-text><claim-text>connecting a plurality of pixels, each having a feature vector that has obtained a determination result indicating an edge pixel, with a line to generate a virtual edge.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The pattern-edge detection method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the model is a decision tree.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The pattern-edge detection method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>selecting training patterns from design data;</claim-text><claim-text>generating training images of real patterns that have been produced based on the training patterns, respectively;</claim-text><claim-text>detecting edges of the real patterns on the training images;</claim-text><claim-text>classifying reference pixels constituting the training images into first reference pixels each constituting an edge and second reference pixels that do not constitute an edge;</claim-text><claim-text>generating first feature vectors of the first reference pixels and second feature vectors of the second reference pixels; and</claim-text><claim-text>constructing the model by the machine learning using training data including the first feature vectors and the second feature vectors.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The pattern-edge detection method according to <claim-ref idref="CLM-00003">claim 3</claim-ref> Of <b>4</b>, wherein the training patterns include a plurality of patterns having at least an edge extending in a first direction, an edge extending in a second direction perpendicular to the first direction, a corner edge, and a terminal edge.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The pattern-edge detection method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the real patterns are patterns formed on the workpiece.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The pattern-edge detection method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein selecting the training patterns from the design data comprises:<claim-text>displaying, on a display screen, a design diagram including a plurality of patterns drawn based on the design data; and</claim-text><claim-text>displaying, on the display screen, the training patterns selected from the plurality of patterns included in the design diagram or an area where the training patterns are located in a visually emphasized manner.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The pattern-edge detection method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>generating multiple brightness profiles of the objective image along multiple search lines each extending in a normal direction with respect to the virtual edge;</claim-text><claim-text>determining multiple edge points based on the multiple brightness profiles; and</claim-text><claim-text>generating a renewed edge by connecting the multiple edge points with a line.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The pattern-edge detection method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:<claim-text>generating, from the design data, a CAD pattern corresponding to the target pattern; and</claim-text><claim-text>measuring a distance from an edge of the CAD pattern to the renewed edge.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A pattern-edge detection apparatus comprising:<claim-text>an image generating device configured to generate an objective image of a target pattern formed on a workpiece; and</claim-text><claim-text>an arithmetic system coupled to the image generating device, the arithmetic system being configured to:</claim-text><claim-text>generate a feature vector representing features of each pixel constituting the objective image;</claim-text><claim-text>input the feature vector to a model constructed by machine learning;</claim-text><claim-text>output, from the model, a determination result indicating whether the pixel having the feature vector is an edge pixel or a non-edge pixel; and</claim-text><claim-text>connect a plurality of pixels, each having a feature vector that has obtained a determination result indicating an edge pixel, with a line to generate a virtual edge.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The pattern-edge detection apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the model is a decision tree.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The pattern-edge detection apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the arithmetic system is configured to:<claim-text>select training patterns from design data;</claim-text><claim-text>generate training images of real patterns that have been produced based on the training patterns, respectively;</claim-text><claim-text>detect edges of the real patterns on the training images;</claim-text><claim-text>classify reference pixels constituting the training images into first reference pixels each constituting an edge and second reference pixels that do not constitute an edge;</claim-text><claim-text>generate first feature vectors of the first reference pixels and second feature vectors of the second reference pixels; and</claim-text><claim-text>construct the model by the machine learning using training data including the first feature vectors and the second feature vectors.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The pattern-edge detection apparatus according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the training patterns include a plurality of patterns having at least an edge extending in a first direction, an edge extending in a second direction perpendicular to the first direction, a corner edge, and a terminal edge.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The pattern-edge detection apparatus according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the real patterns are patterns formed on the workpiece.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The pattern-edge detection apparatus according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the arithmetic system includes a display screen, and<claim-text>the arithmetic system is configured to:<claim-text>display, on the display screen, a design diagram including a plurality of patterns drawn based on the design data; and</claim-text><claim-text>display, on the display screen, the training patterns selected from the plurality of patterns included in the design diagram or an area where the training patterns are located in a visually emphasized manner.</claim-text></claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The pattern-edge detection apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the arithmetic system is configured to:<claim-text>generate multiple brightness profiles of the objective image along multiple search lines each extending in a normal direction with respect to the virtual edge;</claim-text><claim-text>determine multiple edge points based on the multiple brightness profiles; and</claim-text><claim-text>generate a renewed edge by connecting the multiple edge points with a line.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The pattern-edge detection apparatus according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the arithmetic system is configured to:<claim-text>generate, from the design data, a CAD pattern corresponding to the target pattern; and</claim-text><claim-text>measure a distance from an edge of the CAD pattern to the renewed edge.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. (canceled)</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. (canceled)</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. (canceled)</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. (canceled)</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. (canceled)</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. (canceled)</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. (canceled)</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. (canceled)</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The pattern-edge detection method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>generating a training image of a workpiece having a pattern formed thereon with a scanning electron microscope;</claim-text><claim-text>detecting an edge of the pattern on the training image;</claim-text><claim-text>calculating feature vectors of pixels constituting the training image;</claim-text><claim-text>dividing a target area in the training image into an edge region, a near-edge region, and a non-edge region;</claim-text><claim-text>producing training data including feature vectors of first pixels in the edge region, feature vectors of second pixels in the near-edge region, and feature vectors of third pixels in the non-edge region; and</claim-text><claim-text>producing the model by machine learning using the training data.</claim-text></claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The pattern-edge detection method according to <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein where the number of first pixels is denoted by A, and a sum of the number of second pixels and the number of third pixels is denoted by B, a value (AB) obtained by dividing the number A by the number B is a predetermined numerical value.</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The pattern-edge detection method according to <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein the value (AB) obtained by dividing the number A by the number B is in a range of 0.6 to 1.5.</claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The pattern-edge detection method according to <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein the non-edge region is located away from the edge region by a predetermined number of pixels, and the near-edge region is located between the edge region and the non-edge region.</claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. The pattern-edge detection method according to <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein:<claim-text>dividing the target region in the training image into the edge region, the near-edge region, and the non-edge region comprises dividing the target region in the training image into an edge region, an exclusion region, a near-edge region, and a non-edge region;</claim-text><claim-text>the exclusion area is adjacent to the edge region and is located between the edge region and the near-edge region; and</claim-text><claim-text>the training data does not include feature vectors of pixels in the exclusion region.</claim-text></claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The pattern-edge detection method according to <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein the target region includes a first region including a first edge, a second region including a second edge perpendicular to the first edge, and a third region including a corner edge and a terminal edge.</claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. The pattern-edge detection method according to <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein the number of pixels in the first region, the number of pixels in the second region, and the number of pixels in the third region are in a predetermined ratio.</claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. The pattern-edge detection apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>a model generating apparatus, the model generating apparatus including:<claim-text>a memory storing a program for producing the model; and</claim-text><claim-text>an arithmetic device configured to perform arithmetic operations according to instructions included in the program,</claim-text></claim-text><claim-text>wherein the model generating apparatus is configured to:</claim-text><claim-text>obtain, from a scanning electron microscope, a training image of a workpiece having a pattern formed thereon;</claim-text><claim-text>detect an edge of the pattern on the training image;</claim-text><claim-text>calculate feature vectors of pixels constituting the training image;</claim-text><claim-text>divide a target area in the training image into an edge region, a near-edge region, and a non-edge region;</claim-text><claim-text>produce training data including feature vectors of first pixels in the edge region, feature vectors of second pixels in the near-edge region, and feature vectors of third pixels in the non-edge region; and</claim-text><claim-text>produce the model by machine learning using the training data.</claim-text></claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. The pattern-edge detection apparatus according to <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein where the number of first pixels is denoted by A, and a sum of the number of second pixels and the number of third pixels is denoted by B, a value (A/B) obtained by dividing the number A by the number B is a predetermined numerical value.</claim-text></claim><claim id="CLM-00034" num="00034"><claim-text><b>34</b>. The pattern-edge detection apparatus according to <claim-ref idref="CLM-00033">claim 33</claim-ref>, wherein the value (A/B) obtained by dividing the number A by the number B is in a range of 0.6 to 1.5.</claim-text></claim><claim id="CLM-00035" num="00035"><claim-text><b>35</b>. The pattern-edge detection apparatus according to <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein the non-edge region is located away from the edge region by a predetermined number of pixels, and the near-edge region is located between the edge region and the non-edge region.</claim-text></claim><claim id="CLM-00036" num="00036"><claim-text><b>36</b>. The pattern-edge detection apparatus according to <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein:<claim-text>the model generating apparatus is configured to divide the target region in the training image into an edge region, an exclusion region, a near-edge region, and a non-edge region;</claim-text><claim-text>the exclusion area is adjacent to the edge region and is located between the edge region and the near-edge region; and</claim-text><claim-text>the training data does not include feature vectors of pixels in the exclusion region.</claim-text></claim-text></claim><claim id="CLM-00037" num="00037"><claim-text><b>37</b>. The pattern-edge detection apparatus according to <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein the target region includes a first region including a first edge, a second region including a second edge perpendicular to the first edge, and a third region including a corner edge and a terminal edge.</claim-text></claim><claim id="CLM-00038" num="00038"><claim-text><b>38</b>. The pattern-edge detection apparatus according to <claim-ref idref="CLM-00037">claim 37</claim-ref>, wherein the number of pixels in the first region, the number of pixels in the second region, and the number of pixels in the third region are in a predetermined ratio.</claim-text></claim><claim id="CLM-00039" num="00039"><claim-text><b>39</b>. (canceled)</claim-text></claim></claims></us-patent-application>