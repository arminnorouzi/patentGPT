<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004718A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004718</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364342</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>30</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>12</main-group><subgroup>58</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>26</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>30</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>51</main-group><subgroup>046</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>26</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">ASSISTANT FOR PROVIDING INFORMATION ON UNKNOWN TOPICS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>Microsoft Technology Licensing, LLC</orgname><address><city>Redmond</city><state>WA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>HELVIK</last-name><first-name>Torbj&#xf8;rn</first-name><address><city>Oslo</city><country>NO</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Techniques are disclosed for assisting users with unknown topics by automatically presenting information associated with the unknown topics to the users. In an example embodiment, an unknown topic is referred to or discussed during a conversation between multiple users. A candidate definition for the topic is determined, where the candidate definition is known by the user that used the topic. Based on a determination that the topic and the candidate definition are unknown to a second user in the conversation, the topic and the candidate definition are provided to one or more output devices for presentation to the second user.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="176.02mm" wi="92.46mm" file="US20230004718A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="174.50mm" wi="138.09mm" file="US20230004718A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="228.35mm" wi="131.15mm" file="US20230004718A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="209.13mm" wi="86.44mm" file="US20230004718A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="200.07mm" wi="144.86mm" orientation="landscape" file="US20230004718A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="218.69mm" wi="134.03mm" file="US20230004718A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="109.05mm" wi="138.77mm" file="US20230004718A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="189.40mm" wi="94.49mm" file="US20230004718A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="214.04mm" wi="134.37mm" file="US20230004718A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="109.05mm" wi="138.77mm" file="US20230004718A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="214.71mm" wi="151.30mm" orientation="landscape" file="US20230004718A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="188.13mm" wi="154.26mm" file="US20230004718A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="177.38mm" wi="132.42mm" file="US20230004718A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="194.73mm" wi="140.80mm" orientation="landscape" file="US20230004718A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="193.21mm" wi="157.23mm" orientation="landscape" file="US20230004718A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="181.27mm" wi="125.90mm" file="US20230004718A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="172.64mm" wi="164.68mm" file="US20230004718A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="174.50mm" wi="162.90mm" file="US20230004718A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">People regularly attend or participate in conversations throughout a day. This can be especially true at an enterprise, such as a business, a university, or an organization. The conversations can be in-person or online meetings, online chats, presentations, seminars, and the like. In some instances, a first person in the conversation uses or refers to a topic that is unknown to a second person in the conversation. The second user may not be able to search for the topic quickly without losing track of the conversation. Also, in an effort to avoid disrupting the conversation, the second user may not ask for an explanation of the topic during the conversation.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0003" num="0002">Embodiments disclosed herein provide techniques for assisting users with unknown topics by automatically presenting information associated with the unknown topics to the users. In one aspect, a method includes detecting a use of a topic in a conversation between multiple users, such as a first user and a second user, where the first user uses the topic in the conversation. One or more candidate definitions of the topic as known by the first user are determined. The topic and at least one candidate definition are determined to be unknown to the second user. At least one of the one or more candidate definitions of the topic are determined to be presented to the second user and are provided to one or more output devices for presentation to the second user. The output device can be any suitable output device, such as a display, a speaker, a mobile telephone, a tablet, a television, and a projector.</p><p id="p-0004" num="0003">In another aspect, a system includes a processing device and a storage device operably connected to the processing device. The storage device stores instructions, that when executed by the processing device, cause operations to be performed. The operations include detecting a use of a topic in a conversation between multiple users, such as a first user and a second user, where the first user uses the topic in the conversation. One or more candidate definitions of the topic as known by the first user are determined. A determination is made that the topic and at least one candidate definition are unknown to the second user. At least one of the one or more candidate definitions of the topic are determined to be presented to the second user and are provided to one or more output devices for presentation to the second user.</p><p id="p-0005" num="0004">In yet another aspect, a method includes detecting a use of a topic in a meeting between multiple users, such as a first user and a second user, where the first user speaks the topic in the online meeting. A plurality of candidate definitions of the topic that is known by the first user is determined. A determination is made that the topic and the plurality of candidate definitions are unknown to the second user. One or more of the plurality of candidate definitions of the topic are determined to be presented to the second user. At least one of the one or more candidate definitions are caused to be provided to an output device. For example, when the meeting is an online meeting, the at least one candidate definition is displayed in a graphical user interface of the online meeting.</p><p id="p-0006" num="0005">This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0007" num="0006">Non-limiting and non-exhaustive examples are described with reference to the following Figures. The elements of the drawings are not necessarily to scale relative to each other. Identical reference numerals have been used, where possible, to designate identical features that are common to the figures.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a block diagram of a first system in which aspects of the present disclosure may be practiced;</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a block diagram of a second system in which aspects of the present disclosure may be practiced;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a flowchart of a method of generating known topics in accordance with some embodiments;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example global list of known topics, information associated with the topics, users, and confidence scores in accordance with some embodiments;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>B</figref> illustrate a flowchart of a method of providing information on an unknown topic in accordance with some embodiments;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>5</b>C</figref> illustrates an alternative flowchart to the flowchart shown in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> in accordance with some embodiments;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>B</figref> illustrate a flowchart of a method of providing one or more meanings for an unknown acronym in accordance with some embodiments;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example GUI that presents one or more definitions for an unknown acronym in accordance with some embodiments;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIGS. <b>8</b>A-<b>8</b>B</figref> illustrate a flowchart of a method of providing information on an unknown codename in accordance with some embodiments;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example GUI that presents information associated with an unknown codename in accordance with some embodiments;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates a block diagram depicting example physical components of a computing device with which aspects of the disclosure may be practiced;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>11</b>B</figref> illustrate block diagrams illustrating a mobile computing device with which aspects of the present disclosure may be practiced; and</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates a block diagram of a distributed computing system in which aspects of the present disclosure may be practiced.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0021" num="0020">In the following detailed description, references are made to the accompanying drawings that form a part hereof, and in which are shown by way of illustrations specific embodiments or examples. These aspects may be combined, other aspects may be utilized, and structural changes may be made without departing from the present disclosure. Embodiments may be practiced as methods, systems, or devices. Accordingly, embodiments may take the form of a hardware implementation, an entirely software implementation, or an implementation combining software and hardware aspects. The following detailed description is therefore not to be taken in a limiting sense, and the scope of the present disclosure is defined by the appended claims and their equivalents.</p><p id="p-0022" num="0021">Generally, embodiments disclosed herein provide techniques for assisting users with unknown topics by automatically presenting information associated with the unknown topics to the users. In an example embodiment, an unknown topic is referred to or discussed during a conversation between multiple users. Example conversations include, but are not limited to, online meetings, online chats, presentations, seminars, and in-person meetings. An assistant application determines a meaning or information associated with the topic as known by the person (a first user) who used or referred to the topic, and then determines whether a person (a second user) who heard or read the topic during the conversation knows the topic and a candidate definition or information associated with the topic. If a determination is made that the second user does not know the topic and the candidate definition, the assistant application determines or retrieves information for the second user and provides the information to one or more output devices for presentation to the second user. In some embodiments, the assistant application causes the information to be provided to the output device(s).</p><p id="p-0023" num="0022">Technical advantages of the disclosed embodiments include providing targeted or personalized assistance to a user when the user is unfamiliar with a topic. The user is not provided with information when the user already knows of the topic. Additionally or alternatively, the personalized information can be determined, retrieved, and presented to the user automatically without user input or action in a timely manner. In some instances, the information is provided to the user in substantially real-time, for example, while a discussion of a topic that is associated with the information occurs.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a block diagram of a first system in which aspects of the present disclosure may be practiced. The system <b>100</b> includes a computing device <b>102</b> that includes one or more storage devices (collectively referred to as storage device <b>104</b>) and one or more processing devices (collectively referred to as processing device <b>106</b>). The storage device <b>104</b> stores computer-executable instructions or one or more software applications <b>108</b>. A user <b>110</b> interacts with the software application(s) to perform various activities. The activities can include sending, receiving, redirecting, creating, modifying, deleting, and viewing electronic communications <b>112</b>. Example electronic communications include, but are not limited to, emails, text messages, instant messages, online chats, video messages, audio messages, and posts in social media.</p><p id="p-0025" num="0024">The activities may further include creating, deleting, viewing, and/or editing documents <b>114</b>, and organizing and/or attending in person and online meetings <b>116</b>. Other activities can include working on, or managing one or more projects <b>118</b> and setting, modifying, deleting, monitoring, and/or completing tasks <b>120</b>. Some of the tasks <b>120</b> may be related to or in advancement of the project(s) <b>118</b>, while other tasks can be related to other business or personal activities.</p><p id="p-0026" num="0025">These activities by the user <b>110</b> create, delete, and modify activity data <b>128</b> that is stored on the storage device <b>104</b>. The activity data <b>128</b> includes data such as emails, various types of documents, meetings and other calendar information, contacts (people), text messages, and the like. In some embodiments, the activity data <b>128</b> also includes the activity data for multiple users, such as the users in an enterprise. An enterprise includes, but is not limited to, a company, an organization, a university, an association, an institution, or other establishment.</p><p id="p-0027" num="0026">A text-to-speech (TTS) and speech-to-text (STT) application <b>130</b> is stored on the storage device <b>104</b>. The TTS application is operable to convert text into speech (an audio output). The STT application is operable to recognize and convert speech (an audio input) into text.</p><p id="p-0028" num="0027">An assistant application <b>132</b> is stored on the storage device <b>104</b> and is operable to detect one or more topics in the audio input, the text converted from the audio input and/or the text output. The assistant application <b>132</b> is also operable to analyze the activity data <b>128</b> to identify topics. The topics can include, but are not limited to, acronyms, abbreviations, codenames, tools, teams, project names, and organizational units. In some embodiments, the topics are associated with an enterprise.</p><p id="p-0029" num="0028">In one embodiment, a global list of known topics is stored in the topic data <b>134</b>. The global list of known topics can be associated with multiple users, such as the users in an enterprise. As will be described in more detail later, the assistant application <b>132</b> is operable to access the global list to determine whether the user <b>110</b> is unfamiliar with (or does not know of) a topic. When the topic is unknown to the user <b>110</b>, the assistant application <b>132</b> determines which information associated with the topic will be presented to the user <b>110</b>. The assistant application <b>132</b> provides the information for presentation (or causes the information to be provided for presentation). In some instances, the assistant application <b>132</b> updates the global list with new topics and/or information in real-time, in substantially real-time, at regular intervals, at selected times, or on demand.</p><p id="p-0030" num="0029">In some embodiments, the assistant application <b>132</b> includes one or more machine learning mechanisms (e.g., models, algorithms, or applications) that are operable to perform one or more text-mining and/or data-mining mechanisms (e.g., model, algorithm, application). Generally, the text-mining and/or data mining mechanism(s) uses natural language processing to extract the topics and information from the activity data and the text that corresponds to the text or audio input. The machine learning mechanism(s) is adaptable over time such that the assistant application <b>132</b> learns and becomes more efficient and effective at detecting topics, determining which topics are known and unknown to the users, and determining which information is likely to be the information that is relevant to a particular user when providing the information to that user. The machine learning mechanism(s) learns over time based on the user's <b>110</b> interactions with the presentation of the information, the user's <b>110</b> adjustments to the information, new information the user <b>110</b> accesses or interacts with, and other types of user interactions.</p><p id="p-0031" num="0030">A topic and at least some of the information associated with the topic are provided to one or more output devices (collective referred to as output device <b>136</b>). The output device <b>136</b> can be included in the computing device <b>102</b> or may be operably connected to the computing device <b>102</b>. An example output device <b>136</b> includes, but is not limited to, a display device, a speaker (e.g., in combination with TTS application <b>130</b>), a printer, and a display screen included or operably connected to a second computing device (e.g., a tablet, a mobile phone).</p><p id="p-0032" num="0031">In a non-limiting nonexclusive example, the user <b>110</b> is attending a meeting using a collaborative software application, such as MICROSOFT TEAMS. During the meeting, a participant in the meeting mentions an acronym. Since the STT application <b>130</b> is converting the audio input into text in substantially real-time, the assistant application <b>132</b> detects the acronym in the text and accesses the topic data <b>134</b> to determine if the acronym is unknown to the user <b>110</b>. The assistant application <b>132</b> determines one or more meanings of the acronym and provides the one or more meanings to the output device <b>136</b> for presentation in the graphical user interface (GUI) of MICROSOFT TEAMS.</p><p id="p-0033" num="0032">The computing device <b>102</b> can be any suitable type of computing device. Example computing devices include a laptop computer, a tablet, a mobile telephone, a smart phone, a smart watch, a wearable computer, a desktop computer, a gaming device/computer (e.g., Xbox), a television, or a server computing device. These example computing devices are for example purposes only and should not be considered as limiting.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a block diagram of a second system in which aspects of the present disclosure may be practiced. The system <b>200</b> is a distributed system that includes the computing device <b>102</b>, a second computing device <b>202</b>, and a third computing device <b>204</b>. The second and the third computing devices <b>202</b>, <b>204</b> are each operably connected to the computing device <b>102</b> through one or more networks (collectively network <b>206</b>).</p><p id="p-0035" num="0034">The second computing device <b>202</b> includes one or more storage devices (collectively storage device <b>208</b>) that stores one or more applications <b>210</b>. The application(s) <b>210</b> can be at least one of the applications <b>108</b>, or the application(s) <b>210</b> can differ from the applications <b>108</b>. One or more processing devices (collectively processing device <b>212</b>) are operable to execute the application(s) <b>210</b>. The user <b>110</b> interacts with the applications <b>108</b>, <b>210</b> to create activity data <b>128</b>. One or more storage devices (storage device <b>214</b>) are operably connected to the second computing device <b>202</b> and the third computing device <b>204</b> through one or more networks (collectively network <b>216</b>). The storage device <b>214</b> stores the activity data <b>128</b>.</p><p id="p-0036" num="0035">The third computing device <b>204</b> includes one or more storage devices (collectively storage device <b>218</b>) that stores the assistant application <b>132</b> and the topic data <b>134</b>. One or more processing devices (collectively processing device <b>220</b>) are operable to execute the assistant application <b>132</b>. When executed by the processing device <b>220</b>, the assistant application <b>132</b> can receive TTS and/or STT data through the network <b>206</b> to detect one or more topics. The assistant application <b>132</b> may also access the activity data <b>128</b> through the network <b>216</b> to analyze the activity data and detect one or more topics. When one or more topics are detected, the assistant application <b>132</b> determines if the topic(s) is unknown to the user <b>110</b>, and if so, accesses the topic data <b>134</b> to retrieve information associated with the topic(s) to provide to the user <b>110</b>. In one embodiment, the information retrieved by the assistant application <b>132</b> is information that is likely or expected to be relevant to (e.g., assist) the user <b>110</b> in understanding the topic(s) or context of the topic(s).</p><p id="p-0037" num="0036">Networks <b>206</b>, <b>216</b> are illustrative of any suitable type of network, for example, an intranet, and/or a distributed computing network (e.g., the Internet) over which the computing devices <b>102</b>, <b>202</b>, <b>204</b> may communicate with each other and with the storage devices <b>214</b>. Additionally, the computing devices <b>202</b>, <b>204</b> can each be any suitable computing device, such as a mobile telephone, a smart phone, a tablet, a smart watch, a wearable computer, a personal computer a desktop computer, a laptop computer, a gaming device/computer (e.g., Xbox), a television, or a server computing device. Although <figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts three computing devices <b>102</b>, <b>202</b>, <b>204</b> and one storage device <b>214</b>, other embodiments are not limited to this configuration. The system <b>200</b> can include any suitable number of computing devices and/or storage devices.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a flowchart of a method of generating known topics in accordance with some embodiments. The representative method produces a global list of known topics for multiple users, such as the users in an enterprise. In some embodiments, the global list can also include information that is associated with the known topics. As noted earlier, a topic can include, but is not limited to, acronyms, codenames, abbreviations, tools, teams, project names, and organizational units.</p><p id="p-0039" num="0038">Initially, the activity data is analyzed at block <b>300</b> to identify one or more topics, information associated with the topic(s), other topics that are related to the identified topic, the users that know the topics, and/or one or more candidate definitions associated with each topic (blocks <b>302</b>, <b>304</b>, <b>306</b>). In a non-limiting nonexclusive embodiment, the activity data is associated with multiple users in an enterprise, and the activity data is analyzed to detect the topic(s) and one or more candidate definitions for each topic. As discussed previously, the activity data includes, but is not limited to, electronic communications, documents, meetings, tasks, projects, internet browser activities, search history (e.g., browser search history), personal and/or product development, training, and other activities.</p><p id="p-0040" num="0039">Information about at least one topic is determined and retrieved at block <b>304</b>, where the information can include a definition (e.g., an acronym expansion), a description, people associated with the topic, content associated with the topic, and other information about the topic. In some in aspects, the type of information retrieved for a topic is based on the type of topic. For example, a definition can be obtained for an acronym. When the topic is a codename that represents a subject, such as a project, information such as a description of the project and the people associated with the project may be obtained.</p><p id="p-0041" num="0040">Next, as shown in block <b>306</b>, the users that know the topic(s) are identified. To identify the users that know the topics, the activity data is analyzed to identify the one or more topics, the associated information, and the users. For example, the assistant application determines if the user has used the topic in the past. Additionally, in one embodiment, it is assumed a user is familiar with or knows of a topic when the topic is used, discussed, and/or referred to in the activity data of the user (e.g., in an email). For example, a user can refer to a topic in a document and/or another person may refer to a topic in a communication that is received by the user. In some instances, the assistant application determines, via the activity data, if the user has previously searched for, accessed, or otherwise interacted with information on the topic, which strongly indicates the user knows of the topic.</p><p id="p-0042" num="0041">A confidence score for each candidate definition is determined at block <b>308</b> for each user identified at block <b>306</b>. A confidence score indicates a confidence or a probability that the user knows the topic is associated with a particular candidate definition. In some instances, the confidence score is based on the interactions the user has with a topic, the frequency of the interactions, other users associated with the user that used or referred to the topic, any searches performed by the user on the topic, and so on.</p><p id="p-0043" num="0042">The topics, the information associated with the topics, the users that know the topic and associated candidate definitions, and the confidence scores are then stored at block <b>310</b>. In one embodiment, the topic(s), the associated information, the users (or identifiers for the users), and the confidence scores are grouped into a global list of known topics and the global list is stored. In some instances, one or more topics that are related to a particular topic are also stored in the global list of known topics. An example global list of known topics is discussed in more detail in conjunction with <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0044" num="0043">In one embodiment, the activity data is analyzed in real-time, in substantially real-time, at regular intervals, at selected times, and/or on demand to detect new topics and to update the global list. A determination is made at block <b>312</b> as to whether a new topic is detected during the analysis process. If a determination is made that a new topic has not be detected, the method waits at block <b>312</b>. When a determination is made at block <b>312</b> that a new topic is detected, the method continues at block <b>314</b> where the new topic, the information associated with the new topic, the user(s) that know associated candidate information for the new topic, and the confidence score(s) are obtained and the new topic, associated information, and confidence scores are stored in the global list.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example global list of known topics, information associated with the topics, users, and confidence scores in accordance with some embodiments. The example global list <b>400</b> is created at block <b>310</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In the example embodiment shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the global list <b>400</b> includes five columns, a topic column <b>402</b>, a definition column <b>404</b>, a source column <b>406</b>, a related topics column <b>408</b>, and a user id column <b>410</b>.</p><p id="p-0046" num="0045">In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the topic is acronyms, and the topic column <b>402</b> includes the acronym &#x201c;SSR&#x201d; <b>412</b> and the acronym &#x201c;TTS&#x201d; <b>414</b>. The acronym SSR <b>412</b> is associated with the definition <b>416</b> of search success rate, the definition <b>418</b> of service sales representative, and the definition <b>420</b> of small screen rendering. The acronym TTS <b>414</b> is associated with the definition <b>422</b> of time to success, the definition <b>424</b> of teacher training school, the definition <b>426</b> of time to stabilization, and the definition <b>428</b> of tenure track system. Different acronyms can be listed in other embodiments.</p><p id="p-0047" num="0046">The source column <b>406</b> lists an identifier for the source of each definition <b>416</b>, <b>418</b>, <b>420</b>, <b>422</b>, <b>424</b>, <b>426</b>, <b>428</b>. In the illustrated embodiment, the source of each definition <b>416</b>, <b>418</b>, <b>420</b>, <b>422</b>, <b>424</b>, <b>426</b>, <b>428</b> is a uniform resource locator (URL) <b>430</b>. Other types of identifiers can be used in some embodiments. For example, the title of the document, the uniform resource identifier, and/or a file name may be listed as the source of a definition.</p><p id="p-0048" num="0047">The related topics column <b>408</b> lists one or more topics for each acronym <b>412</b>, <b>414</b> that are related to a respective definition <b>416</b>, <b>418</b>, <b>420</b>, <b>422</b>, <b>424</b>, <b>426</b>, <b>428</b>. The related topic(s) are topics that co-occur with the respective definition <b>416</b>, <b>418</b>, <b>420</b>, <b>422</b>, <b>424</b>, <b>426</b> <b>428</b>. In some embodiments, a related topic can co-occur with a respective definition frequently, recurrently, regularly, or a sufficient number of times within the same text or activity data and/or as part of the same subject matter. For example, the related topics <b>432</b> for the definition <b>416</b> are search, ranking, and metrics. The one or more related topics can be used to determine or confirm the correct definition of one or more topics. In a non-limiting nonexclusive example, an acronym is spoken during a meeting. The assistant application can use one or more related topics in addition to the definition(s) as known to the Speaker and possibly other persons (e.g., other persons in the meeting) to determine or confirm the correct definition of the acronym.</p><p id="p-0049" num="0048">When a topic (&#x201c;first topic&#x201d;) with multiple definitions is used in a conversation (e.g., in a meeting), the assistant application identifies one or more other topics that have been used in the conversation. For example, the assistant application analyzes the text of the conversation in substantially real-time to detect the one or more other topics using, for example, one or more text mining and/or data mining applications. In an online chat, the assistant application detects the other topic(s) in the text of the posted messages in substantially real-time (e.g., using one or more text mining and/or data mining applications). In one embodiment, the one or more other topics are collected into a list.</p><p id="p-0050" num="0049">The assistant application determines the overlap between the other topic(s) in the list and the related topic(s) listed in the global list <b>400</b> for the multiple definitions associated with the first topic. The higher the overlap between the other topic(s) in the list and the related topic(s) for a respective definition associated with the first topic, the more likely the respective definition is the correct definition for the first topic. Additionally or alternatively, the assistant application may determine whether the one or more other topics in the list and the related topics listed in the global list <b>400</b> for the multiple definitions associated with the first topic belong to or are associated with the same subject matter. In one embodiment, the assistant application accesses a taxonomy of topics or semantic embeddings of topics to determine if the other topic(s) and the related topics belong to or are associated with the same subject matter. The semantic embeddings of topics can be created using deep learning mechanisms (e.g., algorithms, models, or applications), such as the Word2vec algorithm.</p><p id="p-0051" num="0050">The user id column <b>410</b> lists one or more users (or user identifiers <b>434</b>) that know the acronym <b>412</b>, <b>414</b> and the associated definitions <b>416</b>, <b>418</b>, <b>420</b>, <b>422</b>, <b>424</b>, <b>426</b>, <b>428</b>. In the illustrated embodiment, the user id column <b>410</b> also includes a confidence score <b>436</b> for each identified user. The confidence score indicates a confidence or a probability that the user knows a particular candidate definition is the definition for the topic. In some instances, the confidence score is based on which candidate definitions of the topic the user that used the topic knows, which candidate definitions of the topic are known by other persons in the conversation (e.g., a meeting), which candidate definitions other persons that are working with the user know of, and/or how well do other topics that have been mentioned in the conversation match or relate to the related topics listed in the global list <b>400</b> for each candidate definition. Additional or different factors for a confidence score can be considered in other embodiments.</p><p id="p-0052" num="0051">In the example global list <b>400</b>, each user and an associated confidence score are stored as a &#x3c;user id, score&#x3e; pair, and at least one &#x3c;user id, score&#x3e; pair can be stored with each acronym <b>412</b>, <b>414</b> and associated definition <b>416</b>, <b>418</b>, <b>420</b>, <b>422</b>, <b>424</b>, <b>426</b>, <b>428</b>. For example, for the acronym SSR <b>412</b> and the definition &#x201c;search success rate&#x201d; <b>416</b>, the &#x3c;user id, score&#x3e; pair <b>438</b> includes the user id 2378 and the confidence score 0.8. The confidence score represents the probability (80%) that the user associated with the user id 2378 knows that SSR can mean search success rate. For the same acronym SSR <b>412</b> and the same associated definition <b>416</b>, the &#x3c;user id, score&#x3e; pair <b>440</b> includes the user id 3234 and the confidence score 0.98. The confidence score represents the probability (98%) that the user associated with the user id 3234 knows SSR can mean search success rate. Alternatively, for the acronym TTS <b>414</b> and the definition &#x201c;tenure track system&#x201d; <b>428</b>, the &#x3c;user id, score&#x3e; pair <b>442</b> includes the user id 3721 and the confidence score 0.67. The confidence score represents the probability (67%) that the user associated with the user id 3721 knows that TTS can mean tenure track system.</p><p id="p-0053" num="0052">In some embodiments, the assistant application (e.g., assistant application <b>132</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) pre-generates the global list <b>400</b> and updates the global list by analyzing the activity data of multiple users (e.g., the users in an enterprise) in real-time, in substantially real-time, at selected times, or on demand. Based on the analysis of the activity data, the assistant application detects topics, infers one or more definitions of each topic, and infers which users are familiar with a &#x3c;topic, definition&#x3e; pair in real-time, in substantially real-time, at selected times, or on demand. In some embodiments, assistant application accesses publicly available sources (e.g., the Internet) and enterprise-internal lists of topics with definitions when detecting topics and inferring one or more definitions of each topic.</p><p id="p-0054" num="0053">A global list <b>400</b> can include additional information or omit some of the information shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. For example, the related topics may be omitted in other embodiments. Additionally or alternatively, the confidence score in the user id column can be omitted. In such embodiments, the definitions of a topic that are identified as being known to the user of the topic (e.g., the user that spoke the topic) are presented to another user as candidate definitions for the topic.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>B</figref> illustrate a flowchart of a method of providing information on an unknown topic in accordance with some embodiments. In some instances, the method of <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>B</figref> is performed after the global list <b>400</b> shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref> is initially generated. Although the illustrated method is described in conjunction with a first user that uses a topic and a second user that hears or reads the topic, other embodiments are not limited to this implementation. The method of <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>B</figref> can be performed for multiple users at a time (e.g., multiple users in a meeting).</p><p id="p-0056" num="0055">Initially, as shown in block <b>500</b>, the use of a topic in a conversation is detected, where the conversation includes the first user and the second user. For example, the topic may be used in an online meeting, an online chat, a presentation, a seminar, or an in-person meeting. When the topic is used in an online or an in-person meeting, an STT application (e.g., STT application <b>130</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) converts the audio input of the meeting into text in substantially real-time and an assistant application (e.g., assistant application <b>132</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) analyzes the text in substantially real-time to detect the topic using, for example, one or more text mining and/or data mining applications. In an online chat, the assistant application detects the topic in the text of the posted messages in substantially real-time (e.g., using one or more text mining and/or data mining applications).</p><p id="p-0057" num="0056">One or more candidate definitions of the topic as known by the first user (e.g., the user that spoke or wrote the topic) are determined at block <b>502</b>. In a non-limiting nonexclusive example, the global list is reviewed to determine the candidate definition(s) of the topic that are known by the first user. Additionally or alternatively, the activity data associated with the first user is analyzed to ascertain the one or more candidate definitions of the topic or confirm the candidate definition(s) of the topic. In one embodiment, the first user's activity data is reviewed when the topic is not included in the global list.</p><p id="p-0058" num="0057">The assistant application then determines at block <b>504</b> whether a second user (e.g., a user that heard the topic) knows the topic and an associated candidate definition for the topic. For example, the global list is reviewed to determine which users know the topic and the associated candidate definition. A determination is made at block <b>506</b> as to whether the topic and a candidate definition of the topic are unknown to the second user in the meeting. If the topic and the associated candidate definition are known to the second user, the method continues at block <b>510</b> where a determination is made as to whether another candidate definition is associated with the topic. The method returns to block <b>500</b> if a determination is made that another candidate definition is not available. The method returns to block <b>504</b> if a determination is made that another candidate definition is available.</p><p id="p-0059" num="0058">When a determination is made at block <b>506</b> that a topic and the associated candidate definition are unknown to the second user, the method passes to block <b>508</b> where a determination is made as to whether a high confidence score (or a confidence score that is above a threshold value) is associated with the first user for the candidate definition. For example, as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, each user id can include a &#x3c;user id, score&#x3e; pair, where the confidence score represents a probability that the user associated with the user id knows the candidate definition is likely the definition for the topic. If a determination is made that the confidence score is not high, or is not above a threshold value, the method passes to block <b>510</b>.</p><p id="p-0060" num="0059">When a determination is made at block <b>508</b> that a high confidence score, or a confidence score that is above a threshold value, is associated with the candidate definition, the method continues at block <b>512</b> where the topic and the candidate definition are provided for presentation. For example, the topic and the candidate definition can be displayed on a display device as &#x201c;The first user likely meant SSR to mean Search Success Rate.&#x201d;</p><p id="p-0061" num="0060">In some embodiments, the assistant application considers the confidence score associated with the first user as well as a confidence score associated with the second user for the same candidate definition in determining whether to provide the topic and associated candidate definition for presentation. Since the global list (e.g., the user id column) identifies all users that know of a candidate definition, the first user and the second user may both be listed in the user id column for a particular candidate definition. For example, for the acronym SSR <b>412</b> and the definition <b>416</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the user associated with the user id 2378 may be the first user and the user associated with the user id 2391 can be the second user. In such embodiments, the assistant application can consider the confidence score associated with both the first and the second users when determining whether to provide the topic and associated candidate definition to an output device for presentation to the second user.</p><p id="p-0062" num="0061">In a non-limiting nonexclusive example, a confidence score of 0.85 is associated with the first user for a topic and associated candidate definition and a confidence score of 0.10 is associated with the second user for the same topic and associated candidate definition. Based on the low confidence score for the second user, the assistant application may determine to present the topic and associated candidate definition to the second user. Alternatively, if a confidence score of 0.85 is associated with the first user for a topic and associated candidate definition and a confidence score of 0.64 is associated with the second user for the same topic and associated candidate definition, the assistant application may determine to not present the topic and associated candidate definition to the second user based on the higher confidence score for the second user. In some embodiments, the determination of whether to present or not present a topic and associated candidate definition to a second user in view of a confidence score associated with the second user can be based on the confidence score meeting or exceeding a threshold value (or minimum value).</p><p id="p-0063" num="0062">Next, as shown in block <b>514</b>, additional information related to the topic can be provided for presentation. The additional information includes, but is not limited to, a reference to the source of the definition, the confidence score associated with the definition, and/or content that uses the topic or includes a reference to the topic. For example, a communication such as an email can be identified as a source of a definition of the topic.</p><p id="p-0064" num="0063">The global list is updated to identify the second user as a user that knows the topic and the associated candidate definition (block <b>516</b>). The global list may be updated, for example, based on the presentation of the topic and the associated candidate definition. Additionally or alternatively, the global list can be updated if the second user interacts with the candidate definition (e.g., selects the candidate definition), discusses the topic and/or the candidate definition with other users in the meeting or chat, and/or creates activity data that uses or references the topic and/or the candidate definition. The method then passes to block <b>510</b> and repeats for each candidate definition.</p><p id="p-0065" num="0064">The candidate definitions(s) can be provided to one or more computing devices and/or output devices the second user is able to access during the meeting. In a non-limiting nonexclusive example, for an online meeting or chat, the one or more candidate definitions can be provided to a display screen and displayed in the GUI associated with the online meeting software application or the online chat software application, or in a separate GUI. In an in-person meeting, the candidate definition(s) may be provided to a display screen of another computing device associated with the second user, such as a mobile telephone. Other types of output devices that can present the one or more meanings to the second user include, but are not limited to, a braille reader, a speaker device, headphones or earbuds, a television, and a projector.</p><p id="p-0066" num="0065">The method depicted in <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>B</figref> can produce one or multiple candidate definitions for a topic. <figref idref="DRAWINGS">FIG. <b>5</b>C</figref> illustrates an alternative flowchart to the flowchart shown in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> in accordance with some embodiments. The flowchart depicted in <figref idref="DRAWINGS">FIG. <b>5</b>C</figref> is the same as the flowchart shown in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> except for blocks <b>518</b>, <b>520</b>, <b>522</b>. Although blocks <b>514</b> and <b>516</b> are not shown in <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>, blocks <b>514</b> and <b>516</b> can be performed after block <b>512</b> in some embodiments.</p><p id="p-0067" num="0066">Generally, the method shown in <figref idref="DRAWINGS">FIG. <b>5</b>C</figref> does not include a return loop to review one or more additional candidate definitions for a topic (e.g., block <b>510</b> is omitted). Instead, in <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>, the candidate definition that is associated with the highest confidence score is provided to one or more output devices. At block <b>518</b>, the assistant application determines whether the second user knows each candidate definition for the topic. A determination is made at block <b>520</b> as to whether each candidate definition is unknown to the second user. If a determination is made at block <b>520</b> that the second user knows each candidate definition for the topic, the method returns to block <b>500</b>. When a determination is made at block <b>520</b> that the second user does not know at least one candidate definition for the topic, the method continues at block <b>522</b> where the assistant application reviews the confidence score associated with the first user for each candidate definition and selects the candidate definition with the highest confidence score. The topic and the associated candidate definition with the highest confidence score are provided for presentation at block <b>512</b>.</p><p id="p-0068" num="0067">In some embodiments, the topic is an acronym and one or more meanings for the acronym are presented to a user. <figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>B</figref> illustrate a flowchart of a method of providing one or more meanings for an unknown acronym in accordance with some embodiments. Some of the blocks shown in <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>B</figref> are included in the flowchart of <figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>B</figref> and are identified by the same reference number. For brevity, the descriptions of these blocks are not repeated in the discussion of <figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>B</figref>.</p><p id="p-0069" num="0068">In this example embodiment, the unknown acronym is used by a first user (a speaker) in an online meeting. The usage of the acronym during the online meeting is detected at block <b>500</b>. As described earlier, an STT application (e.g., STT application <b>130</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) is converting the audio input of the online meeting into text in substantially real-time and an assistant application (e.g., assistant application <b>132</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) is analyzing the text in substantially real-time to detect acronyms. In one embodiment, the assistant application detects the acronym using one or more text mining and/or data mining applications.</p><p id="p-0070" num="0069">The meaning of the acronym as understood by the speaker is determined at block <b>602</b>. In a non-limiting nonexclusive example, the global list is reviewed to determine the candidate definition(s) of the acronym that are known by the speaker. Additionally or alternatively, the activity data associated with the speaker is analyzed to ascertain the one or more candidate definitions of the acronym or confirm the candidate definition(s) of the acronym. In one embodiment, the speaker's activity data is reviewed when the acronym is not included in the global list.</p><p id="p-0071" num="0070">Next, as shown in block <b>604</b>, the assistant application determines whether a second user (e.g., a listener) knows the acronym and an associated candidate definition for that acronym. For example, the global list is reviewed to determine which users know the acronym and the associated candidate definition. A determination is made at block <b>506</b> as to whether the acronym is unknown to the listener. If a determination is made that the listener knows the acronym, the method passes to block <b>510</b>.</p><p id="p-0072" num="0071">When a determination is made at block <b>506</b> that the acronym is unknown to the listener, the method continues at block <b>508</b>. Block <b>510</b> or blocks <b>512</b>, <b>514</b>, and <b>516</b> are then performed. In some embodiments, the method shown in <figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>B</figref> can be implemented to execute similar to the method depicted in <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>, where only the candidate definition with the highest confidence score is provided for presentation. Additionally or alternatively, the confidence scores associated with both the speaker and the listener for the acronym and associated candidate definition are considered when determining whether the listener knows the acronym and associated candidate definition (see description of <figref idref="DRAWINGS">FIG. <b>5</b></figref>).</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example GUI that presents one or more meanings for an unknown acronym in accordance with some embodiments. The example GUI <b>700</b> is associated with an online meeting software application. There are four users in the online meeting; Elena, Will, Malik, and Sela. The GUI <b>700</b> presents a display window or panel <b>702</b> for Elena, a panel <b>704</b> for Will, a panel <b>706</b> for Malik, and a panel <b>708</b> for Sela. The GUI <b>700</b> is presented on a computing device that is used by one of the users, for example by Will.</p><p id="p-0074" num="0073">During the online meeting, a user such as Elena uses the acronym &#x201c;AR&#x201d; during her conversation or presentation. In general, the acronym &#x201c;AR&#x201d; has several definitions. For example, &#x201c;AR&#x201d; can mean accounts receivable, augmented reality, annual return, alternate reality, annual report, the state of Arkansas, Army regulation, assault rifle, the chemical element Argon, and annual review. An assistant application (e.g., assistant application <b>132</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) detects the use of the acronym and determines that the acronym &#x201c;AR&#x201d; is unknown to Will. Accordingly, the assistant application causes one or more definitions of &#x201c;AR&#x201d; that have been identified as known to Elena to be displayed in the GUI <b>700</b>.</p><p id="p-0075" num="0074">In the illustrated GUI <b>700</b>, three definitions of &#x201c;AR&#x201d; are displayed. The panel <b>710</b> presents a definition <b>712</b> for AR as &#x201c;Accounts Receivable.&#x201d; Additional information for the definition <b>712</b> is also displayed. The additional information includes a confidence score (85%) <b>714</b> that indicates a probability (85%) that Elena knows that AR can mean &#x201c;Accounts Receivable&#x201d;. The confidence score <b>714</b> is displayed with information that indicates there is a probability of 85% that Elena means accounts receivable when she used the acronym AR. The additional information further includes a source <b>716</b> of the definition <b>712</b> and a graphical representation <b>718</b> of the source <b>716</b>. The source <b>716</b> is a reply email that Will received from Elena. The reply email relates to the Q3 Budget. In some instances, the graphical representation <b>718</b> functions as a link that, when selected (e.g., clicked on), causes the reply email to open, causes an email software application to launch and display the reply email, or causes presentation of a folder that stores the reply email.</p><p id="p-0076" num="0075">The panel <b>720</b> presents a definition <b>722</b> for AR as &#x201c;Annual Return.&#x201d; Additional information for the definition <b>722</b> is also displayed. The additional information includes a source <b>724</b> of the definition <b>722</b> and a graphical representation <b>726</b> of the source <b>724</b>. The source <b>724</b> is a document entitled &#x201c;Income Statement 2020&#x201d; and the acronym AR is discussed or referred to in the document. In some instances, the graphical representation <b>726</b> functions as a link.</p><p id="p-0077" num="0076">The panel <b>728</b> presents a definition <b>730</b> for AR as &#x201c;Annual Report.&#x201d; Additional information for the definition <b>730</b> is also displayed. The additional information includes a source <b>732</b> of the definition <b>730</b>, which is identified as a financial meeting. A graphical representation <b>734</b> that represents an audio recording of the financial meeting is also displayed in the panel <b>728</b>. In some instances, the graphical representation <b>734</b> functions as a link that when selected (e.g., clicked on), causes the audio file of the recording to open, causes a media player to launch and play the audio recording, or causes presentation of a folder that stores the audio recording.</p><p id="p-0078" num="0077">An input element <b>736</b> may be included in the GUI <b>700</b>. Selection of the input element <b>736</b> can cause additional meanings to be presented and/or cause additional information to be presented. The additional information may include the details of the online meeting, information related to the subject matter of the online meeting, contact information for one or more of the users Elena, Will, Malik, or Sela, and other information. The input element <b>736</b> can be any suitable type of input element, including, but not limited to, a button and a checkbox.</p><p id="p-0079" num="0078">In some embodiments, the topic can be a codename that represents a subject such as a project, a task, a client, an event, or other item. In such instances, information related to the subject associated with the codename is presented to a user. <figref idref="DRAWINGS">FIGS. <b>8</b>A-<b>8</b>B</figref> illustrate a flowchart of a method of providing information on an unknown codename in accordance with some embodiments. Some of the blocks shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> are included in the flowchart of <figref idref="DRAWINGS">FIGS. <b>8</b>A-<b>8</b>B</figref> and are identified by the same reference number. For brevity, the descriptions of these blocks are not repeated in the discussion of <figref idref="DRAWINGS">FIGS. <b>8</b>A-<b>8</b>B</figref>.</p><p id="p-0080" num="0079">In this example embodiment, the unknown codename is for a project and is used by a first user in an in-person meeting. The usage of the codename during the in-person meeting is detected at block <b>800</b>. For example, in one embodiment a STT application (e.g., STT application <b>130</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) is receiving the audio input of the in-person meeting and converting the audio input into text in substantially real-time. An assistant application (e.g., assistant application <b>132</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) analyzes the text and detects the usage of the codename.</p><p id="p-0081" num="0080">In some entities, a codename for a project is reused after a period of time. Thus, the codename spoken by the first user can be a codename for a current project or a prior project. Accordingly, the definition of the codename as understood by the first user is determined at block <b>802</b>. In a non-limiting example, the global list is reviewed to locate the codename and one or more candidate definitions of the codename. Additionally or alternatively, the activity data of the first user is analyzed to determine or to confirm the definition of the codename. For example, the first user's activity data can be reviewed when the codename is not included in the global list.</p><p id="p-0082" num="0081">Next, as shown in block <b>804</b>, a determination is made as to whether a candidate definition for the codename is available (e.g., is in the global list). If a determination is made that a candidate definition is not available (e.g., not in the global list and not in the activity data), the method passes to block <b>806</b> where a search of publicly available data is performed, and the search results are provided for presentation. For example, the search of the publicly available data may be an Internet search. The method ends after block <b>806</b> is performed.</p><p id="p-0083" num="0082">When a determination is made at block <b>806</b> that at least one candidate definition is available, the method continues at block <b>808</b> where the assistant application determines whether a second user (e.g., a user that heard the topic) knows the codename and an associated candidate definition for the codename. For example, the global list is reviewed to determine which users know the codename and the associated candidate definition of the codename.</p><p id="p-0084" num="0083">A determination is made at block <b>506</b> as to whether the codename and a candidate definition of the codename are unknown to the second user. If a determination is made that the codename and the meaning are known to the second user, the method passes to block <b>510</b>. When a determination is made at block <b>506</b> that the codename and the candidate definition are unknown to the second user, the method passes to block <b>508</b>.</p><p id="p-0085" num="0084">Information associated with the candidate definition of the codename is retrieved for presentation at block <b>810</b>. The information associated with a project includes, but is not limited to, a description, information on the people who are working on and/or are associated with the project, information on one or more tasks associated with the project, and content that is relevant to the project. In some embodiments, the confidence scores associated with both the first user and the second user for the codename and associated candidate definition are considered when determining whether to present the codename and associated candidate definition to the second user (see description <figref idref="DRAWINGS">FIG. <b>5</b></figref>).</p><p id="p-0086" num="0085">The codename and the information are presented to the second user (block <b>812</b>), and the global list is updated (block <b>516</b>). The information can be provided to one or more computing devices and/or output devices that are accessible to the second user in the in-person meeting. For example, the information may be displayed on a mobile telephone, a laptop computer, a television, and/or a projector.</p><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example GUI that presents information associated with an unknown codename in accordance with some embodiments. The GUI <b>900</b> includes a panel <b>902</b> that presents information <b>904</b> on a project associated with the codename <b>906</b> &#x201c;Mars.&#x201d; The information <b>904</b> includes a description <b>908</b> of the project. The panel <b>902</b> includes an area <b>910</b> for providing information on one or more users that work on or are associated with the project. In the illustrated embodiment, the area <b>910</b> displays graphical representations <b>912</b> of four users, Elena, Will, Malik, and Sela. Each graphical representation <b>912</b> can be a photograph of the user, an image selected by or for the user, or a graphical representation (e.g., avatar) that is associated with the user.</p><p id="p-0088" num="0087">Other information for the users may be presented. For example, the user information <b>914</b> for Elena includes her name <b>916</b>, her title <b>918</b> on the project, and an email address <b>920</b>. The user information <b>922</b> for Will includes his name <b>924</b>, his title <b>926</b> on the project, and an email address <b>928</b>. The user information <b>930</b> for Malik includes his name <b>932</b>, an email address <b>934</b>, and telephone extension <b>936</b>. The user information <b>938</b> for Sela includes her name <b>940</b>, an email address <b>942</b>, and a location <b>944</b> of her office. Other types of information can be displayed in other embodiments. For example, user information such as an address, a facsimile number, a name of an assistant, an indicator that indicates if the user is currently online or in the office (e.g., green circle for online or in office, red circle for offline or out of office), a name of a manager or supervisor, one or more current tasks the user is associated with, and/or a link to a curriculum vitae can be presented.</p><p id="p-0089" num="0088">In some embodiments, the panel <b>902</b> includes an area <b>946</b> that lists or displays content associated with the project. For example, the area <b>946</b> may present links to documents, communications, calendars, and other content that is relevant to the project. In the illustrated GUI <b>900</b>, the area <b>946</b> displays a document D<b>1</b> <b>948</b> and a document D<b>2</b> <b>950</b>. In some instances, at least one graphical representation of a document <b>948</b>, <b>950</b> functions as a link that when selected (e.g., clicked on), causes the document to open, causes a software application to launch and display the document, or causes presentation of a folder that stores the document.</p><p id="p-0090" num="0089">Additional information may be displayed for one or more content items in the area <b>946</b>. In the example embodiment, the additional information <b>952</b> for the document D<b>1</b> <b>948</b> is a title and an author of the document. The additional information <b>954</b> for the document D<b>2</b> <b>950</b> is a title and a date of the last edit to the document. Other additional information can be displayed in other embodiments. For example, a brief description of the content, a version number of the content, and/or a storage location of the content may be presented.</p><p id="p-0091" num="0090">A search tool <b>956</b> can be included in the GUI <b>900</b>. The search tool <b>956</b> enables a user to search for information on a subject (e.g., a codename or a project) and/or for additional information on the project associated with the codename <b>906</b>.</p><p id="p-0092" num="0091"><figref idref="DRAWINGS">FIGS. <b>10</b>-<b>12</b></figref> and the associated descriptions provide a discussion of a variety of operating environments in which aspects of the disclosure may be practiced. However, the devices and systems illustrated and discussed with respect to <figref idref="DRAWINGS">FIGS. <b>10</b>-<b>12</b></figref> are for purposes of example and illustration and are not limiting of a vast number of electronic device configurations that may be utilize for practicing aspects of the disclosure, as described herein.</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a block diagram illustrating physical components (e.g., hardware) of an electronic device with which aspects of the disclosure may be practiced. In a basic configuration, the electronic device <b>1000</b> may include at least one processing device <b>1002</b> and a memory <b>1004</b>. Any suitable processing device <b>1002</b> can be used. For example, the processing device <b>1002</b> may be a microprocessor, an application specific integrated circuit, a field programmable gate array, or combinations thereof.</p><p id="p-0094" num="0093">Depending on the configuration and type of the electronic device <b>1000</b>, the memory <b>1004</b> may comprise, but is not limited to, volatile storage (e.g., random access memory), non-volatile storage (e.g., read-only memory), flash memory, or any combination of such memories. The memory <b>1004</b> may include a number of program modules and data files, such as an operating system <b>1006</b>, program modules <b>1008</b>, and an assistant software application <b>1010</b>. While executing on the processing device <b>1002</b>, the assistant software application <b>1010</b> may perform and/or cause to be performed processes including, but not limited to, the aspects as described herein.</p><p id="p-0095" num="0094">The operating system <b>1006</b>, for example, may be suitable for controlling the operation of the electronic device <b>1000</b>. Furthermore, embodiments of the disclosure may be practiced in conjunction with a graphics library, other operating systems, or any other application program and is not limited to any particular application or system. This basic configuration is illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref> by those components within a dashed line <b>1012</b>.</p><p id="p-0096" num="0095">The electronic device <b>1000</b> may have additional features or functionality. For example, the electronic device <b>1000</b> may also include additional data storage devices (removable and/or non-removable) such as, for example, magnetic disks, optical disks, or tape. Such additional storage is illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref> by a removable storage device <b>1014</b> and a non-removable storage device <b>1016</b>.</p><p id="p-0097" num="0096">The electronic device <b>1000</b> may also have one or more input device(s) <b>1018</b> such as a keyboard, a trackpad, a mouse, a pen, a sound or voice input device, a touch, force and/or swipe input device, etc. The output device(s) <b>1020</b> such as a display, speakers, a printer, etc. may also be included. The aforementioned devices are examples and others may be used. The electronic device <b>1000</b> may include one or more communication devices <b>1022</b> allowing communications with other electronic devices <b>1024</b>. Examples of suitable communication devices <b>1022</b> include, but are not limited to, radio frequency (RF) transmitter, receiver, and/or transceiver circuitry; universal serial bus (USB), parallel, and/or serial ports.</p><p id="p-0098" num="0097">The term computer-readable media as used herein may include storage media or devices. The storage media or devices may include volatile and nonvolatile, removable, and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, or program modules.</p><p id="p-0099" num="0098">The memory <b>1004</b>, the removable storage device <b>1014</b>, and the non-removable storage device <b>1016</b> are all examples of storage devices. Each storage device may include RAM, ROM, electrically erasable read-only memory (EEPROM), flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other article of manufacture which can be used to store information and which can be accessed by the electronic device <b>1000</b>. Any such storage device may be part of the electronic device <b>1000</b>. In one embodiment, the storage device does not include a carrier wave or other propagated or modulated data signal.</p><p id="p-0100" num="0099">Communication media may be embodied by computer readable instructions, data structures, program modules, or other data in a modulated data signal, such as a carrier wave or other transport mechanism, and includes any information delivery media. The term &#x201c;modulated data signal&#x201d; may describe a signal that has one or more characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media may include wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, radio frequency (RF), infrared, and other wireless media.</p><p id="p-0101" num="0100">Furthermore, embodiments of the disclosure may be practiced in an electrical circuit comprising discrete electronic elements, packaged or integrated electronic chips containing logic gates, a circuit utilizing a microprocessor, or on a single chip containing electronic elements or microprocessors. For example, embodiments of the disclosure may be practiced via a system-on-a-chip (SOC) where each or many of the components illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref> may be integrated onto a single integrated circuit. Such an SOC device may include one or more processing units, graphics units, communications units, system virtualization units and various application functionality all of which are integrated (or &#x201c;burned&#x201d;) onto the chip substrate as a single integrated circuit.</p><p id="p-0102" num="0101">When operating via an SOC, the functionality described herein, with respect to the capability of client to switch protocols may be operated via application-specific logic integrated with other components of the electronic device <b>1000</b> on the single integrated circuit (chip). Embodiments of the disclosure may also be practiced using other technologies capable of performing logical operations such as, for example, AND, OR, and NOT, including but not limited to mechanical, optical, fluidic, and quantum technologies. In addition, embodiments of the disclosure may be practiced within a general-purpose computer or in any other circuits or systems.</p><p id="p-0103" num="0102"><figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>11</b>B</figref> illustrate a mobile electronic device <b>1100</b>, for example, a mobile telephone, a smart phone, wearable computer (such as a smart watch), a tablet computer, a laptop computer, and the like, with which embodiments of the disclosure may be practiced. With reference to <figref idref="DRAWINGS">FIG. <b>11</b>A</figref>, one aspect of a mobile electronic device <b>1100</b> for implementing the aspects described herein is illustrated.</p><p id="p-0104" num="0103">In a basic configuration, the mobile electronic device <b>1100</b> is a handheld computer having both input elements and output elements. The mobile electronic device <b>1100</b> typically includes a display <b>1102</b> and one or more input buttons <b>1104</b> that allow the user to enter information into the mobile electronic device <b>1100</b>. The display <b>1102</b> of the mobile electronic device <b>1100</b> may also function as an input device (e.g., a display that accepts touch and/or force input).</p><p id="p-0105" num="0104">If included, an optional side input element <b>1106</b> allows further user input. The side input element <b>1106</b> may be a rotary switch, a button, or any other type of manual input element. In alternative aspects, mobile electronic device <b>1100</b> may incorporate more or less input elements. For example, the display <b>1102</b> may not be a touch screen in some embodiments. In yet another alternative embodiment, the mobile electronic device <b>1100</b> is a portable phone system, such as a cellular phone. The mobile electronic device <b>1100</b> may also include an optional keypad <b>1108</b>. Optional keypad <b>1108</b> may be a physical keypad or a &#x201c;soft&#x201d; keypad generated on the touch screen display.</p><p id="p-0106" num="0105">In various embodiments, the output elements include the display <b>1102</b> for showing a graphical user interface (GUI) of a client or developer portal, a visual indicator <b>1110</b> (e.g., a light emitting diode), and/or an audio transducer <b>1112</b> (e.g., a speaker). In some aspects, the mobile electronic device <b>1100</b> incorporates a vibration transducer for providing the user with tactile feedback. In yet another aspect, the mobile electronic device <b>1100</b> incorporates input and/or output ports, such as an audio input (e.g., a microphone jack), an audio output (e.g., a headphone jack), and a video output (e.g., a HDMI port) for sending signals to or receiving signals from an external device.</p><p id="p-0107" num="0106"><figref idref="DRAWINGS">FIG. <b>11</b>B</figref> is a block diagram illustrating the architecture of one aspect of a mobile electronic device <b>1100</b>. That is, the mobile electronic device <b>1100</b> can incorporate a system (e.g., an architecture) <b>1114</b> to implement some aspects. In one embodiment, the system <b>1114</b> is implemented as a &#x201c;smart phone&#x201d; capable of running one or more applications (e.g., browser, e-mail, calendaring, contact managers, messaging clients, games, media clients/players, diagramming, and sharing applications and so on). In some aspects, the system <b>1114</b> is integrated as an electronic device, such as an integrated personal digital assistant (PDA) and wireless phone.</p><p id="p-0108" num="0107">One or more application programs <b>1116</b> may be loaded into the memory <b>1118</b> and run on or in association with the operating system <b>1120</b>. Examples of the application programs include a phone dialer program, an electronic communication program (e.g., email program, instant message program), a triggering application program, a word processing program, a spreadsheet program, an Internet browser program, and so forth.</p><p id="p-0109" num="0108">The system <b>1114</b> also includes a non-volatile storage area <b>1122</b> within the memory <b>1118</b>. The non-volatile storage area <b>1122</b> may be used to store persistent information that should not be lost when the system <b>1114</b> is powered down.</p><p id="p-0110" num="0109">The application programs <b>1116</b> may use and store information in the non-volatile storage area <b>1122</b>, such as email, attachments or other messages used by an email application, and the like. A synchronization application (not shown) also resides on the system <b>1114</b> and is programmed to interact with a corresponding synchronization application resident on a host computer to keep the information stored on the non-volatile storage area <b>1122</b> synchronized with corresponding information stored at the host computer.</p><p id="p-0111" num="0110">The system <b>1114</b> has a power supply <b>1124</b>, which may be implemented as one or more batteries. The power supply <b>1124</b> may further include an external power source, such as an AC adapter or a powered docking cradle that supplements or recharges the batteries.</p><p id="p-0112" num="0111">The system <b>1114</b> may also include a radio interface layer <b>1126</b> that performs the function of transmitting and receiving radio frequency communications. The radio interface layer <b>1126</b> facilitates wireless connectivity between the system <b>1114</b> and the &#x201c;outside world,&#x201d; via a communications carrier or service provider. Transmissions to and from the radio interface layer <b>1126</b> are conducted under control of the operating system <b>1120</b>. In other words, communications received by the radio interface layer <b>1126</b> may be disseminated to the application programs <b>1116</b> via the operating system <b>1120</b>, and vice versa.</p><p id="p-0113" num="0112">The visual indicator <b>1110</b> may be used to provide visual notifications, and/or an audio interface <b>1128</b> may be used for producing audible notifications via an audio transducer (e.g., audio transducer <b>1112</b> illustrated in <figref idref="DRAWINGS">FIG. <b>11</b>A</figref>). In the illustrated embodiment, the visual indicator <b>1110</b> is a light emitting diode (LED) and the audio transducer <b>1112</b> may be a speaker. These devices may be directly coupled to the power supply <b>1124</b> so that when activated, they remain on for a duration dictated by the notification mechanism even though the processor <b>1130</b> and other components might shut down for conserving battery power. The LED may be programmed to remain on indefinitely until the user takes action to indicate the powered-on status of the device.</p><p id="p-0114" num="0113">The audio interface <b>1128</b> is used to provide audible signals to and receive audible signals from the user (e.g., voice input such as described above). For example, in addition to being coupled to the audio transducer <b>1112</b>, the audio interface <b>1128</b> may also be coupled to a microphone to receive audible input, such as to facilitate a telephone conversation.</p><p id="p-0115" num="0114">The system <b>1114</b> may further include a video interface <b>1132</b> that enables an operation of peripheral device <b>1134</b> (e.g., on-board camera) to record still images, video stream, and the like.</p><p id="p-0116" num="0115">A mobile electronic device <b>1100</b> implementing the system <b>1114</b> may have additional features or functionality. For example, the mobile electronic device <b>1100</b> may also include additional data storage devices (removable and/or non-removable) such as, magnetic disks, optical disks, or tape. Such additional storage is illustrated in <figref idref="DRAWINGS">FIG. <b>11</b>B</figref> by the non-volatile storage area <b>1122</b>.</p><p id="p-0117" num="0116">Data/information generated or captured by the mobile electronic device <b>1100</b> and stored via the system <b>1114</b> may be stored locally on the mobile electronic device <b>1100</b>, as described above, or the data may be stored on any number of storage media that may be accessed by the device via the radio interface layer <b>1126</b> or via a wired connection between the mobile electronic device <b>1100</b> and a separate electronic device associated with the mobile electronic device <b>1100</b>, for example, a server-computing device in a distributed computing network, such as the Internet (e.g., server computing device <b>1114</b> in <figref idref="DRAWINGS">FIG. <b>11</b></figref>). As should be appreciated such data/information may be accessed via the mobile electronic device <b>1100</b> via the radio interface layer <b>1126</b> or via a distributed computing network. Similarly, such data/information may be readily transferred between electronic devices for storage and use according to well-known data/information transfer and storage means, including email and collaborative data/information sharing systems.</p><p id="p-0118" num="0117">As should be appreciated, <figref idref="DRAWINGS">FIG. <b>11</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>11</b>B</figref> are described for purposes of illustrating the present methods and systems and is not intended to limit the disclosure to a particular sequence of steps or a particular combination of hardware or software components.</p><p id="p-0119" num="0118"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates a block diagram of a distributed system in which aspects of the disclosure may be practiced. The system <b>1200</b> includes a general computing device <b>1202</b> (e.g., a desktop computer), a tablet computing device <b>1204</b>, and/or a mobile computing device <b>1206</b>. The general computing device <b>1202</b>, the tablet computing device <b>1204</b>, and the mobile computing device <b>1206</b> can each include the components, or be connected to the components, that are shown associated with the electronic device <b>1000</b> in <figref idref="DRAWINGS">FIG. <b>10</b></figref> or the mobile electronic device <b>1100</b> in <figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>11</b>B</figref>.</p><p id="p-0120" num="0119">The general computing device <b>1202</b>, the tablet computing device <b>1204</b>, and the mobile computing device <b>1206</b> are each configured to access one or more networks (represented by network <b>1208</b>) to interact with the assistant application <b>1210</b> stored on one or more storage devices (represented by storage device <b>1212</b>) and executed on one or more server computing devices (represented by server computing device <b>1214</b>). In some aspects, the server computing device <b>1214</b> can access and/or receive various types of services, communications, documents and information transmitted from other sources, such as a web portal <b>1216</b>, an electronic communications services <b>1218</b>, directory services <b>1220</b>, instant messaging and/or text services <b>1222</b>, and/or social networking services <b>1224</b>. In some instances, these sources may provide robust reporting, analytics, data compilation and/or storage service, etc., whereas other services may provide search engines or other access to data and information, images, graphics, videos, document processing and the like.</p><p id="p-0121" num="0120">As should be appreciated, <figref idref="DRAWINGS">FIG. <b>11</b></figref> is described for purposes of illustrating the present methods and systems and is not intended to limit the disclosure to a particular sequence of steps or a particular combination of hardware or software components.</p><p id="p-0122" num="0121">Aspects of the present disclosure, for example, are described above with reference to block diagrams and/or operational illustrations of methods, systems, GUIs, and computer program products according to aspects of the disclosure. As discussed earlier, the operations noted in the blocks may occur out of the order as shown in any flowchart. For example, two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order, depending upon the functionality/acts involved. Additionally, the functionality or elements shown in one GUI can be used in another GUI, and vice versa.</p><p id="p-0123" num="0122">The description and illustration of one or more aspects provided in this application are not intended to limit or restrict the scope of the disclosure as claimed in any way. The aspects, examples, and details provided in this application are considered sufficient to convey possession and enable others to make and use the best mode of claimed disclosure. The claimed disclosure should not be construed as being limited to any aspect, example, or detail provided in this application. Regardless of whether shown and described in combination or separately, the various features (both structural and methodological) are intended to be selectively included or omitted to produce an embodiment with a particular set of features. Having been provided with the description and illustration of the present application, one skilled in the art may envision variations, modifications, and alternative aspects falling within the spirit of the broader aspects of the general inventive concept embodied in this application that do not depart from the broader scope of the claimed disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method, comprising:<claim-text>detecting a use of a topic in a conversation between a first user and a second user, the first user using the topic in the conversation;</claim-text><claim-text>determining a candidate definition of the topic as known by the first user;</claim-text><claim-text>determining the topic and the candidate definition are unknown to the second user;</claim-text><claim-text>determining the topic and the candidate definition are to be presented to the second user; and</claim-text><claim-text>providing the topic and the candidate definition for presentation.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the candidate definition of the topic as known by the first user comprises reviewing a global list that includes one or more topics, at least one candidate definition for each topic, and an identifier for one or more users that know a respective topic and a respective candidate definition for the respective topic.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the topic and the candidate definition are unknown to the second user comprises reviewing a global list that includes one or more topics, at least one candidate definition for each topic, and an identifier for one or more users that know a respective topic and a respective candidate definition for the respective topic.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the topic and the candidate definition are known to the first user comprises reviewing activity data associated with the first user, the activity data representing one or more activities of the first user.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the candidate definition of the topic for the second user comprises analyzing activity data to determining the candidate definition of the topic, wherein the activity data is associated with the first user, the second user, and one or more users associated with at least one of the first or the second user, and the activity data comprises data associated with activities of each of the first user, the second user, and the one or more users associated with the at least one of the first or the second user.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the activities are associated with one or more of:<claim-text>electronic communications;</claim-text><claim-text>documents;</claim-text><claim-text>tasks;</claim-text><claim-text>projects;</claim-text><claim-text>meetings;</claim-text><claim-text>calendars; or</claim-text><claim-text>contacts.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the conversation is an online meeting; and</claim-text><claim-text>detecting the topic in the conversation between the first user and the second user comprises:</claim-text><claim-text>receiving text of the conversation in substantially real-time; and</claim-text><claim-text>detecting the topic in the text.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the conversation is an online chat; and</claim-text><claim-text>detecting the topic in the conversation between the first user and the second user comprises detecting the topic in a posted message.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A system, comprising:<claim-text>a processing device; and</claim-text><claim-text>a storage device operably connected to the processing device and storing instructions, that when executed by the processing device, cause operations to be performed, the operations comprising:</claim-text><claim-text>detecting a use of a topic in a conversation between a first user and a second user, the first user using the topic in the conversation;</claim-text><claim-text>determining a candidate definition of the topic as known by the first user;</claim-text><claim-text>determining the topic and the candidate definition are unknown to the second user; and</claim-text><claim-text>providing the candidate definition for presentation.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein:<claim-text>the storage device stores a speech-to-text (STT) application;</claim-text><claim-text>the conversation is an in-person meeting;</claim-text><claim-text>the STT application converts audio of the in-person meeting into text in substantially real-time; and</claim-text><claim-text>detecting the topic in the conversation between the first user and the second user comprises detecting the topic in the text.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein determining the candidate definition of the topic as known by the first user comprises reviewing one or more related topics that co-occur with the candidate definition within activity data associated with a common subject matter to determine the candidate definition.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein determining the candidate definition of the topic as known by the first user comprises reviewing a global list that includes one or more topics, at least one candidate definition for each topic, and an identifier for one or more users that know a respective topic and a respective candidate definition for the respective topic.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein determining the topic and the candidate definition are unknown to the second user comprises reviewing a global list that includes one or more topics, at least one candidate definition for each topic, and an identifier for one or more users that know a respective topic and a respective candidate definition for the respective topic.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein:<claim-text>the candidate definition is one of a plurality of candidate definitions; and</claim-text><claim-text>the storage device stores further instructions for:</claim-text><claim-text>for each candidate definition in the plurality of candidate definitions, determining a confidence score for each candidate definition that indicates a confidence that the candidate definition is known by the first user, wherein:</claim-text><claim-text>providing the candidate definition for presentation comprises providing, based on the confidence scores, one or more candidate definitions for presentation.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein providing the candidate definition for presentation comprises providing the candidate definition and additional information associated with the topic for presentation.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the additional information comprises one or more of:<claim-text>a description of the topic;</claim-text><claim-text>one or more persons associated with the topic;</claim-text><claim-text>content relating to the topic; or</claim-text><claim-text>a source of the candidate definition of the topic.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A method, comprising:<claim-text>detecting a use of a topic in a meeting between a first user and a second user, the first user speaking the topic in the meeting;</claim-text><claim-text>determining a plurality of candidate definitions of the topic that is known by the first user;</claim-text><claim-text>determining the topic and the plurality of candidate definitions are unknown to the second user;</claim-text><claim-text>determining a subset of the candidate definitions in the plurality of candidate definitions of the topic for the second user, the subset comprising some but not all of the candidate definitions in the plurality of candidate definitions; and</claim-text><claim-text>causing the subset of candidate definitions to be provided to an output device.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising causing additional information associated with the topic to be provided to the output device, the additional information comprising one or more of:<claim-text>a description of the topic;</claim-text><claim-text>one or more persons associated with the topic;</claim-text><claim-text>content relating to the topic; or</claim-text><claim-text>a source of the candidate definition of the topic.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein:<claim-text>the first and the second users are associated with an enterprise; and</claim-text><claim-text>determining the topic and the plurality of candidate definitions are unknown to the second user comprises reviewing a global list associated with the enterprise that includes one or more topics, at least one candidate definition for each topic, and an identifier for one or more users in the enterprise that know a respective topic and a respective candidate definition for the respective topic.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein determining the topic and the plurality of candidate definitions of the topic are unknown to the second user comprises reviewing a global list that includes one or more topics, at least one candidate definition for each topic, and an identifier for one or more users that know a respective topic and a respective candidate definition for the respective topic.</claim-text></claim></claims></us-patent-application>