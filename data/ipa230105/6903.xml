<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230006904A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230006904</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17806865</doc-number><date>20220614</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>IN</country><doc-number>202141029411</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>43</main-group><subgroup>0852</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>47</main-group><subgroup>33</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>43</main-group><subgroup>0852</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>47</main-group><subgroup>33</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">USING NETWORK INTERFACE CARDS HAVING PROCESSING UNITS TO DETERMINE LATENCY</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Juniper Networks, Inc.</orgname><address><city>Sunnyvale</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Kommula</last-name><first-name>Raja</first-name><address><city>Cupertino</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Sridhar</last-name><first-name>Thayumanavan</first-name><address><city>Sunnyvale</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Mariappan</last-name><first-name>Yuvaraja</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>K N</last-name><first-name>Kiran</first-name><address><city>Bangalore</city><country>IN</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Yavatkar</last-name><first-name>Raj</first-name><address><city>Los Gatos</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A system is configured to compute a latency between a first computing device and a second computing device. The system includes a network interface card (NIC) of a first computing device. The NIC includes a set of interfaces configured to receive one or more packets and send one or more packets. The processing unit is configured to identify information indicative of a forward packet, compute, based on a first time corresponding to the forward packet and a second time corresponding to a reverse packet associated with the forward packet, a latency between the first computing device and a second computing device, wherein the second computing device includes a destination of the forward packet and a source of the reverse packet, and output information indicative of the latency between the first computing device and the second computing device.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="101.77mm" wi="158.75mm" file="US20230006904A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="244.77mm" wi="174.84mm" file="US20230006904A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="243.33mm" wi="167.56mm" orientation="landscape" file="US20230006904A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="232.33mm" wi="159.51mm" orientation="landscape" file="US20230006904A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="234.95mm" wi="170.77mm" orientation="landscape" file="US20230006904A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="226.65mm" wi="171.79mm" file="US20230006904A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="242.15mm" wi="170.69mm" file="US20230006904A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="155.96mm" wi="150.11mm" file="US20230006904A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><p id="p-0002" num="0001">This application claims priority to Indian Provisional Patent Application No. 202141029411, filed on Jun. 30, 2021, the entire contents of which is incorporated herein by reference.</p><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The disclosure relates to computer networks.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0004" num="0003">In a typical cloud data center environment, there is a large collection of interconnected servers that provide computing and/or storage capacity to run various applications. For example, a data center may comprise a facility that hosts applications and services for subscribers, i.e., customers of data center. The data center may, for example, host all of the infrastructure equipment, such as networking and storage systems, redundant power supplies, and environmental controls. In a typical data center, clusters of storage servers and application servers (compute nodes) are interconnected via high-speed switch fabric provided by one or more tiers of physical network switches and routers. More sophisticated data centers provide infrastructure spread throughout the world with subscriber support equipment located in various physical hosting facilities.</p><p id="p-0005" num="0004">The connectivity between the server and the switch fabric occurs at a hardware module called the Network Interface Card (NIC). A conventional NIC includes an application-specific integrated circuit (ASIC) to perform packet forwarding, which includes some basic Layer 2/Layer 3 (L2/L3) functionality. In conventional NICs, the packet processing, policing and other advanced functionality, known as the &#x201c;datapath,&#x201d; is performed by the host central processing unit (CPU), i.e., the CPU of the server that includes the NIC. As a result, the CPU resources in the server are shared by applications running on that server and also by datapath processing. For example, in a 4 core x86 server, one of the cores may be reserved for the datapath, leaving 3 cores (or 75% of CPU) for applications and the host operating system.</p><p id="p-0006" num="0005">Some NIC vendors have begun including an additional processing unit in the NIC itself to offload at least some of the datapath processing from the host CPU to the NIC. The processing unit in the NIC may be, e.g., a multi-core ARM processor with some programmable hardware acceleration provided by a Data Processing Unit (DPU), Field Programmable Gate Array (FPGA), and/or an ASIC. NICs that include such augmented processing power are typically referred to as SmartNICs.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0007" num="0006">In general, techniques are described for an edge services platform that leverages processing units of SmartNICs to augment the processing and networking functionality of a network of servers that include the SmartNICs. Features provided by the edge services platform may include orchestration of NICs; application programming interface (API)-driven deployment of services on NICs; NIC addition, deletion and replacement; monitoring of services and other resources on NICs; and management of connectivity between various services running on the NICs.</p><p id="p-0008" num="0007">A network may include a plurality of servers, where packets travel throughout the network between one or more pairs of the plurality of servers. An amount of time that it takes for a packet to make a round trip between two servers of the plurality of servers defines a latency between the two servers. It may be beneficial to determine latencies between one or more pairs of servers, so that a controller can monitor a performance of the network. Existing techniques for latency measurement and monitoring include Pingmesh, which is a program for determining and displaying the latency between any two servers in a data center using a mesh visualization. Pingmesh creates such diagrams by causing each server to send periodic &#x201c;pings,&#x201d; i.e., Internet Control Message Protocol (ICMP) echo requests to every other server, which then respond immediately with an ICMP echo reply. Pingmesh can alternatively use Transmission Control Protocol (TCP) and HyperText Transfer Protocol (HTTP) pings. Pingmesh collects the latency information based on the ping request/reply round trip times and uses it to generate a mesh visualization.</p><p id="p-0009" num="0008">As described in further detail herein, a processing unit of a NIC may execute an agent that determines a latency between the device hosting the NIC and another device. In determining the latency, the processing unit of the NIC uses few or no computing resources of the host device, such as the host CPU and memory, relative to existing techniques such as Pingmesh that rely on host-generated ping request/replies. In some examples, the agent executing on the NIC processing unit may determine latencies by snooping data or control traffic generated and received the host device and passing through the data path on the NIC. For example, the agent may detect a forward packet for a forward packet flow and a reverse packet for a reverse packet flow, generate obtain timestamp information for the forward packet and the reverse packet, and compute a round trip time from the timestamp information to determine latency between the source device and destination device of the forward and reverse packet flows. The agent may perform a similar process with respect to Address Resolution Protocol (ARP) request/reply. In this way, the agent may determine latency without either the agent or the host having to separately generate send ping requests or responses to the other device for the purpose of latency measurement. The agent may instead snoop on existing data traffic exchanged between the devices. The agent may also perform a similar process with respect to ICMP echo request/reply message pairs. In addition, because the agent is executed on the NIC, the timestamps for at least some of the packets of packet flows, ARP request/reply packets, and ICMP echo request/reply packets are not affected by latencies within the host caused by the kernel network stack, DMA transfers or memory copying between the NIC memory and host memory, interrupts and polling, process context switching, and other non-deterministic timing that generally affects the latency of packet processing. As a result, the techniques may improve the accuracy and reliability of the timestamps and round-trip time computations.</p><p id="p-0010" num="0009">In some examples, a system includes: a network interface card (NIC) of a first computing device, wherein the NIC comprises: a set of interfaces configured to receive one or more packets and send one or more packets; and a processing unit configured to: identify information indicative of a forward packet; compute, based on a first time corresponding to the forward packet and a second time corresponding to a reverse packet associated with the forward packet, a latency between the first computing device and a second computing device, wherein the second computing device includes a destination of the forward packet and a source of the reverse packet; and output information indicative of the latency between the first computing device and the second computing device.</p><p id="p-0011" num="0010">In some examples, a method includes: identifying, by a processing unit of a network interface card (NIC) of a first computing device, information indicative of a forward packet, wherein the NIC includes a set of interfaces, and wherein the set of interfaces is configured to receive one or more packets and send one or more packets; computing, by the processing unit based on a first time corresponding to the forward packet and a second time corresponding to a reverse packet associated with the forward packet, a latency between the first computing device and a second computing device, wherein the second computing device includes a destination of the forward packet and a source of the reverse packet; and outputting, by the processing unit, information indicative of the latency between the first computing device and the second computing device.</p><p id="p-0012" num="0011">In some examples, a non-transitory computer-readable medium includes instructions for causing a processing unit of a network interface card (NIC) of a first computing device to: identify information indicative of a forward packet; compute, based on a first time corresponding to the forward packet and a second time corresponding to a reverse packet associated with the forward packet, a latency between the first computing device and a second computing device, wherein the second computing device includes a destination of the forward packet and a source of the reverse packet; and output information indicative of the latency between the first computing device and the second computing device.</p><p id="p-0013" num="0012">The details of one or more embodiments of this disclosure are set forth in the accompanying drawings and the description below. Other features, objects, and advantages will be apparent from the description and drawings, and from the claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating an example network system having a data center, in accordance with one or more techniques of this disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating an example computing device that uses a network interface card having a separate processing unit, to perform services managed by an edge services platform, in accordance with one or more techniques of this disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a conceptual diagram illustrating a data center with servers that each include a network interface card having a separate processing unit, controlled by an edge services platform, in accordance with one or more techniques of this disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram illustrating an example computing device that uses a network interface card having a separate processing unit, to perform services managed by an edge services platform, in accordance with one or more techniques of this disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram illustrating components of the example network system of <figref idref="DRAWINGS">FIG. <b>1</b></figref> including packet flows, in accordance with one or more techniques of this disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flow diagram illustrating a first example operation for determining a latency between two devices, in accordance with one or more techniques of this disclosure.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flow diagram illustrating a second example operation for determining a latency between two devices, in accordance with one or more techniques of this disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0021" num="0020">Like reference characters denote like elements throughout the description and figures.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating an example network system <b>8</b> having a data center <b>10</b>, in accordance with one or more techniques of this disclosure. In general, data center <b>10</b> provides an operating environment for applications and services for a customer sites <b>11</b> (illustrated as &#x201c;customers <b>11</b>&#x201d;) having one or more customer networks coupled to the data center by service provider network <b>7</b>. Data center <b>10</b> may, for example, host infrastructure equipment, such as networking and storage systems, redundant power supplies, and environmental controls. Service provider network <b>7</b> is coupled public network <b>4</b>, which may represent one or more networks administered by other providers, and may thus form part of a large-scale public network infrastructure, e.g., the Internet. Public network <b>4</b> may represent, for instance, a local area network (LAN), a wide area network (WAN), the Internet, a virtual LAN (VLAN), an enterprise LAN, a layer 3 virtual private network (VPN), an Internet Protocol (IP) intranet operated by the service provider that operates service provider network <b>7</b>, an enterprise IP network, or some combination thereof.</p><p id="p-0023" num="0022">Although customer sites <b>11</b> and public network <b>4</b> are illustrated and described primarily as edge networks of service provider network <b>7</b>, in some examples, one or more of customer sites <b>11</b> and public network <b>4</b> may be tenant networks within data center <b>10</b> or another data center. For example, data center <b>10</b> may host multiple tenants (customers) each associated with one or more virtual private networks (VPNs), each of which may implement one of customer sites <b>11</b>.</p><p id="p-0024" num="0023">Service provider network <b>7</b> offers packet-based connectivity to attached customer sites <b>11</b>, data center <b>10</b>, and public network <b>4</b>. Service provider network <b>7</b> may represent a network that is owned and operated by a service provider to interconnect a plurality of networks. Service provider network <b>7</b> may implement Multi-Protocol Label Switching (MPLS) forwarding and in such instances may be referred to as an MPLS network or MPLS backbone. In some instances, service provider network <b>7</b> represents a plurality of interconnected autonomous systems, such as the Internet, that offers services from one or more service providers.</p><p id="p-0025" num="0024">In some examples, data center <b>10</b> may represent one of many geographically distributed network data centers. As illustrated in the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, data center <b>10</b> may be a facility that provides network services for customers. A customer of the service provider may be a collective entity such as enterprises and governments or individuals. For example, a network data center may host web services for several enterprises and end users. Other exemplary services may include data storage, virtual private networks, traffic engineering, file service, data mining, scientific- or super-computing, and so on. Although illustrated as a separate edge network of service provider network <b>7</b>, elements of data center <b>10</b> such as one or more physical network functions (PNFs) or virtualized network functions (VNFs) may be included within the service provider network <b>7</b> core.</p><p id="p-0026" num="0025">In this example, data center <b>10</b> includes storage and/or compute servers interconnected via switch fabric <b>14</b> provided by one or more tiers of physical network switches and routers, with servers <b>12</b>A-<b>12</b>X (herein, &#x201c;servers <b>12</b>&#x201d;) depicted as coupled to top-of-rack switches <b>16</b>A-<b>16</b>N. Servers <b>12</b> may also be referred to herein as &#x201c;hosts&#x201d; or &#x201c;host devices.&#x201d; Although only servers coupled to TOR switch <b>16</b>A are shown in detail in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, data center <b>10</b> may include many additional servers coupled to other TOR switches <b>16</b> of the data center <b>10</b>.</p><p id="p-0027" num="0026">Switch fabric <b>14</b> in the illustrated example includes interconnected top-of-rack (TOR) (or other &#x201c;leaf&#x201d;) switches <b>16</b>A-<b>16</b>N (collectively, &#x201c;TOR switches <b>16</b>&#x201d;) coupled to a distribution layer of chassis (or &#x201c;spine&#x201d; or &#x201c;core&#x201d;) switches <b>18</b>A-<b>18</b>M (collectively, &#x201c;chassis switches <b>18</b>&#x201d;). Although not shown, data center <b>10</b> may also include, for example, one or more non-edge switches, routers, hubs, gateways, security devices such as firewalls, intrusion detection, and/or intrusion prevention devices, servers, computer terminals, laptops, printers, databases, wireless mobile devices such as cellular phones or personal digital assistants, wireless access points, bridges, cable modems, application accelerators, or other network devices.</p><p id="p-0028" num="0027">In this example, TOR switches <b>16</b> and chassis switches <b>18</b> provide servers <b>12</b> with redundant (multi-homed) connectivity to IP fabric <b>20</b> and service provider network <b>7</b>. Chassis switches <b>18</b> aggregate traffic flows and provides connectivity between TOR switches <b>16</b>. TOR switches <b>16</b> may be network devices that provide layer 2 (MAC) and/or layer 3 (e.g., IP) routing and/or switching functionality. TOR switches <b>16</b> and chassis switches <b>18</b> may each include one or more processors and a memory and can execute one or more software processes. Chassis switches <b>18</b> are coupled to IP fabric <b>20</b>, which may perform layer 3 routing to route network traffic between data center <b>10</b> and customer sites <b>11</b> by service provider network <b>7</b>. The switching architecture of data center <b>10</b> is merely an example. Other switching architectures may have more or fewer switching layers, for instance.</p><p id="p-0029" num="0028">The term &#x201c;packet flow,&#x201d; &#x201c;traffic flow,&#x201d; or simply &#x201c;flow&#x201d; refers to a set of packets originating from a particular source device or endpoint and sent to a particular destination device or endpoint. A single flow of packets may be identified by the 5-tuple: &#x3c;source network address, destination network address, source port, destination port, protocol&#x3e;, for example. This 5-tuple generally identifies a packet flow to which a received packet corresponds. An n-tuple refers to any n items drawn from the 5-tuple. For example, a 2-tuple for a packet may refer to the combination of &#x3c;source network address, destination network address&#x3e; or &#x3c;source network address, source port&#x3e; for the packet.</p><p id="p-0030" num="0029">Each of servers <b>12</b> may be a compute node, an application server, a storage server, or other type of server. For example, each of servers <b>12</b> may represent a computing device, such as an x86 processor-based server, configured to operate according to techniques described herein. Servers <b>12</b> may provide Network Function Virtualization Infrastructure (NFVI) for an NFV architecture.</p><p id="p-0031" num="0030">Servers <b>12</b> host endpoints <b>23</b> (illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> as &#x201c;EPs&#x201d; <b>23</b>) for one or more virtual networks that operate over the physical network represented here by IP fabric <b>20</b> and switch fabric <b>14</b>. Although described primarily with respect to a data center-based switching network, other physical networks, such as service provider network <b>7</b>, may underlay the one or more virtual networks.</p><p id="p-0032" num="0031">Servers <b>12</b> each includes at least one network interface card (NIC) of NICs <b>13</b>A-<b>13</b>X (collectively, &#x201c;NICs <b>13</b>&#x201d;), which each include at least one port with which to exchange packets send and receive packets over a communication link. For example, server <b>12</b>A includes NIC <b>13</b>A.</p><p id="p-0033" num="0032">In some examples, each of NICs <b>13</b> provides one or more virtual hardware components <b>21</b> for virtualized input/output (I/O). A virtual hardware component for I/O maybe a virtualization of a physical NIC <b>13</b> (the &#x201c;physical function&#x201d;). For example, in Single Root I/O Virtualization (SR-IOV), which is described in the Peripheral Component Interface Special Interest Group SR-IOV specification, the PCIe Physical Function of the network interface card (or &#x201c;network adapter&#x201d;) is virtualized to present one or more virtual network interface cards as &#x201c;virtual functions&#x201d; for use by respective endpoints executing on the server <b>12</b>. In this way, the virtual network endpoints may share the same PCIe physical hardware resources and the virtual functions are examples of virtual hardware components <b>21</b>. As another example, one or more servers <b>12</b> may implement Virtio, a para-virtualization framework available, e.g., for the Linux Operating System, that provides emulated NIC functionality as a type of virtual hardware component. As another example, one or more servers <b>12</b> may implement Open vSwitch to perform distributed virtual multilayer switching between one or more virtual NICs (vNICs) for hosted virtual machines, where such vNICs may also represent a type of virtual hardware component. In some instances, the virtual hardware components are virtual I/O (e.g., NIC) components. In some instances, the virtual hardware components are SR-IOV virtual functions and may provide SR-IOV with Data Plane Development Kit (DPDK)-based direct process user space access.</p><p id="p-0034" num="0033">In some examples, including the illustrated example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, one or more of NICs <b>13</b> may include multiple ports. NICs <b>13</b> may be connected to one another via ports of NICs <b>13</b> and communications links to form a NIC fabric <b>23</b> having a NIC fabric topology. NIC fabric <b>23</b> is the collection of NICs <b>13</b> connected to at least one other NIC <b>13</b>.</p><p id="p-0035" num="0034">NICs <b>13</b> each includes a processing unit to offload aspects of the datapath. The processing unit in the NIC may be, e.g., a multi-core ARM processor with hardware acceleration provided by a Data Processing Unit (DPU), Field Programmable Gate Array (FPGA), and/or an ASIC. NICs <b>13</b> may alternatively be referred to as SmartNICs or GeniusNICs.</p><p id="p-0036" num="0035">In accordance with various aspects of the techniques described in this disclosure, an edge services platform leverages processing units <b>25</b> of NICs <b>13</b> to augment the processing and networking functionality of switch fabric <b>14</b> and/or servers <b>12</b> that include NICs <b>13</b>.</p><p id="p-0037" num="0036">Edge services controller <b>28</b> manages the operations of the edge services platform within NIC <b>13</b><i>s </i>by orchestrating services to be performed by processing units <b>25</b>; API driven deployment of services on NICs <b>13</b>; NIC <b>13</b> addition, deletion and replacement within the edge services platform; monitoring of services and other resources on NICs <b>13</b>; and management of connectivity between various services running on the NICs <b>13</b>.</p><p id="p-0038" num="0037">Edge services controller <b>28</b> may communicate information describing services available on NICs <b>13</b>, a topology of NIC fabric <b>13</b>, or other information about the edge services platform to an orchestration system (not shown) or network controller <b>24</b>. Example orchestration systems include OpenStack, vCenter by VMWARE, or System Center by MICROSOFT. Example network controllers <b>24</b> include a controller for Contrail by JUNIPER NETWORKS or Tungsten Fabric. Additional information regarding a controller <b>24</b> operating in conjunction with other devices of data center <b>10</b> or other software-defined network is found in International Application Number PCT/US2013/044378, filed Jun. 5, 2013, and entitled &#x201c;PHYSICAL PATH DETERMINATION FOR VIRTUAL NETWORK PACKET FLOWS;&#x201d; and in U.S. patent application Ser. No. 14/226,509, filed Mar. 26, 2014, and entitled &#x201c;Tunneled Packet Aggregation for Virtual Networks,&#x201d; each which is incorporated by reference as if fully set forth herein.</p><p id="p-0039" num="0038">In some examples, an NIC of a first computing device (e.g., NIC <b>13</b>A of server <b>12</b>A), wherein the NIC includes a set of interfaces configured to receive one or more packets and send one or more packets. A forward packet may represent a packet sent from server <b>12</b>A to another computing device, and a reverse packet may represent received by server <b>12</b>A from the other computing device in response to the forward packet. Consequently, NIC <b>13</b>A may both send and receive packets, and NIC <b>13</b>A may process packets to determine whether a packet represents a forward packet or a reverse packet. In some examples, NIC <b>13</b>A includes a processing unit <b>25</b>A configured to identify information indicative of a forward packet received by the set of interfaces. Processing unit <b>25</b>A may compute, based on a first time corresponding to the forward packet and a second time corresponding to a reverse packet associated with the forward packet, a latency between server <b>12</b>A and a another computing device (e.g., server <b>12</b>X), where server <b>12</b>X includes a destination of the forward packet and a source of the reverse packet. The latency between server <b>12</b>A and server <b>12</b>X may represent an amount of time that it takes for a packet to travel from server <b>12</b>A to <b>12</b>X or travel from server <b>12</b>X to <b>12</b>A. That is, the latency between server <b>12</b>A and server <b>12</b>X may represent one half of an amount of time that it takes for a packet to make a round trip between server <b>12</b>A and server <b>12</b>X. In some examples, processing unit <b>25</b>A may output information indicative of the latency between the first computing device and the second computing device.</p><p id="p-0040" num="0039">It may be beneficial for processing units <b>25</b> of NICs <b>13</b> to analyze one or more forward packets and one or more reverse packets to determine latencies between servers in data center <b>8</b>. The one or more packets processed by That is, by analyzing packets passing through NICs <b>13</b> to determine latency values, processing units <b>25</b> may determine latency values while consuming a smaller amount of network resources as compared with systems that do not use computing resources of NICs to determine latency values. Furthermore, processing units <b>25</b> efficiently use the resources of NICs <b>13</b> to determine one or more latencies based on packets that exist for one or more purposes other than determining latencies. In other words, NICs <b>13</b> may not send ping packets solely for the purpose of determining latency. NICs <b>13</b> analyze packets that have other purposes, thus decreasing an amount of consumed network resources as compared with systems that determine latencies by sending ping packets.</p><p id="p-0041" num="0040">In some examples, edge services controller <b>28</b> is configured to receive the information indicative of the latency between server <b>12</b>A and server <b>12</b>X from NIC <b>13</b>A. Edge services controller <b>28</b> may update a latency table to include the latency between server <b>12</b>A and the server <b>12</b>X. The latency table maintained by edge services controller <b>28</b> may indicate a plurality of latencies, each latency of the plurality of latencies corresponding to a respective pair of servers of servers <b>12</b>. For example, the latency table may include a latency between server <b>12</b>A and server <b>12</b>B, a latency between server <b>12</b>A and server <b>12</b>C, latency between server <b>12</b>B and server <b>12</b>C, and so on. Each time that edge services controller <b>28</b> receives a latency value from one of NICs <b>13</b>A, edge services controller <b>28</b> may maintain the latency table to indicate the received latency value. In some examples, edge services controller <b>28</b> may generate a Pingmesh diagram based on the latency table. Edge services controller <b>20</b> may output the Pingmesh diagram to a user interface so that an administrator can view a health of the network.</p><p id="p-0042" num="0041">In addition to determining a latency between server <b>12</b>A and server <b>12</b>X, processing unit <b>25</b>A may determine a latency between server <b>12</b>A and one or more other servers of servers <b>12</b>. For example, processing unit <b>12</b>A may identify information indicative of a forward packet received NIC <b>13</b>A. NIC <b>13</b>A may determine that a source device of the forward packet is server <b>12</b>A and a destination device of the forward packet is server <b>12</b>C. Processing unit <b>25</b>A may be configured to compute a latency between the server <b>12</b>A and the server <b>12</b>C. Server <b>12</b>C includes a destination of the forward packet and a source of a reverse packet corresponding to the forward packet. Processing unit <b>25</b>A may output information indicative of the latency between the server <b>12</b>A and the server <b>12</b>C. A processing unit of an NIC may determine the latency between a host server of the NIC and one or more other servers of the data center <b>10</b>. For example, processing unit <b>25</b>A of NIC <b>13</b>A may determine a latency between server <b>12</b>A and server <b>12</b>B, a latency between server <b>12</b>A and server <b>12</b>C, a latency between server <b>12</b>A and server <b>12</b>D, a latency between server <b>12</b>A and server <b>12</b>X, and a latency between server <b>12</b>A and one or more other computing devices configured to receive forward packets and output reverse packets. Additionally, or alternatively, processing unit <b>25</b>B of NIC <b>13</b>B may determine a latency between server <b>12</b>B and server <b>12</b>A, a latency between server <b>12</b>B and server <b>12</b>C, a latency between server <b>12</b>B and server <b>12</b>D, a latency between server <b>12</b>B and server <b>12</b>X, and a latency between server <b>12</b>B and one or more other computing devices configured to receive forward packets and output reverse packets. Processing units <b>25</b>C-<b>25</b>X may determine latencies between their respective host servers and other servers within the data center <b>10</b> or outside of data center <b>10</b>.</p><p id="p-0043" num="0042">In some examples, NIC <b>13</b>A may receive a packet. The source device of the forward packet may be server <b>12</b>A and a destination device of the forward packet may be server <b>12</b>X. Processing unit <b>25</b>A may be configured to identify a source internet protocol (IP) address and a destination IP address in a header of the packet. Based on the source IP address and the destination IP address, processing unit <b>25</b>A may determine that the packet is a forward packet originating at server <b>12</b>A and bound for server <b>12</b>X. Consequently, processing unit <b>25</b>A may be configured to determine the latency between server <b>12</b>A and server <b>12</b>X when NIC <b>13</b>A sends the forward packet to server <b>12</b>X, and when NIC <b>13</b>A receives a reverse packet from server <b>12</b>X in response to server <b>12</b>X receiving the forward packet.</p><p id="p-0044" num="0043">In some examples, processing unit <b>25</b>A may be configured to determine the latency between server <b>12</b>A and server <b>12</b>X only when server <b>12</b>X immediately sends a reverse packet in response to receiving the forward packet from server <b>12</b>A. When server <b>12</b>X does not immediately send a reverse packet, processing unit <b>25</b>A might not be configured to determine the latency between server <b>12</b>A and server <b>12</b>X based on the time that the reverse packet arrives at the server <b>12</b>A, since the reverse packet is delayed. Some types of packets are configured to elicit an immediate reverse packet from a destination device. For example, transmission control protocol (TCP) packets having any one or more of the synchronize (SYN) TCP packet flag, the urgent (URG) TCP packet flag, and the push (PSH) TCP packet flag may elicit immediate reverse packets. Additionally, or alternatively, packets sent according to one or both of the internet control message protocol (ICMP) and the address resolution protocol (ARP) may elicit immediate reverse packets. In any case, it may be beneficial for processing unit <b>25</b>A to identify a type of a forward packet and determine whether the type represents a packet type which elicits an immediate reverse packet.</p><p id="p-0045" num="0044">Processing unit <b>25</b>A may create a flow structure in response to identifying a forward packet having a packet type that elicits an immediate response packet. The flow structure may indicate information indicative of a reverse packet that processing unit <b>25</b>A expects to receive in response to outputting the forward packet. For example, when processing unit <b>25</b>A identifies a source IP address and a destination IP address corresponding to the forward packet, processing unit <b>25</b>A may create the flow structure to indicate an expected source IP address of the reverse packet and an expected destination IP address of the reverse packet. The expected source IP address of the reverse packet may represent the destination IP address of the forward packet, and the expected destination IP address of the reverse packet may represent the source IP address of the forward packet. Additionally, or alternatively, processing unit <b>25</b>A may create a timestamp corresponding to a time in which the forward packet departs server <b>12</b>A for the destination device (e.g., server <b>12</b>X).</p><p id="p-0046" num="0045">NIC <b>13</b>A may output the forward packet to server <b>12</b>X. In response to outputting the forward packet to server <b>12</b>X, NIC <b>13</b>A may receive a reverse packet. Processing unit <b>25</b>A is configured to identify information indicative of the reverse packet received by NIC <b>13</b>A. Processing unit <b>25</b>A may determine, based on the flow structure generated by processing unit <b>25</b>A in response to receiving the forward packet, that the reverse packet represents a packet received by NIC <b>13</b>A in response to outputting the forward packet. For example, processing unit <b>25</b>A may identify a source IP address of the reverse packet and a destination IP address of the reverse packet. When the source IP address of the reverse packet and a destination IP address of the reverse packet match the expected source IP address and the expected destination IP address in the flow structure, processing unit <b>25</b>A may determine that the reverse packet represents a packet received by NIC <b>13</b>A in response to outputting the forward packet.</p><p id="p-0047" num="0046">Processing unit <b>25</b>A may determine a latency between server <b>12</b>A and server <b>12</b>X based on a packet round trip time between server <b>12</b>A and server <b>12</b>X. For example, processing unit <b>25</b>A may identify a time corresponding to an arrival of the reverse packet at the NIC <b>13</b>A. Since the timestamp generated by processing unit <b>25</b>A corresponds to a time at which the forward packet departed from server <b>12</b>A, processing unit <b>25</b>A may compute the packet round trip time between server <b>12</b>A and server <b>12</b>X by subtracting the timestamp from the time corresponding to an arrival of the reverse packet at the NIC <b>13</b>A. In some examples, the latency may represent half of the packet round trip time. Processing unit <b>25</b>A may output information indicative of the latency between server <b>12</b>A and server <b>12</b>X to edge services controller <b>28</b>.</p><p id="p-0048" num="0047">In some examples, edge services controller <b>28</b> configures processing unit <b>25</b>A, for instance, to begin measuring latencies between server <b>12</b>A and other servers <b>12</b> or between server <b>12</b>A and a particular other server <b>12</b>. Edge services controller <b>28</b> may configure processing unit <b>25</b>A to stop measuring latencies, or parameterize an algorithm executed by processing unit <b>25</b>A to compute latencies.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating an example computing device that uses a network interface card having a separate processing unit, to perform services managed by an edge services platform, in accordance with one or more techniques of this disclosure. Computing device <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> may represent a real or virtual server and may represent an example instance of any of servers <b>12</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Computing device <b>200</b> includes in this example, a bus <b>242</b> coupling hardware components of a computing device <b>200</b> hardware environment. Bus <b>242</b> couples SR-IOV-capable network interface card (NIC) <b>230</b>, storage disk <b>246</b>, and microprocessor <b>210</b>. A front-side bus may in some cases couple microprocessor <b>210</b> and memory device <b>244</b>. In some examples, bus <b>242</b> may couple memory device <b>244</b>, microprocessor <b>210</b>, and NIC <b>230</b>. Bus <b>242</b> may represent a Peripheral Component Interface (PCI) express (PCIe) bus. In some examples, a direct memory access (DMA) controller may control DMA transfers among components coupled to bus <b>242</b>. In some examples, components coupled to bus <b>242</b> control DMA transfers among components coupled to bus <b>242</b>.</p><p id="p-0050" num="0049">Microprocessor <b>210</b> may include one or more processors each including an independent execution unit (&#x201c;processing core&#x201d;) to perform instructions that conform to an instruction set architecture. Execution units may be implemented as separate integrated circuits (ICs) or may be combined within one or more multi-core processors (or &#x201c;many-core&#x201d; processors) that are each implemented using a single IC (i.e., a chip multiprocessor).</p><p id="p-0051" num="0050">Disk <b>246</b> represents computer readable storage media that includes volatile and/or non-volatile, removable and/or non-removable media implemented in any method or technology for storage of information such as processor-readable instructions, data structures, program modules, or other data. Computer readable storage media includes, but is not limited to, random access memory (RAM), read-only memory (ROM), EEPROM, flash memory, CD-ROM, digital versatile discs (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to store the desired information and that can be accessed by microprocessor <b>210</b>.</p><p id="p-0052" num="0051">Main memory <b>244</b> includes one or more computer-readable storage media, which may include random-access memory (RAM) such as various forms of dynamic RAM (DRAM), e.g., DDR2/DDR3 SDRAM, or static RAM (SRAM), flash memory, or any other form of fixed or removable storage medium that can be used to carry or store desired program code and program data in the form of instructions or data structures and that can be accessed by a computer. Main memory <b>144</b> provides a physical address space composed of addressable memory locations.</p><p id="p-0053" num="0052">Network interface card (NIC) <b>230</b> includes one or more interfaces <b>232</b> configured to exchange packets using links of an underlying physical network. Interfaces <b>232</b> may include a port interface card having one or more network ports. NIC <b>230</b> also include an on-card memory <b>227</b> to, e.g., store packet data. Direct memory access transfers between the NIC <b>230</b> and other devices coupled to bus <b>242</b> may read/write from/to the memory <b>227</b>.</p><p id="p-0054" num="0053">Memory <b>244</b>, NIC <b>230</b>, storage disk <b>246</b>, and microprocessor <b>210</b> provide an operating environment for a software stack that executes a hypervisor <b>214</b> and one or more virtual machines <b>228</b> managed by hypervisor <b>214</b>.</p><p id="p-0055" num="0054">In general, a virtual machine provides a virtualized/guest operating system for executing applications in an isolated virtual environment. Because a virtual machine is virtualized from physical hardware of the host server, executing applications are isolated from both the hardware of the host and other virtual machines.</p><p id="p-0056" num="0055">An alternative to virtual machines is the virtualized container, such as those provided by the open-source DOCKER Container application. Like a virtual machine, each container is virtualized and may remain isolated from the host machine and other containers. However, unlike a virtual machine, each container may omit an individual operating system and provide only an application suite and application-specific libraries. A container is executed by the host machine as an isolated user-space instance and may share an operating system and common libraries with other containers executing on the host machine. Thus, containers may require less processing power, storage, and network resources than virtual machines. As used herein, containers may also be referred to as virtualization engines, virtual private servers, silos, or jails. In some instances, the techniques described herein with respect to containers and virtual machines or other virtualization components.</p><p id="p-0057" num="0056">While virtual network endpoints in <figref idref="DRAWINGS">FIG. <b>2</b></figref> are illustrated and described with respect to virtual machines, other operating environments, such as one or more containers (e.g., a DOCKER container) may implement virtual network endpoints. Containers may be deployed using Kubernetes pods, for example. An operating system kernel (not shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>) may execute in kernel space and may include, for example, a Linux, Berkeley Software Distribution (BSD), another Unix-variant kernel, or a Windows server operating system kernel, available from MICROSOFT.</p><p id="p-0058" num="0057">Computing device <b>200</b> executes a hypervisor <b>214</b> to manage virtual machines <b>228</b>. Example hypervisors include Kernel-based Virtual Machine (KVM) for the Linux kernel, Xen, ESXi available from VMWARE, Windows Hyper-V available from MICROSOFT, and other open-source and proprietary hypervisors. Hypervisor <b>214</b> may represent a virtual machine manager (VMM).</p><p id="p-0059" num="0058">Virtual machines <b>228</b> may host one or more applications, such as virtual network function instances. In some examples, a virtual machine <b>228</b> may host one or more VNF instances, where each of the VNF instances is configured to apply a network function to packets.</p><p id="p-0060" num="0059">Hypervisor <b>214</b> includes a physical driver <b>225</b> to use the physical function <b>221</b> provided by network interface card <b>230</b>. In some cases, network interface card <b>230</b> may also implement SR-IOV to enable sharing the physical network function (I/O) among virtual machines <b>224</b>. Each port of NIC <b>230</b> may be associated with a different physical function. The shared virtual devices, also known as virtual functions, provide dedicated resources such that each of virtual machines <b>228</b> (and corresponding guest operating systems) may access dedicated resources of NIC <b>230</b>, which therefore appears to each of virtual machines <b>224</b> as a dedicated NIC. Virtual functions <b>217</b> may represent lightweight PCIe functions that share physical resources with the physical function <b>221</b> and with other virtual functions <b>216</b>. NIC <b>230</b> may have thousands of available virtual functions according to the SR-IOV standard, but for I/O-intensive applications the number of configured virtual functions is typically much smaller.</p><p id="p-0061" num="0060">Virtual machines <b>228</b> include respective virtual NICs <b>229</b> presented directly into the virtual machine <b>228</b> guest operating system, thereby offering direct communication between NIC <b>230</b> and the virtual machine <b>228</b> via bus <b>242</b>, using the virtual function assigned for the virtual machine. This may reduce hypervisor <b>214</b> overhead involved with software-based, VIRTIO and/or vSwitch implementations in which hypervisor <b>214</b> memory address space of memory <b>244</b> stores packet data and packet data copying from the NIC <b>230</b> to the hypervisor <b>214</b> memory address space and from the hypervisor <b>214</b> memory address space to the virtual machines <b>228</b> memory address space consumes cycles of microprocessor <b>210</b>.</p><p id="p-0062" num="0061">NIC <b>230</b> may further include a hardware-based Ethernet bridge <b>234</b> to perform layer 2 forwarding between virtual functions and physical functions of NIC <b>230</b>. Bridge <b>234</b> thus provides hardware acceleration, via bus <b>242</b>, of inter-virtual machine <b>224</b> packet forwarding and of packet forwarding between hypervisor <b>214</b>, which accesses the physical function via physical driver <b>225</b>, and any of virtual machines <b>224</b>.</p><p id="p-0063" num="0062">Computing device <b>200</b> may be coupled to a physical network switch fabric that includes an overlay network that extends switch fabric from physical switches to software or &#x201c;virtual&#x201d; routers of physical servers coupled to the switch fabric, including virtual router <b>220</b>. Virtual routers may be processes or threads, or a component thereof, executed by the physical servers, e.g., servers <b>12</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, that dynamically create and manage one or more virtual networks usable for communication between virtual network endpoints. In one example, virtual routers implement each virtual network using an overlay network, which provides the capability to decouple an endpoint's virtual address from a physical address (e.g., IP address) of the server on which the endpoint is executing. Each virtual network may use its own addressing and security scheme and may be viewed as orthogonal from the physical network and its addressing scheme. Various techniques may be used to transport packets within and across virtual networks over the physical network. At least some functions of virtual router may be performed as one of services <b>233</b>.</p><p id="p-0064" num="0063">In the example computing device <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, virtual router <b>220</b> executes within hypervisor <b>214</b> that uses physical function <b>221</b> for I/O, but virtual router <b>220</b> may execute within a hypervisor, a host operating system, a host application, one of virtual machines <b>228</b>, and/or processing unit <b>25</b> of NIC <b>230</b>.</p><p id="p-0065" num="0064">In general, each virtual machine <b>228</b> may be assigned a virtual address for use within a corresponding virtual network, where each of the virtual networks may be associated with a different virtual subnet provided by virtual router <b>220</b>. A virtual machine <b>228</b> may be assigned its own virtual layer three (L3) IP address, for example, for sending and receiving communications but may be unaware of an IP address of the computing device <b>200</b> on which the virtual machine is executing. In this way, a &#x201c;virtual address&#x201d; is an address for an application that differs from the logical address for the underlying, physical computer system, e.g., computing device <b>200</b>.</p><p id="p-0066" num="0065">In one implementation, computing device <b>200</b> includes a virtual network (VN) agent (not shown) that controls the overlay of virtual networks for computing device <b>200</b> and that coordinates the routing of data packets within computing device <b>200</b>. In general, a VN agent communicates with a virtual network controller for the multiple virtual networks, which generates commands to control routing of packets. A VN agent may operate as a proxy for control plane messages between virtual machines <b>228</b> and virtual network controller, such as controller <b>24</b>. For example, a virtual machine may request to send a message using its virtual address via the VN agent, and VN agent may in turn send the message and request that a response to the message be received for the virtual address of the virtual machine that originated the first message. In some cases, a virtual machine <b>228</b> may invoke a procedure or function call presented by an application programming interface of VN agent, and the VN agent may handle encapsulation of the message as well, including addressing.</p><p id="p-0067" num="0066">In one example, network packets, e.g., layer three (L3) IP packets or layer two (L2) Ethernet packets generated or consumed by the instances of applications executed by virtual machine <b>228</b> within the virtual network domain may be encapsulated in another packet (e.g., another IP or Ethernet packet) that is transported by the physical network. The packet transported in a virtual network may be referred to herein as an &#x201c;inner packet&#x201d; while the physical network packet may be referred to herein as an &#x201c;outer packet&#x201d; or a &#x201c;tunnel packet.&#x201d; Encapsulation and/or de-capsulation of virtual network packets within physical network packets may be performed by virtual router <b>220</b>. This functionality is referred to herein as tunneling and may be used to create one or more overlay networks. Besides IPinIP, other example tunneling protocols that may be used include IP over Generic Route Encapsulation (GRE), VxLAN, Multiprotocol Label Switching (MPLS) over GRE, MPLS over User Datagram Protocol (UDP), etc.</p><p id="p-0068" num="0067">As noted above, a virtual network controller may provide a logically centralized controller for facilitating operation of one or more virtual networks. The virtual network controller may, for example, maintain a routing information base, e.g., one or more routing tables that store routing information for the physical network as well as one or more overlay networks. Virtual router <b>220</b> of hypervisor <b>214</b> implements a network forwarding table (NFT) <b>222</b>A-<b>222</b>N for N virtual networks for which virtual router <b>220</b> operates as a tunnel endpoint. In general, each NFT <b>222</b> stores forwarding information for the corresponding virtual network and identifies where data packets are to be forwarded and whether the packets are to be encapsulated in a tunneling protocol, such as with a tunnel header that may include one or more headers for different layers of the virtual network protocol stack. Each of NFTs <b>222</b> may be an NFT for a different routing instance (not shown) implemented by virtual router <b>220</b>.</p><p id="p-0069" num="0068">In accordance with techniques described in this disclosure, an edge services platform leverages processing unit <b>25</b> of NIC <b>230</b> to augment the processing and networking functionality of computing device <b>200</b>. Processing unit <b>25</b> includes processing circuitry <b>231</b> to execute services orchestrated by edge services controller <b>28</b>. Processing circuitry <b>231</b> may represent any combination of processing cores, ASICs, FPGAs, or other integrated circuits and programmable hardware. In an example, processing circuitry may include a System-on-Chip (SoC) having, e.g., one more cores, a network interface for high-speed packet processing, one or more acceleration engines for specialized functions (e.g., security/cryptography, machine learning, storage), programmable logic, integrated circuits, and so forth. Such SoCs may be referred to as data processing units (DPUs).</p><p id="p-0070" num="0069">In the example NIC <b>230</b>, processing unit <b>25</b> executes an operating system kernel <b>237</b> and a user space <b>241</b> for services. Kernel may be a Linux kernel, a Unix or BSD kernel, a real-time OS kernel, or other kernel for managing hardware resources of processing unit <b>25</b> and managing user space <b>241</b>.</p><p id="p-0071" num="0070">Services <b>233</b> may include network, security, storage, data processing, co-processing, machine learning or other services. Processing unit <b>25</b> may execute services <b>233</b> and edge service platform (ESP) agent <b>236</b> as processes and/or within virtual execution elements such as containers or virtual machines. As described elsewhere herein, services <b>233</b> may augment the processing power of the host processors (e.g., microprocessor <b>210</b>) by, e.g., enabling the computing device <b>200</b> to offload packet processing, security, or other operations that would otherwise be executed by the host processors.</p><p id="p-0072" num="0071">Processing unit <b>25</b> executes edge service platform (ESP) agent <b>236</b> to exchange data and control data with an edge services controller for the edge service platform. While shown in user space <b>241</b>, ESP agent <b>236</b> may be a kernel module <b>237</b> in some instances.</p><p id="p-0073" num="0072">As an example, ESP agent <b>236</b> may collect and send, to the ESP controller, telemetry data generated by services <b>233</b>, the telemetry data describing traffic in the network, computing device <b>200</b> or network resource availability, resource availability of resources of processing unit <b>25</b> (such as memory or core utilization). As another example, ESP agent <b>236</b> may receive, from the ESP controller, service code to execute any of services <b>233</b>, service configuration to configure any of services <b>233</b>, packets or other data for injection into the network.</p><p id="p-0074" num="0073">Edge services controller <b>28</b> manages the operations of processing unit <b>25</b> by, e.g., orchestrating and configuration services <b>233</b> that are executed by processing unit <b>25</b>; deploying services <b>233</b>; NIC <b>230</b> addition, deletion and replacement within the edge services platform; monitoring of services <b>233</b> and other resources on NIC <b>230</b>; and management of connectivity between various services <b>233</b> running on NIC <b>230</b>. Example resources on NIC <b>230</b> include memory <b>227</b> and processing circuitry <b>231</b>.</p><p id="p-0075" num="0074">Processing unit <b>25</b> may execute a latency agent <b>238</b> to determine the latency between computing device <b>200</b> and one or more other computing devices. In one example, computing device <b>200</b> may represent server <b>12</b>A, and processing unit <b>25</b> may execute latency agent <b>238</b> to determine a latency between server <b>12</b>A and server <b>12</b>X, for instance, based on receiving a forward packet indicating server <b>12</b>A as a source device and indicating server <b>12</b>X as a destination device. In some examples, latency agent <b>238</b> may be a part of ESP agent <b>236</b>. For example, NIC <b>230</b> may receive, from one or more components of computing device <b>200</b> (e.g., microprocessor <b>210</b>), a forward packet. The packets may be generated by applications running on computing device <b>200</b> and executed by microprocessor <b>210</b>, for example.</p><p id="p-0076" num="0075">The packet may travel through ethernet bridge <b>234</b> to interfaces <b>232</b>. Interfaces <b>232</b> may be configured to receive one or more packets and send one or more packets. Consequently, NIC <b>230</b> is configured to receive one or more packets from components of computing device <b>200</b> via bus <b>242</b> and receive one or more packets from other computing devices via interfaces <b>232</b>. Additionally, or alternatively, NIC <b>230</b> is configured to send one or more packets to components of computing device <b>200</b> via bus <b>242</b> and send one or more packets to other computing devices via interfaces <b>232</b>.</p><p id="p-0077" num="0076">When NIC <b>230</b> receives a packet via interfaces <b>232</b> or bus <b>242</b>, processing unit <b>25</b> may sniff the packet off the wire; or ethernet bridge <b>234</b> may be configured with filters that match packets useful for determining a latency between computing device <b>200</b> and another computing device and switch such packets to processing unit <b>25</b> for further processing; or processing unit <b>25</b> may (in some cases) include ethernet bridge <b>234</b> and apply processing packets useful for determining a latency, as described below and elsewhere in this disclosure.</p><p id="p-0078" num="0077">Processing unit <b>25</b> may execute latency agent <b>238</b> to identify information corresponding to the packet and analyze the information. For example, latency agent <b>238</b> may determine whether the packet is useful for determining a latency between computing device <b>200</b> and another computing device. In some examples, when processing unit <b>25</b> receives information corresponding to a packet arriving at NIC <b>230</b>, processing unit <b>25</b> may execute latency agent <b>238</b> to run an algorithm to determine a latency value. The following example computer code may represent an algorithm for processing packet information to determine a latency value:</p><p id="p-0079" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="245pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>foreach packet P</entry></row><row><entry/><entry>&#x2003;if protocolType(P) == TCP</entry></row><row><entry/><entry>&#x2003;&#x2003;forwardFlow = getFlow(P&#x2212;&#x3e;sip, P&#x2212;&#x3e;dip, P&#x2212;&#x3e;sport, P&#x2212;&#x3e;dport)</entry></row><row><entry/><entry>&#x2003;&#x2003;if valid(forwardFlow)</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;rtt = getTime( ) &#x2212; forwardFlow&#x2212;&#x3e;timeStamp</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;updateLatency(P&#x2212;&#x3e;sip, P&#x2212;&#x3e;dip)</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;delete forwardFlow</entry></row><row><entry/><entry>&#x2003;&#x2003;elseif</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;if isSet(P&#x2212;&#x3e;flags, URG || SYN || PSH)</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;reverseFlow = createFlow(P&#x2212;&#x3e;dip, P&#x2212;&#x3e;sip, P&#x2212;&#x3e;dport, P&#x2212;&#x3e;sport)</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;reverseFlow&#x2212;&#x3e;timeStamp = getTime( )</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;endif</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;endif</entry></row><row><entry/><entry>&#x2003;elseif protocolType(P) == ICMP</entry></row><row><entry/><entry>&#x2003;&#x2003;if isRequest(P)</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;Store sip, dip, current time</entry></row><row><entry/><entry>&#x2003;&#x2003;else</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;Update latency for sip, dip</entry></row><row><entry/><entry>&#x2003;&#x2003;endif</entry></row><row><entry/><entry>&#x2003;elseif protocolType(P) == ARP</entry></row><row><entry/><entry>&#x2003;&#x2003;if isRequest(P)</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;Store smac, current time</entry></row><row><entry/><entry>&#x2003;&#x2003;else</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;Update latency for smac, dmac</entry></row><row><entry/><entry>&#x2003;&#x2003;endif</entry></row><row><entry/><entry>&#x2003;endif</entry></row><row><entry/><entry>&#x2003;&#x2003;endfor</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0080" num="0078">As seen in the example computer code, the latency agent <b>238</b> may determine whether a packet arriving at NIC <b>230</b> is according to TCP, ICMP, or ARP (e.g., the &#x201c;if protocolType(P)==TCP,&#x201d; &#x201c;elseif protocolType(P) ICMP,&#x201d; and &#x201c;elseif protocolType(P) ARP&#x201d; lines in the example computer code). The latency agent <b>238</b> may determine whether the packet is according to the TCP, ICMP, or ARP, because some packets sent according to TCP, ICMP, and ARP elicit immediate reverse packets from the destination device. For example, when NIC <b>230</b> sends a forward packet according to ICMP to a destination device, the destination device may immediately send a reverse packet back to NIC <b>230</b> when the destination device receives the forward packet. Consequently, packets sent according to TCP, ICMP, and ARP may be useful for determining a latency between two computing devices, because a source device may determine a round trip time based on a time at which the forward packet is sent from the source and a time at which the reverse packet returns to the source. When latency agent <b>238</b> determines that a packet is not according to TCP, ICMP, or ARP, the algorithm may conclude and latency agent <b>238</b> may apply the algorithm to a next packet arriving at NIC <b>230</b>.</p><p id="p-0081" num="0079">When latency agent <b>238</b> determines that a packet arriving at NIC <b>230</b> is sent according to TCP, latency agent <b>238</b> may proceed to determine whether the packet represents a forward packet. For example, latency agent <b>238</b> may determine a source IP address and a destination IP address corresponding to the packet. When the source IP address corresponds to computing device <b>200</b> and the destination device corresponds to another computing device, the packet represents a forward packet.</p><p id="p-0082" num="0080">When latency agent <b>238</b> determines that a packet arriving at NIC <b>230</b> is a forward packet sent according to TCP, then latency agent <b>238</b> may determine whether the packet includes at least one of a set of packet flags (e.g., the &#x201c;if isSet(P-&#x3e;flags, URG&#x2225;SYN&#x2225;PSH)&#x201d; line in the example computer code). As seen in the example computer code, the set of TCP packet flags may include the synchronize (SYN) TCP packet flag, the urgent (URG) TCP packet flag, and the push (PSH) TCP packet flag. The SYN packet flag, the URG packet flag, and the PSH packet flag may indicate a TCP packet that elicits an immediate reverse packet from the destination device. One or more TCP packets that do not include the SYN packet flag, the URG packet flag, or the PSH packet flag might not elicit an immediate reverse packet from the destination device. Consequently, it may be beneficial for latency agent <b>238</b> to determine latency based on TCP packets that include any one or more of the SYN packet flag, the URG packet flag, or the PSH packet flag.</p><p id="p-0083" num="0081">Processing unit <b>25</b> may execute latency agent <b>238</b> to create a flow structure based on identifying a forward packet sent according to the TCP protocol (e.g., the &#x201c;reverseFlow=createFlow(P-&#x3e;dip, P-&#x3e;sip, P-&#x3e;dport, P-&#x3e;sport)&#x201d; line in the example computer code). The flow structure may include an expected source IP address and an expected destination IP address corresponding to a reverse packet expected to arrive at NIC <b>230</b> in response to the forward packet arriving at the destination device. In some examples, the expected source IP address of the reverse packet is the destination IP address of the forward packet and the expected destination IP address of the reverse packet is the source IP address of the forward packet, since the forward packet and the reverse packet complete a &#x201c;round trip&#x201d; between a pair of devices.</p><p id="p-0084" num="0082">Additionally, or alternatively, processing unit <b>25</b> may create a timestamp based on identifying a forward packet sent according to the TCP protocol (e.g., the &#x201c;reverseFlow-&#x3e;timeStamp=getTime( )&#x201d; line in the example computer code). NIC <b>230</b> may output the forward packet to the destination device. Processing unit <b>25</b> may create the timestamp to indicate an approximate time at which the forward packet departs NIC <b>230</b> for the destination device.</p><p id="p-0085" num="0083">When NIC <b>230</b> receives a reverse packet based on the destination device receiving the forward packet, Processing unit <b>25</b> may identify information corresponding to the reverse packet. For example, processing unit <b>25</b> may execute latency agent <b>238</b> to identify a source IP address and a destination IP address indicated by the reverse packet. Processing unit <b>25</b> may determine, based on the flow structure created for the forward packet, that the reverse packet represents a packet sent by the destination device in response to receiving the forward packet (e.g., the &#x201c;forwardFlow=getFlow(P-&#x3e;sip, P-&#x3e;dip, P-&#x3e;sport, P-&#x3e;dport)&#x201d; and &#x201c;if valid(forwardFlow)&#x201d; lines in the example computer code. Based on determining that the reverse packet corresponds to the forward packet, NIC <b>230</b> may execute the latency agent <b>238</b> to determine a round trip time between computing device <b>200</b> and the destination device of the forward packet (e.g., the &#x201c;rtt=getTime( )&#x2212;forwardFlow-&#x3e;timeStamp&#x201d; line in the example computer code. To determine the round trip time, latency agent <b>238</b> may subtract the timestamp corresponding to the time at which the forward packet departed from NIC <b>230</b> (e.g., &#x201c;forwardFlow-&#x3e;timestamp&#x201d;) from a current time at which the reverse packet arrives at NIC <b>230</b> (e.g., &#x201c;getTime( ))&#x201d;. Since the forward packet immediately prompted the destination device to send the reverse packet, the round trip time indicates a latency between computing device <b>200</b> and the destination device. Subsequently, latency agent <b>238</b> may update the latency between the computing device <b>200</b> and the destination device.</p><p id="p-0086" num="0084">When latency agent <b>238</b> determines that a packet arriving at NIC <b>230</b> is a forward packet sent according to ICMP or ARP, latency agent <b>238</b> may perform a process similar to the process described for TCP packets, except latency agent <b>238</b> might not check packet flags in the example of ICMP and ARP packets.</p><p id="p-0087" num="0085">In some examples, an NIC may use a rate at which a TCP sequence number is moving in an elephant flow to calculate a throughput between the two nodes (e.g., between two of servers <b>12</b>). Latency agent <b>238</b> may execute an algorithm to track one or more packet flows and track throughput corresponding to the one or more packet flows.</p><p id="p-0088" num="0086">In some examples, a media access control (MAC) address table on an NIC identifies whether a the NIC is alive or not. If the NIC is not communicating with any other node, the MAC table entry times out after 3 minutes. ESP agent <b>236</b> and/or latency agent <b>238</b> may use this timeout event to maintain the reachability state of an ESP agent <b>236</b>.</p><p id="p-0089" num="0087">In some examples, edge services controller <b>28</b> may configure, via ESP agent <b>236</b>, latency agent <b>238</b> with a list of endpoints of interest. Such endpoints may be IP addresses of one or more other computing devices. In such cases, latency agent <b>238</b> may only apply the latency determination techniques described herein to compute latencies between computing device <b>200</b> and those computing devices in the list of endpoints of interest. Alternatively, Ethernet bridge <b>234</b> filters may be configured to switch packets to processing unit <b>25</b> that have packet header information identifying such packets as associated with a list of endpoints of interest.</p><p id="p-0090" num="0088">In some examples, the n-tuple (P-&#x3e;sip, P-&#x3e;dip, P-&#x3e;sport, P-&#x3e;dport) packet information used in the above algorithm represent packet information in the inner packet header for overlay/virtual networks. That is, the endpoints are virtual network endpoints such as virtual machines or pods, and the latency information is computed for packets exchanged between virtual network endpoints rather than (or in addition to) between servers.</p><p id="p-0091" num="0089">Processing unit <b>25</b> may measure latency between two nodes (e.g., any two of servers <b>12</b>) of a network by tracking packets that pass through NICs <b>13</b>. In some examples, Processing units <b>25</b> might determine a latency between two devices based ion TCP packets having one or more of a set of packet flags. The destination device may process some TCP packets (e.g., TCP packets without any of the set of packet flags) before the destination device sends a reverse packet. The destination device may send immediate responses to TCP packets with URG, PSH or SYN flags. For example, when server <b>12</b>A sends a forward TCP packet with a URG flag, a PSH flag, or a SYN flag to server <b>12</b>D, server <b>12</b>D may immediately send a reverse packet upon receiving the forward packet. Additionally, or alternatively, destination devices may immediately send reverse packets upon receiving ICMP and ARP requests. As such, processing unit <b>25</b> of NIC <b>230</b> may record a time elapsed from when server computing device <b>200</b> sends the forward packet to when computing device <b>200</b> receives the reverse packet from the destination device. The elapsed time may represent a latency between computing device <b>200</b> and the destination device. By determining the latency between the computing device <b>200</b> and the destination device without sending a probe packet from the computing device <b>200</b> to the destination device, the processing unit <b>25</b> may collect information for a Pingmesh diagram while consuming a smaller amount of network resources as compared with systems that send ping packets to determine latency. Processing unit <b>25</b> may timestamp TCP packets having one or more tags (e.g., URG, PSH or SYN) to compute latency. In some examples, a processing unit (e.g., processing unit <b>25</b>) may execute an algorithm for determining a latency between two servers of servers <b>12</b>.</p><p id="p-0092" num="0090"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a conceptual diagram illustrating a data center with servers that each include a network interface card having a separate processing unit, controlled by an edge services platform, in accordance with one or more techniques of this disclosure. Racks of compute nodes <b>307</b>A-<b>307</b>N (collectively, &#x201c;racks of compute nodes <b>307</b>&#x201d;) may correspond to servers <b>12</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, and switches <b>308</b>A-<b>308</b>N (collectively, &#x201c;switches <b>308</b>&#x201d;) may correspond to the switches of fabric <b>14</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. An agent <b>302</b> or orchestrator <b>304</b> represents software executed by the processing unit (here referred to as a data processing unit or DPU) and receives configuration information for the processing unit and sends telemetry and other information for the NIC that includes the processing unit to orchestrator <b>304</b>. In some examples, agent <b>302</b> includes one or both of ESP agent <b>236</b> and latency agent <b>238</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In some examples, agent <b>302</b> includes a JESP agent. Network services <b>312</b>, L4-L7 services <b>314</b>, telemetry service <b>316</b>, and Linux and software development kit (SDK) services <b>318</b> may represent examples of services <b>233</b>. Orchestrator <b>304</b> may represent an example of edge services controller <b>28</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In some examples, agent <b>302</b> may send one or more computed latencies to orchestrator <b>304</b>. Orchestrator <b>304</b> may maintain a latency table to include latencies received from agent <b>302</b> and agents executed by other devices. Orchestrator <b>304</b> may generate a Pingmesh diagram based on the latency table, where the Pingmesh diagram indicates a health of the network system <b>8</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0093" num="0091">Network automation platform <b>306</b> connects to and manages network devices and orchestrator <b>304</b>, by which network automation platform <b>306</b> can utilize the edge services platform. Network automation platform <b>306</b> may, for example, deploy network device configurations, manage the network, extract telemetry, and analyze and provide indications of the network status.</p><p id="p-0094" num="0092"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram illustrating an example computing device that uses a network interface card having a separate processing unit, to perform services managed by an edge services platform according to techniques described herein. Although virtual machines are shown in this example, other instances of computing device <b>400</b> may also or alternatively run containers, native processes, or other endpoints for packet flows. Different types of vSwitches may be used, such as Open vSwitch or a virtual router (e.g., Contrail). Other types of interfaces between endpoints and NIC are also contemplated, such as tap interfaces, veth pair interfaces, etc.</p><p id="p-0095" num="0093"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram illustrating components of the example network system <b>8</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> including packet flows, in accordance with one or more techniques of this disclosure. As seen in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a first forward packet travels from server <b>12</b>A to server <b>12</b>X via connection <b>502</b>A, connection <b>502</b>B, connection <b>502</b>C, and connection <b>502</b>D. A first reverse packet travels from server <b>12</b>X to server <b>12</b>A via connection <b>504</b>A, connection <b>504</b>B, connection <b>504</b>C, and connection <b>504</b>D. A second forward packet travels from server <b>12</b>A to server <b>12</b>B via connection <b>506</b>A and connection <b>506</b>B. A second reverse packet travels from server <b>12</b>B to server <b>12</b>A via connection <b>508</b>A and connection <b>508</b>B. Processing unit <b>25</b>A may be configured to determine a latency between server <b>12</b>A and server <b>12</b>X based on the first forward packet and the first reverse packet, and processing unit <b>25</b>A may be configured to determine a latency between server <b>12</b>A and server <b>12</b>B based on the second forward packet and the second reverse packet. Consequently, processing unit <b>25</b>A may be configured to determine a latency between two servers based on packets that travel through the switch fabric <b>14</b>, and processing unit <b>25</b>A may be configured to determine a latency between two servers based on packets that travel between endpoints <b>23</b> without travelling through the switch fabric <b>14</b>.</p><p id="p-0096" num="0094"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flow diagram illustrating a first example operation for determining a latency between two devices, in accordance with one or more techniques of this disclosure. For convenience, <figref idref="DRAWINGS">FIG. <b>6</b></figref> is described with respect to network system <b>8</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and computing device <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. However, the techniques of <figref idref="DRAWINGS">FIG. <b>6</b></figref> may be performed by different components of network system <b>8</b> and computing device <b>200</b> or by additional or alternative devices.</p><p id="p-0097" num="0095">NIC <b>230</b> may receive a forward packet (<b>602</b>). In some examples, NIC <b>230</b> may receive the forward packet via bus <b>242</b>, because the forward packet represents a packet originating from computing device <b>200</b> and destined for a real or virtual destination device. The application data for the packet may originate from a process executing on computing device <b>200</b>, such as an application, service (in user space <b>245</b>), a kernel module of the host OS in kernel space <b>243</b>. Processing unit <b>25</b> may identify information corresponding to the forward packet (<b>604</b>) and validate the forward packet based on the information (<b>606</b>). In some examples, processing unit <b>25</b> may validate the forward packet by identifying a source IP address and a destination IP address indicated by the packet. To validate the forward packet, processing unit <b>25</b> may confirm that the packet represents a forward packet. Processing unit <b>25</b> may determine a packet type (<b>608</b>) of the forward packet. For example, to determine the packet type, processing unit may determine whether the packet is sent according to TCP, ICMP, or ARP. If the packet is sent according to TCP, processing unit <b>25</b> may determine whether the packet includes at least one of a set of packet headers. Processing unit <b>25</b> may create a flow structure (<b>610</b>) corresponding to the forward packet. In some examples, the flow structure indicates information corresponding to a reverse packet expected to arrive at NIC <b>230</b> in response to the destination device receiving the forward packet. Processing unit <b>25</b> may create a timestamp (<b>612</b>) corresponding to a time at which the forward packet departs the NIC <b>230</b> for the destination device.</p><p id="p-0098" num="0096">NIC <b>230</b> may send the forward packet (<b>614</b>) to the destination device. In some examples, NIC <b>230</b> is an example of NIC <b>13</b>A of server <b>12</b>A, and the destination device represents server <b>12</b>B, but this is not required. NIC <b>230</b> may represent any of NICs <b>13</b>, and the destination device may represent any of servers <b>12</b> that do not hist NIC <b>230</b>. Server <b>12</b>B may receive the forward packet (<b>616</b>). Server <b>12</b>B may process the forward packet (<b>618</b>). In some examples, server <b>12</b>B may identify that the packet represents a forward packet from server <b>12</b>A. Server <b>12</b>B may send a reverse packet (<b>620</b>) to server <b>12</b>A. In some examples, server <b>12</b>B sends the reverse packet immediately upon detecting an arrival of the forward packet.</p><p id="p-0099" num="0097">NIC <b>230</b> may receive the reverse packet (<b>622</b>). Processing unit <b>25</b> may identify reverse packet information (<b>624</b>) and validate the reverse packet (<b>626</b>). In some examples, processing unit <b>25</b> may validate the reverse packet by determining that a source IP address and a destination IP address of the reverse packet match the expected source IP address and the expected destination IP address of the flow structure (step <b>610</b>). Processing unit <b>25</b> may identify a current time (<b>628</b>) corresponding to the time at which NIC <b>230</b> receives the reverse packet. Processing unit <b>25</b> may compute a latency (<b>630</b>) between server <b>12</b>A and server <b>12</b>B based on the timestamp and the current time. Processing unit <b>25</b> may output information indicative of the latency (<b>632</b>).</p><p id="p-0100" num="0098"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flow diagram illustrating a second example operation for determining a latency between two devices, in accordance with one or more techniques of this disclosure. For convenience, <figref idref="DRAWINGS">FIG. <b>7</b></figref> is described with respect to network system <b>8</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and computing device <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. However, the techniques of <figref idref="DRAWINGS">FIG. <b>7</b></figref> may be performed by different components of network system <b>8</b> and computing device <b>200</b> or by additional or alternative devices.</p><p id="p-0101" num="0099">In some examples, processing unit <b>25</b> of NIC <b>230</b> of computing device <b>200</b> may identify information indicative of a forward packet (<b>702</b>). In some examples, the information includes a source device of the forward packet and a destination device of the forward packet. In some examples, the information includes a protocol of the packet. The information may inform the processing unit <b>25</b> whether the forward packet will cause the destination device to immediately send a reverse packet upon receiving the forward packet, thus allowing the processing unit <b>25</b> to calculate a latency between the first computing device and a second computing device. Based on determining that the information indicates that the forward packet will cause the destination device to immediately send a reverse packet, the processing unit <b>25</b> may determine that it is possible to compute a latency based on the forward packet and the reverse packet.</p><p id="p-0102" num="0100">Processing unit <b>25</b> may compute, based on a first time corresponding to the forward packet and a second time corresponding to a reverse packet associated with the forward packet, a latency between the first computing device and a second computing device (<b>704</b>). In some examples, processing unit <b>25</b> is configured to calculate the latency based on a difference in time between the first time and the second time. Processing unit <b>25</b> may output information indicative of the latency between the first computing device and the second computing device (<b>706</b>).</p><p id="p-0103" num="0101">The techniques described herein may be implemented in hardware, software, firmware, or any combination thereof. Various features described as modules, units or components may be implemented together in an integrated logic device or separately as discrete but interoperable logic devices or other hardware devices. In some cases, various features of electronic circuitry may be implemented as one or more integrated circuit devices, such as an integrated circuit chip or chipset.</p><p id="p-0104" num="0102">If implemented in hardware, this disclosure may be directed to an apparatus such as a processor or an integrated circuit device, such as an integrated circuit chip or chipset. Alternatively or additionally, if implemented in software or firmware, the techniques may be realized at least in part by a computer-readable data storage medium comprising instructions that, when executed, cause a processor to perform one or more of the methods described above. For example, the computer-readable data storage medium may store such instructions for execution by a processor.</p><p id="p-0105" num="0103">A computer-readable medium may form part of a computer program product, which may include packaging materials. A computer-readable medium may comprise a computer data storage medium such as random access memory (RAM), read-only memory (ROM), non-volatile random access memory (NVRAM), electrically erasable programmable read-only memory (EEPROM), Flash memory, magnetic or optical data storage media, and the like. In some examples, an article of manufacture may comprise one or more computer-readable storage media.</p><p id="p-0106" num="0104">In some examples, the computer-readable storage media may comprise non-transitory media. The term &#x201c;non-transitory&#x201d; may indicate that the storage medium is not embodied in a carrier wave or a propagated signal. In certain examples, a non-transitory storage medium may store data that can, over time, change (e.g., in RAM or cache).</p><p id="p-0107" num="0105">The code or instructions may be software and/or firmware executed by processing circuitry including one or more processors, such as one or more digital signal processors (DSPs), general purpose microprocessors, application-specific integrated circuits (ASICs), field-programmable gate arrays (FPGAs), or other equivalent integrated or discrete logic circuitry. Accordingly, the term &#x201c;processor,&#x201d; as used herein may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described herein. In addition, in some aspects, functionality described in this disclosure may be provided within software modules or hardware modules.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system comprising:<claim-text>a network interface card (NIC) of a first computing device, wherein the NIC comprises:<claim-text>a set of interfaces configured to receive one or more packets and send one or more packets; and</claim-text><claim-text>a processing unit configured to:<claim-text>identify information indicative of a forward packet;</claim-text><claim-text>compute, based on a first time corresponding to the forward packet and a second time corresponding to a reverse packet associated with the forward packet, a latency between the first computing device and a second computing device, wherein the second computing device includes a destination of the forward packet and a source of the reverse packet; and</claim-text><claim-text>output information indicative of the latency between the first computing device and the second computing device.</claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the system further comprises:<claim-text>a plurality of computing devices including the first computing device and the second computing device; and</claim-text><claim-text>the controller, wherein the controller is configured to:<claim-text>receive the information indicative of the latency between the first computing device and the second computing device; and</claim-text><claim-text>update a latency table to include the latency between the first computing device and the second computing device, wherein the latency table indicates a plurality of latencies, each latency of the plurality of latencies corresponding to a pair of computing devices of the plurality of computing devices.</claim-text></claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the forward packet represents a first forward packet, wherein the reverse packet represents a first reverse packet, and wherein the processing unit is configured to:<claim-text>identify information indicative of a second forward packet;</claim-text><claim-text>compute, based on a third time corresponding to the second forward packet and a fourth time corresponding to a second reverse packet associated with the second forward packet, a latency between the first computing device and a third computing device, wherein the third computing device includes a destination of the second forward packet and a source of the second reverse packet; and</claim-text><claim-text>output information indicative of the latency between the first computing device and the third computing device.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein to identify the information indicative of the forward packet, the processing unit is configured to:<claim-text>identify a source internet protocol (IP) address and a destination IP address in a header of a packet received at the NIC; and</claim-text><claim-text>determine, based on a source IP address and a destination IP address, that the packet represents the forward packet.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein based on identifying the information indicative of the forward packet, the processing unit is configured to:<claim-text>create a flow structure indicating information corresponding to the reverse packet associated with the forward packet; and</claim-text><claim-text>create a timestamp corresponding to a time in which the forward packet departs the first computing device for the second computing device, wherein the first time is based on the time.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the NIC is configured to output, via the set of interfaces to the second computing device, the forward packet, and wherein the processing unit is further configured to:<claim-text>identify information indicative of a packet received by the set of interfaces;</claim-text><claim-text>determine, based on the flow structure, that the packet represents the reverse packet associated with the forward packet;</claim-text><claim-text>identify a time corresponding to an arrival of the reverse packet at the set of interfaces; and</claim-text><claim-text>determine, based on the timestamp corresponding to the time in which the forward packet departs the set of interfaces and the time corresponding to the arrival of the reverse packet at the set of interfaces, the latency between the first computing device and the second computing device.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processing unit is configured to determine, based on the information indicative of the forward packet, that the forward packet represents at least one packet type of a set of packet types configured within the processing unit as useful for determining latencies.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the at least one packet type indicates that the forward packet is configured for prompting an immediate reverse packet from the second computing device so that a difference between the first time and the second time represents a round trip time between the first computing device and the second computing device.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein to determine that the forward packet represents the at least one packet type of the set of packet types, the processing unit is configured to:<claim-text>determine that the forward packet includes at least one Transmission Control Protocol (TCP) packet flag of a set of TCP packet flags, wherein the set of TCP packet flags include the synchronize (SYN) TCP packet flag, the urgent (URG) TCP packet flag, and the push (PSH) TCP packet flag.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein to determine that the forward packet represents the at least one packet type of the set of packet types, the processing unit is configured to determine that the forward packet is an internet control message protocol (ICMP) packet or an address resolution protocol (ARP) packet.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A method comprising:<claim-text>identifying, by a processing unit of a network interface card (NIC) of a first computing device, information indicative of a forward packet, wherein the NIC includes a set of interfaces, and wherein the set of interfaces is configured to receive one or more packets and send one or more packets;</claim-text><claim-text>computing, by the processing unit based on a first time corresponding to the forward packet and a second time corresponding to a reverse packet associated with the forward packet, a latency between the first computing device and a second computing device, wherein the second computing device includes a destination of the forward packet and a source of the reverse packet; and</claim-text><claim-text>outputting, by the processing unit, information indicative of the latency between the first computing device and the second computing device.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>receiving, by a controller, the information indicative of the latency between the first computing device and the second computing device, wherein a plurality of computing devices includes the first computing device and the second computing device; and</claim-text><claim-text>updating, by the controller, a latency table to include the latency between the first computing device and the second computing device, wherein the latency table indicates a plurality of latencies, each latency of the plurality of latencies corresponding to a pair of computing devices of the plurality of computing devices.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the forward packet represents a first forward packet, wherein the reverse packet represents a first reverse packet, and wherein the method further comprises:<claim-text>identifying, by the processing unit, information indicative of a second forward packet;</claim-text><claim-text>computing, by the processing unit based on a third time corresponding to the second forward packet and a fourth time corresponding to a second reverse packet associated with the second forward packet, a latency between the first computing device and a third computing device, wherein the third computing device includes a destination of the second forward packet and a source of the second reverse packet; and</claim-text><claim-text>outputting, by the processing unit information indicative of the latency between the first computing device and the third computing device.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein identifying the information indicative of the forward packet comprises:<claim-text>identifying, by the processing unit, a source internet protocol (IP) address and a destination IP address in a header of a packet received at the NIC; and</claim-text><claim-text>determining, by the processing unit based on a source IP address and a destination IP address, that the packet represents the forward packet.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein based on identifying the information indicative of the forward packet, the method further comprises:<claim-text>creating, by the processing unit, a flow structure indicating information corresponding to the reverse packet associated with the forward packet; and</claim-text><claim-text>creating, by the processing unit, a timestamp corresponding to a time in which the forward packet departs the first computing device for the second computing device, wherein the first time is based on the time.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising:<claim-text>outputting, by the NIC via the set of interfaces, the forward packet to the second computing device;</claim-text><claim-text>identifying, by the processing unit, information indicative of a packet received by the set of interfaces;</claim-text><claim-text>determining, by the processing unit based on the flow structure, that the packet represents the reverse packet associated with the forward packet;</claim-text><claim-text>identifying, by the processing unit, a time corresponding to an arrival of the reverse packet at the set of interfaces; and</claim-text><claim-text>determining, by the processing unit based on the timestamp corresponding to the time in which the forward packet departs the set of interfaces and the time corresponding to the arrival of the reverse packet at the set of interfaces, the latency between the first computing device and the second computing device.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising determining, by the processing unit based on the information indicative of the forward packet, that the forward packet represents at least one packet type of a set of packet types, the set of packet types configured within the processing unit as useful for determining latencies.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein determining that the forward packet represents the at least one packet type of the set of packet types comprises:<claim-text>determining, by the processing unit, that the forward packet includes at least one transmission control protocol (TCP) packet flag of a set of TCP packet flags, wherein the set of TCP packet flags include the synchronize (SYN) TCP packet flag, the urgent (URG) TCP packet flag, and the push (PSH) TCP packet flag.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein determining that the forward packet represents the at least one packet type of the set of packet types comprises determining, by the processing unit, that the forward packet is an internet control message protocol (ICMP) packet or an address resolution protocol (ARP) packet.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A non-transitory computer-readable medium comprising instructions for causing a processing unit of a network interface card (NIC) of a first computing device to:<claim-text>identify information indicative of a forward packet;</claim-text><claim-text>compute, based on a first time corresponding to the forward packet and a second time corresponding to a reverse packet associated with the forward packet, a latency between the first computing device and a second computing device, wherein the second computing device includes a destination of the forward packet and a source of the reverse packet; and</claim-text><claim-text>output information indicative of the latency between the first computing device and the second computing device.</claim-text></claim-text></claim></claims></us-patent-application>