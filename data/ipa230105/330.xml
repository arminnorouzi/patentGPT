<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000331A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000331</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17940153</doc-number><date>20220908</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>1</main-group><subgroup>06</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>1</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>1</main-group><subgroup>0638</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>1</main-group><subgroup>00009</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>1</main-group><subgroup>00045</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">ENDOSCOPE SYSTEM, ENDOSCOPE, AND DISTANCE CALCULATION METHOD</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/JP2020/010203</doc-number><date>20200310</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17940153</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>OLYMPUS CORPORATION</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>SASAKI</last-name><first-name>Yasuo</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>OLYMPUS CORPORATION</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An endoscope system includes a light source that emits lights with first to n-th wavelengths, a lens that makes the lights with the first to n-th wavelengths parallel lights, a diffractive optical element (DOE) that converges components of the lights with the first to n-th wavelengths, the components being included in the parallel lights, into first to n-th linear lights at mutually different positions, a slit that projects, onto a subject, first to n-th pattern lights based on the first to n-th linear lights, an imager that captures, as one-frame image, an image of the subject onto which the first to n-th pattern lights are projected, and a processor being configured to calculate a distance to the subject or a shape of the subject based on the image captured by the imager.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="155.11mm" wi="122.09mm" file="US20230000331A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="179.66mm" wi="124.12mm" file="US20230000331A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="203.28mm" wi="143.09mm" orientation="landscape" file="US20230000331A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="204.81mm" wi="144.53mm" orientation="landscape" file="US20230000331A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="187.79mm" wi="114.98mm" orientation="landscape" file="US20230000331A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="167.72mm" wi="120.14mm" orientation="landscape" file="US20230000331A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="237.66mm" wi="152.91mm" orientation="landscape" file="US20230000331A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="238.76mm" wi="151.89mm" orientation="landscape" file="US20230000331A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="175.94mm" wi="136.06mm" orientation="landscape" file="US20230000331A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="175.85mm" wi="141.56mm" orientation="landscape" file="US20230000331A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is a continuation of International Patent Application No. PCT/JP2020/010203, having an international filing date of Mar. 10, 2020, which designated the United States, the entirety of which is incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">There is a case where a size of a lesion or the like is wanted to be measured in an endoscope. To accurately measure the size, it is necessary to measure a distance from a scope to the lesion. Distance-measurement has been conventionally performed in three-dimensional measurement of a subject. As a system for the distance-measurement, a parallax system, a Time of Flight (ToF) system, and a structured light system have been known. As points of view for evaluating whether these distance-measurement systems are appropriate, consideration will be given to real-time measurement, real-time processing, and a reduction in diameter. The real-time measurement is a point of view indicating whether measurement is completed in a short period of time to perform distance-measurement of a moving target such as the living body with high accuracy. The real-time processing is a point of view indicating whether distance-measurement calculation can be performed in a short period of time to present information in real time during observation of a subject. The reduction in diameter is a point of view indicating whether a diameter of the leading end of a scope is not too large to mount a distance-measurement mechanism into the leading end of the scope.</p><p id="p-0004" num="0003">The parallax system is also called stereo vision, and a parallax image is acquired by two imaging systems. The parallax system enables acquisition of the parallax image in one frame, and thereby enables the real-time measurement.</p><p id="p-0005" num="0004">In the ToF system, measured is time in which a reflected wave of a light reaches an image sensor. The ToF system enables distance-measurement in one frame, and thereby enables the real-time measurement. Since process load for converting time to a distance is low, the ToF system enables the real-time processing.</p><p id="p-0006" num="0005">In the structured light system, a plurality of pattern lights with mutually different phases is projected onto the subject, and an image of the subject is captured. Since processing load for converting a manner in which each pattern light is reflected to a distance is low, the structured light system enables the real-time processing. Since a mechanism for projecting pattern lights has a small size, the structured light system can reduce a diameter more than the other distance-measurement systems. For example, the specification of United States Patent Application Publication No. 2009/0225321 discloses inclusion of three light sources and a grating, and a distance-measurement method of sequentially turning on the light sources one by one, sequentially projecting three pattern lights whose phases are mutually different, capturing an image of a subject onto which each pattern light is projected, thereby acquiring three images, and calculating a distance from the three images.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0007" num="0006">In accordance with one of some aspect, there is provided an endoscope system comprising:</p><p id="p-0008" num="0007">a light source that emits lights with first to n-th wavelengths;</p><p id="p-0009" num="0008">a lens that makes the lights with the first to n-th wavelengths parallel lights;</p><p id="p-0010" num="0009">a diffractive optical element (DOE) that converges components of the lights with the first to n-th wavelengths, the components being included in the parallel lights, into first to n-th linear lights at mutually different positions;</p><p id="p-0011" num="0010">a slit that projects, onto a subject, first to n-th pattern lights based on the first to n-th linear lights;</p><p id="p-0012" num="0011">an imager that captures, as an one-frame image, an image of the subject onto which the first to n-th pattern lights are projected; and</p><p id="p-0013" num="0012">a processor being configured to calculate a distance to the subject or a shape of the subject based on the image captured by the imager.</p><p id="p-0014" num="0013">In accordance with one of some aspect, there is provided an endoscope comprising:</p><p id="p-0015" num="0014">a lens that makes lights with first to n-th wavelengths parallel lights;</p><p id="p-0016" num="0015">a diffractive optical element (DOE) that converges components of the lights with the first to n-th wavelengths, the components being included in the parallel lights, into first to n-th linear lights at mutually different positions;</p><p id="p-0017" num="0016">a slit that projects, onto a subject, first to n-th pattern lights based on the first to n-th linear lights; and</p><p id="p-0018" num="0017">an imager that captures, as one-frame image, an image of the subject onto which the first to n-th pattern lights are projected.</p><p id="p-0019" num="0018">In accordance with one of some aspect, there is provided a distance calculation method comprising:</p><p id="p-0020" num="0019">a light source emitting lights with first to n-th wavelengths;</p><p id="p-0021" num="0020">a lens making the lights with the first to n-th wavelengths parallel lights;</p><p id="p-0022" num="0021">a diffractive optical element (DOE) converging components of the lights with the first to n-th wavelengths, the components being included in the parallel lights, into first to n-th linear lights at mutually different positions;</p><p id="p-0023" num="0022">a slit projecting, onto a subject, first to n-th pattern lights based on the first to n-th linear lights;</p><p id="p-0024" num="0023">an imager capturing, as one-frame image, an image of the subject onto which the first to n-th pattern lights are projected; and</p><p id="p-0025" num="0024">a processor calculating a distance to the subject or a shape of the subject based on the image captured by the imager.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a configuration example of an endoscope system.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram for describing a first operation example of the endoscope system.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for describing a second operation example of the endoscope system.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram for describing a wavelength of a pattern light.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example of spectral characteristics of an image sensor included in an imaging section.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a first detailed configuration example of the endoscope system.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a second detailed configuration example of the endoscope system.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a first detailed configuration example of a pattern light projection section.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates a second detailed configuration example of the pattern light projection section.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DESCRIPTION OF EXEMPLARY EMBODIMENTS</heading><p id="p-0035" num="0034">The following disclosure provides many different embodiments, or examples, for implementing different features of the provided subject matter. These are, of course, merely examples and are not intended to be limiting. In addition, the disclosure may repeat reference numerals and/or letters in the various examples. This repetition is for the purpose of simplicity and clarity and does not in itself dictate a relationship between the various embodiments and/or configurations discussed. Further, when a first element is described as being &#x201c;connected&#x201d; or &#x201c;coupled&#x201d; to a second element, such description includes embodiments in which the first and second elements are directly connected or coupled to each other, and also includes embodiments in which the first and second elements are indirectly connected or coupled to each other with one or more other intervening elements in between.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a configuration example of an endoscope system <b>10</b>. The endoscope system <b>10</b> includes a pattern light projection section <b>250</b>, an imaging section <b>270</b>, a processing section <b>110</b>, and an illumination light for observation emission section <b>260</b>. <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a case where the endoscope system <b>10</b> includes a control device <b>100</b>, and the processing section <b>110</b> that performs distance-measurement processing is included in the control device <b>100</b>. However, the configuration is not limited thereto, and may be a configuration in which the processing section <b>110</b> that performs distance-measurement processing is arranged in an information processing device arranged outside the control device <b>100</b>. The endoscope system <b>10</b> is, for example, a medical endoscope system, and a video scope used for the upper digestive tract or the lower digestive tract, a rigid scope used for surgery, or the like can be assumed.</p><p id="p-0037" num="0036">The pattern light projection section <b>250</b> projects first to n-th pattern lights onto a subject <b>5</b>, where n is an integer that is equal to or larger than 2, and n=3 in this case. Pattern lights PT<b>1</b> to PT<b>3</b> correspond to first to third pattern lights. The pattern lights PT<b>1</b> to PT<b>3</b> have striped or latticed patterns, and phases of the patterns and light wavelengths are mutually different. The imaging section <b>270</b> captures, as one-frame image, an image of the subject <b>5</b> onto which the pattern lights PT<b>1</b> to PT<b>3</b> are projected. The processing section <b>110</b> calculates a distance to the subject <b>5</b> or a shape of the subject <b>5</b> based on the one-frame image.</p><p id="p-0038" num="0037">The frame mentioned herein is an exposure period to capture one image. For example, frames are periodically repeated when a movie is captured. The above-mentioned one-frame image is captured in one of these frames. For example, as described later with reference to <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref>, the image of the subject <b>5</b> onto which the pattern lights PT<b>1</b> to PT<b>3</b> are projected is captured in a frame between frames in each of which an observation image is captured.</p><p id="p-0039" num="0038">In accordance with the present embodiment, since the image of the subject <b>5</b> onto which the pattern lights PT<b>1</b> to PT<b>3</b> are projected is captured in one frame, an image necessary for distance-measurement in the structured light system can be captured in a short period of time. This enables real-time measurement in the structured light system, and thereby enables distance-measurement of a moving target such as the living body with high accuracy. Since the pattern lights PT<b>1</b> to PT<b>3</b> have mutually different wavelengths, a subject image captured when the pattern lights PT<b>1</b> to PT<b>3</b> are projected is separated from the one-frame image utilizing the difference in wavelength, and a distance can be calculated from this information.</p><p id="p-0040" num="0039">Note that the parallax system puts high load for parallax calculation, has difficulty in performing real-time processing, and requires two imaging systems. Thus, it is difficult to reduce a diameter. In the conventional structured light system, since one frame image is captured with respect to single-time pattern projection, a plurality of frame images needs to be captured for capturing of images for all pattern projection. In this manner, it is preferable that the structured light system be adopted in the point of view of the reduction in the diameter, which is important in the endoscope. In the conventional structured light system, however, the real-time measurement is difficult because a plurality of images is captured, and there is an issue that the conventional structured light system is not suitable for distance-measurement of the moving target such as the living body with high accuracy. The present embodiment enables real-time measurement as described above, and thereby enables distance-measurement of the moving target such as the living body with high accuracy.</p><p id="p-0041" num="0040">The endoscope system <b>10</b> may perform diagnosis support using artificial intelligence (AI). In this case, inputting information of the distance to the subject <b>5</b> or the shape of the subject <b>5</b> together with the observation image to the AI can increase accuracy of diagnosis support.</p><p id="p-0042" num="0041">The shape obtained by distance-measurement is important as evidence when a diagnosis is made about whether or not the region of interest is a lesion. For example, in a case where a polyp is detected, measurement of a size of the polyp presents important evidence in a diagnosis about whether or not the polyp is cancer.</p><p id="p-0043" num="0042">The configuration example illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> is described in detail below. The pattern light projection section <b>250</b> includes first to third light sources S<b>1</b> to S<b>3</b> that emit lights with first to third wavelengths &#x3bb;1 to &#x3bb;3, respectively, and a slit section <b>252</b> in which a plurality of slits is arranged. The pattern light projection section <b>250</b> is also referred to as a pattern light projection device. As described above, pattern lights PT<b>1</b> to PT<b>3</b> have striped or lattice patterns.</p><p id="p-0044" num="0043">The striped pattern is a pattern in which parallel lines are repeated in a periodic or substantially periodic manner In a case where the pattern lights PT<b>1</b> to PT<b>3</b> have the striped patterns, a plurality of linear slits is arranged in the slit section <b>252</b>. The linear slits are parallel to each other, and arrayed in a direction orthogonal to the linear slits.</p><p id="p-0045" num="0044">The lattice pattern is a pattern in which a first line group and a second line group are orthogonal to each other, and, in each line group, parallel lines are repeated in a periodic or substantially periodic manner In a case where the pattern lights PT<b>1</b> to PT<b>3</b> have the lattice patterns, lattice-shaped slits are arranged in the slit section <b>252</b>. That is, a plurality of first linear slits and a plurality of second linear slits orthogonal to the first linear slits are arranged in the slit section <b>252</b>. The slit section <b>252</b> is also referred to as a grating. In addition, the slit section <b>252</b> is a plate-like member provided with slits, and is also referred to as a slit plate.</p><p id="p-0046" num="0045">The light sources S<b>1</b> to S<b>3</b> emit lights whose spectral peak wavelengths are wavelengths &#x3bb;1 to &#x3bb;3, respectively. The light sources S<b>1</b> to S<b>3</b> emit lights with line widths that allow respective spectra to be sufficiently separated from each other, and eject lights with line widths of, for example, several nanometers to several tens of nanometers. The light sources S<b>1</b> to S<b>3</b> are, as described later in a second embodiment, virtual light sources produced by using a laser diode, a diffractive optical element (DOE), and the like. Alternatively, each of the light sources S<b>1</b> to S<b>3</b> may be made of a light-emitting element such as a light-emitting diode and a band-pass filter. In a case where the pattern lights PT<b>1</b> to PT<b>3</b> have the striped patterns, each of the light sources S<b>1</b> to S<b>3</b> is a linear light source that is parallel to the liner slits. The light sources S<b>1</b> to S<b>3</b> are arrayed in a plane that is parallel to a flat surface of the slit section <b>252</b> in a direction that is identical to a direction in which the linear slits are arrayed. In a case where the pattern lights PT<b>1</b> to PT<b>3</b> have the lattice patterns, the light sources S<b>1</b> to S<b>3</b> are point-like light sources, and are arranged at different positions in the plane that is parallel to the flat surface of the slit section <b>252</b>.</p><p id="p-0047" num="0046">Lights from the light sources S<b>1</b> to S<b>3</b> pass through the slits of the slit section <b>252</b>, whereby the pattern lights PT<b>1</b> to PT<b>3</b> are generated. When the pattern lights PT<b>1</b> to PT<b>3</b> are projected onto a flat subject that is parallel to the flat surface of the slit section <b>252</b>, the pattern lights PT<b>1</b> to PT<b>3</b> have the striped or lattice patterns, and phases of stripes or lattices are different from each other. Taking striped pattern lights as an example, assuming that one period of stripes corresponds to a phase of 360 degrees, stripes of the pattern lights PT<b>1</b> to PT<b>3</b> are at ph1to ph3 degrees with respect to a position of 0 degrees serving as a reference, and the ph1 to ph3 degrees have mutually different values. Since the positions of the light sources S<b>1</b> to S<b>3</b> are different from each other, a phase relationship among the pattern lights PT<b>1</b> to PT<b>3</b> varies depending on a distance to the subject. However, in a case where the light sources S<b>1</b> to S<b>3</b> are arranged sufficiently close to each other, the phase relationship can be regarded as constant regardless of the distance to the subject.</p><p id="p-0048" num="0047">The illumination light for observation emission section <b>260</b> emits an illumination light for observation used for capturing the observation image toward the subject <b>5</b>. The observation image is an image for a user to observe the subject <b>5</b>. In terms of comparison with a light and an image for distance-measurement, the illumination light for observation is also referred to as a normal light, and the observation image is also referred to as a normal image. The illumination light for observation is only required to be an illumination light having spectral characteristics in accordance with an observation purpose, and is, for example, a white light or a special light. Examples of the special light include an illuminating light for narrow band imaging (NBI) including a green narrow band light and a blue narrow band light. Note that the illumination light for observation emission section <b>260</b> is also referred to as an illumination light for observation emission device.</p><p id="p-0049" num="0048">The imaging section <b>270</b> includes an objective lens that forms an image of the subject <b>5</b>, and an image sensor that captures the image of the subject <b>5</b> formed by the objective lens. Either the pattern light projection section <b>250</b> or the illumination light for observation emission section <b>260</b> emits a light. When the pattern light projection section <b>250</b> emits a light, the imaging section <b>270</b> captures the image of the subject <b>5</b> onto which the pattern lights PT<b>1</b> to PT<b>3</b> are projected. When the illumination light for observation emission section <b>260</b> emits a light, the imaging section <b>270</b> captures the observation image. The imaging section <b>270</b> includes one image sensor, and this common image sensor captures the image with the illumination light for observation and the image with the pattern light.</p><p id="p-0050" num="0049">The control device <b>100</b> is a device that performs control of the endoscope system <b>10</b>, image processing, and the like. A scope is connected to the control device <b>100</b>, and is provided with the pattern light projection section <b>250</b>, the illumination light for observation emission section <b>260</b>, and the imaging section <b>270</b>. The control device <b>100</b> includes the processing section <b>110</b>.</p><p id="p-0051" num="0050">The processing section <b>110</b> is implemented by a circuit device in which a plurality of circuit components is mounted on a substrate. Alternatively, the processing section <b>110</b> may be an integrated circuit device such as a processor, an application-specific integrated circuit (ASIC), and a field-programmable gate array (FPGA) circuit. The processor is a central processing unit (CPU), a microcomputer, a digital signal processor (DSP), or the like. In a case where the processing section <b>110</b> is the processor, the processor executes a program in which operations of the processing section <b>110</b> are described, and thereby implements the operations of the processing section <b>110</b>. The program is, for example, stored in a memory, which is not illustrated. Note that the processing section <b>110</b> is also referred to as a processing circuit or a processing device.</p><p id="p-0052" num="0051">When the pattern lights PT<b>1</b> to PT<b>3</b> are projected, the processing section <b>110</b> calculates, based on an image captured by the imaging section <b>270</b>, phases at respective positions of the image, and calculates, based on the phases, distances to the subject <b>5</b> at the respective positions of the image. This distance information is information such as a Z map in which a distance with respect to each pixel is calculated, and indicates a three-dimensional shape of the subject <b>5</b>. The processing section <b>110</b> calculates the shape of the subject <b>5</b> from the calculated distance. Various kinds of information of the calculated shape can be assumed. Examples of the information include a length, width, longer diameter, shorter diameter, height, or depth of the region of interest, a contour of the region of interest, or a combination of some of them.</p><p id="p-0053" num="0052">Taking the length of the region of interest as an example, a description will be given of a method of obtaining a length in a physical space from a length on the image. The processing section <b>110</b> obtains the length of the region of interest in the physical space based on the length of the region of interest on the image and the distance to the region of interest. That is, an expected angle of the region of interest viewed from the imaging section <b>270</b> can be found from an angle of view of the imaging section <b>270</b> and the length of the region of interest on the image. The processing section <b>110</b> obtains the length of the region of interest in the physical space from the expected angle and the distance to the region of interest. A value obtained by multiplying the expected angle by the distance to the region of interest is approximately equal to the length of the region of interest in the physical space.</p><p id="p-0054" num="0053">The processing section <b>110</b> may perform gradient correction when calculating the shape of the region of interest. That is, the processing section <b>110</b> calculates a gradient of the region of interest from a distance in the periphery of the region of interest, performs gradient correction on the length or the like calculated on the image, thereby converts the length into a length or the like when the region of interest is at a correct position with respect to the imaging section <b>270</b>, and outputs information of the corrected length or the like as shape information of the subject <b>5</b>.</p><p id="p-0055" num="0054">The processing section <b>110</b> may obtain not only the shape of the region of interest, but also a distance between two parts. For example, in a large intestine endoscope, it is assumed that a distance between a polyp and the anus is measured. In a case where the two parts are separate from each other and not seen within one image, a path therebetween is divided and then distances are measured. That is, the processing section <b>110</b> acquires a plurality of pattern images on the path between the two parts and connects distances calculated from the respective pattern images to calculate the distance between the two parts. The distance between a lesion and the anus serves as a material for determining whether or not functional preservation surgery should be applied.</p><p id="p-0056" num="0055">In the present embodiment, the endoscope system <b>10</b> switches the pattern lights PT<b>1</b> to PT<b>3</b> and the illumination light for observation to be emitted toward the subject <b>5</b>, and acquires the image with the pattern lights PT<b>1</b> to PT<b>3</b> and the observation image in units of one frame. Two examples of this operation are described below. The image captured when the pattern lights PT<b>1</b> to PT<b>3</b> are projected is hereinafter referred to as a pattern image.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram for describing a first operation example of the endoscope system <b>10</b>. The illumination light for observation emission section <b>260</b> emits illumination lights for observation in, among consecutive frames of F<b>1</b> to F<b>7</b>, F<b>1</b>, F<b>3</b>, F<b>5</b>, and F<b>7</b>. In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a high level of a waveform indicates turning-on of a light, and a low level thereof indicates turning-off of a light. The imaging section <b>270</b> captures images in the frames F<b>1</b>, F<b>3</b>, F<b>5</b>, and F<b>7</b> in which the illumination lights for observation are emitted. Each of the images serves as the observation image.</p><p id="p-0058" num="0057">The pattern light projection section <b>250</b> projects the pattern lights PT<b>1</b> to PT<b>3</b> in a frame in which no illumination light for observation is emitted. In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, for example, the pattern light projection section <b>250</b> projects the pattern lights PT<b>1</b> to PT<b>3</b> in the frame F<b>4</b> after a trigger signal is input, and the imaging section <b>270</b> captures the pattern image. In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the high level of the waveform indicates projection of the pattern lights PT<b>1</b> to PT<b>3</b>, and the low level thereof indicates that the pattern lights PT<b>1</b> to PT<b>3</b> are turned off. Time during which the pattern lights PT<b>1</b> to PT<b>3</b> are projected is freely determined, but is preferably short in terms of distance-measurement accuracy. The time may be set, for example, based on luminance of the pattern lights PT<b>1</b> to PT<b>3</b>, necessary distance-measurement accuracy, and the like. The processing section <b>110</b> performs distance-measurement processing based on the pattern image captured in the frame F<b>4</b>. In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the high level of the waveform indicates execution of the distance-measurement processing. In the endoscope, there is a case where accumulative time for acquiring observation images is wanted to be made as long as possible due to an issue of lack of a light quantity of the light source for observation. In this case, a conceivable method is to acquire the observation image assuming two fames in <figref idref="DRAWINGS">FIG. <b>2</b></figref> to be one frame, stop acquisition of the observation image only when the trigger signal is input, and emit pattern lights to acquire the pattern image. In this case, it is necessary to exercise ingenuity such as display of an observation image in a previous frame at the time of display of the observation image.</p><p id="p-0059" num="0058">The trigger signal is input to the processing section <b>110</b> by, for example, a user operation. For example, a button for instructing distance-measurement is arranged on a scope operating section of the scope, and the trigger signal is input from the scope operating section to the processing section <b>110</b> when the button is pressed. Alternatively, the trigger signal may be generated inside the processing section <b>110</b>. For example, the processing section <b>110</b> determines whether or not the region of interest is present in the observation image, and generates the trigger signal when detecting the region of interest in the observation image. The region of interest is, for example, a lesion such as cancer and a polyp. The processing section <b>110</b> performs AI processing or the like to detect the region of interest, and generates the trigger signal. In addition, the processing section <b>110</b> may use a result of distance-measurement of the region of interest detected by the AI processing to further perform AI processing, and thereby increase accuracy of determining the region of interest. For example, the AI processing uses the size, shape, or the like of the region of interest obtained by the distance-measurement.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for describing a second operation example of the endoscope system <b>10</b>. The observation image is captured in each of the frames F<b>1</b>, F<b>3</b>, F<b>5</b>, and F<b>7</b> similarly to <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in all of the frames F<b>2</b>, F<b>4</b>, and F<b>6</b> in which no illumination light for observation is emitted, the pattern light projection section <b>250</b> projects, regardless of the trigger signal, the pattern lights PT<b>1</b> to PT<b>3</b>, and the imaging section <b>270</b> captures the pattern image. The processing section <b>110</b> then performs the distance-measurement processing when the trigger signal is input. That is, the processing section <b>110</b> performs the distance-measurement processing based on the pattern image captured in the frame F<b>4</b> after the trigger signal is input. In this operation example, recording the pattern image in each frame enables execution of the distance-measurement processing afterwards also in frames that have not been subjected to the distance-measurement during observation.</p><p id="p-0061" num="0060">As described with reference to <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref>, the pattern light projection section <b>250</b> simultaneously projects the pattern lights PT<b>1</b> to PT<b>3</b> onto the subject <b>5</b> in the present embodiment. &#x201c;Simultaneously&#x201d; means that there is at least a timing at which all the pattern lights PT<b>1</b> to PT<b>3</b> are projected. Periods for projecting the pattern lights PT<b>1</b> to PT<b>3</b> are not necessarily matched with each other, but it is more preferable that the periods for projection be matched with each other.</p><p id="p-0062" num="0061">The pattern lights PT<b>1</b> to PT<b>3</b> are simultaneously projected in accordance with the present embodiment. Thus, the pattern lights PT<b>1</b> to PT<b>3</b> are projected without time difference in comparison with a method of capturing an image on a frame-by-frame basis with respect to each pattern light. This enables simultaneous obtaining of the pattern images of the moving subject such as the living body image using three pattern lights, and thereby enables distance-measurement with high accuracy.</p><p id="p-0063" num="0062">In accordance with the present embodiment, in a first frame, the illumination light for observation emission section <b>260</b> emits the illumination light for observation toward the subject <b>5</b>, and the imaging section <b>270</b> captures the observation image. In a second frame that is different from the first frame, the pattern light projection section <b>250</b> projects the pattern lights PT<b>1</b> to PT<b>3</b> toward the subject <b>5</b>, and the imaging section <b>270</b> captures the pattern image. The first frame corresponds to any one of F<b>1</b>, F<b>3</b>, F<b>5</b>, and F<b>7</b> illustrated in <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref>. The second frame corresponds to F<b>4</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, or any one of F<b>2</b>, F<b>4</b>, and F<b>6</b> illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0064" num="0063">In accordance with the present embodiment, it is possible to capture the observation image and present the observation image to a user while performing the distance-measurement in the background, and present, to the user, information of the distance or shape obtained by the distance-measurement together with the observation image.</p><heading id="h-0006" level="1">2. Wavelength of Light Source and Distance-Measurement Processing</heading><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram for describing wavelengths &#x3bb;1 to &#x3bb;3 of the pattern lights PT<b>1</b> to PT<b>3</b>. <figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates spectral characteristics of hemoglobin Hb and oxygenated hemoglobin HbO<sub>2</sub>. Note that each of hemoglobin Hb and oxygenated hemoglobin HbO<sub>2 </sub>is hereinafter simply referred to as hemoglobin.</p><p id="p-0066" num="0065">An observation target in a medical endoscope is the living body, but spectral characteristics of the living body are determined mainly by spectral characteristics of hemoglobin. Hence, in the present embodiment, the wavelengths &#x3bb;1 to &#x3bb;3 of the pattern lights PT<b>1</b> to PT<b>3</b> are set based on spectral characteristics of hemoglobin.</p><p id="p-0067" num="0066">The conventional structured light method uses a single color light. This is to equalize reflectances of respective patterns regardless of spectral characteristics of the subject. In a case where the mutually different wavelengths &#x3bb;1 to &#x3bb;3 are used like the present embodiment, it is preferable that reflectances of the subject with the respective wavelengths be identical. Thus, the present embodiment uses a wavelength region in which an absorption coefficient is as flat as possible in spectral characteristics of hemoglobin. That is, the wavelengths &#x3bb;1 to &#x3bb;3 are set so as to avoid a large absorption peak in a region of 450 nm or less and its periphery region in which a change in absorption coefficient is large.</p><p id="p-0068" num="0067">Specifically, the wavelengths &#x3bb;1 to &#x3bb;3 of the pattern lights PT<b>1</b> to PT<b>3</b> belong to a range of 460 nm or more and 700 nm or less. Since there is a small change in spectral characteristics of hemoglobin in the range of 460 nm or more and 700 nm or less, reflectances of respective patterns are almost identical. It is more preferable that the wavelengths &#x3bb;1 to &#x3bb;3 of the pattern lights PT<b>1</b> to PT<b>3</b> belong to a range of 460 nm or more and 520 nm or less. In the range of 460 nm or more and 520 nm or less, there is a smaller change in spectral characteristics of hemoglobin than that in the range of 460 nm or more and 700 nm or less.</p><p id="p-0069" num="0068">A mucosa serving as a target of the medical endoscope has multitudes of capillaries around its surface. As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, since absorption by hemoglobin is large in a wavelength region of 460 nm or less, a return light with a wavelength of 460 nm or less is extremely weak at a position where capillaries are present. In the structured light method, a distance is measured from a light quantity ratio at respective points of the pattern lights PT<b>1</b> to PT<b>3</b>. Thus, if there is a factor other than variations in intensity of lights of the patterns, that is, variations in intensity of return lights due to differences in reflectance of capillaries or the like, it is impossible to perform accurate distance-measurement. For example, in a case where one of the pattern lights PT<b>1</b> to PT<b>3</b> has a wavelength of 460 nm or less, the return light of the pattern light from capillaries is extremely weak in comparison with return lights of the other pattern lights. Consequently, a light quantity becomes inaccurate, and a distance at a position of capillaries cannot be measured accurately. In the present embodiment, setting the wavelengths of the pattern lights PT<b>1</b> to PT<b>3</b> to the range of 460 nm or more and 700 nm or less or the range of 460 nm or more and 520 nm or less makes the light quantity ratio of the return lights accurate, and thereby enables accurate distance-measurement.</p><p id="p-0070" num="0069">Subsequently, a description will be given of the distance-measurement processing that obtains the distance from the image captured by simultaneous emission of the pattern lights PT<b>1</b> to PT<b>3</b>. The following description will be given of an example in which &#x3bb;1=520 nm, &#x3bb;2=500 nm, and &#x3bb;3=480 nm.</p><p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example of spectral characteristics of the image sensor included in the imaging section <b>270</b>. The image sensor includes first to n-th color pixels that receive lights in first to n-th colors, respectively. Assume that n=3, and the image sensor is of a red, green, and blue (RGB) Bayer array type where R is a first color, G is a second color, and B is a third color. In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, KR is relative sensitivity of an R pixel, KG is relative sensitivity of a G pixel, and KB is relative sensitivity of a B pixel.</p><p id="p-0072" num="0071">As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, sensitivity of an i-th color pixel in a j-th wavelength &#x3bb;j is a<sub>ij</sub>, where each of i and j is an integer that is equal to or larger than 1 and equal to or smaller than n. The processing section <b>110</b> extracts images of the subject <b>5</b> when the respective pattern lights are projected based on the sensitivity a<sub>ij </sub>intensity values p<sub>1</sub>, p<sub>2</sub>, and p<sub>3 </sub>of R, G, and B in the pattern image. The processing section <b>110</b> then calculates the distance to the subject <b>5</b> or the shape of the subject <b>5</b> from phases based on the images of the subject <b>5</b> when the respective pattern lights are projected. Details of the processing will be described below.</p><p id="p-0073" num="0072">First, a relationship represented by the following Expression (1) holds between the intensity values p<sub>1</sub>, p<sub>2</sub>, and p<sub>3 </sub>that can be acquired from the pattern image and the images of the subject <b>5</b> when the respective pattern lights to be obtained are projected. In this expression, q<sub>1 </sub>is an intensity value in the image of the subject <b>5</b> when the pattern light PT<b>1</b> with the wavelength &#x3bb;1 is projected. Similarly, q<sub>2 </sub>and q<sub>3 </sub>are intensity values in the respective images of the subject <b>5</b> when the pattern light PT<b>2</b> with the wavelength &#x3bb;2 and the pattern light PT<b>3</b> with the wavelength &#x3bb;3 are projected. Assume that a position in the pattern image is represented by (x, y). The position (x, y) is, for example, pixel coordinates. In the following Expression (1), q<sub>1</sub>, q<sub>2</sub>, and q<sub>3 </sub>on the left-hand side and p<sub>1</sub>, p<sub>2</sub>, and p<sub>3 </sub>on the right-hand side are intensity values regarding the identical position (x, y).</p><p id="p-0074" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mo>[</mo>  <mrow>   <mi>Expression</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mn>1</mn>  </mrow>  <mo>]</mo> </mrow></math></maths><maths id="MATH-US-00001-2" num="00001.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mo>(</mo>      <mtable>       <mtr>        <mtd>         <msub>          <mi>p</mi>          <mn>1</mn>         </msub>        </mtd>       </mtr>       <mtr>        <mtd>         <msub>          <mi>p</mi>          <mn>2</mn>         </msub>        </mtd>       </mtr>       <mtr>        <mtd>         <msub>          <mi>p</mi>          <mn>3</mn>         </msub>        </mtd>       </mtr>      </mtable>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mrow>       <mo>(</mo>       <mtable>        <mtr>         <mtd>          <msub>           <mi>&#x3b1;</mi>           <mrow>            <mn>1</mn>            <mo>&#x2062;</mo>            <mn>1</mn>           </mrow>          </msub>         </mtd>         <mtd>          <msub>           <mi>&#x3b1;</mi>           <mrow>            <mn>1</mn>            <mo>&#x2062;</mo>            <mn>2</mn>           </mrow>          </msub>         </mtd>         <mtd>          <msub>           <mi>&#x3b1;</mi>           <mrow>            <mn>1</mn>            <mo>&#x2062;</mo>            <mn>3</mn>           </mrow>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>&#x3b1;</mi>           <mrow>            <mn>2</mn>            <mo>&#x2062;</mo>            <mn>1</mn>           </mrow>          </msub>         </mtd>         <mtd>          <msub>           <mi>&#x3b1;</mi>           <mn>22</mn>          </msub>         </mtd>         <mtd>          <msub>           <mi>&#x3b1;</mi>           <mrow>            <mn>2</mn>            <mo>&#x2062;</mo>            <mn>3</mn>           </mrow>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>&#x3b1;</mi>           <mrow>            <mn>3</mn>            <mo>&#x2062;</mo>            <mn>1</mn>           </mrow>          </msub>         </mtd>         <mtd>          <msub>           <mi>&#x3b1;</mi>           <mn>32</mn>          </msub>         </mtd>         <mtd>          <msub>           <mi>&#x3b1;</mi>           <mrow>            <mn>3</mn>            <mo>&#x2062;</mo>            <mn>3</mn>           </mrow>          </msub>         </mtd>        </mtr>       </mtable>       <mo>)</mo>      </mrow>      <mo>&#x2062;</mo>      <mrow>       <mo>(</mo>       <mtable>        <mtr>         <mtd>          <msub>           <mi>q</mi>           <mn>1</mn>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>q</mi>           <mn>2</mn>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>q</mi>           <mn>3</mn>          </msub>         </mtd>        </mtr>       </mtable>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>1</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0075" num="0073">Assume that a matrix whose element is a<sub>ij </sub>is A. The wavelengths &#x3bb;1 to &#x3bb;3 are selected so that each row vector of the matrix A is a linearly independent vector, that is, (a<sub>11</sub>, a<sub>12</sub>, a<sub>13</sub>), (a<sub>21</sub>, a<sub>22</sub>, a<sub>23</sub>), and (a<sub>31</sub>, a<sub>32</sub>, a<sub>33</sub>) are linearly independent vectors. Accordingly, since the matrix A has an inverse, the above-mentioned Expression (1) can be modified to the following Expression (2). The processing section <b>110</b> uses the following Expression (2) to calculate the intensity values q<sub>1</sub>, q<sub>2</sub>, and q<sub>3 </sub>at each (x, y).</p><p id="p-0076" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mo>[</mo>  <mrow>   <mi>Expression</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mn>2</mn>  </mrow>  <mo>]</mo> </mrow></math></maths><maths id="MATH-US-00002-2" num="00002.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mo>(</mo>      <mtable>       <mtr>        <mtd>         <msub>          <mi>q</mi>          <mn>1</mn>         </msub>        </mtd>       </mtr>       <mtr>        <mtd>         <msub>          <mi>q</mi>          <mn>2</mn>         </msub>        </mtd>       </mtr>       <mtr>        <mtd>         <msub>          <mi>q</mi>          <mn>3</mn>         </msub>        </mtd>       </mtr>      </mtable>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <msup>       <mrow>        <mo>(</mo>        <mtable>         <mtr>          <mtd>           <msub>            <mi>&#x3b1;</mi>            <mrow>             <mn>1</mn>             <mo>&#x2062;</mo>             <mn>1</mn>            </mrow>           </msub>          </mtd>          <mtd>           <msub>            <mi>&#x3b1;</mi>            <mrow>             <mn>1</mn>             <mo>&#x2062;</mo>             <mn>2</mn>            </mrow>           </msub>          </mtd>          <mtd>           <msub>            <mi>&#x3b1;</mi>            <mrow>             <mn>1</mn>             <mo>&#x2062;</mo>             <mn>3</mn>            </mrow>           </msub>          </mtd>         </mtr>         <mtr>          <mtd>           <msub>            <mi>&#x3b1;</mi>            <mrow>             <mn>2</mn>             <mo>&#x2062;</mo>             <mn>1</mn>            </mrow>           </msub>          </mtd>          <mtd>           <msub>            <mi>&#x3b1;</mi>            <mrow>             <mn>2</mn>             <mo>&#x2062;</mo>             <mn>2</mn>            </mrow>           </msub>          </mtd>          <mtd>           <msub>            <mi>&#x3b1;</mi>            <mrow>             <mn>2</mn>             <mo>&#x2062;</mo>             <mn>3</mn>            </mrow>           </msub>          </mtd>         </mtr>         <mtr>          <mtd>           <msub>            <mi>&#x3b1;</mi>            <mrow>             <mn>3</mn>             <mo>&#x2062;</mo>             <mn>1</mn>            </mrow>           </msub>          </mtd>          <mtd>           <msub>            <mi>&#x3b1;</mi>            <mrow>             <mn>3</mn>             <mo>&#x2062;</mo>             <mn>2</mn>            </mrow>           </msub>          </mtd>          <mtd>           <msub>            <mi>&#x3b1;</mi>            <mrow>             <mn>3</mn>             <mo>&#x2062;</mo>             <mn>3</mn>            </mrow>           </msub>          </mtd>         </mtr>        </mtable>        <mo>)</mo>       </mrow>       <mrow>        <mo>-</mo>        <mn>1</mn>       </mrow>      </msup>      <mo>&#x2062;</mo>      <mrow>       <mo>(</mo>       <mtable>        <mtr>         <mtd>          <msub>           <mi>p</mi>           <mn>1</mn>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>p</mi>           <mn>2</mn>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>p</mi>           <mn>3</mn>          </msub>         </mtd>        </mtr>       </mtable>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0077" num="0074">Note that the following Expressions (3) and (4) correspond to the above-mentioned Expressions (1) and (2) that are rewritten to another description format, but mean the same. A<sub>ij </sub>represents an ij component of the matrix A.</p><p id="p-0078" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Expression 3]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0079" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>p<sub>i</sub>=A<sub>ij</sub>q<sub>j </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0080" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>A<sub>ij</sub>=&#x3b1;<sub>ij </sub>&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0081" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Expression 4]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0082" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>q</i><sub>j</sub>=(<i>A</i><sup>&#x2212;1</sup>)<sub>ji</sub><i>p</i><sub>i </sub>&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0083" num="0075">The processing section <b>110</b> uses a look up table (LUT) to convert the intensity values q<sub>1</sub>, q<sub>2</sub>, and q<sub>3 </sub>to a phase WPh, as indicated by the following Expression (5). The LUT is a table in which a combination of the intensity values q<sub>1</sub>, q<sub>2</sub>, and q<sub>3 </sub>and the phase WPh, and is preliminarily stored in a memory or the like in the control device <b>100</b>. The phase WPh is a wrapped phase, and the processing section <b>110</b> performs unwrapping processing on the phase WPh and obtains the distance from a phase after the unwrapping processing. The unwrapping processing is processing of connecting phases that are discontinuous at a boundary in a period of stripes to convert the phases into continuous phases. That is, in the wrapped phase, a phase of one stripe belongs to 0 to 360 degrees, and a phase of a stripe next to the one stripe also belongs to 0 to 360 degrees. The unwrapping processing is to connect these phases at 0 to 720 degrees.</p><p id="p-0084" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mo>[</mo>     <mrow>      <mi>Expression</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mn>2</mn>     </mrow>     <mo>]</mo>    </mrow>   </mtd>   <mtd>    <mi>&#xf3ba;</mi>   </mtd>  </mtr>  <mtr>   <mtd>    <mrow>     <mrow>      <mo>(</mo>      <mtable>       <mtr>        <mtd>         <msub>          <mi>q</mi>          <mn>1</mn>         </msub>        </mtd>       </mtr>       <mtr>        <mtd>         <msub>          <mi>q</mi>          <mn>2</mn>         </msub>        </mtd>       </mtr>       <mtr>        <mtd>         <msub>          <mi>q</mi>          <mn>3</mn>         </msub>        </mtd>       </mtr>      </mtable>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <msup>       <mrow>        <mo>(</mo>        <mtable>         <mtr>          <mtd>           <msub>            <mi>&#x3b1;</mi>            <mn>11</mn>           </msub>          </mtd>          <mtd>           <msub>            <mi>&#x3b1;</mi>            <mn>12</mn>           </msub>          </mtd>          <mtd>           <msub>            <mi>&#x3b1;</mi>            <mn>13</mn>           </msub>          </mtd>         </mtr>         <mtr>          <mtd>           <msub>            <mi>&#x3b1;</mi>            <mn>21</mn>           </msub>          </mtd>          <mtd>           <msub>            <mi>&#x3b1;</mi>            <mn>22</mn>           </msub>          </mtd>          <mtd>           <msub>            <mi>&#x3b1;</mi>            <mn>23</mn>           </msub>          </mtd>         </mtr>         <mtr>          <mtd>           <msub>            <mi>&#x3b1;</mi>            <mn>31</mn>           </msub>          </mtd>          <mtd>           <msub>            <mi>&#x3b1;</mi>            <mn>32</mn>           </msub>          </mtd>          <mtd>           <msub>            <mi>&#x3b1;</mi>            <mn>33</mn>           </msub>          </mtd>         </mtr>        </mtable>        <mo>)</mo>       </mrow>       <mrow>        <mo>-</mo>        <mn>1</mn>       </mrow>      </msup>      <mo>&#x2062;</mo>      <mrow>       <mo>(</mo>       <mtable>        <mtr>         <mtd>          <msub>           <mi>p</mi>           <mn>1</mn>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>p</mi>           <mn>2</mn>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>p</mi>           <mn>3</mn>          </msub>         </mtd>        </mtr>       </mtable>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0085" num="0076">Assuming that a reference plane is set at a reference distance, a phase of a pattern light on the reference plane is determined. A difference between the phase serving as a reference and the phase obtained by the above-mentioned processing represents a relative distance between the reference plane and the subject <b>5</b>. That is, the processing section <b>110</b> calculates the distance to the subject <b>5</b> from the difference between the phase serving as the predetermined reference and the phase obtained by the above-mentioned processing.</p><p id="p-0086" num="0077">In a case where positions of the light sources S<b>1</b> to S<b>3</b> can be approximated to be sufficiently close to each other, the phase WPh may be obtained by functional calculation as indicated by the following Expression (6). When an argument is v/u, arctan 2 is a function for obtaining an angle of deviation of a point (u, v) in a uv orthogonal coordinates. An argument u of arctan 2 may be a negative value, and a value range is &#x2212;&#x3c0; to +&#x3c0;.</p><p id="p-0087" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mrow>  <mo>[</mo>  <mrow>   <mi>Expression</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mn>6</mn>  </mrow>  <mo>]</mo> </mrow></math></maths><maths id="MATH-US-00004-2" num="00004.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>Wph</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>x</mi>       <mo>,</mo>       <mi>y</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mi>arctan</mi>      <mo>&#x2062;</mo>      <mn>2</mn>      <mo>&#x2062;</mo>      <mrow>       <mo>(</mo>       <mfrac>        <mrow>         <msqrt>          <mn>3</mn>         </msqrt>         <mo>&#x2a2f;</mo>         <mrow>          <mo>(</mo>          <mrow>           <mrow>            <msub>             <mi>q</mi>             <mn>1</mn>            </msub>            <mo>(</mo>            <mrow>             <mi>x</mi>             <mo>,</mo>             <mi>y</mi>            </mrow>            <mo>)</mo>           </mrow>           <mo>-</mo>           <mrow>            <msub>             <mi>q</mi>             <mn>3</mn>            </msub>            <mo>(</mo>            <mrow>             <mi>x</mi>             <mo>,</mo>             <mi>y</mi>            </mrow>            <mo>)</mo>           </mrow>          </mrow>          <mo>)</mo>         </mrow>        </mrow>        <mrow>         <mrow>          <mn>2</mn>          <mo>&#x2a2f;</mo>          <mrow>           <msub>            <mi>q</mi>            <mn>2</mn>           </msub>           <mo>(</mo>           <mrow>            <mi>x</mi>            <mo>,</mo>            <mi>y</mi>           </mrow>           <mo>)</mo>          </mrow>         </mrow>         <mo>-</mo>         <mrow>          <msub>           <mi>q</mi>           <mn>1</mn>          </msub>          <mo>(</mo>          <mrow>           <mi>x</mi>           <mo>,</mo>           <mi>y</mi>          </mrow>          <mo>)</mo>         </mrow>         <mo>-</mo>         <mrow>          <msub>           <mi>q</mi>           <mn>3</mn>          </msub>          <mo>(</mo>          <mrow>           <mi>x</mi>           <mo>,</mo>           <mi>y</mi>          </mrow>          <mo>)</mo>         </mrow>        </mrow>       </mfrac>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>6</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0088" num="0078">The present embodiment enables determination of a phase from one-frame image obtained by simultaneous projection of the pattern lights PT<b>1</b> to PT<b>3</b>, and enables calculation of the distance to the subject <b>5</b> using the phase. Additionally, using a whole-space tabulation method enables measurement of the distance without conversion of a ratio of the pattern lights PT<b>1</b> to PT<b>3</b> at each point to a phase.</p><heading id="h-0007" level="1">3. Detailed Configuration Example</heading><p id="p-0089" num="0079"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a first detailed configuration example of the endoscope system <b>10</b>. The endoscope system <b>10</b> includes a scope <b>200</b>, the control device <b>100</b>, and a display section <b>300</b>. Note that a constituent element that has been already described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref> or the like is denoted by an identical reference sign, and a description thereof is omitted as appropriate.</p><p id="p-0090" num="0080">The scope <b>200</b> includes a flexible section <b>210</b> that is inserted into the living body, an operating section <b>220</b>, a connector <b>240</b> that connects the scope <b>200</b> to the control device <b>100</b>, and a universal code <b>230</b> that connects the operating section <b>220</b> and the connector <b>240</b> to each other.</p><p id="p-0091" num="0081">The pattern light projection section <b>250</b>, the illumination light for observation emission section <b>260</b>, and the imaging section <b>270</b> are arranged at the leading end of the flexible section <b>210</b>. An end portion on the opposite side of the leading end of the flexible section <b>210</b> is connected to the operating section <b>220</b>. The operating section <b>220</b> is a device for performing an angle operation of the flexible section <b>210</b>, an operation of a treatment tool, an operation for air supply and water supply, and the like. An optical fiber <b>251</b>, a light guide <b>261</b>, and a signal line <b>271</b> are arranged inside the flexible section <b>210</b>, the operating section <b>220</b>, and the universal code <b>230</b>. The optical fiber <b>251</b> connects the pattern light projection section <b>250</b> and the connector <b>240</b> to each other. The light guide <b>261</b> connects the illumination light for observation emission section <b>260</b> and the connector <b>240</b> to each other. The signal line <b>271</b> connects the imaging section <b>270</b> and the connector <b>240</b> to each other. The optical fiber <b>251</b>, the light guide <b>261</b>, and the signal line <b>271</b> are connected to an optical fiber, a light guide, and a signal line inside the control device <b>100</b>, respectively, by the connector <b>240</b>.</p><p id="p-0092" num="0082">The control device <b>100</b> includes the processing section <b>110</b>, a storage section <b>120</b>, a pattern light source <b>150</b>, and a light source for observation <b>160</b>.</p><p id="p-0093" num="0083">The light source for observation <b>160</b> is a light source that generates an illumination light for observation. The light source for observation <b>160</b> includes a white light source and an optical system that causes a light emitted from the white light source to be incident on the light guide. The white light source is, for example, a xenon lamp or a white light emitting diode (LED).</p><p id="p-0094" num="0084">The pattern light source <b>150</b> is a light source that emits laser lights with the wavelengths &#x3bb;1 to &#x3bb;3. The pattern light source <b>150</b> includes first to third laser diodes that generate laser lights with the wavelengths &#x3bb;1 to &#x3bb;3, and an optical system that causes the laser lights emitted from the first to third laser diodes to be incident on the optical fiber.</p><p id="p-0095" num="0085">The storage section <b>120</b> is a storage device such as a memory and a hard disk drive. The memory is a semiconductor memory, and a volatile memory such as a random-access memory (RAM), or a nonvolatile memory such as an electrically erasable programmable read-only memory (EEPROM). The storage section <b>120</b> stores a program, data, and the like necessary for an operation of the processing section <b>110</b>. In addition, the storage section <b>120</b> stores the LUT described above with reference to the Expression (5) as a table <b>121</b>. In a case where the phase is obtained by functional calculation like the Expression (6), the table <b>121</b> may be omitted.</p><p id="p-0096" num="0086">The processing section <b>110</b> includes a light source controller <b>111</b>, an image processing section <b>112</b>, a distance-measurement processing section <b>113</b>, and an image output section <b>114</b>. These sections may be implemented by individual hardware circuits. Alternatively, a function of each section may be implemented by the processor implementing a program in which operations of each section is described.</p><p id="p-0097" num="0087">The light source controller <b>111</b> controls the pattern light source <b>150</b> and the light source for observation <b>160</b>. The light source controller <b>111</b> controls a light-emission timing, light emission period, and light quantity of each of the pattern light source <b>150</b> and the light source for observation <b>160</b>.</p><p id="p-0098" num="0088">The image processing section <b>112</b> performs image processing on an image signal input from the imaging section <b>270</b> via the signal line <b>271</b>. The image processing section <b>112</b> performs processing of generating an RGB color image from a raw image. In addition, the image processing section <b>112</b> may perform, for example, white balance processing, gradation processing, highlighting processing, or the like. An image output from the image processing section <b>112</b> in a frame in which the illumination light for observation is emitted is the observation image, and an image output from the image processing section <b>112</b> in a frame in which the pattern lights are projected is the pattern image.</p><p id="p-0099" num="0089">The distance-measurement processing section <b>113</b> performs the distance-measurement processing described with reference to the Expressions (1) to (6) to obtain a distance to each position of the subject from the pattern image. The distance-measurement processing section <b>113</b> obtains the shape from the distance to each point of the subject. As described above, the shape is the shorter diameter, longer diameter, width, length, height, depth, or the like of the region of interest. The information regarding the distance or the shape is hereinafter collectively referred to as distance-measurement information. Note that the distance-measurement processing section <b>113</b> may obtain the distance to each position in the entire region of the pattern image, or may obtain the distance to each position only in a partial region such as the region of interest. The distance-measurement processing section <b>113</b> may obtain, as the shape information, a length, a height, or the like between points designated by the user.</p><p id="p-0100" num="0090">The distance-measurement processing section <b>113</b> may calculate a gradient of the region of interest based on the distance to the periphery of the region of interest, which is a target of calculation of the shape, and perform gradient correction on the shape of the region of interest based on the gradient. For example, the processing section <b>110</b> performs AI processing or the like, which will be described later, to detect the region of interest. The distance-measurement processing section <b>113</b> obtains distances to three or more points in the periphery of the region of interest, and obtains a gradient of the surface of the subject in the periphery of the region of interest based on the distances. The gradient is an angle formed between a line-of-sight of a camera of the imaging section <b>270</b> and the surface. The distance-measurement processing section <b>113</b> performs projective transformation so that the surface of the subject is at a correct position with respect to the imaging section <b>270</b>, and thereby obtains the shape of the region of interest in the subject at the correct position. The shape mentioned herein is a so-called size, and is, for example, the length, the width, the longer diameter, the shorter diameter, or the like.</p><p id="p-0101" num="0091">The image output section <b>114</b> outputs a display image to the display section <b>300</b> based on the observation image and the distance-measurement information. The display section <b>300</b> is a display such as a liquid crystal display device and an electro luminescence (EL) display device. The image output section <b>114</b> displays, for example, an observation image <b>301</b> and distance-measurement information <b>302</b> side by side in a display region of the display section <b>300</b>. Alternatively, the image output section <b>114</b> may superimpose the distance-measurement information on the observation image to display the observation image and the distance-measurement information. For example, information indicating the shape of the region of interest may be superimposed on the region of interest.</p><p id="p-0102" num="0092">Specifically, the image processing section <b>112</b> generates the observation image based on the image captured by the imaging section <b>270</b> in the first frame. The image output section <b>114</b> causes the display section <b>300</b> to display the observation image <b>301</b>. The first frame corresponds to any one of F<b>1</b>, F<b>3</b>, F<b>5</b>, and F<b>7</b> illustrated in <figref idref="DRAWINGS">FIG. <b>2</b> or <b>3</b></figref>. The distance-measurement processing section <b>113</b> calculates, as background processing of display of the observation image <b>301</b>, the distance to the subject or the shape of the subject based on the image captured by the imaging section <b>270</b> in the first frame. The second frame corresponds to F<b>4</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, or any one of F<b>2</b>, F<b>4</b>, and F<b>6</b> illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The image output section <b>114</b> adds the distance-measurement information <b>302</b> based on the distance to the subject or the shape of the subject to the observation image <b>301</b>, and causes the display section <b>300</b> to display the observation image <b>301</b>.</p><p id="p-0103" num="0093">While the pattern light source <b>150</b> is arranged in the control device <b>100</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the pattern light source <b>150</b> may be arranged in the operating section <b>220</b> of the scope <b>200</b>. While the pattern light source <b>150</b> and the light source for observation <b>160</b> are separately arranged in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the light source for observation <b>160</b> may serve as both thereof. In this case, an optical fiber that connects the light source for observation <b>160</b> and the illumination light for observation emission section <b>260</b> to each other baches off in the scope <b>200</b>, and the branched optical fibers are connected to the pattern light projection section <b>250</b>. This can omit connection between the pattern light projection section <b>250</b> and the control device <b>100</b>.</p><p id="p-0104" num="0094"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a second detailed configuration example of the endoscope system <b>10</b>. In <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the processing section <b>110</b> further includes an AI processing section <b>115</b>. Note that a constituent element that has been already described with reference to <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>6</b></figref> or the like is denoted by an identical reference sign, and a description thereof is omitted as appropriate.</p><p id="p-0105" num="0095">The image processing section <b>112</b> generates the observation image based on an image captured by the imaging section <b>270</b> in the first frame. The first frame corresponds to any one of F<b>1</b>, F<b>3</b>, F<b>5</b>, and F<b>7</b> illustrated in <figref idref="DRAWINGS">FIG. <b>2</b> or <b>3</b></figref>. The distance-measurement processing section <b>113</b> calculates the distance to the subject or the shape of the subject based on an image captured by the imaging section in the second frame. The second frame corresponds to F<b>4</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, or any one of F<b>2</b>, F<b>4</b>, and F<b>6</b> illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The AI processing section <b>115</b> performs AI processing based on the observation image and the distance to the subject or the shape of the subject to make determination regarding detection of presence of the region of interest or discrimination of a state. The detection of the presence of the region of interest is detection of whether or not the region of interest serving as a detection target is present in an image. The discrimination of the state of the region of interest is discrimination of a classification category indicating the state of the region of interest. Examples of the classification category include a lesion type such as cancer and a polyp, an index indicating a degree of progress such as a stage of cancer.</p><p id="p-0106" num="0096">A more detailed description will be given with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The observation image captured in the frame F<b>1</b> is input to the AI processing section <b>115</b>, and the AI processing section <b>115</b> performs AI processing based on the observation image to detect the region of interest. When detecting the region of interest, the AI processing section <b>115</b> outputs a trigger signal to the distance-measurement processing section <b>113</b>. The distance-measurement processing section <b>113</b> calculates, based on an image captured in the frame F<b>4</b> after the trigger signal is input, the distance to the subject or the shape of the subject, and inputs the distance to the subject or the shape of the subject to the AI processing section <b>115</b>. The observation image captured in the frame F<b>3</b>, the frame F<b>5</b>, or the like is input to the AI processing section <b>115</b>. The AI processing section <b>115</b> makes determination regarding the detection of the region of interest or the discrimination of the state based on the observation image and the distance to the subject or the shape of the subject. A result of this second determination may be used for generating a trigger signal again. Alternatively, the result of the second determination may be output to the image output section <b>130</b>, and the image output section <b>130</b> may add the result of determination to the observation image and cause the display section <b>300</b> to display the observation image.</p><p id="p-0107" num="0097"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a first detailed configuration example of the pattern light projection section <b>250</b>. The pattern light projection section <b>250</b> includes an incident section <b>256</b>, a DOE <b>253</b>, and the slit section <b>252</b>. The incident section <b>256</b> causes parallel lights including components with wavelengths &#x3bb;1 to &#x3bb;3 to be incident on the DOE <b>253</b>. The slit section <b>252</b>, on which a light emitted from the DOE <b>253</b> is incident, projects the pattern lights PT<b>1</b> to PT<b>3</b> with the wavelengths &#x3bb;1 to &#x3bb;3 onto the subject <b>5</b>.</p><p id="p-0108" num="0098">In accordance with the present embodiment, the pattern light source <b>150</b>, which is a laser light source, is arranged in the control device <b>100</b>, and only a simple optical system including the DOE <b>253</b> and the like is arranged at the leading end of the scope <b>200</b>. This can reduce a diameter of the scope <b>200</b>, and allows the laser light source to project the pattern lights PT<b>1</b> to PT<b>3</b> at high luminance To perform distance-measurement of the moving subject such as the living body with high accuracy, emission time of the pattern lights PT<b>1</b> to PT<b>3</b> needs to be made as short as possible, and using the laser light source enables reduction of emission time. The laser light source has an element that is larger than a light-emitting diode or the like. However, the configuration using the DOE <b>253</b> and the like allows the laser light source to be arranged in the control device <b>100</b> and can reduce the diameter of the scope <b>200</b>. Since there is no need for arranging a heat generation source such as the light-emitting diode at the leading end of the scope <b>200</b>, it is possible to prevent unnecessary heat generation at the leading end of the scope <b>200</b>.</p><p id="p-0109" num="0099">Note that in the ToF system, since an image sensor dedicated to ToF is arranged aside from the image sensor that captures the observation image, it is difficult to reduce the diameter. The present embodiment enables the reduction of the diameter as described above.</p><p id="p-0110" num="0100">Details of the configuration illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref> are described below. The incident section <b>256</b> includes the optical fiber <b>251</b> that guides laser lights, and a collimate lens <b>254</b> that makes lights emitted from an optical fiber <b>251</b> parallel lights. The laser lights with wavelengths &#x3bb;1 to &#x3bb;3 are guided by one optical fiber <b>251</b>, and diffused from an exit end of the optical fiber <b>251</b>. The collimate lens <b>254</b> makes the diffused laser lights parallel lights.</p><p id="p-0111" num="0101">The DOE <b>253</b> converges components with wavelengths &#x3bb;1 to &#x3bb;3 included in the parallel lights to a first linear light LL<b>1</b> to a third linear light LL<b>3</b> at mutually different positions. The slit section <b>252</b> has a plurality of mutually parallel slits. The linear lights LL<b>1</b> to LL<b>3</b> pass through the plurality of slits, whereby the pattern lights PT<b>1</b> to PT<b>3</b> are projected onto the subject.</p><p id="p-0112" num="0102">The linear lights LL<b>1</b> to LL<b>3</b> function as virtual light sources that emit lights toward the slit section <b>252</b>, and correspond to the light sources S<b>1</b> to S<b>3</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Each linear light is parallel to the slits of the slit section <b>252</b>. The linear lights LL<b>1</b> to LL<b>3</b> are arranged at different positions in the direction that is parallel to the plane of the slit section <b>252</b> and straight to the slits.</p><p id="p-0113" num="0103">The DOE <b>253</b> is an optical element that controls an emitted light to have a specific shape by utilizing a diffraction phenomenon. The specific shape is determined depending on a fine structure of the DOE <b>253</b>, and designing the fine structure enables obtaining of a light having a desired shape. In the present embodiment, the DOE <b>253</b> converges m-th order diffracted lights of incident parallel lights to linear lights with a predetermined focal length. A position at which the m-th order diffracted lights are converged is different depending on a wavelength. Since the incident lights includes components with the wavelengths &#x3bb;1 to &#x3bb;3, the m-th order diffracted lights with the respective wavelengths are converged as the linear lights LL<b>1</b> to LL<b>3</b> at mutually different positions. Note that m is an integer that is equal to or larger than 1. While the order is simply described herein as the m-th order, the m-th order may be either the +m-th order or the &#x2212;m-th order.</p><p id="p-0114" num="0104">The DOE <b>253</b> selectively converges the m-th order diffracted lights among zeroth, first, second, . . . order diffracted lights. That is, the m-th order diffracted lights emitted from the DOE <b>253</b> are greater in intensity than diffracted lights other than the m-th order diffracted lights. More specifically, the DOE <b>253</b> emits almost only the m-th order diffracted lights among the zeroth, first, second, . . . order diffracted lights.</p><p id="p-0115" num="0105">The wavelengths &#x3bb;1 to &#x3bb;3 of the laser lights are, for example, at regular intervals. In this case, the DOE <b>253</b> converges the linear lights LL<b>1</b> to LL<b>3</b> so as to be at regular intervals. The &#x201c;interval&#x201d; mentioned herein is an interval in the direction that is parallel to the plane of the slit section <b>252</b> and straight to the slits. With the arrangement of the linear lights LL<b>1</b> to LL<b>3</b> at regular intervals, phases of the pattern lights PT<b>1</b> to PT<b>3</b> are at regular intervals. This enables emission of the pattern lights that are appropriate for the structured light system. In a case where functional calculation described above with reference to the Expression (6) is used, the phases of the pattern lights PT<b>1</b> to PT<b>3</b> need to be at regular intervals. Note that the wavelengths &#x3bb;1 to &#x3bb;3 of the laser lights may be at irregular intervals, and the linear lights LL<b>1</b> to LL<b>3</b> converged by the DOE <b>253</b> may be at irregular intervals. Even in this case, as described above with reference to the Expression (5), using the LUT enables conversion of the pattern image into the distance.</p><p id="p-0116" num="0106"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates a second detailed configuration example of the pattern light projection section <b>250</b>. In <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the pattern light projection section <b>250</b> further includes a mask section <b>255</b>. Note that a constituent element that has been already described with reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref> is denoted by an identical reference sign, and a description thereof is omitted as appropriate.</p><p id="p-0117" num="0107">The mask section <b>255</b> is arranged between the DOE <b>253</b> and the slit section <b>252</b>. The mask section <b>255</b> causes the linear lights LL<b>1</b> to LL<b>3</b> converged from the m-th order diffracted lights to pass therethrough, and masks the diffracted lights other than the m-th order diffracted lights. <figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example in which the mask section <b>255</b> causes the linear lights LL<b>1</b> to LL<b>3</b> converged from the first order diffracted lights to pass therethrough, and masks diffracted lights other than the first order diffracted lights, such as the zeroth order diffracted lights and the second order diffracted lights. The mask section <b>255</b> is a plate-like member that is parallel to the slit section <b>252</b>, and apertures are arranged in the plate-like member. The mask section <b>255</b> is arranged so that the linear lights LL<b>1</b> to LL<b>3</b> pass through the apertures.</p><p id="p-0118" num="0108">The DOE <b>253</b> selectively converges the m-th order diffracted lights, but lights emitted from the DOE <b>253</b> include diffracted lights other than the m-th order diffracted lights. When the diffracted lights other than the m-th order diffracted lights pass through the slit section <b>252</b>, unnecessary pattern lights other than the pattern lights PT<b>1</b> to PT<b>3</b> that are originally intended are mixed in, and there is a possibility for reduction in accuracy of distance-measurement. In accordance with the present embodiment, the arrangement of the mask section <b>255</b> causes the diffracted lights other than the m-th order diffracted lights to be masked, thereby causes only the pattern lights PT<b>1</b> to PT<b>3</b> that are originally intended to be projected, and enables the distance-measurement with high accuracy.</p><p id="p-0119" num="0109">In <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the processing section <b>110</b> may perform AI processing to generate diagnosis support information. In this case, the storage section <b>120</b> stores a trained model that has been trained to generate the diagnosis support information, and the processing section <b>110</b> executes AI processing using the trained model to generate the diagnosis support information. Contents of the AI processing will be described below.</p><p id="p-0120" num="0110">When the region of interest is designated in the observation image, the processing section <b>110</b> acquires distance information of the subject, and measures a length, height, or the like of the region of interest based on the distance information. The region of interest is designated by, for example, a user operation. Alternatively, the processing section <b>110</b> may perform AI image recognition on the observation image to detect the region of interest, and thereby designate the region of interest. The processing section <b>110</b> generates the diagnosis support information with the acquired length, height or the like of the region of interest and the observation image serving as input to the AI processing. The diagnosis support information is information for estimating, for example, whether or not there is a lesion, a type of the lesion, a degree of malignancy of the lesion, a shape of the lesion, and the like. The image output section <b>114</b> of the processing section <b>110</b> causes the display section <b>300</b> to display the diagnosis support information together with the observation image.</p><p id="p-0121" num="0111">The processing section <b>110</b> may perform post-processing to generate the diagnosis support information. That is, the storage section <b>120</b> stores the observation image and the pattern image, and the processing section <b>110</b> may use the observation image and the pattern image stored in the storage section <b>120</b> to execute the AI processing.</p><p id="p-0122" num="0112">Although the embodiments to which the present disclosure is applied and the modifications thereof have been described in detail above, the present disclosure is not limited to the embodiments and the modifications thereof, and various modifications and variations in components may be made in implementation without departing from the spirit and scope of the present disclosure. The plurality of elements disclosed in the embodiments and the modifications described above may be combined as appropriate to implement the present disclosure in various ways. For example, some of all the elements described in the embodiments and the modifications may be deleted. Furthermore, elements in different embodiments and modifications may be combined as appropriate. Thus, various modifications and applications can be made without departing from the spirit and scope of the present disclosure. Any term cited with a different term having a broader meaning or the same meaning at least once in the specification and the drawings can be replaced by the different term in any place in the specification and the drawings.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001 MATH-US-00001-2" nb-file="US20230000331A1-20230105-M00001.NB"><img id="EMI-M00001" he="13.04mm" wi="76.20mm" file="US20230000331A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002 MATH-US-00002-2" nb-file="US20230000331A1-20230105-M00002.NB"><img id="EMI-M00002" he="13.38mm" wi="76.20mm" file="US20230000331A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230000331A1-20230105-M00003.NB"><img id="EMI-M00003" he="12.36mm" wi="76.20mm" file="US20230000331A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004 MATH-US-00004-2" nb-file="US20230000331A1-20230105-M00004.NB"><img id="EMI-M00004" he="10.92mm" wi="76.20mm" file="US20230000331A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An endoscope system comprising:<claim-text>a light source that emits lights with first to n-th wavelengths;</claim-text><claim-text>a lens that makes the lights with the first to n-th wavelengths parallel lights;</claim-text><claim-text>a diffractive optical element (DOE) that converges components of the lights with the first to n-th wavelengths, the components being included in the parallel lights, into first to n-th linear lights at mutually different positions;</claim-text><claim-text>a slit that projects, onto a subject, first to n-th pattern lights based on the first to n-th linear lights;</claim-text><claim-text>an imager that captures, as an one-frame image, an image of the subject onto which the first to n-th pattern lights are projected; and</claim-text><claim-text>a processor being configured to calculate a distance to the subject or a shape of the subject based on the image captured by the imager.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The endoscope system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the lens, the DOE, and the slit simultaneously project the first to n-th pattern lights onto the subject in a frame in which the one-frame image is captured.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The endoscope system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a light source for observation that emits illumination light for observation, wherein</claim-text><claim-text>in a first frame, the light source for observation emits the illumination light for observation, and the imager captures an image of the subject illuminated with the illumination light for observation, and</claim-text><claim-text>in a second frame different from the first frame, the lens, the DOE, and the slit project the first to n-th pattern lights onto the subject, and the imager captures the image of the subject onto which the first to n-th pattern lights are projected.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The endoscope system as defined in <claim-ref idref="CLM-00003">claim 3</claim-ref>,<claim-text>wherein the processor<claim-text>generates an observation image based on the image captured by the imager in the first frame,</claim-text><claim-text>calculates the distance or the shape based on the image captured by the imager in the second frame, and</claim-text><claim-text>performs artificial intelligence (AI) processing based on the observation image and the distance or the shape to make determination regarding detection of presence of a region of interest or discrimination of a state.</claim-text></claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The endoscope system as defined in <claim-ref idref="CLM-00003">claim 3</claim-ref>,<claim-text>wherein the processor<claim-text>generates an observation image based on the image captured by the imager in the first frame, and causes a display to display the observation image,</claim-text><claim-text>calculates the distance or the shape based on the image captured by the imager in the second frame as background processing of display of the observation image, and</claim-text><claim-text>adds information based on the distance or the shape to the observation image and causes the display to display the observation image.</claim-text></claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The endoscope system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein wavelengths of the first to n-th pattern lights belong to a range of 460 nm or more and 700 nm or less.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The endoscope system as defined in <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the wavelengths of the first to n-th pattern lights belong to a range of 460 nm or more and 520 nm or less.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The endoscope system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the imager includes an image sensor having first to n-th color pixels that receive lights in first to n-th colors, respectively,</claim-text><claim-text>when wavelengths of the first to n-th pattern lights are first to n-th wavelengths, and sensitivity in the i-th color pixel (i is an integer that is equal to or larger than 1 and equal to or smaller than n) is a<sub>ij</sub>, and</claim-text><claim-text>the processor extracts an image of the subject when each pattern light of the first to n-th pattern lights is projected based on the sensitivity a<sub>ij </sub>, and intensity values of the first to n-th colors in the one-frame image, and calculates the distance to the subject or the shape of the subject from a phase based on the image of the subject when each pattern light is projected.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The endoscope system as defined in <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein<claim-text>n=3,</claim-text><claim-text>the first to n-th colors are red (R), green (G), and blue (B),</claim-text><claim-text>each row vector of a matrix A whose element is a<sub>ij </sub>is a linearly independent vector, and</claim-text><claim-text>the processor calculates the following expression on an intensity value p<sub>i </sub>in the i-th color at each position of the one-frame image to determine an intensity value q<sub>j </sub>at each position of an image of the subject onto which the j-th pattern light is projected, and calculates the distance to the subject or the shape of the subject from a phase based on the intensity value q<sub>i</sub>.<claim-text><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>q</i><sub>j</sub>=(<i>A</i><sup>&#x2212;1</sup>)<sub>ji</sub><i>p</i><sub>i </sub><?in-line-formulae description="In-line Formulae" end="tail"?></claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The endoscope system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor calculates .a gradient of a region of interest, which is a target of calculation of the shape, based on the distance to a periphery of the region of interest, and performs gradient correction on the shape of the region of interest based on the gradient.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The endoscope system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the DOE emits m-th order diffracted lights (m is an integer that is equal to or larger than 1) of the components with the first to n-th wavelengths, the components being included in the parallel lights, and</claim-text><claim-text>the m-th order diffracted lights are greater in intensity than diffracted lights other than the m-th order diffracted lights.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The endoscope system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a mask arranged between the DOE and the slit, wherein<claim-text>the DOE emits m-th order diffracted lights of the components with the first to n-th wavelengths, the components being included in the parallel lights,</claim-text><claim-text>the m-th order diffracted lights are greater in intensity than diffracted lights other than the m-th order diffracted lights. and</claim-text><claim-text>the mask causes the first to n-th linear lights converged from the m-th order diffracted lights to pass therethrough, and masks diffracted lights other than the m-th order diffracted lights.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The endoscope system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first to n-th wavelengths are at regular intervals.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The endoscope system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising an optical fiber, wherein<claim-text>the light source emits laser lights with first to n-th wavelengths,</claim-text><claim-text>the optical fiber guides the laser lights, and</claim-text><claim-text>the lens is a collimate lens that makes lights emitted from the optical fiber the parallel lights.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. An endoscope comprising:<claim-text>a lens that makes lights with first to n-th wavelengths parallel lights;</claim-text><claim-text>a diffractive optical element (DOE) that converges components of the lights with the first to n-th wavelengths, the components being included in the parallel lights, into first to n-th linear lights at mutually different positions;</claim-text><claim-text>a slit that projects, onto a subject, first to n-th pattern lights based on the first to n-th linear lights; and</claim-text><claim-text>an imager that captures, as one-frame image, an image of the subject onto which the first to n-th pattern lights are projected.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A distance calculation method comprising:<claim-text>a light source emitting lights with first to n-th wavelengths;</claim-text><claim-text>a lens making the lights with the first to n-th wavelengths parallel lights;</claim-text><claim-text>a diffractive optical element (DOE) converging components of the lights with the first to n-th wavelengths, the components being included in the parallel lights, into first to n-th linear lights at mutually different positions;</claim-text><claim-text>a slit projecting, onto a subject, first to n-th pattern lights based on the first to n-th linear lights;</claim-text><claim-text>an imager capturing, as one-frame image, an image of the subject onto which the first to n-th pattern lights are projected; and</claim-text><claim-text>a processor calculating a distance to the subject or a shape of the subject based on the image captured by the imager.</claim-text></claim-text></claim></claims></us-patent-application>