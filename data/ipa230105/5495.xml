<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005496A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005496</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17930212</doc-number><date>20220907</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>27</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>51</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>27</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>51</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">HYBRID INPUT MACHINE LEARNING FRAMEWORKS</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17073638</doc-number><date>20201019</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11468908</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17930212</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>63024046</doc-number><date>20200513</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>63010177</doc-number><date>20200415</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Optum, Inc.</orgname><address><city>Minnetonka</city><state>MN</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Olinger</last-name><first-name>Randy</first-name><address><city>Edina</city><state>MN</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Kelly</last-name><first-name>Damian</first-name><address><city>Kildare</city><country>IE</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>O'Brien</last-name><first-name>Megan</first-name><address><city>Kildare</city><country>IE</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">There is a need for more accurate and more efficient hybrid-input prediction steps/operations. This need can be addressed by, for example, techniques for efficient joint processing of data objects. In one example, a method includes: processing an audio data object using an audio processing machine learning model to generate an audio-based feature data object, processing an acceleration data object using an acceleration processing machine learning model to generate an acceleration-based feature data object, processing the audio-based feature data object and the acceleration-based feature data object using an feature synthesis machine learning model in order to generate a hybrid-input prediction data object; and performing one or more prediction-based actions based at least in part on the hybrid-input prediction data object.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="107.78mm" wi="158.75mm" file="US20230005496A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="251.38mm" wi="197.19mm" orientation="landscape" file="US20230005496A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="242.57mm" wi="199.73mm" orientation="landscape" file="US20230005496A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="229.87mm" wi="199.98mm" orientation="landscape" file="US20230005496A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="257.47mm" wi="176.36mm" file="US20230005496A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="266.62mm" wi="196.93mm" orientation="landscape" file="US20230005496A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="218.52mm" wi="170.86mm" orientation="landscape" file="US20230005496A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="244.86mm" wi="170.86mm" orientation="landscape" file="US20230005496A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="267.21mm" wi="190.58mm" orientation="landscape" file="US20230005496A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="251.38mm" wi="148.59mm" file="US20230005496A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="257.73mm" wi="140.21mm" file="US20230005496A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="251.38mm" wi="140.63mm" file="US20230005496A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="257.22mm" wi="141.05mm" file="US20230005496A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="211.50mm" wi="191.09mm" orientation="landscape" file="US20230005496A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="43.94mm" wi="156.13mm" orientation="landscape" file="US20230005496A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The present application is a continuation of U.S. patent application Ser. No. 17/073,638 filed Oct. 19, 2020, which claims priority to and the benefit of U.S. Provisional Patent Application No. 63/010,177, filed on Apr. 15, 2020 and U.S. Provisional Patent Application No. 63/024,046, filed on May 13, 2020, each of which is incorporated by reference herein in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Various embodiments of the present invention address technical challenges related to generating hybrid-input prediction data objects by joint processing of multiple data objects having different formats (e.g., different sensory formats, different sampling formats, and/or the like) and disclose innovative techniques for improving efficiency and/or reliability of hybrid-input prediction systems.</p><heading id="h-0003" level="1">BRIEF SUMMARY</heading><p id="p-0004" num="0003">In general, embodiments of the present invention provide methods, apparatuses, systems, computing devices, computing entities, and/or the like for performing hybrid-input prediction steps/operations that require processing data contained in multiple data objects. Various embodiments of the present invention disclose techniques for consolidating (e.g., combining, merging and/or the like) data from two or more unrelated data objects and generating hybrid-input prediction data objects.</p><p id="p-0005" num="0004">In accordance with one aspect, a method for joint processing of an audio data object and an acceleration data object is provided. In one embodiment, the method comprises processing the audio data object using an audio processing machine learning model to generate an audio-based feature data object, wherein: the audio processing machine learning model comprises: (i) an audio model fast Fourier transform (FFT) layer that is configured to process the audio data object in order to generate an audio model FFT output and (ii) an audio model one-dimensional convolutional layer that is configured to process the audio model FFT output to generate an audio model convolutional output, and the audio-based feature data object is generated based at least in part on the audio model convolutional output; processing the acceleration data object using an acceleration processing machine learning model to generate an acceleration-based feature data object, wherein: the acceleration processing machine learning model comprises: (i) an acceleration model FFT layer that is configured to process the acceleration data object to generate an acceleration model FFT output, (ii) an acceleration model one-dimensional convolutional layer that is configured to process the acceleration model FFT output to generate an acceleration model convolutional output, and (iii) an acceleration model up-sampling layer that is configured to process the acceleration model convolutional output to generate an acceleration model up-sampling output, and the acceleration-based feature data object is generated based at least in part on the acceleration model up-sampling output; processing the audio-based feature data object and the acceleration-based feature data object using an feature synthesis machine learning model in order to generate a hybrid-input prediction data object; and performing one or more prediction-based actions based at least in part on the hybrid-input prediction data object.</p><p id="p-0006" num="0005">In accordance with another aspect, an apparatus for joint processing of an audio data object and an acceleration data object is provided, the apparatus comprising at least one processor and at least one memory including program code, the at least one memory and the program code configured to, with the processor, cause the apparatus to at least: process the audio data object using an audio processing machine learning model to generate an audio-based feature data object, wherein: the audio processing machine learning model comprises: (i) an audio model fast Fourier transform (FFT) layer that is configured to process the audio data object in order to generate an audio model FFT output and (ii) an audio model one-dimensional convolutional layer that is configured to process the audio model FFT output to generate an audio model convolutional output, and the audio-based feature data object is generated based at least in part on the audio model convolutional output; process the acceleration data object using an acceleration processing machine learning model to generate an acceleration-based feature data object, wherein: the acceleration processing machine learning model comprises: (i) an acceleration model FFT layer that is configured to process the acceleration data object to generate an acceleration model FFT output, (ii) an acceleration model one-dimensional convolutional layer that is configured to process the acceleration model FFT output to generate an acceleration model convolutional output, and (iii) an acceleration model up-sampling layer that is configured to process the acceleration model convolutional output to generate an acceleration model up-sampling output, and the acceleration-based feature data object is generated based at least in part on the acceleration model up-sampling output; process the audio-based feature data object and the acceleration-based feature data object using a feature synthesis machine learning model in order to generate a hybrid-input prediction data object; and perform one or more prediction-based actions based at least in part on the hybrid-input prediction data object.</p><p id="p-0007" num="0006">In accordance with yet another aspect, a computer program product for joint processing of an audio data object and an acceleration data object is provided, the computer program product comprising at least one non-transitory computer-readable storage medium having computer-readable program code portions stored therein, the computer-readable program code portions configured to: process the audio data object using an audio processing machine learning model to generate an audio-based feature data object, wherein: the audio processing machine learning model comprises: (i) an audio model fast Fourier transform (FFT) layer that is configured to process the audio data object in order to generate an audio model FFT output and (ii) an audio model one-dimensional convolutional layer that is configured to process the audio model FFT output to generate an audio model convolutional output, and the audio-based feature data object is generated based at least in part on the audio model convolutional output; process the acceleration data object using an acceleration processing machine learning model to generate an acceleration-based feature data object, wherein: the acceleration processing machine learning model comprises: (i) an acceleration model FFT layer that is configured to process the acceleration data object to generate an acceleration model FFT output, (ii) an acceleration model one-dimensional convolutional layer that is configured to process the acceleration model FFT output to generate an acceleration model convolutional output, and (iii) an acceleration model up-sampling layer that is configured to process the acceleration model convolutional output to generate an acceleration model up-sampling output, and the acceleration-based feature data object is generated based at least in part on the acceleration model up-sampling output; process the audio-based feature data object and the acceleration-based feature data object using a feature synthesis machine learning model in order to generate a hybrid-input prediction data object; and perform one or more prediction-based actions based at least in part on the hybrid-input prediction data object.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007">Having thus described the invention in general terms, reference will now be made to the accompanying drawings, which are not necessarily drawn to scale, and wherein:</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> provides an exemplary overview of a system architecture that can be used to practice embodiments of the present invention.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> provides an example hybrid-input predictive computing entity in accordance with some embodiments discussed herein.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b></figref> provides an example client computing entity in accordance with some embodiments discussed herein.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>4</b></figref> provides an exemplary schematic of a system for performing hybrid-input prediction steps/operations and generating hybrid-input data objects in accordance with some embodiments discussed herein.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>5</b></figref> provides an example acceleration processing machine learning model in accordance with some embodiments discussed herein.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>6</b></figref> provides an example audio processing machine learning model in accordance with some embodiments discussed herein.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>7</b></figref> provides an example feature synthesis machine learning model in accordance with some embodiments discussed herein.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>8</b></figref> provides an example acceleration model dimension adjustment layer in accordance with some embodiments discussed herein.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>9</b></figref> provides a flowchart of an example process for generating an audio-based feature data object in accordance with some embodiments discussed herein.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>10</b></figref> provides a flowchart of an example process for generating an acceleration-based feature data object in accordance with some embodiments discussed herein.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>11</b></figref> provides a flowchart of an example process for generating a hybrid-input prediction data object in accordance with some embodiments discussed herein.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>12</b></figref> provides a flowchart of an example process for generating an acceleration model dimension adjustment output in accordance with some embodiments discussed herein.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>13</b></figref> provides an example graphical representation illustrating various predictive outputs generated by the example hybrid-input prediction data system in accordance with some embodiments discussed herein.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>14</b></figref> provides an operational example of generating alerts based at least in part on corresponding hybrid-input predictive outputs in accordance with some embodiments discussed herein.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0023" num="0022">Various embodiments of the present invention are described more fully hereinafter with reference to the accompanying drawings, in which some, but not all embodiments of the inventions are shown. Indeed, these inventions may be embodied in many different forms and should not be construed as limited to the embodiments set forth herein; rather, these embodiments are provided so that this disclosure will satisfy applicable legal requirements. The term &#x201c;or&#x201d; is used herein in both the alternative and conjunctive sense, unless otherwise indicated. The terms &#x201c;illustrative&#x201d; and &#x201c;exemplary&#x201d; are used to be examples with no indication of quality level. Like numbers refer to like elements throughout. Moreover, while certain embodiments of the present invention are described with reference to predictive data analysis, one of ordinary skill in the art will recognize that the disclosed concepts can be used to perform other types of data analysis.</p><heading id="h-0006" level="1">I. Overview</heading><p id="p-0024" num="0023">Various embodiments of the present invention disclose techniques for performing joint processing of audio data and acceleration data that improves the efficiency of performing the noted joint processing and reliability of the generated results. Typically, predictive data analysis is performed by processing large amounts of data which are not configured and/or optimized for performing steps/operations on data with dissimilar characteristics (e.g., two or more data objects each having different dimensions and/or describing diverse types of information). There is a need for improved systems and methods configured to process data with dissimilar characteristics. Additionally, there is a need for improving hybrid-input prediction steps/operations to utilize less data and/or sparse data as input such that fewer computing resources are utilized overall. Thus, various embodiments of the present invention improve hybrid-input prediction steps/operations by utilizing less input data or sparse input data without information loss while working within the limitations of the available data. The inventors have confirmed, via experiments and theoretical calculations, that various embodiments of the disclosed techniques improve efficiency and accuracy of hybrid-input prediction systems and predictive data analysis relative to various state-of-the-art solutions.</p><p id="p-0025" num="0024">Various embodiments of the present invention utilize machine learning models configured to perform steps/operations (e.g., convolutional steps/operations, up-sampling steps/operations and/or the like) with respect to two or more dissimilar data objects describing unrelated data (e.g., raw audio data and raw accelerometer data) in order to generate hybrid-input prediction data outputs/objects describing important/novel features of the data contained in the two or more data objects. The machine learning models may modify one or both data objects in order to perform subsequent mathematical/arithmetic steps/operations and important/novel features from the data in the two or more data outputs/objects. Accordingly, accurate predictions can be obtained where some of the available data is limited or sparse and/or where it may be undesirable to utilize more resources to obtain richer data due to a lack of complexity in the underlying type of information. By correlating unrelated data to extract important features using the methods described herein, the resulting hybrid-input prediction data outputs/objects contain rich information and lead to more accurate predictions which may be utilized to generate user interface data (e.g., real-time alerts) for an end user.</p><p id="p-0026" num="0025">Accordingly, by utilizing some or all of the innovative techniques disclosed herein for performing hybrid-input prediction steps/operations, various embodiments of the present invention increase efficiency of data processing and accuracy of predictions. In doing so, various embodiments of the present invention make substantial technical contributions to the field of predictive data analysis and substantially improve state-of-the-art hybrid-input prediction systems.</p><heading id="h-0007" level="1">II. Definitions of Certain Terms</heading><p id="p-0027" num="0026">The term &#x201c;audio data object&#x201d; may refer to a data object that describes a set of information (e.g., an input dataset) corresponding to sound such as raw audio data (e.g., recorded audio data for a monitored individual). Raw audio data may comprise one or more sound waves. Each sound wave comprises a wavelength oscillating at a given frequency for a duration of time. An audio data object may describe such raw audio data in a time domain representation or the frequency domain representation. Raw audio data may be sampled at a given sampling rate (e.g., 44.1 kHz) to generate a time domain representation. In general, a data object generated based at least in part on a high sampling rate will generate a data object with more information/data than a data object generated based at least in part on a lower sampling rate. An example audio data object may be represented graphically by plotting extracted values/features as a function of time. An audio data object describing raw audio data in the time domain may comprise a two-dimensional matrix in which a first dimension corresponds with a number of segments in time and a second dimension corresponds with a plurality of values/features based at least in part on the frequencies of the sound waves occurring at corresponding segments in time.</p><p id="p-0028" num="0027">The term &#x201c;acceleration data object&#x201d; may refer to a data object that describes a set of information (e.g., an input dataset) corresponding to acceleration of motion of a body (e.g., a human) with respect to the body's frame (e.g., raw acceleration data). An example acceleration data object may describe a body's change in orientation and/or movements (e.g., vibrations) in two or more directions (e.g., recorded cross-body acceleration data for a monitored individual). Raw acceleration data may be represented graphically as a waveform plotted with respect to time in which positive values indicate an increase in velocity, negative values indicate a decrease in velocity and zero/null values indicate a constant velocity. An acceleration data object may describe such raw acceleration data in a time domain representation or the frequency domain representation by sampling the raw acceleration data at a given sampling rate (e.g., 50 Hz). Sampling raw acceleration data at a high sampling rate (e.g., 44.1 kHz) is undesirable due to a lack of complexity in the underlying data. An acceleration data object describing raw acceleration data in the time domain may comprise a two-dimensional matrix having a length and a width in which a first dimension corresponds with a number of segments in time and a second dimension corresponds with a plurality of features associated with oscillations at different frequencies occurring at corresponding segments in time.</p><p id="p-0029" num="0028">The term &#x201c;audio processing machine learning model&#x201d; may refer to a data object that describes parameters and/or hyper-parameters of machine learning model configured to perform a plurality of steps/operations with respect to an audio data object in order to generate audio-based feature data object. For example, the audio processing machine learning model may comprise a plurality of layers each configured to perform one or more steps/operations with respect to an input data object (e.g., audio data object). Each layer of the audio processing machine learning model may be configured to perform a plurality of steps/operations to modify one or more dimensions of a corresponding input audio data object. An example audio processing machine learning model may comprise at least one audio model fast Fourier transform layer, and at least one audio model one-dimensional convolutional layer.</p><p id="p-0030" num="0029">The term &#x201c;audio model fast Fourier transform (FFT) layer&#x201d; may refer to a data object that describes a layer of an audio processing machine learning model configured to process an audio data object using an FFT function and generate an audio model FFT output. The audio model FFT layer is configured to perform a plurality of steps/operations with respect to an audio data object describing raw audio data in the time domain in order to generate an audio data object describing raw audio data in the frequency domain. The resulting audio model FFT output may have dimensions that are different from the dimensions of the input data object (e.g., audio data object). For example, the audio model FFT layer may apply a mask/filter to the input data object (e.g., audio data object) in order to extract relevant features and generate an audio model FFT output describing such features.</p><p id="p-0031" num="0030">The term &#x201c;audio model FFT output&#x201d; may refer to a data object that describes the output generated by an audio model FFT layer of an audio processing machine learning model. The audio model FFT output may refer to a data object describing a two-dimensional matrix having a length and a width in which a first dimension corresponds with a plurality of segments in time and a second dimension corresponds with a plurality of relevant features.</p><p id="p-0032" num="0031">The term &#x201c;audio-based feature data object&#x201d; may refer to a data object that is generated by an audio processing machine learning model and is configured to describe the output of processing particular audio data by an audio processing machine learning model. An example audio-based feature data object may comprise a two-dimensional matrix having a length and a width in which a first dimension corresponds with a number of segments in time and a second dimension corresponds with a plurality of features associated with frequencies of sound waves occurring at the corresponding segments in time. The audio-based feature data object may comprise dimensions that are different from the dimensions of the input data object (e.g., audio data object) processed by the machine learning model. For instance, the dimensions of the matrix of the audio-based feature data object may be truncated in the first dimension (x-direction) and lengthened in the second dimension (y-direction) such that the audio-based feature data object contains more information sampled over fewer segments in time.</p><p id="p-0033" num="0032">The term &#x201c;audio model one-dimensional convolutional layer&#x201d; may refer to a data object that describes layer of a machine learning model configured to perform one or more convolutional steps/operations with respect to an audio model FFT output and generate an audio model convolutional output. The audio model one-dimensional convolutional layer is configured to extract feature data from an audio model FFT output. For example, the audio model one-dimensional convolutional layer may extract feature data from an audio model FFT output in order to generate an audio-based feature data object with different dimensions from the audio model FFT output. An audio model convolutional output may refer to the output generated by an audio model one-dimensional convolutional layer of an audio processing machine learning model.</p><p id="p-0034" num="0033">The term &#x201c;acceleration processing machine learning model&#x201d; may refer to a data object that describes parameters and/or hyper-parameters of machine learning model configured to perform a plurality of steps/operations with respect to an acceleration data object and generate an acceleration-based feature data object. For example, the acceleration processing machine learning model may comprise a plurality of layers each configured to perform one or more steps/operations with respect to an input data object (e.g., acceleration data object). Each layer of the acceleration processing machine learning model may be configured to perform a plurality of steps/operations to modify one or more dimensions of a corresponding input acceleration data object. An example acceleration processing machine learning model may comprise at least one acceleration model fast Fourier transform layer, and at least one acceleration model one-dimensional convolutional layer.</p><p id="p-0035" num="0034">The term &#x201c;acceleration-based feature data object&#x201d; may refer to a data object that is generated by processing an acceleration data object using an acceleration processing machine learning model. An example acceleration-based feature data object may comprise a two-dimensional matrix having a length and a width in which a first dimension corresponds with a number of segments in time and a second dimension corresponds with a plurality of features associated with oscillations/vibrations occurring at the corresponding segments in time. The acceleration-based feature data object may comprise dimensions that are different from the dimensions of the input data object (e.g., acceleration data object) processed by the machine learning model. For instance, the dimensions of the matrix of the acceleration-based feature data object may be truncated in the first dimension (x-direction) and lengthened in the second dimension (y-direction) such that the acceleration-based feature data object contains more information sampled over fewer segments in time.</p><p id="p-0036" num="0035">The term &#x201c;acceleration model FFT layer&#x201d; may refer to a data object that describes a layer of an acceleration processing machine learning model configured to process an acceleration data object using an FFT function and generate an acceleration model FFT output. For example, the acceleration model FFT layer may be configured to perform a plurality of steps/operations with respect to an acceleration data object describing raw acceleration data in the time to domain to generate an acceleration data object describing raw acceleration data in the frequency domain. The resulting acceleration model FFT output may have dimensions that are different from the dimensions of the input data object (e.g., acceleration data object). For example, the acceleration model FFT layer may apply a mask/filter to the input data object (e.g., acceleration data object) in order to extract relevant features and generate an acceleration model FFT output describing such features.</p><p id="p-0037" num="0036">The term &#x201c;acceleration model FFT output&#x201d; may refer to a data object that describes the output generated by an acceleration model FFT layer of an acceleration processing machine learning model. The acceleration model FFT output may refer to a data object describing a two-dimensional matrix having a length and a width in which a first dimension corresponds with a plurality of segments in time and a second dimension corresponds with a plurality of relevant features.</p><p id="p-0038" num="0037">The term &#x201c;acceleration model one-dimensional convolutional layer&#x201d; may refer to a data object that describes a layer of a machine learning model configured to perform one or more convolutional steps/operations with respect to an input data object (e.g., an acceleration model FFT output) and generate an acceleration model convolutional output. The acceleration model one-dimensional convolutional layer is configured to extract feature data from an acceleration model FFT output. For example, the acceleration model one-dimensional convolutional layer may extract feature data from an acceleration model FFT output in order to generate an acceleration model convolutional output with different dimensions from the acceleration model FFT output.</p><p id="p-0039" num="0038">The term &#x201c;acceleration model convolutional output&#x201d; may refer to a data object that describes the output generated by an acceleration model one-dimensional convolutional layer of an acceleration processing machine learning model. In some embodiments, the acceleration model one-dimensional convolutional layer of an acceleration processing machine learning model processes the acceleration model FFT output of an acceleration model FFT layer in order to generate the acceleration model convolutional output.</p><p id="p-0040" num="0039">The term &#x201c;acceleration model up-sampling layer&#x201d; may refer to a data object that is configured to describe layer of a machine learning model configured to perform one or more up-sampling steps/operations with respect to an input data object (e.g., acceleration model FFT output or acceleration model convolutional output) in order to generate an acceleration model up-sampling output. The acceleration model up-sampling layer may be configured to transform (e.g., lengthen or truncate) at least one dimension of the input data object (e.g., acceleration model FFT output or acceleration model convolutional output). For example, by increasing the length of at least one dimension (e.g., number of time segments) corresponding with the input data object. The acceleration processing machine learning model may comprise a plurality of acceleration model up-sampling layers, each configured to transform a particular dimension of the input data object. An acceleration model up-sampling output may refer to a data object describing the output generated by the acceleration model up-sampling layer of an acceleration processing machine learning model.</p><p id="p-0041" num="0040">The term &#x201c;feature synthesis machine learning model&#x201d; may refer to a data object that is configured to describe parameters and/or hyper-parameters of a machine learning model configured to process two or more input data objects (e.g., an audio-based feature data object and an acceleration-based feature data object) in order to generate an input that integrates features described by both of the two or more input data objects. For example, the feature synthesis machine learning model may be configured to perform a plurality of steps/operations with respect to an audio-based feature data object and an acceleration-based feature object in order to generate a hybrid-input prediction data object. The feature synthesis machine learning model may comprise one or more layers configured to extract sequential information/understanding (e.g., recognize patterns) in historical data contained in the preceding machine learning model(s) and/or layers.</p><p id="p-0042" num="0041">The term &#x201c;acceleration model sequence modeling layer&#x201d; may refer to a data object that describes a layer of an acceleration model dimension adjustment layer of an acceleration model dimension adjustment layer machine learning model configured to process an acceleration model up-sampling output to generate an acceleration model sequence modeling output. An acceleration model sequence modeling output may refer to a data object that describes the output of the acceleration model sequence modeling layer.</p><p id="p-0043" num="0042">The term &#x201c;hybrid-input prediction data object&#x201d; may refer to a data object that describes the output of two or more input data objects (e.g., an audio-based feature data object and an acceleration-based feature data object) generated (e.g., combined) by a feature synthesis machine learning model. An example hybrid-input prediction data object may describe a likelihood of an event across one or more time intervals, a frequency of an event across one or more time intervals or a duration of an event across one or more time intervals. For example, a hybrid-input prediction data object may describe a cough likelihood, a cough frequency and/or a cough duration for a monitored individual across one or more time intervals.</p><p id="p-0044" num="0043">The term &#x201c;target dimension&#x201d; may refer to a desired length of a dimension of a data object, such as an acceleration-based feature data object and/or an audio-based feature data object. In order to perform particular steps/operations (e.g., concatenate) on two or more data objects (e.g., matrices), at least one dimension of each data object must be equal in length. For instance, in order to perform concatenation and/or addition steps/operations on two data objects of different dimensions, at least one dimension of the first data object must be adjusted to match the corresponding dimension of the second data object. An audio feature object target dimension length may refer to a desired dimension length for one of the dimensions of an audio-based feature data object. An acceleration feature object target dimension length may refer to a desired dimension length for one of the dimensions of an acceleration-based feature data object.</p><p id="p-0045" num="0044">The term &#x201c;acceleration model dimension adjustment layer&#x201d; may refer to a data object that describes a layer of an acceleration processing machine learning model that is configured to perform a plurality of steps/operations with respect to an input data object (e.g., an acceleration model up-sampling output), where the plurality of steps/operations are configured to adjust the dimension of an intermediate output of the acceleration processing machine learning in accordance with a target dimension for the intermediate output. An example acceleration model dimension adjustment layer may adjust a dimension of the acceleration model up-sampling output to satisfy a target dimension criteria in accordance with an acceleration feature object target dimension. The acceleration model dimension adjustment layer may comprise one or more layers configured to extract sequential information/understanding (e.g., recognize patterns) in historical data contained in the preceding machine learning model(s) and/or layers. The acceleration model dimension adjustment layer may be configured to modify the sampling rate applied to an input dataset such that one or more dimensions of the corresponding output are lengthened or truncated as desired to the target dimension. An acceleration model dimension adjustment output may refer to a data object that describes the output of an acceleration model dimension adjustment layer of an acceleration processing machine learning model.</p><p id="p-0046" num="0045">The term &#x201c;model up-sampling layer&#x201d; may refer to a data object that describes a layer of the acceleration dimension adjustment layer that is configured to perform an up-sampling step/operation on an acceleration model sequence modeling output to generate a second acceleration model up-sampling output. An up-sampling output may refer to a data object that describes the output of a model up-sampling layer. An acceleration model dimension truncation layer may refer to a layer of the dimension adjustment layer that is configured to process the second acceleration model up-sampling output to generate the acceleration model dimension adjustment output.</p><p id="p-0047" num="0046">The term &#x201c;concatenation layer&#x201d; may refer to a data object that describes a layer of the feature synthesis machine learning model that is configured to combine (e.g., merge, concatenate, stack and/or the like) two or more data objects (e.g., matrices) to generate an output describing the data in the two or more data objects, where the combination of the two or more data objects is intended to concentrate the values of the two or more data objects along a concatenation dimension (e.g., a horizontal dimension or a vertical dimension). For example, the concatenation layer may be configured to process the audio-based feature data object and the acceleration-based feature data object in order to generate a concatenated feature data object. Concatenation of the audio-based feature data object and the acceleration-based feature data object may include vector-by-vector steps/operations to extract new features from the feature data objects. A concatenated feature data object may refer to a data object that describes the output of the concatenation layer.</p><p id="p-0048" num="0047">The term &#x201c;synthesis model sequence modeling layer&#x201d; may refer to a data object that describes a layer of a feature synthesis machine learning model that is configured to perform one or more synthesis model sequence steps/operations with respect to an input data object (e.g., a concatenation feature data object) and generate a synthesis model sequence modeling output. For example, the synthesis model sequence modeling layer, may further modify (e.g., flatten, combine) an input data object to generate the synthesis model sequence modeling output. The synthesis model sequence modeling layer may comprise one or more Gated Recurrent Units (GRUs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) units and/or the like.</p><p id="p-0049" num="0048">The term &#x201c;synthesis model sequence modeling output&#x201d; may refer to a data object that describes the output of one or more synthesis model sequence modeling layers of a feature synthesis machine learning model. In some embodiments, a feature synthesis machine learning model comprising one or more synthesis model sequence modeling layer processes a concatenation layer output of a concatenation layer to generate a synthesis model sequence modeling output.</p><p id="p-0050" num="0049">The term &#x201c;time-distributed fully connected layer&#x201d; may refer to a data object that describes a fully-connected layer of a feature synthesis machine learning model that is configured to perform one or more prediction-based actions with respect to an input data object (e.g. a synthesis model sequence modeling output) and generate a hybrid-input prediction data object. An example of a time-distributed fully connected layer is the time-distributed Dense layer in the Keras application programming interface (API). The time-distributed fully connected layer may process an input data object (e.g., the synthesis model sequence modeling output) by extracting the most relevant feature for each time segment to generate a hybrid input prediction data object. The time-distributed fully connected layer may be configured to determine the likelihood of one or more events based at least in part on the hybrid-input prediction data object. The time-distributed fully connected layer may be configured to determine a frequency or duration corresponding with the one or more events across one or more time intervals. The hybrid input prediction data object may refer to a data object that describes the output of the time-distributed fully connected layer of the feature synthesis machine learning model. An example hybrid input prediction data object may comprise a single vector comprising a single relevant feature corresponding with each time segment.</p><heading id="h-0008" level="1">III. Computer Program Products, Methods, and Computing Entities</heading><p id="p-0051" num="0050">Embodiments of the present invention may be implemented in various ways, including as computer program products that comprise articles of manufacture. Such computer program products may include one or more software components including, for example, software objects, methods, data structures, or the like. A software component may be coded in any of a variety of programming languages. An illustrative programming language may be a lower-level programming language such as an assembly language associated with a particular hardware framework and/or operating system platform. A software component comprising assembly language instructions may require conversion into executable machine code by an assembler prior to execution by the hardware framework and/or platform. Another example programming language may be a higher-level programming language that may be portable across multiple frameworks. A software component comprising higher-level programming language instructions may require conversion to an intermediate representation by an interpreter or a compiler prior to execution.</p><p id="p-0052" num="0051">Other examples of programming languages include, but are not limited to, a macro language, a shell or command language, a job control language, a script language, a database query or search language, and/or a report writing language. In one or more example embodiments, a software component comprising instructions in one of the foregoing examples of programming languages may be executed directly by an operating system or other software component without having to be first transformed into another form. A software component may be stored as a file or other data storage construct. Software components of a similar type or functionally related may be stored together such as, for example, in a particular directory, folder, or library. Software components may be static (e.g., pre-established or fixed) or dynamic (e.g., created or modified at the time of execution).</p><p id="p-0053" num="0052">A computer program product may include non-transitory computer-readable storage medium storing applications, programs, program modules, scripts, source code, program code, object code, byte code, compiled code, interpreted code, machine code, executable instructions, and/or the like (also referred to herein as executable instructions, instructions for execution, computer program products, program code, and/or similar terms used herein interchangeably). Such non-transitory computer-readable storage media include all computer-readable media (including volatile and non-volatile media).</p><p id="p-0054" num="0053">In one embodiment, a non-volatile computer-readable storage medium may include a floppy disk, flexible disk, hard disk, solid-state storage (SSS) (e.g., a solid state drive (SSD), solid state card (SSC), solid state module (SSM), enterprise flash drive, magnetic tape, or any other non-transitory magnetic medium, and/or the like. A non-volatile computer-readable storage medium may also include a punch card, paper tape, optical mark sheet (or any other physical medium with patterns of holes or other optically recognizable indicia), compact disc read only memory (CD-ROM), compact disc-rewritable (CD-RW), digital versatile disc (DVD), Blu-ray disc (BD), any other non-transitory optical medium, and/or the like. Such a non-volatile computer-readable storage medium may also include read-only memory (ROM), programmable read-only memory (PROM), erasable programmable read-only memory (EPROM), electrically erasable programmable read-only memory (EEPROM), flash memory (e.g., Serial, NAND, NOR, and/or the like), multimedia memory cards (MMC), secure digital (SD) memory cards, SmartMedia cards, CompactFlash (CF) cards, Memory Sticks, and/or the like. Further, a non-volatile computer-readable storage medium may also include conductive-bridging random access memory (CBRAM), phase-change random access memory (PRAM), ferroelectric random-access memory (FeRAM), non-volatile random-access memory (NVRAM), magnetoresistive random-access memory (MRAM), resistive random-access memory (RRAM), Silicon-Oxide-Nitride-Oxide-Silicon memory (SONOS), floating junction gate random access memory (FJG RAM), Millipede memory, racetrack memory, and/or the like.</p><p id="p-0055" num="0054">In one embodiment, a volatile computer-readable storage medium may include random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), fast page mode dynamic random access memory (FPM DRAM), extended data-out dynamic random access memory (EDO DRAM), synchronous dynamic random access memory (SDRAM), double data rate synchronous dynamic random access memory (DDR SDRAM), double data rate type two synchronous dynamic random access memory (DDR2 SDRAM), double data rate type three synchronous dynamic random access memory (DDR3 SDRAM), Rambus dynamic random access memory (RDRAM), Twin Transistor RAM (TTRAM), Thyristor RAM (T-RAM), Zero-capacitor (Z-RAM), Rambus in-line memory module (RIMM), dual in-line memory module (DIMM), single in-line memory module (SIMM), video random access memory (VRAM), cache memory (including various levels), flash memory, register memory, and/or the like. It will be appreciated that where embodiments are described to use a computer-readable storage medium, other types of computer-readable storage media may be substituted for or used in addition to the computer-readable storage media described above.</p><p id="p-0056" num="0055">As should be appreciated, various embodiments of the present invention may also be implemented as methods, apparatuses, systems, computing devices, computing entities, and/or the like. As such, embodiments of the present invention may take the form of an apparatus, system, computing device, computing entity, and/or the like executing instructions stored on a computer-readable storage medium to perform certain steps or operations. Thus, embodiments of the present invention may also take the form of an entirely hardware embodiment, an entirely computer program product embodiment, and/or an embodiment that comprises combination of computer program products and hardware performing certain steps or operations.</p><p id="p-0057" num="0056">Embodiments of the present invention are described below with reference to block diagrams and flowchart illustrations. Thus, it should be understood that each block of the block diagrams and flowchart illustrations may be implemented in the form of a computer program product, an entirely hardware embodiment, a combination of hardware and computer program products, and/or apparatuses, systems, computing devices, computing entities, and/or the like carrying out instructions, steps or operations, and similar words used interchangeably (e.g., the executable instructions, instructions for execution, program code, and/or the like) on a computer-readable storage medium for execution. For example, retrieval, loading, and execution of code may be performed sequentially such that one instruction is retrieved, loaded, and executed at a time. In some exemplary embodiments, retrieval, loading, and/or execution may be performed in parallel such that multiple instructions are retrieved, loaded, and/or executed together. Thus, such embodiments can produce specifically-configured machines performing the steps or operations specified in the block diagrams and flowchart illustrations. Accordingly, the block diagrams and flowchart illustrations support various combinations of embodiments for performing the specified instructions, steps or operations.</p><heading id="h-0009" level="1">IV. Exemplary System Framework</heading><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram of an example architecture <b>100</b> for performing hybrid-input prediction steps/operations. The architecture <b>100</b> includes a hybrid-input prediction system <b>101</b> configured to receive data from the client computing entities <b>102</b>, process the data to generate outputs (e.g., hybrid-input predictive data objects) and provide the outputs to the client computing entities <b>102</b> for generating corresponding alerts (e.g., for providing and/or updating a user interface). In some embodiments, hybrid-input prediction system <b>101</b> may communicate with at least one of the client computing entities <b>102</b> using one or more communication networks. Examples of communication networks include any wired or wireless communication network including, for example, a wired or wireless local area network (LAN), personal area network (PAN), metropolitan area network (MAN), wide area network (WAN), or the like, as well as any hardware, software and/or firmware required to implement it (such as, e.g., network routers, and/or the like).</p><p id="p-0059" num="0058">The hybrid-input prediction system <b>101</b> may include a hybrid-input predictive computing entity <b>106</b>, and a storage subsystem <b>108</b>. The hybrid-input predictive computing entity <b>106</b> may be configured to receive requests and/or data from client computing entities <b>102</b>, process the requests and/or data to generate predictive outputs (e.g., hybrid-input prediction data objects), and provide the predictive outputs to the client computing entities <b>102</b>. The client computing entities <b>102</b> may be triggered to transmit requests to the hybrid-input predictive computing entity <b>106</b> in response to events which satisfy certain parameters (e.g., monitored events). Responsive to receiving the predictive outputs, the client computing entities <b>102</b> may generate corresponding alerts and may provide (e.g., transmit, send and/or the like) corresponding user interface data for presentation to user computing entities.</p><p id="p-0060" num="0059">The storage subsystem <b>108</b> may be configured to store at least a portion of the data utilized by the hybrid-input predictive computing entity <b>106</b> to perform hybrid-input prediction steps/operations and tasks. The storage subsystem <b>108</b> may be configured to store at least a portion of operational data and/or operational configuration data including operational instructions and parameters utilized by the hybrid-input predictive computing entity <b>106</b> to perform hybrid-input prediction steps/operations in response to requests. The storage subsystem <b>108</b> may include one or more storage units, such as multiple distributed storage units that are connected through a computer network. Each storage unit in the storage subsystem <b>108</b> may store at least one of one or more data assets and/or one or more data about the computed properties of one or more data assets. Moreover, each storage unit in the storage subsystem <b>108</b> may include one or more non-volatile storage or memory media including but not limited to hard disks, ROM, PROM, EPROM, EEPROM, flash memory, MMCs, SD memory cards, Memory Sticks, CBRAM, PRAM, FeRAM, NVRAM, MRAM, RRAM, SONOS, FJG RAM, Millipede memory, racetrack memory, and/or the like.</p><heading id="h-0010" level="1">Exemplary Hybrid-Input Predictive Computing Entity</heading><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>2</b></figref> provides a schematic of a hybrid-input predictive computing entity <b>106</b> according to one embodiment of the present invention. In general, the terms computing entity, computer, entity, device, system, and/or similar words used herein interchangeably may refer to, for example, one or more computers, computing entities, desktops, mobile phones, tablets, phablets, notebooks, laptops, distributed systems, kiosks, input terminals, servers or server networks, blades, gateways, switches, processing devices, processing entities, set-top boxes, relays, routers, network access points, base stations, the like, and/or any combination of devices or entities adapted to perform the functions, steps/operations, and/or processes described herein. Such functions, steps/operations, and/or processes may include, for example, transmitting, receiving, operating on, processing, displaying, storing, determining, creating/generating, monitoring, evaluating, comparing, and/or similar terms used herein interchangeably. In one embodiment, these functions, steps/operations, and/or processes can be performed on data, content, information, and/or similar terms used herein interchangeably.</p><p id="p-0062" num="0061">As indicated, in one embodiment, the hybrid-input predictive computing entity <b>106</b> may also include one or more network interfaces <b>220</b> for communicating with various computing entities, such as by communicating data, content, information, and/or similar terms used herein interchangeably that can be transmitted, received, operated on, processed, displayed, stored, and/or the like.</p><p id="p-0063" num="0062">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, in one embodiment, the hybrid-input predictive computing entity <b>106</b> may include or be in communication with one or more processing elements <b>205</b> (also referred to as processors, processing circuitry, and/or similar terms used herein interchangeably) that communicate with other elements within the hybrid-input predictive computing entity <b>106</b> via a bus, for example. As will be understood, the processing element <b>205</b> may be embodied in a number of different ways.</p><p id="p-0064" num="0063">For example, the processing element <b>205</b> may be embodied as one or more complex programmable logic devices (CPLDs), microprocessors, multi-core processors, coprocessing entities, application-specific instruction-set processors (ASIPs), microcontrollers, and/or controllers. Further, the processing element <b>205</b> may be embodied as one or more other processing devices or circuitry. The term circuitry may refer to an entirely hardware embodiment or a combination of hardware and computer program products. Thus, the processing element <b>205</b> may be embodied as integrated circuits, application specific integrated circuits (ASICs), field programmable gate arrays (FPGAs), programmable logic arrays (PLAs), hardware accelerators, other circuitry, and/or the like.</p><p id="p-0065" num="0064">As will therefore be understood, the processing element <b>205</b> may be configured for a particular use or configured to execute instructions stored in volatile or non-volatile media or otherwise accessible to the processing element <b>205</b>. As such, whether configured by hardware or computer program products, or by a combination thereof, the processing element <b>205</b> may be capable of performing steps or operations according to embodiments of the present invention when configured accordingly.</p><p id="p-0066" num="0065">In one embodiment, the hybrid-input predictive computing entity <b>106</b> may further include or be in communication with non-volatile media (also referred to as non-volatile storage, memory, memory storage, memory circuitry and/or similar terms used herein interchangeably). In one embodiment, the non-volatile storage or memory may include one or more non-volatile storage or memory media <b>210</b>, including but not limited to hard disks, ROM, PROM, EPROM, EEPROM, flash memory, MMCs, SD memory cards, Memory Sticks, CBRAM, PRAM, FeRAM, NVRAM, MRAM, RRAM, SONOS, FJG RAM, Millipede memory, racetrack memory, and/or the like.</p><p id="p-0067" num="0066">As will be recognized, the non-volatile storage or memory media may store databases, database instances, database management systems, data, applications, programs, program modules, scripts, source code, object code, byte code, compiled code, interpreted code, machine code, executable instructions, and/or the like. The term database, database instance, database management system, and/or similar terms used herein interchangeably may refer to a collection of records or data that is stored in a computer-readable storage medium using one or more database models, such as a hierarchical database model, network model, relational model, entity&#x2014;relationship model, object model, document model, semantic model, graph model, and/or the like.</p><p id="p-0068" num="0067">In one embodiment, the hybrid-input predictive computing entity <b>106</b> may further include or be in communication with volatile media (also referred to as volatile storage, memory, memory storage, memory circuitry and/or similar terms used herein interchangeably). In one embodiment, the volatile storage or memory may also include one or more volatile storage or memory media <b>215</b>, including but not limited to RAM, DRAM, SRAM, FPM DRAM, EDO DRAM, SDRAM, DDR SDRAM, DDR2 SDRAM, DDR3 SDRAM, RDRAM, TTRAM, T-RAM, Z-RAM, RIMM, DIMM, SIMM, VRAM, cache memory, register memory, and/or the like.</p><p id="p-0069" num="0068">As will be recognized, the volatile storage or memory media may be used to store at least portions of the databases, database instances, database management systems, data, applications, programs, program modules, scripts, source code, object code, byte code, compiled code, interpreted code, machine code, executable instructions, and/or the like being executed by, for example, the processing element <b>205</b>. Thus, the databases, database instances, database management systems, data, applications, programs, program modules, scripts, source code, object code, byte code, compiled code, interpreted code, machine code, executable instructions, and/or the like may be used to control certain aspects of the operation of the hybrid-input predictive computing entity <b>106</b> with the assistance of the processing element <b>205</b> and operating system.</p><p id="p-0070" num="0069">As indicated, in one embodiment, the hybrid-input predictive computing entity <b>106</b> may also include one or more network interfaces <b>220</b> for communicating with various computing entities, such as by communicating data, content, information, and/or similar terms used herein interchangeably that can be transmitted, received, operated on, processed, displayed, stored, and/or the like. Such communication may be executed using a wired data transmission protocol, such as fiber distributed data interface (FDDI), digital subscriber line (DSL), Ethernet, asynchronous transfer mode (ATM), frame relay, data over cable service interface specification (DOCSIS), or any other wired transmission protocol. Similarly, the hybrid-input predictive computing entity <b>106</b> may be configured to communicate via wireless client communication networks using any of a variety of protocols, such as general packet radio service (GPRS), Universal Mobile Telecommunications System (UMTS), Code Division Multiple Access 2000 (CDMA2000), CDMA2000 1&#xd7; (1&#xd7;RTT), Wideband Code Division Multiple Access (WCDMA), Global System for Mobile Communications (GSM), Enhanced Data rates for GSM Evolution (EDGE), Time Division-Synchronous Code Division Multiple Access (TD-SCDMA), Long Term Evolution (LTE), Evolved Universal Terrestrial Radio Access Network (E-UTRAN), Evolution-Data Optimized (EVDO), High Speed Packet Access (HSPA), High-Speed Downlink Packet Access (HSDPA), IEEE 802.11 (Wi-Fi), Wi-Fi Direct, 802.16 (WiMAX), ultra-wideband (UWB), infrared (IR) protocols, near field communication (NFC) protocols, Wibree, Bluetooth protocols, wireless universal serial bus (USB) protocols, and/or any other wireless protocol.</p><p id="p-0071" num="0070">Although not shown, the hybrid-input predictive computing entity <b>106</b> may include or be in communication with one or more input elements, such as a keyboard input, a mouse input, a touch screen/display input, motion input, movement input, audio input, pointing device input, joystick input, keypad input, and/or the like. The hybrid-input predictive computing entity <b>106</b> may also include or be in communication with one or more output elements (not shown), such as audio output, video output, screen/display output, motion output, movement output, and/or the like.</p><heading id="h-0011" level="1">Exemplary Client Computing Entity</heading><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>3</b></figref> provides an illustrative schematic representative of a client computing entity <b>102</b> that can be used in conjunction with embodiments of the present invention. In general, the terms device, system, computing entity, entity, and/or similar words used herein interchangeably may refer to, for example, one or more computers, computing entities, desktops, mobile phones, tablets, phablets, notebooks, laptops, distributed systems, kiosks, input terminals, servers or server networks, blades, gateways, switches, processing devices, processing entities, set-top boxes, relays, routers, network access points, base stations, the like, and/or any combination of devices or entities adapted to perform the functions, steps/operations, and/or processes described herein. Client computing entities <b>102</b> can be operated by various parties. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the client computing entity <b>102</b> can include an antenna <b>312</b>, a transmitter <b>304</b> (e.g., radio), a receiver <b>306</b> (e.g., radio), and a processing element <b>308</b> (e.g., CPLDs, microprocessors, multi-core processors, coprocessing entities, ASIPs, microcontrollers, and/or controllers) that provides signals to and receives signals from the transmitter <b>304</b> and receiver <b>306</b>, correspondingly.</p><p id="p-0073" num="0072">The signals provided to and received from the transmitter <b>304</b> and the receiver <b>306</b>, correspondingly, may include signaling information/data in accordance with air interface standards of applicable wireless systems. In this regard, the client computing entity <b>102</b> may be capable of operating with one or more air interface standards, communication protocols, modulation types, and access types. More particularly, the client computing entity <b>102</b> may operate in accordance with any of a number of wireless communication standards and protocols, such as those described above with regard to the hybrid-input predictive computing entity <b>106</b>. In a particular embodiment, the client computing entity <b>102</b> may operate in accordance with multiple wireless communication standards and protocols, such as UMTS, CDMA2000, 1&#xd7;RTT, WCDMA, GSM, EDGE, TD-SCDMA, LTE, E-UTRAN, EVDO, HSPA, HSDPA, Wi-Fi, Wi-Fi Direct, WiMAX, UWB, IR, NFC, Bluetooth, USB, and/or the like. Similarly, the client computing entity <b>102</b> may operate in accordance with multiple wired communication standards and protocols, such as those described above with regard to the hybrid-input predictive computing entity <b>106</b> via a network interface <b>320</b>.</p><p id="p-0074" num="0073">Via these communication standards and protocols, the client computing entity <b>102</b> can communicate with various other entities using concepts such as Unstructured Supplementary Service Data (USSD), Short Message Service (SMS), Multimedia Messaging Service (MMS), Dual-Tone Multi-Frequency Signaling (DTMF), and/or Subscriber Identity Module Dialer (SIM dialer). The client computing entity <b>102</b> can also download changes, add-ons, and updates, for instance, to its firmware, software (e.g., including executable instructions, applications, program modules), and operating system.</p><p id="p-0075" num="0074">According to one embodiment, the client computing entity <b>102</b> may include location determining aspects, devices, modules, functionalities, and/or similar words used herein interchangeably. For example, the client computing entity <b>102</b> may include outdoor positioning aspects, such as a location module adapted to acquire, for example, latitude, longitude, altitude, geocode, course, direction, heading, speed, universal time (UTC), date, and/or various other information/data. In one embodiment, the location module can acquire data, sometimes known as ephemeris data, by identifying the number of satellites in view and the relative positions of those satellites (e.g., using global positioning systems (GPS)). The satellites may be a variety of different satellites, including Low Earth Orbit (LEO) satellite systems, Department of Defense (DOD) satellite systems, the European Union Galileo positioning systems, the Chinese Compass navigation systems, Indian Regional Navigational satellite systems, and/or the like. This data can be collected using a variety of coordinate systems, such as the Decimal Degrees (DD); Degrees, Minutes, Seconds (DMS); Universal Transverse Mercator (UTM); Universal Polar Stereographic (UPS) coordinate systems; and/or the like. Alternatively, the location information/data can be determined by triangulating the client computing entity's <b>102</b> position in connection with a variety of other systems, including cellular towers, Wi-Fi access points, and/or the like. Similarly, the client computing entity <b>102</b> may include indoor positioning aspects, such as a location module adapted to acquire, for example, latitude, longitude, altitude, geocode, course, direction, heading, speed, time, date, and/or various other information/data. Some of the indoor systems may use various position or location technologies including RFID tags, indoor beacons or transmitters, Wi-Fi access points, cellular towers, nearby computing devices (e.g., smartphones, laptops) and/or the like. For instance, such technologies may include the iBeacons, Gimbal proximity beacons, Bluetooth Low Energy (BLE) transmitters, NFC transmitters, and/or the like. These indoor positioning aspects can be used in a variety of settings to determine the location of someone or something to within inches or centimeters.</p><p id="p-0076" num="0075">The client computing entity <b>102</b> may also comprise a user interface (that can include a display <b>316</b> coupled to a processing element <b>308</b>) and/or a user input interface (coupled to a processing element <b>308</b>). For example, the user interface may be a user application, browser, user interface, and/or similar words used herein interchangeably executing on and/or accessible via the client computing entity <b>102</b> to interact with and/or cause display of information/data from the hybrid-input predictive computing entity <b>106</b>, as described herein. The user input interface can comprise any of a number of devices or interfaces allowing the client computing entity <b>102</b> to receive data, such as a keypad <b>318</b> (hard or soft), a touch display, voice/speech or motion interfaces, or other input device. In embodiments including a keypad <b>318</b>, the keypad <b>318</b> can include (or cause display of) the conventional numeric (0-9) and related keys (#, *), and other keys used for operating the client computing entity <b>102</b> and may include a full set of alphabetic keys or set of keys that may be activated to provide a full set of alphanumeric keys. In addition to providing input, the user input interface can be used, for example, to activate or deactivate certain functions, such as screen savers and/or sleep modes.</p><p id="p-0077" num="0076">The client computing entity <b>102</b> can also include volatile storage or memory <b>322</b> and/or non-volatile storage or memory <b>324</b>, which can be embedded and/or may be removable. For example, the non-volatile memory may be ROM, PROM, EPROM, EEPROM, flash memory, MMCs, SD memory cards, Memory Sticks, CBRAM, PRAM, FeRAM, NVRAM, MRAM, RRAM, SONOS, FJG RAM, Millipede memory, racetrack memory, and/or the like. The volatile memory may be RAM, DRAM, SRAM, FPM DRAM, EDO DRAM, SDRAM, DDR SDRAM, DDR2 SDRAM, DDR3 SDRAM, RDRAM, TTRAM, T-RAM, Z-RAM, RIMM, DIMM, SIMM, VRAM, cache memory, register memory, and/or the like. The volatile and non-volatile storage or memory can store databases, database instances, database management systems, data, applications, programs, program modules, scripts, source code, object code, byte code, compiled code, interpreted code, machine code, executable instructions, and/or the like to implement the functions of the client computing entity <b>102</b>. As indicated, this may include a user application that is resident on the entity or accessible through a browser or other user interface for communicating with the hybrid-input predictive computing entity <b>106</b> and/or various other computing entities.</p><p id="p-0078" num="0077">In another embodiment, the client computing entity <b>102</b> may include one or more components or functionality that are the same or similar to those of the hybrid-input predictive computing entity <b>106</b>, as described in greater detail above. As will be recognized, these frameworks and descriptions are provided for exemplary purposes only and are not limiting to the various embodiments.</p><p id="p-0079" num="0078">In various embodiments, the client computing entity <b>102</b> may be embodied as an artificial intelligence (AI) computing entity, such as an Amazon Echo, Amazon Echo Dot, Amazon Show, Google Home, and/or the like. Accordingly, the client computing entity <b>102</b> may be configured to provide and/or receive information/data from a user via an input/output mechanism, such as a display, a camera, a speaker, a voice-activated input, and/or the like. In certain embodiments, an AI computing entity may comprise one or more predefined and executable program algorithms stored within an onboard memory storage module, and/or accessible over a network. In various embodiments, the AI computing entity may be configured to retrieve and/or execute one or more of the predefined program algorithms upon the occurrence of a predefined trigger event.</p><heading id="h-0012" level="1">V. Exemplary System Operations</heading><p id="p-0080" num="0079">Described herein are various techniques for hybrid-input prediction steps/operations on two or more dissimilar data objects. Some of the disclosed techniques may utilize one or more machine learning models to perform hybrid-input prediction steps/operations (e.g., convolutional steps/operations, up-sampling steps/operations and the like). Some of the described techniques utilize a particular configuration of machine learning models and/or layers. The output of a machine learning model and/or layers therein may be supplied as an input for subsequent steps/operations by another machine learning model and/or layer. However, a person of ordinary skill in the art will recognize that hybrid-input prediction steps/operations discussed herein may be performed using different combinations than the particular combinations described herein.</p><p id="p-0081" num="0080">By facilitating efficient hybrid-input prediction steps/operations, various embodiments of the present invention improve hybrid-input prediction steps/operations on two or more dissimilar data objects. Modifying data objects according to the methods disclosed herein reduces the need for additional processing power and in turn reduces processing latency. This in turn reduces the need for large amounts of data to provide accurate predictive outputs.</p><p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. <b>4</b></figref> provides a schematic representation of an example system <b>400</b> for performing hybrid-input prediction steps/operations and generating corresponding predictive outputs/user interface data. The hybrid-input prediction system <b>101</b> may receive data from one or more client computing entities <b>102</b> and store at least a portion of the data in the storage subsystem <b>108</b>. The storage subsystem <b>108</b> may provide acceleration and audio data objects <b>402</b>, <b>404</b> to the hybrid-input predictive computing entity <b>106</b>. The hybrid-input prediction system <b>101</b> may be configured to perform steps/operations that lead to generating hybrid-input prediction data objects <b>411</b> and user interface data. For example, the hybrid-input predictive computing entity <b>106</b> may comprise a plurality of machine learning models each configured to perform an associated set of hybrid-input prediction steps/operations. The hybrid-input prediction system <b>101</b> may process two or more input data objects (e.g., an acceleration data object <b>402</b>, and an audio data object <b>404</b>) and generate a predictive output (e.g., one or more hybrid-input prediction data objects <b>411</b>).</p><heading id="h-0013" level="1">Exemplary Acceleration Processing Machine Learning Model</heading><p id="p-0083" num="0082">As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the hybrid-input prediction system <b>101</b> comprises an acceleration processing machine learning model <b>401</b> configured to process an acceleration data object <b>402</b> and generate an acceleration-based feature data object <b>406</b>. The acceleration processing machine learning model <b>401</b> may refer to a machine learning model configured to perform a plurality of steps/operations with respect to an acceleration data object <b>402</b>. An acceleration data object <b>402</b> may refer to a data object that describes a set of information (e.g., an input dataset) corresponding to acceleration of motion of a body (e.g., a human) with respect to the body's frame (e.g., raw acceleration data). An example acceleration data object <b>402</b> may describe a body's change in orientation and/or movements (e.g., vibrations) in two or more directions (e.g., recorded cross-body acceleration data for a monitored individual). Raw acceleration data may be represented graphically as a waveform plotted with respect to time in which positive values indicate an increase in velocity, negative values indicate a decrease in velocity and zero/null values indicate a constant velocity. An acceleration data object <b>402</b> may describe such raw acceleration data in a time domain representation or the frequency domain representation by sampling the raw acceleration data at a given sampling rate (e.g., 50 Hz). Sampling raw acceleration data at a high sampling rate (e.g., 44.1 kHz) is undesirable due to a lack of complexity in the underlying data. An acceleration data object <b>402</b> describing raw acceleration data in the time domain may comprise a two-dimensional matrix having a length and a width in which a first dimension corresponds with a number of segments in time and a second dimension corresponds with a plurality of features associated with oscillations at different frequencies occurring at corresponding segments in time. An acceleration-based feature data object <b>406</b> may refer to a data object that is generated by processing an acceleration data object <b>402</b> using an acceleration processing machine learning model <b>401</b>. An example acceleration-based feature data object <b>406</b> may comprise a two-dimensional matrix having a length and a width in which a first dimension corresponds with a number of segments in time and a second dimension corresponds with a plurality of features associated with oscillations/vibrations occurring at the corresponding segments in time. The acceleration-based feature data object <b>406</b> may comprise dimensions that are different from the dimensions of the input data object (e.g., acceleration data object <b>402</b>) processed by the acceleration processing machine learning model <b>401</b>. For instance, the dimensions of the matrix of the acceleration-based feature data object <b>406</b> may be truncated in the first dimension (x-direction) and lengthened in the second dimension (y-direction) such that the acceleration-based feature data object <b>406</b> contains more information sampled over fewer segments in time.</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart diagram illustrating an example process for generating an acceleration-based feature data object <b>406</b> by an acceleration processing machine learning model <b>401</b>. The process depicted in <figref idref="DRAWINGS">FIG. <b>10</b></figref> begins at step/operation <b>1002</b>, when the acceleration processing machine learning model <b>401</b> processes the acceleration data object <b>402</b> to generate the acceleration model FFT output <b>512</b>. At step/operation <b>1004</b>, the acceleration processing machine learning model <b>401</b> processes the acceleration model FFT output <b>512</b> to generate an acceleration model convolutional output <b>514</b>. At step/operation <b>1006</b>, the acceleration processing machine learning model <b>401</b> processes the acceleration model convolutional output <b>514</b> to generate an acceleration model up-sampling output <b>516</b>. Then, at step/operation <b>1008</b>, the acceleration processing machine learning model <b>401</b> generates the acceleration-based feature data object <b>406</b> based at least in part on the acceleration model up-sampling output <b>516</b>.</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>5</b></figref> provides a schematic representation of an example system <b>500</b> for generating an acceleration-based feature data object <b>406</b> by an acceleration processing machine learning model <b>401</b>. The acceleration processing machine learning model <b>401</b> may comprise a plurality of layers configured to process the acceleration data object <b>402</b> and generate an acceleration-based feature data object <b>406</b>.</p><p id="p-0086" num="0085">As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref> the acceleration processing machine learning model <b>401</b> comprises an acceleration model FFT layer <b>501</b> configured to process an acceleration data object <b>402</b> using an FFT function and generate an acceleration model FFT output <b>512</b>. The acceleration model FFT layer <b>501</b> may refer to a layer of an acceleration processing machine learning model <b>401</b> configured to process an acceleration data object <b>402</b> using an FFT function and generate an acceleration model FFT output <b>512</b>. For example, the acceleration model FFT layer <b>501</b> may be configured to perform a plurality of steps/operations with respect to an acceleration data object <b>402</b> describing raw acceleration data in the time to domain to generate an acceleration data object <b>402</b> describing raw acceleration data in the frequency domain. The resulting acceleration model FFT output <b>512</b> may have dimensions that are different from the dimensions of the input data object (e.g., acceleration data object <b>402</b>). For example, the acceleration model FFT layer <b>501</b> may apply a mask/filter to the input data object (e.g., acceleration data object <b>402</b>) in order to extract relevant features and generate the acceleration model FFT output <b>512</b> describing such features. The acceleration model FFT output <b>512</b> may refer to the output generated by an acceleration model FFT layer <b>501</b> of an acceleration processing machine learning model <b>401</b>. The acceleration model FFT output <b>512</b> may refer to a data object describing a two-dimensional matrix having a length and a width in which a first dimension corresponds with a plurality of segments in time and a second dimension corresponds with a plurality of relevant features.</p><p id="p-0087" num="0086">As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the acceleration processing machine learning model <b>401</b> comprises an acceleration model one-dimensional convolutional layer <b>503</b> configured to perform one or more convolutional steps/operations with respect to an acceleration model FFT output <b>512</b> and generate an acceleration model convolutional output <b>514</b>. An acceleration model one-dimensional convolutional layer <b>503</b> may refer to a layer of a machine learning model configured to perform one or more convolutional steps/operations with respect to an input data object (e.g., an acceleration model FFT output <b>512</b>) and generate an acceleration model convolutional output <b>514</b>. The acceleration model one-dimensional convolutional layer <b>503</b> is configured to extract feature data from an acceleration model FFT output <b>512</b>. For example, the acceleration model one-dimensional convolutional layer <b>503</b> may extract feature data from an acceleration model FFT output <b>512</b> in order to generate an acceleration model convolutional output <b>514</b> with different dimensions from the acceleration model FFT output <b>512</b>. An acceleration model convolutional output <b>514</b> may refer to a data object that describes the output generated by an acceleration model one-dimensional convolutional layer <b>503</b> of an acceleration processing machine learning model <b>401</b>.</p><p id="p-0088" num="0087">As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the acceleration processing machine learning model <b>401</b> comprises an acceleration model up-sampling layer <b>505</b> configured to perform one or more up-sampling steps/operations with respect to an acceleration model convolutional output <b>514</b> and generate an acceleration model up-sampling output <b>516</b>. The acceleration model up-sampling layer <b>505</b> may refer to a layer of an acceleration processing machine learning model <b>401</b> configured to perform one or more up-sampling steps/operations with respect to an input data object (e.g., acceleration model convolutional output <b>514</b>) in order to generate an acceleration model up-sampling output <b>516</b>. The acceleration model up-sampling layer <b>505</b> may be configured to transform (e.g., lengthen or truncate) at least one dimension of the input data object (e.g., acceleration model convolutional output <b>514</b>). For example, by increasing the length of at least one dimension (e.g., number of time segments) corresponding with the input data object. The acceleration processing machine learning model <b>401</b> may comprise a plurality of acceleration model up-sampling layers <b>505</b>, each configured to transform a particular dimension of a respective input data object. An acceleration model up-sampling output <b>516</b> may refer to a data object describing the output generated by the acceleration model up-sampling layer <b>505</b> of an acceleration processing machine learning model <b>401</b>. As further illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the acceleration processing machine learning model <b>401</b> comprises an acceleration model dimension adjustment layer <b>507</b> configured to perform one or more dimension adjustment steps/operations with respect to an acceleration model up-sampling output <b>516</b> and generate an acceleration model dimension adjustment output <b>518</b>.</p><p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. <b>8</b></figref> provides a schematic <b>800</b> illustrating an exemplary acceleration model dimension adjustment layer <b>507</b>. The acceleration model dimension adjustment layer <b>507</b> may comprise one or more layers of the acceleration processing machine learning model <b>401</b> configured to process the acceleration model up-sampling output <b>516</b> and generate the acceleration model dimension adjustment output <b>518</b>. The acceleration model dimension adjustment layer <b>507</b> may refer to one or more layers of an acceleration processing machine learning model <b>401</b> configured to perform a plurality of up-sampling steps/operations with respect to an input data object (e.g., an acceleration model up-sampling output <b>516</b>). The plurality of steps/operations are configured to adjust the dimension of an intermediate output of the acceleration processing machine learning model <b>401</b> in accordance with a target dimension for the intermediate output. An example acceleration model dimension adjustment layer <b>507</b> may adjust a dimension of the acceleration model up-sampling output to satisfy target dimension criteria in accordance with an acceleration-based feature object target dimension. The target dimension may refer to a desired length of a dimension of a data object. In order to perform particular steps/operations (e.g., concatenate) on two or more data objects (e.g., matrices), at least one dimension of each data object must be equal in length. For instance, in order to perform steps/operations on two data objects of different dimensions, at least one dimension of the first data object must be adjusted to match the corresponding dimension of the second data object. An audio feature object target dimension length may refer to a desired dimension length for one of the dimensions of an audio-based feature data object. An acceleration feature object target dimension length may refer to a desired dimension length for one of the dimensions of an acceleration-based feature data object. The acceleration model dimension adjustment layer <b>507</b> may comprise one or more layers configured to extract sequential information/understanding (e.g., recognize patterns) in historical data contained in the preceding machine learning model(s) and/or layers. The acceleration model dimension adjustment layer <b>507</b> may be configured to modify the sampling rate applied to an input dataset such that one or more dimensions of the corresponding output are lengthened or truncated as desired to the target dimension. An acceleration model dimension adjustment output <b>518</b> may refer to a data object that describes the output of an acceleration model dimension adjustment layer <b>507</b> of an acceleration processing machine learning model <b>401</b>.</p><p id="p-0090" num="0089">As illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the acceleration model dimension adjustment layer <b>507</b> comprises an acceleration model sequence modeling layer <b>801</b> configured to process an acceleration model up-sampling output <b>516</b> to generate an acceleration model sequence modeling output <b>812</b>. The acceleration model sequence modeling layer <b>801</b> may refer to a layer of an acceleration model dimension adjustment layer <b>507</b> of an acceleration processing machine learning model <b>401</b> configured to process an acceleration model up-sampling output <b>516</b> to generate an acceleration model sequence modeling output. An acceleration model sequence modeling output <b>812</b> may refer to a data object that describes the output of the acceleration model sequence modeling layer <b>801</b>. As further illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the acceleration model dimension adjustment layer <b>507</b> comprises a second acceleration up-sampling layer <b>803</b> configured to process the acceleration model sequence modeling output <b>812</b> to generate a second acceleration model up-sampling output <b>814</b>. While various embodiments of the present invention disclose using two up-sampling layers by an acceleration model dimension adjustment layer, a person of ordinary skill in the relevant technology will recognize that an acceleration model dimension adjustment layer may comprise any number of up-sampling layers. As further illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the acceleration model dimension adjustment layer <b>507</b> comprises an acceleration model dimension truncation layer <b>805</b> configured to process the second acceleration model up-sampling output <b>814</b> to generate the acceleration model dimension adjustment output <b>518</b>.</p><p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart diagram illustrating an example process for generating the acceleration model dimension adjustment output <b>518</b> by the acceleration model dimension adjustment layer <b>507</b>. The process depicted in <figref idref="DRAWINGS">FIG. <b>12</b></figref> begins at step/operation <b>1202</b>, when the acceleration model sequence modeling layer <b>801</b> of the acceleration model dimension adjustment layer <b>507</b> processes the acceleration model up-sampling output by adjusting a target dimension in accordance with the acceleration feature object target dimension to generate the acceleration model sequence modeling output <b>812</b>. At step/operation <b>1204</b>, the second acceleration up-sampling layer <b>803</b> of the acceleration model dimension adjustment layer <b>507</b> processes the acceleration modeling sequence modeling output to generate a second acceleration model up-sampling output <b>814</b>. At step/operation <b>1206</b>, the acceleration model dimension truncation layer <b>805</b> or the acceleration model dimension adjustment layer <b>507</b> processes the second acceleration model up-sampling layer to generate the acceleration model dimension adjustment output <b>518</b>.</p><heading id="h-0014" level="1">Exemplary Audio Processing Machine Learning Model</heading><p id="p-0092" num="0091">Returning to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the hybrid-input prediction system <b>101</b> comprises an audio processing machine learning model <b>403</b> configured to process an audio data object <b>404</b> and generate an audio-based feature data object <b>408</b>. The audio processing machine learning model <b>403</b> may refer to a machine learning model configured to perform a plurality of steps/operations with respect to an audio data object <b>404</b>. An audio data object <b>404</b> may refer to may refer to a data object that describes a set of information (e.g., an input dataset) corresponding to sound such as raw audio data (e.g., recorded audio data for a monitored individual). Raw audio data may comprise one or more sound waves. Each sound wave comprises a wavelength oscillating at a given frequency for a duration of time. An audio data object <b>404</b> may describe such raw audio data in a time domain representation or the frequency domain representation. Raw audio data may be sampled at a given sampling rate (e.g., 44.1 kHz) to generate a time domain representation. In general, a data object generated based at least in part on a high sampling rate will generate a data object with more information/data than a data object generated based at least in part on a lower sampling rate. An example audio data object <b>404</b> may be represented graphically by plotting extracted values/features as a function of time. An audio data object <b>404</b> describing raw audio data in the time domain may comprise a two-dimensional matrix in which a first dimension corresponds with a number of segments in time and a second dimension corresponds with a plurality of values/features based at least in part on the frequencies of the sound waves occurring at corresponding segments in time. An audio-based feature data object <b>408</b> may refer to a data object that is generated by an audio processing machine learning model <b>403</b>. An example audio-based feature data object <b>408</b> may comprise a two-dimensional matrix having a length and a width in which a first dimension corresponds with a number of segments in time and a second dimension corresponds with a plurality of features associated with frequencies of sound waves occurring at the corresponding segments in time. The audio-based feature data object <b>408</b> may comprise dimensions that are different from the dimensions of the input data object (e.g., audio data object <b>404</b>) processed by the audio processing machine learning model <b>403</b>. For instance, the dimensions of the matrix of the audio-based feature data object <b>408</b> may be truncated in the first dimension (x-direction) and lengthened in the second dimension (y-direction) such that the audio-based feature data object <b>408</b> contains more information sampled over fewer segments in time.</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. <b>6</b></figref> a schematic representation of an example system <b>600</b> for generating an audio-based feature data object <b>408</b> by an audio processing machine learning model <b>403</b>. The audio processing machine learning model <b>403</b> may comprise a plurality of layers configured to process the audio data object <b>404</b> and generate an audio-based feature data object <b>408</b>. The audio processing machine learning model <b>403</b> may refer to a machine learning model configured to perform a plurality of steps/operations with respect to an audio data object <b>404</b> in order to generate an audio-based feature data object <b>408</b>. For example, the audio processing machine learning model <b>403</b> may comprise a plurality of layers each configured to perform one or more steps/operations with respect to an input data object (e.g., audio data object <b>404</b>). Each layer of the audio processing machine learning model <b>403</b> may be configured to perform a plurality of steps/operations to modify one or more dimensions of a corresponding input (e.g., audio data object <b>404</b>). An example audio processing machine learning model <b>403</b> may comprise at least one audio model fast Fourier transform layer <b>601</b>, and at least one audio model one-dimensional convolutional layer <b>603</b>.</p><p id="p-0094" num="0093">As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the audio processing machine learning model <b>403</b> comprises an audio model fast Fourier transform (FFT) layer <b>601</b> configured to process an audio data object <b>404</b> using an FFT function and generate an audio model FFT output <b>612</b>. The audio model FFT layer <b>601</b> is configured to perform a plurality of steps/operations with respect to an audio data object <b>404</b> describing raw audio data in the time domain in order to generate an audio data object describing raw audio data in the frequency domain. The resulting audio model FFT output <b>612</b> may have dimensions that are different from the dimensions of the input data object (e.g., audio data object <b>404</b>). For example, the audio model FFT layer <b>601</b> may apply a mask/filter to the input data object (e.g., audio data object <b>404</b>) in order to extract relevant features and generate the audio model FFT output <b>612</b> describing such features. The audio model FFT output <b>612</b> may refer to the output generated by an audio model FFT layer <b>601</b> of an audio processing machine learning model <b>403</b>. The audio model FFT output <b>612</b> may refer to a data object describing a two-dimensional matrix having a length and a width in which a first dimension corresponds with a plurality of segments in time and a second dimension corresponds with a plurality of relevant features.</p><p id="p-0095" num="0094">As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the audio processing machine learning model <b>403</b> comprises an audio model one-dimensional convolutional layer <b>603</b> configured to perform one or more convolutional steps/operations with respect to an audio model FFT output <b>612</b> and generate an audio model convolutional output <b>614</b>. The audio model one-dimensional convolutional layer <b>603</b> is configured to perform one or more convolutional steps/operations with respect to an audio model FFT output <b>612</b> and generate an audio model convolutional output <b>614</b>. The audio model one-dimensional convolutional layer <b>603</b> is configured to extract feature data from the audio model FFT output <b>612</b>. For example, the audio model one-dimensional convolutional layer <b>603</b> may extract feature data from the audio model FFT output <b>612</b> in order to generate an audio-based feature data object <b>408</b> with different dimensions from the audio model FFT output <b>612</b>. An audio model convolutional output <b>614</b> may refer to the output or intermediary output generated by an audio model one-dimensional convolutional layer <b>603</b> of an audio processing machine learning model <b>403</b>.</p><p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart diagram illustrating an example process for generating an audio-based feature data object <b>408</b> by an audio processing machine learning model <b>403</b>. The process depicted in <figref idref="DRAWINGS">FIG. <b>9</b></figref> begins at step/operation <b>902</b>, when the audio model FFT layer <b>601</b> of the audio processing machine learning model <b>403</b> processes the audio data object <b>404</b> to generate an audio model FFT output <b>612</b>. At step/operation <b>904</b>, the audio model one-dimensional convolutional layer <b>603</b> or the audio processing machine learning model <b>403</b> processes the audio model FFT output <b>612</b> to generate an audio model convolutional output <b>614</b>. At step/operation <b>906</b>, the audio processing machine learning model generates an audio-based feature data object <b>408</b> based at least in part on the audio model convolutional output <b>614</b>.</p><heading id="h-0015" level="1">Exemplary Feature Synthesis Machine Learning Model</heading><p id="p-0097" num="0096">As further illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the hybrid-input prediction system <b>101</b> comprises a feature synthesis machine learning model <b>405</b>. The feature synthesis machine learning model <b>405</b> may receive, as input, the acceleration-based feature data object <b>406</b> and the audio-based feature data object <b>408</b>. The feature synthesis machine learning model <b>405</b> may process the acceleration-based feature data object <b>406</b> and the audio-based feature data object <b>408</b> and generate a hybrid-input prediction data object <b>411</b>. The feature synthesis machine learning model <b>405</b> may refer to a machine learning model configured to process two or more input data objects (e.g., an audio-based feature data object <b>408</b> and an acceleration-based feature data object <b>406</b>) in order to generate an input that integrates features described by both of the two or more input data objects. For example, the feature synthesis machine learning model <b>405</b> may be configured to perform a plurality of steps/operations with respect to an audio-based feature data object <b>408</b> and an acceleration-based feature data object <b>406</b> in order to generate a hybrid-input prediction data object <b>411</b>. The feature synthesis machine learning model <b>405</b> may comprise one or more layers configured to extract sequential information/understanding (e.g., recognize patterns) in historical data contained in the preceding machine learning model(s) and/or layers. A hybrid-input prediction data object <b>411</b> may refer to a data object that describes the output of two or more input data objects (e.g., an audio-based feature data object <b>408</b> and an acceleration-based feature data object <b>406</b>) generated (e.g., merged, combined and/or the like) by a feature synthesis machine learning model <b>405</b>.</p><p id="p-0098" num="0097"><figref idref="DRAWINGS">FIG. <b>7</b></figref> provides an example system <b>700</b> for generating a hybrid-input prediction data object <b>411</b> by a feature synthesis machine learning model <b>405</b>. As shown, the feature synthesis machine learning model <b>405</b> comprises a plurality of layers configured to process two input data objects (e.g., the audio-based feature data object <b>408</b> and the acceleration-based feature data object <b>406</b>) in order to generate an output (e.g., hybrid-input prediction data object <b>411</b>) that integrates features described by both of the input data objects.</p><p id="p-0099" num="0098">As illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the feature synthesis machine learning model <b>405</b> comprises a concatenation layer <b>701</b> configured to process an acceleration-based feature data object <b>406</b> and an audio-based feature data object <b>408</b> to generate a concatenation feature data object <b>712</b>. The concatenation layer <b>701</b> may refer to a layer of the feature synthesis machine learning model <b>405</b> that is configured to combine (e.g., merge, stack and/or the like) two or more data objects (e.g., matrices) to generate an output describing features in the two or more data objects, where the combination of the two or more data objects is intended to concentrate the values of the two or more data objects along a concatenation dimension (e.g., a horizontal dimension or a vertical dimension). For example, as shown, the concatenation layer <b>701</b> is configured to process the audio-based feature data object <b>408</b> and the acceleration-based feature data object <b>406</b> in order to generate a concatenation feature data object <b>712</b>. Concatenation of the audio-based feature data object <b>408</b> and the acceleration-based feature data object <b>406</b> may include vector-by-vector steps/operations to extract new features from the respective data objects. A concatenation feature data object <b>712</b> may refer to a data object that describes the output of the concatenation layer <b>701</b>.</p><p id="p-0100" num="0099">As illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the feature synthesis machine learning model <b>405</b> comprises a synthesis model sequence modeling layer <b>703</b>. The synthesis model sequence modeling layer <b>703</b> may refer to a layer of a feature synthesis machine learning model <b>405</b> that is configured to perform one or more synthesis model sequence steps/operations with respect to an input data object (e.g., a concatenation feature data object <b>712</b>) and generate a synthesis model sequence modeling output <b>714</b>. For example, the synthesis model sequence modeling layer <b>703</b>, may further modify (e.g., flatten, combine) the input data object to generate the synthesis model sequence modeling output <b>714</b>. The synthesis model sequence modeling layer <b>703</b> may comprise one or more gated recurrent units (GRUs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) units and/or the like. The synthesis model sequence modeling output <b>714</b> may refer to a data object that describes the output of one or more synthesis model sequence modeling layers <b>703</b> of a feature synthesis machine learning model <b>405</b>.</p><p id="p-0101" num="0100">As illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the feature synthesis machine learning model <b>405</b> comprises a time-distributed fully connected layer <b>705</b> configured to process a synthesis model sequence modeling output <b>714</b> to generate a hybrid-input prediction data object <b>411</b>. The time-distributed fully connected layer <b>705</b> may refer to a fully-connected layer of a feature synthesis machine learning model <b>405</b> that is configured to perform one or more prediction-based actions with respect to an input data object (e.g., a synthesis model sequence modeling output <b>714</b>) and generate the hybrid-input prediction data object <b>411</b>. An example of a time-distributed fully connected layer is the time-distributed Dense layer in the Keras application programming interface (API). The time-distributed fully connected layer may process an input data object (e.g., the synthesis model sequence modeling output <b>714</b>) by extracting the most relevant feature for each time segment to generate a hybrid input prediction data object. The time-distributed fully connected layer <b>705</b> may be configured to determine the likelihood of one or more events based at least in part on the hybrid-input prediction data object. The time-distributed fully connected layer may be configured to determine a frequency or duration corresponding with the one or more events across one or more time intervals. The hybrid-input prediction data object <b>411</b> may refer to a data object that describes the output of the time-distributed fully connected layer <b>705</b> of the feature synthesis machine learning model <b>405</b>. An example hybrid-input prediction data object <b>411</b> may comprise a single vector comprising a single relevant feature corresponding with each time segment.</p><p id="p-0102" num="0101"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart diagram illustrating an example process for generating a hybrid-input prediction data object <b>411</b> by a feature synthesis machine learning model <b>405</b>. The process depicted in <figref idref="DRAWINGS">FIG. <b>11</b></figref> begins at step/operation <b>1102</b>, when the concatenation layer <b>701</b> of the feature synthesis machine learning model <b>405</b> processes the acceleration-based feature data object <b>406</b> and the audio-based feature data object <b>408</b> to generate a concatenation feature data object <b>712</b>. At step/operation <b>1104</b>, the synthesis model sequence modeling layer <b>703</b> of the feature synthesis machine learning model <b>405</b> processes the concatenated feature data object to generate a synthesis model sequence modeling output <b>714</b>. Then at step/operation <b>1106</b>, the time-distributed fully connected layer <b>705</b> processes the synthesis model sequence modeling output <b>714</b> to generate the hybrid-input prediction data object <b>411</b>.</p><heading id="h-0016" level="2">Performing Prediction-Based Actions</heading><p id="p-0103" num="0102">In various embodiments, the hybrid-input prediction system <b>101</b> may be configured to further perform or trigger prediction-based actions in response to generated predictive outputs (e.g., one or more generated hybrid-input prediction data objects <b>411</b>). For example, the hybrid-input prediction system <b>101</b> may trigger generation (e.g., by a client computing entity <b>102</b>) of user interface data (e.g., messages, data objects and/or the like) for presentation by a user computing entity.</p><p id="p-0104" num="0103"><figref idref="DRAWINGS">FIG. <b>13</b></figref> provides an example graphical representation <b>1300</b> illustrating an acceleration-based feature data object <b>406</b>, an audio-based feature data object <b>408</b> and a hybrid-input prediction data object <b>411</b> generated by a hybrid-input prediction system <b>101</b>. As shown, the acceleration-based feature data object <b>406</b> comprises a two-dimensional matrix having a length and a width in which a first dimension corresponds with a number of segments in time and a second dimension corresponds with a plurality of features associated with oscillations/vibrations occurring at the corresponding segments in time. The audio-based feature data object <b>408</b> comprises a two-dimensional matrix having a length and a width in which a first dimension corresponds with a number of segments in time and a second dimension corresponds with a plurality of features associated with frequencies of sound waves occurring at the corresponding segments in time. The hybrid-input prediction data object <b>411</b> comprises a single vector comprising a single relevant feature corresponding with each time segment. Each feature corresponds with a likelihood/probability of an event at the corresponding time segment. As shown, the hybrid-input prediction data object <b>411</b> indicates a very high likelihood of an event occurring at a single time segment (&#x201c;1200&#x201d;), based at least in part on the data/features contained in the acceleration-based feature data object <b>406</b> and the audio-based feature data object <b>408</b>.</p><p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. <b>14</b></figref> provides an operational example showing user interface data generated based at least in part on an alert and/or corresponding predictive output (e.g., hybrid-input prediction data object <b>411</b>). The predictive output may be provided (e.g., sent, transmitted and/or the like) to a client computing entity <b>102</b>. The client computing entity <b>102</b> may generate a corresponding alert and provide (e.g., transmitted, sent and/or the like) corresponding user interface data for presentation by a user interface <b>1400</b>. The user interface data may be used for dynamically updating a user interface <b>1400</b>. In some embodiments, the user interface <b>1400</b> may dynamically update the display on a continuous or regular basis or in response to certain triggers. The user interface <b>1400</b> may comprise various features and functionality for accessing, and/or viewing user interface data. In one embodiment, the user interface <b>1400</b> may identify the user (e.g., monitored individual) credentialed for currently accessing the user interface <b>1400</b> (e.g., John Doe). The user interface <b>1400</b> may also comprise messages to the user in the form of banners, headers, notifications, and/or the like. As will be recognized, the described elements are provided for illustrative purposes and are not to be construed as limiting the dynamically updatable interface in any way.</p><heading id="h-0017" level="1">VI. Conclusion</heading><p id="p-0106" num="0105">Many modifications and other embodiments will come to mind to one skilled in the art to which this disclosure pertains having the benefit of the teachings presented in the foregoing descriptions and the associated drawings. Therefore, it is to be understood that the disclosure is not to be limited to the specific embodiments disclosed and that modifications and other embodiments are intended to be included within the scope of the appended claims. Although specific terms are employed herein, they are used in a generic and descriptive sense only and not for purposes of limitation.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method comprising:<claim-text>generating, by one or more processors and using an audio processing machine learning model and an audio data object, an audio-based feature data object, wherein:<claim-text>the audio processing machine learning model comprises (i) an audio model layer that is configured to generate an audio model output based at least in part on the audio data object, and (ii) an audio model one-dimensional convolutional layer that is configured to generate an audio model convolutional output based at least in part on the audio model output, and</claim-text><claim-text>the audio-based feature data object is generated based at least in part on the audio model convolutional output;</claim-text></claim-text><claim-text>generating, by the one or more processors and using an acceleration processing machine learning model and an acceleration data object, an acceleration-based feature data object, wherein:<claim-text>the acceleration processing machine learning model comprises (i) an acceleration model layer that is configured to generate an acceleration model output based at least in part on the acceleration data object, (ii) an acceleration model one-dimensional convolutional layer that is configured to generate an acceleration model convolutional output based at least in part on the acceleration model output, and (iii) an acceleration model up-sampling layer that is configured to generate an acceleration model up-sampling output based at least in part on the acceleration model convolutional output, and</claim-text><claim-text>the acceleration-based feature data object is generated based at least in part on the acceleration model up-sampling output;</claim-text></claim-text><claim-text>generating, by the one or more processors and using a feature synthesis machine learning model and the audio-based feature data object and the acceleration-based feature data object, a hybrid-input prediction data object; and</claim-text><claim-text>initiating, by the one or more processors, the performance of one or more prediction-based actions based at least in part on the hybrid-input prediction data object.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the audio-based feature data object has an audio feature object target dimension length,</claim-text><claim-text>the acceleration-based feature data object has an acceleration feature object target dimension length, and</claim-text><claim-text>the audio feature object target dimension length and the acceleration feature object target dimension length are equal.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computer-implemented method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the acceleration processing machine learning model comprises an acceleration model dimension adjustment layer configured to:<claim-text>generate an acceleration model dimension adjustment output based at least in part on the acceleration model up-sampling output by adjusting a target dimension of the acceleration model up-sampling output in accordance with the acceleration feature object target dimension, and</claim-text><claim-text>generate the audio-based feature data object based at least in part on the acceleration model dimension adjustment output.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computer-implemented method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the dimension adjustment layer comprises:<claim-text>an acceleration model sequence modeling layer that is configured to generate an acceleration model sequence modeling output based at least in part on the acceleration model up-sampling output,</claim-text><claim-text>a second acceleration model up-sampling layer that is configured to generate a second acceleration model up-sampling output based at least in part on the acceleration model sequence modeling output, and</claim-text><claim-text>an acceleration model dimension truncation layer that is configured to generate the acceleration model dimension adjustment output based at least in part on the second acceleration model up-sampling output.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the feature synthesis machine learning model comprises:<claim-text>a concatenation layer that is configured to generate a concatenated feature data object based at least in part on the audio-based feature data object and the acceleration-based feature data object,</claim-text><claim-text>one or more synthesis model sequence modeling layers that are collectively configured to generate a synthesis model sequence modeling output based at least in part on the concatenated feature data object, and</claim-text><claim-text>a time-distributed fully connected layer that is configured to generate the hybrid-input prediction data object based at least in part on the synthesis model sequence modeling output.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the audio data object describes recorded audio data for a monitored individual, and</claim-text><claim-text>the acceleration data object describes recorded cross-body acceleration data for the monitored individual.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computer-implemented method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the hybrid-input prediction data object describes cough likelihoods for the monitored individual across one or more time intervals.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The computer-implemented method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the hybrid-input prediction data object describes cough frequency for the monitored individual across one or more time intervals.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computer-implemented method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the hybrid-input prediction data object describes cough durations for the monitored individual across one or more time intervals.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. An apparatus comprising at least one processor and at least one memory including program code, the at least one memory and the program code configured to, with the processor, cause the apparatus to at least:<claim-text>generate using an audio processing machine learning model and an audio data object, an audio-based feature data object, wherein:<claim-text>the audio processing machine learning model comprises (i) an audio model layer that is configured to generate an audio model output based at least in part on the audio data object, and (ii) an audio model one-dimensional convolutional layer that is configured to generate an audio model convolutional output based at least in part on the audio model output, and</claim-text><claim-text>the audio-based feature data object is generated based at least in part on the audio model convolutional output;</claim-text></claim-text><claim-text>generate using an acceleration processing machine learning model and an acceleration data object, an acceleration-based feature data object, wherein:<claim-text>the acceleration processing machine learning model comprises (i) an acceleration model layer that is configured to generate an acceleration model output based at least in part on the acceleration data object, (ii) an acceleration model one-dimensional convolutional layer that is configured to generate an acceleration model convolutional output based at least in part on the acceleration model output, and (iii) an acceleration model up-sampling layer that is configured to generate an acceleration model up-sampling output based at least in part on the acceleration model convolutional output, and</claim-text><claim-text>the acceleration-based feature data object is generated based at least in part on the acceleration model up-sampling output;</claim-text></claim-text><claim-text>generate using a feature synthesis machine learning model and the audio-based feature data object and the acceleration-based feature data object, a hybrid-input prediction data object; and</claim-text><claim-text>initiate the performance of one or more prediction-based actions based at least in part on the hybrid-input prediction data object.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein:<claim-text>the audio-based feature data object has an audio feature object target dimension length,</claim-text><claim-text>the acceleration-based feature data object has an acceleration feature object target dimension length, and</claim-text><claim-text>the audio feature object target dimension length and the acceleration feature object target dimension length are equal.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The apparatus of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the acceleration processing machine learning model comprises an acceleration model dimension adjustment layer, and the program code is further configured to, with the processor, cause the apparatus to at least:<claim-text>generate an acceleration model dimension adjustment output based at least in part on the acceleration model up-sampling output by adjusting a target dimension of the acceleration model up-sampling output in accordance with the acceleration feature object target dimension, and</claim-text><claim-text>generate the audio-based feature data object based at least in part on the acceleration model dimension adjustment output.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The apparatus of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the dimension adjustment layer comprises:<claim-text>an acceleration model sequence modeling layer that is configured to generate an acceleration model sequence modeling output based at least in part on the acceleration model up-sampling output,</claim-text><claim-text>a second acceleration model up-sampling layer that is configured to generate a second acceleration model up-sampling output based at least in part on the acceleration model sequence modeling output, and</claim-text><claim-text>an acceleration model dimension truncation layer that is configured to generate the acceleration model dimension adjustment output based at least in part on the second acceleration model up-sampling output.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the feature synthesis machine learning model comprises:<claim-text>a concatenation layer that is configured to generate a concatenated feature data object based at least in part on the audio-based feature data object and the acceleration-based feature data object,</claim-text><claim-text>one or more synthesis model sequence modeling layers that are collectively configured to generate a synthesis model sequence modeling output based at least in part on the concatenated feature data object, and</claim-text><claim-text>a time-distributed fully connected layer that is configured to generate the hybrid-input prediction data object based at least in part on the synthesis model sequence modeling output.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein:<claim-text>the audio data object describes recorded audio data for a monitored individual, and</claim-text><claim-text>the acceleration data object describes recorded cross-body acceleration data for the monitored individual.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A computer program product comprising at least one non-transitory computer-readable storage medium having computer-readable program code portions stored therein, the computer-readable program code portions configured to:<claim-text>generate using an audio processing machine learning model and an audio data object, an audio-based feature data object, wherein:<claim-text>the audio processing machine learning model comprises (i) an audio model layer that is configured to generate an audio model output based at least in part on the audio data object, and (ii) an audio model one-dimensional convolutional layer that is configured to generate an audio model convolutional output based at least in part on the audio model output, and</claim-text><claim-text>the audio-based feature data object is generated based at least in part on the audio model convolutional output;</claim-text></claim-text><claim-text>generate using an acceleration processing machine learning model and an acceleration data object, an acceleration-based feature data object, wherein:<claim-text>the acceleration processing machine learning model comprises (i) an acceleration model layer that is configured to generate an acceleration model output based at least in part on the acceleration data object, (ii) an acceleration model one-dimensional convolutional layer that is configured to generate an acceleration model convolutional output based at least in part on the acceleration model output, and (iii) an acceleration model up-sampling layer that is configured to generate an acceleration model up-sampling output based at least in part on the acceleration model convolutional output, and</claim-text><claim-text>the acceleration-based feature data object is generated based at least in part on the acceleration model up-sampling output;</claim-text></claim-text><claim-text>generate using a feature synthesis machine learning model and the audio-based feature data object and the acceleration-based feature data object, a hybrid-input prediction data object; and</claim-text><claim-text>initiate the performance of one or more prediction-based actions based at least in part on the hybrid-input prediction data object.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The computer program product of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein:<claim-text>the audio-based feature data object has an audio feature object target dimension length,</claim-text><claim-text>the acceleration-based feature data object has an acceleration feature object target dimension length, and</claim-text><claim-text>the audio feature object target dimension length and the acceleration feature object target dimension length are equal.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computer program product of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the acceleration processing machine learning model comprises an acceleration model dimension adjustment layer, and the computer-readable program code portions are further configured to:<claim-text>generate an acceleration model dimension adjustment output based at least in part on the acceleration model up-sampling output by adjusting a target dimension of the acceleration model up-sampling output in accordance with the acceleration feature object target dimension, and</claim-text><claim-text>generate the audio-based feature data object based at least in part on the acceleration model dimension adjustment output.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The computer program product of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the dimension adjustment layer comprises:<claim-text>an acceleration model sequence modeling layer that is configured to generate an acceleration model sequence modeling output based at least in part on the acceleration model up-sampling output,</claim-text><claim-text>a second acceleration model up-sampling layer that is configured to generate a second acceleration model up-sampling output based at least in part on the acceleration model sequence modeling output, and</claim-text><claim-text>an acceleration model dimension truncation layer that is configured to generate the acceleration model dimension adjustment output based at least in part on the second acceleration model up-sampling output.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computer program product of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the feature synthesis machine learning model comprises:<claim-text>a concatenation layer that is configured to generate a concatenated feature data object based at least in part on the audio-based feature data object and the acceleration-based feature data object,</claim-text><claim-text>one or more synthesis model sequence modeling layers that are collectively configured to generate a synthesis model sequence modeling output based at least in part on the concatenated feature data object, and</claim-text><claim-text>a time-distributed fully connected layer that is configured to generate the hybrid-input prediction data object based at least in part on the synthesis model sequence modeling output.</claim-text></claim-text></claim></claims></us-patent-application>