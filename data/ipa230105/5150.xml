<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005151A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005151</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17860310</doc-number><date>20220708</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>764</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>16</class><subclass>H</subclass><main-group>30</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>16</class><subclass>H</subclass><main-group>50</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0016</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>764</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180101</date></cpc-version-indicator><section>G</section><class>16</class><subclass>H</subclass><main-group>30</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180101</date></cpc-version-indicator><section>G</section><class>16</class><subclass>H</subclass><main-group>50</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30096</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30068</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10012</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2200</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>2201</main-group><subgroup>03</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">METHODS AND SYSTEMS FOR PERFORMING REAL-TIME RADIOLOGY</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/US2021/012625</doc-number><date>20210108</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17860310</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62958859</doc-number><date>20200109</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Whiterabbit.ai Inc.</orgname><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Su</last-name><first-name>Jason</first-name><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Mathur</last-name><first-name>Rakesh</first-name><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Mombourquette</last-name><first-name>Brent</first-name><address><city>Toronto</city><country>CA</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Matthews</last-name><first-name>Thomas</first-name><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present disclosure provides methods and systems directed to performing real-time and/or AI-assisted radiology. A method for processing an image of a location of a body of a subject may comprise (a) obtaining the image of the location of a body of the subject; (b) using a trained algorithm to classify the image or a derivative thereof to a category among a plurality of categories, wherein the classifying comprises applying an image processing algorithm; (c) directing the image to a first radiologist for radiological assessment if the image is classified to a first category among the plurality of categories, or (ii) directing the image to a second radiologist for radiological assessment, if the image is classified to a second category among the plurality of categories; and (d) receiving a recommendation from the first or second radiologist to examine the subject based at least in part on the radiological assessment.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="196.43mm" wi="158.75mm" file="US20230005151A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="214.38mm" wi="164.34mm" file="US20230005151A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="226.99mm" wi="118.28mm" orientation="landscape" file="US20230005151A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="233.68mm" wi="139.62mm" file="US20230005151A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="231.65mm" wi="155.11mm" file="US20230005151A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="139.78mm" wi="165.78mm" file="US20230005151A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="143.00mm" wi="167.13mm" file="US20230005151A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="170.10mm" wi="163.75mm" file="US20230005151A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="232.07mm" wi="142.41mm" orientation="landscape" file="US20230005151A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="163.83mm" wi="171.03mm" file="US20230005151A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="232.07mm" wi="124.54mm" orientation="landscape" file="US20230005151A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="168.66mm" wi="161.37mm" file="US20230005151A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="131.57mm" wi="161.71mm" file="US20230005151A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="224.37mm" wi="128.61mm" orientation="landscape" file="US20230005151A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="231.99mm" wi="125.22mm" orientation="landscape" file="US20230005151A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="234.78mm" wi="79.16mm" file="US20230005151A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="232.41mm" wi="78.23mm" file="US20230005151A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="234.95mm" wi="79.67mm" file="US20230005151A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="234.95mm" wi="79.42mm" file="US20230005151A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="156.04mm" wi="168.74mm" file="US20230005151A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="154.52mm" wi="167.64mm" file="US20230005151A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="151.64mm" wi="165.02mm" file="US20230005151A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="156.72mm" wi="169.50mm" file="US20230005151A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="153.59mm" wi="167.30mm" file="US20230005151A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="155.28mm" wi="169.42mm" file="US20230005151A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="153.75mm" wi="167.64mm" file="US20230005151A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="155.62mm" wi="169.16mm" file="US20230005151A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="156.72mm" wi="168.66mm" file="US20230005151A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="154.09mm" wi="167.81mm" file="US20230005151A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="127.34mm" wi="169.25mm" file="US20230005151A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00030" num="00030"><img id="EMI-D00030" he="127.51mm" wi="169.16mm" file="US20230005151A1-20230105-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00031" num="00031"><img id="EMI-D00031" he="127.08mm" wi="168.66mm" file="US20230005151A1-20230105-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00032" num="00032"><img id="EMI-D00032" he="128.02mm" wi="169.42mm" file="US20230005151A1-20230105-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00033" num="00033"><img id="EMI-D00033" he="163.83mm" wi="166.12mm" file="US20230005151A1-20230105-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00034" num="00034"><img id="EMI-D00034" he="224.87mm" wi="119.13mm" orientation="landscape" file="US20230005151A1-20230105-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00035" num="00035"><img id="EMI-D00035" he="226.23mm" wi="140.04mm" orientation="landscape" file="US20230005151A1-20230105-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE</heading><p id="p-0002" num="0001">This application is a continuation of International Application No. PCT/US2021/012625, filed Jan. 8, 2021, which claims the benefit of U.S. Provisional Application No. 62/958,859, filed Jan. 9, 2020, each of which is incorporated by reference herein in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Breast cancer is the most widespread cancer in women in the U.S., with over 250 thousand new diagnoses in 2017 alone. About 1 in 8 women will be diagnosed with breast cancer at some point during their lives. Despite improvements in treatment, over 40 thousand women die every year in the U.S. from breast cancer. Substantial progress has been made in reducing breast cancer mortality in part due to the widespread adoption of screening mammography. Breast cancer screening can help identify early-stage cancers, which have much better prognoses and lower treatment costs as compared to late-stage cancers. This difference can be substantial: women with localized breast cancer have a 5-year survival rate of nearly 99%, while women with metastatic breast cancer have a 5-year survival rate of 27%.</p><p id="p-0004" num="0003">Despite these demonstrated benefits, adoption rates for screening mammography are hindered, in part, by poor patient experience, such as long delays in obtaining an appointment, unclear pricing, long wait times to receive exam results, and confusing reports. Further, problems arising from a lack of transparency in pricing are exacerbated by large variations in costs among providers. Similarly, delivery times for receiving exam results are inconsistent among providers. In addition, significant variation in radiologist performance results in patients experiencing very different standards of care depending on location and income.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">The present disclosure provides methods and systems for performing radiological assessment of subjects by stratifying medical image data using artificial intelligence into distinct radiological workflows for further screening and/or diagnostic assessment. Such subjects may include subjects with a disease (e.g., cancer) and subjects without a disease (e.g., cancer). The screening may be for a cancer such as, for example, breast cancer. The stratification may be performed based on disease-related assessments or other assessments (e.g., estimated case difficulty).</p><p id="p-0006" num="0005">In an aspect, the present disclosure provides a method for processing at least one image of a location of a body of the subject, comprising: (a) obtaining the at least one image of the location of a body of the subject; (b) using a trained algorithm to classify the at least one image or a derivative thereof to a category among a plurality of categories, wherein the classifying comprises applying an image processing algorithm to the at least one image or derivative thereof; (c) upon classifying the at least one image or derivative thereof in (b), (i) directing the at least one image or derivative thereof to a first radiologist for radiological assessment if the at least one image is classified to a first category among the plurality of categories, or (ii) directing the at least one image or derivative thereof to a second radiologist for radiological assessment, if the at least one image is classified to a second category among the plurality of categories; and (d) receiving a radiological assessment of the subject from the first or second radiologist based at least in part on a radiological analysis of the at least one image or derivative thereof.</p><p id="p-0007" num="0006">In some embodiments, (b) comprises classifying the at least one image or derivative thereof as normal, ambiguous, or suspicious. In some embodiments, the method further comprises directing the at least one image or derivative thereof to a classifier based on the classification of the at least one image or derivative thereof in (b). In some embodiments, (c) comprises directing the at least one image or derivative thereof to a first radiologist from among a first plurality of radiologists or to a second radiologist from among a second plurality of radiologists for radiological assessment. In some embodiments, the at least one image or derivative thereof is a medical image.</p><p id="p-0008" num="0007">In some embodiments, the trained algorithm is configured to classify the at least one image or derivative thereof as normal, ambiguous, or suspicious at a sensitivity of at least about 80%. In some embodiments, the trained algorithm is configured to classify the at least one image or derivative thereof as normal, ambiguous, or suspicious at a specificity of at least about 80%. In some embodiments, the trained algorithm is configured to classify the at least one image or derivative thereof as normal, ambiguous, or suspicious at a positive predictive value of at least about 80%. In some embodiments, the trained algorithm is configured to classify the at least one image or derivative thereof as normal, ambiguous, or suspicious at a negative predictive value of at least about 80%.</p><p id="p-0009" num="0008">In some embodiments, the trained machine learning algorithm is configured to identify the at least one region of the at least one image or derivative thereof that contains or is suspected of containing the anomalous tissue.</p><p id="p-0010" num="0009">In some embodiments, a trained algorithm classifies the at least one image or a derivative thereof as normal, ambiguous, or suspicious for being indicative of a cancer. In some embodiments, the cancer is breast cancer. In some embodiments, the at least one image or derivative thereof is a three-dimensional image of the breast of the subject. In some embodiments, the trained machine learning algorithm is trained using at least about 100 independent training samples comprising images that are indicative of or suspected of being indicative of a cancer.</p><p id="p-0011" num="0010">In some embodiments, the trained algorithm is trained using a first plurality of independent training samples comprising positive images that are indicative of or suspected of being indicative of a cancer and a second plurality of independent training samples comprising negative images that are not indicative of or not suspected of being indicative of a cancer. In some embodiments, the trained algorithm comprises a supervised machine learning algorithm. In some embodiments, the supervised machine learning algorithm comprises a deep learning algorithm, a support vector machine (SVM), a neural network, or a Random Forest.</p><p id="p-0012" num="0011">In some embodiments, the method further comprises monitoring the subject, wherein the monitoring comprises assessing images of the location of the body of the subject at a plurality of time points, wherein the assessing is based at least in part on the classification of the at least one image or a derivative thereof as normal, ambiguous, or suspicious at each of the plurality of time points. In some embodiments, a difference in the assessment of the images of the body of the subject at the plurality of time points is indicative of one or more clinical indications selected from the group consisting of: (i) a diagnosis of the subject, (ii) a prognosis of the subject, and (iii) an efficacy or non-efficacy of a course of treatment of the subject.</p><p id="p-0013" num="0012">In some embodiments, (c) further comprises (i) directing the at least one image or derivative thereof to a first radiologist among a first set of radiologists for radiological assessment to produce a screening result, based at least in part on whether the at least one image is classified as suspicious; (ii) directing the at least one image or derivative thereof to a second radiologist among a second set of radiologists for radiological assessment to produce a screening result, based at least in part on whether the at least one image is classified as ambiguous; or (iii) directing the at least one image or derivative thereof to a third radiologist among a third set of radiologists for radiological assessment to produce a screening result, based at least in part on whether the at least one image is classified as normal. In some embodiments, (c) further comprises directing the at least one image or derivative thereof to a first radiologist among a first set of radiologists for radiological assessment to produce a screening result, if the at least one image is classified as suspicious. In some embodiments, (c) further comprises directing the at least one image or derivative thereof to a second radiologist among a second set of radiologists for radiological assessment to produce a screening result, if the at least one image is classified as ambiguous. In some embodiments, (c) further comprises directing the at least one image or derivative thereof to a third radiologist among a third set of radiologists for radiological assessment to produce a screening result, if the at least one image is classified as normal. In some embodiments, the screening result of the subject is produced at a same clinic visit as the obtaining of the at least one image or derivative thereof. In some embodiments, the first set of radiologists is located at an on-site clinic, wherein the at least one image or derivative thereof is obtained at the on-site clinic.</p><p id="p-0014" num="0013">In some embodiments, the second set of radiologists comprises expert radiologists, which expert radiologists are trained to classify the at least one image or derivative thereof as normal or suspicious at a greater accuracy than the trained algorithm. In some embodiments, the third set of radiologists is located remotely to an onsite clinic, wherein the at least one image is obtained at the on-site clinic. In some embodiments, the third radiologist of the third set of radiologists performs the radiologist assessment of the at least one image or derivative thereof among a batch comprising a plurality of images, wherein the batch is selected for enhanced efficiency of the radiological assessment.</p><p id="p-0015" num="0014">In some embodiments, the method further comprises performing a diagnostic procedure of the subject, based at least in part on the screening result, to produce a diagnostic result of the subject. In some embodiments, the diagnostic result of the subject is produced at a same clinic visit as the obtaining of the at least one image. In some embodiments, the diagnostic result of the subject is produced within about one hour of the obtaining of the at least one image.</p><p id="p-0016" num="0015">In some embodiments, the at least one image or derivative thereof is directed to the first radiologist, the second radiologist, or the third radiologist based at least in part on additional characteristics of the location of the body of the subject. In some embodiments, the additional characteristics comprise an anatomy, tissue characteristics (e.g., tissue density or physical properties), a presence of a foreign object (e.g., implants), a type of finding, an appearance of disease (e.g., predicted by an algorithm such as a machine learning algorithm), or a combination thereof.</p><p id="p-0017" num="0016">In some embodiments, the at least one image or derivative thereof is directed to the first radiologist, the second radiologist, or the third radiologist based at least in part on additional characteristics of the first radiologist, the second radiologist, or the third radiologist (e.g., a personal ability of the first radiologist, the second radiologist, or the third radiologist to perform a radiological assessment of the at least one image or derivative thereof).</p><p id="p-0018" num="0017">In some embodiments, (c) further comprises generating an alert based at least in part on the directing of the at least one image or derivative thereof to the first radiologist or the directing of the at least one image or derivative thereof to the second radiologist. In some embodiments, the method further comprises transmitting the alert to the subject or to a clinical health care provider of the subject. In some embodiments, the method further comprises transmitting the alert to the subject through a patient mobile application. In some embodiments, the alert is generated in real time or substantially real time as (b).</p><p id="p-0019" num="0018">In some embodiments, applying the image processing algorithm comprises identifying regions of interest within the at least one image or derivative thereof, and labeling the regions of interest to produce at least one labeled image. In some embodiments, the method further comprises storing the at least one labeled image in a database. In some embodiments, the method further comprises storing one or more of the at least one image or derivative thereof and the classification in a database. In some embodiments, the method further comprises generating a presentation of the at least one image based at least in part on one or more of the at least one image or derivative thereof and the classification. In some embodiments, the method further comprises storing the presentation in the database.</p><p id="p-0020" num="0019">In some embodiments, (c) is performed in real time or substantially real time as (b). In some embodiments, the at least one image comprises a plurality of images obtained from the subject, wherein the plurality of images are obtained using different modalities or at different time points. In some embodiments, the classifying comprises processing clinical health data of the subject.</p><p id="p-0021" num="0020">In another aspect, the present disclosure provides a computer system for processing at least one image of a location of a body of the subject: a database that is configured to store the at least one image of the location of a body of the subject; and one or more computer processors operatively coupled to the database, wherein the one or more computer processors are individually or collectively programmed to: (a) use a trained algorithm to classify the at least one image or a derivative thereof to a category among a plurality of categories, wherein the classifying comprises applying an image processing algorithm to the at least one image or derivative thereof; (b) upon classifying the at least one image or derivative thereof in (a), (i) directing the at least image or derivative thereof to a first radiologist for radiological assessment if the at least one image is classified to a first category among the plurality of categories, or (ii) directing the at least one image or derivative thereof to a second radiologist for radiological assessment, if the at least one image is classified to a second category among the plurality of categories; and (c) receive a radiological assessment of the subject from the first or second radiologist based at least in part on a radiological analysis of the at least one image or derivative thereof.</p><p id="p-0022" num="0021">In some embodiments, (a) comprises classifying the at least one image or derivative thereof as normal, ambiguous, or suspicious. In some embodiments, the one or more computer processors are individually or collectively programmed to further direct the at least one image or derivative thereof to a classifier based on the classification of the at least one image or derivative thereof in (a). In some embodiments, (b) comprises directing the at least one image or derivative thereof to a first radiologist from among a first plurality of radiologists or to a second radiologist from among a second plurality of radiologists for radiological assessment. In some embodiments, the at least one image or derivative thereof is a medical image.</p><p id="p-0023" num="0022">In some embodiments, the trained algorithm is configured to classify the at least one image or derivative thereof as normal, ambiguous, or suspicious at a sensitivity of at least about 80%. In some embodiments, the trained algorithm is configured to classify the at least one image or derivative thereof as normal, ambiguous, or suspicious at a specificity of at least about 80%. In some embodiments, the trained algorithm is configured to classify the at least one image or derivative thereof as normal, ambiguous, or suspicious at a positive predictive value of at least about 80%. In some embodiments, the trained algorithm is configured to classify the at least one image or derivative thereof as normal, ambiguous, or suspicious at a negative predictive value of at least about 80%. In some embodiments, the trained machine learning algorithm is configured to identify the at least one region of the at least one image or derivative thereof that contains or is suspected of containing the anomalous tissue.</p><p id="p-0024" num="0023">In some embodiments, a trained algorithm classifies the at least one image or a derivative thereof as normal, ambiguous, or suspicious for being indicative of a cancer. In some embodiments, the cancer is breast cancer. In some embodiments, the at least one image or derivative thereof is a three-dimensional image of the breast of the subject. In some embodiments, the trained machine learning algorithm is trained using at least about 100 independent training samples comprising images that are indicative of or suspected of being indicative of a cancer.</p><p id="p-0025" num="0024">In some embodiments, the trained algorithm is trained using a first plurality of independent training samples comprising positive images that are indicative of or suspected of being indicative of a cancer and a second plurality of independent training samples comprising negative images that are not indicative of or not suspected of being indicative of a cancer. In some embodiments, the trained algorithm comprises a supervised machine learning algorithm. In some embodiments, the supervised machine learning algorithm comprises a deep learning algorithm, a support vector machine (SVM), a neural network, or a Random Forest.</p><p id="p-0026" num="0025">In some embodiments, the one or more computer processors are individually or collectively programmed to further monitor the subject, wherein the monitoring comprises assessing images of the location of the body of the subject at a plurality of time points, wherein the assessing is based at least in part on the classification of the at least one image or a derivative thereof as normal, ambiguous, or suspicious at each of the plurality of time points. In some embodiments, a difference in the assessment of the images of the body of the subject at the plurality of time points is indicative of one or more clinical indications selected from the group consisting of: (i) a diagnosis of the subject, (ii) a prognosis of the subject, and (iii) an efficacy or non-efficacy of a course of treatment of the subject.</p><p id="p-0027" num="0026">In some embodiments, (b) further comprises (i) directing the at least one image or derivative thereof to a first radiologist among a first set of radiologists for radiological assessment to produce a screening result, based at least in part on whether the at least one image or derivative thereof is classified as suspicious; (ii) directing the at least one image or derivative thereof to a second radiologist among a second set of radiologists for radiological assessment to produce a screening result, based at least in part on whether the at least one image or derivative thereof is classified as ambiguous; or (iii) directing the at least one image or derivative thereof to a third radiologist among a third set of radiologists for radiological assessment to produce a screening result, based at least in part on whether the at least one image or derivative thereof is classified as normal. In some embodiments, (b) further comprises directing the at least one image or derivative thereof to a first radiologist among a first set of radiologists for radiological assessment to produce a screening result, if the at least one image is classified as suspicious. In some embodiments, (b) further comprises directing the at least one image or derivative thereof to a second radiologist among a second set of radiologists for radiological assessment to produce a screening result, if the at least one image is classified as ambiguous. In some embodiments, (b) further comprises directing the at least one image or derivative thereof to a third radiologist among a third set of radiologists for radiological assessment to produce a screening result, if the at least one image is classified as normal. In some embodiments, the screening result of the subject is produced at a same clinic visit as the obtaining of the at least one image. In some embodiments, the first set of radiologists is located at an on-site clinic, wherein the at least one image is obtained at the on-site clinic.</p><p id="p-0028" num="0027">In some embodiments, the second set of radiologists comprises expert radiologists, which expert radiologists are trained to classify the at least one image or derivative thereof as normal or suspicious at a greater accuracy than the trained algorithm. In some embodiments, the third set of radiologists is located remotely to an onsite clinic, wherein the at least one image is obtained at the on-site clinic. In some embodiments, the third radiologist of the third set of radiologists performs the radiologist assessment of the at least one image or derivative thereof among a batch comprising a plurality of images, wherein the batch is selected for enhanced efficiency of the radiological assessment.</p><p id="p-0029" num="0028">In some embodiments, the one or more computer processors are individually or collectively programmed to further obtain a diagnostic result of the subject from a diagnostic procedure performed on the subject, based at least in part on the screening result. In some embodiments, the diagnostic result of the subject is produced at a same clinic visit as the obtaining of the at least one image. In some embodiments, the diagnostic result of the subject is produced within about one hour of the obtaining of the at least one image.</p><p id="p-0030" num="0029">In some embodiments, the at least one image or derivative thereof is directed to the first radiologist, the second radiologist, or the third radiologist based at least in part on additional characteristics of the location of the body of the subject. In some embodiments, the additional characteristics comprise an anatomy, tissue characteristics (e.g., tissue density or physical properties), a presence of a foreign object (e.g., implants), a type of finding, an appearance of disease (e.g., predicted by an algorithm such as a machine learning algorithm), or a combination thereof.</p><p id="p-0031" num="0030">In some embodiments, the at least one image or derivative thereof is directed to the first radiologist, the second radiologist, or the third radiologist based at least in part on additional characteristics of the first radiologist, the second radiologist, or the third radiologist (e.g., a personal ability of the first radiologist, the second radiologist, or the third radiologist to perform a radiological assessment of the at least one image or derivative thereof).</p><p id="p-0032" num="0031">In some embodiments, (b) further comprises generating an alert based at least in part on the directing of the at least one image or derivative thereof to the first radiologist or the directing of the at least one image or derivative thereof to the second radiologist. In some embodiments, the one or more computer processors are individually or collectively programmed to further transmit the alert to the subject or to a clinical health care provider of the subject. In some embodiments, the one or more computer processors are individually or collectively programmed to further transmit the alert to the subject through a patient mobile application. In some embodiments, the alert is generated in real time or substantially real time as (a).</p><p id="p-0033" num="0032">In some embodiments, applying the image processing algorithm comprises identifying regions of interest within the at least one image or derivative thereof, and labeling the regions of interest to produce at least one labeled image. In some embodiments, the one or more computer processors are individually or collectively programmed to further store the at least one labeled image in a database. In some embodiments, the one or more computer processors are individually or collectively programmed to further store one or more of the at least one image or derivative thereof and the classification in a database. In some embodiments, the one or more computer processors are individually or collectively programmed to further generate a presentation of the at least one image or derivative thereof based at least in part on one or more of the at least one image and the classification. In some embodiments, the one or more computer processors are individually or collectively programmed to further store the presentation in the database.</p><p id="p-0034" num="0033">In some embodiments, (b) is performed in real time or substantially real time as (a). In some embodiments, the at least one image comprises a plurality of images obtained from the subject, wherein the plurality of images are obtained using different modalities or at different time points. In some embodiments, the classifying comprises processing clinical health data of the subject.</p><p id="p-0035" num="0034">Another aspect of the present disclosure provides a non-transitory computer readable medium comprising machine executable code that, upon execution by one or more computer processors, implements any of the methods above or elsewhere herein.</p><p id="p-0036" num="0035">Another aspect of the present disclosure provides a system comprising one or more computer processors and computer memory coupled thereto. The computer memory comprises machine executable code that, upon execution by the one or more computer processors, implements any of the methods above or elsewhere herein.</p><p id="p-0037" num="0036">Additional aspects and advantages of the present disclosure will become readily apparent to those skilled in this art from the following detailed description, wherein only illustrative embodiments of the present disclosure are shown and described. As will be realized, the present disclosure is capable of other and different embodiments, and its several details are capable of modifications in various obvious respects, all without departing from the disclosure. Accordingly, the drawings and description are to be regarded as illustrative in nature, and not as restrictive.</p><heading id="h-0004" level="1">INCORPORATION BY REFERENCE</heading><p id="p-0038" num="0037">All publications, patents, and patent applications mentioned in this specification are herein incorporated by reference to the same extent as if each individual publication, patent, or patent application was specifically and individually indicated to be incorporated by reference. To the extent publications and patents or patent applications incorporated by reference contradict the disclosure contained in the specification, the specification is intended to supersede and/or take precedence over any such contradictory material.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0039" num="0038">The patent or application file contains at least one drawing executed in color. Copies of this patent or patent application publication with color drawing(s) will be provided by the Office upon request and payment of the necessary fee.</p><p id="p-0040" num="0039">The novel features of the invention are set forth with particularity in the appended claims. A better understanding of the features and advantages of the present invention will be obtained by reference to the following detailed description that sets forth illustrative embodiments, in which the principles of the invention are utilized, and the accompanying drawings (also &#x201c;Figure&#x201d; and &#x201c;FIG.&#x201d; herein), of which:</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example workflow of a method for directing cases for radiological review (e.g., by a radiologist or radiologic technologist), in accordance with disclosed embodiments.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example of a method of using a triage engine configured to stratify a subject who is undergoing mammographic screening by classifying the mammographic data of the subject into one of three different workflows: normal, uncertain, and suspicious, in accordance with disclosed embodiments.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>D</figref> show an example of a user interface for a real-time radiology system, in accordance with disclosed embodiments, including views from the perspective of a mammography technologist or technologist's assistant (<figref idref="DRAWINGS">FIG. <b>3</b>A</figref>), a radiologist (<figref idref="DRAWINGS">FIG. <b>3</b>B</figref>), a billing representative (<figref idref="DRAWINGS">FIG. <b>3</b>C</figref>), and an ultrasound technologist or technologist's assistant (<figref idref="DRAWINGS">FIG. <b>3</b>D</figref>).</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a computer system that is programmed or otherwise configured to implement methods provided herein.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an example plot of detection frequency of breast cancer tumors of various sizes (ranging from 2 mm to 29 mm) that are detected using a real-time radiology system, in accordance with disclosed embodiments.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows an example plot of positive predictive values from screening mammography (PPV1) versus callback rate, in accordance with disclosed embodiments.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an example plot comparing the interpretation time for batches (including control, BI-RADs, and density) (left) and the percentage improvement in interpretation time versus controls (right), across a first set of radiologist, a second set of radiologists, and the overall total set of radiologists, in accordance with disclosed embodiments.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a receiver operating characteristic (ROC) curve indicating the performance of the DNN on a binary classification task as evaluated on a testing dataset, in accordance with disclosed embodiments.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows an example of a schematic of patient flow through clinics with the AI-enabled real-time radiology system and patient mobile application (app), in accordance with disclosed embodiments.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows an example of a schematic of an AI-assisted radiology assessment workflow, in accordance with disclosed embodiments.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows an example of a triage software system developed using machine learning for screening mammography to enable more timely report delivery and follow-up for suspicious cases (e.g., as performed in a batch reading setting), in accordance with disclosed embodiments.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIGS. <b>12</b>A-<b>12</b>D</figref> show examples of synthetic 2D mammography (SM) images derived from digital breast tomosynthesis (DBT) exams for each of the four Breast Imaging Reporting and Data System (BI-RADS) breast density categories: (A) almost entirely fatty (<figref idref="DRAWINGS">FIG. <b>12</b>A</figref>), (B) scattered areas of fibroglandular density (<figref idref="DRAWINGS">FIG. <b>12</b>B</figref>), (C) heterogeneously dense (<figref idref="DRAWINGS">FIG. <b>12</b>C</figref>), and (D) extremely dense (<figref idref="DRAWINGS">FIG. <b>12</b>D</figref>), in accordance with disclosed embodiments.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIGS. <b>13</b>A-<b>13</b>D</figref> show a comparison between a full-field digital mammography (FFDM) image (<figref idref="DRAWINGS">FIG. <b>13</b>A</figref>) and a synthetic 2D mammography (SM) image (<figref idref="DRAWINGS">FIG. <b>13</b>B</figref>) of the same breast under the same compression, in accordance with disclosed embodiments. A zoomed-in region, whose original location is denoted by the white box, is shown for both the FFDM image (<figref idref="DRAWINGS">FIG. <b>13</b>C</figref>) and the SM image (<figref idref="DRAWINGS">FIG. <b>13</b>D</figref>) to highlight the differences in texture and contrast that can occur between the two image types.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIGS. <b>14</b>A-<b>14</b>B</figref> show confusion matrices for the Breast Imaging Reporting and Data System (BI-RADS) breast density task (<figref idref="DRAWINGS">FIG. <b>14</b>A</figref>) and the binary density task (dense, BI-RADS C+D vs. non-dense, BI-RADS A+B) (<figref idref="DRAWINGS">FIG. <b>14</b>B</figref>) evaluated on the full-field digital mammography (FFDM) test set, in accordance with disclosed embodiments. The numbers of test samples (exams) within each bin are shown in parentheses.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIGS. <b>15</b>A-<b>15</b>D</figref> show confusion matrices, evaluated on the Site 1 SM test set, for the Breast Imaging Reporting and Data System (BI-RADS) breast density task without adaptation (<figref idref="DRAWINGS">FIG. <b>15</b>A</figref>), the binary density task (dense, BI-RADS C+D vs. non-dense, BI-RADS A+B) (<figref idref="DRAWINGS">FIG. <b>15</b>B</figref>) without adaptation, the BI-RADS breast density task with adaptation by matrix calibration for 500 training samples (<figref idref="DRAWINGS">FIG. <b>15</b>C</figref>), and the binary density task (dense vs. non-dense) (<figref idref="DRAWINGS">FIG. <b>15</b>B</figref>) with adaptation by matrix calibration for 500 training samples, in accordance with disclosed embodiments. The numbers of test samples (exams) within each bin are shown in parentheses.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIGS. <b>16</b>A-<b>16</b>D</figref> show confusion matrices, evaluated on the Site 2 SM test set, for the Breast Imaging Reporting and Data System (BI-RADS) breast density task without adaptation (<figref idref="DRAWINGS">FIG. <b>16</b>A</figref>), the binary density task (dense, BI-RADS C+D vs. non-dense, BI-RADS A+B) (<figref idref="DRAWINGS">FIG. <b>16</b>B</figref>) without adaptation, the BI-RADS breast density task with adaptation by matrix calibration for 500 training samples (<figref idref="DRAWINGS">FIG. <b>16</b>C</figref>), and the binary density task (dense vs. non-dense) (<figref idref="DRAWINGS">FIG. <b>16</b>D</figref>) with adaptation by matrix calibration for 500 training samples, in accordance with disclosed embodiments. The numbers of test samples (exams) within each bin are shown in parentheses.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIGS. <b>17</b>A-<b>17</b>D</figref> show the impact of the amount of training data on the performance of the adaptation methods, as measured by macroAUC and linearly weighted Cohen's kappa, for the Site 1 dataset (<figref idref="DRAWINGS">FIGS. <b>17</b>A-<b>17</b>B</figref>, respectively) and the Site 2 SM dataset (<figref idref="DRAWINGS">FIGS. <b>17</b>C-<b>17</b>D</figref>, respectively), in accordance with disclosed embodiments.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>18</b></figref> shows an example of a schematic of a real-time radiology assessment workflow.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>19</b></figref> shows an example of a schematic of a real-time radiology assessment workflow.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>20</b></figref> shows an example of a schematic of an AI-assisted radiology assessment workflow in a teleradiology setting.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0061" num="0060">While various embodiments of the invention have been shown and described herein, it will be obvious to those skilled in the art that such embodiments are provided by way of example only. Numerous variations, changes, and substitutions may occur to those skilled in the art without departing from the invention. It should be understood that various alternatives to the embodiments of the invention described herein may be employed.</p><p id="p-0062" num="0061">As used in the specification and claims, the singular form &#x201c;a&#x201d;, &#x201c;an&#x201d;, and &#x201c;the&#x201d; include plural references unless the context clearly dictates otherwise. For example, the term &#x201c;a nucleic acid&#x201d; includes a plurality of nucleic acids, including mixtures thereof.</p><p id="p-0063" num="0062">As used herein, the term &#x201c;subject,&#x201d; generally refers to an entity or a medium that has testable or detectable genetic information. A subject can be a person, individual, or patient. A subject can be a vertebrate, such as, for example, a mammal. Non-limiting examples of mammals include humans, simians, farm animals, sport animals, rodents, and pets. The subject can be a person that has a cancer or is suspected of having a cancer. The subject may be displaying a symptom(s) indicative of a health or physiological state or condition of the subject, such as a cancer (e.g., breast cancer) of the subject. As an alternative, the subject can be asymptomatic with respect to such health or physiological state or condition.</p><p id="p-0064" num="0063">Breast cancer is the most widespread cancer in women in the U.S., with over 250 thousand new diagnoses in 2017 alone. About 1 in 8 women will be diagnosed with breast cancer at some point during their lives. Despite improvements in treatment, over 40 thousand women die every year in the U.S. from breast cancer. Substantial progress has made in reducing breast cancer mortality in part due to the widespread adoption of screening mammography. Breast cancer screening can help identify early-stage cancers, which have much better prognoses and lower treatment costs as compared to late-stage cancers. This difference can be substantial: women with localized breast cancer have a 5-year survival rate of nearly 99%, while women with metastatic breast cancer have a 5-year survival rate of 27%.</p><p id="p-0065" num="0064">Despite these demonstrated benefits, adoption rates for screening mammography are hindered, in part, by poor patient experience, such as long delays in obtaining an appointment, unclear pricing, long wait times to receive exam results, and confusing reports. Further, problems arising from a lack of transparency in pricing are exacerbated by large variations in costs among providers. Similarly, delivery times for receiving exam results are inconsistent among providers. In addition, significant variation in radiologist performance results in patients experiencing very different standards of care depending on location and income.</p><p id="p-0066" num="0065">The present disclosure provides methods and systems for performing real-time radiology of subjects by stratifying medical image data using artificial intelligence into distinct radiological workflows for further screening and/or diagnostic assessment. Such subjects may include subjects with a cancer and subjects without cancer. The screening may be for a cancer such as, for example, breast cancer.</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example workflow of a method for directing cases for radiological review (e.g., by a radiologist, radiologic technician, or radiologic technologist), in accordance with disclosed embodiments. In an aspect, the present disclosure provides a method <b>100</b> for processing at least one image of a location of a body of a subject. The method <b>100</b> may comprise obtaining the image of the location of a body of the subject (as in operation <b>102</b>). Next, the method <b>100</b> may comprise using a trained algorithm to classify the image or a derivative thereof to a category among a plurality of categories (as in operation <b>104</b>). For example, the classifying may comprise applying an image processing algorithm to the image or derivative thereof. Next, the method <b>100</b> may comprise determining whether the image was classified to a first category or a second category among the plurality of categories (as in operation <b>106</b>). If the image was classified to the first category, then the method <b>100</b> may comprise directing the image to a first radiologist for radiological assessment (as in operation <b>108</b>). If the image was classified to the second category, then the method <b>100</b> may comprise directing the image to a second radiologist for radiological assessment (as in operation <b>110</b>). Next, the method <b>100</b> may comprise receiving a recommendation (e.g., from the first or second radiologist, or from another radiologist or physician) to examine the subject based on the radiological assessment of the image (as in operation <b>112</b>).</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example of a method of using a triage engine configured to stratify a subject who is undergoing mammographic screening by classifying the mammographic data of the subject into one of three different workflows: normal, uncertain, and suspicious, in accordance with disclosed embodiments. First, a dataset comprising an electronic health record (EHR) and medical images of a patient are provided. Next, an AI-based triage engine processes the EHR and medical images to analyze and classify the dataset as likely normal, possibly suspicious, or likely suspicious. Next, the patient's dataset is processed by one of three workflows based on the classification of the dataset as normal, uncertain, or suspicious: a normal workflow, an uncertain workflow, and a suspicious workflow, respectively. Each of the three workflows may comprise radiologist review or further AI-based analysis (e.g., by a trained algorithm). The normal workflow may comprise an AI-based (optionally a cloud-based) confirmation that the patient's dataset is normal, upon which the routine screening is complete. For example, a group of radiologists may review the normal workflow cases at high volume and efficiency. Alternatively, the normal workflow may comprise an AI-based (optionally a cloud-based) determination that the patient's dataset is suspicious, upon which an immediate radiologist review of the patient's dataset is ordered. For example, a second group of radiologists may review the suspicious workflow cases at lower volume and lower efficiency (e.g., expert radiologists conducting more detailed radiological assessments). Similarly, the uncertain and suspicious workflow may also comprise an immediate radiologist review of the patient's dataset. In some embodiments, different sets of radiologists are used to review the different workflows, as described elsewhere herein. In some embodiments, the same sets of radiologists are used to review the different workflows (e.g., at different time points depending on a prioritization of the cases for radiological assessment).</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>D</figref> show an example of a user interface for a real-time radiology system, in accordance with disclosed embodiments, including views from the perspective of a mammography technologist or technologist's assistant (<figref idref="DRAWINGS">FIG. <b>3</b>A</figref>), a radiologist (<figref idref="DRAWINGS">FIG. <b>3</b>B</figref>), a billing representative (<figref idref="DRAWINGS">FIG. <b>3</b>C</figref>), and an ultrasound technician or technician's assistant (<figref idref="DRAWINGS">FIG. <b>3</b>D</figref>). The view may include a heatmap showing which regions were identified as suspicious by the AI algorithm. The mammography technologist or technologist's assistant may ask the patient some questions and evaluate the responses to the questions to assess whether the patient is qualified for a real-time radiology assessment. The radiologist may read or interpret the medical images (e.g., mammography images) of the patient in accordance with the real-time radiology methods and systems of the present disclosure. The billing representative may estimate the diagnostic costs based on the patient's qualification for a real-time radiology assessment. The mammography/ultrasound technologist or technologist's assistant may inform the patient to wait for their results of the real-time radiology assessment. The user interface may provide a notification (e.g., generated by an AI-based algorithm) to the technologist or technologist's assistant that an acquired image is of poor quality, so that the technologist or technologist's assistant can make a correction to the acquired image or repeat the image acquisition.</p><heading id="h-0007" level="2">Obtaining Medical Images</heading><p id="p-0070" num="0069">The medical images may be obtained or derived from a human subject (e.g., a patient). The medical images may be stored in a database, such as a computer server (e.g., cloud-based server), a local server, a local computer, or a mobile device (such as smartphone or tablet)). The medical images may be obtained from a subject with cancer, from a subject that is suspected of having cancer, or from a subject that does not have or is not suspected of having cancer.</p><p id="p-0071" num="0070">The medical images may be taken before and/or after treatment of a subject with cancer. Medical images may be obtained from a subject during a treatment or a treatment regime. Multiple sets of medical images may be obtained from a subject to monitor the effects of the treatment over time. The medical images may be taken from a subject known or suspected of having a cancer (e.g., breast cancer) for which a definitive positive or negative diagnosis is not available via clinical tests. The medical images may be taken from a subject suspected of having cancer. The medical images may be taken from a subject experiencing unexplained symptoms, such as fatigue, nausea, weight loss, aches and pains, weakness, or bleeding. The medical images may be taken from a subject having explained symptoms. The medical images may be taken from a subject at risk of developing cancer due to factors such as familial history, age, hypertension or pre-hypertension, diabetes or pre-diabetes, overweight or obesity, environmental exposure, lifestyle risk factors (e.g., smoking, alcohol consumption, or drug use), or presence of other risk factors.</p><p id="p-0072" num="0071">The medical images may be acquired using one or more imaging modalities, such as a mammography scan, a computed tomography (CT) scan, a magnetic resonance imaging (MRI) scan, an ultrasound scan, a digital X-ray scan, a positron emission tomography (PET) scan, a PET-CT scan, a nuclear medicine scan, a thermography scan, an ophthalmy scan, an optical coherence tomography scan, an electrocardiography scan, an endoscopy scan, a diaphanography scan, a bone densitometry scan, an optical scan, or any combination thereof. The medical images may be pre-processed using image processing techniques or deep learning to enhance image characteristics (e.g., contrast, brightness, sharpness), remove noise or artifacts, filter frequency ranges, compress the images to a small file size, or sample or crop the images. The medical images may be raw or reconstructed (e.g., to create a 3-D volume from a plurality of 2-D images). The images may be processed to compute maps that are correlated to tissue properties or functional behavior as in functional MRI (fMRI) or resting state fMRI. The images may be overlaid with heatmaps or additional information showing information like fluid flow. The images may be created from a composite of images from several scans of the same subject or from several subjects.</p><p id="p-0073" num="0072">Trained Algorithms</p><p id="p-0074" num="0073">After obtaining datasets comprising a plurality of medical images of a location of a body of one or more subjects, a trained algorithm may be used to process the datasets to classify the image as normal, ambiguous, or suspicious. For example, the trained algorithm may be used to determine regions of interest (ROIs) in the plurality of medical images of a subject, and to process the ROIs to classify the image as normal, ambiguous, or suspicious. The trained algorithm may be configured to classify the image as normal, ambiguous, or suspicious with an accuracy of at least about 50%, at least about 55%, at least about 60%, at least about 65%, at least about 70%, at least about 75%, at least about 80%, at least about 85%, at least about 90%, at least about 95%, at least about 96%, at least about 97%, at least about 98%, at least about 99%, or more than 99% for at least about 25, at least about 50, at least about 100, at least about 150, at least about 200, at least about 250, at least about 300, at least about 350, at least about 400, at least about 450, at least about 500, or more than about 500 independent samples.</p><p id="p-0075" num="0074">The trained algorithm may comprise a supervised machine learning algorithm. The trained algorithm may comprise a classification and regression tree (CART) algorithm. The supervised machine learning algorithm may comprise, for example, a Random Forest, a support vector machine (SVM), a neural network (e.g., a deep neural network (DNN)), or a deep learning algorithm. The trained algorithm may comprise an unsupervised machine learning algorithm.</p><p id="p-0076" num="0075">The trained algorithm may be configured to accept a plurality of input variables and to produce one or more output values based on the plurality of input variables. The plurality of input variables may comprise features extracted from one or more datasets comprising medical images of a location of a body of a subject. For example, an input variable may comprise a number of potentially cancerous or suspicious regions of interest (ROIs) in the dataset of medical images. The potentially cancerous or suspicious regions of interest (ROIs) may be identified or extracted from the dataset of medical images using a variety of image processing approaches, such as image segmentation. An input variable may also comprise several images from slices in a 3D volume or multiple exams over a course of time. The plurality of input variables may also include clinical health data of a subject.</p><p id="p-0077" num="0076">In some embodiments, the clinical health data comprises one or more quantitative measures of the subject, such as age, weight, height, body mass index (BMI), blood pressure, heart rate, glucose levels. As another example, the clinical health data can comprise one or more categorical measures, such as race, ethnicity, history of medication or other clinical treatment, history of tobacco use, history of alcohol consumption, daily activity or fitness level, genetic test results, blood test results, imaging results, and screening results.</p><p id="p-0078" num="0077">The trained algorithm may comprise one or more modules configured to perform image processing on one or more images (e.g., radiological images), thereby producing a detection or segmentation of the one or more images. The trained algorithm may comprise a classifier, such that each of the one or more output values comprises one of a fixed number of possible values (e.g., a linear classifier, a logistic regression classifier, etc.) indicating a classification of the datasets comprising medical images by the classifier. The trained algorithm may comprise a binary classifier, such that each of the one or more output values comprises one of two values (e.g., {0, 1}, {positive, negative}, {high-risk, low-risk}, or {suspicious, normal}) indicating a classification of the datasets comprising medical images by the classifier. The trained algorithm may be another type of classifier, such that each of the one or more output values comprises one of more than two values (e.g., {0, 1, 2}, {positive, negative, or indeterminate}, {high-risk, intermediate-risk, or low-risk}, or {suspicious, normal, or indeterminate}) indicating a classification of the datasets comprising medical images by the classifier. The output values may comprise descriptive labels, numerical values, or a combination thereof. Some of the output values may comprise descriptive labels. Such descriptive labels may provide an identification, indication, likelihood, or risk of a disease or disorder state of the subject, and may comprise, for example, positive, negative, high-risk, intermediate-risk, low-risk, suspicious, normal, or indeterminate. Such descriptive labels may provide an identification of a follow-up diagnostic procedure or treatment for the subject, and may comprise, for example, a therapeutic intervention, a duration of the therapeutic intervention, and/or a dosage of the therapeutic intervention suitable to treat a cancer or other condition. Such descriptive labels may provide an identification of secondary clinical tests that may be appropriate to perform on the subject, and may comprise, for example, an imaging test, a blood test, a computed tomography (CT) scan, a magnetic resonance imaging (MRI) scan, an ultrasound scan, a digital X-ray, a positron emission tomography (PET) scan, a PET-CT scan, or any combination thereof. As another example, such descriptive labels may provide a prognosis of the cancer of the subject. As another example, such descriptive labels may provide a relative assessment of the cancer (e.g., an estimated stage or tumor burden) of the subject. Some descriptive labels may be mapped to numerical values, for example, by mapping &#x201c;positive&#x201d; to 1 and &#x201c;negative&#x201d; to 0.</p><p id="p-0079" num="0078">Some of the output values may comprise numerical values, such as binary, integer, or continuous values. Such binary output values may comprise, for example, {0, 1}, {positive, negative}, or {high-risk, low-risk}. Such integer output values may comprise, for example, {0, 1, 2}. Such continuous output values may comprise, for example, a probability value of at least 0 and no more than 1. Such continuous output values may comprise, for example, the center coordinates of an ROI. Such continuous output values may indicate a prognosis of the cancer of the subject. Some numerical values may be mapped to descriptive labels, for example, by mapping 1 to &#x201c;positive&#x201d; and 0 to &#x201c;negative.&#x201d; An array or map of numerical values may be produced, such as a probability of cancer map.</p><p id="p-0080" num="0079">Some of the output values may be assigned based on one or more cutoff values. For example, a binary classification of datasets comprising medical images may assign an output value of &#x201c;positive&#x201d; or 1 if the dataset comprising medical images indicates that the subject has at least a 50% probability of having a cancer (e.g., breast cancer). For example, a binary classification of datasets comprising medical images may assign an output value of &#x201c;negative&#x201d; or 0 if the dataset comprising medical images indicates that the subject has less than a 50% probability of having a cancer. In this case, a single cutoff value of 50% is used to classify datasets comprising medical images into one of the two possible binary output values. Examples of single cutoff values may include about 1%, about 2%, about 5%, about 10%, about 15%, about 20%, about 25%, about 30%, about 35%, about 40%, about 45%, about 50%, about 55%, about 60%, about 65%, about 70%, about 75%, about 80%, about 85%, about 90%, about 91%, about 92%, about 93%, about 94%, about 95%, about 96%, about 97%, about 98%, and about 99%.</p><p id="p-0081" num="0080">As another example, a classification of datasets comprising medical images may assign an output value of &#x201c;positive&#x201d; or 1 if the dataset comprising medical images indicates that the subject has a probability of having a cancer of at least about 50%, at least about 55%, at least about 60%, at least about 65%, at least about 70%, at least about 75%, at least about 80%, at least about 85%, at least about 90%, at least about 91%, at least about 92%, at least about 93%, at least about 94%, at least about 95%, at least about 96%, at least about 97%, at least about 98%, at least about 99%, or more. The classification of samples may assign an output value of &#x201c;positive&#x201d; or 1 if the dataset comprising medical images indicates that the subject has a probability of having a cancer of more than about 50%, more than about 55%, more than about 60%, more than about 65%, more than about 70%, more than about 75%, more than about 80%, more than about 85%, more than about 90%, more than about 91%, more than about 92%, more than about 93%, more than about 94%, more than about 95%, more than about 96%, more than about 97%, more than about 98%, or more than about 99%.</p><p id="p-0082" num="0081">The classification of datasets comprising medical images may assign an output value of &#x201c;negative&#x201d; or 0 if the dataset comprising medical images indicates that the subject has a probability of having a cancer of less than about 50%, less than about 45%, less than about 40%, less than about 35%, less than about 30%, less than about 25%, less than about 20%, less than about 15%, less than about 10%, less than about 9%, less than about 8%, less than about 7%, less than about 6%, less than about 5%, less than about 4%, less than about 3%, less than about 2%, or less than about 1%. The classification of dataset comprising medical images may assign an output value of &#x201c;negative&#x201d; or 0 if the dataset comprising medical images indicates that the subject has a probability of having a cancer of no more than about 50%, no more than about 45%, no more than about 40%, no more than about 35%, no more than about 30%, no more than about 25%, no more than about 20%, no more than about 15%, no more than about 10%, no more than about 9%, no more than about 8%, no more than about 7%, no more than about 6%, no more than about 5%, no more than about 4%, no more than about 3%, no more than about 2%, or no more than about 1%.</p><p id="p-0083" num="0082">The classification of datasets comprising medical images may assign an output value of &#x201c;indeterminate&#x201d; or 2 if the dataset comprising medical images is not classified as &#x201c;positive&#x201d;, &#x201c;negative&#x201d;, 1, or 0. In this case, a set of two cutoff values is used to classify datasets comprising medical images into one of the three possible output values. Examples of sets of cutoff values may include {1%, 99%}, {2%, 98%}, {5%, 95%}, {10%, 90%}, {15%, 85%}, {20%, 80%}, {25%, 75%}, {30%, 70%}, {35%, 65%}, {40%, 60%}, and {45%, 55%}. Similarly, sets of n cutoff values may be used to classify datasets comprising medical images into one of n+1 possible output values, where n is any positive integer.</p><p id="p-0084" num="0083">The trained algorithm may be trained with a plurality of independent training samples. Each of the independent training samples may comprise a dataset comprising medical images from a subject, associated datasets obtained by analyzing the medical images (e.g., labels or annotations), and one or more known output values corresponding to the dataset comprising medical images (e.g., the difficulty of reading the images, the time it took read the images, a clinical diagnosis, prognosis, absence, or treatment efficacy of a cancer of the subject). Independent training samples may comprise dataset comprising medical images, and associated datasets and outputs obtained or derived from a plurality of different subjects. Independent training samples may comprise dataset comprising medical images and associated datasets and outputs obtained at a plurality of different time points from the same subject (e.g., on a regular basis such as weekly, monthly, or annually). Independent training samples may be associated with presence of the cancer or disease (e.g., training samples comprising dataset comprising medical images, and associated datasets and outputs obtained or derived from a plurality of subjects known to have the cancer or disease). Independent training samples may be associated with absence of the cancer or disease (e.g., training samples comprising dataset comprising medical images, and associated datasets and outputs obtained or derived from a plurality of subjects who are known to not have a previous diagnosis of the cancer or who have received a negative test result for the cancer or disease).</p><p id="p-0085" num="0084">The trained algorithm may be trained with at least about 50, at least about 100, at least about 250, at least about 500, at least about 1 thousand, at least about 5 thousand, at least about 10 thousand, at least about 15 thousand, at least about 20 thousand, at least about 25 thousand, at least about 30 thousand, at least about 35 thousand, at least about 40 thousand, at least about 45 thousand, at least about 50 thousand, at least about 100 thousand, at least about 150 thousand, at least about 200 thousand, at least about 250 thousand, at least about 300 thousand, at least about 350 thousand, at least about 400 thousand, at least about 450 thousand, or at least about 500 thousand independent training samples. The independent training samples may comprise dataset comprising medical images associated with presence of the disease (e.g., cancer) and/or dataset comprising medical images associated with absence of the disease (e.g., cancer). The trained algorithm may be trained with no more than about 500 thousand, no more than about 450 thousand, no more than about 400 thousand, no more than about 350 thousand, no more than about 300 thousand, no more than about 250 thousand, no more than about 200 thousand, no more than about 150 thousand, no more than about 100 thousand, no more than about 50 thousand, no more than about 25 thousand, no more than about 10 thousand, no more than about 5 thousand, no more than about 1 thousand, no more than about 500, no more than about 250, no more than about 100, or no more than about 50 independent training samples associated with presence of the disease (e.g., cancer). In some embodiments, the dataset comprising medical images is independent of samples used to train the trained algorithm.</p><p id="p-0086" num="0085">The trained algorithm may be trained with a first number of independent training samples associated with presence of the disease (e.g., cancer) and a second number of independent training samples associated with absence of the disease (e.g., cancer). The first number of independent training samples associated with presence of the disease (e.g., cancer) may be no more than the second number of independent training samples associated with absence of the disease (e.g., cancer). The first number of independent training samples associated with presence of the disease (e.g., cancer) may be equal to the second number of independent training samples associated with absence of the disease (e.g., cancer). The first number of independent training samples associated with presence of the disease (e.g., cancer) may be greater than the second number of independent training samples associated with absence of the disease (e.g., cancer).</p><p id="p-0087" num="0086">The trained algorithm may be configured to classify the medical images at an accuracy of at least about 50%, at least about 55%, at least about 60%, at least about 65%, at least about 70%, at least about 75%, at least about 80%, at least about 81%, at least about 82%, at least about 83%, at least about 84%, at least about 85%, at least about 86%, at least about 87%, at least about 88%, at least about 89%, at least about 90%, at least about 91%, at least about 92%, at least about 93%, at least about 94%, at least about 95%, at least about 96%, at least about 97%, at least about 98%, at least about 99%, or more; for at least about 50, at least about 100, at least about 250, at least about 500, at least about 1 thousand, at least about 5 thousand, at least about 10 thousand, at least about 15 thousand, at least about 20 thousand, at least about 25 thousand, at least about 30 thousand, at least about 35 thousand, at least about 40 thousand, at least about 45 thousand, at least about 50 thousand, at least about 100 thousand, at least about 150 thousand, at least about 200 thousand, at least about 250 thousand, at least about 300 thousand, at least about 350 thousand, at least about 400 thousand, at least about 450 thousand, or at least about 500 thousand independent test samples. The accuracy of classifying the medical images by the trained algorithm may be calculated as the percentage of independent test samples (e.g., images from subjects known to have the cancer or subjects with negative clinical test results for the cancer) that are correctly identified or classified as being normal or suspicious.</p><p id="p-0088" num="0087">The trained algorithm may be configured to classify the medical images with a positive predictive value (PPV) of at least about 5%, at least about 10%, at least about 15%, at least about 20%, at least about 25%, at least about 30%, at least about 35%, at least about 40%, at least about 50%, at least about 55%, at least about 60%, at least about 65%, at least about 70%, at least about 75%, at least about 80%, at least about 81%, at least about 82%, at least about 83%, at least about 84%, at least about 85%, at least about 86%, at least about 87%, at least about 88%, at least about 89%, at least about 90%, at least about 91%, at least about 92%, at least about 93%, at least about 94%, at least about 95%, at least about 96%, at least about 97%, at least about 98%, at least about 99%, or more. The PPV of classifying the medical images using the trained algorithm may be calculated as the percentage of medical images identified or classified as being suspicious that correspond to subjects that truly have an abnormal condition (e.g., cancer).</p><p id="p-0089" num="0088">The trained algorithm may be configured to classify the medical images with a negative predictive value (NPV) of at least about 5%, at least about 10%, at least about 15%, at least about 20%, at least about 25%, at least about 30%, at least about 35%, at least about 40%, at least about 50%, at least about 55%, at least about 60%, at least about 65%, at least about 70%, at least about 75%, at least about 80%, at least about 81%, at least about 82%, at least about 83%, at least about 84%, at least about 85%, at least about 86%, at least about 87%, at least about 88%, at least about 89%, at least about 90%, at least about 91%, at least about 92%, at least about 93%, at least about 94%, at least about 95%, at least about 96%, at least about 97%, at least about 98%, at least about 99%, or more. The NPV of classifying the medical images using the trained algorithm may be calculated as the percentage of medical images identified or classified as being normal that correspond to subjects that truly do not have an abnormal condition (e.g., cancer).</p><p id="p-0090" num="0089">The trained algorithm may be configured to classify the medical images with a clinical sensitivity at least about 5%, at least about 10%, at least about 15%, at least about 20%, at least about 25%, at least about 30%, at least about 35%, at least about 40%, at least about 50%, at least about 55%, at least about 60%, at least about 65%, at least about 70%, at least about 75%, at least about 80%, at least about 81%, at least about 82%, at least about 83%, at least about 84%, at least about 85%, at least about 86%, at least about 87%, at least about 88%, at least about 89%, at least about 90%, at least about 91%, at least about 92%, at least about 93%, at least about 94%, at least about 95%, at least about 96%, at least about 97%, at least about 98%, at least about 99%, at least about 99.1%, at least about 99.2%, at least about 99.3%, at least about 99.4%, at least about 99.5%, at least about 99.6%, at least about 99.7%, at least about 99.8%, at least about 99.9%, at least about 99.99%, at least about 99.999%, or more. The clinical sensitivity of classifying the medical images using the trained algorithm may be calculated as the percentage of medical images obtained from subjects known to have a condition (e.g., cancer) that are correctly identified or classified as being suspicious for the condition.</p><p id="p-0091" num="0090">The trained algorithm may be configured to classify the medical images with a clinical specificity of at least about 5%, at least about 10%, at least about 15%, at least about 20%, at least about 25%, at least about 30%, at least about 35%, at least about 40%, at least about 50%, at least about 55%, at least about 60%, at least about 65%, at least about 70%, at least about 75%, at least about 80%, at least about 81%, at least about 82%, at least about 83%, at least about 84%, at least about 85%, at least about 86%, at least about 87%, at least about 88%, at least about 89%, at least about 90%, at least about 91%, at least about 92%, at least about 93%, at least about 94%, at least about 95%, at least about 96%, at least about 97%, at least about 98%, at least about 99%, at least about 99.1%, at least about 99.2%, at least about 99.3%, at least about 99.4%, at least about 99.5%, at least about 99.6%, at least about 99.7%, at least about 99.8%, at least about 99.9%, at least about 99.99%, at least about 99.999%, or more. The clinical specificity of classifying the medical images using the trained algorithm may be calculated as the percentage of medical images obtained from subjects without a condition (e.g., subjects with negative clinical test results for cancer) that are correctly identified or classified as being normal for the condition.</p><p id="p-0092" num="0091">The trained algorithm may be configured to classify the medical images with an Area-Under-Curve (AUC) of at least about 0.50, at least about 0.55, at least about 0.60, at least about 0.65, at least about 0.70, at least about 0.75, at least about 0.80, at least about 0.81, at least about 0.82, at least about 0.83, at least about 0.84, at least about 0.85, at least about 0.86, at least about 0.87, at least about 0.88, at least about 0.89, at least about 0.90, at least about 0.91, at least about 0.92, at least about 0.93, at least about 0.94, at least about 0.95, at least about 0.96, at least about 0.97, at least about 0.98, at least about 0.99, or more. The AUC may be calculated as an integral of the Receiver Operating Characteristic (ROC) curve (e.g., the area under the ROC curve) associated with the trained algorithm in classifying datasets comprising medical images as being normal or suspicious.</p><p id="p-0093" num="0092">The trained algorithm may be adjusted or tuned to improve one or more of the performance, accuracy, PPV, NPV, clinical sensitivity, clinical specificity, or AUC of identifying the cancer. The trained algorithm may be adjusted or tuned by adjusting parameters of the trained algorithm (e.g., a set of cutoff values used to classify a dataset comprising medical images as described elsewhere herein, or parameters or weights of a neural network). The trained algorithm may be adjusted or tuned continuously during the training process or after the training process has completed.</p><p id="p-0094" num="0093">After the trained algorithm is initially trained, a subset of the inputs may be identified as most influential or most important to be included for making high-quality classifications. For example, a subset of the plurality of features of the dataset comprising medical images may be identified as most influential or most important to be included for making high-quality classifications or identifications of cancer. The plurality of features of the dataset comprising medical images or a subset thereof may be ranked based on classification metrics indicative of each individual feature's influence or importance toward making high-quality classifications or identifications of cancer. Such metrics may be used to reduce, in some cases significantly, the number of input variables (e.g., predictor variables) that may be used to train the trained algorithm to a desired performance level (e.g., based on a desired minimum accuracy, PPV, NPV, clinical sensitivity, clinical specificity, AUC, or a combination thereof). For example, if training the trained algorithm with a plurality comprising several dozen or hundreds of input variables in the trained algorithm results in an accuracy of classification of more than 99%, then training the trained algorithm instead with only a selected subset of no more than about 5, no more than about 10, no more than about 15, no more than about 20, no more than about 25, no more than about 30, no more than about 35, no more than about 40, no more than about 45, no more than about 50, or no more than about 100 such most influential or most important input variables among the plurality can yield decreased but still acceptable accuracy of classification (e.g., at least about 50%, at least about 55%, at least about 60%, at least about 65%, at least about 70%, at least about 75%, at least about 80%, at least about 81%, at least about 82%, at least about 83%, at least about 84%, at least about 85%, at least about 86%, at least about 87%, at least about 88%, at least about 89%, at least about 90%, at least about 91%, at least about 92%, at least about 93%, at least about 94%, at least about 95%, at least about 96%, at least about 97%, at least about 98%, or at least about 99%). The subset may be selected by rank-ordering the entire plurality of input variables and selecting a predetermined number (e.g., no more than about 5, no more than about 10, no more than about 15, no more than about 20, no more than about 25, no more than about 30, no more than about 35, no more than about 40, no more than about 45, no more than about 50, or no more than about 100) of input variables with the best classification metrics.</p><heading id="h-0008" level="2">Identifying or Monitoring a Cancer</heading><p id="p-0095" num="0094">After using a trained algorithm to process the dataset comprising a plurality of medical images of a location of a body of a subject to classify the image as normal, ambiguous, or suspicious, a cancer may be identified or monitored in the subject. The identification may be made based at least in part on the classification of the image as normal, ambiguous, or suspicious; a plurality of features extracted from the dataset comprising medical images; and/or clinical health data of the subject. The identification may be made by a radiologist, a plurality of radiologists, or a trained algorithm.</p><p id="p-0096" num="0095">The cancer may be identified in the subject at an accuracy of at least about 50%, at least about 55%, at least about 60%, at least about 65%, at least about 70%, at least about 75%, at least about 80%, at least about 81%, at least about 82%, at least about 83%, at least about 84%, at least about 85%, at least about 86%, at least about 87%, at least about 88%, at least about 89%, at least about 90%, at least about 91%, at least about 92%, at least about 93%, at least about 94%, at least about 95%, at least about 96%, at least about 97%, at least about 98%, at least about 99%, or more. The accuracy of identifying the cancer may be calculated as the percentage of independent test subjects (e.g., subjects known to have the cancer or subjects with negative clinical test results for the cancer) that are correctly identified or classified as having or not having the cancer.</p><p id="p-0097" num="0096">The cancer may be identified in the subject with a positive predictive value (PPV) of at least about 5%, at least about 10%, at least about 15%, at least about 20%, at least about 25%, at least about 30%, at least about 35%, at least about 40%, at least about 50%, at least about 55%, at least about 60%, at least about 65%, at least about 70%, at least about 75%, at least about 80%, at least about 81%, at least about 82%, at least about 83%, at least about 84%, at least about 85%, at least about 86%, at least about 87%, at least about 88%, at least about 89%, at least about 90%, at least about 91%, at least about 92%, at least about 93%, at least about 94%, at least about 95%, at least about 96%, at least about 97%, at least about 98%, at least about 99%, or more. The PPV of identifying the cancer may be calculated as the percentage of independent test subjects identified or classified as having cancer that correspond to subjects that truly have cancer.</p><p id="p-0098" num="0097">The cancer may be identified in the subject with a negative predictive value (NPV) of at least about 5%, at least about 10%, at least about 15%, at least about 20%, at least about 25%, at least about 30%, at least about 35%, at least about 40%, at least about 50%, at least about 55%, at least about 60%, at least about 65%, at least about 70%, at least about 75%, at least about 80%, at least about 81%, at least about 82%, at least about 83%, at least about 84%, at least about 85%, at least about 86%, at least about 87%, at least about 88%, at least about 89%, at least about 90%, at least about 91%, at least about 92%, at least about 93%, at least about 94%, at least about 95%, at least about 96%, at least about 97%, at least about 98%, at least about 99%, or more. The NPV of identifying the cancer using the trained algorithm may be calculated as the percentage of independent test subjects identified or classified as not having cancer that correspond to subjects that truly do not have cancer.</p><p id="p-0099" num="0098">The cancer may be identified in the subject with a clinical sensitivity of at least about 5%, at least about 10%, at least about 15%, at least about 20%, at least about 25%, at least about 30%, at least about 35%, at least about 40%, at least about 50%, at least about 55%, at least about 60%, at least about 65%, at least about 70%, at least about 75%, at least about 80%, at least about 81%, at least about 82%, at least about 83%, at least about 84%, at least about 85%, at least about 86%, at least about 87%, at least about 88%, at least about 89%, at least about 90%, at least about 91%, at least about 92%, at least about 93%, at least about 94%, at least about 95%, at least about 96%, at least about 97%, at least about 98%, at least about 99%, at least about 99.1%, at least about 99.2%, at least about 99.3%, at least about 99.4%, at least about 99.5%, at least about 99.6%, at least about 99.7%, at least about 99.8%, at least about 99.9%, at least about 99.99%, at least about 99.999%, or more. The clinical sensitivity of identifying the cancer may be calculated as the percentage of independent test subjects associated with presence of the cancer (e.g., subjects known to have the cancer) that are correctly identified or classified as having cancer.</p><p id="p-0100" num="0099">The cancer may be identified in the subject with a clinical specificity of at least about 5%, at least about 10%, at least about 15%, at least about 20%, at least about 25%, at least about 30%, at least about 35%, at least about 40%, at least about 50%, at least about 55%, at least about 60%, at least about 65%, at least about 70%, at least about 75%, at least about 80%, at least about 81%, at least about 82%, at least about 83%, at least about 84%, at least about 85%, at least about 86%, at least about 87%, at least about 88%, at least about 89%, at least about 90%, at least about 91%, at least about 92%, at least about 93%, at least about 94%, at least about 95%, at least about 96%, at least about 97%, at least about 98%, at least about 99%, at least about 99.1%, at least about 99.2%, at least about 99.3%, at least about 99.4%, at least about 99.5%, at least about 99.6%, at least about 99.7%, at least about 99.8%, at least about 99.9%, at least about 99.99%, at least about 99.999%, or more. The clinical specificity of identifying the cancer may be calculated as the percentage of independent test subjects associated with absence of the cancer (e.g., subjects with negative clinical test results for the cancer) that are correctly identified or classified as not having cancer.</p><p id="p-0101" num="0100">In some embodiments, the subject may be identified as being at risk of a cancer. After identifying the subject as being at risk of a cancer, a clinical intervention for the subject may be selected based at least in part on the cancer for which the subject is identified as being at risk. In some embodiments, the clinical intervention is selected from a plurality of clinical interventions (e.g., clinically indicated for different types of cancer).</p><p id="p-0102" num="0101">In some embodiments, the trained algorithm may determine that the subject is at risk of a cancer of at least about 5%, at least about 10%, at least about 15%, at least about 20%, at least about 25%, at least about 30%, at least about 35%, at least about 40%, at least about 50%, at least about 55%, at least about 60%, at least about 65%, at least about 70%, at least about 75%, at least about 80%, at least about 81%, at least about 82%, at least about 83%, at least about 84%, at least about 85%, at least about 86%, at least about 87%, at least about 88%, at least about 89%, at least about 90%, at least about 91%, at least about 92%, at least about 93%, at least about 94%, at least about 95%, at least about 96%, at least about 97%, at least about 98%, at least about 99%, or more.</p><p id="p-0103" num="0102">The trained algorithm may determine that the subject is at risk of a cancer at an accuracy of at least about 50%, at least about 55%, at least about 60%, at least about 65%, at least about 70%, at least about 75%, at least about 80%, at least about 81%, at least about 82%, at least about 83%, at least about 84%, at least about 85%, at least about 86%, at least about 87%, at least about 88%, at least about 89%, at least about 90%, at least about 91%, at least about 92%, at least about 93%, at least about 94%, at least about 95%, at least about 96%, at least about 97%, at least about 98%, at least about 99%, at least about 99.1%, at least about 99.2%, at least about 99.3%, at least about 99.4%, at least about 99.5%, at least about 99.6%, at least about 99.7%, at least about 99.8%, at least about 99.9%, at least about 99.99%, at least about 99.999%, or more.</p><p id="p-0104" num="0103">Upon identifying the subject as having the cancer, the subject may be optionally provided with a therapeutic intervention (e.g., prescribing an appropriate course of treatment to treat the cancer of the subject). The therapeutic intervention may comprise a prescription of an effective dose of a drug, a further testing or evaluation of the cancer, a further monitoring of the cancer, or a combination thereof. If the subject is currently being treated for the cancer with a course of treatment, the therapeutic intervention may comprise a subsequent different course of treatment (e.g., to increase treatment efficacy due to non-efficacy of the current course of treatment).</p><p id="p-0105" num="0104">The therapeutic intervention may comprise recommending the subject for a secondary clinical test to confirm a diagnosis of the cancer. This secondary clinical test may comprise an imaging test, a blood test, a computed tomography (CT) scan, a magnetic resonance imaging (MRI) scan, an ultrasound scan, a chest X-ray, a positron emission tomography (PET) scan, a PET-CT scan, or any combination thereof.</p><p id="p-0106" num="0105">The classification of the image as normal, ambiguous, or suspicious; a plurality of features extracted from the dataset comprising medical images; and/or clinical health data of the subject may be assessed over a duration of time to monitor a patient (e.g., subject who has cancer or who is being treated for cancer). In some cases, the classification of the medical images of the patient may change during the course of treatment. For example, the features of the dataset of a patient with decreasing risk of the cancer due to an effective treatment may shift toward the profile or distribution of a healthy subject (e.g., a subject without cancer). Conversely, for example, the features of the dataset of a patient with increasing risk of the cancer due to an ineffective treatment may shift toward the profile or distribution of a subject with higher risk of the cancer or a more advanced cancer.</p><p id="p-0107" num="0106">The cancer of the subject may be monitored by monitoring a course of treatment for treating the cancer of the subject. The monitoring may comprise assessing the cancer of the subject at two or more time points. The assessing may be based at least on the classification of the image as normal, ambiguous, or suspicious; a plurality of features extracted from the dataset comprising medical images; and/or clinical health data of the subject determined at each of the two or more time points.</p><p id="p-0108" num="0107">In some embodiments, a difference in the classification of the image as normal, ambiguous, or suspicious; a plurality of features extracted from the dataset comprising medical images; and/or clinical health data of the subject determined between the two or more time points may be indicative of one or more clinical indications, such as (i) a diagnosis of the cancer of the subject, (ii) a prognosis of the cancer of the subject, (iii) an increased risk of the cancer of the subject, (iv) a decreased risk of the cancer of the subject, (v) an efficacy of the course of treatment for treating the cancer of the subject, and (vi) a non-efficacy of the course of treatment for treating the cancer of the subject.</p><p id="p-0109" num="0108">In some embodiments, a difference in the classification of the image as normal, ambiguous, or suspicious; a plurality of features extracted from the dataset comprising medical images; and/or clinical health data of the subject determined between the two or more time points may be indicative of a diagnosis of the cancer of the subject. For example, if the cancer was not detected in the subject at an earlier time point but was detected in the subject at a later time point, then the difference is indicative of a diagnosis of the cancer of the subject. A clinical action or decision may be made based on this indication of diagnosis of the cancer of the subject, such as, for example, prescribing a new therapeutic intervention for the subject. The clinical action or decision may comprise recommending the subject for a secondary clinical test to confirm the diagnosis of the cancer. This secondary clinical test may comprise an imaging test, a blood test, a computed tomography (CT) scan, a magnetic resonance imaging (MM) scan, an ultrasound scan, a chest X-ray, a positron emission tomography (PET) scan, a PET-CT scan, or any combination thereof.</p><p id="p-0110" num="0109">In some embodiments, a difference in the classification of the image as normal, ambiguous, or suspicious; a plurality of features extracted from the dataset comprising medical images; and/or clinical health data of the subject determined between the two or more time points may be indicative of a prognosis of the cancer of the subject.</p><p id="p-0111" num="0110">In some embodiments, a difference in the classification of the image as normal, ambiguous, or suspicious; a plurality of features extracted from the dataset comprising medical images; and/or clinical health data of the subject determined between the two or more time points may be indicative of the subject having an increased risk of the cancer. For example, if the cancer was detected in the subject both at an earlier time point and at a later time point, and if the difference is a positive difference (e.g., an increase from the earlier time point to the later time point), then the difference may be indicative of the subject having an increased risk of the cancer. A clinical action or decision may be made based on this indication of the increased risk of the cancer, e.g., prescribing a new therapeutic intervention or switching therapeutic interventions (e.g., ending a current treatment and prescribing a new treatment) for the subject. The clinical action or decision may comprise recommending the subject for a secondary clinical test to confirm the increased risk of the cancer. This secondary clinical test may comprise an imaging test, a blood test, a computed tomography (CT) scan, a magnetic resonance imaging (MM) scan, an ultrasound scan, a chest X-ray, a positron emission tomography (PET) scan, a PET-CT scan, or any combination thereof.</p><p id="p-0112" num="0111">In some embodiments, a difference in the classification of the image as normal, ambiguous, or suspicious; a plurality of features extracted from the dataset comprising medical images; and/or clinical health data of the subject determined between the two or more time points may be indicative of the subject having a decreased risk of the cancer. For example, if the cancer was detected in the subject both at an earlier time point and at a later time point, and if the difference is a negative difference (e.g., a decrease from the earlier time point to the later time point), then the difference may be indicative of the subject having a decreased risk of the cancer. A clinical action or decision may be made based on this indication of the decreased risk of the cancer (e.g., continuing or ending a current therapeutic intervention) for the subject. The clinical action or decision may comprise recommending the subject for a secondary clinical test to confirm the decreased risk of the cancer. This secondary clinical test may comprise an imaging test, a blood test, a computed tomography (CT) scan, a magnetic resonance imaging (MRI) scan, an ultrasound scan, a chest X-ray, a positron emission tomography (PET) scan, a PET-CT scan, or any combination thereof.</p><p id="p-0113" num="0112">In some embodiments, a difference in the classification of the image as normal, ambiguous, or suspicious; a plurality of features extracted from the dataset comprising medical images; and/or clinical health data of the subject determined between the two or more time points may be indicative of an efficacy of the course of treatment for treating the cancer of the subject. For example, if the cancer was detected in the subject at an earlier time point but was not detected in the subject at a later time point, then the difference may be indicative of an efficacy of the course of treatment for treating the cancer of the subject. A clinical action or decision may be made based on this indication of the efficacy of the course of treatment for treating the cancer of the subject, e.g., continuing or ending a current therapeutic intervention for the subject. The clinical action or decision may comprise recommending the subject for a secondary clinical test to confirm the efficacy of the course of treatment for treating the cancer. This secondary clinical test may comprise an imaging test, a blood test, a computed tomography (CT) scan, a magnetic resonance imaging (MRI) scan, an ultrasound scan, a chest X-ray, a positron emission tomography (PET) scan, a PET-CT scan, or any combination thereof.</p><p id="p-0114" num="0113">In some embodiments, a difference in the classification of the image as normal, ambiguous, or suspicious; a plurality of features extracted from the dataset comprising medical images; and/or clinical health data of the subject determined between the two or more time points may be indicative of a non-efficacy of the course of treatment for treating the cancer of the subject. For example, if the cancer was detected in the subject both at an earlier time point and at a later time point, and if the difference is a positive or zero difference (e.g., increased or remained at a constant level from the earlier time point to the later time point), and if an efficacious treatment was indicated at an earlier time point, then the difference may be indicative of a non-efficacy of the course of treatment for treating the cancer of the subject. A clinical action or decision may be made based on this indication of the non-efficacy of the course of treatment for treating the cancer of the subject, e.g., ending a current therapeutic intervention and/or switching to (e.g., prescribing) a different new therapeutic intervention for the subject. The clinical action or decision may comprise recommending the subject for a secondary clinical test to confirm the non-efficacy of the course of treatment for treating the cancer. This secondary clinical test may comprise an imaging test, a blood test, a computed tomography (CT) scan, a magnetic resonance imaging (MRI) scan, an ultrasound scan, a chest X-ray, a positron emission tomography (PET) scan, a PET-CT scan, or any combination thereof</p><p id="p-0115" num="0114">Outputting a Report of the Disease</p><p id="p-0116" num="0115">After the cancer is identified or an increased risk of the disease or cancer is monitored in the subject, a report may be electronically outputted that is indicative of (e.g., identifies or provides an indication of) the disease or cancer of the subject. The subject may not display a disease or cancer (e.g., is asymptomatic of the disease or cancer such as a complication). The report may be presented on a graphical user interface (GUI) of an electronic device of a user. The user may be the subject, a caretaker, a physician, a nurse, or another health care worker.</p><p id="p-0117" num="0116">The report may include one or more clinical indications such as (i) a diagnosis of the cancer of the subject, (ii) a prognosis of the disease or cancer of the subject, (iii) an increased risk of the disease or cancer of the subject, (iv) a decreased risk of the disease or cancer of the subject, (v) an efficacy of the course of treatment for treating the disease or cancer of the subject, (vi) a non-efficacy of the course of treatment for treating the disease or cancer of the subject, (vii) a location and/or a level of suspicion of the disease or cancer, and (viii) an efficacy measure of a proposed course of diagnosis of the disease or cancer. The report may include one or more clinical actions or decisions made based on these one or more clinical indications. Such clinical actions or decisions may be directed to therapeutic interventions, or further clinical assessment or testing of the disease or cancer of the subject.</p><p id="p-0118" num="0117">For example, a clinical indication of a diagnosis of the disease or cancer of the subject may be accompanied with a clinical action of prescribing a new therapeutic intervention for the subject. As another example, a clinical indication of an increased risk of the disease or cancer of the subject may be accompanied with a clinical action of prescribing a new therapeutic intervention or switching therapeutic interventions (e.g., ending a current treatment and prescribing a new treatment) for the subject. As another example, a clinical indication of a decreased risk of the disease or cancer of the subject may be accompanied with a clinical action of continuing or ending a current therapeutic intervention for the subject. As another example, a clinical indication of an efficacy of the course of treatment for treating the disease or cancer of the subject may be accompanied with a clinical action of continuing or ending a current therapeutic intervention for the subject. As another example, a clinical indication of a non-efficacy of the course of treatment for treating the disease or cancer of the subject may be accompanied with a clinical action of ending a current therapeutic intervention and/or switching to (e.g., prescribing) a different new therapeutic intervention for the subject. As another example, a clinical indication of a location of disease or cancer may be accompanied with a clinical action of prescribing a new diagnostic test, especially any particular parameters of that test that may be targeted for the indication.</p><heading id="h-0009" level="2">Computer Systems</heading><p id="p-0119" num="0118">The present disclosure provides computer systems that are programmed to implement methods of the disclosure. <figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a computer system <b>401</b> that is programmed or otherwise configured to, for example, train and test a trained algorithm; use the trained algorithm to process medical images to classify the image as normal, ambiguous, or suspicious; identify or monitor a cancer of the subject; and electronically output a report that indicative of the cancer of the subject.</p><p id="p-0120" num="0119">The computer system <b>401</b> can regulate various aspects of analysis, calculation, and generation of the present disclosure, such as, for example, training and testing a trained algorithm; using the trained algorithm to process medical images to classify the image as normal, ambiguous, or suspicious; identifying or monitoring a cancer of the subject; and electronically outputting a report that indicative of the cancer of the subject. The computer system <b>401</b> can be an electronic device of a user or a computer system that is remotely located with respect to the electronic device. The electronic device can be a mobile electronic device.</p><p id="p-0121" num="0120">The computer system <b>401</b> includes a central processing unit (CPU, also &#x201c;processor&#x201d; and &#x201c;computer processor&#x201d; herein) <b>405</b>, which can be a single core or multi core processor, or a plurality of processors for parallel processing. The computer system <b>401</b> also includes memory or memory location <b>410</b> (e.g., random-access memory, read-only memory, flash memory), electronic storage unit <b>415</b> (e.g., hard disk), communication interface <b>420</b> (e.g., network adapter) for communicating with one or more other systems, and peripheral devices <b>425</b>, such as cache, other memory, data storage and/or electronic display adapters. The memory <b>410</b>, storage unit <b>415</b>, interface <b>420</b> and peripheral devices <b>425</b> are in communication with the CPU <b>405</b> through a communication bus (solid lines), such as a motherboard. The storage unit <b>415</b> can be a data storage unit (or data repository) for storing data. The computer system <b>401</b> can be operatively coupled to a computer network (&#x201c;network&#x201d;) <b>430</b> with the aid of the communication interface <b>420</b>. The network <b>430</b> can be the Internet, an internet and/or extranet, or an intranet and/or extranet that is in communication with the Internet.</p><p id="p-0122" num="0121">The network <b>430</b> in some cases is a telecommunication and/or data network. The network <b>430</b> can include one or more computer servers, which can enable distributed computing, such as cloud computing. For example, one or more computer servers may enable cloud computing over the network <b>430</b> (&#x201c;the cloud&#x201d;) to perform various aspects of analysis, calculation, and generation of the present disclosure, such as, for example, training and testing a trained algorithm; using the trained algorithm to process medical images to classify the image as normal, ambiguous, or suspicious; identifying or monitoring a cancer of the subject; and electronically outputting a report that indicative of the cancer of the subject. Such cloud computing may be provided by cloud computing platforms such as, for example, Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform, and IBM cloud. The network <b>430</b>, in some cases with the aid of the computer system <b>401</b>, can implement a peer-to-peer network, which may enable devices coupled to the computer system <b>401</b> to behave as a client or a server.</p><p id="p-0123" num="0122">The CPU <b>405</b> may comprise one or more computer processors and/or one or more graphics processing units (GPUs). The CPU <b>405</b> can execute a sequence of machine-readable instructions, which can be embodied in a program or software. The instructions may be stored in a memory location, such as the memory <b>410</b>. The instructions can be directed to the CPU <b>405</b>, which can subsequently program or otherwise configure the CPU <b>405</b> to implement methods of the present disclosure. Examples of operations performed by the CPU <b>405</b> can include fetch, decode, execute, and writeback.</p><p id="p-0124" num="0123">The CPU <b>405</b> can be part of a circuit, such as an integrated circuit. One or more other components of the system <b>401</b> can be included in the circuit. In some cases, the circuit is an application specific integrated circuit (ASIC).</p><p id="p-0125" num="0124">The storage unit <b>415</b> can store files, such as drivers, libraries and saved programs. The storage unit <b>415</b> can store user data, e.g., user preferences and user programs. The computer system <b>401</b> in some cases can include one or more additional data storage units that are external to the computer system <b>401</b>, such as located on a remote server that is in communication with the computer system <b>401</b> through an intranet or the Internet.</p><p id="p-0126" num="0125">The computer system <b>401</b> can communicate with one or more remote computer systems through the network <b>430</b>. For instance, the computer system <b>401</b> can communicate with a remote computer system of a user. Examples of remote computer systems include personal computers (e.g., portable PC), slate or tablet PC's (e.g., Apple&#xae; iPad, Samsung&#xae; Galaxy Tab), telephones, Smart phones (e.g., Apple&#xae; iPhone, Android-enabled device, Blackberry&#xae;), or personal digital assistants. The user can access the computer system <b>401</b> via the network <b>430</b>.</p><p id="p-0127" num="0126">Methods as described herein can be implemented by way of machine (e.g., computer processor) executable code stored on an electronic storage location of the computer system <b>401</b>, such as, for example, on the memory <b>410</b> or electronic storage unit <b>415</b>. The machine executable or machine readable code can be provided in the form of software. During use, the code can be executed by the processor <b>405</b>. In some cases, the code can be retrieved from the storage unit <b>415</b> and stored on the memory <b>410</b> for ready access by the processor <b>405</b>. In some situations, the electronic storage unit <b>415</b> can be precluded, and machine-executable instructions are stored on memory <b>410</b>.</p><p id="p-0128" num="0127">The code can be pre-compiled and configured for use with a machine having a processer adapted to execute the code, or can be compiled during runtime. The code can be supplied in a programming language that can be selected to enable the code to execute in a pre-compiled or as-compiled fashion.</p><p id="p-0129" num="0128">Aspects of the systems and methods provided herein, such as the computer system <b>401</b>, can be embodied in programming. Various aspects of the technology may be thought of as &#x201c;products&#x201d; or &#x201c;articles of manufacture&#x201d; typically in the form of machine (or processor) executable code and/or associated data that is carried on or embodied in a type of machine readable medium. Machine-executable code can be stored on an electronic storage unit, such as memory (e.g., read-only memory, random-access memory, flash memory) or a hard disk. &#x201c;Storage&#x201d; type media can include any or all of the tangible memory of the computers, processors or the like, or associated modules thereof, such as various semiconductor memories, tape drives, disk drives and the like, which may provide non-transitory storage at any time for the software programming. All or portions of the software may at times be communicated through the Internet or various other telecommunication networks. Such communications, for example, may enable loading of the software from one computer or processor into another, for example, from a management server or host computer into the computer platform of an application server. Thus, another type of media that may bear the software elements includes optical, electrical and electromagnetic waves, such as used across physical interfaces between local devices, through wired and optical landline networks and over various air-links. The physical elements that carry such waves, such as wired or wireless links, optical links or the like, also may be considered as media bearing the software. As used herein, unless restricted to non-transitory, tangible &#x201c;storage&#x201d; media, terms such as computer or machine &#x201c;readable medium&#x201d; refer to any medium that participates in providing instructions to a processor for execution.</p><p id="p-0130" num="0129">Hence, a machine readable medium, such as computer-executable code, may take many forms, including but not limited to, a tangible storage medium, a carrier wave medium or physical transmission medium. Non-volatile storage media include, for example, optical or magnetic disks, such as any of the storage devices in any computer(s) or the like, such as may be used to implement the databases, etc. shown in the drawings. Volatile storage media include dynamic memory, such as main memory of such a computer platform. Tangible transmission media include coaxial cables; copper wire and fiber optics, including the wires that comprise a bus within a computer system. Carrier-wave transmission media may take the form of electric or electromagnetic signals, or acoustic or light waves such as those generated during radio frequency (RF) and infrared (IR) data communications. Common forms of computer-readable media therefore include for example: a floppy disk, a flexible disk, hard disk, magnetic tape, any other magnetic medium, a CD-ROM, DVD or DVD-ROM, any other optical medium, punch cards paper tape, any other physical storage medium with patterns of holes, a RAM, a ROM, a PROM and EPROM, a FLASH-EPROM, any other memory chip or cartridge, a carrier wave transporting data or instructions, cables or links transporting such a carrier wave, or any other medium from which a computer may read programming code and/or data. Many of these forms of computer readable media may be involved in carrying one or more sequences of one or more instructions to a processor for execution.</p><p id="p-0131" num="0130">The computer system <b>401</b> can include or be in communication with an electronic display <b>435</b> that comprises a user interface (UI) <b>440</b> for providing, for example, a visual display indicative of training and testing of a trained algorithm; a visual display of image data indicative of a classification as normal, ambiguous, or suspicious; an identification of a subject as having a cancer; or an electronic report (e.g., diagnostic or radiological report) indicative of the cancer of the subject. Examples of UIs include, without limitation, a graphical user interface (GUI) and web-based user interface.</p><p id="p-0132" num="0131">Methods and systems of the present disclosure can be implemented by way of one or more algorithms. An algorithm can be implemented by way of software upon execution by the central processing unit <b>405</b>. The algorithm can, for example, train and test a trained algorithm; use the trained algorithm to process medical images to classify the image as normal, ambiguous, or suspicious; identify or monitor a cancer of the subject; and electronically output a report that indicative of the cancer of the subject.</p><heading id="h-0010" level="1">EXAMPLES</heading><heading id="h-0011" level="1">Example 1&#x2014;Improving Patient Care with Real-Time Radiology</heading><p id="p-0133" num="0132">Using systems and methods of the present disclosure, a real-time radiology screening and diagnostic workflow was performed on a plurality of patients. As an example, on the first day of the real-time radiology clinic, a patient received immediate results for a normal case, which resulted in the patient feeling relieved and reassured.</p><p id="p-0134" num="0133">As another example, on the next day of the real-time radiology clinic, another patient received a suspicious finding during a screening, and had a diagnostic follow-up performed for the suspicious finding within three hours. The patient was told by the radiologist that her findings were benign and that she is not suspected of having cancer. The patient was very relieved and happy to avoid the anxiety of waiting for a final diagnostic result. On average, such a process may take anywhere from 2 to 8 weeks in the U.S. Even in particular clinics with expedited workflows, the process may take 1 to 2 weeks without the assistance of real-time radiology.</p><p id="p-0135" num="0134">As another example, on another day of the real-time radiology clinic, the AI-based real-time radiology system detected a 3-mm breast cancer tumor, which was confirmed 5 days later by biopsy to be a cancer. <figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an example plot of detection frequency of breast cancer tumors of various sizes (ranging from 2 mm to 29 mm) that are detected by radiologists, in accordance with disclosed embodiments. The real-time radiology system may provide life-saving clinical impact, by reducing the time to treatment. The cancer may continue to grow until this patient receives her next screening or diagnostic procedure, at which time removal and treatment may have been more life threatening, painful, expensive, and have a lower success rate.</p><p id="p-0136" num="0135">As another example, of the real-time radiology clinic, a patient received a diagnostic follow-up procedure for a suspicious finding within 1 hour. A biopsy was needed, but was completed the next business day because the patient was on aspirin. The biopsy confirmed the cancer that was detected by the real-time radiology. The radiology work-up period was reduced from 8 business days to 1 day, and the time to diagnosis was reduced from 1 month to 1 week.</p><p id="p-0137" num="0136">The clinical impact of the real-time radiology system can be measured by screening mammography metrics, such as PPV1 and callback rate. The PPV1 generally refers to the percentage of examinations with an abnormal initial interpretation by a radiologist that result in a tissue diagnosis of cancer within 1 year. The callback rate generally refers to the percentage of examinations with an abnormal initial interpretation (e.g., &#x201c;recall rate&#x201d;). During a 6-week span, a real-time radiology clinic processed 796 patient cases using AI-based analysis, of which 94 cases were flagged to be read by radiologists in real time. A total of 4 cases were diagnosed as cancer, of which 3 cases were confirmed as cancer (e.g., by biopsy).</p><p id="p-0138" num="0137"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows an example plot of positive predictive values from screening mammography (PPV1) versus callback rate, in accordance with disclosed embodiments. The prospective study resulted in a callback rate of 11.8% with a PPV1 of 3.2%. In comparison, a median radiologist has a callback rate of 11.6% with a PPV1 of 4.4%.</p><p id="p-0139" num="0138"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an example plot comparing the interpretation time for reading images in AI-sorted batches (including Bi-RADS Assessment, and density) (left) and the percentage improvement in interpretation time versus controls who read randomly shuffled batches (right), across a first set of radiologist, a second set of radiologists, and the overall total set of radiologists, in accordance with disclosed embodiments. This figure shows that AI-powered workflows can improve radiologist productivity to a statistically significant extent (ranging from about 13% to 21%).</p><heading id="h-0012" level="1">Example 2&#x2014;Classification of Suspicious Findings in Screening Mammography with Deep Neural Networks</heading><p id="p-0140" num="0139">Deep learning may be applied to a variety of computer vision and image processing applications. For example, deep learning may be used to automatically learn image features relevant to a given task and may be used for various tasks from classification to detection to segmentation. Computational models based on deep neural networks (DNNs) may be developed and used in radiology applications, such as screening mammography, to identify suspicious, potentially abnormal, or high-risk lesions and increase radiologist productivity. In some cases, deep learning models are able to match or even surpass human-level performance. In addition, deep learning may be used to help raise the performance of general radiologists to be closer to that of breast imaging specialists. For example, general radiologists generally have poorer cancer detection rates and much higher recall rates compared to fellowship-trained breast radiologists.</p><p id="p-0141" num="0140">Deep learning can be used to perform interpretation of screening mammography, including distinguishing between malignant and benign findings. A DNN model is trained for this task to identify missed cancers or reduce the false positive callbacks, particularly for non-expert readers.</p><p id="p-0142" num="0141">The DNN model was trained using the publicly accessible Digital Database for Screening Mammography (DDSM) dataset (eng.usfedu/cvprg/Mammography/Database.html). DDSM includes 2,620 studies with over 10,000 digitized scanned film mammography images. The images were evenly split between normal mammograms and those with suspicious findings. The normal mammograms were confirmed through a four-year follow-up of the patient. The suspicious findings were further split between biopsy-proven benign findings (51%) and biopsy-proven malignant findings (49%). All cases with obviously benign findings that are not followed up by biopsy as part of routine clinical care were excluded from the dataset. As a result, distinguishing between benign and malignant findings may be more difficult for this dataset than in a typical clinical mammography screening scenario.</p><p id="p-0143" num="0142">The DDSM dataset was divided into subsets including a training dataset, a validation dataset, and a testing dataset. Using the training dataset, a DNN was trained to distinguish malignant findings from benign findings or a normal region of the breast. The datasets included annotations pointing out the locations of tumors in the images, which may be critical in guiding the deep learning process.</p><p id="p-0144" num="0143">The performance of the DNN on this binary classification task was evaluated on the testing dataset through the use of a receiver operating characteristic (ROC) curve (as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>). The DNN model was used to distinguish between malignant and benign findings with high accuracy, as indicated by the area under the ROC curve (AUC) of 0.89. In comparison, expert radiologists are typically able to achieve a sensitivity of 84.4% and a specificity of 90.8% for the task of cancer detection for screening mammography. The DNN model was used to distinguish between malignant and benign findings with a sensitivity of 79.2% and a specificity of 80.0% with the more challenging cases found in the DDSM dataset. The performance gap relative to expert radiologists is in part due to the relatively small size of the dataset, and may be mitigated by incorporating larger training datasets. Further, the DNN model may still be configured to outperform general radiologists in accuracy, sensitivity, specificity, AUC, positive predictive value, negative predictive value, or a combination thereof.</p><p id="p-0145" num="0144">A highly accurate DNN model was developed by training on a limited public benchmark dataset. While the dataset is perhaps more difficult than in the clinical setting, the DNN model was able to distinguish between malignant and benign findings with nearly human-level performance.</p><p id="p-0146" num="0145">A similar DNN model may be trained using the clinical mammography dataset of the Joanne Knight Breast Health Center in St. Louis, in partnership with Washington University in St. Louis. This dataset includes a large medical records database comprising more than 100 thousand patients, including 4 thousand biopsy-confirmed cancer patients, and over 400 thousand imaging sessions comprising 1.5 million images. The dataset may be manually or automatically labeled (e.g., by building annotations) to optimize the deep learning process. Since the performance of DNNs improves significantly with the size of the training dataset, this uniquely massive and rich dataset may lead to a DNN model having dramatic increases in sensitivity and specificity as compared to the DNN model trained on the DDSM data. Such highly accurate DNN models offer opportunities for transformative improvements in breast cancer screening, enabling all women to receive access to specialist-level care.</p><heading id="h-0013" level="1">Example 3&#x2014;Artificial Intelligence (AI)-Powered Radiology Clinics for Early Cancer Detection</heading><p id="p-0147" num="0146">Introduction</p><p id="p-0148" num="0147">Breast cancer is the most widespread cancer in women in the U.S., with over 250 thousand new diagnoses in 2017 alone. About 1 in 8 women will be diagnosed with breast cancer at some point during their lives. Despite improvements in treatment, over 40 thousand women die every year in the U.S. from breast cancer. Substantial progress has made in reducing breast cancer mortality (39% lower since 1989) in part due to the widespread adoption of screening mammography. Breast cancer screening can help identify early-stage cancers, which have much better prognoses and lower treatment costs as compared to late-stage cancers. This difference can be substantial: women with localized breast cancer have a 5-year survival rate of nearly 99%, while women with metastatic breast cancer have a 5-year survival rate of 27%.</p><p id="p-0149" num="0148">Despite these demonstrated benefits, only about half of women currently obtain mammograms at the rate recommended by the American College of Radiology. This low mammography utilization may result in a significant burden to patients and to healthcare systems in the form of worse outcomes and higher costs. Adoption rates for screening mammography are hindered, in part, by poor patient experience, such as long delays in obtaining an appointment, unclear pricing, long wait times to receive exam results, and confusing reports. Further, problems arising from a lack of transparency in pricing are exacerbated by large variations in costs among providers. Similarly, delivery times for receiving exam results are inconsistent among providers.</p><p id="p-0150" num="0149">In addition, significant variation in radiologist performance results in patients experiencing very different standards of care depending on location and income. For example, cancer detection rates are more than twice as high for radiologists in the 90th percentile compared with radiologists in the 10th percentile. False positive rates (e.g., the rate at which healthy patients are mistakenly recalled for follow-up exams) have even larger differences between these two groups. Aggregated across all screening exams done in the U.S., about 96% of patients who are called back are false positives. Given the huge societal and personal burden of cancer, combined with the often poor patient experience, inconsistent screening performance, and large cost variations, AI-based or AI-assisted screening approaches can be developed to significantly improve this clinical accuracy of mammography screening.</p><p id="p-0151" num="0150">Innovations in artificial intelligence and software can be leveraged toward achieving significant improvements to health outcomes, including early, accurate detection of cancer. These improvements may affect one or more steps within the patient journey&#x2014;from cost transparency, appointment scheduling, patient care, radiology workflow, diagnostic accuracy, results delivery, to follow-up. An AI-powered network of imaging centers may be developed to deliver high-quality service, timeliness, accuracy, and cost effectiveness. At such clinics, women may schedule a mammogram instantly, and receive a diagnosis of cancer within a single visit before they leave. The AI-powered clinics may enable the transformation of a traditional two-visit screening-diagnostic paradigm into a single visit, by using &#x201c;real-time radiology&#x201d; methods and systems of the present disclosure. Artificial intelligence may be used to customize the clinical workflow for each patient using a triage engine and to tailor how screening exams are read to significantly enhance radiologist accuracy (e.g., by reducing radiologist fatigue), thereby improving the accuracy of cancer detection. Additional improvements to the screening/diagnosis process can be achieved using AI-based or AI-assisted approaches, such as patient scheduling, improving screening guideline adherence through customer outreach, and the timeliness of report delivery with patient-facing applications. A self-improving system may use AI to build better clinics that generate the data to improve the AI-based system.</p><p id="p-0152" num="0151">A key component of creating the AI-powered radiology network is driving growth through patient acquisition. While other components of the system may streamline processes of a radiology workflow and provide patients with an improved and streamlined experience, patient recruitment and enrollment is important to collect sufficient data to train the AI-powered systems for high performance.</p><p id="p-0153" num="0152">Further, AI-powered clinics may reduce obstacles to screening mammography by improving the patient experience before the patients arrive at a clinic. This may include addressing two key barriers that limit adoption: (1) concerns about the cost of the exam and (2) lack of awareness about conveniently located clinics. When price and availability are completely opaque, as with conventional clinics, significant variations in price and service may exist, thereby creating a barrier to patients' scheduling of appointments.</p><p id="p-0154" num="0153">An AI-based user application may be developed to streamline the scheduling process and offer transparency for patients. The application may be configured to provide users with a map of clinics that accept their insurance as well as available times for appointments. For those with health insurance, screening mammograms, both 2D and 3D, are at no out-of-pocket cost. This, along with any potential costs that might be incurred, may be clearly indicated to the patient at the time of scheduling. Guarantees about the timeliness of exam results may also be presented to the patient, which addresses a potential source of anxiety for patients that may make them less likely to schedule an appointment.</p><p id="p-0155" num="0154">The application may be configured to confirm the patient's insurance and request the work order from the primary care provider (PCP), if necessary, during the scheduling process. The application may be configured to receive user input of pre-exam forms in order to more efficiently process patients during their visit to the clinic. If the patient has any remaining forms remaining to complete prior to the exam, she may be provided with a device at the time of check-in to the clinic, to complete the remaining forms. The application may be configured to facilitate electronic entry of these forms to reduce or eliminate the time-consuming and error-prone task of manually transcribing paper forms, as done under the current standard of care. By facilitating the user entry of paperwork prior to the exam date, the application enables the patient to have a more streamlined experience, and less time and resources are devoted to administrative tasks on-site.</p><p id="p-0156" num="0155">The patient's previously acquired mammograms may also be obtained prior to the exam. For images acquired at partnering clinics, this process may happen transparently to the patient. By obtaining the prior images before the visit, a potential bottleneck to immediate review of newly acquired images may be eliminated.</p><p id="p-0157" num="0156">After scheduling an appointment, the application may be configured to provide the patient with reminders about the upcoming exam in order to increase attendance. The application may also be configured to provide the patient with information about the exam procedures ahead of time, in order to minimize anxiety and to reduce time spent explaining the procedure within the exam room. Further, to develop relationships with primary care physicians (PCPs), referring physicians may be able to confirm that their patients have scheduled a mammography appointment. This will allow doctors to assess compliance and to encourage patients who do not sign up for an appointment in a timely manner following their recommendations.</p><p id="p-0158" num="0157">Real-Time Radiology System</p><p id="p-0159" num="0158">The conventional breast cancer screening paradigm may include significant delays that introduce anxiety of patients. This may reduce the number of women who elect to obtain this preventative care and put them at risk for discovering cancer later when it is more difficult to treat and more deadly. A typical patient may visit a clinic for a screening mammogram, spend about half an hour at the clinic, then leave. She may then wait up to 30 days for a phone call or letter to receive the news that there is a suspicious abnormality on the screening mammogram and that she should schedule a follow-up diagnostic appointment. Next, the patient may wait another week for that appointment, during which she may receive additional imaging to determine if a biopsy is required.</p><p id="p-0160" num="0159">The current paradigm is motivated by the volume of patients that are screened at larger practices (e.g., more than 100 patients per day). These imaging centers typically have at least a 1-2 day backlog of screening exams that needs to be read before the radiologists can process the screening mammograms that were performed on a given day. If any of those cases were to require a diagnostic work-up, that exam often cannot be done right away because of the high variance in the length of diagnostic exams (e.g., ranging from 20 to 120 minutes. Scheduling does not take this into account, leading to prolonged wait times for patients and inefficient workflows for technologists.</p><p id="p-0161" num="0160">Patients who received immediate real-time reading of their screening mammograms may experience significantly less anxiety than those who had not after 3 weeks. In contrast, women who received false positives at screening (normal cases flagged as suspicious) but received an immediate reading experienced nearly the same level of anxiety as women with normal mammograms. Most of these women did not perceive themselves as having an abnormal screen. Those that do, however, tend to seek more medical attention for breast-related concerns and other medical issues. Further, if women know they may leave the mammography clinic with the results of their mammograms, they may be more satisfied with the screening process and may be more likely to follow future screening recommendations. Such increased patient satisfaction may improve member retention among health plans. Additionally, immediate reading of suspicious cases may decrease the time to breast cancer diagnosis, thereby improving patient care and outcomes.</p><p id="p-0162" num="0161">In some cases, clinics are able to offer real-time service by restricting volume. Such clinics may schedule only a few patients at any given time so that, in case the need arises, the patients can immediately follow up the screening procedure with a diagnostic exam. This approach may be expensive, time-consuming, and not amenable to be performed at scale, meaning that most women may still need to wait weeks for potentially life-changing results. Roughly 4 million women may encounter such an unpleasant screening process every year.</p><p id="p-0163" num="0162">Using methods and systems of the present disclosure, an AI-based triage system may be developed for screening mammography.</p><p id="p-0164" num="0163">As screening exam images are received from the clinical imaging system, they may be processed by the AI-powered Triage Engine, which then stratifies the patient's case into one of a plurality of workflows. For example, the plurality of workflows may include two categories (e.g., normal and suspicious). As another example, the plurality of workflows may include three categories (e.g., normal, uncertain, and suspicious). Each of these categories may then be handled by a different set of dedicated radiologists, who are specialized to perform the workflow's particular set of responsibilities.</p><p id="p-0165" num="0164"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows an example of a schematic of patient flow through clinics with the AI-enabled real-time radiology system and patient mobile application (app), in accordance with disclosed embodiments. The patient begins by registering with a website or patient app. Next, the patient uses the patient app to schedule an appointment for radiology screening. Next, the patient uses the patient app to complete pre-examination forms. Next, the patient arrives at the clinic and receives the screening examination. Next, the AI-based radiology assessment is performed on the medical images obtained from the patient's screening examination. Next, the patient's images and examination results are provided to the patient through the patient app. Next, the patient reschedules an appointment, if needed or recommended, using the patient app. The screening examination process may then proceed as before.</p><p id="p-0166" num="0165"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows an example of a schematic of an AI-assisted radiology assessment workflow, in accordance with disclosed embodiments. First, a dataset comprising an electronic health record (EHR) and medical images of a patient are provided. Next, an AI-based triage engine processes the EHR and medical images to analyze and classify the dataset as likely normal, possibly suspicious, or likely suspicious. Next, a workflow distributor module distributes the patient's dataset to one of three workflows based on the classification of the dataset as likely normal, possibly suspicious, or likely suspicious: a normal workflow, an uncertain workflow, and a suspicious workflow, respectively. Each of the three workflows may comprise radiologist review or further AI-based analysis (e.g., by a trained algorithm).</p><p id="p-0167" num="0166">The majority of mammography screening exams may be classified into the normal category. By having a first set of radiologists focusing only on this workflow, the concept of &#x201c;batch reading&#x201d; and the value and productivity gains associated with it can be applied and extended. Since the cases handled by this first set of radiologists may be nearly all normal cases, there may be fewer context-switches and penalties caused by handling highly variable cases. With the AI-based system, reports may be automatically pre-populated, allowing radiologists to spend significantly more time interpreting images rather than writing reports. In the rare case where the radiologist disagrees with the AI assessment of a normal case and instead considers the case suspicious, such cases may be handled as usual and the patient may be scheduled for a diagnostic exam. These normal cases may be further sub-divided into even more homogeneous batches to achieve a productivity improvement by grouping cases that an AI-based system has determined to be similar. For example, batching all AI-determined dense breasts together or batching cases that are visually similar based on AI-derived features.</p><p id="p-0168" num="0167">A smaller fraction of mammography screening exams may be classified into the uncertain workflow. Such sessions may involve findings that the AI system does not classify as normal but that also do not meet the threshold for being outright suspicious. These may typically be the highly complex cases that require significantly more time per session for radiologist assessment as compared than those cases in the normal or suspicious workflow. For these reasons, it may be beneficial to have a separate second set of radiologists focus on performing this smaller volume of work, which has less homogeneity and potentially significantly more interpretation and reporting requirements. These radiologists may be more specialized in reading this difficult cases through more years of experience or training. This specialization may be made even more specific based on categories or features that the AI determines. For example, a group of radiologists may perform better than others at correctly assessing AI-determined tumor masses. Therefore, exams identified as such by the algorithm may be routed to this better suited group of specialists. In some cases, the second set of radiologists is the same as the first set of radiologists, but the radiological assessments of the different sets of cases are performed at different times based on a prioritization of the cases. In some cases, the second set of radiologists is a subset of the first set of radiologists.</p><p id="p-0169" num="0168">The smallest but most important portion of the mammography screening exams may be classified into the suspicious workflow. A third set of radiologists may be assigned to this role to effectively read these cases as their &#x201c;on-call&#x201d; obligations. Most of the radiologist's time may be spent performing scheduled diagnostic exams. However, in the downtime between exams, they may be alerted to any suspicious cases such that they may verify the diagnosis as soon as possible. These cases may be critical to handle efficiently so that the patients can begin their follow-up diagnostic exam as soon as possible. In some cases, the third set of radiologists is the same as the first or second set of radiologists, but the radiological assessments of the different sets of cases are performed at different times based on a prioritization of the cases. In some cases, the third set of radiologists is a subset of the first or second set of radiologists.</p><p id="p-0170" num="0169">In some cases, the workflow may comprise applying an AI-based algorithm to analyze a medical image to determine a difficulty of performing radiological assessment of the medical image, and then prioritizing or assigning the medical image to a set of radiologists (e.g., among a plurality of different sets of radiologists) for radiological assessment based on the determined degree of difficulty. For example, cases with low difficulty (e.g., more &#x201c;routine&#x201d; cases) may be assigned to a set of radiologists having relatively lower degree of skill or experience, while cases with higher difficulty (e.g., more suspicious or non-routine cases) may be assigned to a different set of radiologists having relatively higher degree of skill or experience (specialized radiologists). For example, cases with low difficulty (e.g., more &#x201c;routine&#x201d; cases) may be assigned to a first set of radiologists having relatively lower level of schedule availability, while cases with higher difficulty (e.g., more suspicious or non-routine cases) may be assigned to a different set of radiologists having relatively higher level of schedule availability.</p><p id="p-0171" num="0170">In some cases, the degree of difficulty may be measured by an estimated length of time required to fully assess the image (e.g., about 1 minute, about 2 minutes, about 3 minutes, about 4 minutes, about 5 minutes, about 6 minutes, about 7 minutes, about 8 minutes, about 9 minutes about 10 minutes, about 15 minutes, about 20 minutes, about 25 minutes, about 30 minutes, about 40 minutes, about 50 minutes, about 60 minutes, or more than about 60 minutes. In some cases, the degree of difficulty may be measured by an estimated degree of concordance or agreement of radiological assessment of the medical image across a plurality of independent radiological assessments (e.g., performed by different radiologists or by the same radiologist on different days). For example, the estimated degree of concordance or agreement of radiological assessment may be about 50%, about 55%, about 60%, about 65%, about 70%, about 75%, about 80%, about 85%, about 90%, about 95%, about 96%, about 97%, about 98%, about 99%, or more than about 99%. In some cases, the degree of difficulty may be measured by a desired level of education, experience, or expertise of the radiologist (e.g., less than about 1 year, about 1 year, between 1 and 2 years, about 2 years, between 2 and 3 years, about 3 years, between 3 and 4 years, about 4 years, between 4 and 5 years, about 5 years, between 5 and 6 years, about 6 years, between 6 and 7 years, about 7 years, between 7 and 8 years, about 8 years, between 8 and 9 years, about 9 years, between 9 and 10 years, about 10 years, or more than about 10 years). In some cases, the degree of difficulty may be measured by an estimated sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), or accuracy of the radiological assessment (e.g., about 50%, about 55%, about 60%, about 65%, about 70%, about 75%, about 80%, about 85%, about 90%, about 95%, about 96%, about 97%, about 98%, about 99%, or more than about 99%).</p><p id="p-0172" num="0171">In some cases, the workflow may comprise applying an AI-based algorithm to analyze a medical image to determine a categorization of the medical image, and then prioritizing or assigning the medical image to a set of radiologists (e.g., among a plurality of different sets of radiologists) for radiological assessment based on the determined categorization of the medical image. For example, a set of cases having similar characteristics may be categorized together and assigned to the same radiologist or set of radiologists, thereby achieving a reduction in context switching and an increase in efficiency and accuracy. Similar characteristics may be based on, for example, location of a body where an ROI occurs, a density of tissue, a BIRADS score, etc. In some cases, workflow may comprise applying an AI-based algorithm to analyze a medical image to determine a lesion type of the medical image, and then prioritizing or assigning the medical image to a set of radiologists (e.g., among a plurality of different sets of radiologists) for radiological assessment based on the determined lesion type of the medical image.</p><p id="p-0173" num="0172">In some cases, the workflow may comprise allowing radiologists to assign cases to themselves via a market-based system, whereby each case is assessed by an AI-based algorithm to determine an appropriate price or cost of the radiological assessment. Such a price or cost may be a determined relative value unit to be compensated to each radiologist upon completion of the radiological assessment. For example, each radiological assessment of a case may be priced based on determined characteristics (e.g., difficulty, length of examination time). In such a workflow, cases may not be assigned to radiologists, thereby avoiding the issue of radiologists who choose relatively routine or easy to obtain a high rate of reimbursement per case.</p><p id="p-0174" num="0173">In some cases, the workflow may comprise assigning cases to a radiologist based on an assessed performance of the radiologist (e.g., prior sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), accuracy, or efficiency of the radiologist in performing radiological assessments). Such performance may be determined or refined based on assigning control cases (e.g., positive or negative control cases) to the radiologist in a blinded manner to ensure quality control. For example, radiologists with better performance may be assigned a higher volume of cases or cases with higher value or compensation. By defining these distinct roles for a given radiologist (e.g., for any given day), each workflow can be individually optimized for task-specific needs. The AI-driven triage engine may allow real-time radiology to be delivered to patients at scale. The system may also enable dynamic allocation of cases based on expertise. For example, fellowship-trained breast imagers may be of the most value in the uncertain workflow, where their superior experience may be leveraged. Moreover, we can perform cross-clinic interpretation of screens across a network of clinics can be performed to ensure effective utilization of radiologists' time regardless of any individual clinic's staffing or patient base.</p><p id="p-0175" num="0174">Report delivery may be performed as follows. The Mammography Quality Standards Act (MQSA) mandates that all patients receive a written lay person's summary of their mammography report directly. This report must be sent within 30 days of the mammogram. Verbal results are often used to expedite care and alleviate anxiety, but they must be supported by written reports. Reports can be mailed, sent electronically, or handed to the patient. Typically, clinics may use paper mail to deliver reports to their patients. The AI-based clinic may deliver mammography reports electronically via the patient application. The source images may also be made available electronically, so that the patient may easily obtain and transfer the information to other clinics. Patients in the real-time radiology workflow may receive a screening and diagnostic report immediately before leaving the clinic.</p><p id="p-0176" num="0175">Timely reporting of screening results may be critical to patient satisfaction. Waiting more than two weeks for results and not being able to get in touch with someone to answer questions have been cited as key contributing reasons for patient dissatisfaction (which may in return decrease future screening rates). This system may ensure that a patient does not accidentally receive the wrong report, and that patients do not have uncertainty about when may receive their results come.</p><p id="p-0177" num="0176">The AI-based system may be continually trained as follows. As the clinical practice is operated, new data is continually collected and used to further train and refine the AI system, thereby further improving the quality of care and enabling new improvements to the patient experience. Each patient exam provides the system with an annotated, and possibly biopsy-proven, example to add to the dataset. In particular, the workflow of the real-time radiology system facilitates prioritizing the capture of high-value cases. The identification of false positives and false negatives (truly suspicious cases not flagged) may be crucial for enhancing the system's performance by providing challenging examples with high instructive value. Even cases that are classified correctly (e.g., with respect to the radiologist's review as the ground truth) may provide useful feedback. Incorporating these cases in the training data set may provide the system with a valuable source of information for uncertainty calibration, which ensures that the confidence values produced by the AI-based system are accurate. This may drastically increase the overall robustness and, in turn, trust in the system. By improving the end-to-end patient workflow and maintaining a radiologist in the loop, the AI-based clinical system may automatically discover the important pieces of information outlined above. The resulting system may be always improving and always providing high-quality patient care and radiological assistance.</p><p id="p-0178" num="0177">The AI-powered mammography screening clinics can provide patients with high-quality service and accuracy throughout the screening process. Patients may be able to walk into a clinic, receive a screening for cancer, receive any needed follow-up work, and leave with their diagnosis in hand, thereby completing the entire screening and diagnosis process during the course of a single visit with immediate results. The patient application may be configured to provide price transparency, hassle-free scheduling, error-free form filling, and instantaneous delivery of reports and images, thereby improving the ease, stress, and efficiency of the patient screening process.</p><p id="p-0179" num="0178">The radiologists may be able to provide more accurate and more productive results by employing a specialized set of normal, uncertain, and suspicious (or alternative categorization based on an AI assessment of the images) workflows orchestrated by the AI triage engine. Clinicians may become more capable as the AI system learns and augments their abilities. AI-based or AI-assisted mammography may be delivered to a large population scale with low cost and high efficiency, thereby enhancing the cancer screening process and patient outcomes.</p><heading id="h-0014" level="1">Example 4&#x2014;Real-Time Radiology in Breast Cancer Screening Mammography when Coupled with Artificial Intelligence Technologies</heading><p id="p-0180" num="0179">A software system is developed that is configured to prioritize suspicious screening mammograms for immediate review by radiologists, thereby reducing the time to diagnostic follow-up. The software system is developed with a goal of significantly reducing patient anxiety as well as the overall time to treatment, by shortening the review times for suspicious mammography cases. Reductions in the wait time, which may often be up to about 2-4 weeks between the first and second evaluations, may be expected to extend the life expectancy of those subjects who are actually positive for breast cancer. An additional potential benefit is that the software may reduce the likelihood of missing some cancers.</p><p id="p-0181" num="0180">In some studies, women who are false positives at screening (normal cases flagged as suspicious, BIRADS 0) but receive immediate follow-up may experience nearly the same level of anxiety as women with normal diagnoses. Many of these women may not even perceive themselves as having an abnormal screening result. Therefore, immediate follow-up care may mitigate potential anxiety caused by a false-positive screening result.</p><p id="p-0182" num="0181">On the other hand, women who receive false-positives screening results and are called back for a follow-up diagnostic exam days or weeks later, may seek more medical attention for breast-related concerns and other medical issues. Therefore, women who are able to receive definitive mammography results during the same clinical visit as the mammography scan may be more likely to be satisfied with the screening experience and to have high compliance rates with future screening recommendations.</p><p id="p-0183" num="0182">However, many breast imaging centers may be unable to deliver immediate follow-up exams. This can be due to several challenges including scheduling constraints, timeliness of receiving prior evaluations from other institutions, and productivity loss due to reading each exam immediately after it is acquired. Perhaps most critically, reading several breast screening cases in a batch significantly improves the evaluation accuracy of the reader. This motivates waiting until a large enough batch of cases has been collected before reading an exam, making it impossible to provide immediate results and follow-up examination to a patient if indicated.</p><p id="p-0184" num="0183">Machine learning-based methods are employed to evaluate suspicious findings in mammography and tomosynthesis images. A triage software system is developed using machine learning for screening mammography to enable more timely report delivery and follow-up for suspicious cases (e.g., as performed in a batch reading setting) (as shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>). The medical images are fed into a real-time radiology system for processing. An AI-based triage engine of the real-time radiology system processes the medical images to classify the images as suspicious or not suspicious (e.g., normal or routine). If an image is classified as suspicious by the AI-based triage engine, then the image is sent for immediate radiologist review (e.g., during the same visit or same day as the initial screening appointment). The immediate radiologist review may result in a confirmation of a suspicious case (which results in an immediate diagnostic exam being ordered) or a reversal of the suspicious case (which results in the next scheduled routine annual screening being performed). If an image is classified as not suspicious (e.g., normal or routine) by the AI-based triage engine, then the image is sent for routine radiologist review. The routine radiologist review may result in an assessment of the case being suspicious (which results in a routine diagnostic exam being ordered) or a confirmation of the case as not being suspicious (which results in the next scheduled routine annual screening being performed).</p><p id="p-0185" num="0184">This software enables high-volume breast screening clinics to deliver same-day or same-visit diagnostic follow-up imaging to patients with abnormal-appearing mammography results. Leveraging such rapid diagnostic follow-up imaging can pave the way for breast imaging clinics to deliver the highest accuracy with the highest level of service and to significantly reduce patient anxiety.</p><p id="p-0186" num="0185">Using these machine learning-based approaches, the time-to-treatment of true tumors is reduced so that the patient has an increased probability of a longer lifespan as compared to those patients who are not evaluated by AI and who do not receive the follow-up diagnostic evaluation on the same day.</p><p id="p-0187" num="0186">The machine learning-based approach to evaluate suspicious findings in mammography and tomsynthesis images confers several advantages and objectives as follows. First, the time from initial screening exam to the delivery of diagnostic imaging results is reduced (potentially significantly) for breast cancer screening, and the likelihood of an accurate diagnosis is improved. For example, such diagnoses may be produced with greater sensitivity, specificity, positive predictive value, negative predictive value, area under the receiver operator characteristic (AUROC), or a combination thereof. Second, the approaches that combine radiologists with artificial intelligence may effectively improve the speed and/or quality of the initial evaluation. Third, more advanced diagnostic exams (e.g., additional X-ray based imaging, ultrasound imaging, another type of medical imaging, or a combination thereof) may be completed within a short period (e.g., within 60 minutes) of when the patient receives his or her screening results. Fourth, such methods may advantageously result in improvement in patient satisfaction that is attributable to the more timely delivery of results and follow-up imaging.</p><p id="p-0188" num="0187">Methods</p><p id="p-0189" num="0188">A clinical workflow is optimized to deliver a higher level of service to patients. As more patients and data are collected into training datasets, the machine learning algorithm continuously improves in the accuracy (or sensitivity, specificity, positive predictive value, negative predictive value, AUROC, or a combination thereof) of its computer aided diagnosis.</p><p id="p-0190" num="0189">Computer algorithms and software are developed to automatically classify breast screening images into probably abnormal and normal categories with a high degree of accuracy. Such software can enable high-volume breast screening clinics to deliver same-day or same-visit diagnostic follow-up imaging to patients with abnormal-appearing initial screening results. This will also require evaluating changes to clinical operations, in particular how screening cases are read and how the second diagnostic evaluation can be performed, within 60 minutes of the initial test.</p><p id="p-0191" num="0190">A rapid screening approach is implemented for all patients at a breast screening clinic. About 10% of the patients who are screened have suspicious results and are subsequently recommended for a diagnostic exam to be performed on the same day or during the same visit. The rapid turn-around time of the screening result and follow-up diagnostic exam are enabled by careful coordination between radiologists, clinical staff, and patients in the clinical environment. As more information is collected, the machine learning that is trained with increasingly larger training datasets yields a higher level of accuracy in detecting suspicious mammography scans.</p><p id="p-0192" num="0191">As the acquisitions of screening exams are completed, the images are sent to a router, received by the software, and rapidly classified (e.g., within about one minute). If the screening is marked by the machine learning algorithm as probably normal, then the patient ends her visit and exits the clinic as usual. However, if the screening is flagged by the machine learning algorithm as probably abnormal, then the patient will be asked to wait for up to about 10 minutes while the case is immediately reviewed by the radiologist (as shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>).</p><p id="p-0193" num="0192">Assuming that a given clinic screens about 30 patients per day and a 10% rate of possible positives, about 3 patients per day are typically be found positive by the machine learning algorithm and would be designated as eligible for real-time diagnostic follow-up after review by the radiologist (e.g., usually additional tomosynthesis imaging and possibly an ultrasound exam).</p><p id="p-0194" num="0193">Several metrics are used to demonstrate the effectiveness of the real-time radiology methods and systems. First, the change in the time it takes between a patient's initial screening exam and the delivery of diagnostic imaging results under the routine workflow and the proposed real-time workflow may be measured, in order to capture both changes in the latency of when a screening a case is reviewed, as well as logistics like mailing letters and appointment scheduling.</p><p id="p-0195" num="0194">Second, the real-time radiology model is evaluated continuously (e.g., on a monthly basis) based on the latest data collected. For example, the parameters of the computer vision algorithm are tuned and altered to improve its accuracy for the upcoming subsequent time period of screenings (e.g., one month). The effectiveness of the changes to the computer program are evaluated on a blinded test dataset of hundreds of representative exams and from the interim results from the subsequent time period of screenings.</p><p id="p-0196" num="0195">Third, patient satisfaction surveys are reviewed periodically to help determine how operational processes may be improved to better enable follow-up diagnostic examination within a short period of time (e.g., about 60 minutes).</p><p id="p-0197" num="0196">The following data may be collected for each patient who undergoes a mammographic screening/diagnostic assessment via the real-time radiology workflow: patient demographics (e.g., age, race, height, weight, socioeconomic background, smoking status, etc.), patient imaging data (e.g., acquired by mammography), patient outcomes (e.g., BIRADS for screening and diagnostic exams and biopsy pathology results, where applicable), patient visit event time stamps, patient callback rate for batch-read and real-time cases, and radiologist interpretation time for screening and diagnostic cases.</p><p id="p-0198" num="0197">Using methods and systems of the present disclosure, real-time radiology may be performed with potential benefits including: detecting a tumor that may not have otherwise have been recognized (or may only be recognized until the tumor has progressed), a reduced time to treatment, an improved longevity of the patient due to recognition and treatment compared to traditional evaluation process, and reduced patient anxiety since the waiting time between testing has been eliminated.</p><heading id="h-0015" level="1">Example 5&#x2014;A Multi-Site Study of a Breast Density Deep Learning Model for Full-Field Digital Mammography and Digital Breast Tomosynthesis Exams</heading><p id="p-0199" num="0198">Abstract</p><p id="p-0200" num="0199">Deep learning (DL) models hold promise for mammographic breast density estimation, but performance can be hindered by limited training data or image differences that can occur across clinics. Digital breast tomosynthesis (DBT) exams are increasingly becoming the standard for breast cancer screening and breast density assessment, but much more data is available for full-field digital mammography (FFDM) exams. A breast density DL model was developed in a multi-site setting for synthetic 2D mammography (SM) images derived from 3D DBT exams using FFDM images and limited SM data. A DL model was trained to predict Breast Imaging Reporting and Data System (BI-RADS) breast density using FFDM images acquired from 2008 to 2017 (Site 1: 57492 patients, 750752 images) for a retrospective study. The FFDM model was evaluated on SM datasets from two institutions (Site 1: 3842 patients, 14472 images; Site 2: 7557 patients, 63973 images). Adaptation methods were investigated to improve performance on the SM datasets and the effect of dataset size on each adaptation method was considered. Statistical significance was assessed through use of confidence intervals, and estimated by bootstrapping. Even without adaptation, the model demonstrated close agreement with the original reporting radiologists for all three datasets (Site 1 FFDM: linearly-weighted &#x3ba;w=0.75, 95% confidence interval (CI): [0.74, 0.76]; Site 1 SM: &#x3ba;w=0.71, CI: [0.64, 0.78]; Site 2 SM: &#x3ba;w=0.72, CI: [0.70, 0.75]). With adaptation, performance improved for Site 2 (Site 1: &#x3ba;w=0.72, CI: [0.66, 0.79], Site 2: &#x3ba;w=0.79, CI: [0.76, 0.81]) by use of only 500 SM images. These results establish that the BI-RADS breast density DL model demonstrated a high-level of performance on FFDM and SM images from two institutions by the use of methods requiring no or few SM images.</p><p id="p-0201" num="0200">A multisite study was performed to develop a breast density deep learning model for full-field digital mammography and synthetic mammography, as described by, for example, Matthews et al., &#x201c;A Multisite Study of a Breast Density Deep Learning Model for Full-Field Digital Mammography and Synthetic Mammography,&#x201d; Radiology: Artificial Intelligence, doi.org/10.1148/ryai.2020200015, which is incorporated by reference herein in its entirety.</p><p id="p-0202" num="0201">Introduction</p><p id="p-0203" num="0202">Breast density is an important risk factor for breast cancer, and areas of higher density can mask findings within mammograms leading to lower sensitivity. In some states, clinics are required to inform women of their density. Radiologists typically assess breast density using the Breast Imaging Reporting and Data System (BI-RADS) lexicon, which divides breast density into four categories: almost entirely fatty, scattered areas of fibroglandular density, heterogeneously dense, and extremely dense (as shown in <figref idref="DRAWINGS">FIGS. <b>12</b>A-<b>12</b>D</figref>). Unfortunately, radiologists exhibit intra- and inter-reader variability in the assessment of BI-RADS breast density, which can result in differences in clinical care and estimated risk.</p><p id="p-0204" num="0203"><figref idref="DRAWINGS">FIGS. <b>12</b>A-<b>12</b>D</figref> show examples of synthetic 2D mammography (SM) images derived from digital breast tomosynthesis (DBT) exams for each of the four Breast Imaging Reporting and Data System (BI-RADS) breast density categories: (A) almost entirely fatty (<figref idref="DRAWINGS">FIG. <b>12</b>A</figref>), (B) scattered areas of fibroglandular density (<figref idref="DRAWINGS">FIG. <b>12</b>B</figref>), (C) heterogeneously dense (<figref idref="DRAWINGS">FIG. <b>12</b>C</figref>), and (D) extremely dense (<figref idref="DRAWINGS">FIG. <b>12</b>D</figref>). Images are normalized so that the grayscale intensity windows found in their Digital Imaging and Communications in Medicine (DICOM) headers range from 0.0 to 1.0.</p><p id="p-0205" num="0204">Deep learning (DL) may be employed to assess BI-RADS breast density for both film and full-field digital mammography (FFDM) images, with some models demonstrating closer agreement with consensus estimates than individual radiologists. To realize the promise of using these DL models in clinical practice, two key challenges must be met. First, as breast cancer screening is increasingly moving to digital breast tomosynthesis (DBT) due to improved reader performance, DL models may need to be compatible with DBT exams. <figref idref="DRAWINGS">FIGS. <b>13</b>A-<b>13</b>D</figref> show the differences in image characteristics between 2D images for FFDM and DBT exams. However, the relatively recent adoption of DBT at many institutions means that the datasets available for training DL models are often fairly limited for DBT exams compared with FFDM exams. Second, DL models may need to offer consistent performance across sites, where differences in imaging technology, patient demographics, or assessment practices can impact model performance. To be practical, this may need to be achieved while requiring little additional data from each site.</p><p id="p-0206" num="0205"><figref idref="DRAWINGS">FIGS. <b>13</b>A-<b>13</b>D</figref> show a comparison between a full-field digital mammography (FFDM) image (<figref idref="DRAWINGS">FIG. <b>13</b>A</figref>) and a synthetic 2D mammography (SM) image (<figref idref="DRAWINGS">FIG. <b>13</b>B</figref>) of the same breast of a subject under the same compression; and a zoomed-in region, whose original location is denoted by the white box, both the FFDM image (<figref idref="DRAWINGS">FIG. <b>13</b>C</figref>) and the SM image (<figref idref="DRAWINGS">FIG. <b>13</b>D</figref>) to highlight the differences in texture and contrast that can occur between the two image types. Images are normalized so that the grayscale intensity windows found in their Digital Imaging and Communications in Medicine (DICOM) headers range from 0.0 to 1.0.</p><p id="p-0207" num="0206">A BI-RADS breast density DL model was developed that offers close agreement with the original reporting radiologists for both FFDM and DBT exams at two institutions. A DL model was first trained to predict BI-RADS breast density using a large-scale FFDM dataset from one institution. Then, the model was evaluated on a test set of FFDM exams as well as synthetic 2D mammography (SM) images generated as part of DBT exams (C-View, Hologic, Inc., Marlborough, Mass.), acquired from the same institution and from a separate institution. Adaptation techniques, requiring few SM images, were explored to improve performance on the two SM datasets.</p><p id="p-0208" num="0207">Materials and Methods</p><p id="p-0209" num="0208">The retrospective study was approved by an institutional review board for each of the two sites where data were collected (Site 1: internal institutional review board, Site 2: Western Institutional Review Board). Informed consent was waived and all data were handled according to the Health Insurance Portability and Accountability Act.</p><p id="p-0210" num="0209">Datasets were collected from two sites: Site 1, an academic medical center located in the mid-western region of the United States, and Site 2, an out-patient radiology clinic located in northern California. For Site 1, 191,493 mammography exams were selected (FFDM: n=187,627; SM: n=3,866). The exams were read by one of 11 radiologists with breast imaging experience. For Site 2, 16283 exams were selected. The exams were read by one of 12 radiologists with breast imaging experience ranging from 9 to 41 years. The BI-RADS breast density assessments of the radiologists were obtained from each site's mammography reporting software (Site 1: Magview version 7.1, Magview, Burtonsville, Md.; Site 2: MRS version 7.2.0; MRS Systems Inc. Seattle, Wash.). To facilitate development of our DL models, patients were randomly selected for training (FFDM: 50700, 88%; Site 1 SM: 3169, 82%; Site 2 SM: 6056, 80%), validation (FFDM: 1832, 3%; Site 1 SM: 403, 10%; Site 2 SM: 757, 10%), or testing (FFDM: 4960, 9%; Site 1 SM: 270, 7%; Site 2 SM: 744, 10%) purposes. All exams with a BI-RADS breast density assessment were included. For the test sets, exams were required to have all four standard screening mammography images (the mediolateral oblique and craniocaudal views of the left and right breasts). The distribution of the BI-RADS breast density assessments for each set are shown in Table 1 (Site 1) and Table 2 (Site 2).</p><p id="p-0211" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="357pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 1</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Description of the Site 1 full-field digital mammography (FFDM) and synthetic 2D mammography</entry></row><row><entry>(SM) training (train), validation (val), and test (test) datasets. The total number of patients,</entry></row><row><entry>exams, and images are given for each dataset. The number of images for the four Breast Imaging</entry></row><row><entry>Reporting and Data System (BI-RADS) breast density categories are also provided.</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="7"><colspec colname="offset" colwidth="42pt" align="left"/><colspec colname="1" colwidth="56pt" align="center"/><colspec colname="2" colwidth="56pt" align="center"/><colspec colname="3" colwidth="56pt" align="center"/><colspec colname="4" colwidth="49pt" align="center"/><colspec colname="5" colwidth="49pt" align="center"/><colspec colname="6" colwidth="49pt" align="center"/><tbody valign="top"><row><entry/><entry>FFDM Train</entry><entry>FFDM Val</entry><entry>FFDM Test</entry><entry>SM Train</entry><entry>SM Val</entry><entry>SM Test</entry></row><row><entry/><entry namest="offset" nameend="6" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="7"><colspec colname="1" colwidth="42pt" align="left"/><colspec colname="2" colwidth="56pt" align="char" char="."/><colspec colname="3" colwidth="56pt" align="char" char="."/><colspec colname="4" colwidth="56pt" align="char" char="."/><colspec colname="5" colwidth="49pt" align="char" char="."/><colspec colname="6" colwidth="49pt" align="char" char="."/><colspec colname="7" colwidth="49pt" align="char" char="."/><tbody valign="top"><row><entry>Patients</entry><entry>50700</entry><entry>1832</entry><entry>4960</entry><entry>3169</entry><entry>403</entry><entry>270</entry></row><row><entry>Exams</entry><entry>168208</entry><entry>6157</entry><entry>13262</entry><entry>3189</entry><entry>407</entry><entry>270</entry></row><row><entry>Images</entry><entry>672704</entry><entry>25000</entry><entry>53048</entry><entry>11873</entry><entry>1519</entry><entry>1080</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="13"><colspec colname="1" colwidth="42pt" align="left"/><colspec colname="2" colwidth="28pt" align="right"/><colspec colname="3" colwidth="28pt" align="left"/><colspec colname="4" colwidth="28pt" align="right"/><colspec colname="5" colwidth="28pt" align="left"/><colspec colname="6" colwidth="28pt" align="right"/><colspec colname="7" colwidth="28pt" align="left"/><colspec colname="8" colwidth="21pt" align="right"/><colspec colname="9" colwidth="28pt" align="left"/><colspec colname="10" colwidth="21pt" align="right"/><colspec colname="11" colwidth="28pt" align="left"/><colspec colname="12" colwidth="21pt" align="right"/><colspec colname="13" colwidth="28pt" align="left"/><tbody valign="top"><row><entry>BI-RADS A</entry><entry>80459</entry><entry>(12.0%)</entry><entry>3465</entry><entry>(13.9%)</entry><entry>4948</entry><entry>(9.3%)</entry><entry>1160</entry><entry>(9.8%)</entry><entry>154</entry><entry>(10.1%)</entry><entry>96</entry><entry>(8.9%)</entry></row><row><entry>BI-RADS B</entry><entry>348878</entry><entry>(51.9%)</entry><entry>12925</entry><entry>(51.7%)</entry><entry>27608</entry><entry>(52.0%)</entry><entry>6121</entry><entry>(51.6%)</entry><entry>771</entry><entry>(50.8%)</entry><entry>536</entry><entry>(49.6%)</entry></row><row><entry>BI-RADS C</entry><entry>214465</entry><entry>(33.9%)</entry><entry>7587</entry><entry>(30.3%)</entry><entry>18360</entry><entry>(34.6%)</entry><entry>3901</entry><entry>(32.9%)</entry><entry>530</entry><entry>(33.6%)</entry><entry>388</entry><entry>(35.9%)</entry></row><row><entry>BI-RADS D</entry><entry>28902</entry><entry>(4.3%)</entry><entry>1023</entry><entry>(4.1%)</entry><entry>2132</entry><entry>(4.0%)</entry><entry>691</entry><entry>(5.8%)</entry><entry>84</entry><entry>(5.5%)</entry><entry>60</entry><entry>(5.6%)</entry></row><row><entry namest="1" nameend="13" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0212" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 2</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Description of the Site 2 synthetic 2D mammography (SM) training</entry></row><row><entry>(train), validation (val), and test (test) datasets. The total number</entry></row><row><entry>of patients, exams, and images are given for each dataset. The number</entry></row><row><entry>of images for the four Breast Imaging Reporting and Data System</entry></row><row><entry>(BI-RADS) breast density categories are also provided.</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="offset" colwidth="49pt" align="left"/><colspec colname="1" colwidth="63pt" align="center"/><colspec colname="2" colwidth="56pt" align="center"/><colspec colname="3" colwidth="49pt" align="center"/><tbody valign="top"><row><entry/><entry>Train</entry><entry>Val</entry><entry>Test</entry></row><row><entry/><entry namest="offset" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="49pt" align="left"/><colspec colname="2" colwidth="63pt" align="char" char="."/><colspec colname="3" colwidth="56pt" align="char" char="."/><colspec colname="4" colwidth="49pt" align="char" char="."/><tbody valign="top"><row><entry>Patients</entry><entry>6056</entry><entry>757</entry><entry>744</entry></row><row><entry>Exams</entry><entry>13061</entry><entry>1674</entry><entry>1548</entry></row><row><entry>Images</entry><entry>51241</entry><entry>6540</entry><entry>6192</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="7"><colspec colname="1" colwidth="49pt" align="left"/><colspec colname="2" colwidth="28pt" align="right"/><colspec colname="3" colwidth="35pt" align="left"/><colspec colname="4" colwidth="21pt" align="right"/><colspec colname="5" colwidth="35pt" align="left"/><colspec colname="6" colwidth="21pt" align="right"/><colspec colname="7" colwidth="28pt" align="left"/><tbody valign="top"><row><entry>BI-RADS A</entry><entry>7866</entry><entry>(15.4%)</entry><entry>865</entry><entry>(13.2%)</entry><entry>948</entry><entry>(15.3%)</entry></row><row><entry>BI-RADS B</entry><entry>20731</entry><entry>(40.5%)</entry><entry>2719</entry><entry>(41.6%)</entry><entry>2612</entry><entry>(42.2%)</entry></row><row><entry>BI-RADS C</entry><entry>15706</entry><entry>(30.7%)</entry><entry>2139</entry><entry>(32.7%)</entry><entry>1868</entry><entry>(30.2%)</entry></row><row><entry>BI-RADS D</entry><entry>6938</entry><entry>(13.5%)</entry><entry>817</entry><entry>(12.5%)</entry><entry>764</entry><entry>(12.3%)</entry></row><row><entry namest="1" nameend="7" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0213" num="0210">The two sites serve different patient populations. The patient cohort from Site 1 is 59% Caucasian (34192/58397), 23% African American (13201/58397), 3% Asian (1630/58397), and 1% Hispanic (757/58397) while Site 2 is 58% Caucasian (4350/7557), 1% African American (110/7557), 21% Asian (1594/7557), and 7% Hispanic (522/7557).</p><p id="p-0214" num="0211">Deep Learning Model</p><p id="p-0215" num="0212">The DL model and training procedure were implemented using the pytorch DL framework (pytorch.org, version 1.0), which comprises a deep neural network model. The base model architecture comprised a pre-activation Resnet-34, where the batch normalization layers were replaced with group normalization layers. The model was configured to process as input a single image, corresponding to one of the views from a mammography exam, and produce estimated probabilities that the image is of a breast belonging to each of the BI-RADS breast density categories.</p><p id="p-0216" num="0213">The deep learning (DL) model was trained using the full-field digital mammography (FFDM) dataset (as shown in Table 1) by use of the Adam optimizer with a learning rate of 10<sup>&#x2212;4 </sup>and a weight decay of 10<sup>&#x2212;3</sup>. Weight decay not was applied to the parameters belonging to the normalization layers. The input was resized to 416&#xd7;320 pixels, and the pixel intensity values were normalized so that the grayscale window denoted in the Digital Imaging and Communications in Medicine (DICOM) header ranged from 0.0 to 1.0. Training was performed using mixed precision and gradient checkpointing with batch sizes of 256 distributed across two NVIDIA GTX 1080 Ti graphics processing units (Santa Clara, Calif.). Each batch was sampled such that the probability of selecting a BI-RADS B or BI-RADS C sample was four times that of selecting a BI-RADS A or BI-RADS D sample, which roughly corresponds to the distribution of densities observed in the U.S. Horizontal and vertical flipping were employed for data augmentation. In order to obtain more frequent information on the training progress, epochs were capped at 100 thousand samples compared with a total training set size of over 672 thousand samples. The model was trained for 100 such epochs. Results are reported for the epoch that had the lowest cross entropy loss on the validation set, which occurred after 93 epochs.</p><p id="p-0217" num="0214">The parameters for the vector and matrix calibration methods were chosen by minimizing a cross-entropy loss function by use of the BFGS optimization method (scipy.org, version 1.1.0). The parameters were initialized such that the linear layer corresponded to the identity transformation. Training was stopped when the L2 norm of the gradient was less than 10<sup>&#x2212;6 </sup>or when the number of iterations exceeded 500. Retraining the last fully-connected layer for the fine-tuning method was performed by use of the Adam optimizer with a learning rate of 10&#x2032; and weight decay of 10<sup>&#x2212;5</sup>. The batch size was set to 64. The fully-connected layer was trained from random initialization for 100 epochs, and results were reported for the epoch with the lowest validation cross entropy loss. Training from scratch on the synthetic 2D mammography (SM) datasets was performed following the same procedure as for the base model. For fine-tuning and training from scratch, the size of an epoch was set to the number of training samples.</p><p id="p-0218" num="0215">Domain Adaptation</p><p id="p-0219" num="0216">Domain adaptation was performed to take a model trained on a dataset from one domain (source domain) and transfer its knowledge to a dataset in another domain (target domain), which is typically much smaller in size. Features learned by DL models in the early layers can be general, e.g., domain and task agnostic. Depending on the similarity of domains and tasks, even deeper features learned from one domain can be reused for another domain or task. Models that can be directly applied to the new domain without modification are the to generalize.</p><p id="p-0220" num="0217">Approaches were developed for adapting the DL model trained on FFDM images (source domain) to SM images (target domain) that reuse all the features learned from the FFDM domain. First, to perform calibration of neural networks, a small linear layer was added following the final fully-connected layer. Two forms for the linear layer were considered: (1) where the matrix is diagonal, which is denoted as vector calibration, and (2) where the matrix is allowed to freely vary, which is denoted as matrix calibration. Second, the final fully-connected layer of the Resnet-34 model was retrained on samples from the target domain, which is denoted as fine-tuning.</p><p id="p-0221" num="0218">In order to investigate the impact of the target domain dataset size, the adaptation techniques were repeated for different SM training sets across a range of sizes. The adaptation process was repeated 10 times for each dataset size with different random samples of the training data. For each sample, the training images were randomly selected, without replacement, from the full training set. As a reference, a Resnet-34 model was trained from scratch, e.g., from random initialization, for the largest number of training samples for each SM dataset.</p><p id="p-0222" num="0219">Statistical Analysis</p><p id="p-0223" num="0220">To obtain an exam-level assessment, each image within an exam was processed by the DL model and the resulting probabilities were averaged. Several performance metrics were computed from these average probabilities for the 4-class BI-RADS breast density task and the binary dense (BI-RADS C+D) vs. non-dense (BI-RADS A+B) task: (1) accuracy, estimated based on concordance with the original reporting radiologists, (2) the area under the receiver operating characteristic curve (AUC), and (3) Cohen's kappa (scikit-learn.org, version 0.20.0). Confidence intervals were computed by use of non-Studentized pivotal bootstrapping of the test sets for 8000 random samples. For the 4-class problem, the macroAUC (the average of the four AUC values from the one vs. others tasks) and Cohen's kappa with linear weighting were reported. For the binary density tasks, the predicted dense and non-dense probabilities were computed by summing the predicted probabilities for the corresponding BI-RADS density categories.</p><p id="p-0224" num="0221">Results</p><p id="p-0225" num="0222">Performance of the deep learning model on FFDM exams was evaluated as follows. The trained model was first evaluated on a large held-out test set of FFDM exams from Site 1 (4960 patients, 53048 images, mean age: 56.9, age range: 23-97). In this case, the images were from the same institution and of the same image type as employed to train the model. The BI-RADS breast density distribution predicted by the DL model (A: 8.5%, B: 52.2%, C: 36.1%, D: 3.2%) was similar to that of the original reporting radiologists (A: 9.3%, B: 52.0%, C: 34.6%, D: 4.0%). The DL model exhibited close agreement with the radiologists for the 4-class BI-RADS breast density task across a variety of performance measures (as shown in Table 3), including accuracy (82.2%, 95% confidence interval (CI): [81.6%, 82.9%]) and linearly-weighted Cohen's kappa (&#x3ba;w=0.75, CI: [0.74, 0.76]). A high-level of agreement was also observed for the binary breast density task (accuracy=91.1%, CI: [90.6%, 91.6%], AUC=0.971, CI: [0.968, 0.973], &#x3ba;=0.81, CI: [0.80, 0.82]). As demonstrated by the confusion matrices shown in <figref idref="DRAWINGS">FIGS. <b>14</b>A-<b>14</b>D</figref>, the DL model was rarely off by more than one breast density category (e.g., by calling an extremely dense breast as a scattered outcome; 0.03%, 4/13262). This was learned implicitly by the DL model without any explicit penalties for these types of larger errors.</p><p id="p-0226" num="0223"><figref idref="DRAWINGS">FIGS. <b>14</b>A-<b>14</b>B</figref> show confusion matrices for the Breast Imaging Reporting and Data System (BI-RADS) breast density task (<figref idref="DRAWINGS">FIG. <b>14</b>A</figref>) and the binary density task (dense, BI-RADS C+D vs. non-dense, BI-RADS A+B) (<figref idref="DRAWINGS">FIG. <b>14</b>B</figref>) evaluated on the full-field digital mammography (FFDM) test set. The numbers of test samples (exams) within each bin are shown in parentheses.</p><p id="p-0227" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="329pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 3</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Performance of the deep learning model of the present disclosure on the test set for full-</entry></row><row><entry>field digital mammography (FFDM) exams, for both the 4-class Breast Imaging Reporting and</entry></row><row><entry>Data System (BI-RADS) breast density task and binary density task (dense, BI-RADS C +</entry></row><row><entry>D vs. non-dense, BI-RADS A + B). 95% confidence intervals are given in brackets. Results</entry></row><row><entry>from other studies are shown evaluated on their respective test sets as points of comparison.</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="7"><colspec colname="offset" colwidth="63pt" align="left"/><colspec colname="1" colwidth="42pt" align="center"/><colspec colname="2" colwidth="49pt" align="center"/><colspec colname="3" colwidth="42pt" align="center"/><colspec colname="4" colwidth="42pt" align="center"/><colspec colname="5" colwidth="49pt" align="center"/><colspec colname="6" colwidth="42pt" align="center"/><tbody valign="top"><row><entry/><entry>4-class</entry><entry>4-class</entry><entry>4-class</entry><entry>Binary</entry><entry>Binary</entry><entry>Binary</entry></row><row><entry/><entry>Accuracy</entry><entry>macroAUC</entry><entry>Linear &#x3ba;</entry><entry>Accuracy</entry><entry>AUG</entry><entry>&#x3ba;</entry></row><row><entry/><entry namest="offset" nameend="6" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="7"><colspec colname="1" colwidth="63pt" align="left"/><colspec colname="2" colwidth="42pt" align="center"/><colspec colname="3" colwidth="49pt" align="center"/><colspec colname="4" colwidth="42pt" align="center"/><colspec colname="5" colwidth="42pt" align="center"/><colspec colname="6" colwidth="49pt" align="center"/><colspec colname="7" colwidth="42pt" align="center"/><tbody valign="top"><row><entry>Ours</entry><entry><sup>&#x2003;</sup>82.2</entry><entry>0.952</entry><entry>0.75</entry><entry><sup>&#x2003;</sup>91.1</entry><entry>0.971</entry><entry>0.81</entry></row><row><entry/><entry>[81.6, 82.9]</entry><entry>[0.949, 0.954]</entry><entry>[0.74, 0.76]</entry><entry>[90.6, 91.6]</entry><entry>[0.968, 0.973]</entry><entry>[0.80, 0.82]</entry></row><row><entry>Lehman et al. [19]</entry><entry>77</entry><entry/><entry>0.67</entry><entry>87</entry></row><row><entry/><entry>[76, 78]</entry><entry/><entry>[0.66, 0.68]</entry><entry>[86, 88]</entry></row><row><entry>Wu et al. [36]</entry><entry><sup>&#x2003;</sup>76.7</entry><entry>0.916</entry><entry/><entry><sup>&#x2003;</sup>86.5</entry><entry/><entry>0.65</entry></row><row><entry>Volpara v1.5.0 [3]</entry><entry>57</entry><entry/><entry>0.57</entry><entry>78</entry><entry/><entry>0.64</entry></row><row><entry/><entry/><entry/><entry>[0.55, 0.59]</entry><entry/><entry/><entry>[0.61, 0.66]</entry></row><row><entry>Quantra v2.0 [3]</entry><entry>56</entry><entry/><entry>0.46</entry><entry>83</entry><entry/><entry>0.59</entry></row><row><entry/><entry/><entry/><entry>[0.44, 0.47]</entry><entry/><entry/><entry>[0.57, 0.62]</entry></row><row><entry namest="1" nameend="7" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0228" num="0224">In order to place the results in the context of other studies, the performance of the deep learning model on the FFDM test set was compared with results evaluated on other large FFDM datasets acquired from academic centers and with commercial breast density software (as shown in Table 3). The FFDM DL model appears to offer competitive performance.</p><p id="p-0229" num="0225">Performance of the deep learning model on DBT exams was evaluated as follows. Results were first reported for the Site 1 SM test set (270 patients, 1080 images, mean age: 54.6, age range: 28-72), as this avoids any differences that may occur between the two sites. As shown in Table 4, when performed without adaptation, the model still demonstrated close agreement with the original reporting radiologists for the BI-RADS breast density task (accuracy=79%, CI: [74%, 84%]; &#x3ba;w=0.71, CI: [0.64, 0.78]). The DL model slightly underestimates breast density for SM images (as shown in <figref idref="DRAWINGS">FIGS. <b>15</b>A-<b>15</b>D</figref>), producing a BI-RADS breast density distribution (A: 10.4%, B: 57.8%, C: 28.9%, D: 3.0%) with more non-dense cases and fewer dense cases relative to the radiologists (A: 8.9%, B: 49.6%, C: 35.9%, D: 5.6%). This bias may be due to the differences shown in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, namely that certain regions of the breast appear darker in the SM image. A similar bias has been shown for other automated breast density estimation software [33]. Agreement for the binary density task is also quite high without adaptation (accuracy=88%, CI: [84%, 92%]; x=0.75, CI: [0.67, 0.83]; AUC=0.97, CI: [0.96, 0.99].</p><p id="p-0230" num="0000"><tables id="TABLE-US-00004" num="00004"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="329pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 4</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Performance of methods and systems of the present disclosure for adapting a deep learning</entry></row><row><entry>(DL) model trained on one dataset to another with a set of 500 synthetic 2D mammography</entry></row><row><entry>(SM) images. The datasets are denoted as &#x201c;MM&#x201d; for the full-field digital mammography</entry></row><row><entry>(FFDM) dataset, &#x201c;C1&#x201d; for the Site 1 SM dataset, and &#x201c;C2&#x201d; for the Site 2</entry></row><row><entry>SM dataset. The performance of the model trained from scratch on the FFDM dataset (672 thousand</entry></row><row><entry>training samples) and evaluated on its test set is also shown as a reference. 95% confidence</entry></row><row><entry>intervals, computed by bootstrapping over the test sets, are given in brackets.</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="8"><colspec colname="1" colwidth="42pt" align="left"/><colspec colname="2" colwidth="35pt" align="left"/><colspec colname="3" colwidth="35pt" align="center"/><colspec colname="4" colwidth="49pt" align="center"/><colspec colname="5" colwidth="42pt" align="center"/><colspec colname="6" colwidth="35pt" align="center"/><colspec colname="7" colwidth="49pt" align="center"/><colspec colname="8" colwidth="42pt" align="center"/><tbody valign="top"><row><entry/><entry/><entry>4-class</entry><entry>4-class</entry><entry>4-class</entry><entry>Binary</entry><entry>Binary</entry><entry/></row><row><entry>Datasets</entry><entry>Methods</entry><entry>Accuracy</entry><entry>macroAUC</entry><entry>Linear k</entry><entry>Accuracy</entry><entry>AUC</entry><entry>Binary</entry></row><row><entry namest="1" nameend="8" align="center" rowsep="1"/></row><row><entry>MM</entry><entry/><entry>82.2</entry><entry>0.952</entry><entry>0.75</entry><entry>91.1</entry><entry>0.971</entry><entry>0.81</entry></row><row><entry>MM &#x2192; C1</entry><entry>None</entry><entry>79</entry><entry>0.94</entry><entry>0.71</entry><entry>88</entry><entry>0.97</entry><entry>0.75</entry></row><row><entry/><entry/><entry>[74, 84]</entry><entry>[0.93, 0.96]</entry><entry>[0.64, 0.78]</entry><entry>[84, 92]</entry><entry>[0.96, 0.99]</entry><entry>[0.67, 0.83]</entry></row><row><entry/><entry>Vector</entry><entry>81</entry><entry>0.95</entry><entry>0.73</entry><entry>90</entry><entry>0.97</entry><entry>0.80</entry></row><row><entry/><entry/><entry>[77, 86]</entry><entry>[0.94, 0.97]</entry><entry>[0.67, 0.80]</entry><entry>[87, 94]</entry><entry>[0.96, 0.99]</entry><entry>[0.73, 0.88]</entry></row><row><entry/><entry>Matrix</entry><entry>80</entry><entry>0.95</entry><entry>0.72</entry><entry>91</entry><entry>0.97</entry><entry>0.82</entry></row><row><entry/><entry/><entry>[76, 85]</entry><entry>[0.94, 0.97]</entry><entry>[0.66, 0.79]</entry><entry>[88, 95]</entry><entry>[0.96, 0.99]</entry><entry>[0.76, 0.90]</entry></row><row><entry/><entry>Fine-tune</entry><entry>81</entry><entry>0.95</entry><entry>0.73</entry><entry>90</entry><entry>0.97</entry><entry>0.80</entry></row><row><entry/><entry/><entry>[76, 86]</entry><entry>[0.94, 0.97]</entry><entry>[0.67, 0.80]</entry><entry>[87, 94]</entry><entry>[0.95, 0.99]</entry><entry>[0.73, 0.88]</entry></row><row><entry>MM &#x2192; C2</entry><entry>None</entry><entry>76</entry><entry>0.944</entry><entry>0.72</entry><entry>92</entry><entry>0.980</entry><entry>0.84</entry></row><row><entry/><entry/><entry>[74, 78]</entry><entry>[0.938, 0.951]</entry><entry>[0.70, 0.75]</entry><entry>[91,93]</entry><entry>[0.976, 0.986]</entry><entry>[0.81, 0.87]</entry></row><row><entry/><entry>Vector</entry><entry>79</entry><entry>0.954</entry><entry>0.78</entry><entry>92</entry><entry>0.979</entry><entry>0.83</entry></row><row><entry/><entry/><entry>[77, 81]</entry><entry>[0.949, 0.961]</entry><entry>[0.76, 0.80]</entry><entry>[91, 93]</entry><entry>[0.974, 0.985]</entry><entry>[0.80, 0.86]</entry></row><row><entry/><entry>Matrix</entry><entry>80</entry><entry>0.956</entry><entry>0.79</entry><entry>92</entry><entry>0.983</entry><entry>0.84</entry></row><row><entry/><entry/><entry>[78, 82]</entry><entry>[0.950, 0.965]</entry><entry>[0.76, 0.81]</entry><entry>[91, 94]</entry><entry>[0.978, 0.988]</entry><entry>[0.82, 0.87]</entry></row><row><entry/><entry>Fine-tune</entry><entry>80</entry><entry>0.957</entry><entry>0.79</entry><entry>93</entry><entry>0.984</entry><entry>0.85</entry></row><row><entry/><entry/><entry>[78, 82]</entry><entry>[0.952, 0.964]</entry><entry>[0.77, 0.81]</entry><entry>[92, 94]</entry><entry>[0.979, 0.988]</entry><entry>[0.85, 0.88]</entry></row><row><entry namest="1" nameend="8" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0231" num="0226">After adaptation by matrix calibration with 500 SM images, the density distribution was more similar to that of the radiologists (A: 5.9%, B: 53.7%, C: 35.9%, D: 4.4%), while overall agreement was similar (accuracy=80%, CI: [76%, 85%]; &#x3ba;w=0.72, CI: [0.66, 0.79]). Accuracy for the two dense classes was improved at the expense of the two non-dense classes (as shown in <figref idref="DRAWINGS">FIGS. <b>15</b>A-<b>15</b>D</figref>). A larger improvement is seen for the binary density task, where the Cohen's kappa increased from 0.75 [0.67, 0.83] to 0.82 [0.76, 0.90] (accuracy=91%, CI: [88%, 95%]; AUC=0.97, CI: [0.96, 0.99]).</p><p id="p-0232" num="0227"><figref idref="DRAWINGS">FIGS. <b>15</b>A-<b>15</b>D</figref> show confusion matrices, evaluated on the Site 1 SM test set, for the Breast Imaging Reporting and Data System (BI-RADS) breast density task without adaptation (<figref idref="DRAWINGS">FIG. <b>15</b>A</figref>), the binary density task (dense, BI-RADS C+D vs. non-dense, BI-RADS A+B) (<figref idref="DRAWINGS">FIG. <b>15</b>B</figref>) without adaptation, the BI-RADS breast density task with adaptation by matrix calibration for 500 training samples (<figref idref="DRAWINGS">FIG. <b>15</b>C</figref>), and the binary density task (dense vs. non-dense) (<figref idref="DRAWINGS">FIG. <b>15</b>B</figref>) with adaptation by matrix calibration for 500 training samples. The numbers of test samples (exams) within each bin are shown in parentheses.</p><p id="p-0233" num="0228">A high degree of agreement between the DL model and the original reporting radiologists was also observed for the Site 2 SM test set (744 patients, 6192 images, mean age: 55.2, age range: 30-92) without adaptation (accuracy=76%, CI: [74%, 78%]; &#x3ba;w=0.72 CI: [0.70, 0.75]; as shown in Table 4). The BI-RADS breast density distribution predicted by the DL model (A: 5.7%, B: 48.8%, C: 36.4%, D: 9.1%) was more similar to the distribution found in the Site 1 datasets. The model may have learned a prior from the Site 1 FFDM dataset that may not be optimal for Site 2 where patient demographics are different. The predicted density distribution does not appear to be skewed towards low density estimates as seen for Site 1 (as shown in <figref idref="DRAWINGS">FIGS. <b>16</b>A-<b>16</b>D</figref>). This may suggest some difference in the SM images or their interpretation between the two sites. Agreement for the binary density task was especially strong (accuracy=92%, CI: [91%, 93%]; x=0.84, CI: [0.81, 0.87]; AUC=0.980, CI: [0.976, 0.986]). The excellent performance on the Site 2 dataset without adaptation demonstrates that the DL model may be successfully generalized across sites.</p><p id="p-0234" num="0229">With adaptation by matrix calibration for 500 training samples, performance for the BI-RADS breast density task on the Site 2 SM dataset substantially improved (accuracy=80, CI: [78, 82]; &#x3ba;w=0.79, CI: [0.76, 0.81]). After adaptation, the predicted BI-RADS breast density distribution (A: 16.9%, B: 43.3%, C: 29.4%, D: 10.4%) was more similar to that of the radiologists (A: 15.3%, B: 42.2%, C: 30.2%, D: 12.3%). Adaptation may have helped adjust for the demographic distribution of breast density at this site. Less improvement was seen for the binary breast density task (accuracy=92, CI: [91, 94]; &#x3ba;=0.84, CI: [0.82, 0.87]; AUC=0.983, CI: [0.978, 0.988]).</p><p id="p-0235" num="0230"><figref idref="DRAWINGS">FIGS. <b>16</b>A-<b>16</b>D</figref> show confusion matrices, evaluated on the Site 2 SM test set, for the Breast Imaging Reporting and Data System (BI-RADS) breast density task without adaptation (<figref idref="DRAWINGS">FIG. <b>16</b>A</figref>), the binary density task (dense, BI-RADS C+D vs. non-dense, BI-RADS A+B) (<figref idref="DRAWINGS">FIG. <b>16</b>B</figref>) without adaptation, the BI-RADS breast density task with adaptation by matrix calibration for 500 training samples (<figref idref="DRAWINGS">FIG. <b>16</b>C</figref>), and the binary density task (dense vs. non-dense) (<figref idref="DRAWINGS">FIG. <b>16</b>B</figref>) with adaptation by matrix calibration for 500 training samples. The numbers of test samples (exams) within each bin are shown in parentheses.</p><p id="p-0236" num="0231">The relative performance of different adaptation methods may depend on the number of training samples available for the adaptation, with more training samples benefiting methods with more parameters. <figref idref="DRAWINGS">FIGS. <b>17</b>A-<b>17</b>D</figref> show the impact of the amount of training data on the performance of the adaptation methods, as measured by macroAUC and linearly weighted Cohen's kappa, for the Site 1 dataset (<figref idref="DRAWINGS">FIGS. <b>17</b>A-<b>17</b>B</figref>, respectively) and the Site 2 SM dataset (<figref idref="DRAWINGS">FIGS. <b>17</b>C-<b>17</b>D</figref>, respectively), in accordance with disclosed embodiments. Results are reported across 10 random realizations of the training data for each dataset size (as described elsewhere herein) in order to investigate the uncertainty arising from the selection of the training data rather than from the limited size of the test set, as was done when computing the 95% confidence intervals. Each adaptation method has a range of number of samples where it offers the best performance, with the region corresponding to the number of parameters for the adaptation method (vector calibration: 4+4=8 parameters; matrix calibration: 4&#xd7;4+4=20 parameters; fine-tuning: 512&#xd7;4+4=2052 parameters). When the number of training samples is very small (e.g., less than 100 images), some adaptation methods negatively impacted performance. Even at the largest dataset sizes, the amount of training data was too limited for the Resnet-34 model trained from scratch on SM images to exceed the performance of the models adapted from FFDM.</p><p id="p-0237" num="0232"><figref idref="DRAWINGS">FIGS. <b>17</b>A-<b>17</b>D</figref> show the impact of the number of training samples in the target domain on the performance of the adapted model for the Site 1 synthetic 2D mammography (SM) test set, as measured by macroAUC (<figref idref="DRAWINGS">FIG. <b>17</b>A</figref>) and linearly weighted Cohen's kappa (<figref idref="DRAWINGS">FIG. <b>17</b>B</figref>), and for the Site 2 SM test set, as measured by macroAUC (<figref idref="DRAWINGS">FIG. <b>17</b>C</figref>) and linearly weighted Cohen's kappa (<figref idref="DRAWINGS">FIG. <b>17</b>D</figref>). Results are shown for vector and matrix calibration, and retraining the last fully-connected layer (fine-tuning). Error bars indicate the standard error of the mean computed over 10 random samplings of the training data. Performance prior to adaptation (none) and training from scratch are shown as references. For the Site 1 SM studies, the full-field digital mammography (FFDM) performance served as an additional reference. Note that each graph is shown with its own full dynamic range in order to facilitate comparison of the different adaptation methods for a given metric and dataset.</p><p id="p-0238" num="0233">Discussion</p><p id="p-0239" num="0234">Breast Imaging Reporting and Data System (BI-RADS) breast density may be an important indicator of breast cancer risk and radiologist sensitivity, but intra- and inter-reader variability may limit the effectiveness of this measure. Deep learning (DL) models for estimating breast density may be configured to reduce this variability while still providing accurate assessments. However, these DL models were demonstrated to be applicable to digital breast tomosynthesis (DBT) exams and able to be generalized across institutions, thereby indicating suitability as a useful clinical tool. To overcome the limited training data for DBT exams, a DL model was initially trained on a large set of full-field digital mammography (FFDM) images. When evaluated on a held-out test set of FFDM images, the model showed close agreement with the radiologists reported BI-RADS breast density (&#x3ba;w=0.75, 95% confidence interval (CI): [0.74, 0.76]). The model was then evaluated on two datasets of synthetic 2D mammography (SM) images, which are generated as part of DBT exams. A high level of agreement was also seen for the SM dataset from the same institution as the FFDM data (Site 1: &#x3ba;w=0.71, CI: [0.64, 0.78]) and for the SM dataset from another institution (Site 2: &#x3ba;w=0.72, CI: [0.70, 0.75]). The strong performance of the DL model demonstrates that it may generalize to data from DBT exams and different institutions. Further adaptation of the model for the SM datasets led to some improvement for Site 1 (&#x3ba;w=0.72, CI: [0.66, 0.79]) and a more substantial improvement for Site 2 (&#x3ba;w=0.79, CI: [0.76, 0.81]).</p><p id="p-0240" num="0235">When the assessments of the original reporting radiologists are accepted as the ground truth, the level of inter-reader variability among these radiologists has a large impact on the performance that can be achieved for a given dataset. For example, the performance obtained on the Site 2 SM dataset following adaptation was higher than that obtained on the FFDM dataset used to train the model. This is likely a result of limited inter-reader variability for the Site 2 SM dataset due to over 80% of the exams being read by only two readers.</p><p id="p-0241" num="0236">In contrast with other approaches, the BI-RADS breast density DL model was evaluated on SM images from DBT exams and on data from multiple institutions. Further, as discussed above, the DL model, when evaluated on the FFDM images, demonstrated competitive performance as compared to other DL models and commercial breast density software (&#x3ba;w=0.75, CI: [0.74, 0.76] vs. Lehman et al. 0.67, CI: [0.66, 0.68]; Volpara 0.57, CI: [0.55, 0.59], Quantra 0.46, CI: [0.44, 0.47]) [19, 3]. For each approach, results are reported on their respective test sets, analogously to how our own results are reported.</p><p id="p-0242" num="0237">Other measures of breast density, such as volumetric breast density, may be estimated by automated software for 3D tomosynthesis volumes or projections from DBT exams. Thresholds can be chosen to translate these measures to BI-RADS breast density, but this may result in lower levels of agreement than direct estimation of BI-RADS breast density (e.g. &#x3ba;w=0.47 for agreement between radiologist assessed BI-RADS breast density and that derived from volumetric breast density). Here, BI-RADS breast density is estimated from 2D SM images instead of the 3D tomosynthesis volumes, as this simplifies transfer learning from the FFDM images and mirrors the manner in which breast radiologists assess density.</p><p id="p-0243" num="0238">In some cases, when a deep learning (DL) model is adapted to a new institution, adjustments may be made for differences in image content, patient demographics, or the interpreting radiologists across institutions. This last adjustment may result in a degree of inter-reader variability between the original and adapted DL models, though likely lower than the individual inter-reader variability if the model learns the consensus of each group of radiologists. As a result, the improved DL model performance observed following adaptation for the Site 2 SM dataset may be attributable to differences in patient demographics or radiologist assessment practices compared with the FFDM dataset. The weaker improvement for the Site 1 SM dataset may be attributable to similarities in these same factors. For the comparison of the domain adaptation techniques as a function of the number of training samples, better performance for training a DL model from scratch may be obtained by tuning the number of parameters in the model based on the number of training samples.</p><p id="p-0244" num="0239">These results establish that the broad use of Breast Imaging Reporting and Data System (BI-RADS) breast density deep learning (DL) models holds great promise for improving clinical care. The success of the DL model without adaptation shows that the features learned by the model are largely applicable to both full-field digital mammography (FFDM) images and synthetic 2D mammography (SM) images from digital breast tomosynthesis (DBT) exams, as well as to different readers and institutions. Therefore, BI-RADS breast density DL models may be deployed to new sites and institutions without the additional effort of compiling large-scale datasets and training models from scratch. A BI-RADS breast density DL model that can generalize across sites and image types may be used to perform fast, low-cost, and more consistent estimates of breast density for women.</p><heading id="h-0016" level="1">Example 6&#x2014;Real-Time Radiology for Optimized Radiology Workflows</heading><p id="p-0245" num="0240">A machine learning-based classification system is developed to sort, prioritize, enrich, or otherwise modify radiology interpretation work (e.g., among a plurality of different workflows), based on an analysis of datasets comprising medical images of subjects. The sorting, prioritizing, enriching, or modifying of the cases for radiological assessment may be performed based on the medical image data (instead of only relying on metadata such as labels or annotation information, such as header or database elements, of the image data). For example, the medical image data may be processed by one or more image processing algorithms. The machine learning-based radiology system enables advanced radiology workflows that deliver faster and more accurate diagnoses, by allowing datasets of medical images to be stratified into different radiological assessments based on their suitability for such different assessments. For example, the plurality of different workflows may comprise radiological assessment by a plurality of different sets of radiologists. The radiologists may be on-site or remotely located relative to a clinic where the medical images of patients are acquired.</p><p id="p-0246" num="0241">In some embodiments, the machine learning-based classification system is configured to sort or prioritize radiology interpretation work among a plurality of different workflows, based on an analysis of datasets comprising medical images of subjects. For example, one set of datasets comprising medical images may be prioritized for radiological assessment over another set of datasets comprising medical images, based on the AI triage engine's determination that the first set of datasets has a higher priority or urgency than the second set of datasets.</p><p id="p-0247" num="0242">In some embodiments, the real-time radiology system acquires medical images of a subject through a screening exam, using an AI-enabled triage workflow, and then uses AI to deliver the radiology results (e.g., a screening result and/or a diagnostic result) within minutes (e.g., within about 5 minutes, about 10 minutes, about 15 minutes, about 30 minutes, about 45 minutes, about 60 minutes, about 90 minutes, about 2 hours, about 3 hours, about 4 hours, about 5 hours, about 6 hours, about 7 hours, or about 8 hours) to a patient after acquiring the medical images.</p><p id="p-0248" num="0243">In some embodiments, the real-time radiology system comprises a real-time notification system for interacting with clinical staff of AI-determined alert cases. The notification system is installed at various locations in a screening clinic (e.g., at clinical staff workstations). Users (e.g., physicians and clinical staff) are assigned to roles and receive distinct notifications for each role. The notifications are triggered when an emergency is determined by a trained algorithm for a patient's case. For example, the notifications may contain both advisory information as well as permit users to enter information which can affect the patient's clinical workflow in real-time during the visit. A physician (e.g., treating physician or radiologist) is notified via real-time alerts of these emergency cases as they arise, and uses information from the notification to provide a better diagnosis.</p><p id="p-0249" num="0244">In some embodiments, the real-time radiology system comprises a patient mobile application (app) for sending notifications to patients. The notifications may include the status of their screening/diagnostic visit, the radiological assessments performed on their medical images, presentations constructed from the radiological assessments, etc.</p><p id="p-0250" num="0245">In some embodiments, the real-time radiology system comprises a database configured to acquire, obtain, and store for future retrieval datasets comprising medical images (e.g., radiological images), AI enrichment of datasets (e.g., medical images labeled, annotated, or processed by AI, such as via image processing algorithms), screening results, diagnostic results, and presentations of medical images and results. The real-time radiology system is configured to provide a service to patients and their clinical care providers (e.g., radiologists and clinical staff) to retrieve, access, and view the contents of the database. The real-time radiology system service may support the construction of complex computational graphs from the stored datasets, including chaining together several AI models.</p><p id="p-0251" num="0246"><figref idref="DRAWINGS">FIG. <b>18</b></figref> shows an example of a schematic of a real-time radiology assessment workflow. The real-time radiology assessment workflow may comprise acquiring an image from a subject (e.g., via mammography). The image may be processed using systems and methods (e.g., including AI algorithms) of the present disclosure to detect that the image corresponds to a suspicious case. A clinician may be alerted that the subject is eligible for real-time radiology assessment. While the subject waits in the clinic, the image is directed to a radiologist for radiological assessment, and results of the radiological assessment are provided to the clinician for further review.</p><p id="p-0252" num="0247"><figref idref="DRAWINGS">FIG. <b>19</b></figref> shows another example of a schematic of a real-time radiology assessment workflow. Using systems and methods (e.g., including AI algorithms) of the present disclosure, images of subjects are retrieved from a PACS database and analyzed. If the AI analysis indicates that a given subject (e.g., patient) does not have a suspicious image, then a patient coordinator is notified, who then informs the patient that results will be received at home after a radiological assessment has been performed. If the AI analysis indicates that the patient has a suspicious image, then a technologist is notified, who then either (1) updates the patient history, and notifies a radiologist to perform a radiological assessment and provide results to a patient coordinator, or (2) notifies billing to process an out-of-pocket expense for a follow-up exam of the patient, and notifies the patient coordinator. The patient coordinator may share results with the patient and schedule follow-up appointments as needed.</p><p id="p-0253" num="0248">In some embodiments, the real-time radiology assessment workflow comprises (i) directing an image or derivative thereof to a first radiologist among a first set of radiologists for radiological assessment to produce a screening result, based at least in part on whether the image is classified as suspicious; (ii) directing the image or derivative thereof to a second radiologist among a second set of radiologists for radiological assessment to produce a screening result, based at least in part on whether the image is classified as ambiguous; or (iii) directing the image or derivative thereof to a third radiologist among a third set of radiologists for radiological assessment to produce a screening result, based at least in part on whether the image is classified as normal.</p><p id="p-0254" num="0249">In some embodiments, the real-time radiology assessment workflow comprises directing the image or derivative thereof to a first radiologist among a first set of radiologists for radiological assessment to produce a screening result, if the at least one image is classified as suspicious. In some embodiments, the real-time radiology assessment workflow comprises directing the image or derivative thereof to a second radiologist among a second set of radiologists for radiological assessment to produce a screening result, if the image is classified as ambiguous. In some embodiments, the real-time radiology assessment workflow comprises directing the image or derivative thereof to a third radiologist among a third set of radiologists for radiological assessment to produce a screening result, if the image is classified as normal.</p><p id="p-0255" num="0250">In some embodiments, the screening result of the subject is produced at a same clinic visit as the obtaining of the image or derivative thereof. In some embodiments, the first set of radiologists is located at an on-site clinic (e.g., where the image or derivative thereof is obtained).</p><p id="p-0256" num="0251">In some embodiments, the second set of radiologists comprises expert radiologists (e.g., who are trained to classify the image or derivative thereof as normal or suspicious at a greater accuracy than the trained algorithm). In some embodiments, the third set of radiologists is located remotely to an onsite clinic (e.g., where the image is obtained). In some embodiments, the third radiologist of the third set of radiologists performs the radiologist assessment of the image or derivative thereof among a batch comprising a plurality of images (e.g., where the batch is selected for enhanced efficiency of the radiological assessment).</p><p id="p-0257" num="0252">In some embodiments, the real-time radiology assessment workflow comprises performing a diagnostic procedure of the subject, based at least in part on the screening result, to produce a diagnostic result of the subject. In some embodiments, the diagnostic result of the subject is produced at a same clinic visit as the obtaining of the image. In some embodiments, the diagnostic result of the subject is produced within about one hour of the obtaining of the image.</p><p id="p-0258" num="0253">In some embodiments, the image or derivative thereof is directed to the first radiologist, the second radiologist, or the third radiologist based at least in part on additional characteristics of the location of the body of the subject. In some embodiments, the additional characteristics comprise an anatomy, tissue characteristics (e.g., tissue density or physical properties), a presence of a foreign object (e.g., implants), a type of finding, an appearance of disease (e.g., predicted by an algorithm such as a machine learning algorithm), or a combination thereof.</p><p id="p-0259" num="0254">In some embodiments, the image or derivative thereof is directed to the first radiologist, the second radiologist, or the third radiologist based at least in part on additional characteristics of the first radiologist, the second radiologist, or the third radiologist (e.g., a personal ability of the first radiologist, the second radiologist, or the third radiologist to perform a radiological assessment of the at least one image or derivative thereof).</p><p id="p-0260" num="0255">In some embodiments, the real-time radiology assessment workflow comprises generating an alert based at least in part on the directing of the image or derivative thereof to the first radiologist or the directing of the image or derivative thereof to the second radiologist. In some embodiments, the real-time radiology assessment workflow comprises transmitting the alert to the subject or to a clinical health care provider of the subject. In some embodiments, the real-time radiology assessment workflow comprises transmitting the alert to the subject through a patient mobile application. In some embodiments, the alert is generated in real time or substantially real time as (b).</p><p id="p-0261" num="0256">In some embodiments, the real-time radiology system comprises an AI-powered teleradiology platform. The teleradiology platform comprises an AI-based radiology work distributor that routes cases for review by doctors in real time or substantially real time as the acquisition of medical images. The teleradiology platform may be configured to perform AI-based profiling of image types and doctors to assign each case to a doctor from among a plurality of doctors based on the suitability of the individual doctor at handling, assessing, or interpreting the datasets of the given case. The radiologists may belong to a network of radiologists, each having distinct sets of radiological skills, expertise, and experience. The teleradiology platform may assign cases to doctors based on searching the network for the doctor having the desired combination of skills, expertise, experience, and cost. The radiologists may be on-site or remotely located relative to a clinic where the medical images of patients are acquired. In some embodiments, the expertise of a radiologist may be determined by comparing his or her performance to that of an AI model for various radiologist tasks on an evaluative set of data. The radiologists may be paid for performing the radiological assessment for each individual case that they accept and perform. In some embodiments, the real-time radiology system features dynamic pricing of radiology work based on AI-determined difficulty, urgency, and value of the radiology work (e.g., radiological assessment, interpretation, or review).</p><p id="p-0262" num="0257">In some embodiments, the real-time radiology system is configured to organize, prioritize, or stratify a plurality of medical image cases into subgroups of medical image cases for radiological assessment, interpretation, or review. The stratification of medical image cases may be performed by an AI algorithm to improve human efficiency in evaluating the individual cases, based on the image characteristics of the individual medical image cases. For example, the algorithm may group visually similar or diagnostically similar cases together for human review, such as putting identifying cases with similar lesion types in located in a similar region of anatomy.</p><p id="p-0263" num="0258"><figref idref="DRAWINGS">FIG. <b>20</b></figref> shows an example of a schematic of an AI-assisted radiology assessment workflow in a teleradiology setting. Using systems and methods (e.g., including AI algorithms) of the present disclosure, images of subjects are retrieved from a PACS database and analyzed using AI algorithms to prioritize and rule out cases for radiological assessment (e.g., based on breast density and/or breast cancer risk of the subjects). The AI-assisted radiology assessment workflow may optimize routing of the cases for radiological assessment based on radiologist skill level. For example, a first radiologist may have an average read time of 45 seconds, an expertise level of expert, and a skill for assessing extremely dense breasts. As another example, a second radiologist may have an average read time of 401 seconds and an expertise level of novice. As another example, a third radiologist may have an average read time of 323 seconds and an expertise level of novice. As another example, a fourth radiologist may have an average read time of 145 seconds and an expertise level of novice. For example, a fifth radiologist may have an average read time of 60 seconds, an expertise level of expert, and a skill for assessing benign masses. The AI-assisted radiology assessment workflow may direct a given subject's case to a radiologist selected from among the first, second, third, fourth, or fifth radiologist, based on their average read time, expertise level, and/or skill level appropriate for the given subject's case.</p><p id="p-0264" num="0259">In some embodiments, the AI-assisted radiology assessment workflow comprises (i) directing an image or derivative thereof to a first radiologist among a first set of radiologists for radiological assessment to produce a screening result, based at least in part on whether the image is classified as suspicious; (ii) directing the image or derivative thereof to a second radiologist among a second set of radiologists for radiological assessment to produce a screening result, based at least in part on whether the image is classified as ambiguous; or (iii) directing the image or derivative thereof to a third radiologist among a third set of radiologists for radiological assessment to produce a screening result, based at least in part on whether the image is classified as normal.</p><p id="p-0265" num="0260">In some embodiments, the AI-assisted radiology assessment workflow comprises directing the image or derivative thereof to a first radiologist among a first set of radiologists for radiological assessment to produce a screening result, if the at least one image is classified as suspicious. In some embodiments, the AI-assisted radiology assessment workflow comprises directing the image or derivative thereof to a second radiologist among a second set of radiologists for radiological assessment to produce a screening result, if the image is classified as ambiguous. In some embodiments, the AI-assisted radiology assessment workflow comprises directing the image or derivative thereof to a third radiologist among a third set of radiologists for radiological assessment to produce a screening result, if the image is classified as normal.</p><p id="p-0266" num="0261">In some embodiments, the screening result of the subject is produced at a same clinic visit as the obtaining of the image or derivative thereof. In some embodiments, the first set of radiologists is located at an on-site clinic (e.g., where the image or derivative thereof is obtained).</p><p id="p-0267" num="0262">In some embodiments, the second set of radiologists comprises expert radiologists (e.g., who are trained to classify the image or derivative thereof as normal or suspicious at a greater accuracy than the trained algorithm). In some embodiments, the third set of radiologists is located remotely to an onsite clinic (e.g., where the image is obtained). In some embodiments, the third radiologist of the third set of radiologists performs the radiologist assessment of the image or derivative thereof among a batch comprising a plurality of images (e.g., where the batch is selected for enhanced efficiency of the radiological assessment).</p><p id="p-0268" num="0263">In some embodiments, the AI-assisted radiology assessment workflow comprises performing a diagnostic procedure of the subject, based at least in part on the screening result, to produce a diagnostic result of the subject. In some embodiments, the diagnostic result of the subject is produced at a same clinic visit as the obtaining of the image. In some embodiments, the diagnostic result of the subject is produced within about one hour of the obtaining of the image.</p><p id="p-0269" num="0264">In some embodiments, the image or derivative thereof is directed to the first radiologist, the second radiologist, or the third radiologist based at least in part on additional characteristics of the location of the body of the subject. In some embodiments, the additional characteristics comprise an anatomy, tissue characteristics (e.g., tissue density or physical properties), a presence of a foreign object (e.g., implants), a type of finding, an appearance of disease (e.g., predicted by an algorithm such as a machine learning algorithm), or a combination thereof.</p><p id="p-0270" num="0265">In some embodiments, the image or derivative thereof is directed to the first radiologist, the second radiologist, or the third radiologist based at least in part on additional characteristics of the first radiologist, the second radiologist, or the third radiologist (e.g., a personal ability of the first radiologist, the second radiologist, or the third radiologist to perform a radiological assessment of the at least one image or derivative thereof).</p><p id="p-0271" num="0266">In some embodiments, the AI-assisted radiology assessment workflow comprises generating an alert based at least in part on the directing of the image or derivative thereof to the first radiologist or the directing of the image or derivative thereof to the second radiologist. In some embodiments, the AI-assisted radiology assessment workflow comprises transmitting the alert to the subject or to a clinical health care provider of the subject. In some embodiments, the AI-assisted radiology assessment workflow comprises transmitting the alert to the subject through a patient mobile application. In some embodiments, the alert is generated in real time or substantially real time as (b).</p><p id="p-0272" num="0267">While preferred embodiments of the present invention have been shown and described herein, it will be obvious to those skilled in the art that such embodiments are provided by way of example only. It is not intended that the invention be limited by the specific examples provided within the specification. While the invention has been described with reference to the aforementioned specification, the descriptions and illustrations of the embodiments herein are not meant to be construed in a limiting sense. Numerous variations, changes, and substitutions will now occur to those skilled in the art without departing from the invention. Furthermore, it shall be understood that all aspects of the invention are not limited to the specific depictions, configurations or relative proportions set forth herein which depend upon a variety of conditions and variables. It should be understood that various alternatives to the embodiments of the invention described herein may be employed in practicing the invention. It is therefore contemplated that the invention shall also cover any such alternatives, modifications, variations or equivalents. It is intended that the following claims define the scope of the invention and that methods and structures within the scope of these claims and their equivalents be covered thereby.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-01-88" num="01-88"><claim-text><b>1</b>.-<b>88</b>. (canceled)</claim-text></claim><claim id="CLM-00089" num="00089"><claim-text><b>89</b>. A method for processing at least one image of a location of a body of said subject, comprising:<claim-text>(a) obtaining said at least one image of said location of a body of said subject;</claim-text><claim-text>(b) using a trained algorithm to classify said at least one image or a derivative thereof to a category among a plurality of categories, wherein said classifying comprises applying an image processing algorithm to said at least one image or derivative thereof;</claim-text><claim-text>(c) upon classifying said at least one image or derivative thereof in (b), (i) directing said at least one image or derivative thereof to a first radiologist for radiological assessment if said at least one image is classified to a first category among said plurality of categories, or (ii) directing said at least one image or derivative thereof to a second radiologist for radiological assessment, if said at least one image is classified to a second category among said plurality of categories; and</claim-text><claim-text>(d) receiving a radiological assessment of said subject from said first or second radiologist based at least in part on a radiological analysis of said at least one image or derivative thereof.</claim-text></claim-text></claim><claim id="CLM-00090" num="00090"><claim-text><b>90</b>. The method of <claim-ref idref="CLM-00089">claim 89</claim-ref>, wherein (b) further comprises classifying said at least one image or derivative thereof as normal, ambiguous, or suspicious.</claim-text></claim><claim id="CLM-00091" num="00091"><claim-text><b>91</b>. The method of <claim-ref idref="CLM-00090">claim 90</claim-ref>, further comprising directing said at least one image or derivative thereof to a classifier based on said classification of said at least one image or derivative thereof in (b).</claim-text></claim><claim id="CLM-00092" num="00092"><claim-text><b>92</b>. The method of <claim-ref idref="CLM-00091">claim 91</claim-ref>, wherein (c) further comprises directing said at least one image or derivative thereof to a first radiologist from among a first plurality of radiologists or to a second radiologist from among a second plurality of radiologists for radiological assessment.</claim-text></claim><claim id="CLM-00093" num="00093"><claim-text><b>93</b>. The method of <claim-ref idref="CLM-00089">claim 89</claim-ref>, wherein said at least one image or derivative thereof comprises a medical image.</claim-text></claim><claim id="CLM-00094" num="00094"><claim-text><b>94</b>. The method of <claim-ref idref="CLM-00090">claim 90</claim-ref>, wherein said trained machine learning algorithm is configured to identify at least one region of said at least one image or derivative thereof that contains or is suspected of containing said anomalous tissue.</claim-text></claim><claim id="CLM-00095" num="00095"><claim-text><b>95</b>. The method of <claim-ref idref="CLM-00090">claim 90</claim-ref>, wherein a trained algorithm classifies said at least one image or a derivative thereof as normal, ambiguous, or suspicious for being indicative of a cancer.</claim-text></claim><claim id="CLM-00096" num="00096"><claim-text><b>96</b>. The method of <claim-ref idref="CLM-00095">claim 95</claim-ref>, wherein said cancer comprises breast cancer.</claim-text></claim><claim id="CLM-00097" num="00097"><claim-text><b>97</b>. The method of <claim-ref idref="CLM-00096">claim 96</claim-ref>, wherein said at least one image or derivative thereof comprises a three-dimensional image of at least a portion of a breast of said subject.</claim-text></claim><claim id="CLM-00098" num="00098"><claim-text><b>98</b>. The method of <claim-ref idref="CLM-00089">claim 89</claim-ref>, wherein said trained machine learning algorithm comprises a deep learning algorithm, a support vector machine (SVM), a neural network, or a Random Forest.</claim-text></claim><claim id="CLM-00099" num="00099"><claim-text><b>99</b>. The method of <claim-ref idref="CLM-00089">claim 89</claim-ref>, further comprising monitoring said subject, wherein said monitoring comprises assessing images of said location of said body of said subject at a plurality of time points, wherein said assessing is based at least in part on said classification of said at least one image or a derivative thereof as normal, ambiguous, or suspicious or identified regions suspected of containing anomalous tissue at each of said plurality of time points.</claim-text></claim><claim id="CLM-00100" num="00100"><claim-text><b>100</b>. The method of <claim-ref idref="CLM-00099">claim 99</claim-ref>, wherein a difference in said assessment of said images of said body of said subject at said plurality of time points is indicative of one or more clinical indications selected from the group consisting of: (i) a diagnosis of said subject, (ii) a prognosis of said subject, and (iii) an efficacy or non-efficacy of a course of treatment of said subject.</claim-text></claim><claim id="CLM-00101" num="00101"><claim-text><b>101</b>. The method of <claim-ref idref="CLM-00090">claim 90</claim-ref>, wherein (c) further comprises (i) directing said at least one image or derivative thereof to a first radiologist among a first set of radiologists for radiological assessment to produce a screening result, based at least in part on whether said at least one image or derivative thereof is classified as suspicious; or (ii) directing said at least one image or derivative thereof to a second radiologist among a second set of radiologists for radiological assessment to produce a screening result, based at least in part on whether said at least one image or derivative thereof is classified as normal.</claim-text></claim><claim id="CLM-00102" num="00102"><claim-text><b>102</b>. The method of <claim-ref idref="CLM-00101">claim 101</claim-ref>, wherein said screening result of said subject is produced at a same clinic visit as said obtaining of said at least one image or derivative thereof.</claim-text></claim><claim id="CLM-00103" num="00103"><claim-text><b>103</b>. The method of <claim-ref idref="CLM-00101">claim 101</claim-ref>, wherein said first set of radiologists is located at an on-site clinic, wherein said at least one image or derivative thereof is obtained at said on-site clinic.</claim-text></claim><claim id="CLM-00104" num="00104"><claim-text><b>104</b>. The method of <claim-ref idref="CLM-00101">claim 101</claim-ref>, wherein said first set of radiologists comprises expert radiologists, which expert radiologists are trained to classify said at least one image or derivative thereof as normal or suspicious at a greater accuracy than said trained algorithm.</claim-text></claim><claim id="CLM-00105" num="00105"><claim-text><b>105</b>. The method of <claim-ref idref="CLM-00101">claim 101</claim-ref>, wherein said second set of radiologists is located remotely to an onsite clinic, wherein said at least one image is obtained at said on-site clinic.</claim-text></claim><claim id="CLM-00106" num="00106"><claim-text><b>106</b>. The method of <claim-ref idref="CLM-00101">claim 101</claim-ref>, wherein said second radiologist of said second set of radiologists performs said radiologist assessment of said at least one image or derivative thereof among a batch comprising a plurality of images, wherein said batch is selected for enhanced efficiency of said radiological assessment.</claim-text></claim><claim id="CLM-00107" num="00107"><claim-text><b>107</b>. The method of <claim-ref idref="CLM-00101">claim 101</claim-ref>, further comprising performing a diagnostic procedure of said subject at a same clinic visit as said obtaining of said at least one image, based at least in part on said screening result, to produce a diagnostic result of said subject.</claim-text></claim><claim id="CLM-00108" num="00108"><claim-text><b>108</b>. The method of <claim-ref idref="CLM-00101">claim 101</claim-ref>, wherein said at least one image or derivative thereof is directed to said first radiologist or said second radiologist, based at least in part on additional characteristics classified by a machine learning classifier.</claim-text></claim><claim id="CLM-00109" num="00109"><claim-text><b>109</b>. The method of <claim-ref idref="CLM-00108">claim 108</claim-ref>, wherein said additional characteristics comprise an anatomy, tissue characteristics, a presence of a foreign object, a type of finding, an appearance of disease, or a combination thereof.</claim-text></claim><claim id="CLM-00110" num="00110"><claim-text><b>110</b>. The method of <claim-ref idref="CLM-00089">claim 89</claim-ref>, wherein (c) further comprises generating an alert based at least in part on said directing of said at least one image or derivative thereof to said first radiologist.</claim-text></claim><claim id="CLM-00111" num="00111"><claim-text><b>111</b>. The method of <claim-ref idref="CLM-00110">claim 110</claim-ref>, further comprising transmitting said alert to said subject or to a clinical health care provider of said subject.</claim-text></claim><claim id="CLM-00112" num="00112"><claim-text><b>112</b>. The method of <claim-ref idref="CLM-00110">claim 110</claim-ref>, wherein said alert is generated in real time or substantially real time as (b).</claim-text></claim><claim id="CLM-00113" num="00113"><claim-text><b>113</b>. The method of <claim-ref idref="CLM-00089">claim 89</claim-ref>, wherein applying said image processing algorithm further comprises identifying regions of interest within said at least one image or derivative thereof, and labeling said regions of interest to produce at least one labeled image.</claim-text></claim><claim id="CLM-00114" num="00114"><claim-text><b>114</b>. The method of <claim-ref idref="CLM-00089">claim 89</claim-ref>, wherein (c) is performed in real time or substantially real time as (b).</claim-text></claim><claim id="CLM-00115" num="00115"><claim-text><b>115</b>. The method of <claim-ref idref="CLM-00089">claim 89</claim-ref>, wherein said at least one image comprises a plurality of images obtained from said subject, wherein said plurality of images are obtained using different modalities or at different time points.</claim-text></claim><claim id="CLM-00116" num="00116"><claim-text><b>116</b>. The method of <claim-ref idref="CLM-00089">claim 89</claim-ref>, wherein said classifying further comprises processing clinical health data of said subject.</claim-text></claim><claim id="CLM-00117" num="00117"><claim-text><b>117</b>. A computer system for processing at least one image of a location of a body of said subject:<claim-text>a database that is configured to store said at least one image of said location of a body of said subject; and</claim-text><claim-text>one or more computer processors operatively coupled to said database, wherein said one or more computer processors are individually or collectively programmed to:</claim-text><claim-text>(a) use a trained algorithm to classify said at least one image or a derivative thereof to a category among a plurality of categories, wherein said classifying comprises applying an image processing algorithm to said at least one image or derivative thereof;</claim-text><claim-text>(b) upon classifying said at least one image or derivative thereof in (b), (i) directing said at least image or derivative thereof to a first radiologist for radiological assessment if said at least one image is classified to a first category among said plurality of categories, or (ii) directing said at least one image or derivative thereof to a second radiologist for radiological assessment, if said at least one image is classified to a second category among said plurality of categories; and</claim-text><claim-text>(c) receive a radiological assessment of said subject from said first or second radiologist based at least in part on a radiological analysis of said at least one image or derivative thereof.</claim-text></claim-text></claim><claim id="CLM-00118" num="00118"><claim-text><b>118</b>. A non-transitory computer readable medium comprising machine-executable code that, upon execution by one or more computer processors, implements a method for processing at least one image of a location of a body of said subject, comprising:<claim-text>(a) obtaining said at least one image of said location of a body of said subject;</claim-text><claim-text>(b) using a trained algorithm to classify said at least one image or a derivative thereof to a category among a plurality of categories, wherein said classifying comprises applying an image processing algorithm to said at least one image or derivative thereof;</claim-text><claim-text>(c) upon classifying said at least one image or derivative thereof in (b), (i) directing said at least image or derivative thereof to a first radiologist for radiological assessment if said at least one image is classified to a first category among said plurality of categories, or (ii) directing said at least one image or derivative thereof to a second radiologist for radiological assessment, if said at least one image is classified to a second category among said plurality of categories; and</claim-text><claim-text>(d) receiving a radiological assessment of said subject from said first or second radiologist based at least in part on a radiological analysis of said at least one image or derivative thereof.</claim-text></claim-text></claim></claims></us-patent-application>