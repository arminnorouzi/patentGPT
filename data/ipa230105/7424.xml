<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007425A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007425</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17751425</doc-number><date>20220523</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>19</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>303</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>19</main-group><subgroup>167</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>S</subclass><main-group>2400</main-group><subgroup>11</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">LAYERED DESCRIPTION OF SPACE OF INTEREST</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63217442</doc-number><date>20210701</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Tencent America LLC</orgname><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>TIAN</last-name><first-name>Jun</first-name><address><city>Belle Mead</city><state>NJ</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>XU</last-name><first-name>Xiaozhong</first-name><address><city>State College</city><state>PA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>LIU</last-name><first-name>Shan</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Tencent America LLC</orgname><role>02</role><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Aspects of the disclosure provide methods and apparatuses for audio processing. In some examples, an apparatus for media processing includes processing circuitry. The processing circuitry receive audio inputs associated with a layered description for a space of interest in an audio scene. The space of interest includes a plurality of subspaces. The layered description includes a first layer and a second layer. The first layer has a common node with a first value that is a common attribute value of two or more subspaces in the plurality of subspaces. The second layer has individual nodes respectively associated with each of the plurality of subspaces. The processing circuitry determines the plurality of subspaces of the space of interest based on the layered description, and renders an audio output based on the audio inputs in response to a location of a subject of the audio scene being in the space of interest.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="117.18mm" wi="158.75mm" file="US20230007425A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="94.23mm" wi="128.19mm" orientation="landscape" file="US20230007425A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="234.19mm" wi="173.48mm" orientation="landscape" file="US20230007425A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="223.69mm" wi="174.50mm" orientation="landscape" file="US20230007425A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="238.93mm" wi="108.88mm" orientation="landscape" file="US20230007425A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="184.83mm" wi="138.51mm" orientation="landscape" file="US20230007425A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="197.78mm" wi="139.62mm" orientation="landscape" file="US20230007425A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="193.63mm" wi="126.15mm" orientation="landscape" file="US20230007425A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="218.44mm" wi="124.04mm" orientation="landscape" file="US20230007425A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="224.28mm" wi="129.79mm" orientation="landscape" file="US20230007425A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="221.66mm" wi="140.72mm" orientation="landscape" file="US20230007425A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="160.27mm" wi="131.66mm" file="US20230007425A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="207.86mm" wi="131.66mm" file="US20230007425A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="207.77mm" wi="131.66mm" file="US20230007425A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="231.48mm" wi="179.24mm" orientation="landscape" file="US20230007425A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">INCORPORATION BY REFERENCE</heading><p id="p-0002" num="0001">This present disclosure claims the benefit of priority to U.S. Provisional Application No. 63/217,442, &#x201c;Layered Description of Space of Interest&#x201d; filed on Jul. 1, 2021, which is incorporated by reference herein in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure describes embodiments generally related to audio processing.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">The background description provided herein is for the purpose of generally presenting the context of the disclosure. Work of the presently named inventors, to the extent the work is described in this background section, as well as aspects of the description that may not otherwise qualify as prior art at the time of filing, are neither expressly nor impliedly admitted as prior art against the present disclosure.</p><p id="p-0005" num="0004">In an application of virtual reality or augmented reality, to make a user have the feeling of presence in the virtual world of the application, audio in a virtual scene of the application is perceived as in real world, with sounds coming from associated virtual figures of the virtual scene. In some examples, physical movement of the user in the real world is perceived as having matching movement in the virtual scene in the application. Further, and importantly, the user can interact with the virtual scene using audio that is perceived as realistic and matches the user's experience in the real world.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">Aspects of the disclosure provide methods and apparatuses for audio processing. In some examples, an apparatus for media processing includes processing circuitry. The processing circuitry receive audio inputs associated with a layered description for a space of interest in an audio scene. The space of interest includes a plurality of subspaces. The layered description includes a first layer and a second layer. The first layer has a common node with a first value that is a common attribute value of two or more subspaces in the plurality of subspaces. The second layer has individual nodes respectively associated with each of the plurality of subspaces. The processing circuitry determines the plurality of subspaces of the space of interest based on the layered description, and renders an audio output based on the audio inputs in response to a location of a subject of the audio scene being in the space of interest.</p><p id="p-0007" num="0006">In some examples, the plurality of subspaces are rectangular boxes that are defined by at least a position attribute, an orientation attribute and a size attribute.</p><p id="p-0008" num="0007">According to some aspects of the disclosure, the common node identifies a name for an attribute, and the first value is an attribute value of the attribute, and the processing circuitry can retrieve, from the common node in the first layer, the first value as the attribute value of the attribute for a subspace in the plurality of subspaces.</p><p id="p-0009" num="0008">According to some aspects of the disclosure, the common node identifies a name of an attribute and an index of a subfield of the attribute, and the first value is a subfield attribute value for the subfield of the attribute, and the processing circuitry retrieves, from the common node in the first layer, the first value as the subfield attribute value for the subfield of the attribute of a subspace in the plurality of subspaces.</p><p id="p-0010" num="0009">In some examples, the common node with the first value is common to the plurality of subspaces, and the processing circuitry retrieves, from the common node in the first layer, the first value as an attribute value of an attribute for each of the plurality of subspaces.</p><p id="p-0011" num="0010">In some examples, the common node with the first value is common to a subset of the plurality of subspaces. The processing circuitry retrieves, from the common node in the first layer, the first value as an attribute value of an attribute for a first subspace in response to a first individual node associated with the first subspace missing a value for the attribute. Further, the processing circuitry retrieves, from a second individual node associated with a second subspace, a second value associated with the attribute for the second subspace in response to an existence of the second value associated with the attribute in the second individual node.</p><p id="p-0012" num="0011">In some examples, the common node with the first value is common to a subset of the plurality of subspaces. The processing circuitry retrieves, from the common node in the first layer, the first value as an attribute value of an attribute of a first subspace in response to a first individual node associated with the first subspace missing a value for the attribute. Further, the processing circuitry retrieves, from a second individual node associated with a second subspace, a difference value associated with the attribute of the second subspace, and computes a second value for the attribute of the second subspace based on the first value and the difference value.</p><p id="p-0013" num="0012">In some examples, the processing circuitry receives a bitstream carrying the audio inputs and the layered description of the space of interest as metadata of the audio inputs, and decodes the bitstream to obtain the audio inputs and the layered description of the space of interest.</p><p id="p-0014" num="0013">In some examples, the processing circuitry ignores the audio inputs without rendering in response to the location of the subject of the audio scene being outside of the space of interest.</p><p id="p-0015" num="0014">Aspects of the disclosure also provide a non-transitory computer-readable medium storing instructions which when executed by a computer cause the computer to perform the method for audio processing.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0016" num="0015">Further features, the nature, and various advantages of the disclosed subject matter will be more apparent from the following detailed description and the accompanying drawings in which:</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a diagram illustrating an environment using 6 degrees of freedom (6 DoF) in some examples.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a block diagram of a media system according to an embodiment of the disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an audio scene that is referred to as a canyon scene in some examples.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a description for a space of interest in the canyon scene.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a syntax for a layered description of a space of interest according to an embodiment of the disclosure.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a layered description for a space of interest in some examples.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a syntax for a layered description of a space of interest according to an embodiment of the disclosure.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a layered description for a space of interest in some examples.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows a layered description for a space of interest in some examples.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows a layered description for a space of interest in some examples.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows a flow chart outlining a process according to some embodiment of the disclosure.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>12</b></figref> shows a flow chart outlining a process according to some embodiment of the disclosure.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows a flow chart outlining a process according to some embodiment of the disclosure.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a schematic illustration of a computer system in accordance with an embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DISCRETION OF EMBODIMENTS</heading><p id="p-0031" num="0030">Aspects of the disclosure provide description techniques for a space of interest of an audio scene. Specifically, the description techniques can provide a layered description of a space of interest in an audio scene. The layered description of the space of interest in the audio scene can provide compacted information of the space of interest for audio coding, transmission and rendering.</p><p id="p-0032" num="0031">Generally, an audio scene is a semantically consistent sound segment that is characterized by a few dominant sources of sound. Thus, the audio scene can be modeled as a collection of sound sources. In some examples, the audio scene is dominated by a few of the collection of sound sources. A space of interest in the audio scene can be defined by borders of the space of interest under consideration in the audio scene. The space of interest of the audio scene can be utilized in audio coding, processing, rendering, and the like.</p><p id="p-0033" num="0032">According to some aspects of the disclosure, some technologies attempt to create, or imitate the physical world through digital simulation that is referred to as immersive media. Immersive media processing can be implemented according an immersive media standard, such as Moving Picture Expert Group Immersive (MPEG-I) suite of standards, including &#x201c;immersive audio&#x201d;, &#x201c;immersive video&#x201d;, and &#x201c;systems support.&#x201d; The immersive media standard can support a VR or an AR presentation in which the user can navigate and interact with the environment using 6 degrees of freedom (6 DoF), that include spatial navigation (x, y, z) and user head orientation (yaw, pitch, roll).</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a diagram illustrating an environment using 6 degrees of freedom (6 DoF) in some examples. The 6 degrees of freedom (6 DoF) can be represented by a spatial navigation (x, y, z) and a user head orientation (yaw, pitch, roll).</p><p id="p-0035" num="0034">According to an aspect of the disclosure, immersive media can be used to impart the feeling that a user is actually present in the virtual world. In some examples, audio of a scene is perceived as in the real world, with sounds coming from associated visual figures. For example, sounds are perceived with the correct location and distance in the scene. Physical movement of the user in the real world is perceived as having matching movement in the scene of the virtual world. Further, the user can interact with the scene and cause sounds that are perceived as realistic and matching the user's experience in the real world.</p><p id="p-0036" num="0035">Generally, a region of interest (ROI) includes samples within a data set identified for a particular purpose. The concept of a ROI can be used in many application areas, such as in medical imaging, geographical information systems, computer vision and optical character recognition, and the like.</p><p id="p-0037" num="0036">In an audio scene, a space of interest can be described for a particular audio purpose. A space of interest of an audio scene can be associated with audio sources that can cause audio effects in the space of the audio scene. In some examples, a space of interest can be defined by an audio scene producer. The audio scene producer can define the space of interest in a 3 dimensional (3D) space and audio inputs as the audio sources that can cause audio effects in the space of interest. The audio inputs and the description of the space of interest can be provided to an audio encoder. The audio encoder can encode the audio inputs into a bitstream, and the space of interest can be included as metadata associated with the encoded audio. The bitstream can be provided to a client device. The client device can decode audio content from the bitstream and renders audio according to the space of interest. For example, when a game player moves in a virtual world into the space of interest, the audio content associated with the space of the interest is played.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a block diagram of a media system (<b>200</b>) according to an embodiment of the disclosure. The media system (<b>200</b>) can be used in various use applications, such as immersive media application, augmented reality (AR) application, virtual reality application, video game application, sports game animation application, a teleconference and telepresence application, a media streaming application, and the like.</p><p id="p-0039" num="0038">The media system (<b>200</b>) includes a media server device (<b>210</b>) and a plurality of media client devices, such as media client devices (<b>260</b>A) and (<b>260</b>B) shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, that can be connected by a network (not shown). In an example, the media server device (<b>210</b>) can include one or more devices with audio coding and video coding functionalities. In an example, the media server device (<b>210</b>) includes a single computing device, such as a desktop computer, a laptop computer, a server computer, a tablet computer and the like. In another example, the media server device (<b>210</b>) includes data center(s), server farm(s), and the like. The media server device (<b>210</b>) can receive video and audio content, and compress the video content and audio content into one or more encoded bitstreams in accordance to suitable media coding standards. The encoded bitstreams can be delivered to the media client devices (<b>260</b>A) and (<b>260</b>B) via the network.</p><p id="p-0040" num="0039">The media client devices (e.g., the media client devices (<b>260</b>A) and (<b>260</b>B)) respectively include one or more devices with video coding and audio coding functionality for media applications. In an example, each of the media client devices includes a computing device, such as a desktop computer, a laptop computer, a server computer, a tablet computer, a wearable computing device, a head mounted display (HMD) device, and the like. The media client device can decode the encoded bitstream in accordance to suitable media coding standards. The decoded video contents and audio contents can be used for media play.</p><p id="p-0041" num="0040">The media server device (<b>210</b>) can be implemented using any suitable technology. In the <figref idref="DRAWINGS">FIG. <b>2</b></figref> example, the media server device (<b>210</b>) includes a processing circuit (<b>230</b>) and an interface circuit (<b>211</b>) coupled together.</p><p id="p-0042" num="0041">The processing circuit (<b>230</b>) can include any suitable processing circuitry, such as one or more central processing units (CPUs), one or more graphics processing units (GPUs), application specific integrated circuit, and the like. In the <figref idref="DRAWINGS">FIG. <b>2</b></figref> example, the processing circuit (<b>230</b>) can be configured to include various encoders, such as an audio encoder (<b>240</b>), a video encoder (not shown), and the like. In an example, one or more CPUs and/or GPUs can execute software to function as the audio encoder (<b>240</b>). In another example, the audio encoder (<b>240</b>) can be implemented using application specific integrated circuits.</p><p id="p-0043" num="0042">The interface circuit (<b>211</b>) can interface the media server device (<b>210</b>) with the network. The interface circuit (<b>211</b>) can include a receiving portion that receives signals from the network and a transmitting portion that transmits signals to the network. For example, the interface circuit (<b>211</b>) can transmit signals that carry the encoded bitstreams to other devices, such as the media client device (<b>260</b>A), the media client device (<b>260</b>B), and the like via the network. The interface circuit (<b>211</b>) can receive signals from the media client devices, such as the media client devices (<b>260</b>A) and (<b>260</b>B).</p><p id="p-0044" num="0043">The network is suitably coupled with the media server device (<b>210</b>) and the media client devices (e.g., the media client devices (<b>260</b>A) and (<b>260</b>B)) via wired and/or wireless connections, such as Ethernet connections, fiber-optic connections, WiFi connections, cellular network connections and the like. The network can include network server devices, storage devices, network devices and the like. The components of the network are suitably coupled together via wired and/or wireless connections.</p><p id="p-0045" num="0044">The media client devices (e.g., the media client devices (<b>260</b>A) and (<b>260</b>B)) are respectively configured to decode the coded bitstreams. In an example, each media client device can perform video decoding to reconstruct a sequence of video frames that can be displayed and can perform audio decoding to generate audio signals for playing.</p><p id="p-0046" num="0045">The media client devices, such as the media client devices (<b>260</b>A) and (<b>260</b>B) can be implemented using any suitable technology. In the <figref idref="DRAWINGS">FIG. <b>2</b></figref> example, the media client device (<b>260</b>A) is shown, but not limited to a head mounted display (HMD) with earphones as user equipment that can be used by user A, and the media client device (<b>260</b>B) is shown, but not limited to a smart phone that is used by user B.</p><p id="p-0047" num="0046">In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the media client device (<b>260</b>A) includes an interface circuit (<b>261</b>A), and a processing circuit (<b>270</b>A) coupled together as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, and the media client device (<b>260</b>B) includes an interface circuit (<b>261</b>B), and a processing circuit (<b>270</b>B) coupled together as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0048" num="0047">The interface circuit (<b>261</b>A) can interface the media client device (<b>260</b>A) with the network. The interface circuit (<b>261</b>A) can include a receiving portion that receives signals from the network and a transmitting portion that transmits signals to the network. For example, the interface circuit (<b>261</b>A) can receive signals carrying data, such as signals carrying the encoded bitstream from the network.</p><p id="p-0049" num="0048">The processing circuit (<b>270</b>A) can include suitable processing circuitry, such as CPU, GPU, application specific integrated circuits and the like. The processing circuit (<b>270</b>A) can be configured to include various components, such an audio decoder (<b>271</b>A), a renderer (<b>272</b>A), and the like.</p><p id="p-0050" num="0049">In some examples, the audio decoder (<b>271</b>A) can decode audio content in an encoded bitstream by selecting a decoding tool suitable for a scheme by which the audio content was encoded. Further, the renderer (<b>272</b>A) can generate a final digital product suitable for the media client device (<b>260</b>A) from audio content decoded from the encoded bitstream. It is noted that the processing circuit (<b>270</b>A) can include other suitable components (not shown), such as mixer, post processing circuit, and the like for further audio processing.</p><p id="p-0051" num="0050">Similarly, the interface circuit (<b>261</b>B) can interface the media client device (<b>260</b>B) with the network. The interface circuit (<b>261</b>B) can include a receiving portion that receives signals from the network and a transmitting portion that transmits signals to the network. For example, the interface circuit (<b>261</b>B) can receive signals carrying data, such as signals carrying the encoded bitstream from the network.</p><p id="p-0052" num="0051">The processing circuit (<b>270</b>B) can include suitable processing circuitry, such as CPU, GPU, application specific integrated circuits and the like. The processing circuit (<b>270</b>B) can be configured to include various components, such an audio decoder (<b>271</b>B), a renderer (<b>272</b>B), and the like.</p><p id="p-0053" num="0052">In some examples, the audio decoder (<b>271</b>B) can decode audio content in an encoded bitstream by selecting a decoding tool suitable for a scheme by which the audio content was encoded. Further, the renderer (<b>272</b>B) can generate a final digital product suitable for the media client device (<b>260</b>B) from audio content decoded from the encoded bitstream. It is noted that the processing circuit (<b>270</b>A) can include other suitable components (not shown), such as mixer, post processing circuit, and the like for further audio processing.</p><p id="p-0054" num="0053">According to some aspects of the disclosure, a layered description of a space of interest in an audio scene is used in the media system (<b>200</b>). The media server device (<b>210</b>), the media client devices (e.g., the media client devices (<b>260</b>A) and (<b>260</b>B)) can process the layered description of the space of interest in the audio scene. For example, the processing circuit (<b>230</b>), the processing circuit (<b>270</b>A), the processing circuit (<b>270</b>B) and the like can determine the space of interest of the audio scene based on the layered description of the space of interest of the audio scene.</p><p id="p-0055" num="0054">In some examples, the media server device (<b>210</b>) receives, for an audio scene, audio inputs and a layered description of a space of interest in the audio scene, from an audio source (<b>201</b>) (e.g., an audio injection server, an audio scene producer device, and the like). In some examples, the audio source (<b>201</b>) includes computing circuitry, such as a desktop computer, a laptop computer, a server computer, a tablet computer and the like. The computing circuitry can generate audio inputs for the audio scene and generate the layered description of the space of interest in the audio scene. The audio inputs for the audio scene and the layered description of the space of interest in the audio scene can be provided to the media server device (<b>210</b>).</p><p id="p-0056" num="0055">In some embodiments, the media server device (<b>210</b>) can determine respective media content to send to the media client devices, encode the media content into bitstreams and send the bitstreams to the media client devices. In some examples, the media server device (<b>210</b>) can determine audio content for the media client device (<b>260</b>A) based on information provided by the media client device (<b>260</b>A), such as a scene information in an application (e.g., game scene, VR scene, and the like). The media server device (<b>210</b>) can determine the audio inputs for the scene (referred to as audio scene in term of audio processing) as the audio content. The media server device (<b>210</b>) can encode the audio content into a bitstream with other suitable information. In an example, the encoded audio content for the audio scene and the layered description of the space of interest of the audio scene can be carried by the bitstream. The layered description of the space of interest of the audio scene can be metadata for the audio content. The media server device (<b>210</b>) can send the bitstream to the media client device (<b>260</b>A). It is noted that the encoded audio content and the layered description of the space of interest of the audio scene can be sent separately in some examples.</p><p id="p-0057" num="0056">When the media client device (<b>260</b>A) receives the encoded audio content and the layered description of the space of interest of the audio scene, the audio decoder (<b>271</b>A) can decode the audio content. The renderer (<b>272</b>A) can generate a final digital product suitable for the media client device (<b>260</b>A) from the audio content based on the space of interest of the audio scene. For example, when a subject in an application moves into the space of interest of the audio scene, the renderer (<b>272</b>A) can render the audio content. In an example, when the subject moves out of the space of interest, the audio content is ignored and not rendered.</p><p id="p-0058" num="0057">In some examples, a space of interest may have an irregular shape. For ease of description, the space of interest can be descripted as a combination of a plurality of several areas (also referred to as subspaces) that are of regular shapes. Each of the several subspaces can be descripted using attributes. According to some aspects of the disclosure, some of these subspaces may share some attribute values. In some examples, a layered description of a space of interest of an audio scene can describe the shared attribute values of two or more subspaces in a separate layer from individual attribute values of the subspaces, thus the description of the space of interest can be more compact.</p><p id="p-0059" num="0058">In the following description, rectangular boxes are used as subspaces of the regular shape for describing the space of interest. It is noted that other regular shape, such as sphere, cylinder, cube, and the like can be used in some examples.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an audio scene that is referred to as a canyon scene in some examples. In an audio scene, a space of interest can be specified by one or more rectangular boxes. In the <figref idref="DRAWINGS">FIG. <b>3</b></figref> example, a space of interest (<b>310</b>) in the canyon scene can be described using four overlapping rectangular boxes (<b>301</b>)-(<b>304</b>).</p><p id="p-0061" num="0060">In some examples, each rectangular box can be defined using 4 attributes: an identifier (id) attribute, a position attribute, an orientation attribute, and a size attribute.</p><p id="p-0062" num="0061">The identifier attribute of a rectangular box can have a value that indicates the rectangular box. For example, the rectangular box (<b>301</b>) can be identified by an identifier &#x201c;box:Box<b>1</b>&#x201d;; the rectangular box (<b>302</b>) can be identified by an identifier &#x201c;box:Box<b>2</b>&#x201d;; the rectangular box (<b>303</b>) can be identified by an identifier &#x201c;box:Box<b>3</b>&#x201d;; and the rectangular box (<b>304</b>) can be identified by an identifier &#x201c;box:Box<b>4</b>&#x201d;.</p><p id="p-0063" num="0062">In some examples, the position attribute of a rectangular box can include three values corresponding to coordinates of the center position of the rectangular box in a 3D space, such as corresponding to x, y, z. In an example, the first value having an index of &#x201c;1&#x201d; corresponds to x coordinate of the center position, the second value having an index of &#x201c;2&#x201d; corresponds toy coordinate of the center position, and the third value having an index of &#x201c;3&#x201d; corresponds to z coordinate of the center position.</p><p id="p-0064" num="0063">In some examples, the orientation attribute of a rectangular box can include three values corresponding to rotation angles along X-axis, Y-axis and Z-axis at the center position of the rectangular box. In an example, the first value having an index of &#x201c;1&#x201d; corresponds to a rotation angle along Y-axis at the center position of the rectangular box, the second value having an index of &#x201c;2&#x201d; corresponds to a rotation angle along X-axis at the center position of the rectangular box, and the third value having an index of &#x201c;3&#x201d; corresponds to a rotation angle along Z-axis at the center position of the rectangular box.</p><p id="p-0065" num="0064">In some examples, the size attribute of a rectangular box can include three values corresponding to side lengths along X-axis, Y-axis and Z-axis. In an example, the first value having an index of &#x201c;1&#x201d; corresponds to side length of the rectangular box along X-axis, the second value having an index of &#x201c;2&#x201d; corresponds to side length of the rectangular box along Y-axis, and the third value having an index of &#x201c;3&#x201d; corresponds to side length of the rectangular box along Z-axis.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a description (<b>400</b>) for the space of interest (<b>310</b>) in the canyon scene. The description (<b>400</b>) includes individual nodes respectively for the rectangular boxes (<b>301</b>)-(<b>304</b>). Specifically, the description (<b>400</b>) includes a description node (<b>401</b>) for the rectangular box (<b>301</b>), a description node (<b>402</b>) for the rectangular box (<b>302</b>), a description node (<b>403</b>) for the rectangular box (<b>303</b>), and a description node (<b>404</b>) for the rectangular box (<b>304</b>).</p><p id="p-0067" num="0066">It is noted that the rectangular boxes (<b>301</b>)-(<b>304</b>) have same (common) attribute values that are separately listed in the individual nodes for the rectangular boxes (<b>301</b>)-(<b>304</b>).</p><p id="p-0068" num="0067">Some aspects of the disclosure provide a layered description of a space of interest of an audio scene. The layered description can include a first layer for common attribute values and a second layer for uncommon attribute values. In the layered description, common attribute values of two or more subspaces (e.g., rectangular boxes) can be explicitly listed in the first layer. The common attribute values can be signaled once for all related description of subspaces. Other uncommon attribute values can be separately listed in the second layer. The layered description can be more compact that the description (<b>400</b>).</p><p id="p-0069" num="0068">In some embodiments, the first layer of the description for common attribute values can be presented first, and followed by the second layer of description for uncommon attribute values of each individual rectangular box. The first layer of description can include one or more common nodes respectively describe common attribute values. The second layer of description can include individual nodes respectively for rectangular boxes. The common attribute values that are presented in the common nodes as common attribute values will not be listed again in the individual nodes for the rectangular boxes unless necessary, for example, with non-duplicated information. For each rectangular box of the space description, common attribute values that are already in the common nodes will not be listed. Instead, the rectangular box will share with the common attribute values listed in the common nodes by default.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a syntax (<b>500</b>) for layered description of a space of interest according to an embodiment of the disclosure. The syntax includes a first layer (<b>510</b>) of description for common attribute values shared by two or more of the rectangular boxes, and a second layer (<b>520</b>) for description for uncommon attribute values of individual rectangular boxes.</p><p id="p-0071" num="0070">The first layer (<b>510</b>) includes a plurality of common nodes. Each common node can include a name (e.g., shown by (<b>511</b>)) for identifying an attribute, and a value (e.g., shown by (<b>512</b>)) for the common attribute value.</p><p id="p-0072" num="0071">In an example, four rectangular boxes of a space of interest have a same size, such as &#x201c;20.0 2.50 15.0&#x201d;, and <figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a layered description (<b>600</b>) for the space of interest with four rectangular boxes of the same size using the syntax (<b>500</b>). The layered description (<b>600</b>) includes a first layer (<b>610</b>) of description for common nodes shared by two or more of the rectangular boxes, and a second layer (<b>620</b>) of description for uncommon attribute values of each individual rectangular box.</p><p id="p-0073" num="0072">Specifically, the first layer (<b>610</b>) includes a common node. The common node has a name &#x201c;size&#x201d; (e.g., shown by (<b>611</b>)) for identifying the size attribute, and have a value (e.g., shown by (<b>612</b>)) of &#x201c;20.0 2.50 15.0&#x201d; for specifying the common attribute value that is the size of the rectangular boxes. The second layer (<b>620</b>) includes description for uncommon attribute values of each individual rectangular box, such as the position attribute and the orientation attribute. For example, the second layer (<b>620</b>) includes four individual nodes respectively for the four rectangular boxes.</p><p id="p-0074" num="0073">In the <figref idref="DRAWINGS">FIG. <b>6</b></figref> example, the individual nodes do not include the size attribute. The rectangular boxes that are identified by &#x201c;box:Box<b>1</b>&#x201d;, &#x201c;box:Box<b>2</b>&#x201d;, &#x201c;box:Box<b>3</b>&#x201d;, &#x201c;box:Box<b>4</b>&#x201d; can refer to the common node to retrieve the size attribute value &#x201c;20.0 2.50 15.0&#x201d; for the size attribute.</p><p id="p-0075" num="0074">In some examples, an attribute may have a plurality of subfields. For example, the size attribute of a rectangular box has a first subfield of a side length along X-axis, a second subfield of a side length along Y-axis, and a third subfield of a side length along Z-axis. In some examples, two or more rectangular boxes do not share the whole the size attribute, but can share one or more subfields of the size attribute.</p><p id="p-0076" num="0075">In an example, in the canyon scene example of <figref idref="DRAWINGS">FIG. <b>3</b></figref> and the description in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, four rectangular boxes (<b>301</b>)-(<b>304</b>) of the space of interest (<b>310</b>) have the same height 2.5 (side length along Y-axis), which is the second subfield in the size attribute. In an example, the height information (side length along Y-axis) of the rectangular boxes (<b>301</b>)-(<b>304</b>) can be regarded as a common attribute (e.g., common subfield attribute) and can be signaled only once.</p><p id="p-0077" num="0076">In another example, the four rectangular boxes (<b>301</b>)-(<b>304</b>) share some subfields of the orientation attribute. Specifically, the four rectangular boxes (<b>301</b>)-(<b>304</b>) share the second subfield value (e.g., &#x201c;0&#x201d;) and the third subfield value (e.g., &#x201c;0&#x201d;) of the orientation attribute. In an example, the second subfield and the third subfield of the orientation attribute can be regarded as common subfields of the orientation attribute and signaled once for all rectangular boxes (<b>301</b>)-(<b>304</b>).</p><p id="p-0078" num="0077">In an embodiment, a layered description of the space of interest can include a first layer of description for common attribute values and/or common subfield attribute values, and a second layer of description for uncommon attribute values and/or uncommon subfield attribute values of individual rectangular boxes. For example, a common subfield of an attribute can be listed in a common node at the first layer (also referred to as parent level in an example). The common node can include a name of the attribute, an index for the common subfield, and value for the common subfield attribute value. In the layered description of the space of interest, for a rectangular box, if a subfield of an attribute is already listed in the common node, the subfield value of the attribute in the rectangular box will not be listed, in the second layer, in an individual node associated with the rectangular box. Instead, the attribute of the rectangular box will share with the common subfield attribute value listed in the common node by default in an example.</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a syntax (<b>700</b>) for a layered description of a space of interest according to an embodiment of the disclosure. The syntax includes a first layer (<b>710</b>) of description for common attribute values and/or common subfield attribute values shared by two or more of the rectangular boxes, and a second layer (<b>720</b>) for description for uncommon attribute values and/or uncommon subfield attribute values of individual rectangular boxes.</p><p id="p-0080" num="0079">The first layer (<b>710</b>) includes a plurality of common nodes. Each common node can include a name (e.g., shown by (<b>711</b>)) for identifying an attribute, an index (e.g., shown by (<b>713</b>)) for identifying a subfield, and a value (e.g., shown by (<b>712</b>)) for the common subfield attribute value.</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a layered description (<b>800</b>) for the space of interest (<b>310</b>) with the four rectangular boxes (<b>301</b>)-(<b>304</b>) according to the syntax (<b>700</b>). The layered description (<b>800</b>) includes a first layer (<b>810</b>) of description for common nodes shared by two or more of the rectangular boxes, and a second layer (<b>820</b>) of description for uncommon attribute values and/or uncommon subfield attribute values of individual rectangular boxes.</p><p id="p-0082" num="0081">Specifically, the first layer (<b>810</b>) includes a first common node for subfields of the orientation attribute, and a second common node for a subfield of the size attribute. The first common node has a name &#x201c;orientation&#x201d; (e.g., shown by (<b>811</b>)) for identifying the orientation attribute, has an index &#x201c;2 3&#x201d; (e.g., shown by (<b>813</b>)) for identifying the second subfield and the third subfield, and has a value &#x201c;0.00 0.00&#x201d; (e.g., shown by (<b>812</b>)) for specifying the common subfield attribute values. Thus, the first common node lists that the common value of the second subfield of the orientation attribute is &#x201c;0.00&#x201d;, and the common value of the third subfield of the orientation is &#x201c;0.00&#x201d;.</p><p id="p-0083" num="0082">Similarly, the second common node has a name &#x201c;size&#x201d; for identifying the size attribute, has an index &#x201c;2&#x201d; for identifying the second subfield, and has a value of &#x201c;2.50&#x201d; for specifying the common subfield attribute value. Thus, the second common node list that the common value of the second subfield of the size attribute is &#x201c;2.50&#x201d;.</p><p id="p-0084" num="0083">The second layer (<b>820</b>) includes description for uncommon attribute values and/or uncommon subfield attribute values of individual rectangular boxes, such as the position attribute, the orientation attribute and the size attribute. In the <figref idref="DRAWINGS">FIG. <b>8</b></figref> example, the second layer (<b>820</b>) includes four individual nodes respectively for the four rectangular boxes. The four rectangular boxes share the information in the common nodes. For each individual node, the orientation attribute includes the first subfield value, and does not include the second subfield value and the third subfield value. The second subfield value and the third subfield value of the orientation attribute can be retrieved from the first common node in the first layer (<b>810</b>).</p><p id="p-0085" num="0084">Also in the second layer (<b>820</b>), for each individual node, the size attribute includes the first subfield value and the third subfield value and does not include the second subfield value. The second subfield value of the size attribute can be retrieved from the second common node in the first layer (<b>810</b>).</p><p id="p-0086" num="0085">It is noted that, in the <figref idref="DRAWINGS">FIG. <b>8</b></figref> example, the information in the first common node and the second common node is shared by all of the four rectangular boxes. In some embodiments, information in a common node of the first layer does not need to be shared by all of the individual nodes in the second layer.</p><p id="p-0087" num="0086">In an embodiment, a layered description of the space of interest can include a first layer of description for common attribute values and/or common subfield attribute values shared by a subset of individual nodes for the subspaces (e.g., rectangular boxes), and a second layer of description for uncommon attribute values and/or uncommon subfield attribute values of individual subspaces (e.g., rectangular boxes). If a subspace does not share a common attribute value specified in the common nodes of the first layer, the individual node in the second layer and associated with the subspace can list an individual attribute value that is different from the common attribute value.</p><p id="p-0088" num="0087">In the second layer, for a rectangular box, if an attribute value is the same as the listed in the common node, the attribute will not be listed; if an attribute value of a rectangular box is different from the common attribute, the rectangular box's attribute value can be listed.</p><p id="p-0089" num="0088">In an example, the rectangular box (<b>301</b>) has a side length of &#x201c;29.66&#x201d; along the X-axis that is different from other three rectangular box (<b>302</b>)-(<b>304</b>) that have a side length of &#x201c;19.23&#x201d; along the X-axis.</p><p id="p-0090" num="0089"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows a layered description (<b>900</b>) for the space of interest (<b>310</b>) with the four rectangular boxes (<b>301</b>)-(<b>304</b>) according to the syntax (<b>700</b>). The layered description (<b>900</b>) includes a first layer (<b>910</b>) of description of common nodes for common attribute values shared by two or more of the rectangular boxes, and a second layer (<b>920</b>) of description of individual nodes for uncommon attribute values and/or uncommon subfield attribute values of individual rectangular boxes.</p><p id="p-0091" num="0090">Specifically, the first layer (<b>910</b>) includes a first common node for subfields of the orientation attribute, and a second common node for subfields of the size attribute. The first common node has a name &#x201c;orientation&#x201d; for identifying the orientation attribute, has an index &#x201c;2 3&#x201d; for identifying the second subfield and the third subfield, and has a value &#x201c;0.00 0.00&#x201d; for specifying the common subfield attribute values. Thus, the first common node lists that the common value of the second subfield of the orientation attribute is &#x201c;0.00&#x201d;, and the common value of the third subfield of the orientation is &#x201c;0.00&#x201d;.</p><p id="p-0092" num="0091">Further, the second common node has a name &#x201c;size&#x201d; for identifying the size attribute, has an index &#x201c;1 2&#x201d; for identifying the first subfield and the second subfield, and has a value of &#x201c;19.23 2.50&#x201d; for specifying the common subfield attribute values for a subset of the rectangular boxes, such as the rectangular boxes (<b>302</b>)-(<b>304</b>). Thus, the second common node list that the common values of the first subfield and second subfield of the size attribute are &#x201c;19.23 2.50&#x201d;.</p><p id="p-0093" num="0092">The second layer (<b>920</b>) includes description of individual nodes for uncommon attribute values and/or uncommon subfield attribute values of individual rectangular boxes, such as the position attribute, the orientation attribute and the size attribute. In the second layer (<b>920</b>), for each individual nodes, the orientation attribute includes the first subfield value, and does not include the second subfield value and the third subfield value. The second subfield value and the third subfield value of the orientation attribute can be retrieved from the first common node in the first layer.</p><p id="p-0094" num="0093">In the second layer (<b>920</b>), for individual nodes associated with a subset of the rectangular boxes (e.g., box:Box<b>2</b>, box:Box<b>3</b>, and box:Box<b>4</b>), the size attribute includes the third subfield value and does not include the first subfield and the second subfield value. The first subfield value and second subfield value of the size attribute can be referred to the second common node in the first layer (<b>910</b>), and can be retrieved from the second common node in the first layer (<b>910</b>).</p><p id="p-0095" num="0094">In the second layer (<b>920</b>), in the individual node for the rectangular box (e.g., box:Box<b>1</b>), the size attribute includes the first subfield value, the second subfield and the third subfield value. For example, the size attribute of the rectangular box &#x201c;box:Box<b>1</b>&#x201d; is listed with all three subfield values &#x201c;29.66 2.50 20.47&#x201d; as shown by (<b>921</b>). Thus, information in the individual node can overwrite the information in the common nodes.</p><p id="p-0096" num="0095">In an embodiment, a layered description of the space of interest can include a first layer of description for common attribute values and/or common subfield attribute values shared by a subset of individual nodes for the subspaces (e.g., rectangular boxes), and a second layer of description for uncommon attribute values and/or uncommon subfield attribute values of individual subspaces (e.g., rectangular boxes). If a subspace does not share a common attribute value specified in the common nodes of the first layer, the individual node in the second layer and associated with the subspace can list a difference value that is a difference between an uncommon attribute value of the subspace and the common attribute value in the first layer.</p><p id="p-0097" num="0096">In the second layer, for a rectangular box, if an attribute value is the same as listed in the common node, the attribute value will not be listed; if an attribute value of a rectangular box is different from the common attribute value, a difference between the rectangular box's attribute value and the common attribute value can be listed.</p><p id="p-0098" num="0097">In an example, the rectangular box (<b>301</b>) has a side length of &#x201c;29.66&#x201d; along the X-axis that is different from other three rectangular box (<b>302</b>)-(<b>304</b>) that have a side length of &#x201c;19.23&#x201d; along the X-axis. The difference of the side length of &#x201c;29.66&#x201d; to the side length of &#x201c;19.23&#x201d; is &#x201c;10.43&#x201d;.</p><p id="p-0099" num="0098"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows a layered description (<b>1000</b>) for the space of interest (<b>310</b>) with the four rectangular boxes (<b>301</b>)-(<b>304</b>) according to the syntax (<b>700</b>). The layered description (<b>1000</b>) includes a first layer (<b>1010</b>) of description for common nodes shared by two or more of the rectangular boxes, and a second layer (<b>1020</b>) of description for uncommon attribute values and/or uncommon subfield attribute values of individual rectangular boxes.</p><p id="p-0100" num="0099">Specifically, the first layer (<b>1010</b>) includes a first common node for subfields of the orientation attribute, and a second common node for subfields of the size attribute. The first common node has a name &#x201c;orientation&#x201d; for identifying the orientation attribute, has an index &#x201c;2 3&#x201d; for identifying the second subfield and the third subfield, and has a value &#x201c;0.00 0.00&#x201d; for specifying the common subfield attribute value. Thus, the first common node lists that the common value of the second subfield of the orientation attribute is &#x201c;0.00&#x201d;, and the common value of the third subfield of the orientation is &#x201c;0.00&#x201d;.</p><p id="p-0101" num="0100">Further, the second common node has a name &#x201c;size&#x201d; for identifying the size attribute, has an index &#x201c;1 2&#x201d; for identifying the first subfield and the second subfield, and has a value of &#x201c;19.23 2.50&#x201d; for specifying the common subfield attribute value for a subset of the rectangular boxes, such as the rectangular boxes (<b>302</b>)-(<b>304</b>). Thus, the second common node list that the common values of the first subfield and second subfield of the size attribute are &#x201c;19.23 2.50&#x201d;.</p><p id="p-0102" num="0101">The second layer (<b>1020</b>) includes description of individual nodes for uncommon attribute values and/or uncommon subfield attribute values of individual rectangular boxes, such as the position attribute, the orientation attribute and the size attribute. In the second layer (<b>1020</b>), in each individual node, the orientation attribute includes the first subfield value, and does not include the second subfield value and the third subfield value. The second subfield value and the third subfield value of the orientation attribute can be retrieved from the first common node in the first layer (<b>1010</b>).</p><p id="p-0103" num="0102">In the second layer (<b>1020</b>), in the individual nodes for the subset of rectangular boxes (e.g., box:Box<b>2</b>, box:Box<b>3</b>, and box:Box<b>4</b>), the size attribute includes the third subfield value and does not include the first subfield and the second subfield value. The first subfield value and second subfield value of the size attribute can be referred to the second common node in the first layer (<b>1010</b>), and can be retrieved from the second common node in the first layer (<b>1010</b>).</p><p id="p-0104" num="0103">In the second layer (<b>1020</b>), in the individual node associated with the rectangular box (e.g., box:Box<b>1</b>), the size attribute includes a first subfield difference value, a second subfield difference value and the third subfield value. For example, the size attribute of the rectangular box &#x201c;box:Box<b>1</b>&#x201d; is listed as &#x201c;10.43 0.00 20.47&#x201d; as shown by (<b>1021</b>). By checking with the second common node, the first subfield value of the size attribute of &#x201c;box:Box<b>1</b>&#x201d; can be restored to a sum of &#x201c;19.23&#x201d; and &#x201c;10.43&#x201d; which is equal to 29.66, and the second subfield value of the size attribute of &#x201c;box:Box<b>1</b>&#x201d; can be restored to a sum of &#x201c;2.5&#x201d; and &#x201c;0&#x201d; which is equal to 2.5.</p><p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows a flow chart outlining a process (<b>1100</b>) according to an embodiment of the disclosure. The process (<b>1100</b>) can be performed by an audio source device, such as the audio source device (<b>201</b>). In some embodiments, the process (<b>1100</b>) is implemented in software instructions, thus when the processing circuitry executes the software instructions, the processing circuitry performs the process (<b>1100</b>). The process starts at (S<b>1101</b>) and proceeds to (S<b>1110</b>).</p><p id="p-0106" num="0105">At (S<b>1110</b>), two or more subspaces in a plurality of subspaces for a space of interest are determined having a common attribute value for an attribute.</p><p id="p-0107" num="0106">In some examples, the plurality of subspaces are rectangular boxes. Each rectangular box can be defined by at least a position attribute, an orientation attribute and a size attribute.</p><p id="p-0108" num="0107">At (S<b>1120</b>), a common node is formed in a first layer of a layered description of the space of interest. The common mode includes the common attribute value for the attribute.</p><p id="p-0109" num="0108">At (S<b>1130</b>), the attribute is removed respectively from individual nodes associated with the two or more subspaces. The individual nodes are in a second layer of the layered description of the space of interest.</p><p id="p-0110" num="0109">In an example, the common node identifies a name for an attribute, and the common attribute value. The common attribute value can be removed from the individual nodes associated with the two or more subspaces.</p><p id="p-0111" num="0110">In another example, the common node identifies a name of an attribute, an index of a subfield of the attribute and the common attribute value as a common subfield attribute value. The common subfield attribute value can be removed from the individual nodes associated with the two or more subspaces.</p><p id="p-0112" num="0111">In some examples, the common node is common to the plurality of subspaces. The common attribute value can be removed from each of the individual nodes associated with the plurality of subspaces.</p><p id="p-0113" num="0112">In some examples, the common node is common to a subset of the plurality of subspaces. The common attribute value can be removed from each of individual nodes associated with the subset of the plurality of subspaces. For a subspace that is not in the subset, an individual node associated with the subset can include a different attribute value from the common attribute value for the attribute.</p><p id="p-0114" num="0113">In some examples, the common node is common to a subset of the plurality of subspaces. The common attribute value can be removed from each of individual nodes associated with the subset of the plurality of subspaces. For a subspace that is not in the subset, an individual node associated with the subset can include a difference value of a specific attribute value (of the subspace) to the common attribute value for the attribute.</p><p id="p-0115" num="0114">Then, the process proceeds to (S<b>1199</b>) and terminates.</p><p id="p-0116" num="0115">The process (<b>1100</b>) can be suitably adapted. Step(s) in the process (<b>1100</b>) can be modified and/or omitted. Additional step(s) can be added. Any suitable order of implementation can be used.</p><p id="p-0117" num="0116"><figref idref="DRAWINGS">FIG. <b>12</b></figref> shows a flow chart outlining a process (<b>1200</b>) according to an embodiment of the disclosure. The process (<b>1200</b>) can be performed by a media server device, such as the media server device (<b>210</b>). In some embodiments, the process (<b>1200</b>) is implemented in software instructions, thus when the processing circuitry executes the software instructions, the processing circuitry performs the process (<b>1200</b>). The process starts at (S<b>1201</b>) and proceeds to (S<b>1210</b>).</p><p id="p-0118" num="0117">At (S<b>1210</b>), audio inputs for an audio scene and a layered description of a space of interest associated with the audio inputs are received. The space of interest includes a plurality of subspaces. The layered description includes a first layer and a second layer. The first layer includes a common node with a first value that is a common attribute value of two or more subspaces in the plurality of subspaces. The second layer includes individual nodes respectively associated with the plurality of subspaces.</p><p id="p-0119" num="0118">In some examples, the plurality of subspaces are rectangular boxes. Each rectangular box can be defined by at least a position attribute, an orientation attribute and a size attribute.</p><p id="p-0120" num="0119">At (S<b>1220</b>), the plurality of subspaces of the space of interest are determined based on the layered description.</p><p id="p-0121" num="0120">In an example, the common node identifies a name for an attribute, and the first value is an attribute value of the attribute. The first value can be retrieved from the common node in the first layer as the attribute value of the attribute for a subspace in the plurality of subspaces.</p><p id="p-0122" num="0121">In another example, the common node identifies a name of an attribute and an index of a subfield of the attribute and the first value is a subfield attribute value for the subfield of the attribute. The first value can be retrieved from the common node in the first layer as the subfield attribute value for the subfield of the attribute of a subspace in the plurality of subspaces.</p><p id="p-0123" num="0122">In some examples, the common node with the first value is common to the plurality of subspaces. The first value can be retrieved from the common node in the first layer as an attribute value of an attribute for each of the plurality of subspaces.</p><p id="p-0124" num="0123">In some examples, the common node with the first value is common to a subset of the plurality of subspaces. The first value is retrieved, from the common node in the first layer, as an attribute value of an attribute for a first subspace in response to a first individual node associated with the first subspace missing a value for the attribute. Further, a second value associated with the attribute of a second subspace is retrieved, from a second individual node associated with a second subspace in response to an existence of the second value associated with the attribute in the second individual node.</p><p id="p-0125" num="0124">In some examples, the common node with the first value is common to a subset of the plurality of subspaces. The first value is retrieved, from the common node in the first layer as an attribute value of an attribute of a first subspace in response to a first individual node associated with the first subspace missing a value for the attribute. Further, a difference value with the attribute of a second subspace is retrieved, from a second individual node associated with the second subspace. Then, a second value for the attribute of the second subspace is computed based on the first value and the difference value, such as a sum of the first value and the difference value.</p><p id="p-0126" num="0125">At (S<b>1230</b>), a bitstream carrying the audio inputs and the layered description of the space of interest is transmitted to a client device in response to information provided from the client device. In an example, the information provided from the client device indicates a scene change to the audio scene. In another example the information provided from the client device indicate a movement by a subject in an application being associated with the space of interest, such as moving into the space of interest, and the like.</p><p id="p-0127" num="0126">Then, the process proceeds to (S<b>1299</b>) and terminates.</p><p id="p-0128" num="0127">The process (<b>1200</b>) can be suitably adapted. Step(s) in the process (<b>1200</b>) can be modified and/or omitted. Additional step(s) can be added. Any suitable order of implementation can be used.</p><p id="p-0129" num="0128"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows a flow chart outlining a process (<b>1300</b>) according to an embodiment of the disclosure. The process (<b>1300</b>) can be performed by a media client device, such as the media client device (<b>260</b>A), the media client device (<b>260</b>B), and the like. In some embodiments, the process (<b>1300</b>) is implemented in software instructions, thus when the processing circuitry executes the software instructions, the processing circuitry performs the process (<b>1300</b>). The process starts at (S<b>1301</b>) and proceeds to (S<b>1310</b>).</p><p id="p-0130" num="0129">At (S<b>1310</b>), audio inputs associated with a layered description for a space of interest in an audio scene are received. The space of interest includes a plurality of subspaces. The layered description includes a first layer and a second layer. The first layer has a common node with a first value that is a common attribute value of two or more subspaces in the plurality of subspaces. The second layer has individual nodes respectively associated with each of the plurality of subspaces.</p><p id="p-0131" num="0130">In some examples, a bitstream carrying the audio inputs and the layered description of the space of interest (e.g., as metadata of the audio inputs) is received. The bitstream is decoded to obtain the audio inputs and the layered description of the space of interest.</p><p id="p-0132" num="0131">In some examples, the plurality of subspaces are rectangular boxes. Each rectangular box can be defined by at least a position attribute, an orientation attribute and a size attribute.</p><p id="p-0133" num="0132">At (S<b>1320</b>), the plurality of subspaces of the space of interest are determined based on the layered description.</p><p id="p-0134" num="0133">In an example, the common node identifies a name for an attribute, and the first value is an attribute value of the attribute. The first value can be retrieved from the common node in the first layer as the attribute value of the attribute for a subspace in the plurality of subspaces.</p><p id="p-0135" num="0134">In another example, the common node identifies a name of an attribute and an index of a subfield of the attribute and the first value is a subfield attribute value for the subfield of the attribute. The first value can be retrieved from the common node in the first layer as the subfield attribute value for the subfield of the attribute of a subspace in the plurality of subspaces.</p><p id="p-0136" num="0135">In some examples, the common node with the first value is common to the plurality of subspaces. The first value can be retrieved from the common node in the first layer as an attribute value of an attribute for each of the plurality of subspaces.</p><p id="p-0137" num="0136">In some examples, the common node with the first value is common to a subset of the plurality of subspaces. The first value is retrieved, from the common node in the first layer, as an attribute value of an attribute for a first subspace in response to a first individual node associated with the first subspace missing a value for the attribute. Further, a second value associated with the attribute of a second subspace is retrieved, from a second individual node associated with a second subspace in response to an existence of the second value associated with the attribute in the second individual node.</p><p id="p-0138" num="0137">In some examples, the common node with the first value is common to a subset of the plurality of subspaces. The first value is retrieved, from the common node in the first layer as an attribute value of an attribute of a first subspace in response to a first individual node associated with the first subspace missing a value for the attribute. Further, a difference value with the attribute of a second subspace is retrieved, from a second individual node associated with the second subspace. Then, a second value for the attribute of the second subspace is computed based on the first value and the difference value.</p><p id="p-0139" num="0138">At (S<b>1330</b>), an audio output is rendered based on the audio inputs in response to a location of a subject of the audio scene being in the space of interest. For example, the audio scene corresponds to a gaming scene in a gaming application, and the subject of the audio scene is a game player in the gaming application. The audio output can be rendered in response to the game player moving into the space of interest of the gaming scene. In some examples, the audio inputs can be ignored without rendering in response to the location of the subject of the audio scene being outside of the space of interest.</p><p id="p-0140" num="0139">Then, the process proceeds to (S<b>1399</b>) and terminates.</p><p id="p-0141" num="0140">The process (<b>1300</b>) can be suitably adapted. Step(s) in the process (<b>1300</b>) can be modified and/or omitted. Additional step(s) can be added. Any suitable order of implementation can be used.</p><p id="p-0142" num="0141">The techniques described above, can be implemented as computer software using computer-readable instructions and physically stored in one or more computer-readable media. For example, <figref idref="DRAWINGS">FIG. <b>14</b></figref> shows a computer system (<b>1400</b>) suitable for implementing certain embodiments of the disclosed subject matter.</p><p id="p-0143" num="0142">The computer software can be coded using any suitable machine code or computer language, that may be subject to assembly, compilation, linking, or like mechanisms to create code comprising instructions that can be executed directly, or through interpretation, micro-code execution, and the like, by one or more computer central processing units (CPUs), Graphics Processing Units (GPUs), and the like.</p><p id="p-0144" num="0143">The instructions can be executed on various types of computers or components thereof, including, for example, personal computers, tablet computers, servers, smartphones, gaming devices, internet of things devices, and the like.</p><p id="p-0145" num="0144">The components shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref> for computer system (<b>1400</b>) are exemplary in nature and are not intended to suggest any limitation as to the scope of use or functionality of the computer software implementing embodiments of the present disclosure. Neither should the configuration of components be interpreted as having any dependency or requirement relating to any one or combination of components illustrated in the exemplary embodiment of a computer system (<b>1400</b>).</p><p id="p-0146" num="0145">Computer system (<b>1400</b>) may include certain human interface input devices. Such a human interface input device may be responsive to input by one or more human users through, for example, tactile input (such as: keystrokes, swipes, data glove movements), audio input (such as: voice, clapping), visual input (such as: gestures), olfactory input (not depicted). The human interface devices can also be used to capture certain media not necessarily directly related to conscious input by a human, such as audio (such as: speech, music, ambient sound), images (such as: scanned images, photographic images obtain from a still image camera), video (such as two-dimensional video, three-dimensional video including stereoscopic video).</p><p id="p-0147" num="0146">Input human interface devices may include one or more of (only one of each depicted): keyboard (<b>1401</b>), mouse (<b>1402</b>), trackpad (<b>1403</b>), touch screen (<b>1410</b>), data-glove (not shown), joystick (<b>1405</b>), microphone (<b>1406</b>), scanner (<b>1407</b>), camera (<b>1408</b>).</p><p id="p-0148" num="0147">Computer system (<b>1400</b>) may also include certain human interface output devices. Such human interface output devices may be stimulating the senses of one or more human users through, for example, tactile output, sound, light, and smell/taste. Such human interface output devices may include tactile output devices (for example tactile feedback by the touch-screen (<b>1410</b>), data-glove (not shown), or joystick (<b>1405</b>), but there can also be tactile feedback devices that do not serve as input devices), audio output devices (such as: speakers (<b>1409</b>), headphones (not depicted)), visual output devices (such as screens (<b>1410</b>) to include CRT screens, LCD screens, plasma screens, OLED screens, each with or without touch-screen input capability, each with or without tactile feedback capability&#x2014;some of which may be capable to output two dimensional visual output or more than three dimensional output through means such as stereographic output; virtual-reality glasses (not depicted), holographic displays and smoke tanks (not depicted)), and printers (not depicted).</p><p id="p-0149" num="0148">Computer system (<b>1400</b>) can also include human accessible storage devices and their associated media such as optical media including CD/DVD ROM/RW (<b>1420</b>) with CD/DVD or the like media (<b>1421</b>), thumb-drive (<b>1422</b>), removable hard drive or solid state drive (<b>1423</b>), legacy magnetic media such as tape and floppy disc (not depicted), specialized ROM/ASIC/PLD based devices such as security dongles (not depicted), and the like.</p><p id="p-0150" num="0149">Those skilled in the art should also understand that term &#x201c;computer readable media&#x201d; as used in connection with the presently disclosed subject matter does not encompass transmission media, carrier waves, or other transitory signals.</p><p id="p-0151" num="0150">Computer system (<b>1400</b>) can also include an interface (<b>1454</b>) to one or more communication networks (<b>1455</b>). Networks can for example be wireless, wireline, optical. Networks can further be local, wide-area, metropolitan, vehicular and industrial, real-time, delay-tolerant, and so on. Examples of networks include local area networks such as Ethernet, wireless LANs, cellular networks to include GSM, 3G, 4G, 5G, LTE and the like, TV wireline or wireless wide area digital networks to include cable TV, satellite TV, and terrestrial broadcast TV, vehicular and industrial to include CANBus, and so forth. Certain networks commonly require external network interface adapters that attached to certain general purpose data ports or peripheral buses (<b>1449</b>) (such as, for example USB ports of the computer system (<b>1400</b>)); others are commonly integrated into the core of the computer system (<b>1400</b>) by attachment to a system bus as described below (for example Ethernet interface into a PC computer system or cellular network interface into a smartphone computer system). Using any of these networks, computer system (<b>1400</b>) can communicate with other entities. Such communication can be uni-directional, receive only (for example, broadcast TV), uni-directional send-only (for example CANbus to certain CANbus devices), or bi-directional, for example to other computer systems using local or wide area digital networks. Certain protocols and protocol stacks can be used on each of those networks and network interfaces as described above.</p><p id="p-0152" num="0151">Aforementioned human interface devices, human-accessible storage devices, and network interfaces can be attached to a core (<b>1440</b>) of the computer system (<b>1400</b>).</p><p id="p-0153" num="0152">The core (<b>1440</b>) can include one or more Central Processing Units (CPU) (<b>1441</b>), Graphics Processing Units (GPU) (<b>1442</b>), specialized programmable processing units in the form of Field Programmable Gate Areas (FPGA) (<b>1443</b>), hardware accelerators for certain tasks (<b>1444</b>), graphics adapters (<b>1450</b>), and so forth. These devices, along with Read-only memory (ROM) (<b>1445</b>), Random-access memory (<b>1446</b>), internal mass storage such as internal non-user accessible hard drives, SSDs, and the like (<b>1447</b>), may be connected through a system bus (<b>1448</b>). In some computer systems, the system bus (<b>1448</b>) can be accessible in the form of one or more physical plugs to enable extensions by additional CPUs, GPU, and the like. The peripheral devices can be attached either directly to the core's system bus (<b>1448</b>), or through a peripheral bus (<b>1449</b>). In an example, the screen (<b>1410</b>) can be connected to the graphics adapter (<b>1450</b>). Architectures for a peripheral bus include PCI, USB, and the like.</p><p id="p-0154" num="0153">CPUs (<b>1441</b>), GPUs (<b>1442</b>), FPGAs (<b>1443</b>), and accelerators (<b>1444</b>) can execute certain instructions that, in combination, can make up the aforementioned computer code. That computer code can be stored in ROM (<b>1445</b>) or RAM (<b>1446</b>). Transitional data can be also be stored in RAM (<b>1446</b>), whereas permanent data can be stored for example, in the internal mass storage (<b>1447</b>). Fast storage and retrieve to any of the memory devices can be enabled through the use of cache memory, that can be closely associated with one or more CPU (<b>1441</b>), GPU (<b>1442</b>), mass storage (<b>1447</b>), ROM (<b>1445</b>), RAM (<b>1446</b>), and the like.</p><p id="p-0155" num="0154">The computer readable media can have computer code thereon for performing various computer-implemented operations. The media and computer code can be those specially designed and constructed for the purposes of the present disclosure, or they can be of the kind well known and available to those having skill in the computer software arts.</p><p id="p-0156" num="0155">As an example and not by way of limitation, the computer system having architecture (<b>1400</b>), and specifically the core (<b>1440</b>) can provide functionality as a result of processor(s) (including CPUs, GPUs, FPGA, accelerators, and the like) executing software embodied in one or more tangible, computer-readable media. Such computer-readable media can be media associated with user-accessible mass storage as introduced above, as well as certain storage of the core (<b>1440</b>) that are of non-transitory nature, such as core-internal mass storage (<b>1447</b>) or ROM (<b>1445</b>). The software implementing various embodiments of the present disclosure can be stored in such devices and executed by core (<b>1440</b>). A computer-readable medium can include one or more memory devices or chips, according to particular needs. The software can cause the core (<b>1440</b>) and specifically the processors therein (including CPU, GPU, FPGA, and the like) to execute particular processes or particular parts of particular processes described herein, including defining data structures stored in RAM (<b>1446</b>) and modifying such data structures according to the processes defined by the software. In addition or as an alternative, the computer system can provide functionality as a result of logic hardwired or otherwise embodied in a circuit (for example: accelerator (<b>1444</b>)), which can operate in place of or together with software to execute particular processes or particular parts of particular processes described herein. Reference to software can encompass logic, and vice versa, where appropriate. Reference to a computer-readable media can encompass a circuit (such as an integrated circuit (IC)) storing software for execution, a circuit embodying logic for execution, or both, where appropriate. The present disclosure encompasses any suitable combination of hardware and software.</p><p id="p-0157" num="0156">While this disclosure has described several exemplary embodiments, there are alterations, permutations, and various substitute equivalents, which fall within the scope of the disclosure. It will thus be appreciated that those skilled in the art will be able to devise numerous systems and methods which, although not explicitly shown or described herein, embody the principles of the disclosure and are thus within the spirit and scope thereof.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of media processing in a device, comprising:<claim-text>receiving audio inputs associated with a layered description for a space of interest in an audio scene, the space of interest comprising a plurality of subspaces, the layered description comprising a first layer and a second layer, the first layer having a common node with a first value that is a common attribute value of two or more subspaces in the plurality of subspaces, and the second layer having individual nodes respectively associated with each of the plurality of subspaces;</claim-text><claim-text>determining, by a processor of the device, the plurality of subspaces of the space of interest based on the layered description; and</claim-text><claim-text>rendering, by the processor, an audio output based on the audio inputs in response to a location of a subject of the audio scene being in the space of interest.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the plurality of subspaces are rectangular boxes that are defined by at least a position attribute, an orientation attribute and a size attribute.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the common node identifies a name for an attribute, and the first value is an attribute value of the attribute, and the determining the plurality of subspaces comprises:<claim-text>retrieving, from the common node in the first layer, the first value as the attribute value of the attribute for a subspace in the plurality of subspaces.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the common node identifies a name of an attribute and an index of a subfield of the attribute, and the first value is a subfield attribute value for the subfield of the attribute, and the determining the plurality of subspaces comprises:<claim-text>retrieving, from the common node in the first layer, the first value as the subfield attribute value for the subfield of the attribute of a subspace in the plurality of subspaces.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the common node with the first value is common to the plurality of subspaces, and the determining the plurality of subspaces further comprises:<claim-text>retrieving, from the common node in the first layer, the first value as an attribute value of an attribute for each of the plurality of subspaces.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the common node with the first value is common to a subset of the plurality of subspaces, and the determining the plurality of subspaces further comprises:<claim-text>retrieving, from the common node in the first layer, the first value as an attribute value of an attribute for a first subspace in response to a first individual node associated with the first subspace missing a value for the attribute; and</claim-text><claim-text>retrieving, from a second individual node associated with a second subspace, a second value associated with the attribute for the second subspace in response to an existence of the second value associated with the attribute in the second individual node.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the common node with the first value is common to a subset of the plurality of subspaces, and the determining the plurality of subspaces further comprises:<claim-text>retrieving, from the common node in the first layer, the first value as an attribute value of an attribute of a first subspace in response to a first individual node associated with the first subspace missing a value for the attribute;</claim-text><claim-text>retrieving, from a second individual node associated with a second subspace, a difference value associated with the attribute of the second subspace; and</claim-text><claim-text>computing a second value for the attribute of the second subspace based on the first value and the difference value.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>receiving a bitstream carrying the audio inputs and the layered description of the space of interest as metadata of the audio inputs; and</claim-text><claim-text>decoding the bitstream to obtain the audio inputs and the layered description of the space of interest.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>ignoring the audio inputs without rendering in response to the location of the subject of the audio scene being outside of the space of interest.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. An apparatus of media processing, comprising processing circuitry configured to:<claim-text>receive audio inputs associated with a layered description for a space of interest in an audio scene, the space of interest comprising a plurality of subspaces, the layered description comprising a first layer and a second layer, the first layer having a common node with a first value that is a common attribute value of two or more subspaces in the plurality of subspaces, and the second layer having individual nodes respectively associated with each of the plurality of subspaces;</claim-text><claim-text>determine the plurality of subspaces of the space of interest based on the layered description; and</claim-text><claim-text>render an audio output based on the audio inputs in response to a location of a subject of the audio scene being in the space of interest.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the plurality of subspaces are rectangular boxes that are defined by at least a position attribute, an orientation attribute and a size attribute.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the common node identifies a name for an attribute, and the first value is an attribute value of the attribute, and the processing circuitry is configured to:<claim-text>retrieve, from the common node in the first layer, the first value as the attribute value of the attribute for a subspace in the plurality of subspaces.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the common node identifies a name of an attribute and an index of a subfield of the attribute, and the first value is a subfield attribute value for the subfield of the attribute, and the processing circuitry is configured to:<claim-text>retrieve, from the common node in the first layer, the first value as the subfield attribute value for the subfield of the attribute of a subspace in the plurality of subspaces.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the common node with the first value is common to the plurality of subspaces, and the processing circuitry is configured to:<claim-text>retrieve, from the common node in the first layer, the first value as an attribute value of an attribute for each of the plurality of subspaces.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the common node with the first value is common to a subset of the plurality of subspaces, and the processing circuitry is configured to:<claim-text>retrieve, from the common node in the first layer, the first value as an attribute value of an attribute for a first subspace in response to a first individual node associated with the first subspace missing a value for the attribute; and</claim-text><claim-text>retrieve, from a second individual node associated with a second subspace, a second value associated with the attribute for the second subspace in response to an existence of the second value associated with the attribute in the second individual node.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the common node with the first value is common to a subset of the plurality of subspaces, and the processing circuitry is configured to:<claim-text>retrieve, from the common node in the first layer, the first value as an attribute value of an attribute of a first subspace in response to a first individual node associated with the first subspace missing a value for the attribute;</claim-text><claim-text>retrieve, from a second individual node associated with a second subspace, a difference value associated with the attribute of the second subspace; and</claim-text><claim-text>compute a second value for the attribute of the second subspace based on the first value and the difference value.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the processing circuitry is configured to:<claim-text>receive a bitstream carrying the audio inputs and the layered description of the space of interest as metadata of the audio inputs; and</claim-text><claim-text>decode the bitstream to obtain the audio inputs and the layered description of the space of interest.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the processing circuitry is configured to:<claim-text>ignore the audio inputs without rendering in response to the location of the subject of the audio scene being outside of the space of interest.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A non-transitory computer-readable storage medium storing instructions which when executed by at least one processor cause the at least one processor to perform:<claim-text>receiving audio inputs associated with a layered description for a space of interest in an audio scene, the space of interest comprising a plurality of subspaces, the layered description comprising a first layer and a second layer, the first layer having a common node with a first value that is a common attribute value of two or more subspaces in the plurality of subspaces, and the second layer having individual nodes respectively associated with each of the plurality of subspaces;</claim-text><claim-text>determining the plurality of subspaces of the space of interest based on the layered description; and</claim-text><claim-text>rendering an audio output based on the audio inputs in response to a location of a subject of the audio scene being in the space of interest.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the plurality of subspaces are rectangular boxes that are defined by at least a position attribute, an orientation attribute and a size attribute.</claim-text></claim></claims></us-patent-application>