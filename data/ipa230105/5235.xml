<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005236A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005236</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17943616</doc-number><date>20220913</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20220101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>20</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20220101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>13</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20220101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>255</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>13</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6256</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEM AND METHOD FOR DETERMINING THE GEOGRAPHIC LOCATION IN AN IMAGE</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17141684</doc-number><date>20210105</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11461993</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17943616</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Applied Research Associates, Inc.</orgname><address><city>Albuquerque</city><state>NM</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Warnaar, PhD</last-name><first-name>Dirk B.</first-name><address><city>Raleigh</city><state>NC</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Besaw, PhD</last-name><first-name>Lance E.</first-name><address><city>Avon Lake</city><state>OH</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Methods and media for determining a list of geographic location candidates from an image of an environment are described. Open-source data indicative of the Earth's surface may be obtained and compared with images obtained from online sources. The images may be automatically analyzed using a plurality of modular convolution neural networks to determined probabilities of interest, environment, and if the image is locatable. Further, the resulting images may be analyzed for skyline and ridgeline depth orders and Region of Interest. A geolocation depicted in the image may be determined by comparing the results of the analysis with global geographic data.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="99.82mm" wi="158.75mm" file="US20230005236A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="181.36mm" wi="133.69mm" orientation="landscape" file="US20230005236A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="165.52mm" wi="148.17mm" file="US20230005236A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="229.70mm" wi="153.08mm" orientation="landscape" file="US20230005236A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="196.77mm" wi="152.40mm" orientation="landscape" file="US20230005236A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="222.00mm" wi="134.11mm" file="US20230005236A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="220.13mm" wi="122.09mm" orientation="landscape" file="US20230005236A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="151.47mm" wi="146.90mm" file="US20230005236A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="218.02mm" wi="145.88mm" orientation="landscape" file="US20230005236A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="210.31mm" wi="125.05mm" file="US20230005236A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This patent application is a continuation application claiming priority benefit, with regard to all common subject matter of U.S. patent application Ser. No. 17/141,684, filed Jan. 5, 2021, and entitled &#x201c;SYSTEM AND METHOD FOR DETERMINING THE GEOGRAPHIC LOCATION IN AN IMAGE&#x201d; (&#x201c;the '684 Application&#x201d;). The identified earlier-filed patent application is hereby incorporated by reference in its entirety into the present application</p><p id="p-0003" num="0002">This non-provisional patent application shares certain subject matter in common with earlier-filed U.S. patent application Ser. No. 16/818,552, filed Mar. 13, 2020 and entitled LANDMARK CONFIGURATION MATCHER. The earlier-filed application is hereby incorporated by reference in its entirety into the present application.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">1. Field</heading><p id="p-0004" num="0003">Embodiments of the invention generally relate to image analysis. Specifically, embodiments of the invention are directed to determining a location depicted in an image by analyzing the image.</p><heading id="h-0004" level="1">2. Related Art</heading><p id="p-0005" num="0004">Traditionally, systems for determining geolocation from data such as images require manual annotations. People must sift through images determining the images of interest and significant markers in the images that may be used for determining the location. Many man-hours may be used to review thousands of images to identify images that depict the content that the user desires. When the user identifies the images that contain useful information, the user may then annotate information in the images that may help determine their location. The annotations may be identification of objects in the images such as, for example, buildings, mountains, sky, skyline, trees, roadways, water bodies, or any other information that may be useful in determining the geographic location in the images. This process of review and annotation may be manually intensive and time consuming.</p><p id="p-0006" num="0005">Further, analysts may also review the images to determine if the image is of a Region of Interest (ROI). The analysts may review the images and determine if the images depict the ROI. If the images possibly depict the ROI, then the analyst may proceed with annotation. If the images do not depict the ROI, then the analysts may remove the images from further analysis.</p><p id="p-0007" num="0006">Further still, when results are determined, analysts review the candidate lists and determines which candidates may closely match the locations. The analysts must compare the data from the image analysis with the geolocation data to determine how likely the data represents the geolocation.</p><p id="p-0008" num="0007">The analyst review and annotation is time consuming and inefficient. What is needed is a system and method that can quickly and automatically identify a geolocation utilizing visual information exclusively. Specifically, when an image is received, the system may utilize a plurality of modular Convolution Neural Networks (CNNs) to reduce the number of images for analysis while automatically determining images of interest to the user, if the image depicts a natural outdoor location, and a probability of locatability of the image.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0009" num="0008">Embodiments of the invention address the above-described need by providing for novel techniques for determining a location based on visual information. Specifically, systems and techniques for determining a list of geographic location candidates from an image of an environment are described. Images may be obtained from online sources and automatically analyzed using a plurality of modular convolution neural networks to determined probabilities of interest, environment, and if the image is locatable. Further, the resulting images may be analyzed for skyline, ridgeline depth orders, and region of interest. A geolocation depicted in the image may be determined by comparing the results of the analysis with global geographic data.</p><p id="p-0010" num="0009">In particular, in a first embodiment, a method of determining a geographic location from an image, comprising the steps of obtaining a plurality of images, determining a first probability for each image of the plurality of images that each image of the plurality of images is of an outdoor environment, determining a second probability for each image of the plurality of images that each image is of interest, determining a third probability for each image of the plurality of images that each image depicts a location that can be automatically determined, combining the first probability, the second probability, and the third probability for each image to determine a set of images each with a combined probability above a threshold, determining a skyline in each image of the set of images and storing a first remaining set of images comprising images where the skyline is determined; determining a depth of the skyline in each image of the first remaining set of images and storing a second remaining set of images comprising images where the depth of the skyline is determined, determining an image region of interest for each image of the second remaining set of images and storing a third remaining set of images comprising images where the image region of interest is determined and determining a geolocation for each image of the third remaining set of images.</p><p id="p-0011" num="0010">In a second embodiment, one or more non-transitory computer-readable media storing computer-executable instructions that, when executed by at least one processor, perform the steps of determining a geographic location from an image, comprising the steps of obtaining a plurality of images from the Internet, determining a first probability for each image of the plurality of images that each image of the plurality of images is of an outdoor environment, determining a second probability for each image of the plurality of images that each image contains content that is of interest to the user, determining a third probability for each image of the plurality of images that each image depicts a location that can be automatically determined, combining the first probability, the second probability, and the third probability for each image to determine a set of images each with a combined probability above a threshold, determining a probability map in each image of the set of images, wherein the probability map presents a probable mountain area, a probable sky area, and a probable others area, determining a rough skyline between the probable mountain area and the probable sky area, determining a fine skyline in each image of the set of images and storing a first remaining set of images comprising images where the fine skyline is determined, determining a depth of the skyline in each image of the first remaining set of images and storing a second remaining set of images comprising images where the depth of the skyline is determined, determining an image region of interest for each image of the second remaining set of images and storing a third remaining set of images comprising images where the image region of interest is determined, and determining a geolocation for each image of the third remaining set of images.</p><p id="p-0012" num="0011">In a third embodiment, a method of determining a geographic location from an image, the method comprising the steps of obtaining a plurality of images from an online source, determining a first probability for each image of the plurality of images that each image of the plurality of images is of an outdoor environment, determining a second probability for each image of the plurality of images that each image contains content that is of interest to the user, determining a third probability for each image of the plurality of images that each image depicts a location that can be automatically determined, combining the first probability, the second probability, and the third probability for each image to determine a set of images each with a combined probability above a threshold, determining a skyline in each image of the set of images and storing a first remaining set of images comprising images where the skyline is determined, determining a depth of the skyline in each image of the first remaining set of images and storing a second remaining set of images comprising images where the depth of the skyline is determined, determining a region of interest for each image of the second remaining set of images and storing a third remaining set of images comprising images where the region of interest is determined; and determining a geolocation for each image of the third remaining set of images.</p><p id="p-0013" num="0012">This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the detailed description. This summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter. Other aspects and advantages of the current invention will be apparent from the following detailed description of the embodiments and the accompanying drawing figures.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTIONS OF THE DRAWING FEATURES</heading><p id="p-0014" num="0013">Embodiments of the invention are described in detail below with reference to the attached drawing figures, wherein:</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts an exemplary hardware platform for certain embodiments of the invention;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts an exemplary process of reducing a number of images;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts exemplary flow chart illustrating a method of analyzing images to determine geolocation depicted in the images;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts exemplary images;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> depicts a resized image;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> depicts an exemplary probability map of an image;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>5</b>C</figref> depicts an exemplary segmentation and probability skyline detection;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> depicts exemplary cropping of the image;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>6</b>B</figref> depicts exemplary skyline post-processing in the image; and</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts exemplary ridgeline depth analysis in the image.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0025" num="0024">The drawings do not limit the invention to the specific embodiments disclosed and described herein. The drawings are not necessarily to scale, emphasis instead being placed upon clearly illustrating the principles of the invention.</p><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0026" num="0025">At a high level, embodiments of the invention provide systems and methods of automatically determining a geographic location depicted in an image. Specifically, in some embodiments, the independent modular CNNs analyze images (for example, images scraped from online websites) to separately determine a probability of a depicted naturally outdoor environment, a probability of interest to the user, and a probability of if the image is locatable. The images with the highest probabilities may be analyzed for skyline, ridgeline, and landmarks to determine geolocation candidates. In some embodiments, the candidates may be ranked. When a candidate is determined to be correct, parameters of the CNNs may be updated such that continuous improvement is made to the system.</p><p id="p-0027" num="0026">The following detailed description of embodiments of the invention references the accompanying drawings that illustrate specific embodiments in which the invention can be practiced. The embodiments are intended to describe aspects of the invention in sufficient detail to enable those skilled in the art to practice the invention. Other embodiments can be utilized, and changes can be made without departing from the scope of the invention. The following detailed description is, therefore, not to be taken in a limiting sense. The scope of embodiments of the invention is defined only by the appended claims, along with the full scope of equivalents to which such claims are entitled.</p><p id="p-0028" num="0027">In this description, references to &#x201c;one embodiment,&#x201d; &#x201c;an embodiment,&#x201d; or &#x201c;embodiments&#x201d; mean that the feature or features being referred to are included in at least one embodiment of the technology. Separate reference to &#x201c;one embodiment&#x201d; &#x201c;an embodiment&#x201d;, or &#x201c;embodiments&#x201d; in this description do not necessarily refer to the same embodiment and are also not mutually exclusive unless so stated and/or except as will be readily apparent to those skilled in the art from the description. For example, a feature, structure, or act described in one embodiment may also be included in other embodiments but is not necessarily included. Thus, the technology can include a variety of combinations and/or integrations of the embodiments described herein.</p><p id="p-0029" num="0028">Turning first to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, an exemplary hardware platform <b>100</b> for certain embodiments of the invention is depicted. Computer <b>102</b> can be a desktop computer, a laptop computer, a server computer, a mobile device such as a smartphone or tablet, or any other form factor of general- or special-purpose computing device. Depicted with computer <b>102</b> are several components, for illustrative purposes. In some embodiments, certain components may be arranged differently or absent. Additional components may also be present. Included in computer <b>102</b> is system bus <b>104</b>, whereby other components of computer <b>102</b> can communicate with each other. In certain embodiments, there may be multiple busses or components may communicate with each other directly. Connected to system bus <b>104</b> is central processing unit (CPU) <b>106</b>. Also attached to system bus <b>104</b> are one or more random-access memory (RAM) modules <b>108</b>. Also attached to system bus <b>104</b> is graphics card <b>110</b>. In some embodiments, graphics card <b>110</b> may not be a physically separate card, but rather may be integrated into the motherboard or the CPU <b>106</b>. In some embodiments, graphics card <b>110</b> has a separate graphics-processing unit (GPU) <b>112</b>, which can be used for graphics processing or for general purpose computing (GPGPU). Also, on graphics card <b>110</b> is GPU memory <b>114</b>. Connected (directly or indirectly) to graphics card <b>110</b> is display <b>116</b> for user interaction. In some embodiments no display is present, while in others it is integrated into computer <b>102</b>. Similarly, peripherals such as keyboard <b>118</b> and mouse <b>120</b> are connected to system bus <b>104</b>. Like display <b>116</b>, these peripherals may be integrated into computer <b>102</b> or absent. Also connected to system bus <b>104</b> is local storage <b>122</b>, which may be any form of computer-readable media and may be internally installed in computer <b>102</b> or externally and removably attached.</p><p id="p-0030" num="0029">Computer-readable media include both volatile and nonvolatile media, removable and nonremovable media, and contemplate media readable by a database. For example, computer-readable media include (but are not limited to) RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile discs (DVD), holographic media or other optical disc storage, magnetic cassettes, magnetic tape, magnetic disk storage, and other magnetic storage devices. These technologies can store data temporarily or permanently. However, unless explicitly specified otherwise, the term &#x201c;computer-readable media&#x201d; should not be construed to include physical, but transitory, forms of signal transmission such as radio broadcasts, electrical signals through a wire, or light pulses through a fiber-optic cable. Examples of stored information include computer-useable instructions, data structures, program modules, and other data representations.</p><p id="p-0031" num="0030">Finally, in some embodiments, network interface card (NIC) <b>124</b> is also optionally attached to system bus <b>104</b> and allows computer <b>102</b> to communicate over a network such as network <b>126</b>. NIC <b>124</b> can be any form of network interface known in the art, such as Ethernet, ATM, fiber, Bluetooth, or Wi-Fi (i.e., the IEEE 802.11 family of standards). NIC <b>124</b> connects computer <b>102</b> to local network <b>126</b>, which may also include one or more other computers, such as computer <b>128</b>, and network storage, such as data store <b>130</b>. Generally, a data store such as data store <b>130</b> may be any repository from which information can be stored and retrieved as needed. Examples of data stores include relational or object-oriented databases, spreadsheets, file systems, flat files, directory services such as LDAP and Active Directory, or email storage systems. A data store may be accessible via a complex API (such as, for example, Structured Query Language), a simple API providing only read, write and seek operations, or any level of complexity in between. Some data stores may additionally provide management functions for data sets stored therein such as backup or versioning. Data stores can be local to a single computer such as computer <b>128</b>, accessible on a local network such as local network <b>126</b>, or remotely accessible over Internet <b>132</b>. Local network <b>126</b> is in turn connected to Internet <b>132</b>, which connects many networks such as local network <b>126</b>, remote network <b>134</b> or directly attached computers such as computer <b>136</b>. In some embodiments, computer <b>102</b> can itself be directly connected to Internet <b>132</b>.</p><p id="p-0032" num="0031">In some embodiments, the geolocation system comprises a modular and flexible image analysis process. The geolocation system may receive or otherwise extract images. The images may be input by a user for analysis or the images may be obtained by a system scraping process scraping images from an online source. The images may be obtained from particular websites that promote material of interest. For example, the user may be a government organization and the images may be obtained from a known terrorist organization website. Further, the images may be the result of a query in an online query engine and the images may be downloaded and analyzed to determine the location depicted in the images as described in embodiments below.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts an exemplary process of reducing a number of images that may be obtained by the system referenced generally by the numeral <b>200</b>. In some embodiments, various steps may be performed for determining if the image is a natural outdoor setting, if the image is of interest (or what the user is looking for), and if the scene in the image is locatable. Further, the skyline may be analyzed for relative height and depth, an ROI may be determined from classifications of items in the image, and the images with the highest correct location probability may be determined. At each step the number of acceptable images may be reduced such that each subsequent step is analyzing fewer images than the previous step. Further, some of the modular steps may be performed in any order such that the steps reducing the images the most may be performed first thus initially greatly reducing the number of images. A set of images from the places CNN, locatable CNN, and interest CNN may be analyzed to determine skylines and ridgelines in the images as well as landmarks that may provide information to compare to known stored global information. The information determined from the images may be compared to the known global information to determine a geographic location in the images. A list of candidate images may be created that have the highest probability of producing an accurate geographic location match.</p><p id="p-0034" num="0033">In some embodiments, open-source data of the Earth's surface may be combined to produce a Geographic Knowledge Base (GKB) for comparison with the image data. The GKB may comprise elevation, landmarks, landcover, water bodies, water lines, and any other data that may be useful. Further, in some embodiments, the Earth's surface may be broken up into grids to narrow the field of search to smaller and smaller areas to reduce the data analysis. The GKB is described in greater detail in earlier-filed U.S. patent application Ser. No. 16/818,552 filed Mar. 13, 2020 and entitled LANDMARK CONFIGURATION MATCHER incorporated by reference in its entirety into the present application.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts an exemplary automated process for reducing the images to natural and outdoor, locatable, interesting, and determining probable skylines and ridgelines and determining geolocations depicted in the images generally referenced by numeral <b>300</b>. At step <b>302</b>, the geolocation system obtains images. The images may be received from a user or may be obtained from an online source. In some embodiments, the images may be obtained from the internet through an automated search related to a desired output of the user. For example, the user may be a government organization and the desired outcome may be to find the location of terrorist organizations from images displayed on the Internet. The geolocation system may automatically navigate to known websites or may perform automatic online keyword searches and scrape the webpages for images. The images may be downloaded into the system for subsequent analysis. Exemplary images are depicted in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0036" num="0035">In some embodiments, any images comprising known information may be used to train the CNNs described herein. For example, the exemplary scenario of searching for terrorist locations described above may be one of many possible searches. In some embodiments, any images with known information may be used to train the CNNs. For example, images depicting regions around the world of various sporting events may be used to determine the popularity of various sports in different nations. In another example, images of cats and dogs may be analyzed to determine the popularity of cats vs. dogs across the United States. Further, the CNNs may be trained to recognize cat and dog breeds and the popularity of cat and dog breeds may be mapped across the world or across predefined regions. Any known images may be used to train the CNNs described to determine locations of images that may be sourced from the Internet or from user input. A more detailed description of the training and analysis processes is described below.</p><p id="p-0037" num="0036">At step <b>304</b>, the images may be analyzed to determine a probability of the environment depicted in the images. The images may be analyzed using a places CNN that is trained on indoor and outdoor images. Further, the places CNN may be trained to determine man-made and natural objects. A plurality of categories may be used to determine a probability that the images were taken in a natural outdoor environment. For example, the plurality of categories may be at least open area, natural light, vegetation, hiking, camping, far-away horizon, sunny, foliage, man-made, dry, dirt, rugged, warm, cold, snow, rain, sky, mountains, hills, and any other category that may be used to determine natural, man-made, outdoor, and indoor environment. Exemplary images representing low ranking places images are depicted in <figref idref="DRAWINGS">FIG. <b>4</b></figref> quadrant <b>402</b> and high-ranking places images are depicted in quadrant <b>404</b>.</p><p id="p-0038" num="0037">In some embodiments, the images may be matched to the categories to determine a probability of a natural outdoor environment. For example, if a table, chairs, pictures, walls, lights, and other indoor objects are determined to be depicted with a high probability, a high probability of an indoor environment may be determined. If sky, mountains, river, and other natural outdoor classifications are determined to be depicted in the images, then a high probability of a natural outdoor environment may be determined. The probability may be determined by combining the probability of classifications of each individual object in the image. For example, a known image may be used to train the places CNN. The parameters of the CNN may be optimized to provide results of images that depict natural outdoor environments. The places CNN may analyze the image and it may be determined that there is a likelihood of 80% of an indoor light in the image. If the known images contains the indoor light, the training sequence confirms the correct response and feeds back the correct response. The parameters of the places CNN may be updated to improve the system by recognizing the correct response. The CNN learning functions are described in more detail below.</p><p id="p-0039" num="0038">In some embodiments, a weighted combination of classifications may be used to determine the probability that the image depicts a natural outdoor environment. In some embodiments, the probabilities for each object classification determined in the image may be combined to determine a probability that the image is of a natural outdoor environment. For example, the likelihoods may be multiplied, summed, averaged, or combined in any manner to determine a final probability that the images is of an outdoor environment.</p><p id="p-0040" num="0039">Exemplary images analyzed by using the places CNN are depicted in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The upper left quadrant <b>402</b> depicts images that are found to have a low probability of depicting a natural outdoor environment. The upper right quadrant <b>404</b> depicts images that are found to have a high probability of depicting a natural outdoor environment.</p><p id="p-0041" num="0040">At step <b>306</b>, in some embodiments, the images with the highest probability of depicting a natural outdoor scene are selected and stored for further analysis. The images with the highest probability, a probability above a specified threshold, or rank above a specified threshold may be stored for further analysis. The determined natural outdoor probability for each image may be associated with the image such that the natural outdoor probability may be combined with other probabilities after further analysis.</p><p id="p-0042" num="0041">In some embodiments, the places CNN may be trained to recognize indoor environments, urban environments, environments with a visible coastline, or any other environment that may be of interest to the user. The places CNN may be trained using known images of the type of environment that is interesting to the user.</p><p id="p-0043" num="0042">At step <b>308</b>, in some embodiments, an interest CNN is trained to determine images of interest. The interest CNN may be trained to determine objects in the images that are of interest to the user. The interest CNN may be trained to recognize objects in the images as described above. The interest CNN may be trained to detect man-made and natural objects, text, symbols, faces, and any other items in the images that may be useful in determining a probability of interest based on known training images. For example, the user may be a government organization and may want to determine the location of terrorist training facilities in the Middle East. The interest CNN may be trained on Arabic text, known terrorist organization symbols, known terrorist facial features, types of weapons, types of clothing, types of military or militia clothing, types of vehicles, types of military and law enforcement vehicles, and any other items in the images that may create an interest probability based on the training images. The lower left quadrant <b>404</b> and the lower right quadrant <b>406</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> depict high ranking interest images based on the terrorist and military imagery.</p><p id="p-0044" num="0043">In other exemplary embodiments, the interest CNN may be trained to detect people or objects through facial and object recognition. For example, the interest CNN may be used to detect persons-of-interest, fugitives, hostages, or particular objects. The training may be performed quickly and the interest CNN may be implemented by government agencies such as the Department of Homeland Security and the Federal Bureau of Investigation.</p><p id="p-0045" num="0044">At step <b>310</b>, in some embodiments, the interest CNN may output each image with the interest probability associated with each image. The images with the highest probability, or a probability of interest above a specified threshold, may be stored for further analysis. The interest probability may be based on the number of interesting items, or classification of items, and the probability of interest of the items or item classifications. In some embodiments, the interest probability may be combined with other probabilities after further analysis such that the total number of images to be analyzed may be reduced.</p><p id="p-0046" num="0045">At step <b>312</b>, in some embodiments, a locatable CNN may be trained to determine images that have a high probability of determining the location depicted in the images. The locatable CNN may be trained on images of known locations such that parameters of the locatable CNN are tuned to the highest rate of determining the locations depicted in the images. The locatable CNN may be trained to recognize visible and unique skylines and ridgelines. As with all CNNs, when the images are processed the parameters that led to the highest percentage of recognizable locations may be stored. Further, each time the images are correctly located, the parameters may be adjusted to improve the locatable CNN.</p><p id="p-0047" num="0046">As depicted in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the lower left quadrant <b>406</b> depicts images with high ranking places and interest. However, these images also do not depict distinct skylines and therefore, have a low probability of being locatable. Alternatively, the lower right quadrant <b>408</b> depicts images with high rankings in places, interest, and locatable as the images depict distinct skylines along with a natural outdoor environment and recognizable interest (e.g., military, militia, terrorist, etc.) imagery.</p><p id="p-0048" num="0047">At step <b>314</b>, in some embodiments, the output of the locatable CNN is images with high probability of depicting skylines. The probable skylines in the images may be refined and used to determine the geolocation depicted in the images in subsequent analysis described in embodiments below.</p><p id="p-0049" num="0048">In some embodiments, the images with the highest probability are analyzed for skylines. <figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts the four quadrants described above. In some embodiments, the images that rank with high probabilities for locatable, natural outdoor environment, and interest are analyzed to determine a detailed skyline in the images. In some embodiments, the probability an image is locatable, the probability for a natural outdoor environment, and the probability the image is of interest is combined by combining (for example, summing or multiplying) the probabilities for each image of the plurality of images. If the sum is above a minimum threshold then the image is saved. The saved set of images may then be analyzed to determine skylines. The detailed skylines in the set of images may be used to determine the locations depicted in the images. In some embodiments, the images that do not meet the minimum threshold are discarded. Any image that does not meet a minimum threshold after each CNN analysis may be discarded. By doing do, this process reduces the number of images that are analyzed at each step.</p><p id="p-0050" num="0049">At step <b>316</b>, in some embodiments, a skyline CNN is used to automatically detect and extract the skyline for analysis. First, a rough skyline may be determined as depicted in <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>C</figref>. For example, an image <b>502</b> of the set of images may be analyzed. The edges of the image <b>502</b> may be reflected <b>504</b> and added to sides of the image <b>502</b> such that the full image including the reflected edges may be analyzed without running into problems with the edge of the image <b>502</b>. This ensures that the total area of the image <b>502</b> is analyzed. Further, the image <b>502</b> may be resized <b>506</b> to reduce the amount of data to be analyzed. This may quicken the processing and allow more images to be analyzed in a shorter time frame.</p><p id="p-0051" num="0050">Turning to <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, in some embodiments, an image segmentation CNN is used to determine at least three classifications depicted in the image <b>502</b>. For example, the image <b>502</b> may be segmented into mountains, sky, and other. A probability map <b>508</b> may be determined for the image <b>502</b>. The probability map <b>508</b> depicts each segmented portion of the image <b>502</b> with a probability that an upper segmented portion <b>510</b> is sky, a probability that the middle-segmented portion <b>512</b> is mountains, and a probability that the lower segmented portion <b>514</b> is other (not mountain and not sky).</p><p id="p-0052" num="0051">In some embodiments, atrous convolution is used to segment the images. The atrous convolution can be seen in <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>. In some embodiments, the atrous convolution is performed with rate equal to two to increase the outputs. The accuracy is then improved with the Atrous Spatial Pyramid Pool (ASPP). The output of the ASPP is then smoothed with a Conditional Random Field (CRF) algorithm. When the images are segmented, a rough skyline in the image may be determined. The original image <b>502</b> as well as the previously determined probability map <b>508</b> for segmentation described above may be input into the CRF. In some embodiments, the output of the CRF is a refined probability map for the mountains, sky, and other classifications representing a rough skyline.</p><p id="p-0053" num="0052">In some images, the skyline may be obstructed by the &#x201c;others&#x201d; (i.e., not mountains or sky). The pixels in the image classified as &#x201c;others&#x201d; may be removed, When the &#x201c;other&#x201d; pixels are removed the probable skyline is the boundary between sky and mountain pixels. The probable skyline is stored for further analysis.</p><p id="p-0054" num="0053">In some embodiments, a rough Viterbi tracker is implemented on the probable skyline to provide an estimated skyline between the mountains and the sky. In some embodiments, the input to the Viterbi tracker is the skyline probability. The Viterbi tracker iterates through the disconnected segments of the probable skyline taking the most probable path. As the Viterbi tracker moves along the probable skyline x and y coordinates for the estimated skyline are output. The resulting skyline may have a minimum probability threshold and may have a minimum segment length based on the image size. The result is an estimated rough skyline <b>606</b> as depicted in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>.</p><p id="p-0055" num="0054">In some embodiments, the rough skyline <b>606</b> generated by the above-described techniques is refined as shown in <figref idref="DRAWINGS">FIGS. <b>6</b>A-C</figref> in an iterative process. Because the estimated rough skylines have been determined in the images by the above-described methods, performing the techniques described above in <figref idref="DRAWINGS">FIGS. <b>5</b>A-C</figref> in an iterative process beginning with the rough skyline <b>606</b> results in a fine skyline <b>608</b>. The process of determining a fine skyline <b>608</b> may begin by cropping the images as shown in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>. Small sections, or cropped sections <b>602</b>, of the image <b>502</b> may be analyzed independently. A single cropped section <b>604</b> is depicted. Each cropped section (1, 2, 3, etc.) may be analyzed using the segmentation CNN to produce new probabilities of mountain, sky, and other segments of the image as described above. The new segments, or probability maps, may be run through the atrous CNN as described above. New probability maps may be generated based on the results of the CRF generating a new probable skyline. Next, the Viterbi tracker may be processed again through the new probable skyline creating x and y coordinates of the fine skyline <b>608</b>. The fine skyline <b>608</b> may then move to a post-processing procedure.</p><p id="p-0056" num="0055">Turning back to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, at step <b>318</b>, a skyline post-processing procedure and annotation is performed. In some embodiments, each pixel may be analyzed along the skyline. A patch above and a patch below the pixel may be compared to determine a difference between the patches. If the patches are significantly different, the skyline may be retained at that point. If the patches are similar, then the skyline at that point may be removed. In some embodiments, a Kruskal-Wallace test may be performed to test the difference. For example, a probability that the patches are sampled from the same distribution may be determined. A threshold test, such as the log of the probability being greater than or equal to a number, may be applied to determine if the patches are the same or different. The results of the Kruskal-Wallace test can be seen in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>. If the patches are different then the fine skyline <b>608</b> is retained. If the patches are the same or similar, then the fine skyline <b>608</b> may be removed as shown at <b>610</b> in images <b>612</b> and <b>614</b>. When the final skyline is determined, the skyline is stored for further analysis.</p><p id="p-0057" num="0056">In some embodiments, the depth of the fine skyline <b>608</b> is determined. <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> depicts the fine skyline <b>608</b> with depth classification <b>616</b>, and the fine skyline <b>608</b> with depth clusters <b>618</b>. The depth classification <b>616</b> may be determined by relative depth of the fine skyline <b>608</b> and the depth cluster <b>618</b> may be determined by averaging the relative depths to determine which depths are likely to be from the same ridge. A more detailed explanation is provided below in relation to ridgeline depth analysis.</p><p id="p-0058" num="0057">In some embodiments, the results of the skyline CNN are compared to known skyline images to further tune the parameters of the skyline CNN. When the results of the skyline CNN align with true skyline information in the known images, the parameters can be kept or otherwise combined with the previously stored parameters of the CNN to increase the confidence in predicting the skylines.</p><p id="p-0059" num="0058">In some embodiments, a true test positive rate may be used to determine the probability that a skyline will test true and a positive predictive value may be applied such that the confidence in the skyline CNN may be known. If the confidence in the skyline CNN increases based on parameter changes, then the new parameters may be stored. If the confidence does not change or is decreased, then the new parameters may not be stored. This process for increasing the confidence in the skyline CNN may be applied to any CNNs or algorithms described herein.</p><p id="p-0060" num="0059">At step <b>320</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in some embodiments, a depth analysis of the image <b>502</b> may be performed with a depth CNN. The depth CNN may be trained on known images of mountain ranges to determine the relative depth of the skyline and ridges of the mountains in front of the skyline. The known images of mountain ranges have known ridges and relative depth, or distance between the ridges, such that the depth CNN can be trained to provide depth analysis to a specified minimum probability threshold. As the depth CNN provides more depth analysis, the training data is increased, and the depth CNN can be improved as described in embodiments above. Sample results of the depth analysis are shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0061" num="0060">In some embodiments, the original images and the skylines determined above may be input into the depth CNN. As described above, the images may be cropped to analyze small sections at a time. In some embodiments, the depth predictions may be normalized by providing relative distances between the ridges. In some embodiments, the predicted depth values may be provided as a distance from the source of the image.</p><p id="p-0062" num="0061">In some embodiments, the depth analysis comprises a process of determining a ridge order. When the depth estimates from the depth CNN are determined, an order of the ridges may be determined. The ridge order may be determined by a process starting with finding the peaks of the depth distribution. The peaks of similar depth may be kept together while the peaks that are separated in depth may be kept separate. Further, groups of peaks that may have overlap may be consolidated into one group. The mean depth for each group of peaks may be determined. Small groups may be consolidated, or clustered, and the means and standard deviation of groups may be compared based on z-score. Finally, the groups of ridges may be ordered by mean depth such that ridges that are close to the same mean depth are grouped, or clustered, together. In some embodiments, the same process may be applied to the skyline as described above. <figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts a hillside image <b>702</b> displaying ridgelines <b>704</b> and a skyline <b>706</b>. The subsequent image depicting results of the depth CNN analysis described above depicts shading of different levels of the ridges, or clusters of ridges, and the skyline <b>706</b>.</p><p id="p-0063" num="0062">At step <b>322</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the output and annotation of the depth CNN and subsequent analysis may be the ordered mean depth values associated with the corresponding ridgelines <b>704</b> in the images. The ridgelines <b>704</b> may be presented as normalized depth values based on relative depths of the ridgelines <b>704</b>. In some embodiments, the estimated distance from the source of the image can be determined and stored.</p><p id="p-0064" num="0063">At step <b>324</b>, in some embodiments, a Region of Interest (ROI) is determined based on items in the image using a ROI CNN. In some embodiments, the ROI CNN may comprise hierarchical models and scene classification that may be used to determine the ROI. In some embodiments, geographical cells with a balanced number of images may be used, and each cell may be a classification. A geolocation estimation may be based at least in part on the depicted scene in the image comprising scene classifications and the geographical cell input that comprises geographic regions. The output may be a probability of various geographic locations. The various geographic locations may be ranked based on the probability and the regions may be analyzed in the order of the highest ranked to lowest.</p><p id="p-0065" num="0064">In some embodiments, the system may use open-source software such as S2 Geometry library utilizing 6 face cubes or a spherical surface to model the surface of the Earth. Further, the scene classification may utilize ResNet152 to classify scenes as indoor, outdoor natural, and outdoor man-made. The system may automatically connect to the open-source software via a custom Application Program Interface (API) that allows for fully automatic transfer of data between the system and the open-source software as described above.</p><p id="p-0066" num="0065">At step <b>326</b>, the output of the ROI analysis may associate probable ROIs with the images. The ROIs and the images may be analyzed by the Landmark Configuration (LC) matcher. The LC matcher may compare the data determined from the images with data in each ROI. In some embodiments, the ROIs are ranked with a high probability to a low probability and the ROIs are analyzed in order of high rank to low rank.</p><p id="p-0067" num="0066">At step <b>328</b>, in some embodiments, the LC matcher is used to determine the geolocation depicted in the images. The data from the images such as, for example, the skyline elevation data, ridgeline data, skyline annotations, and any objects recognized in the images may be compared to geolocation data stored in a database for each ROI and image.</p><p id="p-0068" num="0067">In some embodiments, a process for determining a list of geolocation candidates may comprise combining global open-source data indicative of features of the Earth's surface and object on the Earth's surface and comparing with data obtained by a user. The set of images may be compared to the global data to determine a list of candidate geographic locations of the environment depicted in the image. The global data may be geographic data that, in some embodiments, may be open-source data. The geographic data may be obtained via satellites, aerial vehicles, and ground/water collection methods. The global data may include images, video, annotation, any user added text, location information, relative distance information between objects, topography, landmarks, or any other information that may be useful as described in embodiments herein. Opensource data may be collected from Digital Elevation Models based on monoscopic or stereo pair imaging, radar, and elevation measurements. In some embodiments, the Digital Elevation Models are collected from Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER), Shuttle Radar Topography (SRTM), TerraSAR-X add-ons for digital elevation measurements (TanDEM-X), and any other DEM.</p><p id="p-0069" num="0068">Further, in some embodiments, open-source land cover data may be obtained for comparison with the images. The land cover data may be obtained from GeoCover Landsat 7 data, LandScan global population distribution data for urban and rural environments, and Dynamic Land Cover Dataset (AU DLCD) for vegetation indexes.</p><p id="p-0070" num="0069">In some embodiments, crowd-sourced datasets of streets, rivers, landmarks, and any other information supplied by people may be used. In one example, this information may be obtained from Open Street Map (OSM). Further, in some embodiments, country lines (and/or other geopolitical boundaries) may be obtained from outlines (such as ArcGIS data), shoreline data may be obtained from the National Oceanic and Atmospheric Organization (NOAA), and cell tower information may be used from Open Cell ID. The data may be combined to create information indicative of any region in the world and may generally be referred to as region-specific data or global data.</p><p id="p-0071" num="0070">The global data covers most of the world and, therefore, provides expansive coverage of the Earth for visual-based location determination. The global data may be combined to create data that is indicative of elevation, landmarks, and natural and man-made structures at worldwide locations. In some embodiments, the global data from each of the sources may be combined and into a large data set and masks created for categorization and efficient access and comparison. Further, the data may be broken up into various regional locations based on grid creation over the Earth's surface.</p><p id="p-0072" num="0071">In some embodiments, the global data may be processed to create a Geolocation Knowledge Base (GKB). The global data (comprising, for example, streets, rivers, landmarks, elevations, and other environmental object data) may be separated into one-degree latitude by one-degree longitude cells across the surface of the Earth. In some embodiments, masks may be created for different objects contained within each cell. For example, masks may be created for ocean, land, vegetation, desert, forest, urban, rural, mountains, hills, valleys, ridgelines, houses, buildings, or any other feature in the environment that may be categorized to create efficient search queries and stored in the GKB.</p><p id="p-0073" num="0072">The set of images that are processed in blocks <b>302</b>-<b>328</b> and output from the skyline CNN may be compared to the data from the GKB to determine the geolocation in the images. At step <b>330</b>, a list of geolocation candidates is analyzed and output from the LC matcher. In some embodiments, when the lowest error geolocations are found, a process of uncertainty polygon and result clustering is performed to determine the result confidence. Further, ranking the candidates based on the error calculations and pruning to only candidates with error below, or rank above, predetermined thresholds may be performed.</p><p id="p-0074" num="0073">At steps <b>332</b>-<b>334</b>, the final list of geographic location candidates may be ranked using a rank CNN. The rank CNN may be trained on known images that coincide with the images that the user desires and contains known geolocations. The rank CNN may determine and provide a rank for each image of the set of images processed by the LC matcher. The images that are determined to have the highest probability of accurate geolocation matches are ranked highest. Therefore, a minimum threshold of image rank may be provided to reduce the number of images. For example, the list of geolocation candidates may be any amount from one candidate to the total global data. An amount threshold may be selected such as, for example, 300, 100, 10, or any other number. In some embodiments, the threshold for candidate list amount may be based on the probability for each image. In some such embodiments, only candidates with a minimum rank are presented to the user.</p><p id="p-0075" num="0074">At step <b>336</b>, the system determines which, if any, images qualify for retraining the CNNs. A minimum ranking threshold may be established. When an image is ranked above the threshold, the image may be stored for training the CNNs of the geolocation system. At steps <b>338</b>-<b>340</b>, in some embodiments, the system continuously or periodically retrains when new training data becomes available. As described above, as image data is collected and processed, that data may be used to retrain the CNNs. The system may be implemented as a continuous learning system. The system may be automatically updated and, in some embodiments, manually updated with manual annotations and known image uploads for training purposes. When images are correctly identified, or identify features above a specified probability threshold, the system parameters may be updated or replaced. A true test positive rate may be used to determine the probability that the CNNs test true and a positive predictive value may be applied such that the confidence in the various CNNs may be known. If the confidence in the CNNs increases based on the results of LC matcher, the new parameters for the corresponding CNN may be stored. If the confidence does not change or is decreased, then the new parameters are not stored. In some embodiments, the threshold may also be updated. As the system increases accuracy, the threshold may increase to only allow updates that improve the system. This process for increasing the confidence in the CNNs may be applied to any CNNs or algorithms described herein.</p><p id="p-0076" num="0075">In some embodiments, each CNN is modular and processes data independently. The images may be processed through the CNNs in any order. The order may be modified to optimize the reduction of images such that the first CNN reduces the number of images the most. In this way, the system may be optimized for efficient processing. In some embodiments, the system may further provide efficient processing by utilizing cloud-based support. For example, Kubernetes clusters may be utilized for high-priority and efficient processing of large volumes of images and image data. Selective processing may be used to maximize efficiency within a fixed budget by clustering. In some embodiments, only the highest priority images are processed based on the output of each cluster.</p><p id="p-0077" num="0076">In some embodiments, the file structure may be required to store a large number of files, store files with the same name, store all files associated with a query in a single folder, and support easy lookup of query files by all processes described above. To meet these extensive requirements the files may be named according to time of processing. For example, the structure may be arranged such that a file is stored under a root, by year, by month, by day, bay hour, and with a unique identification number. Further, the files may be stored as a key value pair in the database.</p><p id="p-0078" num="0077">Many different arrangements of the various components depicted, as well as components not shown, are possible without departing from the scope of the claims below. Embodiments of the invention have been described with the intent to be illustrative rather than restrictive. Alternative embodiments will become apparent to readers of this disclosure after and because of reading it. Alternative means of implementing the aforementioned can be completed without departing from the scope of the claims below. Certain features and subcombinations are of utility and may be employed without reference to other features and subcombinations and are contemplated within the scope of the claims. Although the invention has been described with reference to the embodiments illustrated in the attached drawing figures, it is noted that equivalents may be employed, and substitutions made herein without departing from the scope of the invention as recited in the claims.</p><p id="p-0079" num="0078">Having thus described various embodiments of the invention, what is claimed as new and desired to be protected by Letters Patent includes the following:</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of determining a geographic location from an image, the method comprising:<claim-text>obtaining the image,</claim-text><claim-text>wherein a location where the image was taken are unknown;</claim-text><claim-text>determining that the image depicts an outdoor environment and content of interest;</claim-text><claim-text>determining a skyline in the image;</claim-text><claim-text>determining an elevation of the skyline;</claim-text><claim-text>determining a depth of the skyline; and</claim-text><claim-text>determining the location where the image was taken based at least in part on the elevation of the skyline and the depth of the skyline.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the image is obtained from a plurality of images from online resources and the skyline includes a plurality of features.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>determining a first probability associated with the image and indicative of the image depicting the outdoor environment;</claim-text><claim-text>determining a second probability associated with the image indicative of the image depicting characteristics from which the location can be determined; and</claim-text><claim-text>combining the first probability and the second probability to generate a total probability,</claim-text><claim-text>wherein the total probability is indicative of a likelihood of determining the location that the image was taken.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>determining a ridgeline in the image;</claim-text><claim-text>estimating a ridgeline depth and a ridgeline elevation of the ridgeline; and</claim-text><claim-text>determining the location further based at least in part on the ridgeline depth and the ridgeline elevation.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the content of interest is indicative of military or illegal activity.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>iteratively determining the skyline to produce a fine-grain elevation of the skyline and a fine-grain depth of the skyline; and</claim-text><claim-text>determining the location where the image was taken based at least in part on the fine-grain elevation of the skyline and the fine-grain depth of the skyline.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>determining man-made objects in the image; and</claim-text><claim-text>determining the location that the image was taken further based at least in part on the man-made objects.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>determining objects obscuring the skyline;</claim-text><claim-text>masking the objects;</claim-text><claim-text>estimating the skyline obscured by the objects to obtain an estimated skyline; and</claim-text><claim-text>determining the location that the image was taken further based at least in part on the estimated skyline.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. One or more non-transitory computer-readable media storing computer-executable instructions that, when executed by at least one processor, perform a method of determining a geographic location from an image, the method comprising:<claim-text>obtaining a plurality of images,</claim-text><claim-text>wherein locations where the plurality of images were taken are unknown;</claim-text><claim-text>filtering one or more images from the plurality of images,</claim-text><claim-text>wherein the one or more images depict outdoor environments and content of interest;</claim-text><claim-text>determining a skyline in the one or more images;</claim-text><claim-text>determining an elevation of the skyline in the one or more images;</claim-text><claim-text>determining a depth of the skyline in the one or more images;</claim-text><claim-text>determining a ridgeline in the one or more images;</claim-text><claim-text>estimating a ridgeline depth and a ridgeline elevation of the ridgeline; and</claim-text><claim-text>determining a location where the one or more images was taken based at least in part on the elevation of the skyline, the ridgeline elevation, the depth of the skyline, and the ridgeline depth.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The media of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the method further comprises:<claim-text>determining a probability of determining the location for the one or more images; and</claim-text><claim-text>determining the one or more images from the probability.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The media of <claim-ref idref="CLM-00010">claim 10</claim-ref>,<claim-text>wherein the probability is above a minimum threshold,</claim-text><claim-text>wherein the method further comprises ordering the one or more images from a highest combined probability to a lowest combined probability to prioritize higher probability images.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The media of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein t the method further comprises:<claim-text>determining a mean depth and a standard deviation of the ridgeline depth of each of ridgelines in each of the one or more images; and</claim-text><claim-text>grouping the ridgelines in each image based on the mean depth and the standard deviation.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The media of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the one or more images are obtained from online resources.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The media of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the method further comprises:<claim-text>determining other objects in the one or more images; and</claim-text><claim-text>determining a probability of determining the location of the one or more images based on the other objects and characteristics of the outdoor environments.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The media of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the method further comprises:<claim-text>filtering a set of images from the one or more images based on the probability of determining the one or more images; and</claim-text><claim-text>determining the location for each image of the set of images based at least in part on the elevation of the skyline, the elevation of the ridgeline, the depth of the skyline, the depth of the ridgeline, and the other objects.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The media of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the other objects are man-made objects.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A method of determining a geographic location from an image, the method comprising:<claim-text>obtaining a plurality of images,</claim-text><claim-text>wherein locations where the plurality of images were taken are unknown;</claim-text><claim-text>filtering a set of images from the plurality of images,</claim-text><claim-text>wherein the set of images depict outdoor environments and content of interest;</claim-text><claim-text>determining outdoor characteristics of the outdoor environments in the plurality of images;</claim-text><claim-text>determining a probability of determining each location at which each image of the plurality of images was taken based at least in part on the outdoor characteristics;</claim-text><claim-text>filtering a subset of images from the set of images,</claim-text><claim-text>wherein each image of the subset is associated with a likelihood above a threshold of determining the locations where each image of the subset was taken;</claim-text><claim-text>determining a skyline in each image of the subset;</claim-text><claim-text>determining an elevation of the skyline in each image;</claim-text><claim-text>determining a depth of the skyline in each image; and</claim-text><claim-text>determining each location where each image of the subset was taken based at least in part on the elevation of the skyline and the depth of the skyline.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising ordering the subset from a highest combined probability to a lowest combined probability, wherein the lowest combined probability is above the threshold.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising:<claim-text>determining other objects in the plurality of images; and</claim-text><claim-text>determining the probability of determining each location of each image further based on the other objects and the outdoor characteristics of the outdoor environments.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising:<claim-text>iteratively determining the skyline in each image to produce a fine-grain elevation of the skyline and a fine-grain depth of the skyline in each image; and</claim-text><claim-text>determining each location where each image of the subset was taken based at least in part on the fine-grain elevation of the skyline and the fine-grain depth of the skyline.</claim-text></claim-text></claim></claims></us-patent-application>