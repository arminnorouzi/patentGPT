<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005101A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005101</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17778260</doc-number><date>20201204</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2019-229315</doc-number><date>20191219</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>40</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>40</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>012</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">INFORMATION PROCESSING APPARATUS, INFORMATION PROCESSING METHOD, AND RECORDING MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SONY GROUP CORPORATION</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>SUGIHARA</last-name><first-name>Kenji</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>SONY GROUP CORPORATION</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/045173</doc-number><date>20201204</date></document-id><us-371c12-date><date>20220519</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">There is provided an information processing apparatus that enables provision of a more natural viewing experience to a user, an information processing method, and a recording medium. A control unit controls display of a two-dimensional image including a plurality of objects having distance information in a three-dimensional coordinate system with a viewpoint position of a user as a reference. The control unit controls a display magnification of the two-dimensional image corresponding to a movement amount of the viewpoint position in a real space on the basis of distance information of the objects that is a region of interest of the user. The present disclosure can be applied to, for example, an HMD that presents an omnidirectional image.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="105.24mm" wi="158.75mm" file="US20230005101A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="185.93mm" wi="137.41mm" orientation="landscape" file="US20230005101A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="185.93mm" wi="137.41mm" orientation="landscape" file="US20230005101A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="185.93mm" wi="137.41mm" orientation="landscape" file="US20230005101A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="203.45mm" wi="94.32mm" orientation="landscape" file="US20230005101A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="236.30mm" wi="156.80mm" orientation="landscape" file="US20230005101A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="236.30mm" wi="156.80mm" orientation="landscape" file="US20230005101A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="153.75mm" wi="120.65mm" file="US20230005101A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="156.21mm" wi="134.28mm" orientation="landscape" file="US20230005101A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="233.51mm" wi="134.70mm" orientation="landscape" file="US20230005101A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="198.71mm" wi="149.52mm" file="US20230005101A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="236.30mm" wi="156.80mm" orientation="landscape" file="US20230005101A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="236.30mm" wi="156.80mm" orientation="landscape" file="US20230005101A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="198.71mm" wi="149.52mm" file="US20230005101A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="236.30mm" wi="156.80mm" orientation="landscape" file="US20230005101A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="191.60mm" wi="156.80mm" orientation="landscape" file="US20230005101A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="236.30mm" wi="156.55mm" orientation="landscape" file="US20230005101A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="182.63mm" wi="123.27mm" file="US20230005101A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="129.62mm" wi="149.01mm" file="US20230005101A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="236.30mm" wi="156.80mm" orientation="landscape" file="US20230005101A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="155.11mm" wi="120.23mm" orientation="landscape" file="US20230005101A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present disclosure relates to an information processing apparatus, an information processing method, and a recording medium, and in particular, to an information processing apparatus that enables provision of a more natural viewing experience to a user, an information processing method, and a recording medium.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">A conventional technology of providing a virtual reality (VR) experience with a high immersive feeling by displaying an image of a virtual space on a head mounted display (HMD) worn on the head of a user is known.</p><p id="p-0004" num="0003">Patent Document 1 discloses a technology of presenting a graphical user interface (GUI) for performing zoom control of a predetermined region in a virtual space.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Document</heading><p id="p-0005" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0004">Patent Document 1: Japanese Patent Application Laid-Open No. 2019-139673</li></ul></p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><heading id="h-0006" level="1">Problems to be Solved by the Invention</heading><p id="p-0006" num="0005">In a VR experience, it is desirable to provide a natural user interface (NUI) instead of the GUI. For example, it is desirable that a forward and backward movement of a user in a real space is reflected in zoom control of an object in a virtual space.</p><p id="p-0007" num="0006">In the VR experience, for example, in a case where a two-dimensional image such as an omnidirectional image is projected three-dimensionally, a viewing experience of three degrees of freedom (3DoF) reflecting rotation of a viewpoint (head) of the user is provided. However, in the two-dimensional image projected three-dimensionally, since a translational movement of the user's viewpoint in the front-back direction, the left-right direction, and the up-down direction is not reflected as in 6DoF, there is a possibility that the user feels uncomfortable.</p><p id="p-0008" num="0007">The present disclosure has been made in view of such a situation, and an object thereof is to enable provision of a more natural viewing experience to a user who views a two-dimensional image projected three-dimensionally.</p><heading id="h-0007" level="1">Solutions to Problems</heading><p id="p-0009" num="0008">An information processing apparatus of the present disclosure is an information processing apparatus including a control unit that controls display of a two-dimensional image including a plurality of objects having distance information in a three-dimensional coordinate system with a viewpoint position of a user as a reference, in which the control unit controls a display magnification of the two-dimensional image corresponding to a movement amount of the viewpoint position in a real space on the basis of the distance information of the objects that is a region of interest of the user.</p><p id="p-0010" num="0009">An information processing method of the present disclosure is an information processing method including: by an information processing apparatus, controlling display of a two-dimensional image including a plurality of objects having distance information in a three-dimensional coordinate system with a viewpoint position of a user as a reference; and controlling a display magnification of the two-dimensional image corresponding to a movement amount of the viewpoint position in a real space on the basis of the distance information of the objects that is a region of interest of the user.</p><p id="p-0011" num="0010">A recording medium of the present disclosure is a computer-readable recording medium in which a program is recorded, the program configured to cause execution of processing of: controlling display of a two-dimensional image including a plurality of objects having distance information in a three-dimensional coordinate system with a viewpoint position of a user as a reference; and controlling a display magnification of the two-dimensional image corresponding to a movement amount of the viewpoint position in a real space on the basis of the distance information of the objects that is a region of interest of the user.</p><p id="p-0012" num="0011">In the present disclosure, display of a two-dimensional image including a plurality of objects having distance information is controlled in a three-dimensional coordinate system with a viewpoint position of a user as a reference, and a display magnification of the two-dimensional image corresponding to a movement amount of the viewpoint position in a real space is controlled on the basis of the distance information of the objects that is a region of interest of the user.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0008" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram for explaining presentation of a two-dimensional image by a technology according to the present disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram for explaining presentation of a two-dimensional image by a technology according to the present disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for explaining presentation of a two-dimensional image by a technology according to the present disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating a configuration example of an image presentation system to which the technology according to the present disclosure is applied.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram for explaining appearance of an image in the real world.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram for explaining appearance of an image in an omnidirectional image.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram illustrating an external configuration of an HMD according to the present embodiment.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a block diagram illustrating a hardware configuration example of the HMD.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram illustrating a functional configuration example of the HMD.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart for explaining first display processing of the omnidirectional image.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram for explaining a change in a radius of the omnidirectional image.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram for explaining a problem in the first display processing.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart for explaining second display processing of the omnidirectional image.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram for explaining a movement of a center position of the omnidirectional image.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a diagram for explaining an expected movement of the center position of the omnidirectional image.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a diagram for explaining a problem in the second display processing.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a flowchart for explaining third display processing of the omnidirectional image.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a flowchart for explaining the third display processing of the omnidirectional image.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a diagram for explaining a movement of the center position of the omnidirectional image at a viewpoint position.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a block diagram illustrating a configuration example of a computer.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0009" level="1">MODE FOR CARRYING OUT THE INVENTION</heading><p id="p-0033" num="0032">Hereinafter, modes (hereinafter referred to as embodiments) for implementing the present disclosure will be described. Note that the description will be given in the following order.</p><p id="p-0034" num="0033">1. Outline of technology according to the present disclosure</p><p id="p-0035" num="0034">2. Presentation of omnidirectional image and problems thereof</p><p id="p-0036" num="0035">3. Configuration of HMD</p><p id="p-0037" num="0036">4. First display example of omnidirectional image</p><p id="p-0038" num="0037">5. Second display example of omnidirectional image</p><p id="p-0039" num="0038">6. Third display example of omnidirectional image</p><p id="p-0040" num="0039">7. Computer configuration example</p><p id="p-0041" num="0040">&#x3c;1. Outline of Technology According to the Present Disclosure&#x3e;</p><p id="p-0042" num="0041">(Presentation of Two-Dimensional Image)</p><p id="p-0043" num="0042">An image presentation system to which the technology according to the present disclosure (the present technology) is applied displays a two-dimensional image including a plurality of objects having distance information in a three-dimensional coordinate system with a viewpoint position of a user as a reference.</p><p id="p-0044" num="0043">For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a two-dimensional image <b>20</b> is displayed at a position away from a viewpoint position U of a user <b>10</b> by a predetermined distance in an xyz coordinate system with the viewpoint position U of the user <b>10</b> as an origin. The two-dimensional image <b>20</b> is not a three-dimensional image including computer graphics (CG) data, but is, for example, an image obtained by three-dimensionally projecting two-dimensional data obtained by imaging the real world.</p><p id="p-0045" num="0044">The two-dimensional image <b>20</b> includes a first object <b>21</b> and a second object <b>22</b>. Each of the object <b>21</b> and the object <b>22</b> has distance information. The distance information corresponds to, for example, a real-world distance (actual distance) between a camera and each of the objects <b>21</b>, <b>22</b> at the time of imaging of the two-dimensional image <b>20</b>. In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, it is assumed that the object <b>22</b> exists at a position farther than the object <b>21</b> in the real world and has distance information larger than the object <b>21</b> in the two-dimensional image <b>20</b>.</p><p id="p-0046" num="0045">In the image presentation system to which the present technology is applied, the display magnification of the two-dimensional image <b>20</b> corresponding to a movement amount of the viewpoint position U of the user <b>10</b> in the real space is controlled on the basis of distance information of an object in front of the line-of-sight of the user <b>10</b>. Specifically, a change amount of the display magnification of the two-dimensional image <b>20</b> corresponding to the movement amount of the viewpoint position U of the user <b>10</b> is made different between a case where the line-of-sight of the user <b>10</b> is on the object <b>21</b> as indicated by the arrow #<b>1</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref> and a case where the line-of-sight of the user is on the object <b>22</b> as indicated by the arrow #<b>2</b>.</p><p id="p-0047" num="0046">For example, in a case where the line-of-sight of the user <b>10</b> is on the object <b>21</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the two-dimensional image <b>20</b> is enlarged and displayed by the amount by which the viewpoint position U approaches the two-dimensional image <b>20</b>.</p><p id="p-0048" num="0047">On the other hand, in a case where the line-of-sight of the user <b>10</b> is on the object <b>22</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the two-dimensional image <b>20</b> is enlarged and displayed by the amount by which the viewpoint position U approaches the two-dimensional image <b>20</b>, and the change amount of the display magnification is smaller than that in the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0049" num="0048">In the real world, in a case where the user moves forward by a certain distance, an object closer to the user looks larger, but the appearance of an object farther from the user does not change much.</p><p id="p-0050" num="0049">In the image presentation system to which the present technology is applied, the change amount of the display magnification of the two-dimensional image <b>20</b> corresponding to the movement amount of the viewpoint position U is larger as the distance information of the object in front of the line-of-sight of the user <b>10</b> is smaller. On the other hand, the change amount of the display magnification of the two-dimensional image <b>20</b> corresponding to the movement amount of the viewpoint position U is smaller as the distance information of the object in front of the line-of-sight of the user <b>10</b> is larger. As a result, a more natural viewing experience close to the real world can be provided to the user <b>10</b> viewing the two-dimensional image <b>20</b>.</p><p id="p-0051" num="0050">(Configuration Example of Image Presentation System)</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram illustrating a configuration example of an image presentation system that implements the above-described image presentation.</p><p id="p-0053" num="0052">The image presentation system in <figref idref="DRAWINGS">FIG. <b>4</b></figref> includes an information processing apparatus <b>50</b> and a display device <b>60</b>.</p><p id="p-0054" num="0053">The information processing apparatus <b>50</b> is configured as, for example, a personal computer (PC). The information processing apparatus <b>50</b> supplies a two-dimensional image obtained by imaging to the display device <b>60</b> that three-dimensionally projects the two-dimensional image. The display device <b>60</b> is configured as, for example, an HMD worn on the head of the user, and includes a non-transmissive display unit. A two-dimensional image is displayed on the display unit.</p><p id="p-0055" num="0054">The information processing apparatus <b>50</b> includes a control unit <b>51</b> and a storage unit <b>52</b>.</p><p id="p-0056" num="0055">The control unit <b>51</b> controls display of the two-dimensional image stored in the storage unit <b>52</b> on the display device <b>60</b> in a three-dimensional coordinate system having the viewpoint position of the user as a reference. The storage unit <b>52</b> stores a two-dimensional image including a plurality of objects having distance information. The control unit <b>51</b> controls a display magnification of the two-dimensional image corresponding to a movement amount of the viewpoint position of the user in a real space on the basis of distance information of the object that is a region (region of interest) of interest of the user in the two-dimensional image displayed on the display device <b>60</b>.</p><p id="p-0057" num="0056">Note that, in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the information processing apparatus <b>50</b> is configured separately from the display device <b>60</b>, but may be configured integrally with the display device <b>60</b>.</p><p id="p-0058" num="0057">In the image presentation system of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, an omnidirectional image is presented as the two-dimensional image, but the present invention is not limited thereto, and for example, a 180-degree half celestial sphere image may be presented.</p><p id="p-0059" num="0058">&#x3c;2. Presentation of Omnidirectional Image and Problems Thereof&#x3e;</p><p id="p-0060" num="0059">The omnidirectional image reproduces a 360-degree image captured by a 360-degree camera by fixing a positional relationship between the viewpoint position and a presentation surface. The omnidirectional image needs to be presented following the movement of the head even in a case where the user viewing the omnidirectional image moves the head. However, in a case where the user moves the head by approaching or looking into an object in the image at the time of viewing the omnidirectional image, the position of the omnidirectional image is fixed, and thus the appearance of the omnidirectional image does not match the appearance when the user moves the head in the real world.</p><p id="p-0061" num="0060">For example, as illustrated on the left side of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, it is assumed that the user <b>10</b> looks at a tree <b>71</b> and a house <b>72</b> in a field of view <b>70</b> of the real world. The distance between the user <b>10</b> and the tree <b>71</b> is 1 m, and the distance between the user <b>10</b> and the house <b>72</b> is 3 m.</p><p id="p-0062" num="0061">In a case where the user <b>10</b> moves forward by 0.5 m from the state on the left side of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, as illustrated on the right side of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the distance between the user <b>10</b> and the tree <b>71</b> is 0.5 m, and the distance between the user <b>10</b> and the house <b>72</b> is 2.5 m. At this time, in the field of view <b>70</b> of the user <b>10</b>, a nearby object (tree <b>71</b>) looks large only by slightly approaching, while a distant object (house <b>72</b>) does not change much in appearance even by slightly approaching. As described above, the appearance in the real world is affected by a change in distance as an object is closer.</p><p id="p-0063" num="0062">On the other hand, as illustrated on the left side of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, it is assumed that the user <b>10</b> views the tree <b>71</b> and the house <b>72</b> in a viewing region <b>80</b> of an omnidirectional image. The distance (radius of the omnidirectional image) between the user <b>10</b> and the viewing region <b>80</b> is 1 m.</p><p id="p-0064" num="0063">In a case where the user <b>10</b> moves forward by 0.5 m from the state on the left side of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the distance between the user <b>10</b> and the viewing region <b>80</b> is 0.5 m as illustrated on the right side of <figref idref="DRAWINGS">FIG. <b>6</b></figref>. At this time, in the viewing region <b>80</b> of the user <b>10</b>, a nearby object (tree <b>71</b>) looks large only by slightly approaching, and a distant object (house <b>72</b>) also looks large only by slightly approaching. As described above, in the appearance in the omnidirectional image, all objects are equally affected by a change in distance.</p><p id="p-0065" num="0064">In a case where the position of the omnidirectional image is fixed and the head of the user is moved, the appearance of the viewing region (the size and position of the image) changes depending on the distance between the viewpoint position and the presentation surface. For example, in a case where the actual distance between the camera and the object at the time of imaging the omnidirectional image matches the radius of the omnidirectional image, it matches the appearance when the head is moved in the real world. On the other hand, various objects having different actual distances from the camera are reflected in the omnidirectional image. Therefore, in a case where the radius of the omnidirectional image is adjusted to the actual distance of a specific object, it does not match the actual distance of other objects, and there is a possibility that the user feels uncomfortable.</p><p id="p-0066" num="0065">Therefore, in the following, a configuration will be described in which a display magnification of an omnidirectional image corresponding to a movement amount of a viewpoint position (head) of a user is controlled by changing a radius of the omnidirectional image on the basis of distance information of the object that is a region of interest of the user in the omnidirectional image.</p><p id="p-0067" num="0066">&#x3c;3. Configuration of HMD&#x3e;</p><p id="p-0068" num="0067">(External Configuration)</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram illustrating an external configuration of an HMD according to the present embodiment.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an HMD <b>100</b> worn on the head of the user <b>10</b>.</p><p id="p-0071" num="0070">The HMD <b>100</b> is configured as a display device capable of displaying an omnidirectional image, and includes a non-transmissive display. An omnidirectional image <b>110</b> is displayed on the display.</p><p id="p-0072" num="0071">A field of view (viewing region) of the user <b>10</b> in the omnidirectional image <b>110</b> is moved by the user <b>10</b> wearing the HMD <b>100</b> on the head changing the orientation of the head.</p><p id="p-0073" num="0072">(Hardware Configuration Example)</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a block diagram illustrating a hardware configuration example of the HMD <b>100</b> according to the present embodiment.</p><p id="p-0075" num="0074">The HMD <b>100</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref> includes a central processor unit (CPU) <b>121</b>, a memory <b>122</b>, a sensor unit <b>123</b>, an input unit <b>124</b>, an output unit <b>125</b>, and a communication unit <b>126</b>. These are interconnected via a bus <b>127</b>.</p><p id="p-0076" num="0075">The CPU <b>121</b> executes processing for achieving various functions of the HMD <b>100</b> according to programs, data, and the like stored in the memory <b>122</b>.</p><p id="p-0077" num="0076">The memory <b>122</b> includes a storage medium such as a semiconductor memory or a hard disk, and stores programs and data for processing by the CPU <b>121</b>.</p><p id="p-0078" num="0077">The sensor unit <b>123</b> includes various sensors such as an image sensor, a microphone, a gyro sensor, and an acceleration sensor. Various types of sensor information acquired by the sensor unit <b>123</b> are also used for processing by the CPU <b>121</b>.</p><p id="p-0079" num="0078">The input unit <b>124</b> includes buttons, keys, a touch panel, and the like. The output unit <b>125</b> includes the above-described display, a speaker, and the like. The communication unit <b>126</b> is configured as a communication interface that mediates various types of communication.</p><p id="p-0080" num="0079">(Functional Configuration Example)</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram illustrating a functional configuration example of the HMD <b>100</b>.</p><p id="p-0082" num="0081">The HMD <b>100</b> in <figref idref="DRAWINGS">FIG. <b>9</b></figref> includes a control unit <b>150</b>, a sensor unit <b>160</b>, a storage unit <b>170</b>, and a display unit <b>180</b>. The sensor unit <b>160</b>, the storage unit <b>170</b>, and the display unit <b>180</b> correspond to the sensor unit <b>123</b>, the memory <b>122</b>, and the display constituting the output unit <b>125</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, respectively.</p><p id="p-0083" num="0082">The control unit <b>150</b> includes a viewpoint position/line-of-sight direction acquisition unit <b>151</b>, a head position and posture acquisition unit <b>152</b>, a radius setting unit <b>153</b>, a region-of-interest setting unit <b>154</b>, and a display control unit <b>155</b>. The functional blocks included in the control unit <b>150</b> are implemented by the CPU <b>121</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref> executing a predetermined program.</p><p id="p-0084" num="0083">The viewpoint position/line-of-sight direction acquisition unit <b>151</b> acquires a viewpoint position and a line-of-sight direction of the user <b>10</b> in a virtual space on the basis of a viewpoint position and a line-of-sight direction of the user <b>10</b> in the real space sensed by the sensor unit <b>160</b>, and supplies the viewpoint position and the line-of-sight direction to the radius setting unit <b>153</b> and the region-of-interest setting unit <b>154</b>.</p><p id="p-0085" num="0084">The head position and posture acquisition unit <b>152</b> acquires a position and posture of the head of the user <b>10</b> in the virtual space on the basis of a position and posture of the head of the user <b>10</b> in the real space sensed by the sensor unit <b>160</b>, and supplies the position and posture to the region-of-interest setting unit <b>154</b>.</p><p id="p-0086" num="0085">The radius setting unit <b>153</b> sets a radius of the omnidirectional image stored in the storage unit <b>170</b> on the basis of the viewpoint position of the user <b>10</b> acquired by the viewpoint position/line-of-sight direction acquisition unit <b>151</b>. The omnidirectional image in which the radius is set is supplied to the region-of-interest setting unit <b>154</b> and the display control unit <b>155</b>.</p><p id="p-0087" num="0086">The storage unit <b>170</b> stores an omnidirectional image <b>171</b>. The omnidirectional image <b>171</b> includes a plurality of objects having distance information <b>171</b><i>a</i>. The distance information <b>171</b><i>a </i>is depth data or the like acquired at the time of imaging the omnidirectional image.</p><p id="p-0088" num="0087">The region-of-interest setting unit <b>154</b> sets a region of interest of the user <b>10</b> in the omnidirectional image having the radius set by the radius setting unit <b>153</b> on the basis of the line-of-sight direction of the user <b>10</b> acquired by the viewpoint position/line-of-sight direction acquisition unit <b>151</b>. The set region of interest is supplied to the radius setting unit <b>153</b>. The radius setting unit <b>153</b> sets a radius of the omnidirectional image on the basis of the distance information of the object that is the region of interest set by the region-of-interest setting unit <b>154</b> in the omnidirectional image.</p><p id="p-0089" num="0088">The display control unit <b>155</b> causes the display unit <b>180</b> to display the omnidirectional image having the radius set by the radius setting unit <b>153</b>.</p><p id="p-0090" num="0089">Hereinafter, a display example of the omnidirectional image in the HMD <b>100</b> will be described.</p><p id="p-0091" num="0090">&#x3c;4. First Display Example of Omnidirectional Image&#x3e;</p><p id="p-0092" num="0091">(First Display Processing)</p><p id="p-0093" num="0092">First, first display processing of an omnidirectional image will be described with reference to a flowchart in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. The processing of <figref idref="DRAWINGS">FIG. <b>10</b></figref> is performed when the user <b>10</b> wearing the HMD <b>100</b> on the head starts viewing the omnidirectional image.</p><p id="p-0094" num="0093">In step S<b>11</b>, as illustrated in A of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the viewpoint position/line-of-sight direction acquisition unit <b>151</b> acquires the viewpoint position U and a line-of-sight direction V of the user <b>10</b> in the virtual space.</p><p id="p-0095" num="0094">The viewpoint position U of the user <b>10</b> may be a position of the HMD <b>100</b> in the real space or may be an intermediate position between binocular lenses included in the HMD <b>100</b>. Furthermore, in a case where the eyeball positions of both eyes of the user <b>10</b> are estimated, the viewpoint position U may be an intermediate position of the eyeball positions of both eyes.</p><p id="p-0096" num="0095">The line-of-sight direction V of the user <b>10</b> may be a median value of the line-of-sight directions of both eyes of the user <b>10</b> obtained by the line-of-sight detection device built in the HMD <b>100</b>, or may be the line-of-sight direction of one eye in a case where only the line-of-sight direction of one eye can be obtained. Furthermore, the orientation of the face of the user <b>10</b> estimated from the position and posture of the head of the user <b>10</b> acquired by the head position and posture acquisition unit <b>152</b> may be the line-of-sight direction V of the user <b>10</b>. Moreover, a direction input from a pointing device such as a VR controller operated by the user <b>10</b> may be set as the line-of-sight direction V of the user <b>10</b>.</p><p id="p-0097" num="0096">In step S<b>12</b>, as illustrated in A of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the radius setting unit <b>153</b> sets a radius r of the omnidirectional image with the viewpoint position U of the user <b>10</b> as a center position P so that the omnidirectional image has a viewable size. In other words, it is sufficient that the size of the omnidirectional image at the start of presentation is not a non-viewable size for the user <b>10</b>. Examples of the non-viewable size include a size in which the diameter of the omnidirectional image is less than the distance between the two eyes of the user <b>10</b>, and a size in which the radius of the omnidirectional image is outside the view clipping range or less than the shortest presentation distance of the HMD <b>100</b>.</p><p id="p-0098" num="0097">In step S<b>13</b>, as illustrated in B of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the region-of-interest setting unit <b>154</b> sets a region of interest A on the omnidirectional image from an intersection of the line-of-sight direction V of the user <b>10</b> and the omnidirectional image.</p><p id="p-0099" num="0098">The region of interest A may be an intersection of the line-of-sight direction V and the omnidirectional image, or may be an object region showing an object including the intersection. The object region is obtained by specifying an object on the omnidirectional image using semantic segmentation, a visual saliency map, or the like. Furthermore, the object region may be obtained by acquiring depth data on the omnidirectional image using semantic segmentation or the like.</p><p id="p-0100" num="0099">Furthermore, in a case where a virtual object is arranged in the omnidirectional image, when the line-of-sight direction V of the user <b>10</b> intersects with the virtual object arranged in the omnidirectional image instead of an object on the omnidirectional image, the display region of the virtual object may be set as the region of interest A.</p><p id="p-0101" num="0100">Moreover, in a case where the convergence angle of both eyes is acquired, a search region including a depth may be set on the basis of the line-of-sight direction V and the convergence angle, and a region near the search region may be set as the region of interest A.</p><p id="p-0102" num="0101">In step S<b>14</b>, as illustrated in B of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the radius setting unit <b>153</b> acquires an actual distance d to the camera at the time of imaging the object shown in the region of interest A on the basis of the distance information <b>171</b><i>a </i>stored in the storage unit <b>170</b>.</p><p id="p-0103" num="0102">In a case where the distance information corresponding to the region of interest A is uniform, the distance information is set as the actual distance d.</p><p id="p-0104" num="0103">In a case where there is a plurality of pieces of distance information corresponding to the region of interest A, the distance information corresponding to a specific point in the region of interest A is set as the actual distance d. The specific point is the center of the region of interest A, a point closest to the camera in the region of interest A, a point having the highest saliency and visual attraction, or the like. Furthermore, an average value of the distance information corresponding to the region of interest A or a weighted average value weighted by saliency or the like may be set as the actual distance d.</p><p id="p-0105" num="0104">Furthermore, in a case where there is no distance information corresponding to the region of interest A, the distance information of a neighboring area may be acquired, or depth information estimated on the basis of the line-of-sight directions and the convergence angle of both eyes of the user <b>10</b> may be acquired as the distance information.</p><p id="p-0106" num="0105">In step S<b>15</b>, as illustrated in C of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the radius setting unit <b>153</b> changes the size of the omnidirectional image such that the radius r of the omnidirectional image matches the actual distance d with the viewpoint position U as the center. For example, the radius r of the omnidirectional image increases as the distance information of the object that is the region of interest A increases. In the example in C of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the omnidirectional image is enlarged, but the radius r of the omnidirectional image may be changed so that the omnidirectional image is reduced according to the acquired actual distance d.</p><p id="p-0107" num="0106">The timing at which the radius r of the omnidirectional image is changed may be a timing at which the region of interest A is changed (the line-of-sight of the user <b>10</b> moves) or a timing at which the head (viewpoint position) of the user <b>10</b> moves in the real space. Furthermore, in a configuration in which the position of the virtual camera is controlled by a VR controller or the like, the radius r of the omnidirectional image may be changed at the timing when the virtual camera moves.</p><p id="p-0108" num="0107">In a case where the viewpoint position U of the user <b>10</b> goes out of the omnidirectional image due to a change (enlargement or reduction) in the size of the omnidirectional image or the movement of the user <b>10</b>, the omnidirectional image may be moved in conjunction with the position of the virtual camera so that the viewpoint position U of the user <b>10</b> falls within the omnidirectional image. In this case, the omnidirectional image may be made non-viewable by increasing the transmittance or decreasing the brightness in the omnidirectional image.</p><p id="p-0109" num="0108">Note that, in a case where there is no distance information corresponding to the region of interest A, the radius r of the omnidirectional image may not be changed.</p><p id="p-0110" num="0109">Now, in step S<b>15</b>, when the radius r of the omnidirectional image is changed, in step S<b>16</b>, it is determined whether or not the user <b>10</b> ends the viewing of the omnidirectional image. In a case where it is determined that the user <b>10</b> does not end the viewing of the omnidirectional image, the processing proceeds to step S<b>17</b>.</p><p id="p-0111" num="0110">In step S<b>17</b>, as illustrated in A of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the viewpoint position/line-of-sight direction acquisition unit <b>151</b> updates (newly acquires) the viewpoint position U and the line-of-sight direction V of the user <b>10</b> in the virtual space. Thereafter, the process returns to step S<b>13</b>, and the processing in step S<b>13</b> and subsequent steps is repeated.</p><p id="p-0112" num="0111">On the other hand, in step S<b>16</b>, in a case where it is determined that the user <b>10</b> ends the viewing of the omnidirectional image, the display processing of the omnidirectional image ends.</p><p id="p-0113" num="0112">According to the above processing, even in a case where the user moves the head while viewing the omnidirectional image, the change in the appearance (the size and position of the image) of the region of interest conforms to the distance perceived from the omnidirectional image, and matches the appearance when the user moves the head in the real world. As a result, it is possible to provide a more natural viewing experience to the user who views the omnidirectional image without giving an uncomfortable feeling to the user.</p><p id="p-0114" num="0113">(Problem of First Display Processing)</p><p id="p-0115" num="0114">In the above-described first display processing, as illustrated in A of <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the omnidirectional image is presented with the viewpoint position U of the user <b>10</b> as the center position P.</p><p id="p-0116" num="0115">However, as illustrated in B of <figref idref="DRAWINGS">FIG. <b>12</b></figref>, in a case where the size of the omnidirectional image is changed in a state where the viewpoint position U of the user <b>10</b> is moved by the movement of the head of the user <b>10</b>, the viewing region of the user <b>10</b> in the omnidirectional image changes.</p><p id="p-0117" num="0116">Specifically, as illustrated in C of <figref idref="DRAWINGS">FIG. <b>12</b></figref>, in a case where the viewpoint position U of the user <b>10</b> moves, when the omnidirectional image is enlarged so that the radius r of the omnidirectional image matches the actual distance d, an image different from the image in the viewing region before enlargement is shown in the enlarged viewing region indicated by a dotted line in the drawing.</p><p id="p-0118" num="0117">&#x3c;5. Second Display Example of Omnidirectional Image&#x3e;</p><p id="p-0119" num="0118">(Second Display Processing)</p><p id="p-0120" num="0119">Here, with reference to the flowchart of <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the second display processing of the omnidirectional image in which the radius of the omnidirectional image is changed such that the viewing region of the user becomes the same before and after the movement of the viewpoint position in a case where the viewpoint position is moved from the center position of the omnidirectional image will be described. The processing of <figref idref="DRAWINGS">FIG. <b>13</b></figref> is also performed when the user <b>10</b> wearing the HMD <b>100</b> on the head starts viewing the omnidirectional image.</p><p id="p-0121" num="0120">Note that the processing of steps S<b>21</b> to S<b>24</b>, S<b>26</b>, and S<b>27</b> of the flowchart of <figref idref="DRAWINGS">FIG. <b>13</b></figref> is similar to the processing of steps S<b>11</b> to S<b>14</b>, S<b>16</b>, and S<b>17</b> of the flowchart of <figref idref="DRAWINGS">FIG. <b>10</b></figref>, respectively, and thus description thereof is omitted.</p><p id="p-0122" num="0121">That is, after the actual distance d to the camera at the time of imaging the object shown in the region of interest A is acquired in step S<b>24</b>, in step S<b>25</b>, the radius setting unit <b>153</b> moves the center position of the omnidirectional image and makes the radius r of the omnidirectional image match the actual distance d.</p><p id="p-0123" num="0122">Specifically, as illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, when the center position P&#x2032; of the omnidirectional image before enlargement, the center position P of the omnidirectional image after enlargement, and the viewpoint position U of the user <b>10</b> are set, the radius setting unit <b>153</b> matches the radius r of the omnidirectional image with the actual distance d such that PU=d/rP&#x2032;U is satisfied. Here, PU represents a distance (vector) from P to U, and P&#x2032;U represents a distance (vector) from P&#x2032; to U.</p><p id="p-0124" num="0123">For example, the distance from the viewpoint position U to all points on the omnidirectional image is multiplied by d/r with the viewpoint position U of the user <b>10</b> as a reference. Therefore, the center position P of the omnidirectional image after enlargement moves to a position obtained by multiplying the distance from the viewpoint position U to the center position P&#x2032; of the omnidirectional image before enlargement by d/r. As a result, PU=d/rP&#x2032;U is satisfied.</p><p id="p-0125" num="0124">Furthermore, the PU is obtained from PU=d/rP&#x2032;U using the center position P&#x2032; of the omnidirectional image before enlargement and the viewpoint position U of the user <b>10</b>. Then, the center position P&#x2032; of the omnidirectional image before enlargement is moved to a position U-PU with the viewpoint position U of the user <b>10</b> as a reference, and the radius r of the omnidirectional image is changed to the actual distance d.</p><p id="p-0126" num="0125">Although the example in which the omnidirectional image is enlarged has been described above, also in a case where the omnidirectional image is reduced, the center position of the omnidirectional image is moved and the radius of the omnidirectional image is changed in a similar manner.</p><p id="p-0127" num="0126">According to the above processing, even in a case where the viewpoint position U of the user <b>10</b> moves, the size of the omnidirectional image is changed with the viewpoint position U after the movement as a reference. Therefore, as illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the viewing region of the user <b>10</b> in the omnidirectional image can be prevented from being changed. As a result, it is possible to provide a more natural viewing experience to the user who views the omnidirectional image without giving an uncomfortable feeling to the user.</p><p id="p-0128" num="0127">(Problem of Second Display Processing)</p><p id="p-0129" num="0128">As illustrated in <figref idref="DRAWINGS">FIG. <b>15</b></figref>, even in a case where the viewpoint position U of the user <b>10</b> moves during viewing of the omnidirectional image, it is desirable that the viewpoint position U returns to the original position by the movement in the opposite direction of the same distance so that the same omnidirectional image as that before the movement can be viewed.</p><p id="p-0130" num="0129">However, in the above-described second display processing, in a case where the size of the omnidirectional image is changed in a state where the viewpoint position U of the user <b>10</b> has moved as illustrated in A of <figref idref="DRAWINGS">FIG. <b>16</b></figref>, the center position of the omnidirectional image moves from P&#x2032; to P as illustrated in B of <figref idref="DRAWINGS">FIG. <b>16</b></figref>.</p><p id="p-0131" num="0130">From this state, as illustrated in C of <figref idref="DRAWINGS">FIG. <b>16</b></figref>, even if the viewpoint position U of the user <b>10</b> returns to the center position P&#x2032; of the omnidirectional image before enlargement, the user <b>10</b> cannot view the same omnidirectional image as before the movement.</p><p id="p-0132" num="0131">&#x3c;6. Third Display Example of Omnidirectional Image&#x3e;</p><p id="p-0133" num="0132">Here, with reference to the flowcharts in <figref idref="DRAWINGS">FIGS. <b>17</b> and <b>18</b></figref>, third display processing of the omnidirectional image in which, in a case where the viewpoint position moves toward the center position of the omnidirectional image before the movement, the center position of the omnidirectional image after the radius is changed is moved to the center position of the omnidirectional image before the movement will be described. The processing of <figref idref="DRAWINGS">FIGS. <b>17</b> and <b>18</b></figref> is also performed when the user <b>10</b> wearing the HMD <b>100</b> on the head starts viewing the omnidirectional image.</p><p id="p-0134" num="0133">Note that the processing of steps S<b>31</b>, S<b>32</b>, S<b>34</b> to S<b>36</b>, S<b>39</b>, and S<b>40</b> in the flowcharts of <figref idref="DRAWINGS">FIGS. <b>17</b> and <b>18</b></figref> is similar to the processing of steps S<b>21</b> to S<b>27</b> in the flowchart of <figref idref="DRAWINGS">FIG. <b>13</b></figref>, respectively, and thus the description thereof will be omitted.</p><p id="p-0135" num="0134">That is, after the radius r of the omnidirectional image is set with the viewpoint position U of the user <b>10</b> as the center position P in step S<b>32</b>, in step S<b>33</b>, the radius setting unit <b>153</b> holds the center position P of the omnidirectional image as the center position P&#x2032; before enlargement.</p><p id="p-0136" num="0135">Thereafter, in step S<b>36</b>, the center position P of the omnidirectional image moves and the radius r of the omnidirectional image changes to the actual distance d, and then in step S<b>37</b>, the radius setting unit <b>153</b> determines whether or not the viewpoint position U of the user <b>10</b> is moving toward the center position P&#x2032; before enlargement.</p><p id="p-0137" num="0136">For example, in a case where the change amount per unit time in the distance between the viewpoint position U of the user <b>10</b> and the center position P&#x2032; before enlargement is equal to or greater than a predetermined amount, it is determined that the viewpoint position U is moving toward the center position P&#x2032;. Furthermore, it may be determined that the viewpoint position U is moving toward the center position P&#x2032; in a case where the distance between the viewpoint position U of the user <b>10</b> and the center position P&#x2032; before enlargement is equal to or less than a predetermined distance. Moreover, it may be determined that the viewpoint position U is moving toward the center position P&#x2032; in a case where the operation of pulling the head is detected by the gesture determination using the machine learning on the basis of the orientation of the face of the user <b>10</b> and the movement amount and movement direction of the viewpoint position U of the user <b>10</b>.</p><p id="p-0138" num="0137">As illustrated in <figref idref="DRAWINGS">FIG. <b>19</b></figref>, in a case where it is determined that the viewpoint position U is moving toward the center position P&#x2032;, the processing proceeds to step S<b>38</b>, and the radius setting unit <b>153</b> moves the center position P of the omnidirectional image to the center position P&#x2032; before enlargement.</p><p id="p-0139" num="0138">In the movement of the center position of the omnidirectional image, the movement speed may be controlled. For example, the center position of the omnidirectional image may move at a predetermined constant speed, or the speed may change during the movement, such as slowly moving at the start or end of the movement. Moreover, the center position of the omnidirectional image may move at a speed according to the movement speed of the head of the user <b>10</b>.</p><p id="p-0140" num="0139">Furthermore, the center position of the omnidirectional image may move according to the viewpoint position U of the user <b>10</b>. For example, as the viewpoint position U approaches the center position P&#x2032; before enlargement, the center position P after enlargement may approach the center position P&#x2032; before enlargement. Specifically, when the distance between the viewpoint position U and the center position P&#x2032; before enlargement is halved, the distance between the center position P after enlargement and the center position P&#x2032; before enlargement may be changed at the same ratio as the distance between the viewpoint position U and the center position P&#x2032; before enlargement, for example, the distance between the center position P after enlargement and the center position P&#x2032; before enlargement is halved. Furthermore, by weighting the distance between the viewpoint position U and the center position P&#x2032; before enlargement, the movement amount of the center position P after enlargement may be increased as the viewpoint position U approaches the center position P&#x2032; before enlargement.</p><p id="p-0141" num="0140">On the other hand, in a case where it is determined in step S<b>37</b> that the viewpoint position U is not moving toward the center position P&#x2032;, the processing proceeds to step S<b>40</b>, and after the viewpoint position U and the line-of-sight direction V of the user <b>10</b> are updated, the processing returns to step <b>334</b>.</p><p id="p-0142" num="0141">According to the above processing, it is possible to avoid accumulation of the deviation of the center position of the omnidirectional image, which occurs in a case where the viewpoint position U of the user <b>10</b> moves. As a result, when the viewpoint position U of the user <b>10</b> returns to the center position P&#x2032; of the omnidirectional image before enlargement, the user <b>10</b> can view the same omnidirectional image as before the movement.</p><p id="p-0143" num="0142">&#x3c;7. Computer Configuration Example&#x3e;</p><p id="p-0144" num="0143">The series of processing described above can be also performed by hardware or can be performed by software. In a case where a series of processing is performed by software, a program constituting the software is installed in a computer. Here, the computer includes a computer incorporated in dedicated hardware and a general-purpose personal computer capable of executing various functions by installing various programs, for example, and the like.</p><p id="p-0145" num="0144"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a block diagram showing a configuration example of a hardware of a computer that executes the above-described series of processing by a program.</p><p id="p-0146" num="0145">In a computer, a CPU <b>501</b>, a read only memory (ROM) <b>502</b>, and a random access memory (RAM) <b>503</b> are mutually connected by a bus <b>504</b>.</p><p id="p-0147" num="0146">An input and output interface <b>505</b> is further connected to the bus <b>504</b>. An input unit <b>506</b>, an output unit <b>507</b>, a storage unit <b>508</b>, a communication unit <b>509</b>, and a drive <b>510</b> are connected to the input and output interface <b>505</b>.</p><p id="p-0148" num="0147">The input unit <b>506</b> includes a keyboard, a mouse, a microphone, and the like. The output unit <b>507</b> includes a display, a speaker, and the like. The storage unit <b>508</b> includes a hard disk, a nonvolatile memory, and the like. The communication unit <b>509</b> includes a network interface and the like. The drive <b>510</b> drives a removable medium <b>511</b> such as a magnetic disk, an optical disk, a magneto-optical disk, or a semiconductor memory.</p><p id="p-0149" num="0148">In the computer configured as described above, for example, the CPU <b>501</b> loads the program stored in the storage unit <b>508</b> into the RAM <b>503</b> via the input and output interface <b>505</b> and the bus <b>504</b>, and executes the program, so that the above-described series of processing is performed.</p><p id="p-0150" num="0149">The program executed by the computer (CPU <b>501</b>) can be provided by being recorded on the removable medium <b>511</b> as a package medium or the like, for example. Furthermore, the program can be provided via a wired or wireless transmission medium such as a local area network, the Internet, or digital satellite broadcasting.</p><p id="p-0151" num="0150">In the computer, a program can be installed in the storage unit <b>508</b> via the input and output interface <b>505</b> by mounting the removable medium <b>511</b> to the drive <b>510</b>. Furthermore, the program can be received by the communication unit <b>509</b> via a wired or wireless transmission medium and installed in the storage unit <b>508</b>. In addition, the program can be installed in the ROM <b>502</b> or the storage unit <b>508</b> in advance.</p><p id="p-0152" num="0151">Note that the program executed by the computer may be a program of processing in chronological order according to the order described in the present specification or may be a program of processing in parallel or at necessary timing such as when a call is made.</p><p id="p-0153" num="0152">The embodiments of the present disclosure are not limited to the above-described embodiments, and various modifications are possible without departing from the gist of the present disclosure.</p><p id="p-0154" num="0153">The effects described in the present specification are merely examples and are not intended to be limiting, and other effects may be provided.</p><p id="p-0155" num="0154">Moreover, the present disclosure can adopt the following configuration.</p><p id="p-0156" num="0155">(1)</p><p id="p-0157" num="0156">An information processing apparatus including</p><p id="p-0158" num="0157">a control unit that controls display of a two-dimensional image including a plurality of objects having distance information in a three-dimensional coordinate system with a viewpoint position of a user as a reference,</p><p id="p-0159" num="0158">in which the control unit controls a display magnification of the two-dimensional image corresponding to a movement amount of the viewpoint position in a real space on the basis of the distance information of the objects that is a region of interest of the user.</p><p id="p-0160" num="0159">(2)</p><p id="p-0161" num="0160">The information processing apparatus according to (1),</p><p id="p-0162" num="0161">in which a first object and a second object have different pieces of the distance information, and</p><p id="p-0163" num="0162">the control unit varies a change amount of the display magnification between a case where the region of interest is the first object and a case where the region of interest is the second object.</p><p id="p-0164" num="0163">(3)</p><p id="p-0165" num="0164">The information processing apparatus according to (2),</p><p id="p-0166" num="0165">in which, in a case where the second object has the distance information larger than the distance information of the first object, when the region of interest is the second object, the control unit decreases the change amount of the display magnification as compared with a case where the region of interest is the first object.</p><p id="p-0167" num="0166">(4)</p><p id="p-0168" num="0167">The information processing apparatus according to any one of (1) to (3),</p><p id="p-0169" num="0168">in which the two-dimensional image is an omnidirectional image, and</p><p id="p-0170" num="0169">the control unit controls the display magnification of the omnidirectional image corresponding to the movement amount of the viewpoint position by changing a radius of the omnidirectional image on the basis of the distance information of the object that is the region of interest.</p><p id="p-0171" num="0170">(5)</p><p id="p-0172" num="0171">The information processing apparatus according to (4),</p><p id="p-0173" num="0172">in which the distance information is an actual distance between a camera and the object at time of imaging the omnidirectional image, and</p><p id="p-0174" num="0173">the control unit matches the radius of the omnidirectional image with the actual distance to the object in which the region of interest is present.</p><p id="p-0175" num="0174">(6)</p><p id="p-0176" num="0175">The information processing apparatus according to (5),</p><p id="p-0177" num="0176">in which the control unit changes the radius of the omnidirectional image with the viewpoint position as a center.</p><p id="p-0178" num="0177">(7)</p><p id="p-0179" num="0178">The information processing apparatus according to (6),</p><p id="p-0180" num="0179">in which, in a case where the viewpoint position is moved from a center position of the omnidirectional image, the control unit changes the radius of the omnidirectional image such that a viewing region of the user becomes the same before and after movement of the viewpoint position.</p><p id="p-0181" num="0180">(8)</p><p id="p-0182" num="0181">The information processing apparatus according to (7),</p><p id="p-0183" num="0182">in which the control unit moves the center position of the omnidirectional image on the basis of the viewpoint position after the movement to change the radius of the omnidirectional image.</p><p id="p-0184" num="0183">(9)</p><p id="p-0185" num="0184">The information processing apparatus according to (8),</p><p id="p-0186" num="0185">in which, in a case where the viewpoint position moves toward the center position of the omnidirectional image before the movement, the control unit moves the center position of the omnidirectional image after the radius is changed to the center position of the omnidirectional image before the movement.</p><p id="p-0187" num="0186">(10)</p><p id="p-0188" num="0187">The information processing apparatus according to any one of (1) to (9),</p><p id="p-0189" num="0188">in which the control unit sets the region of interest on the basis of an intersection of a line-of-sight direction of the user and the two-dimensional image.</p><p id="p-0190" num="0189">(11)</p><p id="p-0191" num="0190">The information processing apparatus according to (10),</p><p id="p-0192" num="0191">in which the control unit sets a region in which the object including the intersection is shown in the two-dimensional image as the region of interest.</p><p id="p-0193" num="0192">(12)</p><p id="p-0194" num="0193">The information processing apparatus according to any one of (1) to (11),</p><p id="p-0195" num="0194">in which the control unit changes the display magnification of the two-dimensional image at a timing when the region of interest changes.</p><p id="p-0196" num="0195">(13)</p><p id="p-0197" num="0196">The information processing apparatus according to any one of (1) to (11),</p><p id="p-0198" num="0197">in which the control unit changes the display magnification of the two-dimensional image at a timing when the viewpoint position changes in the real space.</p><p id="p-0199" num="0198">(14)</p><p id="p-0200" num="0199">The information processing apparatus according to any one of (1) to (13),</p><p id="p-0201" num="0200">in which the control unit causes a head mounted display (HMD) worn on a head of the user to display the two-dimensional image.</p><p id="p-0202" num="0201">(15)</p><p id="p-0203" num="0202">The information processing apparatus according to (14),</p><p id="p-0204" num="0203">in which the viewpoint position is a position of the HMD.</p><p id="p-0205" num="0204">(16)</p><p id="p-0206" num="0205">The information processing apparatus according to (14),</p><p id="p-0207" num="0206">in which the viewpoint position is a position based on a position of a lens of the HMD.</p><p id="p-0208" num="0207">(17)</p><p id="p-0209" num="0208">An information processing method including:</p><p id="p-0210" num="0209">by an information processing apparatus,</p><p id="p-0211" num="0210">controlling display of a two-dimensional image including a plurality of objects having distance information in a three-dimensional coordinate system with a viewpoint position of a user as a reference; and</p><p id="p-0212" num="0211">controlling a display magnification of the two-dimensional image corresponding to a movement amount of the viewpoint position in a real space on the basis of the distance information of the objects that is a region of interest of the user.</p><p id="p-0213" num="0212">(18)</p><p id="p-0214" num="0213">A computer-readable recording medium in which a program is recorded, the program configured to cause execution of processing of:</p><p id="p-0215" num="0214">controlling display of a two-dimensional image including a plurality of objects having distance information in a three-dimensional coordinate system with a viewpoint position of a user as a reference; and</p><p id="p-0216" num="0215">controlling a display magnification of the two-dimensional image corresponding to a movement amount of the viewpoint position in a real space on the basis of the distance information of the objects that is a region of interest of the user.</p><heading id="h-0010" level="1">REFERENCE SIGNS LIST</heading><p id="p-0217" num="0000"><ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0216"><b>50</b> Information processing apparatus</li>    <li id="ul0002-0002" num="0217"><b>51</b> Control unit</li>    <li id="ul0002-0003" num="0218"><b>52</b> Storage unit</li>    <li id="ul0002-0004" num="0219"><b>60</b> Display device</li>    <li id="ul0002-0005" num="0220"><b>100</b> HMD</li>    <li id="ul0002-0006" num="0221"><b>121</b> CPU</li>    <li id="ul0002-0007" num="0222"><b>122</b> Memory</li>    <li id="ul0002-0008" num="0223"><b>123</b> Sensor unit</li>    <li id="ul0002-0009" num="0224"><b>124</b> Input unit</li>    <li id="ul0002-0010" num="0225"><b>125</b> Output unit</li>    <li id="ul0002-0011" num="0226"><b>126</b> Communication unit</li>    <li id="ul0002-0012" num="0227"><b>150</b> Control unit</li>    <li id="ul0002-0013" num="0228"><b>160</b> Sensor unit</li>    <li id="ul0002-0014" num="0229"><b>170</b> Storage unit</li>    <li id="ul0002-0015" num="0230"><b>180</b> Display unit</li>    <li id="ul0002-0016" num="0231"><b>151</b> Line-of-sight position/line-of-sight direction acquisition unit</li>    <li id="ul0002-0017" num="0232"><b>152</b> Head position and posture acquisition unit</li>    <li id="ul0002-0018" num="0233"><b>153</b> Radius setting unit</li>    <li id="ul0002-0019" num="0234"><b>154</b> Region-of-interest setting unit</li>    <li id="ul0002-0020" num="0235"><b>155</b> Display control unit</li>    <li id="ul0002-0021" num="0236"><b>171</b> Omnidirectional image</li>    <li id="ul0002-0022" num="0237"><b>171</b><i>a </i>Distance information</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An information processing apparatus comprising<claim-text>a control unit that controls display of a two-dimensional image including a plurality of objects having distance information in a three-dimensional coordinate system with a viewpoint position of a user as a reference,</claim-text><claim-text>wherein the control unit controls a display magnification of the two-dimensional image corresponding to a movement amount of the viewpoint position in a real space on a basis of the distance information of the objects that is a region of interest of the user.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein a first object and a second object have different pieces of the distance information, and</claim-text><claim-text>the control unit varies a change amount of the display magnification between a case where the region of interest is the first object and a case where the region of interest is the second object.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The information processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>,<claim-text>wherein, in a case where the second object has the distance information larger than the distance information of the first object, when the region of interest is the second object, the control unit decreases the change amount of the display magnification as compared with a case where the region of interest is the first object.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the two-dimensional image is an omnidirectional image, and</claim-text><claim-text>the control unit controls the display magnification of the omnidirectional image corresponding to the movement amount of the viewpoint position by changing a radius of the omnidirectional image on a basis of the distance information of the object that is the region of interest.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The information processing apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>,<claim-text>wherein the distance information is an actual distance between a camera and the object at time of imaging the omnidirectional image, and</claim-text><claim-text>the control unit matches the radius of the omnidirectional image with the actual distance to the object in which the region of interest is present.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The information processing apparatus according to <claim-ref idref="CLM-00005">claim 5</claim-ref>,<claim-text>wherein the control unit changes the radius of the omnidirectional image with the viewpoint position as a center.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The information processing apparatus according to <claim-ref idref="CLM-00006">claim 6</claim-ref>,<claim-text>wherein, in a case where the viewpoint position is moved from a center position of the omnidirectional image, the control unit changes the radius of the omnidirectional image such that a viewing region of the user becomes the same before and after movement of the viewpoint position.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The information processing apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>,<claim-text>wherein the control unit moves the center position of the omnidirectional image on a basis of the viewpoint position after the movement to change the radius of the omnidirectional image.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The information processing apparatus according to <claim-ref idref="CLM-00008">claim 8</claim-ref>,<claim-text>wherein, in a case where the viewpoint position moves toward the center position of the omnidirectional image before the movement, the control unit moves the center position of the omnidirectional image after the radius is changed to the center position of the omnidirectional image before the movement.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the control unit sets the region of interest on a basis of an intersection of a line-of-sight direction of the user and the two-dimensional image.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The information processing apparatus according to <claim-ref idref="CLM-00010">claim 10</claim-ref>,<claim-text>wherein the control unit sets a region in which the object including the intersection is shown in the two-dimensional image as the region of interest.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the control unit changes the display magnification of the two-dimensional image at a timing when the region of interest changes.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the control unit changes the display magnification of the two-dimensional image at a timing when the viewpoint position changes in the real space.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the control unit causes a head mounted display (HMD) worn on a head of the user to display the two-dimensional image.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The information processing apparatus according to <claim-ref idref="CLM-00014">claim 14</claim-ref>,<claim-text>wherein the viewpoint position is a position of the HMD.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The information processing apparatus according to <claim-ref idref="CLM-00014">claim 14</claim-ref>,<claim-text>wherein the viewpoint position is a position on a basis of a position of a lens of the HMD.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. An information processing method comprising:<claim-text>by an information processing apparatus,</claim-text><claim-text>controlling display of a two-dimensional image including a plurality of objects having distance information in a three-dimensional coordinate system with a viewpoint position of a user as a reference; and</claim-text><claim-text>controlling a display magnification of the two-dimensional image corresponding to a movement amount of the viewpoint position in a real space on a basis of the distance information of the objects that is a region of interest of the user.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. A computer-readable recording medium in which a program is recorded, the program configured to cause execution of processing of:<claim-text>controlling display of a two-dimensional image including a plurality of objects having distance information in a three-dimensional coordinate system with a viewpoint position of a user as a reference; and</claim-text><claim-text>controlling a display magnification of the two-dimensional image corresponding to a movement amount of the viewpoint position in a real space on a basis of the distance information of the objects that is a region of interest of the user.</claim-text></claim-text></claim></claims></us-patent-application>