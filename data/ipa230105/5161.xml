<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005162A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005162</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17850665</doc-number><date>20220627</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2021-111258</doc-number><date>20210705</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>246</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>579</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>90</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>248</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>579</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>90</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>74</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30248</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10016</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30244</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">IMAGE PROCESSING SYSTEM, IMAGE PROCESSING METHOD, AND STORAGE MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>TOYOTA JIDOSHA KABUSHIKI KAISHA</orgname><address><city>Toyota-shi</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>NAKANO</last-name><first-name>Yusuke</first-name><address><city>Nagoya-shi</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>TOYOTA JIDOSHA KABUSHIKI KAISHA</orgname><role>03</role><address><city>Toyota-shi</city><country>JP</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An image processing system includes an image acquisition unit that acquires a plurality of images including a moving object image, an image capturing direction data calculation unit that calculates image capturing direction data indicating an image capturing direction in which an imaging device captures an image of a moving object at a time when the images is captured, a feature amount calculation unit that calculates a feature amount of the moving object image extracted from the images, and an associating unit that associates the moving objects in the images with each other based on the image capturing direction data and the feature amount.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="102.53mm" wi="133.01mm" file="US20230005162A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="137.84mm" wi="135.04mm" file="US20230005162A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="178.48mm" wi="129.96mm" file="US20230005162A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="119.13mm" wi="85.17mm" file="US20230005162A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="98.13mm" wi="98.04mm" file="US20230005162A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="106.26mm" wi="121.92mm" file="US20230005162A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="122.00mm" wi="119.04mm" file="US20230005162A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="133.77mm" wi="121.24mm" file="US20230005162A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="184.15mm" wi="129.96mm" file="US20230005162A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="111.42mm" wi="75.78mm" file="US20230005162A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="220.56mm" wi="119.97mm" file="US20230005162A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application claims priority to Japanese Patent Application No. 2021-111258 filed on Jul. 5, 2021, incorporated herein by reference in its entirety.</p><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">1. Technical Field</heading><p id="p-0003" num="0002">The present disclosure relates to an image processing system, an image processing method, and a storage medium.</p><heading id="h-0004" level="1">2. Description of Related Art</heading><p id="p-0004" num="0003">There is a well-known technique of capturing an image of a moving object such as a vehicle and tracking behaviors of each of moving objects over a relatively wide area. Related art Japanese Unexamined Patent Application Publication No. 10-105690 (JP 10-105690 A) discloses a disclosure for monitoring a vehicle within a monitored area using a plurality of behavior detection units. The behavior detection unit disclosed in JP 10-105690 A stores images of several vehicles detected from captured images, and ID numbers assigned to distinguish these vehicles. When the detected vehicle leaves a vehicle detection area, each behavior detection unit stores the information in a buffer. An adjacent behavior detection unit compares a captured image of each vehicle with the vehicle image stored in the buffer of the immediately preceding behavior detection unit, and assigns an ID number to the vehicle based on a comparison result. It is possible to track a vehicle by assigning the same ID number of the same vehicle.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0005" num="0004">The behavior detection unit disclosed in JP 10-105690 A compares a plurality of captured images and assigns the ID number to the vehicle based on comparison results. However, in a case where the moving object such as a vehicle shows different posture or lighting in the monitored area, the same vehicle may be wrongfully recognized as a different vehicle.</p><p id="p-0006" num="0005">The present disclosure provides an image processing system, an image processing method, and a storage medium, each of which is capable of accurately associating moving objects to be tracked in a plurality of images with each other.</p><p id="p-0007" num="0006">An image processing system according to a first aspect of the present disclosure includes an image acquisition unit configured to acquire a plurality of images including a moving object image, an image capturing direction information calculation unit configured to calculate image capturing direction information indicating an image capturing direction in which an imaging device captures an image of a moving object at a time when the images is captured, a feature amount calculation unit configured to calculate a feature amount of the moving object image extracted from the images, and an associating unit configured to associate the moving objects in the images with each other based on the image capturing direction information and the feature amount.</p><p id="p-0008" num="0007">In the first aspect, an image processing system includes an image acquisition unit configured to acquire a plurality of images including a moving object image, a feature amount calculation unit configured to calculate a feature amount of the moving object image extracted from the images, and a feature amount of another moving object image extracted from the same image as an image including the moving object image, and an associating unit configured to associate the moving objects in the images with each other, based on the feature amount of the moving object image and the feature amount of the other moving object image.</p><p id="p-0009" num="0008">An image processing method according to a second aspect of the present disclosure includes acquiring a plurality of images including a moving object image, calculating image capturing direction information indicating an image capturing direction in which an imaging device captures an image of a moving object at a time when the images is captured, calculating a feature amount of the moving object image extracted from the images, and associating the moving objects in the images with each other based on the image capturing direction information and the feature amount.</p><p id="p-0010" num="0009">An image processing method according to the second aspect includes acquiring a plurality of images including a moving object image, calculating a feature amount of the moving object image extracted from the images, and a feature amount of the other moving object image extracted from the same image as an image including the moving object image, and associating the moving objects in the images with each other, based on the feature amount of the moving object image and the feature amount of the other moving object image.</p><p id="p-0011" num="0010">A non-transitory storage medium according to a third aspect of the present disclosure stores an image processing program to cause a computer to execute acquiring a plurality of images including a moving object image, calculating image capturing direction information indicating an image capturing direction in which an imaging device captures an image of a moving object at a time when the images is captured, calculating a feature amount of the moving object image extracted from the images, and associating the moving object in the images with each other based on the image capturing direction information and the feature amount.</p><p id="p-0012" num="0011">In the third aspect, a non-transitory storage medium storing an image processing program causes a computer to execute acquiring a plurality of images including a moving object image, calculating a feature amount of the moving object image extracted from the images, and a feature amount of the other moving object image extracted from the same image as an image including the moving object image, and associating the moving objects in the images with each other, based on the feature amount of the moving object image and the feature amount of the other moving object image.</p><p id="p-0013" num="0012">With each aspect of the present disclosure, the moving object is able to accurately associate the moving objects to be tracked in the images with each other.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0014" num="0013">Features, advantages, and technical and industrial significance of exemplary embodiments of the disclosure will be described below with reference to the accompanying drawings, in which like signs denote like elements, and wherein:</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating a configuration of an image processing system according to a first embodiment;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart illustrating a process of the image processing system according to the first embodiment;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an explanatory diagram illustrating the image processing system according to the first embodiment;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an explanatory diagram illustrating the image processing system according to the first embodiment;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an explanatory diagram illustrating a case where image capturing directions are similar in the image processing system according to the first embodiment;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is an explanatory diagram illustrating a case where image capturing directions are not similar in the image processing system according to the first embodiment;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is an explanatory diagram illustrating an image processing system according to a second embodiment;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart illustrating a process of the image processing system according to the second embodiment;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating a process of identifying a nearby vehicle in the image processing system according to the second embodiment;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is an explanatory diagram illustrating an image processing system according to the second embodiment; and</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram illustrating a configuration example of hardware.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION OF EMBODIMENTS</heading><heading id="h-0008" level="1">First Embodiment</heading><p id="p-0026" num="0025">An image processing system <b>100</b> according to the present embodiment will be described hereinbelow. The image processing system <b>100</b> is an information processing system that acquires a plurality of images including a moving object to be tracked and associates the moving objects in the images with each other. The moving object may include, for example, a vehicle, a person, an animal, or a robot. However the moving object is not limited thereto, and the present disclosure may be applied to various mobile objects that may be tracked.</p><p id="p-0027" num="0026">In the present embodiment, a case where the moving object to be tracked is a vehicle <b>200</b> traveling on a road will be described as an example. The image processing system <b>100</b> acquires a plurality of images captured by a camera installed on the road. The image processing system <b>100</b> extracts an image of the vehicle <b>200</b> from the plurality of acquired images, and associates the vehicles <b>200</b> in the plurality of images with each other.</p><p id="p-0028" num="0027">Hereinafter, embodiments of the present disclosure will be described with reference to drawings. <figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating a configuration of the image processing system <b>100</b> according to the present embodiment. The image processing system <b>100</b> includes a camera CAM<b>1</b>, a camera CAM<b>2</b>, an image acquisition unit <b>101</b>, a camera information acquisition unit <b>102</b>, an object detection unit <b>103</b>, an object feature extraction unit <b>104</b>, a comparison unit <b>105</b>, an ID output unit <b>106</b>, a detection model <b>107</b>, a feature extraction model <b>108</b>, a comparison model <b>109</b>, and a learning unit <b>110</b>.</p><p id="p-0029" num="0028">The cameras CAM<b>1</b> and CAM<b>2</b> are imaging devices that capture images of the vehicle <b>200</b> to be tracked. The camera CAM<b>1</b> or CAM<b>2</b> may be collectively referred to simply as a &#x201c;camera&#x201d; hereinbelow. The camera is installed in an area where an image of the vehicle <b>200</b> can be captured. The camera may be, for example, an RGB camera, a monochrome camera, a grayscale camera, or an IR camera. However the camera is not limited thereto, and various cameras may be used. The number of cameras may be one or more. Two cameras will be adopted hereinbelow, but three or more cameras may be used. A combination of different types of cameras may also be employed. For example, the camera CAM<b>1</b> may be an RGB camera and the camera CAM<b>2</b> may be a monochrome camera.</p><p id="p-0030" num="0029">The captured image obtained from the camera may be a still image or a moving image. The image capturing direction of the camera may or may not be fixed. For example, the camera may capture a 360-degree image, and may capture an image in a different direction at a predetermined time interval.</p><p id="p-0031" num="0030">The camera is installed in an area where an image of the moving object to be tracked can be captured. In the present embodiment, since the vehicle <b>200</b> is tracked, it is assumed that the camera is installed on the road on which the vehicle <b>200</b> travels. The image processing system <b>100</b> and the plurality of cameras may be connected by a wired or wireless network. In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the cameras CAM<b>1</b> and CAM<b>2</b> are included in the image processing system <b>100</b>, but they do not have to be mounted in the same device. For example, the cameras CAM<b>1</b> and CAM<b>2</b> may be installed on the road, and other functional units may be installed in a predetermined server.</p><p id="p-0032" num="0031">The captured image may include a moving object other than the moving object to be monitored. For example, a moving object different from the moving object to be monitored may be included in the captured image. A moving object different from the moving object to be monitored may be referred to as &#x201c;the other moving object&#x201d; hereinbelow. The other moving object may be, for example, a vehicle traveling in the same traveling direction as the vehicle to be monitored. However, the other moving object is not limited thereto, and the other moving object may be a person, an animal, a robot, or the like.</p><p id="p-0033" num="0032">The image acquisition unit <b>101</b> acquires several captured images including a moving object image to be tracked from the camera. The image acquisition unit <b>101</b> acquires a captured image including the vehicle <b>220</b> from the cameras CAM<b>1</b> and CAM<b>2</b>. The image acquisition unit <b>101</b> may acquire a captured image each time the image is captured by the camera, or may acquire captured images at a predetermined time interval. The image acquisition unit <b>101</b> acquires time stamp information indicating a date and time when the image is captured together with the captured image.</p><p id="p-0034" num="0033">The camera information acquisition unit <b>102</b> acquires camera parameters and outputs the acquired camera parameters to the object feature extraction unit <b>104</b>.</p><p id="p-0035" num="0034">The object detection unit <b>103</b> uses the detection model <b>107</b> to detect an area of an object included in the captured image acquired by the image acquisition unit <b>101</b>.</p><p id="p-0036" num="0035">The object feature extraction unit <b>104</b> functions as the image capturing direction information calculation unit and the feature amount calculation unit. The image capturing direction information calculation unit is a functional unit that calculates image capturing direction information indicating the image capturing direction in which the camera captures an image of a moving object when the image is captured. Further, the feature amount calculation unit is a functional unit that calculates a feature amount of the moving object image extracted from the plurality of images. Each function will be described hereinbelow. The detailed procedure of each process will be described later.</p><p id="p-0037" num="0036">The object feature extraction unit <b>104</b> (image capturing direction information calculation unit) calculates image capturing direction information indicating the image capturing direction in which the camera captures the image of the moving object at a time when the plurality of captured images acquired by the image acquisition unit <b>101</b>. The image capturing direction information can represent, for example, the image capturing direction in which each of the cameras CAM<b>1</b> and CAM<b>2</b> captures the image of the vehicle <b>200</b> using unit direction vectors n<sub>1 </sub>and n<sub>2</sub>. The object feature extraction unit <b>104</b> generates the unit direction vectors n<sub>1 </sub>and n<sub>2</sub>. Hereinbelow, an arrow on an alphabetic character indicating a vector may be omitted, and only a term may be used to indicate that it is a vector.</p><p id="p-0038" num="0037">Although it is described here that both the unit direction vectors n<sub>1 </sub>and n<sub>2 </sub>are generated, it is not necessary to generate them at the same time. The unit direction vectors n<sub>1 </sub>and n<sub>2 </sub>may be generated at different times. For example, the unit direction vector n<sub>1 </sub>or n<sub>2 </sub>generated may be stored in a storage unit (not shown). The same applies to image vectors i<sub>1 </sub>and i<sub>2</sub>, integrated vectors g<sub>1 </sub>and g<sub>2</sub>, and feature vectors f<sub>1 </sub>and f<sub>2 </sub>described later.</p><p id="p-0039" num="0038">Further, the object feature extraction unit <b>104</b> (feature amount calculation unit) calculates the feature amount of the moving object image detected by the object detection unit <b>103</b>. In particular, the object feature extraction unit <b>104</b> calculates the feature amounts of the image of the vehicle <b>200</b>, which is detected from the images captured by the cameras CAM<b>1</b> and CAM<b>2</b>, by adding the unit direction vectors n<sub>1 </sub>and n<sub>2</sub>, respectively. The respective feature amount can be represented by feature vectors f<sub>1 </sub>and f<sub>2</sub>. The object feature extraction unit <b>104</b> generates the feature vectors f<sub>1 </sub>and f<sub>2 </sub>by executing the following process.</p><p id="p-0040" num="0039">The object feature extraction unit <b>104</b> calculates image vectors i<sub>1 </sub>and i<sub>2 </sub>of the images captured by the cameras CAM<b>1</b> and CAM<b>2</b>, respectively. The image vectors i<sub>1 </sub>and i<sub>2 </sub>may indicate a pixel value of each pixel in each image.</p><p id="p-0041" num="0040">The object feature extraction unit <b>104</b> integrates the unit direction vector n<sub>1 </sub>and the image vector i<sub>1 </sub>to generate an integrated vector g<sub>1</sub>. Similarly, the object feature extraction unit <b>104</b> integrates the unit direction vector n<sub>2 </sub>and the image vector i<sub>2 </sub>to generate an integrated vector g<sub>2</sub>. The object feature extraction unit <b>104</b> uses the feature extraction model <b>108</b> to generate the feature vectors f<sub>1 </sub>and f<sub>2 </sub>from the integrated vectors g<sub>1 </sub>and g<sub>2</sub>, respectively.</p><p id="p-0042" num="0041">As stated above, the object feature extraction unit <b>104</b> generates the feature vector f<sub>1 </sub>indicating the feature amount of the vehicle <b>200</b> captured by the camera CAM<b>1</b>, and the feature vector f<sub>2 </sub>indicating the feature amount of the vehicle <b>200</b> captured by the camera CAM<b>2</b>.</p><p id="p-0043" num="0042">The object feature extraction unit <b>104</b> may calculate a feature amount of the other moving object image extracted from the same image as the image including the vehicle <b>200</b>. For example, in a case where the vehicle <b>200</b> and a vehicle <b>210</b> different from the vehicle <b>200</b> are included in the same image, the object feature extraction unit <b>104</b> may calculate the feature amounts of the vehicle <b>200</b> and the vehicle <b>210</b>, respectively, by the process stated above.</p><p id="p-0044" num="0043">The comparison unit <b>105</b> and the ID output unit <b>106</b> function as an associating unit. The associating unit is a functional unit that associates the moving objects in the images with each other based on the image capturing direction information and the feature amount of the captured image. In particular, the comparison unit <b>105</b> compares the feature vectors f<sub>1 </sub>and f<sub>2 </sub>generated by the object feature extraction unit <b>104</b> using the comparison model <b>109</b>. The ID output unit <b>106</b> assigns a moving object ID to the vehicle <b>200</b> included in the image based on the comparison result made by the comparison unit <b>105</b>. The moving object ID is identification information for identifying the moving object included in the image. Consequently, it is possible to associate the vehicles <b>200</b> in the plurality of images with each other.</p><p id="p-0045" num="0044">In the comparison stated above, the comparison unit <b>105</b> may associate the moving objects in the images in different ways according to whether the unit direction vectors n<sub>1 </sub>and n<sub>2 </sub>are similar to each other. Whether or not the unit direction vectors n<sub>1 </sub>and n<sub>2 </sub>are similar may be determined by comparison with a predetermined threshold provided in advance.</p><p id="p-0046" num="0045">Further, the comparison unit <b>105</b> may compare the vehicle <b>200</b> using different references depending on whether the unit direction vectors n<sub>1 </sub>and n<sub>2 </sub>are similar to each other. For example, in a case where the unit direction vectors n<sub>1 </sub>and n<sub>2 </sub>are similar to each other, the comparison unit <b>105</b> may make a comparison using a reference including a shape of the vehicle <b>200</b>, and in a case where the unit direction vectors n<sub>1 </sub>and n<sub>2 </sub>are not similar to each other, the comparison unit <b>105</b> may make a comparison using a reference not including the shape of the vehicle <b>200</b>. In a case where the unit direction vectors n<sub>1 </sub>and n<sub>2 </sub>are not similar to each other, the comparison unit <b>105</b> may make a comparison using, for example, a reference including at least color information of the moving object.</p><p id="p-0047" num="0046">The comparison unit <b>105</b> may compare the feature vectors f<sub>1 </sub>and f<sub>2 </sub>in a case where the unit direction vectors n<sub>1 </sub>and n<sub>2 </sub>are similar to each other, and may not make this comparison in a case where they are not similar to each other. Further, the comparison unit <b>105</b> may compare the feature vectors f<sub>1 </sub>and f<sub>2 </sub>based on the feature amount of an image other than the image of the vehicle <b>200</b>.</p><p id="p-0048" num="0047">The ID output unit <b>106</b> assigns the moving object ID to the extracted moving object image as a function of the comparison result made by the comparison unit <b>105</b>. As a result of the comparison by the comparison unit <b>105</b>, in a case where it is determined that the moving object is the same in the plurality of images, the ID output unit <b>106</b> assigns the same moving object ID to the moving object image.</p><p id="p-0049" num="0048">The learning unit <b>110</b> trains the detection model <b>107</b>, the feature extraction model <b>108</b>, and the comparison model <b>109</b>, respectively.</p><p id="p-0050" num="0049">A process executed by the image processing system <b>100</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>. <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart illustrating a process of the image processing system <b>100</b>.</p><p id="p-0051" num="0050">The image acquisition unit <b>101</b> acquires a captured image from the camera (S<b>101</b>). The captured image includes the time stamp information indicating a date and time when the image is captured.</p><p id="p-0052" num="0051">The object detection unit <b>103</b> uses the detection model <b>107</b> to execute processes of extracting and identifying the input image. The object detection unit <b>103</b> extracts a moving object area including a moving object such as a vehicle, a bicycle, or a pedestrian in the image. In the present embodiment, the object detection unit <b>103</b> extracts an image area of the vehicle <b>200</b> (S<b>103</b>). The object detection unit <b>103</b> may extract the moving object area using general object recognition technology based on a deep learning network.</p><p id="p-0053" num="0052">The object feature extraction unit <b>104</b> calculates a direction vector of the vehicle from camera parameters acquired from the camera information acquisition unit <b>102</b> and the object area extracted in step S<b>103</b> (S<b>105</b>).</p><p id="p-0054" num="0053">The object feature extraction unit <b>104</b> generates an integrated vector obtained by integrating a feature of the object image extracted in step S<b>103</b> and the direction vector calculated in step S<b>105</b>. The object feature extraction unit <b>104</b> takes the integrated vector generated as an input, and generates a feature vector using a feature extractor such as a neural network (S<b>107</b>).</p><p id="p-0055" num="0054">The comparison unit <b>105</b> compares the feature vector of the target object with a feature vector of a past frame (S<b>109</b>). The comparison unit <b>105</b> determines whether the feature vector of the target object is equivalent to the feature vector of the past frame (S<b>111</b>). In a case where it is determined that the feature vectors are the same (YES in S<b>111</b>), the ID output unit <b>106</b> assigns the same ID (S<b>113</b>) and ends the process.</p><p id="p-0056" num="0055">In a case where it is determined that the feature vectors are not the same in step S<b>111</b> (NO in S<b>111</b>), the process proceeds to step S<b>117</b>. The comparison unit <b>105</b> determines whether there is the other feature vector (S<b>117</b>). In a case where it is determined that there is the other feature vector (YES in S<b>117</b>), the process returns to step S<b>109</b>. In a case where it is determined that there is no other feature vector (NO in S<b>117</b>), the process proceeds to step S<b>115</b>. In this case, the ID output unit <b>106</b> assigns a new ID (S<b>115</b>) and ends the process.</p><p id="p-0057" num="0056">The processes of steps S<b>105</b>, S<b>107</b>, and S<b>109</b> mentioned above will be described in detail with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating an outline of the process until the unit direction vector n<sub>1</sub>, the image vector i<sub>1</sub>, the integrated vector g<sub>1</sub>, and the feature vector f<sub>1 </sub>are generated in a case where the image of the vehicle <b>200</b> is captured by the camera CAM<b>1</b>. The unit direction vector n<sub>2</sub>, the image vector i<sub>2</sub>, the integrated vector g<sub>2</sub>, and the feature vector f<sub>2 </sub>in a case where the image of the vehicle <b>200</b> is captured by the camera CAM<b>2</b> can be generated in the same manner.</p><p id="p-0058" num="0057">The image vector i<sub>1 </sub>or i<sub>2 </sub>may be collectively referred to as an &#x201c;image vector i&#x201d; hereinbelow. Similarly, the unit direction vector n<sub>1 </sub>or n<sub>2 </sub>may be collectively referred to as a &#x201c;unit direction vector n&#x201d;, the integrated vector g<sub>1 </sub>or g<sub>2 </sub>may be collectively referred to as an &#x201c;integrated vector g&#x201d;, and the feature vector f<sub>1 </sub>or f<sub>2 </sub>may be collectively referred to as a &#x201c;feature vector f&#x201d;.</p><p id="p-0059" num="0058">In step S<b>105</b> stated above, the unit direction vector n of the vehicle <b>200</b> is calculated from the camera parameters and the object area extracted in step S<b>103</b> by the following procedure. The camera parameters are represented as a projection matrix A having 3 rows and 4 columns. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a gravity center position of the vehicle in the 3D coordinate system is P(X, Y, Z), and a gravity center position in the image coordinate system is p(u, v).</p><p id="p-0060" num="0059">A relationship between P(X, Y, Z) and p(u, v) is represented by the following equation (1).</p><p id="p-0061" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>S</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mtable>       <mtr>        <mtd>         <mi>u</mi>        </mtd>       </mtr>       <mtr>        <mtd>         <mi>v</mi>        </mtd>       </mtr>       <mtr>        <mtd>         <mn>1</mn>        </mtd>       </mtr>      </mtable>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mi>A</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mtable>       <mtr>        <mtd>         <mi>X</mi>        </mtd>       </mtr>       <mtr>        <mtd>         <mi>Y</mi>        </mtd>       </mtr>       <mtr>        <mtd>         <mi>Z</mi>        </mtd>       </mtr>       <mtr>        <mtd>         <mn>1</mn>        </mtd>       </mtr>      </mtable>      <mo>)</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>1</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0062" num="0060">In equation (1), S represents a scale parameter. The projection matrix A is a known matrix obtained by calibrating the camera CAM<b>1</b>. Further, the projection matrix A can be represented by the following equations (2) and (3).</p><p id="p-0063" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>A</mi>     <mo>=</mo>     <mrow>      <mrow>       <mo>(</mo>       <mtable>        <mtr>         <mtd>          <mrow>           <msub>            <mover>             <mi>a</mi>             <mo>&#x2192;</mo>            </mover>            <mn>1</mn>           </msub>           <mo>&#x2062;</mo>           <msub>            <mi>b</mi>            <mn>1</mn>           </msub>          </mrow>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <msub>            <mover>             <mi>a</mi>             <mo>&#x2192;</mo>            </mover>            <mn>2</mn>           </msub>           <mo>&#x2062;</mo>           <msub>            <mi>b</mi>            <mn>2</mn>           </msub>          </mrow>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <msub>            <mover>             <mi>a</mi>             <mo>&#x2192;</mo>            </mover>            <mn>3</mn>           </msub>           <mo>&#x2062;</mo>           <msub>            <mi>b</mi>            <mn>3</mn>           </msub>          </mrow>         </mtd>        </mtr>       </mtable>       <mo>)</mo>      </mrow>      <mo>=</mo>      <mrow>       <mo>(</mo>       <mrow>        <mover>         <mi>A</mi>         <mo>~</mo>        </mover>        <mo>&#x2062;</mo>        <mrow>         <semantics definitionURL="">          <mo>&#x2758;</mo>          <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>         </semantics>         <mi>b</mi>        </mrow>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00002-2" num="00002.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mover>       <mi>a</mi>       <mo>&#x2192;</mo>      </mover>      <mi>i</mi>     </msub>     <mo>=</mo>     <mrow>      <mo>(</mo>      <mrow>       <msub>        <mi>a</mi>        <mrow>         <mi>i</mi>         <mo>&#x2062;</mo>         <mn>1</mn>        </mrow>       </msub>       <mo>,</mo>       <msub>        <mi>a</mi>        <mrow>         <mi>i</mi>         <mo>&#x2062;</mo>         <mn>2</mn>        </mrow>       </msub>       <mo>,</mo>       <msub>        <mi>a</mi>        <mrow>         <mi>i</mi>         <mo>&#x2062;</mo>         <mn>3</mn>        </mrow>       </msub>      </mrow>      <mo>)</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0064" num="0061">The following equation (4) can be obtained from equations (1) to (3).</p><p id="p-0065" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mo>(</mo>      <mtable>       <mtr>        <mtd>         <mi>X</mi>        </mtd>       </mtr>       <mtr>        <mtd>         <mi>Y</mi>        </mtd>       </mtr>       <mtr>        <mtd>         <mi>Z</mi>        </mtd>       </mtr>      </mtable>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mrow>       <mrow>        <mo>-</mo>        <msup>         <mover>          <mi>A</mi>          <mo>~</mo>         </mover>         <mrow>          <mo>-</mo>          <mn>1</mn>         </mrow>        </msup>       </mrow>       <mo>&#x2062;</mo>       <mi>b</mi>      </mrow>      <mo>+</mo>      <mrow>       <mi>s</mi>       <mo>&#x2062;</mo>       <mrow>        <msup>         <mover>          <mi>A</mi>          <mo>~</mo>         </mover>         <mrow>          <mo>-</mo>          <mn>1</mn>         </mrow>        </msup>        <mo>(</mo>        <mtable>         <mtr>          <mtd>           <mi>u</mi>          </mtd>         </mtr>         <mtr>          <mtd>           <mi>v</mi>          </mtd>         </mtr>         <mtr>          <mtd>           <mn>1</mn>          </mtd>         </mtr>        </mtable>        <mo>)</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>4</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0066" num="0062">In step S<b>105</b>, the unit direction vector n of the vehicle <b>200</b> is calculated by the following equation (5).</p><p id="p-0067" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mover>      <mi>n</mi>      <mo>&#x2192;</mo>     </mover>     <mo>=</mo>     <mfrac>      <mover>       <mi>N</mi>       <mo>&#x2192;</mo>      </mover>      <mrow>       <mo>&#xf605;</mo>       <mover>        <mi>N</mi>        <mo>&#x2192;</mo>       </mover>       <mo>&#xf606;</mo>      </mrow>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>5</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0068" num="0063">In equation (5), a direction vector N is represented by the following equation (6).</p><p id="p-0069" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mover>      <mi>N</mi>      <mo>&#x2192;</mo>     </mover>     <mo>=</mo>     <mrow>      <msup>       <mover>        <mi>A</mi>        <mo>~</mo>       </mover>       <mrow>        <mo>-</mo>        <mn>1</mn>       </mrow>      </msup>      <mo>(</mo>      <mtable>       <mtr>        <mtd>         <mi>u</mi>        </mtd>       </mtr>       <mtr>        <mtd>         <mi>v</mi>        </mtd>       </mtr>       <mtr>        <mtd>         <mn>1</mn>        </mtd>       </mtr>      </mtable>      <mo>)</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>6</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0070" num="0064">In step S<b>107</b>, the feature vector f is calculated by the following procedure. A detection frame D of the vehicle <b>200</b> shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> is normalized to a rectangular area of K*K. The detection frame D may be the image area detected by the object detection unit <b>103</b>. A raster scan is performed on the normalized rectangular area. Pixel values of the rectangular area are arranged in order of the raster scan and defined as the image vector i as shown in the following equation (7). The number of dimensions is equal to the number of pixels in the normalized rectangular area.</p><p id="p-0071" num="0000"><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mover>      <mi>i</mi>      <mo>&#x2192;</mo>     </mover>     <mo>=</mo>     <mrow>      <mo>(</mo>      <mtable>       <mtr>        <mtd>         <msub>          <mi>i</mi>          <mn>1</mn>         </msub>        </mtd>       </mtr>       <mtr>        <mtd>         <msub>          <mi>i</mi>          <mn>2</mn>         </msub>        </mtd>       </mtr>       <mtr>        <mtd>         <mo>&#x22ee;</mo>        </mtd>       </mtr>       <mtr>        <mtd>         <msub>          <mi>i</mi>          <msup>           <mi>k</mi>           <mn>2</mn>          </msup>         </msub>        </mtd>       </mtr>      </mtable>      <mo>)</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>7</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0072" num="0065">The unit direction vector n calculated in equation (5) of step S<b>105</b> and the image vector i defined in equation (7) are integrated to generate the integrated vector g represented by the following equation (8).</p><p id="p-0073" num="0000"><maths id="MATH-US-00007" num="00007"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mover>      <mi>g</mi>      <mo>&#x2192;</mo>     </mover>     <mo>=</mo>     <mrow>      <mrow>       <mo>(</mo>       <mtable>        <mtr>         <mtd>          <mover>           <mi>n</mi>           <mo>&#x2192;</mo>          </mover>         </mtd>        </mtr>        <mtr>         <mtd>          <mover>           <mi>i</mi>           <mo>&#x2192;</mo>          </mover>         </mtd>        </mtr>       </mtable>       <mo>)</mo>      </mrow>      <mo>=</mo>      <mrow>       <mo>(</mo>       <mtable>        <mtr>         <mtd>          <msub>           <mi>n</mi>           <mn>1</mn>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>n</mi>           <mn>2</mn>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>n</mi>           <mn>3</mn>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>i</mi>           <mn>1</mn>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>i</mi>           <mn>2</mn>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <mo>&#x22ee;</mo>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>i</mi>           <msup>            <mi>k</mi>            <mn>2</mn>           </msup>          </msub>         </mtd>        </mtr>       </mtable>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>8</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0074" num="0066">The integrated vector g generated by equation (8) is input to a learner of the feature extraction model <b>108</b>, and the feature vector f is obtained as an output. <figref idref="DRAWINGS">FIG. <b>4</b></figref> shows one example of structures of the feature extraction model <b>108</b> and the comparison model <b>109</b>. The integrated vector g<sub>2 </sub>of the target object and the integrated vector g<sub>1 </sub>of the past frame are shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0075" num="0067">Various machine learning models can be used as the learner. For example, a convolutional neural network (CNN) may be used.</p><p id="p-0076" num="0068">In step S<b>109</b>, the feature vector f<sub>2 </sub>of the target object obtained in step S<b>107</b> is compared with the feature vector f<sub>1 </sub>of the past frame. The comparison unit <b>105</b> inputs the feature vectors f<sub>1 </sub>and f<sub>2 </sub>to a learner of the comparison model <b>109</b>, and determines that they are the same if an output is 1 and that they are different if it is 0. Various machine learning models can be used as the learner. For example, a hierarchical neural network may be used.</p><p id="p-0077" num="0069">Various general-purpose CNNs can be used as the feature extraction model <b>108</b>. For example, VGG, EfficientNet or ResNet can be used. Further, a general-purpose fully-connected multi-layer perceptron can be used as the comparison model <b>109</b>. The feature extraction model <b>108</b> and the comparison model <b>109</b> are trained at the same time.</p><p id="p-0078" num="0070">The teacher data is a set of data represented by the equation (9) and a label represented by equation (10).</p><p id="p-0079" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>{<i>{right arrow over (g)}</i><sub>t</sub><i>:l</i><sub>t</sub>}<sub>t=1, . . . ,T</sub>&#x2003;&#x2003;(9)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0080" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>l</i><sub>t</sub><i>=id</i><sub>t</sub>(<i>t=</i>1, . . . ,<i>T</i>)&#x2003;&#x2003;(10)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0081" num="0071">The integrated vector g is already defined in equation (8). &#x201c;id<sub>t</sub>&#x201d; is a moving object ID of the moving object at a time tin equation (10). When inputting the integrated vectors g<sub>1 </sub>and g<sub>2</sub>, it is learned that 1 is input when the moving object IDs are the same and 0 is input when they are different.</p><p id="p-0082" num="0072">As described above, the unit direction vector n from the camera can be added to the image vector i of the moving object image to generate the feature vector f to be used for image comparison in the present embodiment. Consequently, it is possible to associate the moving objects in the images with each other with higher accuracy than when comparing based on the image vector i only.</p><p id="p-0083" num="0073"><figref idref="DRAWINGS">FIGS. <b>5</b> and <b>6</b></figref> are block diagrams respectively illustrating the process of the image processing system <b>100</b> according to the present embodiment. As shown by a white arrow, the vehicle <b>200</b> is traveling from a point A<b>1</b> on the right side of the diagram to a point A<b>2</b> on the left side of the diagram. The image of the vehicle <b>200</b> is captured by the camera CAM<b>1</b> and the camera CAM<b>2</b> at the points A<b>1</b> and A<b>2</b>, respectively.</p><p id="p-0084" num="0074">In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the cameras CAM<b>1</b> and CAM<b>2</b> are arranged so as to capture the vehicle <b>200</b> from the same direction. In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the cameras CAM<b>1</b> and CAM<b>2</b> capture images of the vehicle <b>200</b> from the front. On the other hand, the cameras CAM<b>1</b> and CAM<b>2</b> are arranged such that they face opposite directions in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The camera CAM<b>1</b> captures an image of the vehicle <b>200</b> from the front, and the camera CAM<b>2</b> captures an image of the vehicle <b>200</b> from the rear. Consequently, the vehicle <b>200</b> is shown as having different shapes in the images captured at the points A<b>1</b> and A<b>2</b>.</p><p id="p-0085" num="0075">In the image processing system <b>100</b> according to the present embodiment, the images may be compared with different references in a case shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> and in a case shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, thereby associating the vehicle <b>200</b>. For example, in a case shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, associating is carried out using a reference including a shape of the vehicle <b>200</b>. The reference including the shape of the vehicle <b>200</b> may be a reference using features such as the shape and texture (pixel value of each pixel) of the vehicle <b>200</b>.</p><p id="p-0086" num="0076">In a case shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, associating may be carried out using a reference not including a shape of the vehicle <b>200</b>. The reference that does not include the shape of the vehicle <b>200</b> may be a reference that uses, for example, color characteristics of the vehicle <b>200</b>. For example, it may be a reference using features such as overall color of the vehicle <b>200</b> (e.g. white) and a pattern of the vehicle <b>200</b>. It is possible to accurately associate the vehicles <b>200</b> in the images with each other by making a comparison using such a feature that does not rely on a posture of the camera.</p><p id="p-0087" num="0077">Further, in a case where the image capturing directions are significantly different as in the cameras CAM<b>1</b> and CAM<b>2</b> shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the captured image of the camera CAM<b>2</b> may not be used for image associating. Accordingly, it is possible to compare only images having similar image capturing directions, and thus to associate the image with the weighted features related to the shape of the vehicle <b>200</b>.</p><p id="p-0088" num="0078"><figref idref="DRAWINGS">FIGS. <b>5</b> and <b>6</b></figref> show an example in which the two cameras capture an image of the vehicle <b>200</b> from the same direction (front) and an example in which two cameras capture an image of the vehicle <b>200</b> from opposite directions (front and rear), but the present disclosure is not limited thereto. The image capturing direction may be any direction. For example, an image capturing direction of an image captured from a side surface or an upper surface of the vehicle <b>200</b> may be included. Therefore, the image capturing directions may differ by 90 degrees. Associating may be carried out differently depending on whether the image capturing directions are similar to each other. Whether the image capturing directions are similar to each other may be determined by comparing the unit direction vectors n<sub>1 </sub>and n<sub>2 </sub>when the camera captures an image of the vehicle <b>200</b> with a predetermined threshold.</p><p id="p-0089" num="0079">Further, without being limited thereto, the image processing system <b>100</b> may optionally weight and associate the vehicles <b>200</b> in the images with each other.</p><p id="p-0090" num="0080">As described above, the image acquisition unit <b>101</b> acquires the plurality of images including the vehicle <b>200</b> from the camera in the image processing system <b>100</b> according to the present embodiment. The object feature extraction unit <b>104</b> calculates the image capturing direction information (unit direction vector n) indicating the image capturing direction in which the camera captures the image of the moving object image, and the feature amount (image vector i) of each moving object image extracted from the plurality of images. The object feature extraction unit <b>104</b> calculates the integrated vector g from the unit direction vector n and the image vector i, and then calculates the feature vector f from the integrated vector g. The comparison unit <b>105</b> compares the feature vector fin the images captured at different times. The ID output unit <b>106</b> associates the moving objects in the images with each other by assigning the same moving object ID to the same vehicle <b>200</b> based on the comparison result made by the comparison unit <b>105</b>.</p><p id="p-0091" num="0081">With such a configuration, the image processing system <b>100</b> can associate based on, for example, color information that does not rely on the posture of the camera when it is determined that the unit direction vectors n are different from each other, i.e., the captured images show different shape characteristics of the vehicle <b>200</b>. Therefore, with the image processing system <b>100</b> of the present embodiment, it is possible to learn an optimum ID inference engine in response to an arrangement of the camera. This prevents erroneous recognition of the captured moving object and enables accurate tracking of the moving object.</p><heading id="h-0009" level="1">Second Embodiment</heading><p id="p-0092" num="0082">A second embodiment of the present disclosure will be described hereinbelow. In the first embodiment, the moving object is associate the moving objects in the images with each other based on the image capturing direction information (direction vector) calculated by the object feature extraction unit <b>104</b> (image capturing direction information calculation unit) and the feature amount (image vector) of each captured image. In the present embodiment, associating is carried out by adding information of the other moving object different from the vehicle <b>200</b>, which is extracted in the captured image, instead of the direction vector.</p><p id="p-0093" num="0083"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram respectively illustrating the process of the image processing system <b>100</b> according to the present embodiment. As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, it is assumed that vehicles <b>210</b> and <b>220</b> exist in the vicinity of the vehicle <b>200</b> at the point A<b>1</b>, and that the vehicles <b>210</b>, <b>220</b>, <b>200</b> are traveling in the same direction. It is also assumed that a target for associating in the images is the vehicle <b>200</b>, not the vehicles <b>210</b> and <b>220</b>.</p><p id="p-0094" num="0084">A configuration of the image processing system <b>100</b> according to the present embodiment is the same as that shown in the block diagram of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The configuration of the image processing system <b>100</b> according to the present embodiment will be described referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Descriptions of the same configuration as that of the first embodiment will be omitted.</p><p id="p-0095" num="0085">The image acquisition unit <b>101</b> acquires several captured images including a moving object image from the camera. The captured image includes a moving object other than the vehicle <b>200</b>. As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, it is assumed that the vehicles <b>200</b>, <b>210</b>, and <b>220</b> are included in the same image from among those captured by the camera CAM<b>1</b>. It is assumed that the vehicle <b>200</b> and <b>210</b> are included in the same image, but not the vehicle <b>220</b>, from among those captured by the camera CAM<b>2</b>.</p><p id="p-0096" num="0086">The object detection unit <b>103</b> uses the detection model <b>107</b> to detect object areas of the vehicles <b>200</b> to <b>220</b> from the image acquired by the image acquisition unit <b>101</b>.</p><p id="p-0097" num="0087">The object feature extraction unit <b>104</b> functions as the feature amount calculation unit. The object feature extraction unit <b>104</b> calculates the feature amount of each image for the vehicles <b>200</b> to <b>220</b> extracted by the object detection unit <b>103</b>. Similar to the first embodiment, the object feature extraction unit <b>104</b> generates the image vector i indicating the feature amount of each image.</p><p id="p-0098" num="0088">The object feature extraction unit <b>104</b> may specify a vehicle having a moving speed similar to that of the vehicle <b>200</b> to be associated with, from among the vehicles <b>210</b> and <b>220</b> which are not to be associated with, and calculate the image vector i. The moving speeds of the vehicles <b>200</b> to <b>220</b> may be calculated based on the plurality of images captured and acquired by the camera CAM<b>1</b>. For example, the plurality of images captured by the camera CAM <b>1</b> at predetermined time intervals may be compared and then the moving speed may be calculated based on intervals for capturing images and respective moving distances of the vehicles <b>200</b> to <b>220</b>. Not limited thereto, the speed of each vehicle may be calculated using another algorithm.</p><p id="p-0099" num="0089">In <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the traveling speeds of the vehicles <b>200</b>, <b>210</b>, and <b>220</b> at the point A<b>1</b> are indicated by a speed vector V<sub>i</sub>, a speed vector V<sub>j</sub>, and a speed vector V<sub>K</sub>, respectively. It is assumed that the vehicle <b>200</b> and the vehicle <b>210</b> are traveling at relatively similar speeds, and the vehicle <b>220</b> is traveling at a speed relatively higher than those speeds. The object feature extraction unit <b>104</b> generates the image vector i of the image corresponding to the vehicle <b>210</b> which has a moving speed similar to that of the vehicle <b>200</b>.</p><p id="p-0100" num="0090">The object feature extraction unit <b>104</b> may set a predetermined threshold in advance and determine whether the moving speeds of the vehicles are similar to each other. The object feature extraction unit <b>104</b> may determine that the moving speeds of the vehicles are similar to each other, in a case where a difference between the moving speed of the vehicle <b>200</b> and the moving speed of the other vehicle is equal to or less than a predetermined value. A vehicle having a moving speed similar to that of the vehicle <b>200</b> may be referred to as a &#x201c;nearby vehicle&#x201d; for convenience of description hereinbelow.</p><p id="p-0101" num="0091">There may be a plurality of nearby vehicles. In addition, a priority may be set for the plurality of nearby vehicles and used for associating the vehicles <b>200</b> in the images with each other. For example, the priority may increase as the moving speed of nearby vehicles becomes more similar to the moving speed of the vehicle <b>200</b>, and the associating may be weighted based on the priority.</p><p id="p-0102" num="0092">The description will be continued returning to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The comparison unit <b>105</b> and the ID output unit <b>106</b> have functions of the associating unit. The associating unit associates the vehicles <b>200</b> in the images with each other based on the feature amount of the image including the vehicle <b>200</b> and the feature amount of the images other than the vehicle <b>200</b>.</p><p id="p-0103" num="0093">In particular, the comparison unit <b>105</b> compares the plurality of images by comparing the feature vectors of the vehicle <b>200</b> generated by the object feature extraction unit <b>104</b>. The comparison unit <b>105</b> may acquire the comparison result by using the comparison model <b>109</b>. The ID output unit <b>106</b> assigns the moving object ID according to the comparison result made by the comparison unit <b>105</b>.</p><p id="p-0104" num="0094">A process executed by the image processing system <b>100</b> of the present embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>. <figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart illustrating the process executed by the image processing system <b>100</b> according to the present embodiment.</p><p id="p-0105" num="0095">The image acquisition unit <b>101</b> acquires a captured image from the camera (S<b>201</b>). The captured image includes the time stamp information indicating a date and time when the image is captured. The object detection unit <b>103</b> uses the detection model <b>107</b> to extract and identify the input image, and extracts a moving object area including a moving object such as a vehicle, a bicycle, or a pedestrian in the image (S<b>203</b>). The object detection unit <b>103</b> may extract the moving object area using general object recognition technology based on a deep learning network.</p><p id="p-0106" num="0096">The object feature extraction unit <b>104</b> calculates an image vector of the nearby vehicle in the captured image (S<b>205</b>). A process for identifying the nearby vehicle will be described hereinbelow with reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref>. <figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating the process for identifying the nearby vehicle.</p><p id="p-0107" num="0097">The object feature extraction unit <b>104</b> adds a vehicle having a moving speed similar to that of the vehicle <b>200</b> (hereinafter referred to as a &#x201c;node&#x201d;) to the same grouping circle <b>300</b>, and deletes a node having a moving speed not similar to that of the vehicle <b>200</b> from the grouping circle <b>300</b>. The object feature extraction unit <b>104</b> adds or deletes nodes by the following process.</p><p id="p-0108" num="0098">The object feature extraction unit <b>104</b> determines the vehicle <b>200</b> to associate the vehicles <b>200</b> in the images with each other as a root node (S<b>301</b>). The object feature extraction unit <b>104</b> determines a candidate node to be grouped with the vehicle <b>200</b> (S<b>303</b>). For example, the object feature extraction unit <b>104</b> sets a vehicle within a predetermined distance from a center of gravity of an orthorectified image of the target vehicle as a candidate node. In the example shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the vehicles <b>210</b> and <b>220</b> are candidate nodes. Not limited thereto, the candidate node may be determined using any condition. For example, all moving objects in the captured image may be used as candidate nodes.</p><p id="p-0109" num="0099">From among the candidate nodes, the object feature extraction unit <b>104</b> determines, as a node in the same grouping circle <b>300</b> as the vehicle <b>200</b>, a vehicle in which an absolute value of the difference between its speed vector and the speed vector of the vehicle <b>200</b> is equal or less than a predetermined value (S<b>305</b>). Edge strength is a reciprocal of the absolute value of the difference. The object feature extraction unit <b>104</b> determines the vehicle <b>210</b> as the node of the same grouping circle <b>300</b> as the vehicle <b>200</b>.</p><p id="p-0110" num="0100">The object feature extraction unit <b>104</b> deletes any candidate nodes not satisfying the condition stated in step S<b>305</b> (S<b>307</b>). The vehicle <b>220</b> is deleted in the present embodiment.</p><p id="p-0111" num="0101">Consequently, the object feature extraction unit <b>104</b> can identify a nearby vehicle that belongs to the same group as the vehicle <b>200</b>. In the present embodiment, the object feature extraction unit <b>104</b> identifies the vehicle <b>210</b> as a vehicle that is nearby the vehicle <b>200</b>.</p><p id="p-0112" num="0102">The description will be continued returning to <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The object feature extraction unit <b>104</b> generates the feature vector using the image vectors of the vehicles <b>200</b> and <b>210</b> (S<b>207</b>). Since a method of generating each feature vector is the same as that of the first embodiment, detailed descriptions thereof will be omitted.</p><p id="p-0113" num="0103"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram corresponding to <figref idref="DRAWINGS">FIG. <b>4</b></figref> of the first embodiment, and shows one example of the structure of the feature extraction model <b>108</b> and the comparison model <b>109</b>. The object feature extraction unit <b>104</b> generates an image vector i of the vehicle <b>200</b> and an image vector j of the vehicle <b>210</b>, respectively, in the same manner as in equation (7) in the first embodiment.</p><p id="p-0114" num="0104">The object feature extraction unit <b>104</b> integrates the image vectors i and j to generate the integrated vector g in the same manner as in equation (8) of the first embodiment. The integrated vector g is represented by the following equation (11).</p><p id="p-0115" num="0000"><maths id="MATH-US-00008" num="00008"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mover>      <mi>g</mi>      <mo>&#x2192;</mo>     </mover>     <mo>=</mo>     <mrow>      <mrow>       <mo>(</mo>       <mtable>        <mtr>         <mtd>          <mover>           <mi>i</mi>           <mo>&#x2192;</mo>          </mover>         </mtd>        </mtr>        <mtr>         <mtd>          <mover>           <mi>j</mi>           <mo>&#x2192;</mo>          </mover>         </mtd>        </mtr>       </mtable>       <mo>)</mo>      </mrow>      <mo>=</mo>      <mrow>       <mo>(</mo>       <mtable>        <mtr>         <mtd>          <msub>           <mi>i</mi>           <mn>1</mn>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>i</mi>           <mn>2</mn>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <mo>&#x22ee;</mo>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>i</mi>           <msup>            <mi>k</mi>            <mn>2</mn>           </msup>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>j</mi>           <mn>1</mn>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>j</mi>           <mn>2</mn>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <mo>&#x22ee;</mo>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>j</mi>           <msup>            <mi>k</mi>            <mn>2</mn>           </msup>          </msub>         </mtd>        </mtr>       </mtable>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>11</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0116" num="0105">The object feature extraction unit <b>104</b> generates the feature vector f from the integrated vector g in the same manner as in the first embodiment. The object feature extraction unit <b>104</b> generates the feature vector f<sub>2 </sub>of the target object and the feature vector f<sub>1 </sub>of the past frame, respectively.</p><p id="p-0117" num="0106">The comparison unit <b>105</b> compares the feature vectors f<sub>1 </sub>and f<sub>2 </sub>obtained in step S<b>207</b> (S<b>209</b>). When inputting the feature vectors f<sub>1 </sub>and f<sub>2</sub>, it is learned that 1 is input when the moving object IDs are the same and 0 is input when they are different.</p><p id="p-0118" num="0107">Since the subsequent processes are the same as those of the first embodiment, descriptions thereof will be omitted. The processes of steps S<b>211</b> to S<b>217</b> correspond to steps S<b>111</b> to S<b>117</b> described with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, respectively.</p><p id="p-0119" num="0108">In a case where there is a plurality of nearby vehicles, the feature vectors may be generated for all the nearby vehicles, or alternatively, the feature vectors may be generated for a limited number of nearby vehicles used for associating. For example, a priority order used for associating may be set depending on the moving speed of the vehicle <b>200</b>, a positional relationship with the vehicle <b>200</b>, and the like.</p><p id="p-0120" num="0109">As stated above, in the image processing system <b>100</b> of the present embodiment, the feature vector is generated by adding image information of the vehicle <b>210</b> existing in the vicinity of the vehicle <b>200</b> to associate the vehicles <b>200</b> in the images with each other. Further, the nearby vehicle is specified by using the distance from the vehicle <b>200</b>, and the difference between the moving speeds of the vehicle <b>200</b> and the vehicle <b>210</b>.</p><p id="p-0121" num="0110">Consequently, it is possible to associate the vehicles <b>200</b> in the images with each other more accurately by adding the image information of the moving object in the vicinity of the vehicle <b>200</b>, which can be considered as belonging to the same group as the vehicle <b>200</b>, to generate the feature, as compared to a case of using image information of the vehicle <b>200</b> only. Even in a case where there is the plurality of vehicles having shape characteristics similar to the vehicle <b>200</b>, it is possible to appropriately associate the vehicle <b>200</b> with the vehicles <b>200</b> in the plurality of images.</p><p id="p-0122" num="0111">The present embodiment may be implemented by combining the configurations described in the first embodiment. The vehicle <b>200</b> may be associate the vehicles in the images with each other by using both the direction vector described in the first embodiment and the image vector of the nearby vehicle described in the present embodiment.</p><heading id="h-0010" level="1">Configuration Example of Hardware</heading><p id="p-0123" num="0112">Each functional component of the image processing system <b>100</b> may be implemented by hardware that implements each functional component (e.g. hard-wired electronic circuit), or by a combination of hardware and software (e.g. a combination of an electronic circuit and a program that controls the circuit). Hereinafter, a case where each functional component of the image processing system <b>100</b> is implemented by a combination of hardware and software will be further described.</p><p id="p-0124" num="0113"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a block diagram illustrating a hardware configuration of a computer <b>900</b> that implements the image processing system <b>100</b>. The computer <b>900</b> may be a dedicated computer designed to implement the image processing system <b>100</b>, or may be a general-purpose computer. The computer <b>900</b> may be a portable computer such as a smartphone or a tablet terminal.</p><p id="p-0125" num="0114">For example, each function of the image processing system <b>100</b> is implemented on the computer <b>900</b> by installing a predetermined application on the computer <b>900</b>. The application is composed of a program for implementing the functional components of the image processing system <b>100</b>.</p><p id="p-0126" num="0115">The computer <b>900</b> has a bus <b>902</b>, a processor <b>904</b>, a memory <b>906</b>, a storage device <b>908</b>, an input/output interface <b>910</b>, and a network interface <b>912</b>. The bus <b>902</b> is a data transmission line for the processor <b>904</b>, the memory <b>906</b>, the storage device <b>908</b>, the input/output interface <b>910</b>, and the network interface <b>912</b> to transmit and receive data to and from each other. However, a method of connecting the processor <b>904</b> and other components to each other is not limited to the bus connection.</p><p id="p-0127" num="0116">The processor <b>904</b> may be one of various processors such as a central processing unit (CPU), a graphics processing unit (GPU), or a field-programmable gate array (FPGA). The memory <b>906</b> is a main storage device implemented by using, for example, a random access memory (RAM). The storage device <b>908</b> is an auxiliary storage device implemented by using, for example, a hard disk, a solid state drive (SSD), a memory card, or a read only memory (ROM).</p><p id="p-0128" num="0117">The input/output interface <b>910</b> is an interface for connecting the computer <b>900</b> and the input/output device. For example, an input device such as a keyboard and an output device such as a display device are connected to the input/output interface <b>910</b>. For example, the camera described in the embodiments may be connected to the input/output interface <b>910</b>.</p><p id="p-0129" num="0118">The network interface <b>912</b> is an interface for connecting the computer <b>900</b> to a network. This network may be a LAN (Local Area Network) or a WAN (Wide Area Network).</p><p id="p-0130" num="0119">The storage device <b>908</b> stores a program (program that implements the applications stated above) to implement each functional component of the image processing system <b>100</b>. The processor <b>904</b> reads this program into the memory <b>906</b> and executes such that each functional component of the image processing system <b>100</b> is implemented.</p><p id="p-0131" num="0120">Each of the processors executes one or more programs containing instructions for causing the computer to perform the algorithm. The program includes instructions (or software code) for causing the computer to perform at least one function described in the embodiments when loaded into the computer. The program may be stored on a computer-readable non-transitory medium or a tangible storage medium. Non-limiting examples of computer-readable or tangible storage media include random-access memory (RAM), read-only memory (ROM), flash memory, solid-state drive (SSD) or other memories, CD-ROM, digital versatile disc (DVD), Blu-ray&#xae; disc or other optical disc storages, magnetic cassette, magnetic tape, magnetic disk storage or other magnetic storage devices. The program may be transmitted on a computer-readable transitory medium or a communication medium. Non-limiting examples of transitory or communication media include electrical, optical, acoustic, or other forms of propagating signals.</p><p id="p-0132" num="0121">The present disclosure is not limited to the embodiments stated above, and may be altered as appropriate without departing from the spirit of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005162A1-20230105-M00001.NB"><img id="EMI-M00001" he="12.36mm" wi="76.20mm" file="US20230005162A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002 MATH-US-00002-2" nb-file="US20230005162A1-20230105-M00002.NB"><img id="EMI-M00002" he="15.16mm" wi="76.20mm" file="US20230005162A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230005162A1-20230105-M00003.NB"><img id="EMI-M00003" he="8.81mm" wi="76.20mm" file="US20230005162A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230005162A1-20230105-M00004.NB"><img id="EMI-M00004" he="8.13mm" wi="76.20mm" file="US20230005162A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230005162A1-20230105-M00005.NB"><img id="EMI-M00005" he="8.81mm" wi="76.20mm" file="US20230005162A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006" nb-file="US20230005162A1-20230105-M00006.NB"><img id="EMI-M00006" he="12.70mm" wi="76.20mm" file="US20230005162A1-20230105-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00007" nb-file="US20230005162A1-20230105-M00007.NB"><img id="EMI-M00007" he="22.61mm" wi="76.20mm" file="US20230005162A1-20230105-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00008" nb-file="US20230005162A1-20230105-M00008.NB"><img id="EMI-M00008" he="26.42mm" wi="76.20mm" file="US20230005162A1-20230105-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An image processing system comprising:<claim-text>an image acquisition unit configured to acquire a plurality of images including a moving object image;</claim-text><claim-text>an image capturing direction information calculation unit configured to calculate image capturing direction information indicating an image capturing direction in which an imaging device captures an image of a moving object at a time when the images is captured;</claim-text><claim-text>a feature amount calculation unit configured to calculate a feature amount of the moving object image extracted from the plurality of images; and</claim-text><claim-text>an associating unit configured to associate the moving objects in the images with each other based on the image capturing direction information and the feature amount.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The image processing system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the image capturing direction information is a direction vector from the imaging device toward the moving object; and</claim-text><claim-text>the associating unit is configured to associate the moving objects in the images with each other in different ways according to whether the direction vectors are similar in the images.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The image processing system according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the associating unit is configured to associate the moving objects in the images with each other using different references whether the direction vectors are similar in the images.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The image processing system according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the associating unit is configured to:<claim-text>associate the moving objects in the images with each other using a reference including a shape of the moving object in a case where the direction vectors are similar in the images; and</claim-text><claim-text>associate the moving objects in the images with each other using a reference not including the shape of the moving object in a case where the direction vectors are not similar in the images.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The image processing system according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the associating unit is configured to associate the moving objects in the images with each other using a reference at least including color information of the moving object in a case where the direction vectors are not similar in the images.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The image processing system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the feature amount calculation unit is configured to further calculate a feature amount of another moving object image extracted from the same image as an image including the moving object image; and</claim-text><claim-text>the associating unit is configured to associate the moving objects in the images with each other further based on the feature amount of the other moving object image.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. An image processing system comprising:<claim-text>an image acquisition unit configured to acquire a plurality of images including a moving object image;</claim-text><claim-text>a feature amount calculation unit configured to calculate a feature amount of the moving object image extracted from the images, and a feature amount of another moving object image extracted from the same image as an image including the moving object image; and</claim-text><claim-text>an associating unit configured to associate the moving objects in the images with each other, based on the feature amount of the moving object image and the feature amount of the other moving object image.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The image processing system according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the feature amount calculation unit is configured to calculate the feature amount of the other moving object image in a case where a difference between a moving speed of the moving object and a moving speed of another moving object is equal to or less than a predetermined value.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. An image processing method comprising:<claim-text>acquiring a plurality of images including a moving object image;</claim-text><claim-text>calculating image capturing direction information indicating an image capturing direction in which an imaging device captures an image of a moving object at a time when the images is captured;</claim-text><claim-text>calculating a feature amount of the moving object image extracted from the images; and</claim-text><claim-text>associating the moving objects in the images with each other based on the image capturing direction information and the feature amount.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. An image processing method comprising:<claim-text>acquiring a plurality of images including a moving object image;</claim-text><claim-text>calculating a feature amount of the moving object image extracted from the images, and a feature amount of another moving object image extracted from the same image as an image including the moving object image; and</claim-text><claim-text>associating the moving objects in the images with each other, based on the feature amount of the moving object image and the feature amount of the other moving object image.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A non-transitory storage medium storing an image processing program causing a computer to execute:<claim-text>acquiring a plurality of images including a moving object image;</claim-text><claim-text>calculating image capturing direction information indicating an image capturing direction in which an imaging device captures an image of a moving object at a time when the images is captured;</claim-text><claim-text>calculating a feature amount of the moving object image extracted from the images; and</claim-text><claim-text>associating the moving objects in the images with each other based on the image capturing direction information and the feature amount.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A non-transitory storage medium storing an image processing program causing a computer to execute:<claim-text>acquiring a plurality of images including a moving object image;</claim-text><claim-text>calculating a feature amount of the moving object image extracted from the images, and a feature amount of another moving object image extracted from the same image as an image including the moving object image; and</claim-text><claim-text>associating the moving objects in the images with each other, based on the feature amount of the moving object image and the feature amount of the other moving object image.</claim-text></claim-text></claim></claims></us-patent-application>