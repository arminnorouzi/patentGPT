<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004805A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004805</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17940773</doc-number><date>20220908</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>901</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>58</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>44</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>9024</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>58</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>454</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEM AND METHOD FOR PROVIDING OBJECT-LEVEL DRIVER ATTENTION REASONING WITH A GRAPH CONVOLUTION NETWORK</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16745935</doc-number><date>20200117</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11507830</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17940773</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62899950</doc-number><date>20190913</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Honda Motor Co., Ltd.</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Tawari</last-name><first-name>Ashish</first-name><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Zhang</last-name><first-name>Zehua</first-name><address><city>Bloomington</city><state>IN</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A system and method for providing object-level driver attention reasoning with a graph convolution network that include receiving image data associated with a plurality of image clips of a surrounding environment of a vehicle and determining anchor objectness scores and anchor importance scores associated with relevant objects included within the plurality of image clips. The system and method also include analyzing the anchor objectness scores and anchor importance scores associated with relevant objects and determining top relevant objects with respect to an operation of the vehicle. The system and method further include passing object node features and edges of an interaction graph through the graph convolution network to update features of each object node through interaction with other object nodes and determining importance scores for the top relevant objects.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="114.30mm" wi="158.75mm" file="US20230004805A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="201.68mm" wi="145.80mm" orientation="landscape" file="US20230004805A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="206.25mm" wi="124.04mm" orientation="landscape" file="US20230004805A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="232.58mm" wi="142.66mm" orientation="landscape" file="US20230004805A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="230.21mm" wi="132.16mm" orientation="landscape" file="US20230004805A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="117.86mm" wi="113.88mm" file="US20230004805A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="172.21mm" wi="114.05mm" file="US20230004805A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is a continuation of, and claims priority to U.S. application Ser. No. 16/745,935 filed on Jan. 17, 2020 which claims priority to U.S. Provisional Application Ser. No. 62/899,950 filed on Sep. 13, 2019, both of which are expressly incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Automated driving in highly interactive scenarios is challenging as it involves different levels of scene analysis, situation understanding, intention prediction, decision making, and planning. There have been advances in modeling that pertains to determining object detection and object level reasoning. Much work has focused on pixel-level saliency based on human eye gaze. However, human eye gaze may sometimes fall on regions with no relationship to a driving task that may skew determinations with respect to scene analysis, situation understanding, intention prediction, decision making, and planning. Recent work has also focused on the detection of object-level reasoning. However, there is a requirement of an input of goal information. Additionally, object detection and tracking for each frame of a clip is required. This requirement may utilize higher computational processing power and may be time-consuming.</p><heading id="h-0003" level="1">BRIEF DESCRIPTION</heading><p id="p-0004" num="0003">According to one aspect, a computer-implemented method for providing object-level attention reasoning. The computer-implemented method includes receiving images associated with a plurality of image clips of a surrounding environment of a vehicle. The computer-implemented method also includes analyzing the images and determining a plurality of relevant objects that are included within the surrounding environment that influence an operation of the vehicle. The computer-implemented method additionally includes inputting data associated with the plurality of relevant objects to a graph convolution network to determine importance scores for each of the plurality of relevant objects. The computer-implemented method further includes determining a plurality of top relevant objects based on the importance scores that are to be accounted for to autonomously control the vehicle to be operated to complete attention reasoning within the surrounding environment of a vehicle.</p><p id="p-0005" num="0004">According to another aspect, system for providing object-level attention reasoning that includes a memory that stores instructions that are executed by a processor. The instructions include receiving images associated with a plurality of image clips of a surrounding environment of a vehicle. The instructions also include analyzing the images and determining a plurality of relevant objects that are included within the surrounding environment that influence an operation of the vehicle. The instructions additionally include inputting data associated with the plurality of relevant objects to a graph convolution network to determine importance scores for each of the plurality of relevant objects. The instructions further include determining a plurality of top relevant objects based on the importance scores that are to be accounted for to autonomously control the vehicle to be operated to complete attention reasoning within the surrounding environment of a vehicle.</p><p id="p-0006" num="0005">According to yet another aspect, a non-transitory computer readable storage medium that instructions that are executed by a computer, which includes a processor to perform a method. The method includes receiving images associated with a plurality of image clips of a surrounding environment of a vehicle. The method also includes analyzing the images and determining a plurality of relevant objects that are included within the surrounding environment that influence an operation of the vehicle. The method additionally includes inputting data associated with the plurality of relevant objects to a graph convolution network to determine importance scores for each of the plurality of relevant objects. The method further includes determining a plurality of top relevant objects based on the importance scores that are to be accounted for to autonomously control the vehicle to be operated to complete attention reasoning within the surrounding environment of a vehicle.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0007" num="0006">The novel features believed to be characteristic of the disclosure are set forth in the appended claims. In the descriptions that follow, like parts are marked throughout the specification and drawings with the same numerals, respectively. The drawing figures are not necessarily drawn to scale and certain figures can be shown in exaggerated or generalized form in the interest of clarity and conciseness. The disclosure itself, however, as well as a preferred mode of use, further objects and advances thereof, will be best understood by reference to the following detailed description of illustrative embodiments when read in conjunction with the accompanying drawings, wherein:</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic view of an exemplary operating environment for implementing systems and methods for providing object-level driver attention reasoning with a graph convolution network according to an exemplary embodiment of the present disclosure;</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a general schematic overview for providing object-level driver attention reasoning with the graph convolution network by an attention reasoning application according to an exemplary embodiment of the present disclosure;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a process flow diagram of a method for determining and fusing anchor scores associated with relevant objects that are located within a surrounding environment of a vehicle according to an exemplary embodiment of the present disclosure;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a process flow diagram of a method of utilizing non-maximal suppression for object detection of the relevant objects and determining top relevant object proposals according to an exemplary embodiment of the present disclosure;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an illustrative example of a plurality of bounding boxes and associated importance scores that may be computed for relevant objects located within the surrounding environment of the vehicle according to an exemplary embodiment of the present disclosure;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a process flow diagram of a method for passing the K node features and the edges of the interaction graph through the graph convolution network and determining importance scores for objects located within the surrounding environment of the vehicle according to an exemplary embodiment of the present disclosure; and</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a process flow diagram of a method for providing object-level driver attention reasoning with a graph convolution network according to an exemplary embodiment of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0015" num="0014">The following includes definitions of selected terms employed herein. The definitions include various examples and/or forms of components that fall within the scope of a term and that may be used for implementation. The examples are not intended to be limiting.</p><p id="p-0016" num="0015">A &#x201c;bus&#x201d;, as used herein, refers to an interconnected architecture that is operably connected to other computer components inside a computer or between computers. The bus may transfer data between the computer components. The bus may be a memory bus, a memory controller, a peripheral bus, an external bus, a crossbar switch, and/or a local bus, among others. The bus can also be a vehicle bus that interconnects components inside a vehicle using protocols such as Media Oriented Systems Transport (MOST), Controller Area network (CAN), Local Interconnect Network (LIN), among others.</p><p id="p-0017" num="0016">&#x201c;Computer communication&#x201d;, as used herein, refers to a communication between two or more computing devices (e.g., computer, personal digital assistant, cellular telephone, network device) and can be, for example, a network transfer, a file transfer, an applet transfer, an email, a hypertext transfer protocol (HTTP) transfer, and so on. A computer communication can occur across, for example, a wireless system (e.g., IEEE 802.11), an Ethernet system (e.g., IEEE 802.3), a token ring system (e.g., IEEE 802.5), a local area network (LAN), a wide area network (WAN), a point-to-point system, a circuit switching system, a packet switching system, among others.</p><p id="p-0018" num="0017">A &#x201c;disk&#x201d;, as used herein can be, for example, a magnetic disk drive, a solid state disk drive, a floppy disk drive, a tape drive, a Zip drive, a flash memory card, and/or a memory stick. Furthermore, the disk can be a CD-ROM (compact disk ROM), a CD recordable drive (CD-R drive), a CD rewritable drive (CD-RW drive), and/or a digital video ROM drive (DVD ROM). The disk can store an operating system that controls or allocates resources of a computing device.</p><p id="p-0019" num="0018">A &#x201c;memory&#x201d;, as used herein can include volatile memory and/or non-volatile memory. Non-volatile memory can include, for example, ROM (read only memory), PROM (programmable read only memory), EPROM (erasable PROM), and EEPROM (electrically erasable PROM). Volatile memory can include, for example, RAM (random access memory), synchronous RAM (SRAM), dynamic RAM (DRAM), synchronous DRAM (SDRAM), double data rate SDRAM (DDR SDRAM), and direct RAM bus RAM (DRRAM). The memory can store an operating system that controls or allocates resources of a computing device.</p><p id="p-0020" num="0019">A &#x201c;module&#x201d;, as used herein, includes, but is not limited to, non-transitory computer readable medium that stores instructions, instructions in execution on a machine, hardware, firmware, software in execution on a machine, and/or combinations of each to perform a function(s) or an action(s), and/or to cause a function or action from another module, method, and/or system. A module may also include logic, a software controlled microprocessor, a discrete logic circuit, an analog circuit, a digital circuit, a programmed logic device, a memory device containing executing instructions, logic gates, a combination of gates, and/or other circuit components. Multiple modules may be combined into one module and single modules may be distributed among multiple modules.</p><p id="p-0021" num="0020">An &#x201c;operable connection&#x201d;, or a connection by which entities are &#x201c;operably connected&#x201d;, is one in which signals, physical communications, and/or logical communications may be sent and/or received. An operable connection may include a wireless interface, a physical interface, a data interface and/or an electrical interface.</p><p id="p-0022" num="0021">A &#x201c;processor&#x201d;, as used herein, processes signals and performs general computing and arithmetic functions. Signals processed by the processor may include digital signals, data signals, computer instructions, processor instructions, messages, a bit, a bit stream, or other means that may be received, transmitted and/or detected. Generally, the processor may be a variety of various processors including multiple single and multicore processors and co-processors and other multiple single and multicore processor and co-processor architectures. The processor may include various modules to execute various functions.</p><p id="p-0023" num="0022">A &#x201c;vehicle&#x201d;, as used herein, refers to any moving vehicle that is capable of carrying one or more human occupants and is powered by any form of energy. The term &#x201c;vehicle&#x201d; includes, but is not limited to: cars, trucks, vans, minivans, SUVs, motorcycles, scooters, boats, go-karts, amusement ride cars, rail transport, personal watercraft, and aircraft. In some cases, a motor vehicle includes one or more engines. Further, the term &#x201c;vehicle&#x201d; may refer to an electric vehicle (EV) that is capable of carrying one or more human occupants and is powered entirely or partially by one or more electric motors powered by an electric battery. The EV may include battery electric vehicles (BEV) and plug-in hybrid electric vehicles (PHEV). The term &#x201c;vehicle&#x201d; may also refer to an autonomous vehicle and/or self-driving vehicle powered by any form of energy. The autonomous vehicle may or may not carry one or more human occupants. Further, the term &#x201c;vehicle&#x201d; may include vehicles that are automated or non-automated with pre-determined paths or free-moving vehicles.</p><p id="p-0024" num="0023">A &#x201c;value&#x201d; and &#x201c;level&#x201d;, as used herein may include, but is not limited to, a numerical or other kind of value or level such as a percentage, a non-numerical value, a discrete state, a discrete value, a continuous value, among others. The term &#x201c;value of X&#x201d; or &#x201c;level of X&#x201d; as used throughout this detailed description and in the claims refers to any numerical or other kind of value for distinguishing between two or more states of X. For example, in some cases, the value or level of X may be given as a percentage between 0% and 100%. In other cases, the value or level of X could be a value in the range between 1 and 10. In still other cases, the value or level of X may not be a numerical value, but could be associated with a given discrete state, such as &#x201c;not X&#x201d;, &#x201c;slightly x&#x201d;, &#x201c;x&#x201d;, &#x201c;very x&#x201d; and &#x201c;extremely x&#x201d;.</p><heading id="h-0006" level="2">I. System Overview</heading><p id="p-0025" num="0024">Referring now to the drawings, wherein the showings are for purposes of illustrating one or more exemplary embodiments and not for purposes of limiting same, <figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic view of an exemplary operating environment <b>100</b> for implementing systems and methods for providing object-level driver attention reasoning with a graph convolution network according to an exemplary embodiment of the present disclosure. The components of the environment <b>100</b>, as well as the components of other systems, hardware architectures, and software architectures discussed herein, may be combined, omitted, or organized into different architectures for various embodiments.</p><p id="p-0026" num="0025">Generally, the environment <b>100</b> includes an ego vehicle (vehicle) <b>102</b> with an electronic control unit (ECU) <b>104</b> that executes one or more applications, operating systems, vehicle system and subsystem user interfaces, among others. The ECU <b>104</b> may also execute an object-level driver attention reasoning application (attention reasoning application) <b>106</b> that may be configured to provide object-level driver attention reasoning with a graph convolutional network (GCN) <b>108</b>. The attention reasoning application <b>106</b> may be configured to allow estimation of important objects that are located within a surrounding environment (e.g., predetermined surrounding distance) of the vehicle <b>102</b> are for making real-time decisions that may explain and/or mimic driver attention reasoning with respect to operation of the vehicle <b>102</b> within the surrounding environment of the vehicle <b>102</b>.</p><p id="p-0027" num="0026">As discussed in more detail below, the attention reasoning application <b>106</b> may be configured to receive continuous video of a surrounding environment of the vehicle <b>102</b>. The video may be divided into red green blue (RGB) image clips and a target frame may be selected. In one configuration, the attention reasoning application <b>106</b> may be configured to complete an objectness determination and an object importance determination to with respect to the determination of one or more relevant objects that may have a potential influence with respect to the driving/operation of the vehicle <b>102</b>. The one or more relevant objects may include, but may not be limited to, automobiles, pedestrians, bicyclists, traffic lights, road signs, lamp posts, trains, buses, etc. that may have an influence with respect to the driving/operation of the vehicle <b>102</b>.</p><p id="p-0028" num="0027">Upon detecting the one or more relevant objects, the attention reasoning application <b>106</b> may analyze data associated with the one or more relevant objects to determine top-K relevant object proposals (top relevant objects) of the top k number (e.g., top 5) of relevant objects that may be most likely to influence the path of the vehicle <b>102</b> (e.g., interfere with the path, influence a change in trajectory of the vehicle <b>102</b>) during the driving/operation of the vehicle <b>102</b>. The application <b>106</b> may determine object node features and edges of an interaction graph that is representative of the surrounding environment of the vehicle <b>102</b> based on the top relevant objects.</p><p id="p-0029" num="0028">As discussed in more detail below, such data may be inputted to a graph convolutional neural network (GCN) <b>108</b> to update features of each object node which may be updated by interacting with other object nodes through graph convolution to determine importance scores for each top relevant object located within the surrounding environment of the vehicle <b>102</b>. The importance scores may be applied with respect to object level-driving attention reasoning. In one embodiment, one or more of the top rated relevant objects may be further classified as important objects based on a comparison between the respective importance scores and a predetermined importance scoring threshold value.</p><p id="p-0030" num="0029">Stated differently, based on the assigned object importance scores, one or more top relevant objects may be classified as important objects based on a respective degree of influence with respect to driving control decisions when providing autonomous driving capabilities and driving assistance systems. For example, the one or more top relevant objects may be assigned with respective importance scores and the one or more top relevant objects may be classified as important objects based on a comparison of the respective importance scores and the predetermined importance scoring threshold value that may pertain to a potential overlap and/or a distance with respect to a current and/or potential path of the vehicle <b>102</b>.</p><p id="p-0031" num="0030">In one or more embodiments, the attention reasoning application <b>106</b> may be configured to analyze the assigned object importance scores associated with one or more important objects that are determined to be located within the surrounding environment of the vehicle <b>102</b>. In particular, the attention reasoning application <b>106</b> may be configured to control the vehicle <b>102</b> to preemptively adapt to the positions, locations, and/or potential overlap between one or more important objects that may be based on their respective importance scores. Accordingly, one or more important objects with higher importance scores may have a respective higher degree of influence with respect to driving control decisions that may be utilized to provide autonomous driving capabilities and to provide driving assistance systems. Therefore, the attention reasoning application <b>106</b> may effectively model the interactions among objects located within the surrounding environment of the vehicle <b>102</b> with a computed interaction graph and may utilize the GCN <b>108</b> to determine an importance of particular important objects to be accounted for when providing autonomous driving capabilities and driving assistance systems with attention reasoning.</p><p id="p-0032" num="0031">With continued reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in addition to the ECU <b>104</b>, the vehicle <b>102</b> may also include a plurality of additional components, for example, vehicle sensors <b>110</b>, a memory <b>112</b>, a vehicle autonomous controller <b>114</b>, a vehicle camera system <b>116</b>, one or more cameras <b>118</b>, and a vehicle systems/control units <b>120</b>. In one or more embodiments, the ECU <b>104</b> may include a microprocessor, one or more application-specific integrated circuit(s) (ASIC), or other similar devices. The ECU <b>104</b> may also include internal processing memory, an interface circuit, and bus lines for transferring data, sending commands, and communicating with the plurality of components of the vehicle <b>102</b>.</p><p id="p-0033" num="0032">The ECU <b>104</b> may also include a communication device (not shown) for sending data internally within (e.g., between one or more components) the vehicle <b>102</b> and communicating with externally hosted computing systems (e.g., external to the vehicle <b>102</b>). Generally, the ECU <b>104</b> may communicate with the memory <b>112</b> to execute the one or more applications, operating systems, vehicle system and subsystem user interfaces, and the like that are stored within the memory <b>112</b>.</p><p id="p-0034" num="0033">In one embodiment, the ECU <b>104</b> may communicate with the vehicle autonomous controller <b>114</b> to execute autonomous driving commands to operate the vehicle <b>102</b> to be fully autonomously driven or semi-autonomously driven in a particular manner that may be based on the level of importance that may be associated with one or more important objects. In some cases, the autonomous driving commands may also be executed to operate the vehicle <b>102</b> and/or one or more external factors to be fully autonomously driven or semi-autonomously driven in a particular manner that may be based on one or more additional factors that may include, but may not be limited to, a lane in which the vehicle <b>102</b> is traveling, status of traffic signals, traffic patterns, traffic regulations, etc.</p><p id="p-0035" num="0034">As discussed below, based on the classification of one or more important objects and the analysis of the object importance scores associated with each of the one or more important objects located within the surrounding environment of the vehicle <b>102</b>, the attention reasoning application <b>106</b> may be configured to communicate one or more commands (e.g., data signals) to the ECU <b>104</b> and/or a vehicle autonomous controller <b>114</b> of the vehicle <b>102</b> to autonomously control the vehicle <b>102</b> to be operated based on the importance score(s).</p><p id="p-0036" num="0035">The respective importance score associated with each top relevant object may thereby be utilized to provide autonomous driving capabilities and to provide driving assistance systems based on one or more commands that may be provided by the vehicle autonomous controller <b>114</b> and/or the ECU <b>104</b> to one or more of the vehicle systems/control units <b>120</b> of the vehicle <b>102</b>. The vehicle autonomous controller <b>114</b> may be configured to provide the one or more commands to one or more of the vehicle systems/control units <b>120</b> to provide full autonomous or semi-autonomous control of the vehicle <b>102</b>. Such autonomous control of the vehicle <b>102</b> may be provided by sending one or more commands to control one or more of the vehicle systems/control units <b>120</b> to operate (e.g., drive) the vehicle <b>102</b> during one or more circumstances (e.g., when providing driver assist controls), and/or to fully control driving of the vehicle <b>102</b> during an entire trip of the vehicle <b>102</b>.</p><p id="p-0037" num="0036">The one or more commands may be provided to one or more vehicle systems/control units <b>120</b> that include, but are not limited to an engine control unit, a braking control unit, a transmission control unit, a steering control unit, and the like to control the vehicle <b>102</b> to be autonomously driven based on data communicated by the attention reasoning application <b>106</b>. As such, the vehicle <b>102</b> may be autonomously or semi-autonomously operated based on the level of importance that may be associated with the positions, locations, and/or potential overlap with respect to the one or more important objects.</p><p id="p-0038" num="0037">In one embodiment, the attention reasoning application <b>106</b> may be configured to communicate with the ECU <b>104</b> and/or the one or more vehicle systems/control units <b>120</b> to provide warnings/alerts to a driver of the vehicle (e.g., if the vehicle <b>102</b> is being driven by a driver and not autonomously) for enhancement purposes based on the degree of influence associated with each of the important objects with respect to driving control decisions for a driver of the vehicle <b>102</b>. The vehicle systems/control units <b>120</b> may also include Advanced Driver Assistance Systems (ADAS), for example, an adaptive cruise control system, a blind spot monitoring system, a collision mitigation system, a lane departure warning system, among others (not individually shown) that may be utilized to provide warnings/alerts to the driver of the vehicle <b>102</b> (e.g., in a circumstance that the vehicle <b>102</b> is being driven by a driver and not being fully autonomously controlled) for preemptive purposes based on the level of importance that may be associated with the positions, locations, and/or potential overlap with respect to the one or more important objects.</p><p id="p-0039" num="0038">In one embodiment, the vehicle systems/control units <b>120</b> may be operably connected to vehicle sensors <b>110</b> of the vehicle <b>102</b>. The vehicle sensors <b>110</b> may include, but are not limited to, sensors associated with the vehicle systems/control units <b>120</b> and other sensors associated with one or more electronic components and/or mechanical components (not shown) of the vehicle <b>102</b>. In one or more embodiments, one or more of the vehicle sensors <b>110</b> may provide sensor data to the vehicle autonomous controller <b>114</b> to be utilized in addition to data that is communicated by the attention reasoning application <b>106</b> to autonomously control the vehicle <b>102</b>.</p><p id="p-0040" num="0039">In one configuration, the memory <b>112</b> of the vehicle <b>102</b> may be configured to store one or more executable files associated with one or more operating systems, applications, associated operating system data, application data, vehicle system and subsystem user interface data, and the like that are executed by the ECU <b>104</b>. In one or more embodiments, the memory <b>112</b> of the vehicle <b>102</b> may be accessed by the attention reasoning application <b>106</b> to store data, for example, image data associated with RGB images received by the application <b>106</b> that may be output by the vehicle camera system <b>116</b>.</p><p id="p-0041" num="0040">In some embodiments, the memory <b>112</b> may include one or more object models (not shown) that may be associated with one or more types of objects. The one or more object models may represent values that include a range of sizes and features (based on image data) that are associated to respective types of objects (e.g., automobiles, pedestrians, bicyclists, traffic light, road sign, lamp post, train, bus, etc.) In some configurations, the application <b>106</b> may analyze the object models to identify particular objects that may be utilized by the vehicle systems/control units <b>120</b> to provide specific object warnings (e.g., pedestrian crossing warning, blind spot warning).</p><p id="p-0042" num="0041">With continued reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the vehicle camera system <b>116</b> may include one or more of the cameras <b>118</b> that may be positioned in one or more directions and at one or more areas to capture one or more images of the surrounding environment of the vehicle <b>102</b>. The one or more cameras <b>118</b> of the vehicle camera system <b>116</b> may be disposed at external front portions of the vehicle <b>102</b>, including, but not limited to different portions of the vehicle dashboard, vehicle bumper, vehicle front lighting units, vehicle fenders, and the windshield. In one embodiment, the one or more cameras <b>118</b> may be configured as RGB cameras that may capture RGB bands that are configured to capture rich information about object appearance, as well as relationships and interactions between the vehicle <b>102</b> and objects within the surrounding environment of the vehicle <b>102</b>.</p><p id="p-0043" num="0042">In other embodiments, the one or more cameras <b>118</b> may be configured as stereoscopic cameras that are configured to capture environmental information in the form three-dimensional images. In one or more configurations, the one or more cameras <b>118</b> may be configured to capture one or more first person viewpoint RGB images/videos of the surrounding environment of the vehicle <b>102</b>. The vehicle camera system <b>116</b> may be configured to convert one or more RGB images/videos (e.g., sequences of images) into image data that is communicated to the attention reasoning application <b>106</b> to be analyzed.</p><p id="p-0044" num="0043">As discussed, the image data that may be provided by the vehicle camera system <b>116</b> to the attention reasoning application <b>106</b> may be further evaluated and processed based on the utilization of a plurality of neural networks <b>108</b>, <b>122</b>, <b>124</b>, <b>126</b>. The attention reasoning application <b>106</b> may be configured to utilize the plurality of neural networks <b>108</b>, <b>122</b>, <b>124</b>, <b>126</b> that may be configured to receive particular inputs of data and to output respective data that may be utilized to determine an importance score that is associated with each top relevant object that is located within the surrounding environment of the vehicle <b>102</b>. As discussed, such a determination may be mainly made based on image data pertaining to the RGB images/video of the surrounding environment of the vehicle <b>102</b> that are captured by the one or more cameras <b>118</b> of vehicle camera system <b>116</b>.</p><p id="p-0045" num="0044">In an exemplary embodiment, the plurality of neural networks <b>108</b>, <b>122</b>, <b>124</b>, <b>126</b> may include, but may not be limited to, the GCN <b>108</b>, a reason proposal neural network (RPN) <b>122</b>, an I3D convolutional neural network (I3D) <b>124</b>, and a multi-layer perceptron neural network (MLP) <b>126</b>. However, it is to be appreciated that one or more additional or alternate types/configurations of neural networks (e.g., deep neural networks, convolutional networks, additional convolutional layers, fully connected layers, etc.) may be included within the environment <b>100</b>. In one or more embodiments, the plurality of neural networks <b>108</b>-<b>126</b> may be operably controlled by a neural network processing unit <b>128</b>.</p><p id="p-0046" num="0045">The neural network processing unit <b>128</b> may be configured to provide processing capabilities to be configured to utilize machine learning/deep learning to provide artificial intelligence capabilities that may be utilized to output data by each of the plurality of neural networks <b>108</b>-<b>126</b> to the attention reasoning application <b>106</b>. The neural network processing unit <b>128</b> may process information that is provided as inputs and may access one or more stored pre-trained datasets to provide various functions, that may include, but may not be limited to, objectness scoring, importance scoring, feature recognition and scoring, computer vision, speed recognition, machine translation, path prediction, autonomous driving commands, and the like.</p><p id="p-0047" num="0046">In an exemplary embodiment, the RPN <b>122</b> may be configured to receive one or target frames as inputs that may be inputs. The RPN <b>122</b> may be configured as a faster R-CNN network that may analyze the target input frame(s) against pre-trained dataset. The RPN <b>122</b> may utilize an anchor and apply Iowa (intersection over union) values to compute a plurality of anchor boxes upon one or more objects that may be located within the surrounding environment of the vehicle <b>102</b> as included within the target frame(s). As discussed below, the RPN <b>122</b> may be configured to output anchor objectness scores that may be associated with one or more objects that may be encompassed within one or more anchor boxes computed by the RPN <b>122</b> to determine one or more relevant objects out of the one or more objects located within the surrounding environment of the vehicle <b>102</b>.</p><p id="p-0048" num="0047">In one embodiment, the I3D <b>124</b> may be configured as a convolutional neural network that may be trained for image/video classification on a pre-trained dataset. The I3D <b>124</b> may be configured as a feature extractor to extract features and to determine anchor importance scores that may be fused with the anchor objectness scores to determine top-k relevant object proposals that may be most likely to interfere with the path of the vehicle <b>102</b> during the driving/operation of the vehicle <b>102</b>.</p><p id="p-0049" num="0048">In one or more embodiments, the GCN <b>108</b> may be configured to receive inputs in the form of interactions through a computed interaction graph with features pooled from the I3D <b>124</b> as object nodes and edges. The GCN <b>108</b> may be configured to analyze the nodes that may represent top relevant objects and graph edges as determined using an agency matrix (e.g., an input feature matrix N&#xd7;F&#xb0; feature matrix, X, where N is the number of nodes and F&#xb0; is the number of input features for each node and an N&#xd7;N matrix representation of the graph structure such as the adjacency matrix A) to thereby update the node features based on neighbor nodes and to output the updated node features to capture to a long-range relationship with other nodes. Stated differently, the GCN <b>108</b> may be configured to output updated node features of object nodes associated with top relevant objects by interacting with neighboring object nodes through graph convolution based on learnt interaction graph edges.</p><p id="p-0050" num="0049">In one embodiment, the updated node features may be inputted to the MLP <b>126</b>. The MLP <b>126</b> may include multi-layer perceptrons that are simple computational units that have weighted input signals and may compute final importance score estimations that may be associated with each of top relevant object. As discussed, based on the analysis of the assigned object importance scores, the attention reasoning application <b>106</b> may be configured to classify one or more top relevant objects as important objects and may communicate one or more commands to the ECU <b>104</b> and/or a vehicle autonomous controller <b>114</b> of the vehicle <b>102</b> to autonomously control the vehicle <b>102</b> based on the importance score(s) that may be associated with one or more important objects.</p><heading id="h-0007" level="2">II. The Object-Level Driver Attention Reasoning Application and Related Methods</heading><p id="p-0051" num="0050">Components of the attention reasoning application <b>106</b> will now be described according to an exemplary embodiment and with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In an exemplary embodiment, the attention reasoning application <b>106</b> may be stored on the memory <b>112</b> and executed by the ECU <b>104</b> of the vehicle <b>102</b>. In another embodiment, the attention reasoning application <b>106</b> may be stored on an externally hosted computing infrastructure (not shown) and may be accessed by a communication device (not shown) of the vehicle <b>102</b> to be executed by the ECU <b>104</b> of the vehicle <b>102</b>.</p><p id="p-0052" num="0051">The general functionality of attention reasoning application <b>106</b> will now be discussed. In an exemplary embodiment, the attention reasoning application <b>106</b> may include a proposal determinant module <b>130</b>, a graph conversion module <b>132</b>, an object scoring module <b>134</b>, and a vehicle control module <b>136</b>. However, it is appreciated that the attention reasoning application <b>106</b> may include one or more additional modules and/or sub-modules that are included in lieu of the modules <b>130</b>-<b>136</b>.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a general schematic overview <b>200</b> for providing object-level driver attention reasoning with the GCN <b>108</b> by the attention reasoning application <b>106</b> according to an exemplary embodiment of the present disclosure. The components of the schematic overview <b>200</b> will be described in detail below with respect to the execution of methods by the attention reasoning application <b>106</b>. <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a process flow diagram of a method <b>300</b> for determining and fusing anchor scores associated with relevant objects that are located within the surrounding environment of the vehicle <b>102</b> according to an exemplary embodiment of the present disclosure. <figref idref="DRAWINGS">FIG. <b>3</b></figref> will be described with reference to the components of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and <figref idref="DRAWINGS">FIG. <b>2</b></figref> though it is to be appreciated that the method <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be used with other systems/components. The method <b>300</b> may begin at block <b>302</b>, wherein the method <b>300</b> may include receiving image data <b>202</b> from the vehicle camera system <b>116</b>.</p><p id="p-0054" num="0053">In an exemplary embodiment, the proposal determinant module <b>130</b> of the attention reasoning application <b>106</b> may be configured to communicate with the vehicle camera system <b>116</b> to receive image data <b>202</b> (as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>). As discussed above, the image data <b>202</b> may pertain to one or more RGB images/video of the surrounding environment of the vehicle <b>102</b> that are captured by one or more cameras <b>118</b> that are operably connected to the vehicle camera system <b>116</b>. In some embodiments, the proposal determinant module <b>130</b> may package and store the image data on the memory <b>112</b> to be evaluated at one or more points in time.</p><p id="p-0055" num="0054">The method <b>300</b> may proceed to block <b>304</b>, wherein the method <b>300</b> may include analyzing the image data <b>202</b> and determining a target frame <b>204</b> and RGB image clips <b>208</b> for a predetermined period of time. In an exemplary embodiment, the proposal determinant module <b>130</b> may be configured to analyze the image data to determine a plurality of RGB image clips <b>208</b>. The RGB image clips <b>208</b> may include spliced sectionalized image clips of an entire video of a length that pertains to a predetermined period of time (e.g., 7 seconds). Each of the RGB image clips <b>208</b> may include an n number of pixels that include portions of the surrounding environment of the vehicle <b>102</b> as captured within the RGB images/video by the camera(s) <b>118</b> of the vehicle camera system <b>116</b>. The proposal determinant module <b>130</b> be further configured to determine image frames that are included within the RGB image clips <b>208</b>.</p><p id="p-0056" num="0055">In an exemplary embodiment, the proposal determinant module <b>130</b> may select a target frame <b>204</b> from the image frames that are included within the RGB image clips <b>208</b>. In one configuration, the target frame <b>204</b> may include a middle frame (in the online detection setting) of the image frames that are included within the RGB image clips <b>208</b>. In another configuration, the target frame <b>204</b> may include a last frame (in the online detection setting) of the image frames that are included within the RGB image clips <b>208</b>.</p><p id="p-0057" num="0056">The method <b>300</b> may proceed to block <b>306</b>, wherein the method <b>300</b> may include inputting the target frame <b>204</b> to the RPN <b>122</b> to output anchor objectness scores. In an exemplary embodiment, the target frame <b>204</b> may be inputted to the RPN <b>122</b>. The RPN <b>122</b> may be pre-trained to compute anchor boxes around objects determined to be relevant objects that may pertain to static objects that may possibly influence the operation of the vehicle <b>102</b> (e.g., traffic signs, traffic lights, buildings, mailbox, trees, and the like) apart from pathways and ceiling (e.g., predetermined area above the vehicle <b>102</b>). The RPN <b>122</b> may additionally compute anchor boxes around objects determined to be relevant objects that may pertain to dynamic objects that may possibly influence the operation of the vehicle <b>102</b> (e.g., automobiles, pedestrians, bicyclists, bus, and the like).</p><p id="p-0058" num="0057">In an exemplary embodiment, the RPN <b>122</b> may be configured to predict object proposals at each spatial location of the surrounding environment of the vehicle <b>102</b> captured within the target frame <b>204</b>. The RPN <b>122</b> may be configured to predict a class-agnostic objectiveness score and may complete bounding box refinement for the anchor boxes to thereby output anchor objectness scores <b>206</b> associated with the one or more refined bounding boxes. The anchor objectness scores <b>206</b> may thereby provide objectness scores with respect to likelihood of objects that include relevant objects that may possibly influence the operation of the vehicle <b>102</b>. In one embodiment, the anchor objectness scores <b>206</b> may be compared to a predetermined objectness threshold score. Proposals that may include anchor objectness scores <b>206</b> that are below the predetermined objectness threshold score may indicate object classes other than the relevant objects (e.g., that do not include static and dynamic objects that should be considered as relevant since they may not possibly influence the operation of the vehicle <b>102</b>) are removed (e.g., as irrelevant stuff) since they are irrelevant with respect to determine importance scoring for important objects that may influence the vehicle <b>102</b>. Accordingly, objectness scores that may be associated with relevant objects (e.g., as relevant things) that may impact the travel of the vehicle <b>102</b>.</p><p id="p-0059" num="0058">In an illustrative example, the anchor boxes with coordinates of (x<b>1</b>, y<b>1</b>, x<b>2</b>, y<b>2</b>)=(0.07, 0.91, 0.97, 1.0) (all box coordinates rescaled to [0, 1]) may be computed so that the target frame <b>204</b> may contain the same number of proposals as another target frame <b>204</b> that may be determined for RGB image clips <b>208</b> determined during a different period of time. The anchor boxes may thereby be analyzed to compute bounding box refinement. Accordingly, one or more bounding boxes may be associated with anchor objectness scores <b>206</b>. In one embodiment, the anchor objectness scores may be output to the proposal determinant module <b>130</b> of the attention reasoning application <b>106</b>.</p><p id="p-0060" num="0059">The method <b>300</b> may proceed to block <b>308</b>, wherein the method <b>300</b> may include inputting RGB image clips <b>208</b> to the I3D <b>124</b> to output anchor importance scores <b>210</b>. In one embodiment, the proposal determinant module <b>130</b> may input the RGB image clips <b>208</b> to the I3D <b>124</b> to be analyzed. The I3D <b>124</b> may be configured as a pre-trained convolutional neural network that may be configured to complete feature extraction of features included within the RGB image clips <b>208</b>. In particular, the I3D <b>124</b> may be pre-trained to extract rich temporal and spatial features of which the temporal motion information may be important for reasoning both the vehicle's and other (dynamic) relevant object's intentions and future movements.</p><p id="p-0061" num="0060">In particular, the I3D <b>124</b> enables spatial appearance information with respect to extracted features which may be utilized by the proposal determinant module <b>130</b> to determine inherent characteristics of each relevant object as determined to exist within the surrounding environment of the vehicle <b>102</b>. In one embodiment, given T&#x2032; continuous RGB image clips <b>208</b>, &#x201c;B<sub>t.t+T&#x2032;&#x2212;1 </sub>&#x2208; R<sup>T&#x2032;&#xd7;H&#x2032;&#xd7;W&#x2032;&#xd7;3</sup>&#x201d; the proposal determinant module <b>130</b> may be configured to input the RGB image clips <b>208</b> through the I3D <b>124</b> to extract features F &#x2208; R<sup>T&#xd7;H&#xd7;W&#xd7;C </sup>from a last mixing layer. The I3D <b>124</b> may thereby output anchor importance scores <b>210</b> that pertain to the the inherent characteristics of the relevant objects based on the analysis of extracted features with respect to both the vehicle's and other (dynamic) relevant object's intentions and future movements to the proposal determinant module <b>130</b>.</p><p id="p-0062" num="0061">The method <b>300</b> may proceed to block <b>310</b>, wherein the method <b>300</b> may include fusing the anchor objectness scores and anchor importance scores. In an exemplary embodiment, the proposal determinant module <b>130</b> may fuse the anchor objectness scores associated with each of the relevant objects and the anchor importance scores associated with the inherent characteristics of each of the respective relevant objects. The proposal determinant module <b>130</b> may fuse the anchor scores such that they are aggregated temporally by convolution along a temporal dimension. The proposal determinant module <b>130</b> may thereby determine a fused anchor score <b>212</b> that is associated with each respective relevant object that is included within the target frame <b>204</b> and the RGB image clips <b>208</b>.</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a process flow diagram of a method <b>400</b> of utilizing non-maximal suppression for object detection of the relevant objects and determining top relevant object proposals according to an exemplary embodiment of the present disclosure. <figref idref="DRAWINGS">FIG. <b>4</b></figref> will be described with reference to the components of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and <figref idref="DRAWINGS">FIG. <b>2</b></figref> though it is to be appreciated that the method <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> may be used with other systems/components. The method <b>400</b> may begin at block <b>402</b>, wherein the method <b>400</b> may include completing non-maximal suppression on relevant objects associated with fused anchor scores.</p><p id="p-0064" num="0063">In some circumstances, numerous anchor boxes that may be associated with respective fused anchor scores may be computed for particular objects (e.g., as portions of objects may be included in more than one anchor box). The proposal determinant module <b>130</b> may be configured to use non-maximal suppression <b>214</b> on the fused anchor scores associated with the (numerous) anchor boxes that may be computed for the relevant objects. The non-maximal suppression may use an intersection-over-union (IoU) threshold with respect to each of the anchor boxes and may analyze the fused anchor scores to select top box proposals that are associated with each respective relevant object. Accordingly, the proposal determinant module <b>130</b> may compute new bounding boxes based on the top box proposals that may fully encompass the pixels of the target frame/RGB image clips that include each of the respective relevant objects determined to be included within the surrounding environment of the vehicle <b>102</b>.</p><p id="p-0065" num="0064">As shown in the illustrative example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, bounding boxes <b>502</b>-<b>518</b> may be computed for relevant objects that may include automobiles, pedestrians, and other road side objects that may possibly have an influence with respect to the driving/operation of the vehicle <b>102</b>. The target frame <b>500</b> may be updated with the bounding boxes <b>502</b>-<b>518</b> that encapsulate each of the relevant objects. Accordingly, each relevant object may no longer be associated with numerous anchor boxes based on the utilization of non-maximal suppression <b>214</b>. In one embodiment, the fused anchor scores associated with each of the anchor boxes that previously included portions of each respective relevant object may be aggregated to determine aggregate fused anchor scores that may be associated with each respective bounding boxes <b>502</b>-<b>518</b> that encapsulate each of the respective relevant objects.</p><p id="p-0066" num="0065">Referring again to the method <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the method <b>400</b> may proceed to block <b>404</b>, wherein the method <b>400</b> may include determining the top-K proposals <b>216</b>. In an exemplary embodiment, the proposal determinant module <b>130</b> may be configured to analyze the bounding boxes <b>502</b>-<b>518</b> computed using non-maximal suppression <b>214</b> and may determine top-K proposals to determine top relevant objects. The top-k proposals may include a top k number of relevant objects that are considered the most relevant (e.g., k number of most relevant) with respect to having an influence on the driving/operation of the vehicle <b>102</b>. In one embodiment, the proposal determinant module <b>130</b> may be configured to classify the top-k proposals as top relevant objects that may be further analyzed, as discussed below.</p><p id="p-0067" num="0066">For example, with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref> again, the relevant objects encompassed within a k number of the bounding boxes <b>502</b>-<b>518</b> may be included as top-K proposals and may be classified as top relevant objects based on their vicinity to the vehicle <b>102</b> from the point of view of the vehicle <b>102</b> as captured within the target frame <b>500</b>. In an exemplary embodiment, upon classifying the top relevant objects based on the top-k proposals <b>216</b>, the proposal determinant module <b>130</b> may communicate data pertaining to the top relevant objects, the anchor importance scores <b>210</b>, and the fused anchor scores <b>212</b> to the graph conversion module <b>132</b> of the attention reasoning application <b>106</b>.</p><p id="p-0068" num="0067">With continued reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the method <b>400</b> may proceed to block <b>406</b>, wherein the method <b>400</b> may include completing ROI pooling <b>218</b> of the anchor importance scores <b>210</b> and the top relevant objects to determine object node features. In an exemplary embodiment, the graph conversion module <b>132</b> may be configured to convert the data associated with the surrounding environment of the vehicle <b>102</b> and the top relevant objects into an interaction graphic format that includes nodes and edges of an interaction graph that corresponds to the each of the top relevant objects located within the surrounding environment of the vehicle <b>102</b> captured within the target frame <b>204</b> and/or the RGB image clips <b>208</b>.</p><p id="p-0069" num="0068">In one embodiment, the graph conversion module <b>132</b> may be configured to complete region of interest pooling <b>218</b> to determine object node features <b>220</b> associated with the top relevant objects. In particular, the ROI pooling <b>218</b> may be utilized to extract a (fixed-sized) feature map for each bounding box associated with a top relevant object. Extracted features F that may be derived based on machine learning completed by the I3D <b>124</b> to output the anchor importance scores <b>210</b>, discussed above, may be aggregated temporally with one-layer convolution along only a temporal dimension by setting both a kernel size and a stride to T&#xd7;1&#xd7;1.</p><p id="p-0070" num="0069">From the feature maps associated with each of the bounding boxes associated with top-K proposals, F &#x2208; R<sup>1&#xd7;H&#xd7;W&#xd7;C </sup>the features {f<sub>i</sub>|i=1, 2, . . . , N} are pooled using ROI pooling <b>218</b>. Accordingly, object nodes of the interaction graph that may be associated with the top relevant objects may be determined. In one configuration, the temporal dimension may be removed resulting in feature vectors {f<sub>i </sub>&#x2208; R<sup>C</sup>|i=1, 2, . . . , N} for each object node.</p><p id="p-0071" num="0070">The method <b>400</b> may proceed to block <b>408</b>, wherein the method <b>400</b> may include computing interaction graph edges using adjacency matrix <b>222</b>. In an exemplary embodiment, the graph conversion module <b>132</b> may be configured to utilize an adjacency matrix <b>222</b> to formulate graph edges. The adjacency matrix <b>222</b> may be configured to connect object nodes that are adjacent to one another to determine edges. In one embodiment, the graph conversion module <b>132</b> may be configured to predict the edge strength E<sub>ij </sub>by estimating how closely two object nodes i and j interact with each other. Given node features v<sub>i </sub>and v<sub>j</sub>, an interaction score IS<sub>ij </sub>may be computed by:</p><p id="p-0072" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>IS<sub>ij</sub>=&#x3a6;(&#x393;(<i>v</i><sub>i</sub>)&#x2225;&#x393;&#x2032;(<i>v</i><sub>j</sub>)),<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0073" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>i, j &#x2208; {1, 2, . . . , N}<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0074" num="0000">where &#x393;(&#x22c5;) and &#x393;(&#x22c5;)&#x2032; are linear transformations with different learning parameters,</p><p id="p-0075" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3a6;(<i>x</i>)=&#x3d5;<i>x </i><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0076" num="0000">is linear transformation with &#xd8; as learned parameters and &#x2225; denotes concatenations.</p><p id="p-0077" num="0071">With an interaction matrix, IS obtained by computing interaction scores for each pair of object nodes. The graph conversion module <b>132</b> may be configured to calculate E<sub>ij </sub>by applying softmax on IS similarly as well as adding an identity matrix I<sub>N </sub>&#x2208; R<sup>N&#xd7;N </sup>to force self-attention:</p><p id="p-0078" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>E</mi>    <mi>ij</mi>   </msub>   <mo>=</mo>   <mfrac>    <msup>     <mi>e</mi>     <msub>      <mi>IS</mi>      <mi>ij</mi>     </msub>    </msup>    <mrow>     <msubsup>      <mo>&#x2211;</mo>      <mrow>       <mi>k</mi>       <mo>=</mo>       <mn>1</mn>      </mrow>      <mi>N</mi>     </msubsup>     <msup>      <mi>e</mi>      <msub>       <mi>IS</mi>       <mi>ik</mi>      </msub>     </msup>    </mrow>   </mfrac>  </mrow>  <mo>&#x2062;</mo>  <mtext></mtext>  <mrow>   <mi>i</mi>   <mo>,</mo>   <mrow>    <mi>j</mi>    <mo>&#x2208;</mo>    <mrow>     <mo>{</mo>     <mrow>      <mn>1</mn>      <mo>,</mo>      <mn>2</mn>      <mo>,</mo>      <mo>&#x2026;</mo>      <mtext>   </mtext>      <mo>,</mo>      <mi>N</mi>     </mrow>     <mo>}</mo>    </mrow>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0079" num="0072">Accordingly, the model learned as an interaction graph itself is based on each object node's features (associated with each top relevant object). The learnt edge strength E<sub>ij </sub>indicates how much effect object node j will have on updating object node i's features through graph convolution and may therefore reflect how closely they are interacting.</p><p id="p-0080" num="0073">In an alternate embodiment, the graph conversion module <b>132</b> may be configured to formulate each graph edge with feature similarity between object nodes i and j. With node features v<sub>i </sub>and v<sub>j </sub>the similarity S<sub>ij </sub>may be measured by:</p><p id="p-0081" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>S</i><sub>ij</sub>=&#x393;(<i>v</i><sub>i</sub>)<sup>T</sup>&#x393;&#x2032;(<i>v</i><sub>j</sub>), <i>i, j &#x2208; {</i>1, 2, . . . , <i>N}</i> where &#x393;(<i>v</i>)=<i>wv &#x393;&#x2032;</i>(<i>v</i>)=<i>w&#x2032;v </i><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0082" num="0000">&#x393;(&#x22c5;) and &#x393;(&#x22c5;)&#x2032; are linear transformations with different learning parameters &#x3c9; and &#x3c9;&#x2032;. By computing S<sub>ij </sub>for each pair of object nodes, a similarity matrix S &#x2208; R<sup>N&#xd7;N </sup>may be determined after which E<sub>ij </sub>is obtained by applying softmax on each column of S for normalization:</p><p id="p-0083" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>E</mi>    <mi>ij</mi>   </msub>   <mo>=</mo>   <mfrac>    <msup>     <mi>e</mi>     <msub>      <mi>IS</mi>      <mi>ij</mi>     </msub>    </msup>    <mrow>     <msubsup>      <mo>&#x2211;</mo>      <mrow>       <mi>k</mi>       <mo>=</mo>       <mn>1</mn>      </mrow>      <mi>N</mi>     </msubsup>     <msup>      <mi>e</mi>      <msub>       <mi>IS</mi>       <mi>ik</mi>      </msub>     </msup>    </mrow>   </mfrac>  </mrow>  <mo>&#x2062;</mo>  <mtext></mtext>  <mrow>   <mi>i</mi>   <mo>,</mo>   <mrow>    <mi>j</mi>    <mo>&#x2208;</mo>    <mrow>     <mo>{</mo>     <mrow>      <mn>1</mn>      <mo>,</mo>      <mn>2</mn>      <mo>,</mo>      <mo>&#x2026;</mo>      <mtext>   </mtext>      <mo>,</mo>      <mi>N</mi>     </mrow>    </mrow>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0084" num="0074">In one configuration, since &#x393;(&#x22c5;) and &#x393;(&#x22c5;)&#x2032; are different transformations, a direction graph may be formulated and may be utilized to determine the importance score associated with each top relevant object. However, for purposes of simplicity, this disclosure will focus on the interaction graph being utilized to determine the importance score associated with each top relevant object.</p><p id="p-0085" num="0075"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a process flow diagram of a method <b>600</b> for passing the K node features and the edges of the interaction graph through the GCN <b>108</b> and determining importance scores for objects located within the surrounding environment of the vehicle <b>102</b> according to an exemplary embodiment of the present disclosure. <figref idref="DRAWINGS">FIG. <b>6</b></figref> will be described with reference to the components of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and <figref idref="DRAWINGS">FIG. <b>2</b></figref> though it is to be appreciated that the method <b>600</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref> may be used with other systems/components. In an exemplary embodiment, upon determining the object nodes and the edges of the interaction graph that is associated with the surrounding environment of the vehicle <b>102</b> captured within the target frame <b>204</b> and/or the RGB image clips <b>208</b>, the graph conversion module <b>132</b> may communicate respective data to the object scoring module <b>134</b> of the attention reasoning application <b>106</b>.</p><p id="p-0086" num="0076">The method <b>600</b> may begin at block <b>602</b>, wherein the method <b>600</b> may include inputting data associated with the interaction graph to the GCN <b>108</b> to update object node features. With the graph E formulated, the object scoring module <b>134</b> may input data associated with the object node features <b>220</b> and the edges of the interaction graph as computing using the adjacency matrix <b>222</b> to the GCN <b>108</b>. The GCN <b>108</b> may be utilized to update the object node features <b>220</b> by interacting with other object nodes through graph convolution based on the learnt interaction graph edges. Accordingly, the object nodes may update each other. One layer of graph convolution may be represented as:</p><p id="p-0087" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>V&#x2032;=&#x3c3;</i>(<i>EVW</i>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0088" num="0000">where E &#x2208; R<sup>N&#xd7;N </sup>is an edge matrix, V=[v<sub>1</sub>, v<sub>2</sub>, . . . , v<sub>n</sub>] &#x2208; R<sup>N&#xd7;C </sup>is an input node feature matrix, and W &#x2208; R<sup>C&#x2208;C </sup>is a weight matrix of the layer. &#x3c3;<sup>(&#x22c5;) </sup>is a non-linear function. In one embodiment, three graph convolutional layers may be stacked and C&#x2032;=C may be set for all of them. The output of the GCN <b>108</b> may be updated feature matrix U &#x2208; R<sup>N&#xd7;C </sup>with updated node features <b>224</b> to capture a long-range relationship with other object nodes. These features are updated by interacting with other object nodes based on the learnt interaction graph edges (e.g., interaction graph edges).</p><p id="p-0089" num="0077">With continued reference to the method <b>600</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the method <b>600</b> may proceed to block <b>604</b>, wherein the method <b>600</b> may include inputting the updated node features <b>224</b> to the MLP <b>126</b> to evaluate per-node features and determining importance scores for each top relevant object. In an exemplary embodiment, upon the GCN <b>108</b> outputting the updated node features <b>224</b>, the object scoring module <b>134</b> may input the updated node features <b>224</b> to the MLP <b>126</b> to perform per-node object importance estimation. In particular, the object scoring module <b>134</b> may apply global average pooling on the extracted features F from I3D <b>124</b> (discussed above) to obtain a global descriptor D &#x2208; R<sup>1&#xd7;C </sup>which are further tiled N times and concatenated with the updated node features <b>224</b> U. Each row of the resulting features Y &#x2208; R<sup>N&#xd7;2C </sup>is inputted to the MLP <b>126</b> for the final importance score estimation. Stated differently, the MLP <b>126</b> may be trained to output importance scores for each top relevant object <b>226</b> within a probability range (0-1) based on the updated node features <b>224</b> that are concatenated with the global descriptor and fed into the MLP <b>126</b>.</p><p id="p-0090" num="0078">The method <b>600</b> may proceed to block <b>606</b>, wherein the method <b>600</b> may include providing vehicle autonomous control <b>228</b> to account for one or more important objects. In an exemplary embodiment, upon determining the importance scores for each of the top relevant objects that are determined by the MLP <b>126</b>, the object scoring module <b>134</b> may communicate scoring data to the vehicle control module <b>136</b> of the attention reasoning application <b>106</b>. In one configuration, the vehicle control module <b>136</b> may be configured to compare the importance scores for each top relevant object <b>226</b> to a predetermined importance scoring threshold value that pertains to objects that may that may have an immediate/high degree of influence with respect to the driving/operation of the vehicle <b>102</b>.</p><p id="p-0091" num="0079">Accordingly, the vehicle control module <b>136</b> may compare the importance score for each top relevant object <b>232</b> to the predetermined importance scoring threshold value to account for objects that are assigned an importance score that are above the predetermined importance threshold value. In one embodiment, upon determining one or more top relevant objects that include importance scores that are above the predetermined importance scoring threshold value, the vehicle control module <b>136</b> may classify the respective top relevant objects as important objects.</p><p id="p-0092" num="0080">In an exemplary embodiment, upon classifying the one or more important objects, the vehicle control module <b>136</b> may determine a ranking of importance of the important objects based on a scale of highest to lowest importance scores that were assigned to the respective objects by the MLP <b>128</b>. For example, important objects that may be closely located to the vehicle <b>102</b> and/or may or potentially located within the vehicle's path may include an importance score that may be ranked higher than an important object that may not be located as close to the vehicle <b>102</b> or may not be located within the vehicle's path.</p><p id="p-0093" num="0081">In one or more embodiment, upon ranking the importance of the important objects, the vehicle control module <b>136</b> may be configured to communicate one or more commands (e.g., data signals) to the vehicle autonomous controller <b>114</b> and/or the ECU <b>104</b> to autonomously control the vehicle <b>102</b> to account for the one or more important objects. In some circumstances, the one or more important objects may be accounted for based on the ranked importance of the important objects. Referring again to the illustrative example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, as shown each of the bounding boxes <b>502</b>-<b>518</b> may be associated with respective importance scores. The vehicle autonomous controller <b>114</b> and/or the ECU <b>104</b> may communicate with one or more of the control units of the vehicle <b>102</b> to thereby control the operation of the vehicle <b>102</b>.</p><p id="p-0094" num="0082">The vehicle control module <b>136</b> may determine one or more actions with respect to vehicle autonomous control <b>228</b> of the vehicle <b>102</b> to operate the vehicle <b>102</b> in a manner that is influenced by the locations, positions, and/or trajectories of one or more important objects that are located within the surrounding environment of the vehicle <b>102</b>. More specifically, the vehicle <b>102</b> may be controlled to execute one or more actions that may be conducted in a particular manner(s) (e.g., with the application of a particular speed, acceleration, steering angle, throttle angle, braking force, etc.) to account for the one or more highly important road users.</p><p id="p-0095" num="0083">Referring again to the illustrative example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the vehicle control module <b>136</b> may determine one or more actions with respect to vehicle autonomous control <b>228</b> of the vehicle <b>102</b> to operate the vehicle <b>102</b> in a manner that is influenced by the locations, positions, and/or trajectories of one or more important objects that may be classified as such based on an importance score that is above 0.0070. Accordingly, the vehicle <b>102</b> may thereby be operated in a manner that accounts for the one or more important objects such that the one or more important objects may have a varying level of influence based on the respective importance scores that may be applied toward the driving/operation of the vehicle <b>102</b>.</p><p id="p-0096" num="0084">In one or more embodiments, one or more of the vehicle sensors <b>110</b> may provide sensor data to the vehicle autonomous controller <b>114</b> to be utilized in complement to the commands provided by the vehicle control module <b>136</b> to thereby operate the vehicle <b>102</b> in a manner that is influenced by the locations, positions, and/or trajectories of one or more important objects that are located within the surrounding environment of the vehicle <b>102</b> and/or sensor based observations associated with the surrounding environment of the vehicle <b>102</b>.</p><p id="p-0097" num="0085"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a process flow diagram of a method <b>700</b> for providing object-level driver attention reasoning with a graph convolution network according to an exemplary embodiment of the present disclosure. <figref idref="DRAWINGS">FIG. <b>7</b></figref> will be described with reference to the components of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and <figref idref="DRAWINGS">FIG. <b>2</b></figref> though it is to be appreciated that the method <b>700</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref> may be used with other systems/components. The method <b>700</b> may begin at block <b>702</b>, wherein the method <b>700</b> may include receiving image data associated with a plurality of image clips of a surrounding environment of a vehicle <b>102</b>.</p><p id="p-0098" num="0086">The method <b>700</b> may proceed to block <b>704</b>, wherein the method <b>700</b> may include determining anchor objectness scores and anchor importance scores associated with relevant objects included the plurality of image clips. The method <b>700</b> may proceed to block <b>706</b>, wherein the method <b>700</b> may include analyzing the anchor objectness scores and anchor importance scores associated with relevant objects and determining top relevant objects with respect to an operation of the vehicle <b>102</b>. In one embodiment, object node features associated with the top relevant objects and edges of an interaction graph that pertain to the surrounding environment of the vehicle <b>102</b> are determined. The method <b>700</b> may proceed to block <b>708</b>, wherein the method <b>700</b> may include passing the object node features and the edges of the interaction graph through the GCN <b>108</b> to update features of each object node through interaction with other object nodes and determining importance scores for the top relevant objects.</p><p id="p-0099" num="0087">It should be apparent from the foregoing description that various exemplary embodiments of the disclosure may be implemented in hardware. Furthermore, various exemplary embodiments may be implemented as instructions stored on a non-transitory machine-readable storage medium, such as a volatile or non-volatile memory, which may be read and executed by at least one processor to perform the operations described in detail herein. A machine-readable storage medium may include any mechanism for storing information in a form readable by a machine, such as a personal or laptop computer, a server, or other computing device. Thus, a non-transitory machine-readable storage medium excludes transitory signals but may include both volatile and non-volatile memories, including but not limited to read-only memory (ROM), random-access memory (RAM), magnetic disk storage media, optical storage media, flash-memory devices, and similar storage media.</p><p id="p-0100" num="0088">It should be appreciated by those skilled in the art that any block diagrams herein represent conceptual views of illustrative circuitry embodying the principles of the disclosure. Similarly, it will be appreciated that any flow charts, flow diagrams, state transition diagrams, pseudo code, and the like represent various processes which may be substantially represented in machine readable media and so executed by a computer or processor, whether or not such computer or processor is explicitly shown.</p><p id="p-0101" num="0089">It will be appreciated that various implementations of the above-disclosed and other features and functions, or alternatives or varieties thereof, may be desirably combined into many other different systems or applications. Also that various presently unforeseen or unanticipated alternatives, modifications, variations or improvements therein may be subsequently made by those skilled in the art which are also intended to be encompassed by the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230004805A1-20230105-M00001.NB"><img id="EMI-M00001" he="12.70mm" wi="76.20mm" file="US20230004805A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230004805A1-20230105-M00002.NB"><img id="EMI-M00002" he="12.70mm" wi="76.20mm" file="US20230004805A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method for providing object-level attention reasoning comprising:<claim-text>receiving images associated with a plurality of image clips of a surrounding environment of a vehicle;</claim-text><claim-text>analyzing the images and determining a plurality of relevant objects that are included within the surrounding environment that influence an operation of the vehicle;</claim-text><claim-text>inputting data associated with the plurality of relevant objects to a graph convolution network to determine importance scores for each of the plurality of relevant objects; and</claim-text><claim-text>determining a plurality of top relevant objects based on the importance scores that are to be accounted for to autonomously control the vehicle to be operated to complete attention reasoning within the surrounding environment of a vehicle.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein receiving images include analyzing the images received from a vehicle camera system of the vehicle and determining a target image frame from the plurality of image clips.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein analyzing the images and determining the plurality of relevant objects includes determining a target image frame from the plurality of image clips and inputting the target image frame to a regional proposal network to output objectness scores.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computer-implemented method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the objectness scores are associated with a likelihood that plurality of relevant objects included within the plurality of image clips influence the operation of the vehicle.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computer-implemented method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, inputting data associated with the plurality of relevant objects to the graph convolutional network to determine the importance scores for each of the plurality of relevant objects.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computer-implemented method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, further including determining object node features and edges of an interaction graph that are representative of the surrounding environment of the vehicle based on the plurality of relevant objects.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computer-implemented method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein determining object node features includes analyzing the objectness scores and the importance scores by completing region of interest pooing of the importance scores and the top relevant objects to determine the object node features.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The computer-implemented method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein determining the edges of the interaction graph include using an adjacency matrix to compute the edges of the interaction graph.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computer-implemented method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein determining the importance scores includes inputting updated node features of each object node output by the graph convolution network to a multi-layer perceptron neural network to determine the importance scores for each of the plurality of relevant objects.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A system for providing object-level attention reasoning comprising:<claim-text>a memory storing instructions when executed by a processor cause the processor to:</claim-text><claim-text>receive images associated with a plurality of image clips of a surrounding environment of a vehicle;</claim-text><claim-text>analyze the images and determining a plurality of relevant objects that are included within the surrounding environment that influence an operation of the vehicle;</claim-text><claim-text>input data associated with the plurality of relevant objects to a graph convolution network to determine importance scores for each of the plurality of relevant objects; and</claim-text><claim-text>determine a plurality of top relevant objects based on the importance scores that are to be accounted for to autonomously control the vehicle to be operated to complete attention reasoning within the surrounding environment of a vehicle.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein receiving images include analyzing the images received from a vehicle camera system of the vehicle and determining a target image frame from the plurality of image clips.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein analyzing the images and determining the plurality of relevant objects includes determining a target image frame from the plurality of image clips and inputting the target image frame to a regional proposal network to output objectness scores.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the objectness scores are associated with a likelihood that plurality of relevant objects included within the plurality of image clips influence the operation of the vehicle.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein inputting data associated with the plurality of relevant objects to the graph convolutional network to determine the importance scores for each of the plurality of relevant objects.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further including determining object node features and edges of an interaction graph that are representative of the surrounding environment of the vehicle based on the plurality of relevant objects.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein determining object node features includes analyzing the objectness scores and the importance scores by completing region of interest pooing of the importance scores and the top relevant objects to determine the object node features.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein determining the edges of the interaction graph include using an adjacency matrix to compute the edges of the interaction graph.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein determining the importance scores includes inputting updated node features of each object node output by the graph convolution network to a multi-layer perceptron neural network to determine the importance scores for each of the plurality of relevant objects.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A non-transitory computer readable storage medium storing instructions that when executed by a computer, which includes a processor perform a method, the method comprising:<claim-text>receiving images associated with a plurality of image clips of a surrounding environment of a vehicle;</claim-text><claim-text>analyzing the images and determining a plurality of relevant objects that are included within the surrounding environment that influence an operation of the vehicle;</claim-text><claim-text>inputting data associated with the plurality of relevant objects to a graph convolution network to determine importance scores for each of the plurality of relevant objects; and</claim-text><claim-text>determining a plurality of top relevant objects based on the importance scores that are to be accounted for to autonomously control the vehicle to be operated to complete attention reasoning within the surrounding environment of a vehicle.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer readable storage medium of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein determining the importance scores includes inputting updated node features of each object node output by the graph convolution network to a multi-layer perceptron neural network to determine the importance scores for each of the plurality of relevant objects.</claim-text></claim></claims></us-patent-application>