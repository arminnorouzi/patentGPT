<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000395A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221220" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000395</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17931782</doc-number><date>20220913</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>11</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>R</subclass><main-group>1</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>R</subclass><main-group>25</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>1116</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>1</main-group><subgroup>1091</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>6803</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>25</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>011</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">POSTURE DETECTION USING HEARING INSTRUMENTS</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/US2021/021940</doc-number><date>20210311</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17931782</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62990182</doc-number><date>20200316</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Starkey Laboratories, Inc.</orgname><address><city>Eden Prairie</city><state>MN</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Burwinkel</last-name><first-name>Justin</first-name><address><city>Eden Prairie</city><state>MN</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Rozenman</last-name><first-name>Roy</first-name><address><city>Mishmar David</city><country>IL</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A processing system obtain signals that are generated by or generated based on sensors that are included in one or more hearing instruments. Additionally, the processing system determine, based on the signals, whether a posture of a user of the hearing instruments is a target posture. The processing system generate information based on the posture of the user.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="234.95mm" wi="138.01mm" file="US20230000395A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="260.77mm" wi="143.68mm" file="US20230000395A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="245.79mm" wi="154.52mm" file="US20230000395A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="236.47mm" wi="172.30mm" file="US20230000395A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="241.89mm" wi="126.92mm" file="US20230000395A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="245.79mm" wi="162.73mm" file="US20230000395A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="239.01mm" wi="162.73mm" file="US20230000395A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><p id="p-0002" num="0001">This application claims priority to U.S. Provisional Patent Application 62/990,182, filed Mar. 16, 2020, the entire content of which is incorporated by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">This disclosure relates to hearing instruments.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Hearing instruments are devices designed to be worn on, in, or near one or more of a user's ears. Common types of hearing instruments include hearing assistance devices (e.g., &#x201c;hearing aids&#x201d;), earbuds, headphones, hearables, cochlear implants, and so on. In some examples, a hearing instrument may be implanted or integrated into a user. Some hearing instruments include additional features beyond just environmental sound-amplification. For example, some modern hearing instruments include advanced audio processing for improved functionality, controlling and programming the hearing instruments, wireless communication with external devices including other hearing instruments (e.g., for streaming media), and so on.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">This disclosure describes techniques for detecting a posture of a user of one or more hearing instruments and determining whether the posture of the user is a target posture for the user. Example target activities include sitting, standing, walking, sleeping, and so on. A processing system may generate information about the posture of the user and provide the information to the user, another person, or one or more computing devices.</p><p id="p-0006" num="0005">In one example, this disclosure describes a method comprising: obtaining, by a processing system, signals that are generated by or generated based on data from one or more sensors that are included in one or more hearing instruments; determining, by the processing system, based on the signals, whether a posture of a user of the one or more hearing instruments is a target posture for the user; and generating, by the processing system, information based on the posture of the user.</p><p id="p-0007" num="0006">In another example, this disclosure describes a system comprising: one or more hearing instruments, wherein the one or more hearing instruments include sensors; a processing system comprising one or more processors implemented in circuitry, wherein the one or more processors are configured to: obtain signals that are generated by or generated based on data from the sensors; determine, based on the signals, whether a posture of a user of the one or more hearing instruments is a target posture of the user; and generate information based on the posture of the user.</p><p id="p-0008" num="0007">In another example, this disclosure describes a system comprising: means for obtaining signals that are generated by or generated based on data from one or more sensors that are included in one or more hearing instruments; means for determining, based on the signals, whether a posture of a user of the one or more hearing instruments is a target posture for the user; and means for generating information based on the posture of the user.</p><p id="p-0009" num="0008">In another example, this disclosure describes a computer-readable medium comprising instructions stored thereon that, when executed, cause one or more processors to: obtain signals that are generated by or generated based on data from one or more sensors that are included in one or more hearing instruments; determine, based on the signals, whether a posture of a user of the one or more hearing instruments is a target posture for the user; and generate information based on the posture of the user.</p><p id="p-0010" num="0009">The details of one or more aspects of the disclosure are set forth in the accompanying drawings and the description below. Other features, objects, and advantages of the techniques described in this disclosure will be apparent from the description, drawings, and claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a conceptual diagram illustrating an example system that includes one or more hearing instruments, in accordance with one or more techniques of this disclosure.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating example components of a hearing instrument, in accordance with one or more techniques of this disclosure.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating example components of a computing device, in accordance with one or more techniques of this disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart illustrating an example operation in accordance with one or more techniques described in this disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram illustrating example components of a hearing instrument and a computing device, in accordance with one or more techniques of this disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram illustrating example components of a hearing instrument, a computing device, and a wearable device, in accordance with one or more techniques of this disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0017" num="0016">Poor posture is a common cause of musculoskeletal pain and other health problems. Poor posture often involves excess curvature of the thoracic and cervical spine. Such excess curvature may hinder breathing, impede circulation of blood or other internal fluids, cause pinched nerves, cause muscle stiffness, cause bone loss, cause headaches, and cause other medical conditions. Poor posture may also be a psychiatric indicator. Certain postures may be markers of aging, muscular dystrophy, Parkinson's disease, and camptocormia. In contrast, certain types of postures may be healthier. For example, a neutral spine posture may be a healthier spinal position for sitting or standing. In the neutral spine posture, the cervical spine is bent anteriorly, the thoracic spine is bent posteriorly, and the lumbar spine is bent anteriorly within specific ranges.</p><p id="p-0018" num="0017">There are currently devices on the market for coaching users on proper posture. However, there are a number of problems associated with such devices. For instance, it is difficult for existing devices to discreetly provide information to users about their posture. For instance, users may not want other people to hear a device reminding the users to sit up straight. Similarly, providing haptic information causes audible sounds that may be disturbing or draw unwanted attention. Moreover, certain types of improper postures, such as forward jutting of the head, may be difficult to detect reliably using back-worn sensors or may be difficult to detect discreetly. Thus, posture detection and provision of information about the posture may be difficult or unreliable for such devices.</p><p id="p-0019" num="0018">Techniques of this disclosure may address one or more of these problems. As described herein, a processing system may obtain one or more signals that are generated by or generated based on data from one or more sensors that are included in one or more hearing instruments. The processing system may determine, based on the signals, whether a posture of a user of the one or more hearing instruments is a target posture for the user. In this disclosure, there may be different target postures for different activities. Additionally, the processing system may generate information about the posture of the user.</p><p id="p-0020" num="0019">The use of sensors in hearing instruments may address one or more of the problems mentioned above because these sensors are essentially at stable positions relative to the user's head and therefore may be able to detect postures that are otherwise not detectable or reliably detectable. Moreover, hearing instruments may be able to provide discreet audio information to users that other people are not able to hear. The techniques of this disclosure may be especially advantageous because poor posture is especially a problem among older adults, who are also the most likely to use hearing instruments, such as hearing aids. Thus, it would be less surprising to see a hearing instrument worn by an older adult, even if that older adult does not have hearing loss that would otherwise cause the older adult to use a hearing aid, thereby potentially avoiding stigma associated with wearable a head-mounted sensor device.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a conceptual diagram illustrating an example system <b>100</b> that includes hearing instruments <b>102</b>A, <b>102</b>B, in accordance with one or more techniques of this disclosure. This disclosure may refer to hearing instruments <b>102</b>A and <b>102</b>B collectively, as &#x201c;hearing instruments <b>102</b>.&#x201d; A user <b>104</b> may wear hearing instruments <b>102</b>. In some instances, user <b>104</b> may wear a single hearing instrument. In other instances, user <b>104</b> may wear two hearing instruments, with one hearing instrument for each ear of user <b>104</b>.</p><p id="p-0022" num="0021">Hearing instruments <b>102</b> may include one or more of various types of devices that are configured to provide auditory stimuli to user <b>104</b> and that are designed for wear and/or implantation at, on, or near an ear of user <b>104</b>. Hearing instruments <b>102</b> may be worn, at least partially, in the ear canal or concha. One or more of hearing instruments <b>102</b> may include behind the ear (BTE) components that are worn behind the ears of user <b>104</b>. In some examples, hearing instruments <b>102</b> include devices that are at least partially implanted into or integrated with the skull of user <b>104</b>. In some examples, one or more of hearing instruments <b>102</b> provides auditory stimuli to user <b>104</b> via a bone conduction pathway.</p><p id="p-0023" num="0022">In any of the examples of this disclosure, each of hearing instruments <b>102</b> may include a hearing assistance device. Hearing assistance devices include devices that help a user hear sounds in the user's environment. Example types of hearing assistance devices may include hearing aid devices, Personal Sound Amplification Products (PSAPs), cochlear implant systems (which may include cochlear implant magnets, cochlear implant transducers, and cochlear implant processors), bone-anchored or osseointegrated hearing aids, and so on. In some examples, hearing instruments <b>102</b> are over-the-counter, direct-to-consumer, or prescription devices. Furthermore, in some examples, hearing instruments <b>102</b> include devices that provide auditory stimuli to user <b>104</b> that correspond to artificial sounds or sounds that are not naturally in the environment of user <b>104</b>, such as recorded music, computer-generated sounds, or other types of sounds. For instance, hearing instruments <b>102</b> may include so-called &#x201c;hearables,&#x201d; earbuds, earphones, or other types of devices that are worn on or near the ears of user <b>104</b>. Some types of hearing instruments provide auditory stimuli to user <b>104</b> corresponding to sounds from the environment of user <b>104</b> and also artificial sounds.</p><p id="p-0024" num="0023">In some examples, one or more of hearing instruments <b>102</b> includes a housing or shell that is designed to be worn in the ear for both aesthetic and functional reasons and encloses the electronic components of the hearing instrument. Such hearing instruments may be referred to as in-the-ear (ITE), in-the-canal (ITC), completely-in-the-canal (CIC), or invisible-in-the-canal (IIC) devices. In some examples, one or more of hearing instruments <b>102</b> may be behind-the-ear (BTE) devices, which include a housing worn behind the ear that contains all of the electronic components of the hearing instrument, including the receiver (e.g., a speaker). The receiver conducts sound to an earbud inside the ear via an audio tube. In some examples, one or more of hearing instruments <b>102</b> are receiver-in-canal (RIC) hearing-assistance devices, which include housings worn behind the ears that contain electronic components and housings worn in the ear canals that contain receivers.</p><p id="p-0025" num="0024">Hearing instruments <b>102</b> may implement a variety of features that help user <b>104</b> hear better. For example, hearing instruments <b>102</b> may amplify the intensity of incoming sound, amplify the intensity of certain frequencies of the incoming sound, translate or compress frequencies of the incoming sound, and/or perform other functions to improve the hearing of user <b>104</b>. In some examples, hearing instruments <b>102</b> implement a directional processing mode in which hearing instruments <b>102</b> selectively amplify sound originating from a particular direction (e.g., to the front of user <b>104</b>) while potentially fully or partially canceling sound originating from other directions. In other words, a directional processing mode may selectively attenuate off-axis unwanted sounds. The directional processing mode may help user <b>104</b> understand conversations occurring in crowds or other noisy environments. In some examples, hearing instruments <b>102</b> use beamforming or directional processing cues to implement or augment directional processing modes.</p><p id="p-0026" num="0025">In some examples, hearing instruments <b>102</b> reduce noise by canceling out or attenuating certain frequencies. Furthermore, in some examples, hearing instruments <b>102</b> may help user <b>104</b> enjoy audio media, such as music or sound components of visual media, by outputting sound based on audio data wirelessly transmitted to hearing instruments <b>102</b>.</p><p id="p-0027" num="0026">Hearing instruments <b>102</b> may be configured to communicate with each other. For instance, in any of the examples of this disclosure, hearing instruments <b>102</b> may communicate with each other using one or more wireless communication technologies. Example types of wireless communication technology include Near-Field Magnetic Induction (NFMI) technology, 900 MHz technology, BLUETOOTH&#x2122; technology, WI-FI&#x2122; technology, audible sound signals, ultrasonic communication technology, infrared communication technology, inductive communication technology, or other types of communication that do not rely on wires to transmit signals between devices. In some examples, hearing instruments <b>102</b> use a 2.4 GHz frequency band for wireless communication. In examples of this disclosure, hearing instruments <b>102</b> may communicate with each other via non-wireless communication links, such as via one or more cables, direct electrical contacts, and so on.</p><p id="p-0028" num="0027">As shown in the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, system <b>100</b> may also include a computing system <b>106</b>. In other examples, system <b>100</b> does not include computing system <b>106</b>. Computing system <b>106</b> includes one or more computing devices, each of which may include one or more processors. For instance, computing system <b>106</b> may include one or more mobile devices (e.g., smartphones, tablet computers, etc.), server devices, personal computer devices, handheld devices, wireless access points, smart speaker devices, smart televisions, medical alarm devices, smart key fobs, smartwatches, motion or presence sensor devices, smart displays, screen-enhanced smart speakers, wireless routers, wireless communication hubs, prosthetic devices, mobility devices, special-purpose devices, hearing instrument accessory devices, and/or other types of devices. Hearing instrument accessory devices may include devices that are configured specifically for use with hearing instruments <b>102</b>. Example types of hearing instrument accessory devices may include charging cases for hearing instruments <b>102</b>, storage cases for hearing instruments <b>102</b>, media streamer devices, phone streamer devices, external microphone devices, remote controls for hearing instruments <b>102</b>, and other types of devices specifically designed for use with hearing instruments <b>102</b>.</p><p id="p-0029" num="0028">Actions described in this disclosure as being performed by computing system <b>108</b> may be performed by one or more of the computing devices of computing system <b>108</b>. One or more of hearing instruments <b>102</b> may communicate with computing system <b>108</b> using wireless or non-wireless communication links. For instance, hearing instruments <b>102</b> may communicate with computing system <b>108</b> using any of the example types of communication technologies described elsewhere in this disclosure.</p><p id="p-0030" num="0029">Furthermore, as shown in the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, system <b>100</b> may include a wearable device <b>107</b> separate from hearing instruments <b>102</b>. Wearable device <b>107</b> may include one or more processors <b>112</b>D. Furthermore, wearable device <b>107</b> may include one or more sensors <b>114</b>C. Wearable device <b>107</b> may be configured to communicate with one or more of hearing instruments <b>102</b> and/or one or more devices in computing system <b>106</b>. Wearable device <b>107</b> may include one of a variety of different types of devices. For example, wearable device <b>107</b> may be worn on a back of user <b>104</b>. In this example, wearable device <b>107</b> may be held onto the back of user <b>104</b> with an adhesive, held in place by straps or a garment, or otherwise held in position on the back of user <b>104</b>. In another example, wearable device <b>107</b> includes a pendant worn around a neck of user <b>104</b>. In another example, wearable device <b>107</b> is worn on a neck or a shoulder of user <b>104</b>.</p><p id="p-0031" num="0030">In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, hearing instrument <b>102</b>A includes a speaker <b>108</b>A, a microphone <b>110</b>A, one or more processors <b>112</b>A, and one or more sensors <b>114</b>A. Hearing instrument <b>102</b>B includes a speaker <b>108</b>B, a microphone <b>110</b>B, one or more processors <b>112</b>B, and one or more sensors <b>114</b>B. This disclosure may refer to speaker <b>108</b>A and speaker <b>108</b>B collectively as &#x201c;speakers <b>108</b>.&#x201d; This disclosure may refer to microphone <b>110</b>A and microphone <b>110</b>B collectively as &#x201c;microphones <b>110</b>.&#x201d; This disclosure may refer to sensors <b>114</b>A, sensors <b>114</b>B, and sensors <b>114</b>C collectively as &#x201c;sensors <b>114</b>.&#x201d; Computing system <b>106</b> includes one or more processors <b>112</b>C. Processors <b>112</b>C may be distributed among one or more devices of computing system <b>106</b>. This disclosure may refer to processors <b>112</b>A, <b>112</b>B, <b>112</b>C, and <b>112</b>D collectively as &#x201c;processors <b>112</b>.&#x201d; Processors <b>112</b> may be implemented in circuitry and may include microprocessors, application-specific integrated circuits, digital signal processors, or other types of circuits.</p><p id="p-0032" num="0031">As noted above, hearing instruments <b>102</b>A, <b>102</b>B, computing system <b>106</b>, and wearable device <b>107</b> may be configured to communicate with one another. Accordingly, processors <b>112</b> may be configured to operate together as a processing system <b>116</b>. Thus, discussion in this disclosure of actions performed by processing system <b>116</b> may be performed by one or more processors in one or more of hearing instrument <b>102</b>A, hearing instrument <b>102</b>B, computing system <b>106</b>, or wearable device <b>107</b>, either separately or in coordination. Moreover, it should be appreciated that processing system <b>116</b> does not need to include each of processors <b>112</b>A, <b>112</b>B, <b>112</b>C, <b>112</b>D. For instance, processing system <b>116</b> may be limited to processors <b>112</b>A and not processors <b>112</b>B, <b>112</b>C, or <b>112</b>D.</p><p id="p-0033" num="0032">It will be appreciated that hearing instruments <b>102</b>, computing system <b>106</b>, and/or wearable device <b>107</b> may include components in addition to those shown in the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, e.g., as shown in the examples of <figref idref="DRAWINGS">FIG. <b>2</b></figref> and <figref idref="DRAWINGS">FIG. <b>3</b></figref>. For instance, each of hearing instruments <b>102</b> may include one or more additional microphones configured to detect sound in an environment of user <b>104</b>. The additional microphones may include omnidirectional microphones, directional microphones, or other types of microphones.</p><p id="p-0034" num="0033">As described herein, processing system <b>116</b> may obtain one or more signals that are generated by or generated based on data from sensors <b>114</b>A, <b>114</b>B that are included in one or more of hearing instruments <b>102</b>A, <b>102</b>B. Processing system <b>116</b> may determine, based on the signals, a posture of a user of the hearing instruments <b>102</b>A, <b>102</b>B. For example, processing system <b>116</b> may determine, based on the signals, whether a posture of user <b>104</b> is a target posture for user <b>104</b>. Additionally, processing system <b>116</b> may generate information based on the posture of user <b>104</b>. For instance, processing system <b>116</b> may generate information that reminds user <b>104</b> to adopt the target posture of the current posture of user <b>104</b> is not the target posture.</p><p id="p-0035" num="0034">In some examples, processing system <b>116</b> may obtain one or more signals that are generated by wearable device <b>107</b>. For instance, processing system <b>116</b> may obtain signals that are generated by or based on data generated by sensors <b>114</b>C of wearable device <b>107</b>. Processing system <b>116</b> may determine the posture of user <b>104</b> based on the signals generated by or generated based on data from sensors <b>114</b>A, <b>114</b>B, and based on the signals generated by or generated based on data from sensors <b>114</b>C. Use of the signals generated by wearable device <b>107</b> may enhance the ability of processing system <b>116</b> to determine the posture of user <b>104</b>.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating example components of hearing instrument <b>102</b>A, in accordance with one or more aspects of this disclosure. Hearing instrument <b>102</b>B may include the same or similar components of hearing instrument <b>102</b>A shown in the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Thus, the discussion of <figref idref="DRAWINGS">FIG. <b>2</b></figref> may apply with respect to hearing instrument <b>102</b>B. In the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, hearing instrument <b>102</b>A includes one or more storage devices <b>202</b>, one or more communication units <b>204</b>, a receiver <b>206</b>, one or more processors <b>208</b>, one or more microphones <b>210</b>, sensors <b>114</b>A, a power source <b>214</b>, an external speaker <b>215</b>, and one or more communication channels <b>216</b>. Communication channels <b>216</b> provide communication between storage devices <b>202</b>, communication unit(s) <b>204</b>, receiver <b>206</b>, processor(s) <b>208</b>, microphone(s) <b>210</b>, sensors <b>114</b>A, external speaker <b>215</b>, and potentially other components of hearing instrument <b>102</b>A. Components <b>202</b>, <b>204</b>, <b>206</b>, <b>208</b>, <b>210</b>, <b>114</b>A, <b>215</b>, and <b>216</b> may draw electrical power from power source <b>214</b>.</p><p id="p-0037" num="0036">In the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, each of components <b>202</b>, <b>204</b>, <b>206</b>, <b>208</b>, <b>210</b>, <b>114</b>A, <b>214</b>, <b>215</b>, and <b>216</b> are contained within a single housing <b>218</b>. For instance, in examples where hearing instrument <b>102</b>A is a BTE device, each of components <b>202</b>, <b>204</b>, <b>206</b>, <b>208</b>, <b>210</b>, <b>114</b>A, <b>214</b>, <b>215</b>, and <b>216</b> may be contained within a behind-the-ear housing. In examples where hearing instrument <b>102</b>A is an ITE, ITC, CIC, or IIC device, each of components <b>202</b>, <b>204</b>, <b>206</b>, <b>208</b>, <b>210</b>, <b>114</b>A, <b>214</b>, <b>215</b>, and <b>216</b> may be contained within an in-ear housing. However, in other examples of this disclosure, components <b>202</b>, <b>204</b>, <b>206</b>, <b>208</b>, <b>210</b>, <b>114</b>A, <b>214</b>, <b>215</b>, and <b>216</b> are distributed among two or more housings. For instance, in an example where hearing instrument <b>102</b>A is a RIC device, receiver <b>206</b>, one or more of microphones <b>210</b>, and one or more of sensors <b>114</b>A may be included in an in-ear housing separate from a behind-the-ear housing that contains the remaining components of hearing instrument <b>102</b>A. In such examples, a RIC cable may connect the two housings.</p><p id="p-0038" num="0037">Furthermore, in the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, sensors <b>114</b>A include an inertial measurement unit (IMU) <b>226</b> that is configured to generate data regarding the motion of hearing instrument <b>102</b>A. IMU <b>226</b> may include a set of sensors. For instance, in the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, IMU <b>226</b> includes one or more accelerometers <b>228</b>, a gyroscope <b>230</b>, a magnetometer <b>232</b>, combinations thereof, and/or other sensors for determining the motion of hearing instrument <b>102</b>A. Furthermore, in the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, hearing instrument <b>102</b>A may include one or more additional sensors <b>236</b>. Additional sensors <b>236</b> may include a photoplethysmography (PPG) sensor, blood oximetry sensors, blood pressure sensors, electrocardiograph (EKG) sensors, body temperature sensors, electroencephalography (EEG) sensors, environmental temperature sensors, environmental pressure sensors, environmental humidity sensors, skin galvanic response sensors, and/or other types of sensors. As shown in the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, additional sensors <b>236</b> may include a barometer <b>237</b>. In other examples, hearing instrument <b>102</b>A and sensors <b>114</b>A may include more, fewer, or different components. Processing system <b>116</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) may use signals by sensor <b>114</b>A and/or data from sensors <b>114</b>A to determine a posture of user <b>104</b>.</p><p id="p-0039" num="0038">Storage device(s) <b>202</b> may store data. Storage device(s) <b>202</b> may include volatile memory and may therefore not retain stored contents if powered off. Examples of volatile memories may include random access memories (RAM), dynamic random access memories (DRAM), static random access memories (SRAM), and other forms of volatile memories known in the art. Storage device(s) <b>202</b> may include non-volatile memory for long-term storage of information and may retain information after power on/off cycles. Examples of non-volatile memory may include flash memories or forms of electrically programmable memories (EPROM) or electrically erasable and programmable (EEPROM) memories.</p><p id="p-0040" num="0039">Communication unit(s) <b>204</b> may enable hearing instrument <b>102</b>A to send data to and receive data from one or more other devices, such as a device of computing system <b>106</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>), another hearing instrument (e.g., hearing instrument <b>102</b>B), a hearing instrument accessory device, a mobile device, wearable device <b>107</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>), or other types of devices. Communication unit(s) <b>204</b> may enable hearing instrument <b>102</b>A to use wireless or non-wireless communication technologies. For instance, communication unit(s) <b>204</b> enable hearing instrument <b>102</b>A to communicate using one or more of various types of wireless technology, such as a BLUETOOTH&#x2122; technology, 3G, 4G, 4G LTE, 5G, ZigBee, WI-FI&#x2122;, Near-Field Magnetic Induction (NFMI), ultrasonic communication, infrared (IR) communication, or another wireless communication technology. In some examples, communication unit(s) <b>204</b> may enable hearing instrument <b>102</b>A to communicate using a cable-based technology, such as a Universal Serial Bus (USB) technology.</p><p id="p-0041" num="0040">Receiver <b>206</b> includes one or more speakers for generating audible sound. In the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, receiver <b>206</b> includes speaker <b>108</b>A (<figref idref="DRAWINGS">FIG. <b>1</b></figref>). The speakers of receiver <b>206</b> may generate sounds that include a range of frequencies. In some examples, the speakers of receiver <b>206</b> includes &#x201c;woofers&#x201d; and/or &#x201c;tweeters&#x201d; that provide additional frequency range. Receiver <b>206</b> may output audible information to user <b>104</b> about the posture of user <b>104</b>. As shown in the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, hearing instrument <b>102</b>A may also include an external speaker <b>215</b> that is configured to generate sound that is not directed into an ear canal of user <b>104</b>.</p><p id="p-0042" num="0041">Processor(s) <b>208</b> include processing circuits configured to perform various processing activities. Processor(s) <b>208</b> may process signals generated by microphone(s) <b>210</b> to enhance, amplify, or cancel-out particular channels within the incoming sound. Processor(s) <b>208</b> may then cause receiver <b>206</b> to generate sound based on the processed signals. In some examples, processor(s) <b>208</b> include one or more digital signal processors (DSPs). In some examples, processor(s) <b>208</b> may cause communication unit(s) <b>204</b> to transmit one or more of various types of data. For example, processor(s) <b>208</b> may cause communication unit(s) <b>204</b> to transmit data to computing system <b>106</b>. Furthermore, communication unit(s) <b>204</b> may receive audio data from computing system <b>106</b> and processor(s) <b>208</b> may cause receiver <b>206</b> to output sound based on the audio data. In the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, processor(s) <b>208</b> include processors <b>112</b>A (<figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0043" num="0042">Microphone(s) <b>210</b> detect incoming sound and generate one or more electrical signals (e.g., an analog or digital electrical signal) representing the incoming sound. In the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, microphones <b>210</b> include microphone <b>110</b>A (<figref idref="DRAWINGS">FIG. <b>1</b></figref>). In some examples, microphone(s) <b>210</b> include directional and/or omnidirectional microphones.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating example components of computing device <b>300</b>, in accordance with one or more aspects of this disclosure. <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates only one particular example of computing device <b>300</b>, and many other example configurations of computing device <b>300</b> exist. Computing device <b>300</b> may be a computing device in computing system <b>106</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0045" num="0044">As shown in the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, computing device <b>300</b> includes one or more processors <b>302</b>, one or more communication units <b>304</b>, one or more input devices <b>308</b>, one or more output device(s) <b>310</b>, a display screen <b>312</b>, a power source <b>314</b>, one or more storage device(s) <b>316</b>, and one or more communication channels <b>318</b>. Computing device <b>300</b> may include other components. For example, computing device <b>300</b> may include physical buttons, microphones, speakers, communication ports, and so on. Communication channel(s) <b>318</b> may interconnect each of components <b>302</b>, <b>304</b>, <b>308</b>, <b>310</b>, <b>312</b>, and <b>316</b> for inter-component communications (physically, communicatively, and/or operatively). In some examples, communication channel(s) <b>318</b> may include a system bus, a network connection, an inter-process communication data structure, or any other method for communicating data. Power source <b>314</b> may provide electrical energy to components <b>302</b>, <b>304</b>, <b>308</b>, <b>310</b>, <b>312</b> and <b>316</b>.</p><p id="p-0046" num="0045">Storage device(s) <b>316</b> may store information required for use during operation of computing device <b>300</b>. In some examples, storage device(s) <b>316</b> have the primary purpose of being a short-term and not a long-term computer-readable storage medium. Storage device(s) <b>316</b> may be volatile memory and may therefore not retain stored contents if powered off. In some examples, storage device(s) <b>316</b> includes non-volatile memory that is configured for long-term storage of information and for retaining information after power on/off cycles. In some examples, processor(s) <b>302</b> of computing device <b>300</b> may read and execute instructions stored by storage device(s) <b>316</b>.</p><p id="p-0047" num="0046">Computing device <b>300</b> may include one or more input devices <b>308</b> that computing device <b>300</b> uses to receive user input. Examples of user input include tactile, audio, and video user input. Input device(s) <b>308</b> may include presence-sensitive screens, touch-sensitive screens, mice, keyboards, voice responsive systems, microphones, motion sensors capable of detecting gestures (e.g., head nods or tapping), or other types of devices for detecting input from a human or machine.</p><p id="p-0048" num="0047">Communication unit(s) <b>304</b> may enable computing device <b>300</b> to send data to and receive data from one or more other computing devices (e.g., via a communication network, such as a local area network or the Internet). For instance, communication unit(s) <b>304</b> may be configured to receive data sent by hearing instrument(s) <b>102</b>, receive data generated by user <b>104</b> of hearing instrument(s) <b>102</b>, receive and send data, receive and send messages, and so on. In some examples, communication unit(s) <b>304</b> may include wireless transmitters and receivers that enable computing device <b>300</b> to communicate wirelessly with the other computing devices. For instance, in the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, communication unit(s) <b>304</b> include a radio <b>306</b> that enables computing device <b>300</b> to communicate wirelessly with other computing devices, such as hearing instruments <b>102</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>). Examples of communication unit(s) <b>304</b> may include network interface cards, Ethernet cards, optical transceivers, radio frequency transceivers, or other types of devices that are able to send and receive information. Other examples of such communication units may include BLUETOOTH&#x2122;, 3G, 4G, 5G, and WI-FI&#x2122; radios, Universal Serial Bus (USB) interfaces, etc. Computing device <b>300</b> may use communication unit(s) <b>304</b> to communicate with one or more hearing instruments (e.g., hearing instruments <b>102</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>, <figref idref="DRAWINGS">FIG. <b>2</b></figref>)). Additionally, computing device <b>300</b> may use communication unit(s) <b>304</b> to communicate with one or more other devices.</p><p id="p-0049" num="0048">Output device(s) <b>310</b> may generate output. Examples of output include tactile, audio, and video output. Output device(s) <b>310</b> may include presence-sensitive screens, sound cards, video graphics adapter cards, speakers, displays such as liquid crystal displays (LCD) or light emitting displays (LEDs), or other types of devices for generating output. Output device(s) <b>310</b> may include display screen <b>312</b>. In some examples, output device(s) <b>310</b> may include virtual reality, augmented reality, or mixed reality display devices.</p><p id="p-0050" num="0049">Processor(s) <b>302</b> may read instructions from storage device(s) <b>316</b> and may execute instructions stored by storage device(s) <b>316</b>. Execution of the instructions by processor(s) <b>302</b> may configure or cause computing device <b>300</b> to provide at least some of the functionality ascribed in this disclosure to computing device <b>300</b> or components thereof (e.g., processor(s) <b>302</b>). As shown in the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, storage device(s) <b>316</b> include computer-readable instructions associated with operating system <b>320</b>, application modules <b>322</b>A-<b>322</b>N (collectively, &#x201c;application modules <b>322</b>&#x201d;), and a companion application <b>324</b>.</p><p id="p-0051" num="0050">Execution of instructions associated with operating system <b>320</b> may cause computing device <b>300</b> to perform various functions to manage hardware resources of computing device <b>300</b> and to provide various common services for other computer programs. Execution of instructions associated with application modules <b>322</b> may cause computing device <b>300</b> to provide one or more of various applications (e.g., &#x201c;apps,&#x201d; operating system applications, etc.). Application modules <b>322</b> may provide applications, such as text messaging (e.g., SMS) applications, instant messaging applications, email applications, social media applications, text composition applications, and so on.</p><p id="p-0052" num="0051">Companion application <b>324</b> is an application that may be used to interact with hearing instruments <b>102</b>, view information about hearing instruments <b>102</b>, or perform other activities related to hearing instruments <b>102</b>, thus serving as a companion to hearing instruments <b>102</b>. Execution of instructions associated with companion application <b>324</b> by processor(s) <b>302</b> may cause computing device <b>300</b> to perform one or more of various functions. For example, execution of instructions associated with companion application <b>324</b> may cause computing device <b>300</b> to configure communication unit(s) <b>304</b> to receive data from hearing instruments <b>102</b> and use the received data to present data to a user, such as user <b>104</b> or a third-party user. In some examples, companion application <b>324</b> is an instance of a web application or server application. In some examples, such as examples where computing device <b>300</b> is a mobile device or other type of computing device, companion application <b>324</b> may be a native application.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart illustrating an example operation <b>400</b> in accordance with one or more techniques of this disclosure. Other examples of this disclosure may include more, fewer, or different actions. In some examples, actions in the flowcharts of this disclosure may be performed in parallel or in different orders.</p><p id="p-0054" num="0053">In the example of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, processing system <b>116</b> may obtain signals that are generated by or generated based on data from one or more sensors that are included in hearing instruments <b>102</b> (<b>402</b>). As described elsewhere in this disclosure, hearing instruments <b>102</b> are configured to output sound. Signals generated based on data from the sensors may include data that includes features extracted from signals directly produced by the sensors or data otherwise generated by processing the signals produced by the sensors.</p><p id="p-0055" num="0054">Furthermore, in the example of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, processing system <b>116</b> may determine, based on the signals, whether a posture of user <b>104</b> of hearing instruments <b>102</b> is a target posture (<b>404</b>). The target posture may be for a specific activity, such as sitting or standing. In some examples, the target posture is a neutral spine posture. In some examples, the target posture is a posture that is intermediate between a preintervention posture of user <b>104</b> and the neutral spine posture. In other examples, the target posture may be a supine posture, e.g., when the activity is sleeping. The target posture may be established by a healthcare professional, may be a preset position, may be established by user <b>104</b>, or may otherwise be established.</p><p id="p-0056" num="0055">Processing system <b>116</b> may determine whether the posture of user <b>104</b> is the target posture for an activity in various ways. For instance, in some examples, processing system <b>116</b> may store net displacement values that includes a net displacement value for each degree of freedom in a plurality of degrees of freedom. The degrees of freedom may correspond to anterior/posterior movement, superior/inferior movement, lateral movement, roll, pitch, and yaw. The net displacement value for a degree of freedom indicates a net amount of displacement in a direction corresponding to the degree of freedom. For instance, the net displacement value for an anterior/posterior degree of freedom may indicate that hearing instrument <b>102</b>A has moved 1.5 inches in the anterior direction. Additionally, in this example, processing system <b>116</b> performs a calibration process. As part of performing the calibration process, processing system <b>116</b> may reset the net displacement values based on receiving an indication (e.g., from user <b>104</b>, a clinician, or other person) that user <b>104</b> has assumed the target posture. For instance, processing system <b>116</b> may reset each of the net displacement values to 0. Subsequently, processing system <b>116</b> may update the net displacement values based on the signals. Processing system <b>116</b> may determine that user <b>104</b> has the target posture based on the net displacement values. For instance, processing system <b>116</b> may determine that user <b>104</b> has the target posture based on each of the net displacement values being within a respective predefined range of target values corresponding to the target posture. The respective predefined range is a range of net displacement values that may be consistent with user <b>104</b> having the target posture.</p><p id="p-0057" num="0056">Furthermore, in some examples where processing system <b>116</b> stores and uses net displacement values, processing system <b>116</b> may analyze the signals to identify segments of the signals corresponding to posture-related movements that are distinct from walking or other locomotion-related movements. For instance, processing system <b>116</b> may include a machine-learning model (e.g., an artificial neural network, etc.) that classifies segments of the signals as being associated with posture-related movements. Processing system <b>116</b> may update the net displacement values based only on segments of the signals that are associated with posture-related movements. This may allow processing system <b>116</b> to ensure that the net displacement values reflect the net displacement of hearing instruments <b>102</b> attributable to changes in the posture of user <b>104</b>, not overall displacement of user <b>104</b>.</p><p id="p-0058" num="0057">In some examples, to determine whether the posture of user <b>104</b> is the target posture, processing system <b>116</b> may determine, based on the signals, a current direction of gravity. For instance, in this example, the sensors may include one or more accelerometers (e.g., accelerometers <b>228</b> of IMU <b>226</b>) configured to detect acceleration caused by Earth's gravity. Furthermore, in this example, processing system <b>116</b> may perform a calibration process. As part of performing the calibration process, processing system <b>116</b> may establish, based on receiving an indication that user <b>104</b> has assumed the target posture (e.g., a voice command, a tapping input to one or more of hearing instruments <b>102</b>, a command to a mobile device, etc.), a gravity bias value for the target posture based on the current direction of gravity. The gravity bias value may indicate an angle between a predetermined axis of hearing instrument <b>102</b>A and a direction of gravitational acceleration. Establishing the gravity bias value for the target posture allows processing system <b>116</b> to determine what gravity bias value corresponds to the target posture. In this example, after calibrating the gravity bias value, processing system <b>116</b> updates a current gravity bias value based on subsequent information in the signals. Thus, as user <b>104</b> subsequently moves around, processing system <b>116</b> may update the current gravity bias value so that the gravity bias value continues to indicate the current angle between the predetermined axis of hearing instrument <b>102</b>A and the direction of gravitational acceleration. In this example, processing system <b>116</b> may determine whether the posture of user <b>104</b> is the target posture based on the gravity bias value for the target posture and the current gravity bias value. For example, processing system <b>116</b> may determine that the posture of user <b>104</b> is the target posture when the gravity bias value is consistent with the target posture (e.g., the current gravity bias value is within a range of the gravity bias value recorded during calibration). For instance, user <b>104</b> may not have the target posture (e.g., user <b>104</b> may have a poor posture) if an anterior tilt of the head of user <b>104</b> is too large. In some examples, determining that the gravity bias value is consistent with the target posture may be a necessary but not sufficient condition for determining that user <b>104</b> has the target posture. For instance, user <b>104</b> might not have the target posture if the head of user <b>104</b> is thrust forward but is held level.</p><p id="p-0059" num="0058">In some examples, the sensors include one or more microphones (e.g., microphones <b>210</b>) and one or more of hearing instruments <b>102</b> include a speaker (e.g., external speaker <b>215</b>). In such examples, processing system <b>116</b> may cause the speaker to periodically emit a sound, such as an ultrasonic or subsonic sound. The signals may include one or more audio signals detected by the microphones. Furthermore, in this example, processing system <b>116</b> may obtain information, via the microphones, indicating reflections of the sound emitted by the speaker in the one or more audio signals, e.g., by sending a signal to start detection of sound. During a calibration process, processing system <b>116</b> may instruct user <b>104</b> to assume the target posture and, in response to receiving an indication that user <b>104</b> has assumed the target posture, processing system <b>116</b> may cause the speaker to emit the sound and determine a delay of reflections of the sound detected by the microphones. The delay may be considered a delay for the target posture. Processing system <b>116</b> may determine whether user <b>104</b> has the target posture based in part on a delay of the detected reflections of subsequently sounds emitted by the speaker. For instance, processing system <b>116</b> may compare a current delay to the delay for the target posture to determine whether the head of user <b>104</b> has moved away from the target posture. In this example, the sound may reflect off horizontal surfaces (e.g., the floor or ceiling). If the head of user <b>104</b> has moved downward (e.g., due to poor posture), the delay of the detected reflections decreases. If the head of user <b>104</b> has moved upward, the delay of the detected reflections increases.</p><p id="p-0060" num="0059">In some examples, the sensors include a barometer (e.g., barometer <b>237</b> (<figref idref="DRAWINGS">FIG. <b>2</b></figref>)) and the signals include a signal from the barometer. In this example, processing system <b>116</b> may perform a calibration process. As part of performing the calibration process, processing system <b>116</b> may determine, based on receiving an indication that user <b>104</b> has assumed the target posture, a target altitude value based on the signal from the barometer. Altitude values may be in terms of barometric pressure, height above sea level, height relative to another level (e.g., a previous level), direction of altitude movement over time, or otherwise expressed. The target altitude value corresponds to an altitude of hearing instrument <b>102</b>A when user <b>104</b> is in the target posture. In this example, processing system <b>116</b> may determine, based on the signal from the barometer, an altitude of the one or more hearing instruments. Furthermore, in this example, processing system <b>116</b> may determine whether the posture of user <b>104</b> is the target posture based in part on the altitude of the one or more hearing instruments. For instance, the head of user <b>104</b> is typically lower when user <b>104</b> is not in the target posture. Thus, if the current altitude of the barometer is less than the target altitude value by at least a specific amount, processing system <b>116</b> may determine that user <b>104</b> does not have the target posture. In some examples, processing system <b>116</b> may obtain satellite-based navigation information (e.g., Global Positioning System (GPS) information) for one or more of hearing instruments <b>102</b>. In such examples, processing system <b>116</b> may use the satellite-based navigation information to establish a target altitude value when user <b>104</b> is in the target posture and may use later satellite-based navigation information for one or more of hearing instruments <b>102</b> to determine a current altitude. Processing system <b>116</b> may then use the current altitude and target altitude value as described above.</p><p id="p-0061" num="0060">In some examples, user <b>104</b> has a secondary device equipped with instruments for detecting altitude (e.g., a barometer, a satellite-based navigation system, etc.) that is separate from hearing instruments <b>102</b> (and in some examples, separate from wearable device <b>107</b>). For instance, the secondary device may be a mobile phone or other type of device that is typically carried near the waist of user <b>104</b>. Processing system <b>116</b> may establish target altitude values based on data from hearing instruments <b>102</b> and also the secondary device. Processing system <b>116</b> may later obtain current altitude values for hearing instruments <b>102</b> and the secondary device. Processing system <b>116</b> may determine a difference between the current altitude values. If the difference between the current altitude values is less than a threshold based on the difference between the target altitude values (e.g., the difference between the target altitude values minus x percent), processing system <b>116</b> may determine that user <b>104</b> does not have the target posture.</p><p id="p-0062" num="0061">In some examples, as part of determining whether the posture of user <b>104</b> is the target posture, processing system <b>116</b> may determine, based on one or more of the signals, whether a walking gait of user <b>104</b> is consistent with a camptocormia posture. In this example, processing system <b>116</b> may determine that user <b>104</b> does not have the target posture based on the walking gait of user <b>104</b> being consistent with the camptocormia posture. Processing system <b>116</b> may determine whether the walking gait of user <b>104</b> is consistent with the camptocormia posture based on IMU signals (e.g., signals from one or more IMUs of hearing instruments <b>102</b>) in the signals. When user <b>104</b> is walking in the camptocormia posture, the IMU signals will indicate a forward-backward swaying motion in the walking gait of user <b>104</b>. Thus, processing system <b>116</b> may determine that user <b>104</b> has the camptocormia posture when the IMU signals indicate that the forward-backward swaying motion in the walking gait of user <b>104</b> exceeds a threshold level.</p><p id="p-0063" num="0062">In some examples, the signals generated by or generated based on data from sensors in hearing instruments <b>102</b> may be considered first signals and processing system <b>116</b> may obtain one or more second signals that are generated by wearable device <b>107</b>, which are separate from the one or more hearing instruments <b>102</b>. Processing system <b>116</b> may determine whether user <b>104</b> has the target posture based on the first signals and the second signals.</p><p id="p-0064" num="0063">The second signals may include one or more types of information. For example, wearable device <b>107</b> may include a device that is worn on an upper back of user <b>104</b>. In this example, wearable device <b>107</b> may include an IMU or other sensors to determine a gravity bias value for wearable device <b>107</b>. The second signals may include the gravity bias value for wearable device <b>107</b>. By comparing the gravity bias value for wearable device <b>107</b> and a gravity bias value determined based on sensors in hearing instruments <b>102</b> to corresponding gravity bias values determined during a calibration process, processing system <b>116</b> may determine whether a posture of user <b>104</b> is the target posture.</p><p id="p-0065" num="0064">In some examples, wearable device <b>107</b> may be worn on a back of user <b>104</b> and wearable device <b>107</b> may generate a wireless signal detected by one or more of hearing instruments <b>102</b>. The second signals may include the wireless signal. One or more characteristics of the wireless signal may change based on a distance between wearable device <b>107</b> and hearing instruments <b>102</b>. For instance, if wearable device <b>107</b> is worn on the thoracic spine of user <b>104</b>, the distance between wearable device <b>107</b> and hearing instruments <b>102</b> may be minimized when hearing instruments <b>102</b> are directly superior to wearable device <b>107</b> and may increase as the head of user <b>104</b> moves anteriorly relative to wearable device <b>107</b>, as is typical when user <b>104</b> is slouching, thus the amplitude of the wireless signal may decrease when user <b>104</b> is slouching or a phase delay of the wireless signal as detected by hearing instruments <b>102</b> may be different depending on the distance of wearable device <b>107</b> from hearing instruments <b>102</b>. In another example, if user <b>104</b> is slouching, an amplitude of the wireless signal may be more attenuated than when user <b>104</b> is not slouching because the wireless signal may pass through or around more of the body of user <b>104</b> when the head of user <b>104</b> is held further in front of the shoulders of user <b>104</b>. Thus, in any of these examples, as part of determining whether the posture of user <b>104</b> is the target posture, processing system <b>116</b> may determine, based on one or more characteristics of the wireless signal, whether the posture of user <b>104</b> is the target posture.</p><p id="p-0066" num="0065">In another examples, wearable device <b>107</b> includes a pendant worn around a neck of user <b>104</b>. In this example, if user <b>104</b> is slouched forward, the pendant may swing in an anterior/posterior direction. However, if user <b>104</b> is sitting or walking with a neutral spine posture, the pendant may rest against the chest of user <b>104</b> or a swing of the pendant is interrupted by the chest or neck of user <b>104</b>. Accordingly, in this example, wearable device <b>107</b> may include an IMU or other types of sensors to detect swinging of the pendant. Processing system <b>116</b> may obtain signals generated by sensors of wearable device <b>107</b> and determine, based at least in part on these signals, whether the posture of user <b>104</b> is the target posture. For instance, processing system <b>116</b> may determine that user <b>104</b> is not in the target posture if the signals generated by the IMU of wearable device <b>107</b> indicate an uninterrupted swinging motion of the pendant.</p><p id="p-0067" num="0066">In the examples of this disclosure that involve a calibration process, such calibration processes may occur multiple times. For instance, a calibration process may occur each time sensor device <b>107</b> is put on user <b>104</b>. In some examples, the calibration process may be performed on a periodic basis (e.g., daily, weekly, etc.). In some examples, the calibration process may occur when processing system <b>104</b> determines that user <b>104</b> has changed activities.</p><p id="p-0068" num="0067">In some examples, processing system <b>116</b> may obtain signals from multiple wearable devices in addition to hearing instruments <b>102</b>. For example, processing system <b>116</b> may obtain signals from one or more wearable devices worn on the back, chest, or shoulders of user <b>104</b> and/or obtain signals from a wearable device that includes a neck-worn pendant, e.g., as described elsewhere in this disclosure, or other wearable device.</p><p id="p-0069" num="0068">In some examples, processing system <b>116</b> may determine whether user <b>104</b> is in a target posture based on multiple factors. For example, processing system <b>116</b> may determine based on a gravity bias value whether the current posture of user <b>104</b> is inconsistent with the target posture. Furthermore, in this example, processing system <b>116</b> may determine based on net displacement values whether the current posture of user <b>104</b> is the target posture. In this example, processing system <b>116</b> may make the determination that the posture of user <b>104</b> is the target posture if both of these two factors indicate that the posture of user <b>104</b> is the target posture and may determine that the posture of user <b>104</b> is not the target posture if either or both of these two factors indicate that the posture of user <b>104</b> is not the target posture.</p><p id="p-0070" num="0069">In some examples, user <b>104</b> may have different target postures for different activities, such as sitting, standing, walking, playing a sport, and so on. Processing system <b>116</b> may automatically identify the activity of user <b>104</b> based on signals generated by or generated based on data from sensors in one or more of hearing instruments <b>102</b> and/or wearable device <b>107</b>. Processing system <b>116</b> may determine whether the posture of user <b>104</b> is the target posture for the detected activity. In some examples, processing system <b>116</b> may receive an indication of user input specifying the activity.</p><p id="p-0071" num="0070">Processing system <b>116</b> may generate information based on the posture of user <b>104</b> (<b>406</b>). Generating information may include calculating, selecting, determining, or arriving at the information. Processing system <b>116</b> may generate the information based on the posture of user <b>104</b> in various ways. For instance, in one example, processing system <b>116</b> may generate information that causes one or more of hearing instruments <b>102</b> to output auditory information regarding the posture of user <b>104</b> to user <b>104</b> while user <b>104</b> is using hearing instruments <b>102</b>. In this example, because hearing instruments <b>102</b> provide the auditory information to user <b>104</b>, typically only user <b>104</b> is able to hear the auditory information. Thus, people around user <b>104</b> are not aware of the auditory information to user <b>104</b> and the auditory information is therefore more discreet. The auditory information may instruct user <b>104</b> to assume a target posture. In some examples, the auditory information may include information about whether user <b>104</b> has met posture goals for user <b>104</b>. In some examples, the auditory information may include information about how much time user <b>104</b> should spend in the target posture an upcoming time period (e.g., 1 hour) in order to meet the posture goals for user <b>104</b>. The auditory information may take the form of verbal information or non-verbal audible cues, such as beeps or tones.</p><p id="p-0072" num="0071">In some examples, processing system <b>116</b> may generate an email message or other type of message, such as an app-based notification or text message, containing the information based on the posture of user <b>104</b>. In such examples, the message may include statistical information about the posture of user <b>104</b>. For instance, the statistical information may include information about an amount of time user <b>104</b> spent in a target posture versus an amount of time user <b>104</b> was not in the target posture.</p><p id="p-0073" num="0072">In some examples, processing system <b>116</b> may provide the generated information to one or more computing devices. For example, processing system <b>116</b> may provide the generated information to a computing device that uses the information for statistical analysis or scientific research.</p><p id="p-0074" num="0073">In some examples, generating the information based on the posture of user <b>104</b> may involve generating feedback to user <b>104</b> about whether the posture of user <b>104</b> is the target posture. For instance, the feedback may inform user <b>104</b> that user <b>104</b> is or is not currently in the target posture.</p><p id="p-0075" num="0074">Furthermore, in some examples, the target posture is a target posture for sleeping. In this example, processing system <b>116</b> may determine whether user <b>104</b> is asleep. Processing system <b>116</b> may determine whether user <b>104</b> is asleep in one or more of a variety of ways. For instance, in one example, processing system <b>116</b> may obtain signals (e.g., from sensors, such as sensors of IMU <b>226</b>, inward-facing microphones, and/or other types of sensors, in hearing instruments <b>102</b>) regarding a respiration pattern of user <b>104</b>. In this example, processing system <b>116</b> may determine that user <b>104</b> is asleep based on the respiration pattern of user <b>104</b> being consistent with user <b>104</b> being asleep. In some examples, processing system <b>116</b> may obtain EEG signals (e.g., from EEG sensors included in one or more of hearing instruments <b>102</b>). Processing system <b>116</b> may determine that user <b>104</b> is asleep based on the EEG signals. In some examples, processing system <b>116</b> may implement an artificial neural network or other machine-learning system that determines whether the obtained signals indicate that user <b>104</b> is asleep.</p><p id="p-0076" num="0075">In addition to determining whether user <b>104</b> is asleep, processing system <b>116</b> may also determine whether user <b>104</b> is in a target posture for sleep. For instance, sleeping in a sitting posture may be associated with muscle stiffness, neck pain, shoulder pain, and other symptoms. Furthermore, sleeping in a prone posture (e.g., as opposed to a supine posture), or vice versa, may be associated with breathing difficulties or snoring. Accelerometers in hearing instruments <b>102</b> may differentiate the supine, prone, and upright postures based on directions of gravitational bias. In accordance with one or more techniques of this disclosure, processing system <b>116</b> may generate the information based on the posture of user <b>104</b> based on user <b>104</b> being asleep and not having the target posture for sleeping.</p><p id="p-0077" num="0076">For example, if processing system <b>116</b> determines that user <b>104</b> is sleeping and is not in the target posture for sleep, processing system <b>116</b> may cause one or more of hearing instruments <b>102</b> to output audio data encouraging user <b>104</b> to change to the target posture for sleep. In some examples, if processing system <b>116</b> determines that user <b>104</b> is sleeping and is not in the target posture for sleep, processing system <b>116</b> may activate one or more haptic information units in one or more of hearing instruments <b>102</b> to encourage user <b>104</b> to change to the target posture for sleep. In some examples, if processing system <b>116</b> determines that user <b>104</b> is sleeping and user <b>104</b> is not in the target posture for sleep, processing system <b>116</b> may output a command to an adjustable bed (e.g., adjustable mechanically or by air) used by user <b>104</b> to elevate or lower a portion of the bed in a way that promotes the target posture (e.g., elevating a head portion of the bed may cause user <b>104</b> to shift out of a prone posture and into a supine posture). In a similar example, if processing system <b>116</b> determines that user <b>104</b> is sleeping in an upright position in a piece of adjustable furniture (e.g., an adjustable chair, bed, hospital bed), processing system <b>116</b> may automatically output a command to the piece of adjustable furniture to gently lower a portion of the piece of adjustable furniture to a more appropriate sleeping position.</p><p id="p-0078" num="0077">In some examples, if processing system <b>116</b> determines that user <b>104</b> is sleeping and user <b>104</b> is not in the target posture for sleep, processing system <b>116</b> may generate a message (e.g., email message, notification message, etc.) informing user <b>104</b> of the benefit of sleeping in the target posture for sleep, informing user <b>104</b> of various statistics regarding user <b>104</b> sleeping in or not in the target posture for sleep, and so on. Determining whether user <b>104</b> is in a target posture for sleep using sensors in hearing devices may be especially advantageous in examples where the hearing devices are ITE, CIC, or IIC devices because such devices might be less disruptive to the sleep of user <b>104</b> than other types of sensor devices.</p><p id="p-0079" num="0078">In some examples where the information generated by processing system <b>116</b> is provided to one or more people other than user <b>104</b>, such information may be used to determine whether a medical intervention, such as a physical or mental wellness check, may be desirable. For example, development of poor posture may be a sign of depression. Accordingly, in this example, a healthcare provider or family member receiving information indicating that user <b>104</b> has developed poor (e.g., non-target posture) may want to check in on the mental wellness of user <b>104</b>. Similarly, camptocormia or other disease in user <b>104</b> may develop over time. The information generated by processing system <b>116</b> may be evaluated by a healthcare provider or other person who may then arrange a physical exam of user <b>104</b> to check whether user <b>104</b> has developed camptocormia or other disease. In another example, if user <b>104</b> complains of headaches, muscle aches, neck soreness, or other associated problems, a clinician or other type of person may review the information about the posture of user <b>104</b> generated by processing system <b>116</b> to determine whether the posture of user <b>104</b> may be a factor in development of such problems. In some such examples, processing system <b>116</b> does not necessarily compare the posture of user <b>104</b> to a target posture. Rather, in some such examples, the information generated by processing system <b>116</b> about the posture of user <b>104</b> may include other types of information, such as a head angle relative to the thoracic or cervical spine of user <b>104</b>, etc. In some examples, the target posture may be considered to be a reference posture instead of a posture that user <b>104</b> is attempting to attain.</p><p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram illustrating example components of hearing instrument <b>102</b>A and computing device <b>300</b>, in accordance with one or more techniques of this disclosure. <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram illustrating example components of hearing instrument <b>102</b>A, computing device <b>300</b>, and wearable device <b>107</b>, in accordance with one or more techniques of this disclosure. <figref idref="DRAWINGS">FIG. <b>5</b></figref> and <figref idref="DRAWINGS">FIG. <b>6</b></figref> are provided to illustrate example locations of sensors and examples of how functionality of processing system <b>116</b> may be distributed among hearing instruments <b>102</b>, computing system <b>106</b>, and wearable device <b>107</b>. Although <figref idref="DRAWINGS">FIG. <b>5</b></figref> and <figref idref="DRAWINGS">FIG. <b>6</b></figref> are described with respect to hearing instrument <b>102</b>A such description may apply to hearing instrument <b>102</b>B, or functionality ascribed in this disclosure to hearing instrument <b>102</b>A may be distributed between hearing instrument <b>102</b>A and hearing instrument <b>102</b>B.</p><p id="p-0081" num="0080">In the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, computing device <b>500</b> may be a computing device in computing system <b>106</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>). Furthermore, in the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, hearing instrument <b>102</b>A includes IMU <b>226</b>, a feature extraction unit <b>502</b>, a progress analysis unit <b>504</b>, a reminder unit <b>506</b>, and an audio information unit <b>508</b>. Computing device <b>500</b> includes an application <b>510</b>, a progress analysis unit <b>512</b>, a cloud logging unit <b>514</b>, a training program plan <b>516</b>, and educational content <b>518</b>.</p><p id="p-0082" num="0081">In the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, feature extraction unit <b>502</b> may obtain signals, e.g., from IMU <b>226</b>, and extract features from the signals. The extracted features may include data indicating relevant aspects of the signals. For example, feature extraction unit <b>502</b> may determine a current gravity bias value or current net displacement values based on signals from IMU <b>226</b>. In such examples, hearing instrument <b>102</b>A may transmit the extracted features to computing device <b>500</b>.</p><p id="p-0083" num="0082">Application <b>510</b> of computing device <b>500</b> may be one of application modules <b>322</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>) or companion application <b>324</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>). Application <b>510</b> of computing device <b>500</b> may determine, based on the extracted features and/or other data, whether a posture of user <b>104</b> is a target posture for an activity of user <b>104</b> (e.g., sitting, standing, sleeping, etc.). Application <b>510</b> may also generate information based on the posture of user <b>104</b>. Generating the information may include causing audio information unit <b>508</b> of hearing instrument <b>102</b>A to output audible information to user <b>104</b> while user <b>104</b> is using hearing instrument <b>102</b>A.</p><p id="p-0084" num="0083">Progress analysis unit <b>504</b> of hearing instrument <b>102</b>A and/or progress analysis unit <b>512</b> of computing device <b>300</b> may determine whether user <b>104</b> is progressing toward a posture goal for user <b>104</b>. User <b>104</b> or another person may set the posture goal for user <b>104</b>. The posture goal may specify a desired amount or percentage of time during a time period (e.g., an hour, day, week, etc.) in which user <b>104</b> is to have a target posture, such as a neutral spine posture. In some instances, the target posture may be a posture that is not a neutral spine posture but is an improved posture relative to a previous posture assumed by user <b>104</b>. Because long periods of poor posture may result in weakened muscles or ingrained habits, it may not be comfortable or reasonable for user <b>104</b> to maintain the target posture at all times. Rather, user <b>104</b> may need to work toward the posture goal for user <b>104</b> over time.</p><p id="p-0085" num="0084">Reminder unit <b>506</b> of hearing instrument <b>102</b>A may provide automated reminders to user <b>104</b> to assume the target posture. In some examples, reminder unit <b>506</b> may provide reminders to user <b>104</b> using audio information unit <b>508</b>. Reminder unit <b>506</b> may provide reminders to user <b>104</b> on a periodic basis. In some examples, reminder unit <b>104</b> may provide reminders to user <b>104</b> on an event-driven basis. For instance, reminder unit <b>506</b> may provide reminders to user <b>104</b> when user <b>104</b> transitions from a sitting posture to a standing posture, or vice versa.</p><p id="p-0086" num="0085">In examples where computing device <b>300</b> is a mobile device of user <b>104</b>, such as a mobile phone, a hearing instrument accessory device for hearing instruments <b>102</b>A, or another type of device close to hearing instruments <b>102</b>, cloud logging unit <b>514</b> may upload data regarding the posture of user <b>104</b> to a cloud-based computing data, e.g., for storage and/or further processing.</p><p id="p-0087" num="0086">In the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, computing device <b>300</b> may store one or more of a training program plan <b>516</b> and educational content <b>518</b>. Training program plan <b>516</b> may include data, such as text, images, and/or videos, that describe a plan for user <b>104</b> to improve the posture of user <b>104</b>. Training program plan <b>516</b> may include data that is specific to user <b>104</b>. Educational content <b>518</b> may include content to educate user <b>104</b> or other people about posture. In some examples, education content <b>518</b> is not specific to user <b>104</b>. Computing device <b>300</b> may output training program plan <b>516</b> and/or educational content <b>518</b> for display (e.g., on display screen <b>312</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>)) and/or provide audio output (e.g., using one or more of output devices <b>310</b>) (<figref idref="DRAWINGS">FIG. <b>3</b></figref>)) based on training program plan <b>516</b> and/or educational content <b>518</b>. In some examples, computing device <b>300</b> may cause hearing instruments <b>102</b> to output audio based on training program plan <b>516</b> and/or educational content <b>518</b>.</p><p id="p-0088" num="0087">In the example of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, hearing instrument <b>102</b>A (or hearing instrument <b>102</b>B) includes IMU <b>226</b>, feature extraction unit <b>502</b>, reminder unit <b>506</b>, audio information unit <b>508</b>. Furthermore, in the example of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, computing device <b>300</b> includes application <b>510</b>, progress analysis unit <b>512</b>, cloud logging unit <b>514</b>, training program plan <b>516</b>, and educational content <b>518</b>. IMU <b>226</b>, feature extraction unit <b>502</b>, reminder unit <b>506</b>, audio information unit <b>508</b>, application <b>510</b>, progress analysis unit <b>512</b>, cloud logging unit <b>514</b>, training program plan <b>516</b>, and educational content <b>518</b> may have the same roles and functions as described above with respect to <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0089" num="0088">However, in the example of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, wearable device <b>107</b> includes an IMU <b>600</b>, a feature extraction unit <b>602</b>, a motion information unit <b>604</b>, and a reminder unit <b>606</b>. IMU <b>600</b> may detect motion of wearable device <b>107</b>. IMU <b>600</b> may be implemented in a similar way as IMU <b>226</b> (<figref idref="DRAWINGS">FIG. <b>2</b></figref>). Like feature extraction unit <b>502</b>, feature extraction unit <b>602</b> may extract features from signals from IMU <b>600</b> and/or other sensors. Feature extraction unit <b>602</b> may transmit the extracted features to hearing instrument <b>102</b>A or computing device <b>300</b>. In some examples, wearable device <b>107</b> may receive features extracted by feature extraction unit <b>502</b> from hearing instrument <b>102</b>A. In some such examples, wearable device <b>107</b> may determine based in part on the received features and extracted features from signals of sensors in wearable device <b>107</b> (e.g., IMU <b>600</b>) whether a posture of user <b>104</b> is a target posture.</p><p id="p-0090" num="0089">Motion information unit <b>604</b> of wearable device <b>107</b> may detect if user <b>104</b> is static or in transit (e.g., walking or running) to adjust the analysis accordingly. In some examples, if user <b>104</b> is in transit, application <b>510</b> does not determine whether the posture of user <b>104</b> is the target posture. In some examples, different target postures may be established for user <b>104</b> for times when user <b>104</b> is in transit and times when user <b>104</b> is not in transit. In such examples, application <b>510</b> may evaluate the posture of user <b>104</b> using the appropriate target posture for whether user <b>104</b> is static or in transit.</p><p id="p-0091" num="0090">Furthermore, in the example of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, reminder unit <b>606</b> of wearable device <b>107</b> may provide reminders to user <b>104</b> about the posture of user <b>104</b>. In some examples, reminder unit <b>606</b> may provide the reminders to user <b>104</b> instead of reminder unit <b>506</b>. In some examples, reminder unit <b>606</b> may provide audible or haptic reminders about the posture of user <b>104</b>.</p><p id="p-0092" num="0091">In this disclosure, ordinal terms such as &#x201c;first,&#x201d; &#x201c;second,&#x201d; &#x201c;third,&#x201d; and so on, are not necessarily indicators of positions within an order, but rather may be used to distinguish different instances of the same thing. Examples provided in this disclosure may be used together, separately, or in various combinations. Furthermore, with respect to examples that involve personal data regarding a user, it may be required that such personal data only be used with the permission of the user.</p><p id="p-0093" num="0092">It is to be recognized that depending on the example, certain acts or events of any of the techniques described herein can be performed in a different sequence, may be added, merged, or left out altogether (e.g., not all described acts or events are necessary for the practice of the techniques). Moreover, in certain examples, acts or events may be performed concurrently, e.g., through multi-threaded processing, interrupt processing, or multiple processors, rather than sequentially.</p><p id="p-0094" num="0093">In one or more examples, the functions described may be implemented in hardware, software, firmware, or any combination thereof. If implemented in software, the functions may be stored on or transmitted over, as one or more instructions or code, a computer-readable medium and executed by a hardware-based processing unit. Computer-readable media may include computer-readable storage media, which corresponds to a tangible medium such as data storage media, or communication media including any medium that facilitates transfer of a computer program from one place to another, e.g., according to a communication protocol. In this manner, computer-readable media generally may correspond to (1) tangible computer-readable storage media which is non-transitory or (2) a communication medium such as a signal or carrier wave. Data storage media may be any available media that can be accessed by one or more computers or one or more processing circuits to retrieve instructions, code and/or data structures for implementation of the techniques described in this disclosure. A computer program product may include a computer-readable medium.</p><p id="p-0095" num="0094">By way of example, and not limitation, such computer-readable storage media may include RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage, or other magnetic storage devices, flash memory, cache memory, or any other medium that can be used to store desired program code in the form of instructions or store data structures and that can be accessed by a computer. Also, any connection is properly termed a computer-readable medium. For example, if instructions are transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium. It should be understood, however, that computer-readable storage media and data storage media do not include connections, carrier waves, signals, or other transient media, but are instead directed to non-transient, tangible storage media. Disk and disc, as used herein, includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), and Blu-ray disc, where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable media.</p><p id="p-0096" num="0095">Functionality described in this disclosure may be performed by fixed function and/or programmable processing circuitry. For instance, instructions may be executed by fixed function and/or programmable processing circuitry. Such processing circuitry may include one or more processors, such as one or more digital signal processors (DSPs), general purpose microprocessors, application specific integrated circuits (ASICs), field programmable logic arrays (FPGAs), or other equivalent integrated or discrete logic circuitry. Accordingly, the term &#x201c;processor,&#x201d; as used herein may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described herein. In addition, in some aspects, the functionality described herein may be provided within dedicated hardware and/or software modules. Also, the techniques could be fully implemented in one or more circuits or logic elements. Processing circuits may be coupled to other components in various ways. For example, a processing circuit may be coupled to other components via an internal device interconnect, a wired or wireless network connection, or another communication medium.</p><p id="p-0097" num="0096">The techniques of this disclosure may be implemented in a wide variety of devices or apparatuses, an integrated circuit (IC) or a set of ICs (e.g., a chip set). Various components, modules, or units are described in this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques, but do not necessarily require realization by different hardware units. Rather, as described above, various units may be combined in a hardware unit or provided by a collection of interoperative hardware units, including one or more processors as described above, in conjunction with suitable software and/or firmware.</p><p id="p-0098" num="0097">Various examples have been described. These and other examples are within the scope of the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>obtaining, by a processing system, signals that are generated by or generated based on data from one or more sensors that are included in one or more hearing instruments;</claim-text><claim-text>determining, by the processing system, based on the signals, whether a posture of a spine of a user of the one or more hearing instruments is a target posture for the user; and</claim-text><claim-text>generating, by the processing system, information based on the posture of the user.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. (canceled)</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the method further comprises:<claim-text>storing, by the processing system, net displacement values that include a net displacement value for each degree of freedom in a plurality of degrees of freedom;</claim-text><claim-text>performing a calibration process, wherein performing the calibration process comprises, based on receiving an indication that the user has assumed the target posture, resetting, by the processing system, the net displacement values; and</claim-text><claim-text>updating, by the processing system, the net displacement values based on the signals, and</claim-text></claim-text><claim-text>determining whether the posture of the spine of the user is the target posture comprises determining, by the processing system, that the posture of the spine of the user is the target posture based on the net displacement values.</claim-text></claim-text></claim><claim id="CLM-004-8" num="004-8"><claim-text><b>4</b>-<b>8</b>. (canceled)</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the signals are first signals,</claim-text><claim-text>the method further comprises obtaining, by the processing system, one or more second signals that are generated by a wearable device separate from the one or more hearing instruments, and</claim-text><claim-text>determining whether the posture of the spine of the user is the target posture comprises determining, by the processing system, whether the posture of the spine of the user is the target posture based on the first signals and the second signals.</claim-text></claim-text></claim><claim id="CLM-10-11" num="10-11"><claim-text><b>10</b>-<b>11</b>. (canceled)</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein:<claim-text>determining whether the posture of the spine of the user is the target posture comprises determining, by the processing system, based on a change to a characteristic of a wireless signal in the second signals, a distance of one or more of the hearing instruments relative to the wearable device; and</claim-text><claim-text>determining whether the posture of the spine of the user is the target posture comprises determining, by the processing system, based at least in part on the distance, whether the posture of the spine of the user is the target posture.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the method further comprises determining, by the processing system, whether the user is asleep, and</claim-text><claim-text>generating the information comprises generating, by the processing system, the information based on the user being asleep.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the information comprises causing the one or more hearing instruments to output auditory information regarding the posture of the spine of the user to the user while the user is using the one or more hearing instruments.</claim-text></claim><claim id="CLM-15-16" num="15-16"><claim-text><b>15</b>-<b>16</b>. (canceled)</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A system comprising:<claim-text>one or more hearing instruments, wherein the one or more hearing instruments include sensors; and</claim-text><claim-text>a processing system comprising one or more processors implemented in circuitry, wherein the one or more processors are configured to:<claim-text>obtain signals that are generated by or generated based on data from the sensors;</claim-text><claim-text>determine, based on the signals, whether a posture of a spine of a user of the one or more hearing instruments is a target posture of the spine of the user; and</claim-text><claim-text>generate information based on the posture of the spine of the user.</claim-text></claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein:<claim-text>the hearing instruments include one or more of the processors, or</claim-text><claim-text>the system comprises a computing device that includes one or more of the processors, wherein the computing device is one of: a mobile device, a hearing instrument accessory device, a server, or a computer.</claim-text></claim-text></claim><claim id="CLM-19-20" num="19-20"><claim-text><b>19</b>-<b>20</b>. (canceled)</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the target posture is a neutral spine posture.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein:<claim-text>the system comprises one or more storage devices configured to store net displacement values that includes a net displacement value for each degree of freedom in a plurality of degrees of freedom, and</claim-text><claim-text>the processing system is configured to:<claim-text>perform a calibration process, wherein the processing system is configured to, as part of performing the calibration process and based on receiving an indication that the user has assumed the target posture, reset the net displacement values;</claim-text><claim-text>update the net displacement values based on the signals; and</claim-text><claim-text>determine that the posture of the spine of the user is the target posture based on the net displacement values.</claim-text></claim-text></claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The system of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the one or more processors are configured to determine that the posture of the spine of the user is the target posture based on each of the net displacement values being within a respective predefined range.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein:<claim-text>the one or more processors are further configured to:<claim-text>determine, based on the signals, a current direction of gravity;</claim-text><claim-text>based on receiving an indication that the user has assumed the target posture, establish a gravity bias value for the target posture based on the current direction of gravity; and</claim-text><claim-text>after establishing the gravity bias value for the target posture, update a current gravity bias value based on subsequent information in the signals, and</claim-text></claim-text><claim-text>the one or more processors are configured to, as part of determining whether the posture of the spine of the user is the target posture, determine, based on the current gravity bias value and the gravity bias value for the target posture, whether the posture of the spine of the user is the target posture.</claim-text></claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein:<claim-text>the sensors include one or more microphones,</claim-text><claim-text>one or more of the one or more hearing instruments includes a speaker, and</claim-text><claim-text>the one or more processors are further configured to:<claim-text>cause the speaker to periodically emit a sound, wherein the signals include one or more audio signals detected by the one or more microphones;</claim-text><claim-text>obtain information, via the one or more microphones, indicating reflections of the sound emitted by the speaker in the one or more audio signals; and</claim-text><claim-text>determine whether the posture of the spine of the user is the target posture based in part on a delay of the reflections of the sound.</claim-text></claim-text></claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein:<claim-text>the sensors include a barometer,</claim-text><claim-text>the signals include a signal from the barometer, and</claim-text><claim-text>the one or more processors are further configured to:<claim-text>based on receiving an indication that the user has assumed the target posture, determine a target altitude value based on the signal from the barometer;</claim-text><claim-text>determine, based on the signal from the barometer, an altitude of the one or more hearing instruments; and</claim-text><claim-text>determine whether the posture of the spine of the user is the target posture based in part on the altitude of the one or more hearing instruments.</claim-text></claim-text></claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the one or more processors are configured such that, as part of determining whether the posture of the spine of the user is the target posture, the one or more processors:<claim-text>determine, based on one or more of the signals, whether a walking gait of the user is consistent with a camptocormia posture; and</claim-text><claim-text>determine that the posture of the spine of the user is not the target posture based on the walking gait of the user being consistent with the camptocormia posture.</claim-text></claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein:<claim-text>the system further comprises a wearable device separate from the one or more hearing instruments,</claim-text><claim-text>the signals are first signals,</claim-text><claim-text>the one or more processors are further configured to:<claim-text>obtain second signals that are generated by the wearable device, and</claim-text><claim-text>determine whether the posture of the spine of the user is the target posture based on the first signals and the second signals.</claim-text></claim-text></claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. The system of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the wearable device is wearable on a back of the user.</claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The system of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the wearable device is a pendant that is wearable around a neck of the user.</claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. The system of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein:<claim-text>the one or more processors are configured to, as part of determining whether the posture of the spine of the user is the target posture, determine, based on a change to a characteristic of a wireless signal in the second signals, a distance of one or more of the hearing instruments relative to the wearable device; and</claim-text><claim-text>determine whether the posture of the spine of the user is the target posture comprises determining, by the processing system, based at least in part on the distance, whether the posture of the spine of the user is the target posture.</claim-text></claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the one or more processors are further configured to:<claim-text>determine whether the user is asleep; and</claim-text><claim-text>generate the information based on the user being asleep.</claim-text></claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the one or more processors are configured to cause the one or more of the hearing instruments to output auditory information regarding the posture of the spine of the user to the user while the user is using the hearing instruments.</claim-text></claim><claim id="CLM-34-37" num="34-37"><claim-text><b>34</b>-<b>37</b>. (canceled)</claim-text></claim><claim id="CLM-00038" num="00038"><claim-text><b>38</b>. A non-transitory computer-readable medium comprising instructions stored thereon that, when executed, cause one or more processors to:<claim-text>obtain signals that are generated by or generated based on data from one or more sensors that are included in one or more hearing instruments;</claim-text><claim-text>determine, based on the signals, whether a posture of a spine of a user of the one or more hearing instruments is a target posture for the user; and</claim-text><claim-text>generate information based on the posture of the spine of the user.</claim-text></claim-text></claim><claim id="CLM-00039" num="00039"><claim-text><b>39</b>. (canceled)</claim-text></claim></claims></us-patent-application>