<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230001576A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230001576</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17673559</doc-number><date>20220216</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>25</class><subclass>J</subclass><main-group>9</main-group><subgroup>16</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>25</class><subclass>J</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>25</class><subclass>J</subclass><main-group>9</main-group><subgroup>1612</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0234</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>25</class><subclass>J</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>25</class><subclass>J</subclass><main-group>9</main-group><subgroup>1664</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>25</class><subclass>J</subclass><main-group>9</main-group><subgroup>1697</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>B</subclass><main-group>2219</main-group><subgroup>39001</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>B</subclass><main-group>2219</main-group><subgroup>40519</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">ADAPTIVE MOBILE MANIPULATION APPARATUS AND METHOD</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63217109</doc-number><date>20210630</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Delta Electronics Int'l (Singapore) Pte Ltd</orgname><address><city>Singapore</city><country>SG</country></address></addressbook><residence><country>SG</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Chen</last-name><first-name>Yuh-Rong</first-name><address><city>Singapore</city><country>SG</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Hu</last-name><first-name>Guoqiang</first-name><address><city>Singapore</city><country>SG</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Cheng</last-name><first-name>Chia Loon</first-name><address><city>Singapore</city><country>SG</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An adaptive manipulation apparatus and method are provided. The adaptive manipulation method includes steps of providing a mobile manipulation apparatus comprising a manipulator, a sensor and a processor for a manipulation of an object placed on a carrier having a plurality of markers spaced apart from each other, the sensor detecting the plurality of markers to obtain a run time marker information, the processor, according to the base-case motion plan, generating a run time motion plan, wherein the run time motion plan comprises a plurality of second pose-aware actions, and the plurality of second pose-aware actions are modified from the plurality of first pose-aware actions according to the run time marker information, and the processor further executing the run time motion plan for controlling the manipulator to manipulate the object.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="78.06mm" wi="158.75mm" file="US20230001576A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="175.60mm" wi="117.60mm" orientation="landscape" file="US20230001576A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="102.45mm" wi="126.15mm" orientation="landscape" file="US20230001576A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="58.50mm" wi="125.39mm" orientation="landscape" file="US20230001576A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="116.16mm" wi="119.04mm" orientation="landscape" file="US20230001576A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="110.41mm" wi="123.61mm" orientation="landscape" file="US20230001576A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="211.07mm" wi="139.87mm" orientation="landscape" file="US20230001576A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="209.80mm" wi="135.30mm" orientation="landscape" file="US20230001576A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="78.57mm" wi="140.55mm" orientation="landscape" file="US20230001576A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="200.24mm" wi="127.59mm" orientation="landscape" file="US20230001576A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="107.95mm" wi="89.41mm" orientation="landscape" file="US20230001576A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="137.84mm" wi="143.26mm" orientation="landscape" file="US20230001576A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Application Ser. No. 63/217,109 filed on Jun. 30, 2021, the disclosure of which is incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD OF THE INVENTION</heading><p id="p-0003" num="0002">The present invention relates to an adaptive mobile manipulation apparatus and method, and more particularly to an adaptive mobile manipulation apparatus and method utilizing a plurality of markers.</p><heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading><p id="p-0004" num="0003">Material handling and logistics are two important tasks in warehouses and factories. These tasks are usually performed by manpower, which leads to safety risks and operation costs. With the development of mobile manipulator, it is now possible to automate these tasks. However, challenges remain.</p><p id="p-0005" num="0004">The first challenge is the navigation of a mobile manipulator. With the help of LASER range scanners, or LIDAR, and advanced control algorithms, an Automated Guided Vehicle (AGV) is now able to move to a target location. However, the accuracy is about 10 centimeters in position and about 10 degrees in orientation.</p><p id="p-0006" num="0005">The second challenge is the localization of the target object or manipulating area, which involves estimating its pose, including the position and the orientation thereof. Techniques such as computer vision and machine learning are capable of doing this in limited conditions such as good lighting. However, due to the placement of camera(s) on the mobile manipulator and various lighting conditions in the warehouse or factory, the outcome is not stable. Besides, these techniques are computationally intensive and not suitable for a mobile manipulator, which has limited battery power and computational power. If the manipulation scenario changes, the mathematical models also have to be re-established. Techniques using squared planar fiducial markers such as ArUco and ALVAR are popular methods used to detect the markers' poses by placing the marker on the object. Given the size of a square marker, the position and orientation of the marker, namely, the pose of the marker, can then be determined by its size and shape in the camera image. The resulting position estimation is accurate (usually within one or two millimeters), but the orientation estimation heavily depends on environmental conditions such as lighting and fluctuates within a short period of time.</p><p id="p-0007" num="0006">The third challenge is about motion planning. This includes moving the mobile manipulator to a specific location and using the manipulator to perform manipulation task. Traditionally, &#x201c;teaching&#x201d; is the technique used on production line for a fixed manipulator to perform repetitive tasks such as pick and place, screwing. An engineer guides and programs the manipulator through a sequence of movements that represent the task. However, due to the position and orientation errors from moving the mobile platform (AGV), there exists a position offset and an orientation offset from the manipulator to the target and this makes traditional &#x201c;teaching&#x201d; technique not suitable for a mobile manipulator.</p><p id="p-0008" num="0007">In addition to the above-mentioned challenges, artificial intelligence and machine learning are popular techniques for solving the above-mentioned problems in academic research. However, it can be practically infeasible for small businesses to have a team of researchers focused on this due to the financial cost. It is appropriate to provide a low-cost framework to solve this problem.</p><p id="p-0009" num="0008">Therefore, there is a need of providing an adaptive mobile manipulation apparatus and method distinct from prior art in order to solve the above drawbacks.</p><heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0010" num="0009">The present disclosure provides an adaptive mobile manipulation apparatus and method in order to overcome at least one of the above-mentioned drawbacks.</p><p id="p-0011" num="0010">The present disclosure also provides an adaptive manipulation method which classifies the actions for object manipulation into pose-aware actions and non-pose-aware actions and further associates the pose-aware actions with localization information obtained by detecting the markers, and thus, the pose-aware actions with high accuracy can be achieved through a low-cost framework of an adaptive mobile manipulation apparatus.</p><p id="p-0012" num="0011">In accordance with an aspect of the present disclosure, an adaptive manipulation method is provided. The adaptive manipulation method includes steps of providing a mobile manipulation apparatus including a manipulator, a sensor and a processor for a manipulation of an object placed on a carrier having a plurality of markers spaced apart from each other, providing a base-case motion plan including a plurality of first pose-aware actions, the sensor detecting the plurality of markers to obtain a run time marker information, the processor, according to the base-case motion plan, generating a run time motion plan, wherein the run time motion plan includes a plurality of second pose-aware actions, and the plurality of second pose-aware actions are modified from the plurality of first pose-aware actions according to the run time marker information, and the processor further executing the run time motion plan for controlling the manipulator to manipulate the object.</p><p id="p-0013" num="0012">In an embodiment, each of the first pose-aware actions of the base-case motion plan includes variables and a base-case marker information corresponding to the plurality of markers.</p><p id="p-0014" num="0013">In an embodiment, the method further includes steps of the processor calculating a difference between the base-case marker information and the run time marker information, and the processor generating the plurality of second pose-aware actions according to the plurality of first pose-aware actions and the difference.</p><p id="p-0015" num="0014">In an embodiment, both the run time marker information and the base-case marker information include positions and orientations between the plurality of markers and the sensor.</p><p id="p-0016" num="0015">In an embodiment, the manipulator further includes an end effector and a joint. The first and the second pose-aware actions include moving the end effector by position and orientation relative to the object. The first and the second pose-aware actions respectively further include at least one of the following: moving the end effector to a target pose, traversing the end effector through a trajectory, and moving the end effector associating with the run time marker information.</p><p id="p-0017" num="0016">In an embodiment, the object is placed at a fixed location on the carrier. In another embodiment, the markers include visual markers or fiducial markers. In further another embodiment, the sensor includes a camera.</p><p id="p-0018" num="0017">In accordance with another aspect of the present invention, an adaptive manipulation apparatus is provided. The adaptive mobile manipulation apparatus includes a manipulator, a sensor, and a processor. The processor is coupled to the manipulator and the sensor, and configured to perform the following steps: retrieving a base-case motion plan including a plurality of first pose-aware actions, driving the sensor to detect a plurality of markers located on a carrier to obtain a run time marker information, according to the base-case motion plan, generating a run time motion plan, wherein the run time motion plan includes a plurality of second pose-aware actions, and the plurality of second pose-aware actions are modified from the plurality of first pose-aware actions according to the run time marker information, and executing the run time motion plan for controlling the manipulator to manipulate an object placed on the carrier.</p><p id="p-0019" num="0018">The above contents of the present disclosure will become more readily apparent to those ordinarily skilled in the art after reviewing the following detailed description and accompanying drawings, in which:</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically illustrates the design of an adaptive mobile manipulation system;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>2</b></figref> schematically illustrates the basic architecture of an adaptive mobile manipulation apparatus;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>3</b></figref> schematically illustrates the flow chart of a motion plan;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>4</b></figref> schematically illustrates the setup of a manipulating workspace;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>5</b></figref> schematically illustrates the setup of a camera sensor and markers;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>6</b></figref> schematically illustrates the implementation flow chart for creating a base-case motion plan;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>7</b></figref> schematically illustrates the execution flow chart of a motion plan at run time;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>8</b></figref> schematically illustrates the flow chart of a manipulation process for manipulating a target object;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>9</b></figref> schematically illustrates the process of obtaining marker information from a camera sensor;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>10</b></figref> schematically illustrates the marker positions in the base-case motion plan and the run time motion plan; and</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>11</b></figref> schematically illustrates the idea of calculating position and orientation offsets.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT</heading><p id="p-0031" num="0030">The present disclosure will now be described more specifically with reference to the following embodiments. It is to be noted that the following descriptions of preferred embodiments of this disclosure are presented herein for purpose of illustration and description only. It is not intended to be exhaustive or to be limited to the precise form disclosed.</p><p id="p-0032" num="0031">The present disclosure is to provide a framework for object manipulation (picking up, placing, or modifying objects etc.) in the settings of warehouses or factory production lines, such that an engineer or operator can easily design a motion plan with affordable financial cost.</p><p id="p-0033" num="0032">Four parts of the details of the present disclosure, including (1) the design of system, (2) the architecture of an adaptive manipulation apparatus, (3) the design of a teaching-based adaptive mobile manipulation, and (4) the algorithms used to obtain localization information from multiple markers, will be described as following.</p><p id="p-0034" num="0033">(1) The Design of System</p><p id="p-0035" num="0034">The system includes the physical setup of the environment, including an adaptive mobile manipulation apparatus, a carrier for placing a target object, and markers placed on the carrier and spaced apart from each other. Generally, the carrier is the rigid body shelf in the warehouse or factory and different shelves are distinguished by their identification numbers, i.e. shelf ID. Please refer to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. <figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically illustrates the design of an adaptive mobile manipulation system. The adaptive mobile manipulation system includes three main components, which are (a) the floor (ground) <b>101</b> of the warehouse or the factory, (b) an adaptive mobile manipulation apparatus includes an AGV component <b>111</b>, a manipulator <b>112</b> with a manipulating tool <b>113</b>, a sensor <b>114</b> with the numeral symbol <b>115</b> representing the effective view volume of the sensor <b>114</b>, and (c) a carrier <b>121</b>, a target object <b>122</b>, collision objects <b>123</b>, and markers <b>124</b>, <b>125</b>. Depending on different environmental settings and practical demands, the sensor <b>114</b> can be a camera sensor, such as 2D/RGB camera, and the markers <b>124</b>, <b>125</b> can be, e.g., visual or fiducial markers, without limitation.</p><p id="p-0036" num="0035">The carrier <b>121</b> is specially designed to house the target object <b>122</b>. Hence, it is assumed that the relative poses among the carrier <b>121</b>, the target object <b>122</b>, the collision objects <b>123</b> and the markers <b>124</b>, <b>125</b> are fixed. The poses of other components can be calculated from the pose of the carrier <b>121</b> if the latter is known. Two markers <b>124</b>, <b>125</b> are positioned horizontally on the carrier <b>121</b> facing approximately in the same direction at the approximately same height from the ground <b>101</b>. For best result, it is suggested that (a) the marker is at least 35 mm in size, (b) additional white-color border of at least 3 mm wide surrounding each marker, and (c) 100 mm apart from each other measured from their centers.</p><p id="p-0037" num="0036">Although there are only one adaptive mobile manipulation apparatus and only one carrier are shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, it is just for illustration. There could be many of them (could be different types) in the operating area. There could also be multiple pairs of markers on a single carrier for manipulation in different directions or different types of mobile manipulation apparatuses.</p><p id="p-0038" num="0037">The objective of a &#x201c;manipulation task&#x201d; is to move the mobile manipulation apparatus to a location near the carrier <b>121</b> and perform manipulation at the target object <b>122</b>. Hence, before performing manipulation, the AGV component <b>111</b> of the adaptive mobile manipulation apparatus moves to a pose nearby the carrier <b>121</b> with a specific pair of markers within the effective view volume <b>115</b> of the sensor <b>114</b> of the mobile manipulation apparatus, and the target object <b>122</b> is within the reach of the manipulator <b>112</b> using existing navigation techniques.</p><p id="p-0039" num="0038">Accordingly, the following reasonable assumptions are made. Firstly, the manipulation task is divided into two steps&#x2014;navigation (using the AGV component <b>111</b>) and manipulation (using the manipulator <b>112</b>). Secondly, the AGV component <b>111</b> is able to navigate to a target position and orientation accurately enough such that the target object <b>122</b> is within the reach of the manipulator <b>112</b>. However, a margin of error (position and orientation offsets) is allowed. Lastly, the target object <b>122</b> is placed on a specially designed carrier <b>121</b> and hence the related poses of the target object <b>122</b> are fixed with the carrier <b>121</b>. In other words, with the knowledge of the pose of the carrier <b>121</b>, the pose of the target object <b>122</b> can be calculated.</p><p id="p-0040" num="0039">(2) The Basic Architecture of an Adaptive Mobile Manipulation Apparatus</p><p id="p-0041" num="0040">Please refer to <figref idref="DRAWINGS">FIG. <b>2</b></figref>. <figref idref="DRAWINGS">FIG. <b>2</b></figref> schematically illustrates the basic architecture of an adaptive mobile manipulation apparatus. This architecture, which is commonly seen in industrial mobile manipulation apparatuses, includes the following components that are electrically coupled: an AGV component <b>201</b>, a processor <b>202</b>, ranging devices <b>203</b>, a sensor <b>204</b>, and a manipulator <b>205</b> with an end effector (EFF) <b>206</b> and at least a joint <b>207</b>, wherein the processor <b>202</b> is configured to perform the computing and communication for manipulating the target object. The present disclosure mainly focuses on the manipulation and hence only the processor <b>202</b>, the sensor <b>204</b>, the manipulator <b>205</b>, and the EFF <b>206</b> are described.</p><p id="p-0042" num="0041">(3) Teaching-Based Adaptive Manipulation</p><p id="p-0043" num="0042">(3.1) Motion Plan</p><p id="p-0044" num="0043">Following the paragraphs described above, the manipulation task is defined as using the manipulator to manipulate objects without direct physical contact by the human labor after the adaptive mobile manipulation apparatus has reached the pose for manipulation. The manipulation task includes a series of manipulation actions and can be defined as a &#x201c;motion plan&#x201d;. Please refer to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, which schematically illustrates the flow chart of a motion plan with n steps. The steps transit from Action <b>1</b> <b>301</b> to Action <b>2</b> <b>302</b>, to Action <b>3</b> <b>303</b>, and so on until Action n <b>304</b>. Possible actions include, but are not limited to, (a) moving to a target joint state, (b) moving EFF to a target pose, (c) EFF traversing through a trajectory, (d) EFF moving with a position offset with respect to the coordinate of the manipulator, (e) EFF moving with a position offset with respect to the coordinate of the target object, and (f) tool action (e.g., open/close gripper) or other related actions (e.g., light on/off, conveyor on/off). Note that a series of (b) form a &#x201c;trajectory&#x201d; for (c). In the motion plan, there could also be collision objects, which are the objects in the scene that could collide with the manipulator and should be avoided.</p><p id="p-0045" num="0044">Without loss of generality, only the actions listed below in the Table I are considered in the present disclosure. The possible actions in a motion plan are classified by pose-awareness. The actions which are classified as pose-aware are those directly related to the manipulation of the target object after the manipulator and the EFF are within a range capable of reaching the target object. Note that it is possible to have actions with online adjustment using wrist camera or other sensors, which can be combined with this framework.</p><p id="p-0046" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE I</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Possible actions in a motion plan</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="56pt" align="left"/><colspec colname="2" colwidth="84pt" align="left"/><colspec colname="3" colwidth="63pt" align="left"/><tbody valign="top"><row><entry/><entry>Pose-awareness</entry><entry>Action</entry><entry>Variable Required</entry></row><row><entry/><entry namest="offset" nameend="3" align="center" rowsep="1"/></row><row><entry/><entry>Pose-aware</entry><entry>Moving EFF to a pose</entry><entry>Target EFF pose, </entry></row><row><entry/><entry/><entry/><entry>Marker positions</entry></row><row><entry/><entry/><entry>EFF traversing through a</entry><entry>Trajectory, </entry></row><row><entry/><entry/><entry>trajectory</entry><entry>Marker positions</entry></row><row><entry/><entry/><entry>EFF moving with </entry><entry>Position offset, </entry></row><row><entry/><entry/><entry>position offset</entry><entry>Marker</entry></row><row><entry/><entry/><entry>with respect to </entry><entry>positions</entry></row><row><entry/><entry/><entry>target object's</entry><entry/></row><row><entry/><entry/><entry>coordinate</entry><entry/></row><row><entry/><entry>Non pose-</entry><entry>Moving to a joint state</entry><entry>Joint state (angles)</entry></row><row><entry/><entry>aware</entry><entry>EFF moving with </entry><entry>Position offset</entry></row><row><entry/><entry/><entry>position offset</entry><entry/></row><row><entry/><entry/><entry>with respect to </entry><entry/></row><row><entry/><entry/><entry>manipulator's</entry><entry/></row><row><entry/><entry/><entry>coordinate</entry><entry/></row><row><entry/><entry/><entry>Tool and other actions</entry><entry>Control variables</entry></row><row><entry/><entry namest="offset" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0047" num="0045">(3.2) Creating a Base-Case Motion Plan Using &#x201c;Teaching&#x201d; and its Implementation</p><p id="p-0048" num="0046">Please refer to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. <figref idref="DRAWINGS">FIG. <b>4</b></figref> schematically illustrates the setup of a manipulating workspace. As shown, the numeral symbol <b>401</b> indicates the carrier, e.g. the rigid body shelf, the numeral symbols <b>402</b> and <b>403</b> indicate two squared fiducial markers, the numeral symbol <b>404</b> indicates the target object, the numeral symbols <b>411</b> and <b>412</b> respectively indicate the manipulator and the EFF, and the numeral symbol <b>413</b> indicates the AGV component. It can be observed that the relative pose between the target object <b>404</b> and the rigid body shelf <b>401</b> is fixed. However, the relative poses between the manipulator <b>411</b> and the EFF <b>412</b> with the rigid body shelf <b>401</b> and the target object <b>404</b> depend on the pose (position and orientation) of the mobile manipulation apparatus.</p><p id="p-0049" num="0047">However, if a motion plan for such manipulation task for a specific mobile manipulation pose configuration is provided, this motion plan is able to be modified for other different mobile manipulation pose configurations. This motion plan is defined as the &#x201c;base-case motion plan&#x201d; and the purpose of &#x201c;teaching&#x201d; is to create it. A base-case motion plan can be designed manually, computationally, or through teaching. This section covers the process to create the base-case motion plan using teaching and its implementation.</p><p id="p-0050" num="0048">(3.2.1) Environmental Setup</p><p id="p-0051" num="0049">In order to apply position and orientation fixes to adjust actions in the base-case motion plan, additional information needs to be captured for the base-case motion plan. In the present disclosure, each pose-aware action is associated with a pair of fiducial markers. Hence, as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, which schematically illustrates the setup of the camera sensor and markers, a pair of fiducial markers <b>511</b>, <b>512</b> must be within the effective view volume <b>522</b> of the camera sensor <b>521</b> once the adaptive mobile manipulation apparatus stops moving. In practice, a single pair of markers can be associated with the entire base-case motion plan. Without loss of generality, it is also possible to use different pairs of markers for different actions, which will be considered in this disclosure.</p><p id="p-0052" num="0050">(3.2.2) Base-Case Motion Plan and its Digital Representation</p><p id="p-0053" num="0051">Given an arbitrary mobile manipulator pose, a motion plan to perform the manipulation task can be modified into a base-case motion plan by adding marker information to each pose-aware action. Hence, to create a base-case motion plan, an additional step is required to detect the pair of markers to obtain base-case marker information for being associated with each pose-aware action. This can be done by using squared fiducial marker technique. Such technique can provide a stream of estimated poses (position and orientation) of a marker using the image frames come from an RGB camera. However, the values could be fluctuating, and a filter that can be applied to the pose stream is presented in a latter section and is designed to improve the detections. The following Table II shows the data structures used in actions and how different types of variables are represented in a computer system.</p><p id="p-0054" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE II</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Actions and their digital representation</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="42pt" align="left"/><colspec colname="2" colwidth="56pt" align="left"/><colspec colname="3" colwidth="105pt" align="left"/><tbody valign="top"><row><entry/><entry>Variable</entry><entry>Data structure</entry><entry>Explanation</entry></row><row><entry/><entry namest="offset" nameend="3" align="center" rowsep="1"/></row><row><entry/><entry>EFF pose</entry><entry>((x, y, z), </entry><entry>A tuple consists of position and</entry></row><row><entry/><entry/><entry>(rx, ry, rz, rw))</entry><entry>orientation</entry></row><row><entry/><entry>Trajectory</entry><entry>[P<sub>1</sub>, P<sub>2</sub>, . . . , P<sub>k</sub>]</entry><entry>An array pf P<sub>i</sub>'s, each P<sub>i </sub>is </entry></row><row><entry/><entry/><entry/><entry>an EFF pose</entry></row><row><entry/><entry>Position </entry><entry>(x, y, z)</entry><entry>Position offset with respect to</entry></row><row><entry/><entry>offset</entry><entry/><entry>manipulator's coordinate</entry></row><row><entry/><entry>Joint state</entry><entry>[j<sub>1</sub>, j<sub>2</sub>, . . . , j<sub>k</sub>]</entry><entry>An array of joint angles, size</entry></row><row><entry/><entry/><entry/><entry>depends on the number </entry></row><row><entry/><entry/><entry/><entry>of joints of a</entry></row><row><entry/><entry/><entry/><entry>manipulator</entry></row><row><entry/><entry>Control</entry><entry>N/A</entry><entry>Depends on the </entry></row><row><entry/><entry>variables</entry><entry/><entry>semantic scenario,</entry></row><row><entry/><entry/><entry/><entry>not a scope of the </entry></row><row><entry/><entry/><entry/><entry>present disclosure</entry></row><row><entry/><entry>Marker</entry><entry>{n<sub>L</sub>: (x, y, z), </entry><entry>A tuple consists of </entry></row><row><entry/><entry>positions</entry><entry>n<sub>R</sub>: (x, y, z)}</entry><entry>two marker ids</entry></row><row><entry/><entry/><entry/><entry>and their positions</entry></row><row><entry/><entry namest="offset" nameend="3" align="center" rowsep="1"/></row><row><entry/><entry namest="offset" nameend="3" align="left" id="FOO-00001">1. x, y, z, rx, ry, rz, rw are real numbers but usually represented as double precision floating point numbers in a computer system.</entry></row><row><entry/><entry namest="offset" nameend="3" align="left" id="FOO-00002">2. j<sub>1</sub>, j<sub>2</sub>, . . . , j<sub>k </sub>are real numbers but can be simplified and represented as double precision floating point numbers in the range between &#x2212;&#x3c0; and &#x3c0;.</entry></row><row><entry/><entry namest="offset" nameend="3" align="left" id="FOO-00003">3. n<sub>L</sub>, n<sub>R </sub>are left and right marker IDs.</entry></row></tbody></tgroup></table></tables></p><p id="p-0055" num="0052">Base on the paragraphs described above, the motion plan defined earlier for a base-case motion plan can be extended as an ordered list of actions with positions of a pair of square fiducial markers stored if the action is pose-aware. The details of the data structures used for a base-case motion plan in a computer system are illustrated below:</p><p id="p-0056" num="0053">Base case motion plan=[a]</p><p id="p-0057" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>a</i>=Pose<sub>EFF</sub>|Trajectory|Offset<sub>target</sub><i>|JS</i>|Offset<sub>manipulator</sub>|Action<sub>other </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0058" num="0054">Pose<sub>EFF</sub>=((p,q), (p<sub>L</sub>, p<sub>R</sub>))</p><p id="p-0059" num="0055">Trajectory=([(p,q)], (p<sub>L</sub>, p<sub>R</sub>))</p><p id="p-0060" num="0056">Offset<sub>target</sub>=((x,y,z),(p<sub>L</sub>,p<sub>R</sub>))</p><p id="p-0061" num="0057">JS=[j]</p><p id="p-0062" num="0058">Offset<sub>manipulator</sub>=(x,y,z)</p><p id="p-0063" num="0059">p,p<sub>L</sub>,p<sub>R</sub>=(x,y,z)</p><p id="p-0064" num="0060">q=(rx, ry, rz, rw)</p><p id="p-0065" num="0061">x,y,z,rx,ry,rz,rw,j are real numbers.</p><p id="p-0066" num="0062">The notations used in former paragraphs are as follows:</p><p id="p-0067" num="0063">[u]: an ordered list of &#x201c;u&#x201d;</p><p id="p-0068" num="0064">a: an action</p><p id="p-0069" num="0065">|: or</p><p id="p-0070" num="0066">Pose<sub>EFF</sub>: EFF Pose</p><p id="p-0071" num="0067">Trajectory: EFF Trajectory</p><p id="p-0072" num="0068">Offset<sub>target</sub>: EFF movement offset along target object's coordinate</p><p id="p-0073" num="0069">JS: manipulator's joint state</p><p id="p-0074" num="0070">Offset<sub>manipulator</sub>: EFF movement offset along manipulator's coordinate</p><p id="p-0075" num="0071">Action<sub>other</sub>: other actions that does not affect the manipulator state</p><p id="p-0076" num="0072">p,p<sub>L</sub>,p<sub>R</sub>: position, position of left marker, position of right marker</p><p id="p-0077" num="0073">q: orientation (Euler angles or quaternion)</p><p id="p-0078" num="0074">Note that without loss of generality, it is assumed the origin (0, 0, 0) and the world coordinate system are aligned with the base of the manipulator.</p><p id="p-0079" num="0075">(3.2.3) Create Base-Case Motion Plan Using Teaching</p><p id="p-0080" num="0076">When doing manipulator onboard programming, teaching is used to specify a state (mostly a joint state) by moving the manipulator to a desired configuration instead of giving the values of this joint state. In the present disclosure, this concept is extended further to the entire motion plan and the user guides the manipulator through a series of actions during this process.</p><p id="p-0081" num="0077">Please refer to <figref idref="DRAWINGS">FIG. <b>6</b></figref>. <figref idref="DRAWINGS">FIG. <b>6</b></figref> schematically illustrates the implementation flow chart of a teaching technique for creating the base-case motion plan. At the beginning of teaching <b>601</b>, an empty ordered list &#x201c;actions&#x201d; is initialized to store the motion plan. Next, as shown in step <b>602</b>, the user indicates the next action in the motion plan, or the user has finished creating the motion plan. Meanwhile, a dictionary data structure ({ }) &#x201c;curr_action&#x201d; is initialized as empty. In the decision step <b>603</b>, user input from previous step <b>602</b> is checked.</p><p id="p-0082" num="0078">If the action is a pose-aware action, corresponding variables from Table II are collected in step <b>604</b> and stored into &#x201c;curr_action&#x201d; along with the type of the action. In this step, these variables can be collected directly from the manipulator after the user operates the manipulator to the desired pose. Next step <b>605</b> is to collect the base-case marker information from left and right markers using known technique described earlier and the base-case marker information is stored in &#x201c;curr_action&#x201d; along with marker IDs, in which marker IDs are given by the user. An algorithm that collects a series of samples and applies a filter to filter out extreme value for providing better values is presented in Section 4.1. Then, &#x201c;curr_action&#x201d; is appended to the end of &#x201c;actions&#x201d; in step <b>606</b>. Accordingly, the pose-aware actions associated with the base-case marker information for the base-case motion plan are created and defined as first pose-aware actions.</p><p id="p-0083" num="0079">Similarly, if the action is a non-pose-aware action, corresponding variables from Table II is collected in step <b>607</b> and stored into &#x201c;curr_action&#x201d; along with the type of the action. In this step, these variables can be collected (1) directly from the manipulator after the user operates the manipulator to the desired joint state, and (2) through user keyboard input (for example, EFF position offset, close/open the gripper, or other options). The system performs corresponding action upon receiving user input, then &#x201c;curr_action&#x201d; is appended to the end of &#x201c;actions&#x201d; in step <b>608</b>. Accordingly, the non-pose-aware actions for the base-case motion plan are created and defined as first non-pose-aware actions.</p><p id="p-0084" num="0080">If it indicates that the user has finished creating the base-case motion plan, &#x201c;actions&#x201d; is then flattened as a string data structure and stored with a unique name specified by the user for later use in step <b>609</b>. Then the process moves to the finish state <b>610</b>.</p><p id="p-0085" num="0081">(3.3) Adjusting Base-Case Motion Plan for Run Time Scenario</p><p id="p-0086" num="0082">Please refer to <figref idref="DRAWINGS">FIG. <b>7</b></figref>. After retrieving a base-case motion plan for a specific scenario, the process to modify it for run time execution is illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. In Step <b>701</b>, the input is a base-case motion plan with first pose-aware actions and first non-pose-aware actions described in Section 3.2, which can be looked up in the computer storage and identified by its name. Then, go through and process each element (which is an action) in the motion plan. First is to check if the size of actions is 0 as shown in step <b>702</b>. If it is 0, the process is finished and moved to the finish state <b>721</b>. Otherwise, retrieve the first element in actions as curr_action <b>711</b>. Next, determine if this is a pose-aware action as shown in step <b>712</b>. If this is not a pose-aware action, the action is executed as shown in step <b>715</b>. Otherwise, the processor drives the sensor to detect the markers specified in this action for obtaining run time marker information using the filter and algorithm in Section 4.1 as shown in step <b>713</b>. Then, the run time marker information is passed to step <b>714</b> to calculate the position and orientation offsets using the algorithms in Section 4.2 and used to modify the action using the algorithms in Section 4.3. Then, the modified action is executed as shown in step <b>715</b>, the first action in actions is removed as shown in step <b>716</b> and actions are processed again using the same flow.</p><p id="p-0087" num="0083">That is, the run time motion plan is modified from the base-case motion plan. First, according to the run time marker information obtained by the sensor, the first pose-aware actions of the base-case motion plan are modified into different pose-aware actions, which are defined as second pose-aware actions in the run time motion plan. Further, the first non-pose-aware actions of the base-case motion plan are not modified and executed directly at run time, which are defined as second non-pose-aware actions in the run time motion plan.</p><p id="p-0088" num="0084">Accordingly, in summary, the process for the adaptive mobile manipulation apparatus to manipulate the target object is as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The process starts at step <b>801</b>. Then, in step <b>802</b>, the processor retrieves a base-case motion plan that has already created. Next, the processor drives the sensor to detect the markers for obtaining the run time marker information as in step <b>803</b>. According to the run time marker information, the processor modifies the base-case motion plan with the first pose-aware actions and the first non-pose-aware action into the run time motion plan with the second pose-aware actions and the second non-pose-aware actions in step <b>804</b>. Then, as shown in step <b>805</b>, the run time motion plan is executed, thereby controlling the manipulator to manipulate the target object.</p><p id="p-0089" num="0085">(4) Algorithms for Modifying Base-Case Motion Plan</p><p id="p-0090" num="0086">(4.1) Filtering Algorithm for Getting Stable Marker Positions</p><p id="p-0091" num="0087">Please refer to <figref idref="DRAWINGS">FIG. <b>9</b></figref>. The process of obtaining marker information from the camera sensor is shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. RGB camera is used to obtain image stream in step <b>901</b>. Then, the images in the stream as shown in step <b>902</b> are processed using existing squared fiducial marker localization techniques such as ArUco or AR Tracker Alvar as shown in step <b>903</b>. The output from the step <b>903</b> is stream of marker ids along with their positions and orientations. Consecutive k data points for each marker are retrieved with orientations removed in step <b>904</b>. In this implementation, k is set to 300 during teaching and 30 during run time. Then the data from the step <b>904</b> is processed by the algorithm in step <b>905</b> to filter out extreme values and output in step <b>906</b>.</p><p id="p-0092" num="0088">The filtering algorithm in step <b>905</b> is presented below.</p><p id="p-0093" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="21pt" align="center"/><colspec colname="2" colwidth="196pt" align="left"/><thead><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>&#x2003;</entry><entry>&#x2003;&#x2003;Input:</entry></row><row><entry/><entry>P<sub>m </sub>= (p<sub>m1</sub>,p<sub>m2</sub>, ... ... ,p<sub>mk</sub>),&#x2200;m &#x2208; M,p<sub>mi </sub>= (x<sub>mi</sub>,y<sub>mi</sub>,z<sub>mi</sub>)</entry></row><row><entry/><entry>&#x2003;&#x2003;Algorithm:</entry></row><row><entry> </entry></row><row><entry/><entry><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <mn>1.</mn>   <mtext>   </mtext>   <mi>Compute</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <msubsup>    <mi>C</mi>    <mi>m</mi>    <mn>1</mn>   </msubsup>  </mrow>  <mo>=</mo>  <mrow>   <mrow>    <mo>(</mo>    <mrow>     <mfrac>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>k</mi>       </munderover>       <msub>        <mi>x</mi>        <mi>mi</mi>       </msub>      </mrow>      <mi>k</mi>     </mfrac>     <mo>,</mo>     <mfrac>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>k</mi>       </munderover>       <msub>        <mi>y</mi>        <mi>mi</mi>       </msub>      </mrow>      <mi>k</mi>     </mfrac>     <mo>,</mo>     <mfrac>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>k</mi>       </munderover>       <msub>        <mi>z</mi>        <mi>mi</mi>       </msub>      </mrow>      <mi>k</mi>     </mfrac>    </mrow>    <mo>)</mo>   </mrow>   <mo>,</mo>   <mrow>    <mo>&#x2200;</mo>    <msub>     <mi>P</mi>     <mi>m</mi>    </msub>   </mrow>  </mrow> </mrow></math></maths></entry></row><row><entry> </entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2009;<maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mi>q</mi>  <mo>=</mo>  <mrow>   <mo>[</mo>   <mfrac>    <mi>k</mi>    <mn>2</mn>   </mfrac>   <mo>]</mo>  </mrow> </mrow></math></maths></entry></row><row><entry> </entry></row><row><entry/><entry>2. Sort P<sub>m </sub>by their distances to C<sub>m</sub><sup>1 </sup>in ascending order</entry></row><row><entry/><entry>3. Let P<sub>m</sub>&#x2032; be the first q element from sorted P<sub>m </sub>in previous step</entry></row><row><entry> </entry></row><row><entry/><entry><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mrow>  <mrow>   <mn>4.</mn>   <mtext>   </mtext>   <mi>Compute</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <msub>    <mi>C</mi>    <mi>m</mi>   </msub>  </mrow>  <mo>=</mo>  <mrow>   <mrow>    <mo>(</mo>    <mrow>     <mfrac>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>q</mi>       </munderover>       <msub>        <mi>x</mi>        <mi>mi</mi>       </msub>      </mrow>      <mi>q</mi>     </mfrac>     <mo>,</mo>     <mfrac>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>q</mi>       </munderover>       <msub>        <mi>y</mi>        <mi>mi</mi>       </msub>      </mrow>      <mi>q</mi>     </mfrac>     <mo>,</mo>     <mfrac>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>q</mi>       </munderover>       <msub>        <mi>z</mi>        <mi>mi</mi>       </msub>      </mrow>      <mi>q</mi>     </mfrac>    </mrow>    <mo>)</mo>   </mrow>   <mo>,</mo>   <mrow>    <mo>&#x2200;</mo>    <msubsup>     <mi>P</mi>     <mi>m</mi>     <mo>&#x2032;</mo>    </msubsup>   </mrow>  </mrow> </mrow></math></maths></entry></row><row><entry> </entry></row><row><entry/><entry>&#x2003;&#x2003;Output:</entry></row><row><entry/><entry>C<sub>m </sub>= (x<sub>m</sub>,y<sub>m</sub>,z<sub>m</sub>),&#x2200;m &#x2208; M</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0094" num="0089">The notations used in former paragraphs are as follows:</p><p id="p-0095" num="0090">M: the set of markers to be localized</p><p id="p-0096" num="0091">m: marker m</p><p id="p-0097" num="0092">P<sub>m</sub>: the k samples for a specific marker m</p><p id="p-0098" num="0093">p<sub>mi</sub>: i-th sample in P<sub>m </sub>with position (x<sub>mi</sub>, y<sub>mi</sub>, z<sub>mi</sub>)</p><p id="p-0099" num="0094">C<sub>m</sub>: final position for marker m</p><p id="p-0100" num="0095">Other notations for temporal variables are self-explanatory</p><p id="p-0101" num="0096">The output from the filtering algorithm is then used to modify the base-case motion plan.</p><p id="p-0102" num="0097">Note that by placing 3 markers in an L shape (or more markers), it will be able to determine position offset in 3-dimension as well as Roll/Pitch/Yaw. Above information can then be used to handle the case that the height of the shelf is changed. The motion plan modification is similar.</p><p id="p-0103" num="0098">(4.2) Algorithm for Getting Position and Orientation Offsets Between Base-Case Motion Plan and Run Time Motion Plan</p><p id="p-0104" num="0099">In the base-case motion plan, the base-case marker information is associated with each action. This along with the run time marker information detected at run time is used to calculate the position offset and orientation offset and to be applied to modify the base-case motion plan, in which the first pose-aware actions are accordingly modified into the second pose-aware actions. Please refer to <figref idref="DRAWINGS">FIG. <b>10</b></figref>. <figref idref="DRAWINGS">FIG. <b>10</b></figref> schematically illustrates a difference between the marker information respectively in the base-case motion plan and the run time motion plan. As shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, a and b are the left and right marker positions in the base-case motion plan <b>1001</b>, c and d are the left and right marker positions detected for the run time motion plan <b>1002</b>, respectively.</p><p id="p-0105" num="0100">Please refer to <figref idref="DRAWINGS">FIG. <b>11</b></figref>. The idea of calculating position and orientation offsets is shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. Note that the positions are based on the mobile manipulator's coordinate. The numeral symbol <b>1101</b> indicates the relationship between the marker positions, which has already been shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. The position offset is from c to a and orientation offset is theta<sub>z</sub>. From here, the height information (Z) is dropped due to the assumption that the environment within a factory or warehouse is a flat plane. This results in the relationship <b>1102</b>, in which the symbols a, b, c, and d are mapped to the symbols a&#x2032;, b&#x2032;, c&#x2032;, and d&#x2032;, respectively. The symbols a&#x2032;, b&#x2032;, c&#x2032;, and d&#x2032; only contain 2-dimension information (X and Y). Note that this is a projection onto the X-Y plane. Next, a&#x2032; and c&#x2032; are translated to O (0, 0) and the same translation matrices are applied to b&#x2032; (a&#x2032; to O) and d&#x2032; (c&#x2032; to O) and resulting in symbols a&#x2033;, b&#x2033;, c&#x2033;, and d&#x2033; in the relationship <b>1103</b>. Then the calculation of position and orientation offsets can be done using the formulas below.</p><p id="p-0106" num="0101">Input:</p><p id="p-0107" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>a</i>=(<i>x</i><sub>a</sub><i>,y</i><sub>a</sub><i>,z</i><sub>a</sub>), <i>b</i>=(<i>x</i><sub>b</sub><i>,y</i><sub>b</sub><i>,z</i><sub>b</sub>), <i>c</i>=(<i>x</i><sub>c</sub><i>,y</i><sub>c</sub><i>,z</i><sub>c</sub>), <i>d</i>=(<i>x</i><sub>d</sub><i>,y</i><sub>d</sub><i>,z</i><sub>d</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0108" num="0102">Position Offset:</p><p id="p-0109" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>(&#x394;<sub>x</sub>,&#x394;<sub>y</sub>,0)=(<i>x</i><sub>c</sub><i>&#x2212;x</i><sub>a</sub><i>,y</i><sub>c</sub><i>&#x2212;y</i><sub>a</sub>,0)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0110" num="0103">Orientation Offset:</p><p id="p-0111" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mrow>  <msub>   <mi>theta</mi>   <mi>z</mi>  </msub>  <mo>=</mo>  <mrow>   <msup>    <mi>cos</mi>    <mrow>     <mo>-</mo>     <mn>1</mn>    </mrow>   </msup>   <mo>&#x2062;</mo>   <mfrac>    <mrow>     <mi>r</mi>     <mo>&#xb7;</mo>     <mi>s</mi>    </mrow>    <mrow>     <mrow>      <semantics definitionURL="">       <mo>&#x2758;</mo>       <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>      </semantics>      <mi>r</mi>      <semantics definitionURL="">       <mo>&#x2758;</mo>       <annotation encoding="Mathematica">"\[RightBracketingBar]"</annotation>      </semantics>     </mrow>     <mo>&#xb7;</mo>     <mrow>      <semantics definitionURL="">       <mo>&#x2758;</mo>       <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>      </semantics>      <mi>s</mi>      <semantics definitionURL="">       <mo>&#x2758;</mo>       <annotation encoding="Mathematica">"\[RightBracketingBar]"</annotation>      </semantics>     </mrow>    </mrow>   </mfrac>  </mrow> </mrow></math></maths></p><p id="p-0112" num="0104">where:</p><p id="p-0113" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>r</i>=(<i>x</i><sub>r</sub><i>,y</i><sub>r</sub>)=(<i>x</i><sub>b</sub><i>&#x2212;x</i><sub>a</sub><i>,y</i><sub>b</sub><i>&#x2212;y</i><sub>a</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0114" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>s</i>=(<i>x</i><sub>s</sub><i>,y</i><sub>s</sub>)=(<i>x</i><sub>d</sub><i>&#x2212;x</i><sub>c</sub><i>,y</i><sub>d</sub><i>&#x2212;y</i><sub>c</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0115" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>r&#xb7;s=x</i><sub>r</sub><i>x</i><sub>s</sub><i>+y</i><sub>r</sub><i>y</i><sub>s </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0116" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>|<i>r|&#xb7;|s</i>|=&#x221a;{square root over (<i>x</i><sub>r</sub><sup>2</sup><i>+y</i><sub>r</sub><sup>2</sup>)}+&#x221a;{square root over (<i>x</i><sub>s</sub><sup>2</sup><i>+y</i><sub>s</sub><sup>2</sup>)}<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0117" num="0105">(4.3) Algorithms for Modifying Base-Case Motion Plan</p><p id="p-0118" num="0106">With position offset (&#x394;<sub>x</sub>, &#x394;<sub>y</sub>, 0) and orientation offset theta<sub>z</sub>, the base-case motion plan can now be adjusted into the run time motion plan for performing the manipulation. Only pose-aware actions in the motion plan are required to be modified, at least including &#x201c;moving EFF to a pose&#x201d;, &#x201c;EFF traversing through a trajectory&#x201d; and &#x201c;EFF moving with position offset with respect to target's coordinate&#x201d; (refer to Table I). The calculations of adjustment are described in Sections 4.3.1 and 4.3.2.</p><p id="p-0119" num="0107">(4.3.1) EFF Pose and Trajectory</p><p id="p-0120" num="0108">For action type &#x201c;moving EFF to a pose&#x201d;, a single EFF pose needs to be modified. On the other hand, &#x201c;EFF traversing through a trajectory&#x201d; action contains a series of EFF poses and each needs to be recalculated. Hence both can use the same algorithm to calculate new target EFF pose as presented below.</p><p id="p-0121" num="0109">Input:</p><p id="p-0122" num="0110">EFF Pose in Base-Case Motion Plan</p><p id="p-0123" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>pose(<i>p,q</i>),<i>p</i>=(<i>x,y,z</i>),<i>q</i>=(<i>q</i><sub>x</sub><i>,q</i><sub>y</sub><i>,q</i><sub>z</sub><i>,q</i><sub>w</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0124" num="0111">Marker Information in Base-Case Motion Plan</p><p id="p-0125" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>l</i>=(<i>x</i><sub>l</sub><i>,y</i><sub>l</sub><i>,z</i><sub>l</sub>), <i>r</i>=(<i>x</i><sub>r</sub><i>,y</i><sub>r</sub><i>,z</i><sub>r</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0126" num="0000"><tables id="TABLE-US-00004" num="00004"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="21pt" align="left"/><colspec colname="1" colwidth="133pt" align="left"/><colspec colname="2" colwidth="63pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>Position offset from Section 4.2</entry><entry>(&#x394;<sub>x</sub>, &#x394;<sub>y</sub>, 0)</entry></row><row><entry/><entry>Rotation offset from Section 4.2</entry><entry>&#x3b8; = theta<sub>z</sub></entry></row><row><entry/><entry>Quaternion rotation equivalent to &#x3b8;</entry><entry>q<sub>r</sub></entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0127" num="0112">Algorithm:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0113">1. Translate l,p to origin of XY plane: l&#x2032;=(0, 0, z<sub>l</sub>)</li>    </ul>    </li></ul></p><p id="p-0128" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>p</i>&#x2032;=(<i>x</i><sub>p&#x2032;</sub><i>,y</i><sub>p&#x2032;</sub><i>,z</i><sub>p</sub>)=(<i>x&#x2212;x</i><sub>a</sub><i>,y&#x2212;y</i><sub>a</sub><i>,z</i>)<?in-line-formulae description="In-line Formulae" end="tail"?><ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0114">2. Rotate p&#x2032; by &#x3b8; above z axis:</li>    </ul>    </li></ul></p><p id="p-0129" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>p</i>&#x2033;=(<i>x</i><sub>p&#x2032;</sub>cos &#x3b8;&#x2212;<i>y</i><sub>p&#x2032;</sub>sin &#x3b8;,<i>x</i><sub>p&#x2032;</sub>,sin &#x3b8;+<i>y</i><sub>p&#x2032;</sub>cos &#x3b8;,<i>z</i><sub>p</sub>)=(<i>x</i><sub>p&#x2033;</sub><i>,y</i><sub>p&#x2033;</sub><i>,z</i><sub>p</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?><ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0115">3. Translate p&#x2033; back and add offset to get new target pose:</li>    </ul>    </li></ul></p><p id="p-0130" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>p</i><sup>n</sup>=(<i>x</i><sub>p&#x2033;</sub><i>+x</i><sub>a</sub>+&#x394;<sub>x</sub><i>,y</i><sub>p</sub><i>&#x2033;+y</i><sub>a</sub>+&#x394;<sub>y</sub><i>,z</i><sub>p</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?><ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0116">4. Apply q<sub>r </sub>to q, where x denotes quaternion multiplication</li>    </ul>    </li></ul></p><p id="p-0131" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>q</i><sup>n</sup><i>=q</i><sub>r</sub><i>&#xd7;q </i><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0132" num="0117">Output:<ul id="ul0009" list-style="none">    <li id="ul0009-0001" num="0000">    <ul id="ul0010" list-style="none">        <li id="ul0010-0001" num="0118">Final EFF pose (p<sup>n</sup>, q<sup>n</sup>)</li>    </ul>    </li></ul></p><p id="p-0133" num="0119">(4.3.2) Movement Position Offset</p><p id="p-0134" num="0120">Action type &#x201c;EFF moving with position offset with respect to target's coordinate&#x201d; can be calculated using the equations calculating of new EFF movement offset as following.</p><p id="p-0135" num="0121">Input:</p><p id="p-0136" num="0122">EFF Movement in Base-Case Motion Plan</p><p id="p-0137" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x394;=(&#x394;<sub>x</sub>,&#x394;<sub>y</sub>,&#x394;<sub>z</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0138" num="0123">Rotation (about Z axis) &#x3b8;=theta<sub>z </sub></p><p id="p-0139" num="0124">Algorithm:</p><p id="p-0140" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>New EFF movement &#x394;&#x2032;=(&#x394;&#x2032;<sub>x</sub>,&#x394;&#x2032;<sub>y</sub>,&#x394;&#x2032;<sub>z</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0141" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>where: &#x394;&#x2032;<sub>x</sub>=&#x394;<sub>x </sub>cos &#x3b8;&#x2212;&#x394;<sub>y </sub>sin &#x3b8;<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0142" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x394;&#x2032;<sub>y</sub>=&#x394;<sub>x </sub>sin &#x3b8;+&#x394;<sub>y </sub>cos &#x3b8;<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0143" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x394;&#x2032;<sub>z</sub>=&#x394;<sub>z </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0144" num="0125">To summarize, this framework provides a process to create a base-case motion plan according to the base-case marker information. Then, with the run time marker information obtained through the method using two squared fiducial markers provided in the present disclosure, the base-case motion plan can be adjusted into the run time motion plan using the methods provided to compensate both position and orientation offsets.</p><p id="p-0145" num="0126">In brief, the present disclosure has the following advantages:</p><p id="p-0146" num="0127">1. Low cost: the cost for setup the system is affordable, which includes RGB camera and cost to print the markers.</p><p id="p-0147" num="0128">2. Easy deployment: markers can be easily deployed to the field within the view of camera, and there is no requirement on measurement and alignment.</p><p id="p-0148" num="0129">3. Accuracy: multi-marker system from this disclosure provides good accuracy on finding position offset and orientation offset with respect to the base-case motion plan.</p><p id="p-0149" num="0130">4. A method (&#x201c;teaching&#x201d;) for creating the base-case motion plan makes it realistic to be adopted in the industrial without a research team.</p><p id="p-0150" num="0131">5. Local information for manipulation: only local information for manipulation is used and stored in this disclosure, which is relatively cheaper than constructing an accurate global 3D environmental map and makes re-arrangement of the environmental settings easy.</p><p id="p-0151" num="0132">From the above descriptions, the present disclosure provides an adaptive mobile manipulation method which classifies the actions for object manipulation into pose-aware actions and non-pose-aware actions and further associates the pose-aware actions with localization information obtained by detecting the markers, and thus, the pose-aware actions with high accuracy can be achieved through a low-cost framework of an adaptive manipulation apparatus.</p><p id="p-0152" num="0133">While the invention has been described in terms of what is presently considered to be the most practical and preferred embodiments, it is to be understood that the invention needs not be limited to the disclosed embodiment. On the contrary, it is intended to cover various modifications and similar arrangements included within the spirit and scope of the appended claims which are to be accorded with the broadest interpretation so as to encompass all such modifications and similar structures.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230001576A1-20230105-M00001.NB"><img id="EMI-M00001" he="10.92mm" wi="49.36mm" file="US20230001576A1-20230105-M00001.TIF" alt="embedded image " img-content="table" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230001576A1-20230105-M00002.NB"><img id="EMI-M00002" he="5.67mm" wi="8.13mm" file="US20230001576A1-20230105-M00002.TIF" alt="embedded image " img-content="table" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230001576A1-20230105-M00003.NB"><img id="EMI-M00003" he="11.26mm" wi="49.36mm" file="US20230001576A1-20230105-M00003.TIF" alt="embedded image " img-content="table" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230001576A1-20230105-M00004.NB"><img id="EMI-M00004" he="5.25mm" wi="76.20mm" file="US20230001576A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An adaptive mobile manipulation method, comprising steps of:<claim-text>providing a mobile manipulation apparatus comprising a manipulator, a sensor and a processor for a manipulation of an object placed on a carrier having a plurality of markers spaced apart from each other;</claim-text><claim-text>providing a base-case motion plan comprising a plurality of first pose-aware actions;</claim-text><claim-text>the sensor detecting the plurality of markers to obtain a run time marker information;</claim-text><claim-text>the processor, according to the base-case motion plan, generating a run time motion plan, wherein the run time motion plan comprises a plurality of second pose-aware actions, and the plurality of second pose-aware actions are modified from the plurality of first pose-aware actions according to the run time marker information; and</claim-text><claim-text>the processor further executing the run time motion plan for controlling the manipulator to manipulate the object.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the first pose-aware actions of the base-case motion plan comprises variables and a base-case marker information corresponding to the plurality of markers.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method as claimed in <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising steps of:<claim-text>the processor calculating a difference between the base-case marker information and the run time marker information; and</claim-text><claim-text>the processor generating the plurality of second pose-aware actions according to the plurality of first pose-aware actions and the difference.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method as claimed in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein both the run time marker information and the base-case marker information comprise positions and orientations between the plurality of markers and the sensor.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the manipulator further comprises an end effector and a joint.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method as claimed in <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the first and the second pose-aware actions respectively comprise moving the end effector by position and orientation relative to the object.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method as claimed in <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the first and the second pose-aware actions respectively comprise at least one of the following actions:<claim-text>moving the end effector to a target pose;</claim-text><claim-text>traversing the end effector through a trajectory; and</claim-text><claim-text>moving the end effector associating with the run time marker information.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the object is placed at a fixed location on the carrier.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the markers comprise visual markers or fiducial markers.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the sensor comprises a camera.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. An adaptive mobile manipulation apparatus, comprising:<claim-text>a manipulator;</claim-text><claim-text>a sensor; and</claim-text><claim-text>a processor, coupled to the manipulator and the sensor, configured to perform the following steps:</claim-text><claim-text>retrieving a base-case motion plan comprising a plurality of first pose-aware actions;</claim-text><claim-text>driving the sensor to detect a plurality of markers located on a carrier to obtain a run time marker information;</claim-text><claim-text>according to the base-case motion plan, generating a run time motion plan, wherein the run time motion plan comprises a plurality of second pose-aware actions, and the plurality of second pose-aware actions are modified from the plurality of first pose-aware actions according to the run time marker information; and</claim-text><claim-text>executing the run time motion plan for controlling the manipulator to manipulate an object placed on the carrier.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The mobile manipulation apparatus as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the sensor comprises a camera.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The mobile manipulation apparatus as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the markers comprise visual markers or fiducial markers.</claim-text></claim></claims></us-patent-application>