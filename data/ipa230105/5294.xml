<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005295A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005295</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17838205</doc-number><date>20220611</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>2021107310028</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>16</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>021</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>171</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6217</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>021</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>172</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">NON-CONTACT FACIAL BLOOD PRESSURE MEASUREMENT METHOD BASED ON 3D CNN</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>ZHEJIANG NORMAL UNIVERSITY</orgname><address><city>Jinhua</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>XIONG</last-name><first-name>Jiping</first-name><address><city>Jinhua</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>CHEN</last-name><first-name>Zehui</first-name><address><city>Jinhua</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>LI</last-name><first-name>Jinhong</first-name><address><city>Jinhua</city><country>CN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A non-contact facial blood pressure measurement method based on 3D CNN is disclosed, which belongs to the technical field of computer vision. The method includes the following steps. S<b>110</b>: collecting an actual face video sample and training a blood pressure prediction model based on face images using 3D CNN neural network. S<b>120</b>: obtaining a face video in real time through a HD camera. S<b>130</b>: recognizing face key points in the face video obtained in S<b>120</b> through dlib face recognition model, selecting a face region of interest, and extracting face images from the region. S<b>140</b>: performing a wavelet transform operation on the face images extracted in S<b>130</b> to remove noise. S<b>150</b>: inputting seven consecutive frames of the face images into the 3D CNN blood pressure prediction model trained in S<b>110</b> to obtain a blood pressure value of the measured person. The disclosure realizes non-contact facial blood pressure measurement.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="99.48mm" wi="106.76mm" file="US20230005295A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="118.11mm" wi="108.71mm" file="US20230005295A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="123.61mm" wi="151.30mm" file="US20230005295A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This patent application claims the benefit and priority of Chinese Patent Application No. 202110731002 0.8 filed on Jun. 30, 2021, the disclosure of which is incorporated by reference herein in its entirety as part of the present application.</p><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to the technical field of computer vision, and more specifically, to a non-contact facial blood pressure measurement method based on 3D CNN.</p><heading id="h-0003" level="1">BACKGROUND ART</heading><p id="p-0004" num="0003">According to the Report On Cardiovascular Disease In China (2018), the prevalence and mortality of cardiovascular diseases in China are still on the rise. The report estimates that there are 290 million people with cardiovascular diseases, of which hypertension accounts for 245 million. Relevant surveys also show that the mortality of cardiovascular diseases ranks first, higher than that of tumors and other diseases, accounting for more than 40% of residents' disease deaths. Blood pressure refers to the lateral pressure acting on the blood vessel wall per unit area when blood flows in the blood vessel, which changes continuously in each heartbeat cycle. The maximum value of blood pressure is called systolic blood pressure, and the normal range is 90&#x2dc;140mmHg. The minimum value of blood pressure is called diastolic blood pressure, <sub>and </sub>the normal range is 60&#x2dc;90 mmHg. The accuracy and real-time of blood pressure value plays an extremely important role in the diagnosis and treatment of hypertension related diseases.</p><p id="p-0005" num="0004">Although the traditional invasive blood pressure measurement has accurate measurement results, and the arterial intubation method is known as the &#x201c;gold standard&#x201d;, it has been gradually replaced by non-invasive method because of great harm to patients and complex operation. Non-invasive measurement is also divided into intermittent and continuous. Although the products based on the flat tension method can measure the human blood pressure continuously for a long time and the results are more accurate, they need to be positioned in one position for a long time in use. It is difficult for the pressure sensor to ensure that the position will not deviate. At the same time, pressing the wrist for a long time makes the subjects feel uncomfortable. The cuff measurement based on the principle of constant volume method is not convenient for long-term observation of blood pressure, and will compress the arm, which will also produce discomfort.</p><p id="p-0006" num="0005">In recent years, the emergence of photoplethysmography (PPG) provides a new direction for blood pressure measurement. Pulse wave can reflect a lot of information about the subject's cardiovascular function, and theoretically, the formation of pulse is closely related to blood pressure. Non-contact blood pressure measurement has received extensive attention due to the technical requirements of low cost, simplicity and portability, wide application of small semiconductor components.</p><p id="p-0007" num="0006">There are few technical schemes to directly realize blood pressure measurement after image preprocessing by collecting continuous face images based on the strong feature extraction ability of CNN network, especially for the blood pressure measurement based on the temporal dimension characteristics of face region. For example, the patent with Publication No. CN110706826A (A non-contact real-time multi person heart rate and blood pressure measurement method based on video image) uses the second-order differential of the skin color image PPG signals of the whole face as the feature to calculate the blood pressure, without considering the characteristic information of the blood pressure in the time dimension. And in the patent with Publication No. CN111728602A (a non-contact blood pressure measurement device based on PPG), the blood pressure value is obtained by extracting PPG signals in multiple facial regions and then the signals are input into the trained LSTM model. Although this method makes use of timing data information, it does not take into account the characteristics of facial spatial dimension, and it needs to extract and process pulse wave signals, which is cumbersome. Another example is the patent with Publication No. CN110090010A (a non-contact blood pressure measurement method and system). In this method, the three primary color video trace curves of two regions of interest are extracted, and the pulse wave signal is extracted by blind source separation method. This method selects less regions of interest, and has very high requirements for parameters in the calculation method, which is easy to be affected by video frame rate, blind source separation effect, etc.</p><p id="p-0008" num="0007">Because face video contains rich features extracted from temporal and spatial dimensions, it is necessary to propose a non-contact facial blood pressure measurement method that captures the features of temporal and spatial dimensions to improve the accuracy of blood pressure measurement. In order to solve the above problems, a non-contact facial blood pressure measurement method based on 3D CNN is proposed.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0009" num="0008">The purpose of the disclosure is to obtain the video images of the face through the camera, detect the key points of the face using the dlib face recognition model, then determine the region of interest through the key points of the face, extract the face images from the region of interest, preprocess the obtained images by wavelet transform, and then input continuous frames of the face images into the trained 3D CNN blood pressure prediction model to obtain the blood pressure value of the measured person, so that the problems raised in the background technology may be solved.</p><p id="p-0010" num="0009">In order to achieve the above purpose, technical solutions of the present disclosure are specifically described as follows.</p><p id="p-0011" num="0010">A non-contact facial blood pressure measurement method based on 3D CNN is provided, which includes the following steps.</p><p id="p-0012" num="0011">S<b>110</b>: collecting an actual face video sample and training a blood pressure prediction model based on face images using 3D CNN neural network.</p><p id="p-0013" num="0012">S<b>120</b>: obtaining a face video in real time through a HD camera.</p><p id="p-0014" num="0013">S<b>130</b>: recognizing face key points in the face video obtained in S<b>120</b> through dlib face recognition model, selecting a face region of interest, and extracting face images from the region.</p><p id="p-0015" num="0014">S<b>140</b>: performing a wavelet transform operation on the face images extracted in S<b>130</b> to remove noise.</p><p id="p-0016" num="0015">S<b>150</b>: inputting seven consecutive frames of the face images into the 3D CNN blood pressure prediction model trained in S<b>110</b> to obtain a blood pressure value of the measured person.</p><p id="p-0017" num="0016">Preferably, the training 3D CNN blood pressure prediction model in S<b>110</b> includes the following steps.</p><p id="p-0018" num="0017">A<b>1</b>: recording a face video through a HD camera.</p><p id="p-0019" num="0018">A<b>2</b>: obtaining real-time blood pressure values through a cuff electronic sphygmomanometer.</p><p id="p-0020" num="0019">A<b>3</b>: detecting face key points in the face video obtained in A<b>1</b>, selecting a region of interest, and extracting face images of the region of interest.</p><p id="p-0021" num="0020">A<b>4</b>: preprocessing images, that is, performing a wavelet transform operation on the face images extracted in A<b>3</b> to remove noise, inputting seven consecutive frames of the face images and the corresponding real blood pressure values into a constructed 3D CNN model for training, then training the model based on a mean square error loss function, and finally obtaining the 3D CNN blood pressure prediction model.</p><p id="p-0022" num="0021">Preferably, in S<b>120</b>, when recording the face video of the measured person through the HD camera, the face of the measured person needs to be completely unobstructed. The face video recording needs to be carried out in a bright and stable environment. And at the same time, the face receives light evenly, and there is no obvious dark light area on the face. During the face video recording, the measured person shall keep the body stable, the head shall not shake or tremble, and the face shall be facing the camera until a set collecting time is reached. And in case of large shaking, the recording shall be carried out again.</p><p id="p-0023" num="0022">Preferably, the extracting face images in S<b>130</b> includes the following steps.</p><p id="p-0024" num="0023">B<b>1</b>: detecting four coordinate extreme values of the face in each frame of the face images through dlib face recognition model to determine a position of the face.</p><p id="p-0025" num="0024">B<b>2</b>: detecting 68 key points of the face, wherein positions of the key points includes chin, eyes, nose, mouth and other regions; and drawing an overall contour of the face through the key points.</p><p id="p-0026" num="0025">B<b>3</b>: determine the region of interest through the key points of the face, which includes the left and right cheeks, forehead, human middle, chin and nasal wing, and extracting and saving an image with a size of 50&#xd7;50 in each the region of interest.</p><p id="p-0027" num="0026">Preferably, the performing a wavelet transform operation on the face images in S<b>140</b> includes the following steps.</p><p id="p-0028" num="0027">C<b>1</b>: performing a wavelet transform on the images.</p><p id="p-0029" num="0028">C<b>2</b>: performing a threshold quantization on high-frequency coefficients after hierarchical decomposition.</p><p id="p-0030" num="0029">C<b>3</b>: reconstructing image signals by two-dimensional wavelet.</p><p id="p-0031" num="0030">Preferably, the blood pressure value of the measured person obtained in S<b>150</b> includes systolic blood pressure and diastolic blood pressure. The obtained blood pressure value is compared with a normal range of the blood pressure to judge whether the blood pressure value of the measured person is in the normal range.</p><p id="p-0032" num="0031">Compared with the prior art, the disclosure provides a non-contact facial blood pressure measurement method based on 3D CNN, which has the following beneficial effects.</p><p id="p-0033" num="0032">(1) The disclosure uses dlib face recognition model to detect the key points of each frame image of the face video, and prevents the position movement of the region of interest caused by face movement or body shaking.</p><p id="p-0034" num="0033">(2) The disclosure makes full use of the function of 3D convolution operation to extract spatial and temporal features from video data for action recognition, and uses 3D feature extractor to operate in spatial and temporal dimensions, so as to capture the motion information of multiple consecutive frames in face video stream.</p><p id="p-0035" num="0034">(3) The disclosure sets a 3D convolution neural network architecture based on 3D convolution feature extraction. The 3D CNN architecture generates multiple information channels from adjacent video frames, performs convolution and down sampling in each channel respectively, and obtains the final feature representation by combining the information from all channels.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a flowchart of the non-contact facial blood pressure measurement method based on 3D CNN proposed by the disclosure.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram of face key point detection of the non-contact facial blood pressure measurement method based on 3D CNN proposed by the disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0038" num="0037">The technical scheme in the embodiments of the disclosure will be clearly and completely described below in combination with the attached drawings in the embodiments of the disclosure. Obviously, the described embodiments are only part of the embodiments of the disclosure, not all of the embodiments.</p><heading id="h-0007" level="1">Embodiment 1</heading><p id="p-0039" num="0038">Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref> and <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a non-contact facial blood pressure measurement method based on 3D CNN includes the following steps.</p><p id="p-0040" num="0039">S<b>110</b>: collecting an actual face video sample and training a blood pressure prediction model based on face images using 3D CNN neural network.</p><p id="p-0041" num="0040">S<b>120</b>: obtaining a face video in real time through a HD camera.</p><p id="p-0042" num="0041">S<b>130</b>: recognizing face key points in the face video obtained in S<b>120</b> through dlib face recognition model, selecting a face region of interest, and extracting face images from the region.</p><p id="p-0043" num="0042">S<b>140</b>: performing a wavelet transform operation on the face images extracted in S<b>130</b> to remove noise.</p><p id="p-0044" num="0043">S<b>150</b>: inputting seven consecutive frames of the face images into the 3D CNN blood pressure prediction model trained in S<b>110</b> to obtain a blood pressure value of the measured person.</p><p id="p-0045" num="0044">In S<b>120</b>, when recording the face video of the measured person through the HD camera, the face of the measured person needs to be completely unobstructed. The face video recording needs to be carried out in a bright and stable environment. And at the same time, the face receives light evenly, and there is no obvious dark light area on the face. During the face video recording, the measured person shall keep the body stable, the head shall not shake or tremble, and the face shall be facing the camera until a set collecting time is reached. And in case of large shaking, the recording shall be carried out again.</p><p id="p-0046" num="0045">The blood pressure value of the measured person obtained in S<b>150</b> includes systolic blood pressure and diastolic blood pressure. The obtained blood pressure value is compared with a normal range of the blood pressure to judge whether the blood pressure value of the measured person is in the normal range.</p><heading id="h-0008" level="1">Embodiment 2</heading><p id="p-0047" num="0046">The embodiment 2 is based on embodiment 1, but the difference is as follows.</p><p id="p-0048" num="0047">The training 3D CNN blood pressure prediction model in S<b>110</b> includes the following steps.</p><p id="p-0049" num="0048">A<b>1</b>: recording a face video through a HD camera.</p><p id="p-0050" num="0049">A<b>2</b>: obtaining real-time blood pressure values through a cuff electronic sphygmomanometer.</p><p id="p-0051" num="0050">A<b>3</b>: detecting face key points in the face video obtained in A<b>1</b>, selecting a region of interest, and extracting face images of the region of interest.</p><p id="p-0052" num="0051">A<b>4</b>: preprocessing images, that is, performing a wavelet transform operation on the face images extracted in A<b>3</b> to remove noise, inputting seven consecutive frames of the face images and the corresponding real blood pressure values into a constructed 3D CNN model for training, then training the model based on a mean square error loss function, and finally obtaining the 3D CNN blood pressure prediction model.</p><p id="p-0053" num="0052">Specifically, the first layer of 3D CNN architecture is the hardwired layer, which processes the original frames to generate signals of multiple channels, then processes the multiple channels respectively, and finally combines the information of all channels to obtain the final features.</p><p id="p-0054" num="0053">The information of three channels is extracted from each frame, which are grayscale, gradient in x and y directions. The three channels of grayscale, gradients in x and y directions can be calculated per frame, and each channel is convolved using the set convolution kernel to extract different features.</p><p id="p-0055" num="0054">The disclosure makes full use of the function of 3D convolution operation to extract spatial and temporal features from video data for action recognition. By using the 3D feature extractor to operate in the spatial and temporal dimensions, the motion information of multiple consecutive frames in the face video stream is captured.</p><p id="p-0056" num="0055">The disclosure sets a 3D convolution neural network architecture based on 3D convolution feature extraction. The 3D CNN architecture generates multiple information channels from adjacent video frames, performs convolution and down sampling in each channel respectively, and obtains the final feature representation by combining the information from all channels.</p><heading id="h-0009" level="1">Embodiment 3</heading><p id="p-0057" num="0056">The embodiment 3 is based on embodiment 1 and 2, but the difference is as follows.</p><p id="p-0058" num="0057">The extracting face images in S<b>130</b> includes the following steps.</p><p id="p-0059" num="0058">B<b>1</b>: detecting four coordinate extreme values of the face in each frame of the face images through dlib face recognition model to determine a position of the face.</p><p id="p-0060" num="0059">B<b>2</b>: detecting 68 key points of the face, wherein positions of the key points includes chin, eyes, nose, mouth and other regions; and drawing an overall contour of the face through the key points.</p><p id="p-0061" num="0060">B<b>3</b>: determine the region of interest through the key points of the face, which includes the left and right cheeks, forehead, human middle, chin and nasal wing, and extracting and saving an image with a size of 50&#xd7;50 in each the region of interest.</p><p id="p-0062" num="0061">The disclosure uses dlib face recognition model to detect the key points of each frame image of the face video, and prevents the position movement of the region of interest caused by face movement or body shaking.</p><heading id="h-0010" level="1">Embodiment 4</heading><p id="p-0063" num="0062">The embodiment 4 is based on embodiment 1 to 3, but the difference is as follows.</p><p id="p-0064" num="0063">The performing a wavelet transform operation on the face images in S<b>140</b> includes the following steps.</p><p id="p-0065" num="0064">C<b>1</b>: performing a wavelet transform on the images.</p><p id="p-0066" num="0065">C<b>2</b>: performing a threshold quantization on high-frequency coefficients after hierarchical decomposition.</p><p id="p-0067" num="0066">C<b>3</b>: reconstructing image signals by two-dimensional wavelet.</p><p id="p-0068" num="0067">The above is only the preferred specific embodiments of the disclosure, but the protection scope of the disclosure is not limited to this. Within the technical scope disclosed by the disclosure, the equivalent replacement or change implemented according to the technical scheme and inventive concept of the disclosure by any technician familiar with the technical field shall be covered by the protection scope of the disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A non-contact facial blood pressure measurement method based on 3D CNN, comprising the following steps:<claim-text>S<b>110</b>: collecting an actual face video sample and training a blood pressure prediction model based on face images using 3D CNN neural network;</claim-text><claim-text>S<b>120</b>: obtaining a face video in real time through a HD camera;</claim-text><claim-text>S<b>130</b>: recognizing face key points in the face video obtained in S<b>120</b> through dlib face recognition model, selecting a face region of interest, and extracting face images from the region;</claim-text><claim-text>S<b>140</b>: performing a wavelet transform operation on the face images extracted in S<b>130</b> to remove noise; and</claim-text><claim-text>S<b>150</b>: inputting seven consecutive frames of the face images into the 3D CNN blood pressure prediction model trained in S<b>110</b> to obtain a blood pressure value of the measured person.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The non-contact facial blood pressure measurement method based on 3D CNN of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the training 3D CNN blood pressure prediction model in S<b>110</b> comprises the following steps:<claim-text>A<b>1</b>: recording a face video through a HD camera;</claim-text><claim-text>A<b>2</b>: obtaining real-time blood pressure values through a cuff electronic sphygmomanometer;</claim-text><claim-text>A<b>3</b>: detecting face key points in the face video obtained in A<b>1</b>, selecting a region of interest, and extracting face images of the region of interest;</claim-text><claim-text>A<b>4</b>: preprocessing images, that is, performing a wavelet transform operation on the face images extracted in A<b>3</b> to remove noise, inputting seven consecutive frames of the face images and the corresponding real blood pressure values into a constructed 3D CNN model for training, then training the model based on a mean square error loss function, and finally obtaining the 3D CNN blood pressure prediction model.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The non-contact facial blood pressure measurement method based on 3D CNN of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein in S<b>120</b>, when recording the face video of the measured person through the HD camera, the face of the measured person needs to be completely unobstructed; the face video recording needs to be carried out in a bright and stable environment, and at the same time, the face receives light evenly, and there is no obvious dark light area on the face; during the face video recording, the measured person shall keep the body stable, the head shall not shake or tremble, and the face shall be facing the camera until a set collecting time is reached; and in case of large shaking, the recording shall be carried out again.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The non-contact facial blood pressure measurement method based on 3D CNN of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the extracting face images in S<b>130</b> comprises the following steps:<claim-text>B<b>1</b>: detecting four coordinate extreme values of the face in each frame of the face images through dlib face recognition model to determine a position of the face;</claim-text><claim-text>B<b>2</b>: detecting 68 key points of the face, wherein positions of the key points comprises chin, eyes, nose, mouth and other regions; and drawing an overall contour of the face through the key points; and</claim-text><claim-text>B<b>3</b>: determine the region of interest through the key points of the face, comprising the left and right cheeks, forehead, human middle, chin and nasal wing, and extracting and saving an image with a size of 50&#xd7;50 in each the region of interest.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The non-contact facial blood pressure measurement method based on 3D CNN of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the performing a wavelet transform operation on the face images in S<b>140</b> comprises the following steps:<claim-text>C<b>1</b>: performing a wavelet transform on the images;</claim-text><claim-text>C<b>2</b>: performing a threshold quantization on high-frequency coefficients after hierarchical decomposition; and</claim-text><claim-text>C<b>3</b>: reconstructing image signals by two-dimensional wavelet.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The non-contact facial blood pressure measurement method based on 3D CNN of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the blood pressure value of the measured person obtained in S<b>150</b> comprises systolic blood pressure and diastolic blood pressure, and the obtained blood pressure value is compared with a normal range of the blood pressure to judge whether the blood pressure value of the measured person is in the normal range.</claim-text></claim></claims></us-patent-application>