<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005169A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005169</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17942825</doc-number><date>20220912</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>521</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>521</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>0454</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10028</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">LIDAR POINT SELECTION USING IMAGE SEGMENTATION</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16815123</doc-number><date>20200311</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11481913</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17942825</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>GM Cruise Holdings LLC</orgname><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Deegan</last-name><first-name>Thomas</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Zhao</last-name><first-name>Yue</first-name><address><city>Sunnyvale</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The subject disclosure relates to techniques for selecting points of an image for processing with LiDAR data. A process of the disclosed technology can include steps for receiving an image comprising a first image object and a second image object, processing the image to place a bounding box around the first image object and the second image object, and processing an image area within the bounding box to identify a first image mask corresponding with a first pixel region of the first image object and a second image mask corresponding with a second pixel region of the second image object. Systems and machine-readable media are also provided.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="214.21mm" wi="153.92mm" file="US20230005169A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="223.01mm" wi="165.35mm" file="US20230005169A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="197.95mm" wi="166.71mm" orientation="landscape" file="US20230005169A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="197.95mm" wi="167.05mm" orientation="landscape" file="US20230005169A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="228.09mm" wi="155.96mm" file="US20230005169A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="229.19mm" wi="172.38mm" orientation="landscape" file="US20230005169A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="159.26mm" wi="121.24mm" orientation="landscape" file="US20230005169A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation and claims the benefit of U.S. application Ser. No. 16/815,123, filed on Mar. 11, 2020, which is expressly incorporated by reference herein in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">1. Technical Field</heading><p id="p-0003" num="0002">The subject technology provides solutions for facilitating distance estimations of image objects and in particular, for using machine-learning models to segment potentially occluded image objects for distance estimation using Light Detection and Ranging (LiDAR) data.</p><heading id="h-0004" level="1">2. Introduction</heading><p id="p-0004" num="0003">Image processing systems that are configured to perform object recognition often use bounding boxes to identify image regions corresponding with locations of objects of interest. In some applications, objects need to be transformed into three-dimensional (<b>3</b>D) space.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0005" num="0004">Certain features of the subject technology are set forth in the appended claims. However, the accompanying drawings, which are included to provide further understanding, illustrate disclosed aspects and together with the description serve to explain the principles of the subject technology. In the drawings:</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example system that can be used to select points in images for processing with Light Detection and Ranging (LiDAR) data, according to some aspects of the disclosed technology.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example of an initial bounding box placement performed using a bounding box placement process of the disclosed technology.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example of an image mask placement using an image mask placement process, according to some aspects of the disclosed technology.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates steps of an example process for selecting points in an image for processing with LiDAR data, according to some aspects of the disclosed technology.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example environment that includes an autonomous vehicle in communication with a remote computing system, according to some aspects of the disclosed technology.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example processor-based system with which some aspects of the subject technology can be implemented.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0012" num="0011">The detailed description set forth below is intended as a description of various configurations of the subject technology and is not intended to represent the only configurations in which the subject technology can be practiced. The appended drawings are incorporated herein and constitute a part of the detailed description. The detailed description includes specific details for the purpose of providing a more thorough understanding of the subject technology. However, it will be clear and apparent that the subject technology is not limited to the specific details set forth herein and may be practiced without these details. In some instances, structures and components are shown in block diagram form in order to avoid obscuring the concepts of the subject technology.</p><p id="p-0013" num="0012">As described herein, one aspect of the present technology is the gathering and use of data available from various sources to improve quality and experience. The present disclosure contemplates that in some instances, this gathered data may include personal information. The present disclosure contemplates that the entities involved with such personal information respect and value privacy policies and practices.</p><p id="p-0014" num="0013">In some image processing techniques, bounding boxes are used to identify an image region that contains one or more objects (image objects) of potential interest. However, using conventional bounding box processing techniques, it is not uncommon for multiple image objects to overlap in two-dimensional (<b>2</b>D) pixel space. For example, an object of potential interest may be partially occluded by an object of no interest; alternatively, two objects of interest may partially occlude one another. Close proximities (or occlusions) between image objects make it difficult to disaggregate and classify the image objects, as well as to perform object ranging, for example, to determine locations of different image objects in three-dimensional (<b>3</b>D) space. Such errors can be especially problematic for image-processing needed to enable autonomous vehicle (AV) navigation and guidance. For example, autonomous vehicles rely heavily on thousands of images received from onboard cameras or sensors every minute to navigate on roadways. Each of these images capture many objects, such as pedestrians, other vehicles, sidewalks, road signs, etc. However, all of these objects are captured in two-dimensional (2D) space as images. Thus, there is a need to maximize highly accurate data during transformation from 2D space into 3D space. In other words, there is a need for facilitating highly accurate distance estimations of objects in images.</p><p id="p-0015" num="0014">Aspects of the disclosed technology address the limitations of conventional distance estimations of objects in images by using machine-learning models to segment the images and select points within the segments of the images for processing with Light Detection and Ranging (LiDAR) data. More specifically, one or more neural networks are trained to classify pixels based on objects in the image at the corresponding pixel and place bounding boxes around the objects. The one or more neural networks also segment images into image segments or pixel regions that can be selected. The one or more neural networks may then combine the bounding boxes and the pixel regions to select specific pixels or points on the image to belong to each object. Then, the selected points of the image can be processed with LiDAR data to determine the depth of the object at the selected points of the image.</p><p id="p-0016" num="0015">As understood by those of skill in the art, machine-learning based classification techniques can vary depending on the desired implementation. For example, machine-learning classification schemes can utilize one or more of the following, alone or in combination: hidden Markov models, recurrent neural networks (RNNs), convolutional neural networks (CNNs); Deep Learning networks, Bayesian symbolic methods, general adversarial networks (GANs), support vector machines, image registration methods, and/or applicable rule-based systems. Where regression algorithms are used, they can include but are not limited to: a Stochastic Gradient Descent Regressors, and/or Passive Aggressive Regressors, etc.</p><p id="p-0017" num="0016">Machine learning classification models can also be based on clustering algorithms (e.g., a Mini-batch K-means clustering algorithm), a recommendation algorithm (e.g., a Miniwise Hashing algorithm, or Euclidean Locality-Sensitive Hashing (LSH) algorithm), and/or an anomaly detection algorithm, such as a Local outlier factor. Additionally, machine-learning models can employ a dimensionality reduction approach, such as, one or more of: a Mini-batch Dictionary Learning algorithm, an Incremental Principal Component Analysis (PCA) algorithm, a Latent Dirichlet Allocation algorithm, and/or a Mini-batch K-means algorithm, etc.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example system <b>100</b> that can be used to facilitate distance estimates of objects, according to some aspects of the disclosed technology. System <b>100</b> includes one or more cameras <b>102</b> that capture and store unprocessed (unbounded) images into a repository of unclassified images <b>104</b> from which the unprocessed images are provided to one or more neural networks <b>106</b>. Similarly, the system <b>100</b> also includes one or more Light Detection and Ranging (LiDAR) instruments <b>114</b> that record and capture LiDAR data, for example, that is stored in a pointcloud database <b>116</b>. The LiDAR data can represent the same objects recorded by cameras <b>102</b>; that is, the LiDAR data corresponds to the images captured by the one or more cameras <b>102</b>.</p><p id="p-0019" num="0018">The one or more neural networks <b>106</b> can be configured to receive unclassified images <b>104</b> and to to identify one or more image objects in the unprocessed images. Neural networks <b>106</b> can then place bounding boxes around the one or more image objects, and output the bounded image objects to a repository of bounded image objects <b>108</b>. For example, an image may include a person (i.e. a first image object) occluded by a bush (i.e. a second image object); thus, the one or more neural network <b>106</b> will bound the person with a first bounding box and the bush with a second bounding box. However, in some instances, parts of the bush may overlap the first bounding box and parts of the person may overlap the second bounding box.</p><p id="p-0020" num="0019">The one or more neural networks <b>106</b> can also be configured to receive and segment, based upon detected objects, the unbounded image into pixel regions associated with the detected objects. The one or more neural networks <b>106</b> can then identify image masks corresponding to the pixel regions. The image masks are then stored in a repository of image masks <b>110</b>.</p><p id="p-0021" num="0020">The bounded image objects and the image masks of interest are combined to select points <b>112</b> for processing with LiDAR data. Selected points <b>112</b> are then processed <b>118</b> with the corresponding LiDAR data stored in the pointcloud <b>116</b> to determine distance estimates <b>120</b> corresponding to each selected point in the bounded image objects.</p><p id="p-0022" num="0021">In some instances, image processing <b>118</b> may identify masks corresponding to detected objects only within the bounded image objects. In other words, the image processing <b>118</b> may occur within an image area within the bounding box to identify the image masks corresponding to the pixel region of the bounded image object. In these instances, the total processing power utilized is reduced because the image processing <b>118</b> is not occurring for all objects in the image.</p><p id="p-0023" num="0022">In some implementations, the one or more neural networks <b>106</b> can process one or more pixels in the pixel regions to determine classification labels for the image objects. The one or more neural networks <b>106</b> can then associate the classification label with the image objects. Examples of the foregoing embodiments are discussed in relation to graphical examples provided by <figref idref="DRAWINGS">FIGS. <b>2</b>- <b>5</b></figref>, discussed below.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>2</b></figref> graphically illustrates placement of bounding boxes <b>212</b>, <b>222</b> performed using a process of the disclosed technology. In the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, image <b>200</b> can represent an initial output of a bounding box process performed by one or more neural networks. The one or more neural networks are configured to detect one or more image objects, such as vehicles, people, signs, etc. In this example, a first image object <b>210</b> and a second image object <b>220</b> are detected (e.g., using machine-learning techniques). The first image object <b>210</b> is a pedestrian walking and the second image object <b>220</b> is a vehicle. The one or more neural networks insert into image <b>200</b> a first bounding box <b>212</b> encompassing a pixel area of the first image object <b>210</b> and a second bounding box <b>222</b> encompassing a pixel area of the second image object <b>220</b>. As shown, the first image object <b>210</b> is partially occluded by the second image object <b>220</b>, such that the first bounding box <b>212</b> includes a portion <b>224</b> of the second image object <b>204</b> (the pole). In other words, the pedestrian is positioned behind the pole, partially occluded by the pole. Similarly, the second bounding box <b>222</b> includes a portion <b>214</b> of the first image object <b>210</b> (the pedestrian). It is understood that a similar bounding box processing methodology can be applied to one or more other image objects in the same image set.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a placement of image masks <b>312</b>, <b>322</b> performed using a process of the disclosed technology. In particular, image <b>300</b> illustrates an example in which image masks <b>312</b>, <b>322</b> are identified in the image <b>300</b>. As shown, in some instances, the image masks <b>312</b>, <b>322</b> are identified solely within bounding boxes <b>212</b>, <b>222</b>. As further illustrated, a first image mask <b>312</b> is identified corresponding with a first pixel region of the first image object <b>210</b> and a second image mask <b>322</b> is identified corresponding with a second pixel region of the second image object <b>220</b>. The image masks <b>312</b>, <b>322</b> thus more accurately encompasses and identify respectively the first and second image objects <b>210</b>, <b>220</b>. Additionally, as discussed above, additional processing steps can be performed to identify semantic labels that are associated with the image object. In the example provided by images <b>200</b>, <b>300</b>, the semantic label &#x201c;person&#x201d; may be associated with the first image object <b>210</b> and the semantic label &#x201c;car&#x201d; may be associated with the second image object <b>220</b>.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>3</b></figref> further illustrates points (shown as dots) within the image masks <b>312</b>, <b>322</b>. These points are selected for processing with LiDAR data, such that LiDAR data corresponding to the location of the points is used to process the points for distance estimates. Thus, each point corresponds to a distance estimate that may be used by an autonomous vehicle. In some instances, the location of the points may be used to process distance estimates for each image object <b>210</b>, <b>220</b> as a whole. In other words, the output of the points processed with LiDAR data will result in a depth of or distance estimate of the image object <b>210</b>, <b>220</b> instead of distance estimates for each individual point.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates steps of an example process <b>400</b> for point selection for processing with LiDAR data, according to some aspects of the disclosed technology. Process <b>400</b> begins with step <b>402</b> in which an image is received. More specifically, the image is received from a first data set recorded by one or more cameras. In some instances, the image may be received from a machine-learning model that has processed the image. In other instances, as discussed above, the image may be an unprocessed image that contains one or more unbounded image objects. The image comprises at least a first image object and a second image object, such that at least a portion of the first image object is occluded or obscured by at least a portion of the second image object. For example, a leg of a person (i.e. a portion of the first image object) is partially occluded by a stop sign (i.e. the second image object). Furthermore, we may be interested in selecting points associated with the person for processing with LiDAR data. In other words, we may be interested in determining a distance estimate of the person, but not the stop sign.</p><p id="p-0028" num="0027">For clarity and discussion purposes, the following will discuss an instance, in which at least a portion of the first image object is obscured by at least a portion of the first second image object, wherein the first image object is a primary image object of interest for LiDAR point selection. However, it is to be understood that the first image object and the second image object may be used interchangeably to label or select image objects. In some instances, the opposite may be true, in that at least a portion of the second image object is occluded or obscured by at least a portion of the first image object and the second image object is the image object of interest. In some instances, both the first and second image objects may occlude or obscure at least a portion of the other image object. Moreover, one of ordinary skill in the art will understand that any combination of image objects may be of interest for LiDAR point selection.</p><p id="p-0029" num="0028">In step <b>404</b>, the image is processed to identify and place a bounding box around the image object of interest (i.e. the first image object) and the portion of the second image object that partially occludes the first image object. In some instances, the image is processed through one or more neural networks, such that processing the image to place the bounding box around the first image object and the second image object is performed using a first machine-learning model.</p><p id="p-0030" num="0029">In step <b>406</b>, the image is processed to identify image masks corresponding to pixel regions of each image object. Thus, the image is processed to identify a first image mask corresponding with a first pixel region of the first image object and a second image mask corresponding with a second pixel region of the second image object. In some instances, only an image area within the bounding box of the image is processed, such that the overall processing power is reduced. In some instances, the image is processed through one or more neural networks, such that processing the image or image area within the bounding box to identify the first image mask and second image masks is performed using a first machine-learning model.</p><p id="p-0031" num="0030">In step <b>408</b>, one or more pixels in the pixel regions are processed to determine classification labels for the image objects. Thus, one or more pixels in the first pixel region are processed to determine a first classification label for the first object and one or more pixels in the second pixel region are processed to determine a second classification label for the second image object.</p><p id="p-0032" num="0031">In step <b>410</b>, the classification labels are associated with the corresponding image objects. In other words, the first classification label is associated with the first image object and the second classification label is associated with the second image object.</p><p id="p-0033" num="0032">In step <b>412</b>, the pixel regions are processed with LiDAR data to determine a depth of the corresponding image objects. Thus, the first pixel region is processed to determine a depth of the first image object based on LiDAR data and the second pixel region is processed to determine a depth of the second image object based on LiDAR data.</p><p id="p-0034" num="0033">In step <b>414</b>, the image objects are associated with a range or distance estimate based upon the determined depths, which is based on LiDAR data. Thus, the first image object is associated with a first range or distance estimate based on LiDAR data and the second image object is associated with a second range or distance estimate based on LiDAR data.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates environment <b>500</b> that includes an autonomous vehicle <b>502</b> in communication with a remote computing system <b>550</b>.</p><p id="p-0036" num="0035">Autonomous vehicle <b>502</b> can navigate about roadways without a human driver based upon sensor signals output by sensor systems <b>504</b>-<b>506</b> of the autonomous vehicle <b>502</b>. The autonomous vehicle <b>502</b> includes a plurality of sensor systems <b>504</b>-<b>506</b> (a first sensor system <b>504</b> through an Nth sensor system <b>506</b>). The sensor systems <b>504</b>-<b>506</b> are of different types and are arranged about the autonomous vehicle <b>502</b>. For example, the first sensor system <b>504</b> may be a camera sensor system, and the Nth sensor system <b>506</b> may be a lidar sensor system. Other exemplary sensor systems include radar sensor systems, global positioning system (GPS) sensor systems, inertial measurement units (IMU), infrared sensor systems, laser sensor systems, sonar sensor systems, and the like.</p><p id="p-0037" num="0036">The autonomous vehicle <b>502</b> further includes several mechanical systems that are used to effectuate appropriate motion of the autonomous vehicle <b>502</b>. For instance, the mechanical systems can include but are not limited to, a vehicle propulsion system <b>530</b>, a braking system <b>532</b>, and a steering system <b>534</b>. The vehicle propulsion system <b>530</b> may include an electric motor, an internal combustion engine, or both. The braking system <b>532</b> can include an engine brake, brake pads, actuators, and/or any other suitable componentry that is configured to assist in decelerating the autonomous vehicle <b>502</b>. The steering system <b>534</b> includes suitable componentry that is configured to control the direction of movement of the autonomous vehicle <b>502</b> during navigation.</p><p id="p-0038" num="0037">The autonomous vehicle <b>502</b> further includes a safety system <b>536</b> that can include various lights and signal indicators, parking brake, airbags, etc. The autonomous vehicle <b>502</b> further includes a cabin system <b>538</b> that can include cabin temperature control systems, in-cabin entertainment systems, etc.</p><p id="p-0039" num="0038">The autonomous vehicle <b>502</b> additionally comprises an internal computing system <b>510</b> that is in communication with the sensor systems <b>504</b>-<b>506</b> and the systems <b>530</b>, <b>532</b>, <b>534</b>, <b>536</b>, and <b>538</b>. The internal computing system includes at least one processor and at least one memory having computer-executable instructions that are executed by the processor. The computer-executable instructions can make up one or more services responsible for controlling the autonomous vehicle <b>502</b>, communicating with remote computing system <b>550</b>, receiving inputs from passengers or human co-pilots, logging metrics regarding data collected by sensor systems <b>504</b>-<b>506</b> and human co-pilots, etc.</p><p id="p-0040" num="0039">Internal computing system <b>510</b> can include control service <b>512</b> that is configured to control the operation of vehicle propulsion system <b>530</b>, braking system <b>532</b>, steering system <b>534</b>, safety system <b>536</b>, and cabin system <b>538</b>. The control service <b>512</b> receives sensor signals from the sensor systems <b>504</b>-<b>506</b> as well communicates with other services of the internal computing system <b>510</b> to effectuate operation of the autonomous vehicle <b>502</b>. In some embodiments, control service <b>512</b> may carry out operations in concert one or more other systems of autonomous vehicle <b>502</b>.</p><p id="p-0041" num="0040">The internal computing system <b>510</b> can also include a constraint service <b>514</b> to facilitate safe propulsion of the autonomous vehicle <b>502</b>. The constraint service <b>514</b> includes instructions for activating a constraint based on a rule-based restriction upon operation of the autonomous vehicle <b>502</b>. For example, the constraint may be a restriction upon navigation that is activated in accordance with protocols configured to avoid occupying the same space as other objects, abide by traffic laws, circumvent avoidance areas, etc. In some embodiments, the constraint service can be part of the control service <b>512</b>.</p><p id="p-0042" num="0041">The internal computing system <b>510</b> can also include a communication service <b>516</b>. The communication service can include both software and hardware elements for transmitting and receiving signals from/to the remote computing system <b>550</b>. The communication service <b>516</b> is configured to transmit information wirelessly over a network, for example, through an antenna array that provides personal cellular (long-term evolution (LTE), 3G, 5G, etc.) communication.</p><p id="p-0043" num="0042">In some embodiments, one or more services of the internal computing system <b>510</b> are configured to send and receive communications to remote computing system <b>550</b> for such reasons as reporting data for training and evaluating machine learning algorithms, requesting assistance from remoting computing system or a human operator via remote computing system <b>550</b>, software service updates, ridesharing pickup and drop off instructions etc.</p><p id="p-0044" num="0043">The internal computing system <b>510</b> can also include a latency service <b>518</b>. The latency service <b>518</b> can utilize timestamps on communications to and from the remote computing system <b>550</b> to determine if a communication has been received from the remote computing system <b>550</b> in time to be useful. For example, when a service of the internal computing system <b>510</b> requests feedback from remote computing system <b>550</b> on a time-sensitive process, the latency service <b>518</b> can determine if a response was timely received from remote computing system <b>550</b> as information can quickly become too stale to be actionable. When the latency service <b>518</b> determines that a response has not been received within a threshold, the latency service <b>518</b> can enable other systems of autonomous vehicle <b>502</b> or a passenger to make necessary decisions or to provide the needed feedback.</p><p id="p-0045" num="0044">The internal computing system <b>510</b> can also include a user interface service <b>520</b> that can communicate with cabin system <b>538</b> in order to provide information or receive information to a human co-pilot or human passenger. In some embodiments, a human co-pilot or human passenger may be required to evaluate and override a constraint from constraint service <b>514</b>, or the human co-pilot or human passenger may wish to provide an instruction to the autonomous vehicle <b>502</b> regarding destinations, requested routes, or other requested operations.</p><p id="p-0046" num="0045">As described above, the remote computing system <b>550</b> is configured to send/receive a signal from the autonomous vehicle <b>502</b> regarding reporting data for training and evaluating machine learning algorithms, requesting assistance from remote computing system <b>550</b> or a human operator via the remote computing system <b>550</b>, software service updates, rideshare pickup and drop off instructions, etc.</p><p id="p-0047" num="0046">The remote computing system <b>550</b> includes an analysis service <b>552</b> that is configured to receive data from autonomous vehicle <b>502</b> and analyze the data to train or evaluate machine learning algorithms for operating the autonomous vehicle <b>502</b>. The analysis service <b>552</b> can also perform analysis pertaining to data associated with one or more errors or constraints reported by autonomous vehicle <b>502</b>.</p><p id="p-0048" num="0047">The remote computing system <b>550</b> can also include a user interface service <b>554</b> configured to present metrics, video, pictures, sounds reported from the autonomous vehicle <b>502</b> to an operator of remote computing system <b>550</b>. User interface service <b>554</b> can further receive input instructions from an operator that can be sent to the autonomous vehicle <b>502</b>.</p><p id="p-0049" num="0048">The remote computing system <b>550</b> can also include an instruction service <b>556</b> for sending instructions regarding the operation of the autonomous vehicle <b>502</b>. For example, in response to an output of the analysis service <b>552</b> or user interface service <b>554</b>, instructions service <b>556</b> can prepare instructions to one or more services of the autonomous vehicle <b>502</b> or a co-pilot or passenger of the autonomous vehicle <b>502</b>.</p><p id="p-0050" num="0049">The remote computing system <b>550</b> can also include a rideshare service <b>558</b> configured to interact with ridesharing application <b>570</b> operating on (potential) passenger computing devices. The rideshare service <b>558</b> can receive requests to be picked up or dropped off from passenger ridesharing app <b>570</b> and can dispatch autonomous vehicle <b>502</b> for the trip. The rideshare service <b>558</b> can also act as an intermediary between the ridesharing app <b>570</b> and the autonomous vehicle wherein a passenger might provide instructions to the autonomous vehicle to <b>102</b> go around an obstacle, change routes, honk the horn, etc.</p><p id="p-0051" num="0050">As described herein, one aspect of the present technology is the gathering and use of data available from various sources to improve quality and experience. The present disclosure contemplates that in some instances, this gathered data may include personal information. The present disclosure contemplates that the entities involved with such personal information respect and value privacy policies and practices.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows an example of computing system <b>600</b>, which can be for example any computing device making up internal computing system <b>510</b>, remote computing system <b>550</b>, (potential) passenger device executing rideshare app <b>570</b>, or any component thereof in which the components of the system are in communication with each other using connection <b>605</b>. Connection <b>605</b> can be a physical connection via a bus, or a direct connection into processor <b>610</b>, such as in a chipset architecture. Connection <b>605</b> can also be a virtual connection, networked connection, or logical connection.</p><p id="p-0053" num="0052">In some embodiments, computing system <b>600</b> is a distributed system in which the functions described in this disclosure can be distributed within a datacenter, multiple data centers, a peer network, etc. In some embodiments, one or more of the described system components represents many such components each performing some or all of the function for which the component is described. In some embodiments, the components can be physical or virtual devices.</p><p id="p-0054" num="0053">Example system <b>600</b> includes at least one processing unit (CPU or processor) <b>610</b> and connection <b>605</b> that couples various system components including system memory <b>615</b>, such as read-only memory (ROM) <b>620</b> and random access memory (RAM) <b>625</b> to processor <b>610</b>. Computing system <b>600</b> can include a cache of high-speed memory <b>612</b> connected directly with, in close proximity to, or integrated as part of processor <b>610</b>.</p><p id="p-0055" num="0054">Processor <b>610</b> can include any general purpose processor and a hardware service or software service, such as services <b>632</b>, <b>634</b>, and <b>636</b> stored in storage device <b>630</b>, configured to control processor <b>610</b> as well as a special-purpose processor where software instructions are incorporated into the actual processor design. Processor <b>610</b> may essentially be a completely self-contained computing system, containing multiple cores or processors, a bus, memory controller, cache, etc. A multi-core processor may be symmetric or asymmetric.</p><p id="p-0056" num="0055">To enable user interaction, computing system <b>600</b> includes an input device <b>645</b>, which can represent any number of input mechanisms, such as a microphone for speech, a touch-sensitive screen for gesture or graphical input, keyboard, mouse, motion input, speech, etc. Computing system <b>600</b> can also include output device <b>635</b>, which can be one or more of a number of output mechanisms known to those of skill in the art. In some instances, multimodal systems can enable a user to provide multiple types of input/output to communicate with computing system <b>600</b>. Computing system <b>600</b> can include communications interface <b>640</b>, which can generally govern and manage the user input and system output. There is no restriction on operating on any particular hardware arrangement, and therefore the basic features here may easily be substituted for improved hardware or firmware arrangements as they are developed.</p><p id="p-0057" num="0056">Storage device <b>630</b> can be a non-volatile memory device and can be a hard disk or other types of computer readable media which can store data that are accessible by a computer, such as magnetic cassettes, flash memory cards, solid state memory devices, digital versatile disks, cartridges, random access memories (RAMs), read-only memory (ROM), and/or some combination of these devices.</p><p id="p-0058" num="0057">The storage device <b>630</b> can include software services, servers, services, etc., that when the code that defines such software is executed by the processor <b>610</b>, it causes the system to perform a function. In some embodiments, a hardware service that performs a particular function can include the software component stored in a computer-readable medium in connection with the necessary hardware components, such as processor <b>610</b>, connection <b>605</b>, output device <b>635</b>, etc., to carry out the function.</p><p id="p-0059" num="0058">For clarity of explanation, in some instances, the present technology may be presented as including individual functional blocks including functional blocks comprising devices, device components, steps or routines in a method embodied in software, or combinations of hardware and software.</p><p id="p-0060" num="0059">Embodiments within the scope of the present disclosure may also include tangible and/or non-transitory computer-readable storage media or devices for carrying or having computer-executable instructions or data structures stored thereon. Such tangible computer-readable storage devices can be any available device that can be accessed by a general purpose or special purpose computer, including the functional design of any special purpose processor as described above. By way of example, and not limitation, such tangible computer-readable devices can include RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other device which can be used to carry or store desired program code in the form of computer-executable instructions, data structures, or processor chip design. When information or instructions are provided via a network or another communications connection (either hardwired, wireless, or combination thereof) to a computer, the computer properly views the connection as a computer-readable medium. Thus, any such connection is properly termed a computer-readable medium. Combinations of the above should also be included within the scope of the computer-readable storage devices.</p><p id="p-0061" num="0060">Computer-executable instructions include, for example, instructions and data which cause a general purpose computer, special purpose computer, or special purpose processing device to perform a certain function or group of functions. Computer-executable instructions also include program modules that are executed by computers in stand-alone or network environments. Generally, program modules include routines, programs, components, data structures, objects, and the functions inherent in the design of special-purpose processors, etc. that perform tasks or implement abstract data types. Computer-executable instructions, associated data structures, and program modules represent examples of the program code means for executing steps of the methods disclosed herein. The particular sequence of such executable instructions or associated data structures represents examples of corresponding acts for implementing the functions described in such steps.</p><p id="p-0062" num="0061">Other embodiments of the disclosure may be practiced in network computing environments with many types of computer system configurations, including personal computers, hand-held devices, multi-processor systems, microprocessor-based or programmable consumer electronics, network PCs, minicomputers, mainframe computers, and the like. Embodiments may also be practiced in distributed computing environments where tasks are performed by local and remote processing devices that are linked (either by hardwired links, wireless links, or by a combination thereof) through a communications network. In a distributed computing environment, program modules may be located in both local and remote memory storage devices.</p><p id="p-0063" num="0062">The various embodiments described above are provided by way of illustration only and should not be construed to limit the scope of the disclosure. For example, the principles herein apply equally to optimization as well as general improvements. Various modifications and changes may be made to the principles described herein without following the example embodiments and applications illustrated and described herein, and without departing from the spirit and scope of the disclosure. Claim language reciting &#x201c;at least one of&#x201d; a set indicates that one member of the set or multiple members of the set satisfy the claim.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method, comprising:<claim-text>receiving, from a first data set recorded by one or more cameras, an image comprising one or more pixels making up a first image object and a second image object, wherein the first image object is partially occluded by the second image object;</claim-text><claim-text>processing the image to place a bounding box around the first image object;</claim-text><claim-text>processing the one or more pixels to determine a depth of the first image object and a depth of the second image object based on LiDAR data; and</claim-text><claim-text>classifying the first image object and the second image object.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the classifying the first image object and the second image object further comprises:<claim-text>associating the first image object with a first semantic label.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computer-implemented method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the first semantic label being used in 3-dimensional transformation of the image for controlling operation of an Autonomous Vehicle (AV).</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>processing the one or more pixels within the bounding box to identify a first image mask corresponding to the first image object, and to identify a second image mask corresponding to the second image object.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein processing the image to place the bounding box around the image object is performed using a first machine-learning model.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computer-implemented method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein processing the one or more pixels within the bounding box to identify the first image mask is performed using a second machine-learning model.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>associating a range with the image object based on LiDAR data.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A system, comprising:<claim-text>one or more processors; and</claim-text><claim-text>a computer-readable medium comprising instructions stored therein, which when executed by the processors, cause the processors to:<claim-text>receive, from a first data set recorded by one or more LiDAR cameras outputting LiDAR data, an image comprising a first image object and a second image object;</claim-text><claim-text>process the image to place a bounding box around the first image object and the second image object;</claim-text><claim-text>process the image in the bounding box to determine a depth of the first image and a depth the second image object based on the LiDAR data;</claim-text><claim-text>processing one or more pixels to determine a first classification label for the first image object; and</claim-text><claim-text>processing the one or more pixels to determine a second classification label for the second image object.</claim-text></claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the one or more processors are configured to execute the computer-readable instructions to:<claim-text>process the image within the bounding box to identify a first image mask corresponding with a first pixel region making up the first image object and a second image mask corresponding with a second pixel region making up the second image object; and</claim-text><claim-text>identify one or more first pixels in the first image object and the second image object using the bounding box, the first image mask, and the second image mask.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the one or more processors are configured to execute the computer-readable instructions to:<claim-text>associate the first image object with the first classification label, wherein the first classification label is a semantic label.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the first classification label being used in 3-dimensional transformation of the image for controlling operation of an Autonomous Vehicle (AV).</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the one or more processors are configured to execute the computer-readable instructions to process the image to place the bounding box around the first image object and the second image object is performed using a first machine-learning model.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the one or more processors are configured to execute the computer-readable instructions to process the image within the bounding box to identify the first image mask is performed using a second machine-learning model.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the one or more processors are configured to execute the computer-readable instructions to:<claim-text>associate a first range with the first image object and a second range with the second image object based on the LiDAR data.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory computer-readable medium comprising instructions stored therein, which when executed by one or more processors, causes the one or more processors to:<claim-text>receive, from a first data set recorded by one or more LiDAR cameras outputting LiDAR data, an image comprising a first image object and a second image object;</claim-text><claim-text>process the image to place a bounding box around the first image object and the second image object;</claim-text><claim-text>process the image in the bounding box to determine a depth of the first image and a depth the second image object based on the LiDAR data;</claim-text><claim-text>processing one or more pixels to determine a first classification label for the first image object; and</claim-text><claim-text>processing the one or more pixels to determine a second classification label for the second image object.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the one or more processors are configured to execute the computer-readable instructions to:<claim-text>process the image within the bounding box to identify a first image mask corresponding with a first pixel region making up the first image object and a second image mask corresponding with a second pixel region making up the second image object; and</claim-text><claim-text>identify one or more first pixels in the first image object and the second image object using the bounding box, the first image mask, and the second image mask.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the one or more processors are configured to execute the computer-readable instructions to:<claim-text>associate the first image object with the first classification label, wherein the first classification label is a semantic label.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the first classification label being used in 3-dimensional transformation of the image for controlling operation of an Autonomous Vehicle (AV).</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the one or more processors are configured to execute the computer-readable instructions to process the image to place the bounding box around the first image object and the second image object is performed using a first machine-learning model.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the one or more processors are configured to execute the computer-readable instructions to process the image within the bounding box to identify the first image mask is performed using a second machine-learning model.</claim-text></claim></claims></us-patent-application>