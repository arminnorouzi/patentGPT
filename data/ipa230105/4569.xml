<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004570A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004570</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17778389</doc-number><date>20191120</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>2457</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>41</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>24578</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>24575</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>41</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Systems and methods for generating document score adjustments</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Canva Pty Ltd</orgname><address><city>Surry Hills, NSW</city><country>AU</country></address></addressbook><residence><country>AU</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Tune</last-name><first-name>Paul Li Shern</first-name><address><city>Sydney</city><country>AU</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Hazlewood</last-name><first-name>Robert Matthew</first-name><address><city>Sydney</city><country>AU</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Tskhay</last-name><first-name>Victoria</first-name><address><city>Sydney</city><country>AU</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/AU2019/051275</doc-number><date>20191120</date></document-id><us-371c12-date><date>20220519</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Disclosed is a computer-implemented method for determining a score adjustment for a search document, comprising determining a first attractiveness model of a first document from one or more documents based on one or more user interactions associated with the first document; determining a second attractiveness model of a second document from one or more documents based on one or more user interactions associated with the second document; determining one or more pairwise comparisons of documents based on the first and second attractiveness models of the first and second documents; training an adjustment model based on the pairwise comparisons of documents; and inputting the search document into the adjustment model to determine the score adjustment.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="231.48mm" wi="148.67mm" file="US20230004570A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="257.22mm" wi="163.58mm" file="US20230004570A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="211.16mm" wi="163.91mm" file="US20230004570A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="254.17mm" wi="164.25mm" file="US20230004570A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="254.17mm" wi="166.79mm" file="US20230004570A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="259.25mm" wi="161.63mm" file="US20230004570A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="241.05mm" wi="150.62mm" file="US20230004570A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="255.52mm" wi="172.47mm" file="US20230004570A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="241.05mm" wi="152.32mm" file="US20230004570A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="249.60mm" wi="143.43mm" file="US20230004570A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD OF THE INVENTION</heading><p id="p-0002" num="0001">The present disclosure is directed to searching and ranking documents. Specifically, the present disclosure is directed to generating and applying document score adjustments in search results.</p><heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading><p id="p-0003" num="0002">The developments described in this section are known to the inventors. However, unless otherwise indicated, it should not be assumed that any of the developments described in this section qualify as prior art merely by virtue of their inclusion in this section, or that those developments are known to a person of ordinary skill in the art.</p><p id="p-0004" num="0003">Generally speaking, searching involves ranking (or scoring) documents according to a query. Some approaches to searching allow for a two-step process: an initial document score is generated by a search engine based on the search query and, following this, a document score adjustment (also referred to as a document boost) is applied to generate an adjusted document score. Applying a document score adjustment to the base scores generated by the search engine allows the order in which documents are returned to be adjusted, for example to elevate documents that are deemed to be more important, popular, profitable, or for any other reason. Traditionally, document score adjustment is performed by applying rules to the base score returned by a search engine. These rules are typically handcrafted by a search relevance engineer with domain expertise.</p><p id="p-0005" num="0004">Such handcrafting means that it is often up to the search relevance engineer to determine whether a rule is accurately adjusting scores of documents in a search. However, the rules are often difficult to apply across a range of documents and it can be difficult to determine whether the rules operate accurately for a large number of documents. Further handcrafting rules requires a significant amount of domain knowledge which if configured incorrectly can result in contradictory rules.</p><p id="p-0006" num="0005">Some approaches to automation exist, however they can be quite computationally inefficient because such approaches often require multiple comparisons between documents, particularly in order to determine an attractiveness of a document (that is, a perceived relevance of the document by a user). Therefore there exists a need for an automated way of adjusting scores of documents that is computationally efficient.</p><heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0007" num="0006">Described herein is a computer-implemented method for determining a score adjustment for a search document, comprising: determining a first attractiveness model of a first document from one or more documents based on one or more user interactions associated with the first document; determining a second attractiveness model of a second document from one or more documents based on one or more user interactions associated with the second document; determining one or more pairwise comparisons of documents based on the first and second attractiveness models of the first and second document; determining training an adjustment model based on the pairwise comparisons of documents; and inputting the search document into the adjustment model to determine the score adjustment.</p><p id="p-0008" num="0007">Described herein is a computer processing system comprising: a processing unit; a communication interface; a non-transient storage medium readable by a processor, the storage medium storing instructions executable by one or more processors to cause the one or more processors to: determine a first attractiveness model of a first document from one or more documents based on one or more user interactions associated with the first document; determine a second attractiveness model of a second document from one or more documents based on one or more user interactions associated with the second document; determine one or more pairwise comparisons of documents based on the first and second attractiveness models of the first and second documents; train an adjustment model based on the pairwise comparisons of documents; and input the search document into the adjustment model to determine the score adjustment.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0009" num="0008">In the drawings:</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example search system that includes a system for generating document score adjustments.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b></figref> provides a block diagram of a general-purpose computer processing system configurable to perform various features of the present disclosure.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of a method for determining a search score.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of a method for generating document score adjustments.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of a method for determining pairwise comparisons.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart of a first method for training a neural network to learn document score adjustment.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example neural network to learn document score adjustment according to the first method illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart of a second method for training a neural network to learn document score adjustment.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example structure for a neural network to learn document score adjustment according to the second method illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0019" num="0018">While the invention is amenable to various modifications and alternative forms, specific embodiments are shown by way of example in the drawings and are described in detail. It should be understood, however, that the drawings and detailed description are not intended to limit the invention to the particular form disclosed. The intention is to cover all modifications, equivalents, and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0020" num="0019">In the following description, for the purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the present invention. It will be apparent, however, that the present invention may be practiced without these specific details. In some instances, well-known structures and devices are shown in block diagram form in order to avoid unnecessary obscuring.</p><p id="p-0021" num="0020">As described above, when a search engine performs a search (based on a search query) the search engine processes the search query to generate initial document scores for all documents available to the search engine and covered by the search. The initial document scores allow for an initial document ranking&#x2014;with, for example, documents having a higher score being (based on the search engine's processing) more relevant to the search query than documents having a lower score.</p><p id="p-0022" num="0021">This disclosure relates to systems and methods for adjusting document scores in search results: i.e. adjusting initial document scores as returned by a search engine to change the rank order of the documents. In particular, this disclosure relates to query invariant document score adjustment that learns from user preferences as evidenced in user interactions relating to documents.</p><p id="p-0023" num="0022">Boosting, in the art, typically refers to elevating certain documents in a search result. In this disclosure, reference is made to a score adjustment which reflects that a document's score may be adjusted either to boost the document (so its position in the initial rank order generated by the search engine rises) or downgrade the document (so its position in the initial rank order falls).</p><p id="p-0024" num="0023">At a high level, an overall search process as described herein includes the following steps&#x2014;<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0024">1. Perform search, by a search engine, in accordance with a search query.</li>        <li id="ul0002-0002" num="0025">2. Determine, by the search engine, a base score based on the initial search.</li>        <li id="ul0002-0003" num="0026">3. Determine and apply a score adjustment (e.g. document boost) for one or more documents.</li>        <li id="ul0002-0004" num="0027">4. Determine an updated search score based on the base score and score adjustment.</li>    </ul>    </li></ul></p><p id="p-0025" num="0028">Expanding out step 3 in more detail, this disclosure provides a process for determining a document score adjustment that involves:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0029">a. Tracking user interaction data of documents in a document database.</li>        <li id="ul0004-0002" num="0030">b. Determining a document user interaction model based on the user interaction data.</li>        <li id="ul0004-0003" num="0031">c. Calculating a set of pairwise comparisons of documents based on the document user interaction model.</li>        <li id="ul0004-0004" num="0032">d. Using the set of pairwise comparisons as a feedback mechanism, train a neural network to learn how to adjust scores.</li>        <li id="ul0004-0005" num="0033">e. Therefore a given document can be input into the neural network (or a copy) and a corresponding output of the neural network is a score adjustment for that document.</li>    </ul>    </li></ul></p><p id="p-0026" num="0034">A score adjustment can be simply a value. This is often referred to in the art as an additive boost but the techniques described herein can be used to generate either an additive or subtractive score adjustment (e.g. a score adjustment can be either positive or negative or zero). A score adjustment could also be a multiplier or more complex function which takes the base score as an input and then produces an adjusted score.</p><p id="p-0027" num="0035">Notably, the processing described herein provides for a document score adjustment that is determined independently of an initial document score produced by an initial search and independently of the search query that generated the initial document score. Therefore, a score adjustment can be calculated and associated with the document. When a search process performs step (3), the score adjustment, typically a multiplicative boost, can be quickly applied, allowing for a final ranking of documents to be calculated computationally efficiently within the search engine.</p><heading id="h-0006" level="1">DOCUMENTS</heading><p id="p-0028" num="0036">For the purposes of this description a document is an electronic or computer file of any type. Documents may also include fonts, colours, and images that are used when displaying the document on an electronic device, printed to paper or otherwise viewed. Example documents include templates, text files, and images.</p><p id="p-0029" num="0037">The terms &#x201c;document&#x201d; and &#x201c;images&#x201d; are used in similar contexts in this disclosure. Though document is a more general term than images, many of the example documents provided in this disclosure are images. It is to be noted that similar techniques of the present disclosure can be applied to other document types, such as templates.</p><heading id="h-0007" level="2">Document Metadata</heading><p id="p-0030" num="0038">Document metadata is information about a document that is generally part of, or attached to, the document itself. Document metadata may not be readily available on the face of the document.</p><p id="p-0031" num="0039">Documents may also contain other elements such as graphic images, photographs, tables and charts, each of which can have its own metadata. By way of example, document metadata may include details such as file size, date of document creation, the names of the author and user who most recently modified the document, the dates of any changes, the total edit time, etc. Specific document types may have additional document metadata. For example, an image that is a photograph may have details of the camera that took the photograph (such as make, model), the date/time the photograph was taken, and any camera settings stored as document metadata for the image.</p><heading id="h-0008" level="2">Searching and Initial Document Scores</heading><p id="p-0032" num="0040">As described above, initial processing in response to a search query is performed by a search engine. Various search engines exist and can be used with the present disclosure&#x2014;e.g. Apache Solr.</p><p id="p-0033" num="0041">A search engine typically performs three general steps to search documents. First, the search engine converts documents into a machine-readable format, often referred to in the art as indexing. A second step is parsing a search query received from a user to understand, in a machine-readable way, the terms of the query. The search terms can be images or keywords, for example. A third step is mapping whereby the search engine maps the user query to the documents stored in the document database to find a result. The search engine may implement one or more specified rules for mapping that determine if a given document is sufficiently relevant or appropriate for the query. The mapping process determines document scores that reflect how well the user query maps to the documents.</p><p id="p-0034" num="0042">Therefore, initial search processing generates a set of initial document scores. Each document score indicates the search engine's determination as to the relevance (or otherwise) of the document the score is associated with to the search query. In some embodiments, an initial ranking based on the scores can be performed.</p><heading id="h-0009" level="2">Final Document Scores/Rankings</heading><p id="p-0035" num="0043">The present disclosure is concerned with additional processing that is performed after a search engine generates the set of base scores (that is, as above the initial document scores for documents). This additional processing involves applying a post-search score adjustment (e.g. a boost) to each document score and then generating a final ranking based on the adjusted document scores. This allows for the initial ranking generated by the search engine to be changed, for example by boosting certain documents above others.</p><heading id="h-0010" level="2">Document Score Adjustments</heading><p id="p-0036" num="0044">As described above, a document score adjustment is an operation performed on an initial search engine score to modify or adjust a position of a document in a final result from a search query. Documents may be elevated (or downgraded) in a search to address specific search concerns, popularity or other manual intervention. For example, a document score adjustment can be used to boost high quality content so that it is more accessible to users or downgrade low quality content so it is less accessible to users.</p><p id="p-0037" num="0045">Document score adjustments include additive and multiplicative score adjustments. For example, a multiplicative score adjustment (boost) of a document d, where 0&#x3c;=boost<sub>d</sub>&#x3c;=1, may have a formula of:</p><p id="p-0038" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Score<sub>d</sub>=base score<sub>d</sub>*(1+boost<sub>d</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0039" num="0046">In this example, the multiplicative score adjustment preserves reflexivity, equivalence, anti-symmetry and transitivity. In contrast an additive score adjustment of a document d, where 0&#x3c;=boost<sub>d</sub>&#x3c;=1, may have a formula of:</p><p id="p-0040" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Score<sub>d</sub>=base score<sub>d</sub>+boost<sub>d </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><heading id="h-0011" level="1">Example System</heading><p id="p-0041" num="0047"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example system <b>100</b> in which the features and techniques described herein are implemented. In this example system <b>100</b>, there are three user computers <b>104</b>A, <b>104</b>B, <b>104</b>C that maintain a client module <b>108</b> for interacting with the search system <b>102</b>. Each of these computers is connected to a network <b>106</b>. The network <b>106</b> could be a suitable network of any kind such as the internet. Also connected directly to the network <b>106</b> is a search system <b>102</b>, and specifically a search engine <b>140</b>. A search engine <b>140</b> is an application that sorts, or ranks, a collection of documents in response to a search query, typically specified by a user in the form of keywords or natural language.</p><p id="p-0042" num="0048">The collection of documents in this example are documents maintained in a document database <b>112</b>. The document database <b>112</b>, therefore, stores local documents that can be searched. This database <b>112</b> can include many types of documents such as templates, images and graphics (including raster and vector). In some embodiments, a search can also cover external documents (accessible, for example, via network <b>106</b>), however for ease of description the present embodiments are described as searching local documents in the document database <b>112</b> only.</p><p id="p-0043" num="0049">On receiving and processing a search query, the search engine <b>140</b> initially generates a base score for each of the documents covered by the search (e.g. all documents in database <b>112</b>, unless documents are filtered out by the search query).</p><p id="p-0044" num="0050">The search system <b>102</b> includes a user interaction database <b>114</b>. The user interaction database <b>114</b> is a collection of user interaction data that has been collected and stored by the search system <b>102</b>. By way of example, user interaction data may include click (or document selection) data reflecting which documents have been clicked on (selected) by a user when presented in search results. In some embodiments, click data includes a position of the document in a search result at the time it was clicked/selected. Click data can also include a context for the user interaction, that is, which other documents a user had the choice of clicking on.</p><p id="p-0045" num="0051">The search system <b>102</b> also include pairwise comparisons database <b>116</b> which stores pairwise comparison data. The pairwise comparisons data is generated by the search system <b>102</b> based on the user interaction data and an attractiveness model (described further below) that is generated for each of the documents. The pairwise comparisons measure a pair of documents to indicate which one of the two documents is more likely to be clicked, or otherwise interacted with by the user, than the other.</p><p id="p-0046" num="0052">In some embodiments, additional parameters can be used to determine if a document was examined (rather than simply clicked), or whether the user was satisfied with the document they clicked. For the purposes of the examples provided below, an attractiveness score is a metric of a perceived relevance of a document based on document user interaction in the form of user clicks in a search result.</p><p id="p-0047" num="0053">The score adjustment system <b>110</b> includes a number of modules. These modules include a document module <b>122</b>, a user interaction model <b>124</b>, a score adjustment module <b>128</b> and a search module <b>130</b>. The score adjustment module <b>128</b> comprises two submodules: a pairwise comparisons module <b>132</b>, which is used to build the pairwise comparisons dataset <b>116</b>, and an adjustment model module <b>134</b> which is used to train the adjustment model <b>118</b> and generate an adjustment output.</p><p id="p-0048" num="0054">The document module <b>122</b> maintains the code and logic for handling the documents in the document database <b>112</b>. The search module <b>130</b> maintains the code and logic for handling the search engine <b>130</b> and retrieving data in the search database <b>142</b>.</p><p id="p-0049" num="0055">The user interaction module <b>124</b> maintains the code and logic for handling the user interaction data stored in the user interaction database <b>114</b>. In this example system, user interaction data in the user interaction database <b>114</b> are associated with a document in the document database so that the user interactions on a document can be determined. In some cases, however, the user interactions will be associated with a query or other information that can be used to give context to the user interactions.</p><p id="p-0050" num="0056">The score adjustment module <b>128</b> and search module <b>130</b> are described further below.</p><heading id="h-0012" level="2">Computer System</heading><p id="p-0051" num="0057">The present invention is necessarily implemented using an electronic device. The electronic device is, or will include, a computer processing system.</p><p id="p-0052" num="0058"><figref idref="DRAWINGS">FIG. <b>2</b></figref> provides a block diagram of one example of a computer processing system <b>200</b> which may be for example search system <b>102</b>, the document score adjustment system <b>110</b>, or the user computers <b>104</b>A, <b>104</b>B, <b>104</b>C.</p><p id="p-0053" num="0059">System <b>200</b> as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a general-purpose computer processing system. It will be appreciated that <figref idref="DRAWINGS">FIG. <b>2</b></figref> does not illustrate all functional or physical components of a computer processing system. For example, no power supply or power supply interface has been depicted, however system <b>200</b> will either carry a power supply or be configured for connection to a power supply (or both). It will also be appreciated that the particular type of computer processing system will determine the appropriate hardware and architecture, and alternative computer processing systems suitable for implementing aspects of the invention may have additional, alternative, or fewer components than those depicted, combine two or more components, and/or have a different configuration or arrangement of components.</p><p id="p-0054" num="0060">The computer processing system <b>200</b> includes at least one processing unit <b>202</b>. The processing unit <b>202</b> may be a single computer-processing device (e.g. a central processing unit, graphics processing unit, or other computational device), or may include a plurality of computer processing devices. In some instances all processing will be performed by processing unit <b>202</b>, however in other instances processing may also, or alternatively, be performed by remote processing devices accessible and useable (either in a shared or dedicated manner) by the system <b>200</b>.</p><p id="p-0055" num="0061">Through a communications bus <b>204</b> the processing unit <b>202</b> is in data communication with a one or more machine-readable storage (memory) devices that store instructions and/or data for controlling operation of the processing system <b>100</b>. In this instance system <b>200</b> includes a system memory <b>206</b> (e.g. a BIOS), volatile memory <b>208</b> (e.g. random access memory such as one or more DRAM modules), and non-volatile memory <b>210</b> (e.g. one or more hard disk or solid state drives).</p><p id="p-0056" num="0062">System <b>200</b> also includes one or more interfaces, indicated generally by <b>112</b>, via which system <b>100</b> interfaces with various devices and/or networks. Generally speaking, other devices may be physically integrated with system <b>200</b>, or may be physically separate. Where a device is physically separate from system <b>200</b>, connection between the device and system <b>100</b> may be via wired or wireless hardware and communication protocols, and may be a direct or an indirect (e.g. networked) connection.</p><p id="p-0057" num="0063">Wired connection with other devices/networks may be by any appropriate standard or proprietary hardware and connectivity protocols. For example, system <b>200</b> may be configured for wired connection with other devices/communications networks by one or more of: USB; FireWire; eSATA; Thunderbolt; Ethernet; OS/2; Parallel; Serial; HDMI; DVI; VGA; SCSI; AudioPort. Other wired connections are, of course, possible.</p><p id="p-0058" num="0064">Wireless connection with other devices/networks may similarly be by any appropriate standard or proprietary hardware and communications protocols. For example, system <b>100</b> may be configured for wireless connection with other devices/communications networks using one or more of: infrared; Bluetooth; Wi-Fi; near field communications (NFC); Global System for Mobile Communications (GSM), Enhanced Data GSM Environment (EDGE), long term evolution (LTE), wideband code division multiple access (W-CDMA), code division multiple access (CDMA). Other wireless connections are, of course, possible.</p><p id="p-0059" num="0065">Generally speaking, the devices to which system <b>200</b> connects&#x2014;whether by wired or wireless means&#x2014;allow data to be input into/received by system <b>200</b> for processing by the processing unit <b>202</b>, and data to be output by system <b>100</b>. Example devices are described below, however it will be appreciated that not all computer-processing systems will include all mentioned devices, and that additional and alternative devices to those mentioned may well be used.</p><p id="p-0060" num="0066">For example, system <b>200</b> may include or connect to one or more input devices by which information/data is input into (received by) system <b>200</b>. Such input devices may include physical buttons, alphanumeric input devices (e.g. keyboards), pointing devices (e.g. mice, track pads and the like), touchscreens, touchscreen displays, microphones, accelerometers, proximity sensors, GPS devices and the like. System <b>200</b> may also include or connect to one or more output devices controlled by system <b>100</b> to output information. Such output devices may include devices such as indicators (e.g. LED, LCD or other lights), displays (e.g. CRT displays, LCD displays, LED displays, plasma displays, touch screen displays), audio output devices such as speakers, vibration modules, and other output devices. System <b>200</b> may also include or connect to devices which may act as both input and output devices, for example memory devices (hard drives, solid state drives, disk drives, compact flash cards, SD cards and the like) which system <b>100</b> can read data from and/or write data to, and touch-screen displays which can both display (output) data and receive touch signals (input).</p><p id="p-0061" num="0067">System <b>200</b> may also connect to communications networks (e.g. the Internet, a local area network, a wide area network, a personal hotspot etc.) to communicate data to and receive data from networked devices, which may themselves be other computer processing systems.</p><p id="p-0062" num="0068">It will be appreciated that system <b>200</b> may be any suitable computer processing system such as, by way of non-limiting example, a desktop computer, a laptop computer, a netbook computer, tablet computer, a smart phone, a Personal Digital Assistant (PDA), a cellular telephone, a web appliance. Typically, system <b>200</b> will include at least user input and output devices <b>214</b> and (if the system is to be networked) a communications interface <b>216</b> for communication with a network <b>218</b>. The number and specific types of devices which system <b>200</b> includes or connects to will depend on the particular type of system <b>200</b>. For example, if system <b>200</b> is a desktop computer it will typically connect to physically separate devices such as (at least) a keyboard, a pointing device (e.g. mouse), a display device (e.g. a LCD display). Alternatively, if system <b>200</b> is a laptop computer it will typically include (in a physically integrated manner) a keyboard, pointing device, a display device, and an audio output device. Further alternatively, if system <b>200</b> is a tablet device or smartphone, it will typically include (in a physically integrated manner) a touchscreen display (providing both input means and display output means), an audio output device, and one or more physical buttons.</p><p id="p-0063" num="0069">System <b>200</b> stores or has access to instructions and data which, when processed by the processing unit <b>202</b>, configure system <b>200</b> to receive, process, and output data. Such instructions and data will typically include an operating system such as Microsoft Windows&#xae;, Apple OSX, Apple <b>105</b>, Android, Unix, or Linux.</p><p id="p-0064" num="0070">System <b>200</b> also stores or has access to instructions and data (i.e. software) which, when processed by the processing unit <b>202</b>, configure system <b>200</b> to perform various computer-implemented processes/methods in accordance with embodiments of the invention (as described below). It will be appreciated that in some cases part or all of a given computer-implemented method will be performed by system <b>200</b> itself, while in other cases processing may be performed by other devices in data communication with system <b>200</b>.</p><p id="p-0065" num="0071">Instructions and data are stored on a non-transient machine-readable medium accessible to system <b>200</b>. For example, instructions and data may be stored on non-transient memory <b>210</b>. Instructions may be transmitted to/received by system <b>200</b> via a data signal in a transmission channel enabled (for example) by a wired or wireless network connection.</p><heading id="h-0013" level="1">Example Method</heading><p id="p-0066" num="0072">This section describes a computer implemented method for searching documents and producing a search result. The process <b>300</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0067" num="0073">The method <b>300</b> starts when a user provides a search query to the search system <b>102</b>, and specifically the search engine <b>140</b>. A search query can be a simple character to a string of characters. The search query may include one or more keywords, which may be separated by operators such as &#x2018;not&#x2019;, &#x2018;and&#x2019;, &#x2018;or&#x2019; etc. The search engine <b>140</b> then performs <b>302</b> a search based on the search query. Typically, the search engine <b>140</b> will determine a relevance score for each document based on a metric of how relevant the document is to the query.</p><p id="p-0068" num="0074">The search engine <b>140</b> then uses the initial results from this search to determine <b>304</b> a base score for each document in the search result. A base index for a document may simply be the relevance score for that document as determined by the search engine at step <b>302</b>. However, in some embodiments it is possible for the search engine <b>140</b> to process the relevance score. In other embodiments, other forms of preprocessing may be performed. The results of the search (including a search score and associated rank for the document in the search) can be stored in the search database <b>142</b>.</p><p id="p-0069" num="0075">At <b>306</b>, the search system <b>102</b> then determines if a document score adjustment has been calculated. Step <b>306</b> can be performed on a document by document basis. Alternatively, it may be possible to perform the step <b>306</b> in respect of a single document and, if a document score has not been calculated for that single document, infer that document scores will not have been calculated for any documents. It should be noted that this does not materially affect how the present disclosure operates.</p><p id="p-0070" num="0076">If document score adjustments have not been calculated, the search system <b>102</b> proceeds to calculate <b>308</b> document score adjustments before proceeding to <b>310</b>. The manner in which document score adjustments are calculated is described below.</p><p id="p-0071" num="0077">If the document score adjustments have been calculated, the search system <b>102</b> calculates updated document scores <b>310</b>. To do so, for each document in the search result the search system <b>102</b> identifies the score adjustment for that document and applies it to the base score for that document. For example, if score adjustments are multiplicative, applying the score adjustment involves multiplying the base score by the score adjustment.</p><p id="p-0072" num="0078">At <b>312</b>, the search engine updates the search scores using the updated scores calculated at <b>310</b>. At this point the search results can be reranked according to the updated search score. The rankings of the documents can be finalised at this point and the search engine <b>140</b> can provide the results back to the user.</p><heading id="h-0014" level="2">Calculating Document Score Adjustments</heading><p id="p-0073" num="0079">This section and the following sections describe a computer implemented for calculating a document score adjustment including process <b>400</b>, <b>500</b>, and <b>600</b>/<b>800</b> (processes <b>600</b> and <b>800</b> relating to alternative embodiments).</p><p id="p-0074" num="0080">Process <b>400</b> involves determining a document score adjustment. The process <b>400</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0075" num="0081">In the present embodiments, assessing whether one document is more attractive than the other is performed by hypothesis testing. Hypothesis testing which is tested over a significant volume (the precise amount would likely depend on the number of documents in the document database and volume of user interactions) leads to an assessment of attractiveness of a document with some level of confidence. The pairwise comparisons therefore is a dataset of hypotheses about attractiveness of documents. Determining <b>401</b> the pairwise comparisons therefore is the starting point of the process <b>400</b>. This dataset is used later to train <b>414</b> a neural network (the adjustment model <b>118</b>) to learn how to determine a score adjustment. Once trained, the neural network is used to determine <b>416</b> a score adjustment.</p><p id="p-0076" num="0082">Initially the score adjustment module <b>128</b> determines <b>402</b> a search query. An example of determining a query could be retrieving stored results of an historical search query, such as &#x201c;blue bird singing&#x201d;. Determining a query may also therefore involve determining <b>404</b> a stored score of each of the documents in the document database as the document scores for a query (as produced by the search engine <b>140</b> and stored, for example, in the search database) may also be stored. In some cases, not all documents will have document scores or some documents may be ignored. For example, some documents with a score of zero or below a specific threshold of relevance may be discarded, at least for the purposes of determining a score adjustment.</p><p id="p-0077" num="0083">From this score, the documents can be ranked in order so as to provide a position for each of the documents. The score adjustment module <b>108</b> may also rank the documents based on the score provided above.</p><p id="p-0078" num="0084">The score adjustment module <b>108</b> then retrieves <b>406</b> the user interaction data from the user interaction database <b>114</b> for each of the documents that were score<sub>d </sub>previously at <b>404</b>. In the present example, user interaction data includes how many times a document was clicked in a given search result. Other user interaction data may also be tracked such as time spent on a mouse hover over or clicking on an expand option (if there is such an option).</p><p id="p-0079" num="0085">The score adjustment module <b>108</b> then determines <b>408</b> a document attractiveness model based on the user interaction data. As is described elsewhere in this disclosure, attractiveness is a perceived relevance of a document in respect of other documents. This disclosure provides for modelling attractiveness of a document as set out below.</p><p id="p-0080" num="0086">User interaction can be noisy and not necessarily determinative of relevance of a document. There are many user studies known in the art that indicate that there are biases that may affect how a user may click on, or otherwise interact with, a document. There is, for example, attention bias to visually salient documents. Novelty bias occurs where the search engine produces previously unseen documents. There are many other types of bias. These biases may be taken into account by introducing random variables and using them to perform an estimation of attractiveness of a document.</p><p id="p-0081" num="0087">An example model is to take into account the number of clicks on the document. In this example model, the random variable A<sub>i</sub>&#x2208;{0, 1} models whether a user finds a document i attractive. The associated probability of attractiveness &#x3b8;<sub>A </sub>is a random variable distributed according to a Beta distribution. This is the attractiveness model used in the examples in this disclosure.</p><p id="p-0082" num="0088">One example for modelling attractiveness is a Beta-Bernoulli model that takes into account the number of clicks on the document. A random variable can be used to model whether a user finds document i attractive or not. The associated probability of attractiveness in the model is a random variable distributed according to the Beta distribution (which has support between 0 and 1). That is, a probability of attractiveness of document i can be modelled as a Beta-distributed random variable.</p><p id="p-0083" num="0089">Once an attractiveness model is determined, the score adjustment module <b>128</b> determines <b>410</b> pairwise comparisons for the query determined at <b>402</b>. In this example, a pairwise comparison is determining for a pair of documents which one is perceived to be most relevant (that is, &#x201c;attractive&#x201d;). This is calculated by a comparing a pair of documents based on the document attractiveness model for each of the documents.</p><p id="p-0084" num="0090">The following describes how a comparison of attractiveness of two documents can be calculated using an approach of statistical hypothesis testing with an example hypothesis test.</p><p id="p-0085" num="0091">Assume that an average attractiveness of a first document A<sub>1 </sub>is greater than an average attractiveness of a second document A<sub>2</sub>. As the attractiveness of A<sub>1 </sub>(and A<sub>2</sub>) is a random variable, there is a probability that the attractiveness of A<sub>1 </sub>is less than A<sub>2</sub>. In order to test a hypothesis H that A<sub>1 </sub>is more attractive than A<sub>2</sub>, the probability of a comparison of the attractiveness of A<sub>1 </sub>and the attractiveness of A<sub>2 </sub>can be computed. In the example where attractiveness is modelled with the Beta-Bernoulli model the hypothesis H can be calculated by the following equation:</p><p id="p-0086" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <mi>Pr</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <mrow>    <msub>     <mi>&#x3b8;</mi>     <msub>      <mi>A</mi>      <mn>1</mn>     </msub>    </msub>    <mo>&#x2264;</mo>    <msub>     <mi>&#x3b8;</mi>     <msub>      <mi>A</mi>      <mn>2</mn>     </msub>    </msub>   </mrow>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <mrow>     <mrow>      <mi>B</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <msub>        <mi>&#x3b1;</mi>        <mn>1</mn>       </msub>       <mo>,</mo>       <msub>        <mi>&#x3b2;</mi>        <mn>1</mn>       </msub>      </mrow>      <mo>)</mo>     </mrow>     <mo>&#x2062;</mo>     <mrow>      <mi>B</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <msub>        <mi>&#x3b1;</mi>        <mn>2</mn>       </msub>       <mo>,</mo>       <msub>        <mi>&#x3b2;</mi>        <mn>2</mn>       </msub>      </mrow>      <mo>)</mo>     </mrow>    </mrow>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <msubsup>     <mo>&#x222b;</mo>     <mrow>      <mo>-</mo>      <mn>1</mn>     </mrow>     <mn>0</mn>    </msubsup>    <mrow>     <msubsup>      <mo>&#x222b;</mo>      <mn>0</mn>      <mn>1</mn>     </msubsup>     <mrow>      <msup>       <mrow>        <msup>         <mi>w</mi>         <mrow>          <msub>           <mi>&#x3b1;</mi>           <mn>1</mn>          </msub>          <mo>-</mo>          <mn>1</mn>         </mrow>        </msup>        <mo>(</mo>        <mrow>         <mi>w</mi>         <mo>-</mo>         <mi>z</mi>        </mrow>        <mo>)</mo>       </mrow>       <mrow>        <msub>         <mi>&#x3b1;</mi>         <mn>2</mn>        </msub>        <mo>-</mo>        <mn>1</mn>       </mrow>      </msup>      <mo>&#x2062;</mo>      <msup>       <mrow>        <mo>(</mo>        <mrow>         <mn>1</mn>         <mo>-</mo>         <mi>w</mi>        </mrow>        <mo>)</mo>       </mrow>       <mrow>        <msub>         <mi>&#x3b2;</mi>         <mn>1</mn>        </msub>        <mo>-</mo>        <mn>1</mn>       </mrow>      </msup>      <mo>&#x2062;</mo>      <msup>       <mrow>        <mo>(</mo>        <mrow>         <mn>1</mn>         <mo>-</mo>         <mi>w</mi>         <mo>+</mo>         <mi>z</mi>        </mrow>        <mo>)</mo>       </mrow>       <mrow>        <msub>         <mi>&#x3b2;</mi>         <mn>2</mn>        </msub>        <mo>-</mo>        <mn>1</mn>       </mrow>      </msup>      <mo>&#x2062;</mo>      <mi>dwdz</mi>     </mrow>    </mrow>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0087" num="0000">Where the probability of attractiveness &#x3b8; is a random variable distributed according to the Beta distribution, w is &#x3b8;<sub>A1 </sub>and z is &#x3b8;<sub>A2</sub>&#x2212;&#x3b8;<sub>A1</sub>. Further in this equation, B(&#x3b1;, &#x3b2;) is the beta function. However, this equation can be computationally expensive. As a result, this disclosure provides alternative approximations. One example alternative approximation is to approximate a Beta random variable with a Gaussian random variable.</p><p id="p-0088" num="0092">Given a large enough &#x3b1; and &#x3b2; (the parameters of the Beta function above) then a random variable X&#x2dc;B(&#x3b1;, &#x3b2;) converges according to the following equation, where</p><p id="p-0089" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mrow>   <mi>&#xb5;</mi>   <mo>=</mo>   <mfrac>    <mi>&#x3b1;</mi>    <mrow>     <mi>&#x3b1;</mi>     <mo>+</mo>     <mi>&#x3b2;</mi>    </mrow>   </mfrac>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0090" num="0000">that is, the mean of X:</p><p id="p-0091" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x221a;{square root over (&#x3b1;+&#x3b2;)}(<i>X</i>&#x2212;&#x3bc;)&#x2192;<i>N</i>(0,&#x3bc;(1&#x2212;&#x3bc;))<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0092" num="0093">The random variable X can be expressed as</p><p id="p-0093" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mrow>  <mi>X</mi>  <mo>=</mo>  <mfrac>   <msub>    <mi>G</mi>    <mn>1</mn>   </msub>   <mrow>    <msub>     <mi>G</mi>     <mn>1</mn>    </msub>    <mo>+</mo>    <msub>     <mi>G</mi>     <mn>2</mn>    </msub>   </mrow>  </mfrac> </mrow></math></maths></p><p id="p-0094" num="0094">In this equation, G<sub>1</sub>&#x2dc;Gamma(&#x3b1;, 2) and G<sub>2</sub>&#x2dc;Gamma(&#x3b2;, 2). As &#x3b1; and &#x3b2; tend to &#x221e;, the gamma distributions converge according to the following formula:</p><p id="p-0095" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mrow>  <mfrac>   <mrow>    <msub>     <mi>G</mi>     <mn>1</mn>    </msub>    <mo>-</mo>    <mrow>     <mn>2</mn>     <mo>&#x2062;</mo>     <mi>&#x3b1;</mi>    </mrow>   </mrow>   <msqrt>    <mrow>     <mn>4</mn>     <mo>&#x2062;</mo>     <mi>&#x3b1;</mi>    </mrow>   </msqrt>  </mfrac>  <semantics definitionURL="">   <mo>&#x2192;</mo>   <annotation encoding="Mathematica">"\[Rule]"</annotation>  </semantics>  <mrow>      <mrow>    <mo>(</mo>    <mrow>     <mn>0</mn>     <mo>,</mo>     <mn>1</mn>    </mrow>    <mo>)</mo>   </mrow>  </mrow> </mrow></math></maths><maths id="MATH-US-00004-2" num="00004.2"><math overflow="scroll"> <mrow>  <mfrac>   <mrow>    <msub>     <mi>G</mi>     <mn>2</mn>    </msub>    <mo>-</mo>    <mrow>     <mn>2</mn>     <mo>&#x2062;</mo>     <mi>&#x3b2;</mi>    </mrow>   </mrow>   <msqrt>    <mrow>     <mn>4</mn>     <mo>&#x2062;</mo>     <mi>&#x3b2;</mi>    </mrow>   </msqrt>  </mfrac>  <semantics definitionURL="">   <mo>&#x2192;</mo>   <annotation encoding="Mathematica">"\[Rule]"</annotation>  </semantics>  <mrow>      <mrow>    <mo>(</mo>    <mrow>     <mn>0</mn>     <mo>,</mo>     <mn>1</mn>    </mrow>    <mo>)</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0096" num="0095">Therefore the convergence equation becomes:</p><p id="p-0097" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mrow>  <mrow>   <msqrt>    <mrow>     <mi>&#x3b1;</mi>     <mo>+</mo>     <mi>&#x3b2;</mi>    </mrow>   </msqrt>   <mo>&#x2062;</mo>   <mrow>    <mo>(</mo>    <mrow>     <mfrac>      <mrow>       <msub>        <mi>G</mi>        <mn>1</mn>       </msub>       <mo>-</mo>       <mrow>        <mn>2</mn>        <mo>&#x2062;</mo>        <mi>&#x3b1;</mi>       </mrow>      </mrow>      <msqrt>       <mrow>        <mn>4</mn>        <mo>&#x2062;</mo>        <mrow>         <mi>&#x3b1;</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <mi>&#x3b1;</mi>          <mo>+</mo>          <mi>&#x3b2;</mi>         </mrow>         <mo>)</mo>        </mrow>       </mrow>      </msqrt>     </mfrac>     <mo>,</mo>     <mfrac>      <mrow>       <msub>        <mi>G</mi>        <mn>2</mn>       </msub>       <mo>-</mo>       <mrow>        <mn>2</mn>        <mo>&#x2062;</mo>        <mi>&#x3b2;</mi>       </mrow>      </mrow>      <msqrt>       <mrow>        <mn>4</mn>        <mo>&#x2062;</mo>        <mrow>         <mi>&#x3b2;</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <mi>&#x3b1;</mi>          <mo>+</mo>          <mi>&#x3b2;</mi>         </mrow>         <mo>)</mo>        </mrow>       </mrow>      </msqrt>     </mfrac>    </mrow>    <mo>)</mo>   </mrow>  </mrow>  <semantics definitionURL="">   <mo>&#x2192;</mo>   <annotation encoding="Mathematica">"\[Rule]"</annotation>  </semantics>  <mrow>      <mrow>    <mrow>     <mo>(</mo>     <mrow>      <mn>0</mn>      <mo>,</mo>      <msub>       <mi>I</mi>       <mn>2</mn>      </msub>     </mrow>     <mo>)</mo>    </mrow>    <mo>.</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0098" num="0096">If n=&#x3b1;+&#x3b2; then this equation can be rewritten as</p><p id="p-0099" num="0000"><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <mrow>  <msqrt>   <mi>n</mi>  </msqrt>  <mo>&#x2062;</mo>  <mrow>   <mo>(</mo>   <mrow>    <mrow>     <mfrac>      <msub>       <mi>G</mi>       <mn>1</mn>      </msub>      <mrow>       <mi>n</mi>       <mo>&#x2062;</mo>       <msqrt>        <mrow>         <mn>4</mn>         <mo>&#x2062;</mo>         <mi>&#x3bc;</mi>        </mrow>       </msqrt>      </mrow>     </mfrac>     <mo>-</mo>     <msqrt>      <mi>&#x3bc;</mi>     </msqrt>    </mrow>    <mo>,</mo>    <mrow>     <mfrac>      <msub>       <mi>G</mi>       <mn>2</mn>      </msub>      <mrow>       <mi>n</mi>       <mo>&#x2062;</mo>       <msqrt>        <mrow>         <mn>4</mn>         <mo>&#x2062;</mo>         <mrow>          <mo>(</mo>          <mrow>           <mn>1</mn>           <mo>-</mo>           <mi>&#x3bc;</mi>          </mrow>          <mo>)</mo>         </mrow>        </mrow>       </msqrt>      </mrow>     </mfrac>     <mo>-</mo>     <msqrt>      <mrow>       <mn>1</mn>       <mo>-</mo>       <mi>&#x3bc;</mi>      </mrow>     </msqrt>    </mrow>   </mrow>   <mo>)</mo>  </mrow> </mrow></math></maths></p><p id="p-0100" num="0097">Given a function defined as</p><p id="p-0101" num="0000"><maths id="MATH-US-00007" num="00007"><math overflow="scroll"> <mrow>  <mrow>   <mi>&#x210a;</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <mrow>    <mi>x</mi>    <mo>,</mo>    <mi>y</mi>   </mrow>   <mo>)</mo>  </mrow>  <mo>:=</mo>  <mfrac>   <mrow>    <mi>x</mi>    <mo>&#x2062;</mo>    <msqrt>     <mi>&#x3bc;</mi>    </msqrt>   </mrow>   <mrow>    <mrow>     <mi>x</mi>     <mo>&#x2062;</mo>     <msqrt>      <mi>&#x3bc;</mi>     </msqrt>    </mrow>    <mo>+</mo>    <mrow>     <mi>y</mi>     <mo>&#x2062;</mo>     <msqrt>      <mrow>       <mn>1</mn>       <mo>-</mo>       <mi>&#x3bc;</mi>      </mrow>     </msqrt>    </mrow>   </mrow>  </mfrac> </mrow></math></maths></p><p id="p-0102" num="0098">A first derivative with respect to x and y is</p><p id="p-0103" num="0000"><maths id="MATH-US-00008" num="00008"><math overflow="scroll"> <mrow>  <mrow>   <msup>    <mi>&#x210a;</mi>    <mo>&#x2032;</mo>   </msup>   <mo>(</mo>   <mrow>    <mi>x</mi>    <mo>,</mo>    <mi>y</mi>   </mrow>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mo>(</mo>   <mrow>    <mfrac>     <mrow>      <mi>y</mi>      <mo>&#x2062;</mo>      <msqrt>       <mrow>        <mi>&#x3bc;</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mn>1</mn>         <mo>-</mo>         <mi>&#x3bc;</mi>        </mrow>        <mo>)</mo>       </mrow>      </msqrt>     </mrow>     <msup>      <mrow>       <mo>(</mo>       <mrow>        <mrow>         <mi>x</mi>         <mo>&#x2062;</mo>         <msqrt>          <mi>&#x3bc;</mi>         </msqrt>        </mrow>        <mo>+</mo>        <mrow>         <mi>y</mi>         <mo>&#x2062;</mo>         <msqrt>          <mrow>           <mn>1</mn>           <mo>-</mo>           <mi>&#x3bc;</mi>          </mrow>         </msqrt>        </mrow>       </mrow>       <mo>)</mo>      </mrow>      <mn>2</mn>     </msup>    </mfrac>    <mo>,</mo>    <mrow>     <mo>-</mo>     <mfrac>      <mrow>       <mi>x</mi>       <mo>&#x2062;</mo>       <msqrt>        <mrow>         <mi>&#x3bc;</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <mn>1</mn>          <mo>-</mo>          <mi>&#x3bc;</mi>         </mrow>         <mo>)</mo>        </mrow>       </msqrt>      </mrow>      <msup>       <mrow>        <mo>(</mo>        <mrow>         <mrow>          <mi>x</mi>          <mo>&#x2062;</mo>          <msqrt>           <mi>&#x3bc;</mi>          </msqrt>         </mrow>         <mo>+</mo>         <mrow>          <mi>y</mi>          <mo>&#x2062;</mo>          <msqrt>           <mrow>            <mn>1</mn>            <mo>-</mo>            <mi>&#x3bc;</mi>           </mrow>          </msqrt>         </mrow>        </mrow>        <mo>)</mo>       </mrow>       <mn>2</mn>      </msup>     </mfrac>    </mrow>   </mrow>   <mo>)</mo>  </mrow> </mrow></math></maths></p><p id="p-0104" num="0099">This can be reduced by the delta method to the following:</p><p id="p-0105" num="0000"><maths id="MATH-US-00009" num="00009"><math overflow="scroll"> <mrow>  <mrow>   <msqrt>    <mi>n</mi>   </msqrt>   <mo>&#x2062;</mo>   <mrow>    <mo>(</mo>    <mrow>     <mrow>      <mi>&#x210a;</mi>      <mo>(</mo>      <mrow>       <mfrac>        <msub>         <mi>G</mi>         <mn>1</mn>        </msub>        <mrow>         <mi>n</mi>         <mo>&#x2062;</mo>         <msqrt>          <mrow>           <mn>4</mn>           <mo>&#x2062;</mo>           <mi>&#x3bc;</mi>          </mrow>         </msqrt>        </mrow>       </mfrac>       <mo>,</mo>       <mfrac>        <msub>         <mi>G</mi>         <mn>2</mn>        </msub>        <mrow>         <mi>n</mi>         <mo>&#x2062;</mo>         <msqrt>          <mrow>           <mn>4</mn>           <mo>&#x2062;</mo>           <mrow>            <mo>(</mo>            <mrow>             <mn>1</mn>             <mo>-</mo>             <mi>&#x3bc;</mi>            </mrow>            <mo>)</mo>           </mrow>          </mrow>         </msqrt>        </mrow>       </mfrac>      </mrow>      <mo>)</mo>     </mrow>     <mo>-</mo>     <mrow>      <mi>&#x210a;</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <msqrt>        <mi>&#x3bc;</mi>       </msqrt>       <mo>,</mo>       <msqrt>        <mrow>         <mn>1</mn>         <mo>-</mo>         <mi>&#x3bc;</mi>        </mrow>       </msqrt>      </mrow>      <mo>)</mo>     </mrow>    </mrow>    <mo>)</mo>   </mrow>  </mrow>  <semantics definitionURL="">   <mo>&#x2192;</mo>   <annotation encoding="Mathematica">"\[Rule]"</annotation>  </semantics>  <mrow>   <msup>    <mrow>     <mi>&#x210a;</mi>     <mo>&#x2061;</mo>     <mo>(</mo>     <mrow>      <msqrt>       <mi>&#x3bc;</mi>      </msqrt>      <mo>,</mo>      <msqrt>       <mrow>        <mn>1</mn>        <mo>-</mo>        <mi>&#x3bc;</mi>       </mrow>      </msqrt>     </mrow>     <mo>)</mo>    </mrow>    <mi>T</mi>   </msup>      <mrow>    <mo>(</mo>    <mrow>     <mn>0</mn>     <mo>,</mo>     <msub>      <mi>I</mi>      <mn>2</mn>     </msub>    </mrow>    <mo>)</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0106" num="0100">As &#x3b1; and &#x3b2; tend to &#x221e;, this equation simplifies to</p><p id="p-0107" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x221a;{square root over (<i>n</i>)}(<i>X</i>&#x2212;&#x3bc;)&#x2192;<img id="CUSTOM-CHARACTER-00001" he="3.22mm" wi="2.79mm" file="US20230004570A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/>(0,&#x3bc;(1&#x2212;&#x3bc;))<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0108" num="0101">Given this, it is possible to determine a Gaussian approximation of Y where</p><p id="p-0109" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Y=&#x3b8;</i><sub>A</sub><sub><sub2>1</sub2></sub>&#x2212;&#x3b8;<sub>A</sub><sub><sub2>2</sub2></sub>.<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0110" num="0102">Then the Gaussian approximation of Y, which can be denoted {tilde over (Y)}, is, by the properties of Gaussian random variables is</p><p id="p-0111" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><img id="CUSTOM-CHARACTER-00002" he="3.22mm" wi="2.79mm" file="US20230004570A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/>(&#x3bc;<sub>1</sub>&#x2212;&#x3bc;<sub>2</sub>,&#x3c3;<sub>1</sub><sup>2</sup>+&#x3c3;<sub>2</sub><sup>2</sup>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0112" num="0103">The cumulative distribution function (cdf) for a standard Gaussian distribution is</p><p id="p-0113" num="0000"><maths id="MATH-US-00010" num="00010"><math overflow="scroll"> <mrow>  <mrow>   <mi>&#x3a6;</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <mi>x</mi>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <msqrt>     <mrow>      <mn>2</mn>      <mo>&#x2062;</mo>      <mi>&#x3c0;</mi>     </mrow>    </msqrt>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <msubsup>     <mo>&#x222b;</mo>     <mrow>      <mo>-</mo>      <mi>&#x221e;</mi>     </mrow>     <mi>x</mi>    </msubsup>    <mrow>     <mrow>      <mi>exp</mi>      <mo>(</mo>      <mrow>       <mo>-</mo>       <mfrac>        <msup>         <mi>t</mi>         <mn>2</mn>        </msup>        <mn>2</mn>       </mfrac>      </mrow>      <mo>)</mo>     </mrow>     <mo>&#x2062;</mo>     <mi>dt</mi>    </mrow>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0114" num="0104">The approximation becomes</p><p id="p-0115" num="0000"><maths id="MATH-US-00011" num="00011"><math overflow="scroll"> <mrow>  <mrow>   <mrow>    <mi>Pr</mi>    <mo>&#x2061;</mo>    <mo>(</mo>    <mrow>     <mi>Y</mi>     <mo>&#x2264;</mo>     <mn>0</mn>    </mrow>    <mo>)</mo>   </mrow>   <mo>&#x2248;</mo>   <mrow>    <mi>Pr</mi>    <mo>&#x2061;</mo>    <mo>(</mo>    <mrow>     <mover>      <mi>Y</mi>      <mo>~</mo>     </mover>     <mo>&#x2264;</mo>     <mn>0</mn>    </mrow>    <mo>)</mo>   </mrow>  </mrow>  <mo>=</mo>  <mrow>   <mi>&#x3a6;</mi>   <mo>(</mo>   <mfrac>    <mrow>     <mi>y</mi>     <mo>-</mo>     <mrow>      <mo>(</mo>      <mrow>       <msub>        <mi>&#x3bc;</mi>        <mn>1</mn>       </msub>       <mo>-</mo>       <msub>        <mi>&#x3bc;</mi>        <mn>2</mn>       </msub>      </mrow>      <mo>)</mo>     </mrow>    </mrow>    <mrow>     <mo>(</mo>     <mrow>      <msubsup>       <mi>&#x3c3;</mi>       <mn>1</mn>       <mn>2</mn>      </msubsup>      <mo>+</mo>      <msubsup>       <mi>&#x3c3;</mi>       <mn>2</mn>       <mn>2</mn>      </msubsup>     </mrow>     <mo>)</mo>    </mrow>   </mfrac>   <mo>)</mo>  </mrow> </mrow></math></maths><maths id="MATH-US-00011-2" num="00011.2"><math overflow="scroll"> <mi>Where</mi></math></maths><maths id="MATH-US-00011-3" num="00011.3"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>&#x3c3;</mi>    <mi>i</mi>   </msub>   <mo>=</mo>   <msqrt>    <mrow>     <mrow>      <msub>       <mi>&#x3bc;</mi>       <mi>i</mi>      </msub>      <mo>(</mo>      <mrow>       <mn>1</mn>       <mo>-</mo>       <msub>        <mi>&#x3bc;</mi>        <mi>i</mi>       </msub>      </mrow>      <mo>)</mo>     </mrow>     <mo>/</mo>     <mrow>      <mo>(</mo>      <mrow>       <msub>        <mi>&#x3b1;</mi>        <mi>i</mi>       </msub>       <mo>+</mo>       <msub>        <mi>&#x3b2;</mi>        <mi>i</mi>       </msub>      </mrow>      <mo>)</mo>     </mrow>    </mrow>   </msqrt>  </mrow>  <mo>,</mo>  <mrow>   <mi>i</mi>   <mo>=</mo>   <mn>1</mn>  </mrow>  <mo>,</mo>  <mn>2.</mn> </mrow></math></maths></p><p id="p-0116" num="0105">The Y approximation above is used in this disclosure as an example attractiveness model to approximate the probability of a document being more attractive than another document.</p><p id="p-0117" num="0106">Each pairwise comparison is scored for significance, which generally indicates whether the measured difference in attractiveness is above a margin of error. A pairwise comparison is stored in the pairwise comparisons dataset <b>116</b> if the significance of the pairwise comparison is above a specified threshold. This process is illustrated in more detail in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0118" num="0107">At step <b>412</b>, the score adjustment module <b>128</b> determines <b>412</b> whether to continue pairwise comparisons based on a different query. The score adjustment module <b>128</b> may determine for example that there are a statistically significant sample of query and document pair combinations that have been analysed. The score adjustment module may set additional criteria for finishing the pairwise comparisons.</p><p id="p-0119" num="0108">If the score adjustment module <b>128</b> determines the process to continue, the score adjustment module <b>128</b> can return to <b>402</b> to determine a new query and repeat the processing above for a new query. This will build a set of pairwise comparisons for multiple queries. The steps of calculating pairwise comparisons are repeated until there are a sufficient amount of pairwise relevance comparisons stored in the pairwise comparison dataset <b>116</b>.</p><p id="p-0120" num="0109">Once the pairwise comparison dataset has been determined, the dataset can be used as a feedback mechanism to train a neural network to learn how to perform a score adjustment on a document or documents. Training a neural network in this manner is outlined in more detail in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0121" num="0110">Once the neural network is trained, then it is possible to utilise the neural network to determine <b>416</b> a score adjustment for a document based on the neural network. That is, a given document can be input (typically input includes features of a document) into the neural network which then generates a score adjustment for that document. The score adjustment can then be used by a search engine such as <b>140</b> (or other process) to update document scores per <b>310</b> above.</p><heading id="h-0015" level="2">Pairwise Comparison Dataset</heading><p id="p-0122" num="0111">As described above, it is possible to generate a dataset of pairwise comparisons from a model of attractiveness of one or more documents in the results of a query. Pairwise comparisons involve determining for a pair of documents which one is perceived to be most relevant. This is illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0123" num="0112">In the present disclosure, pairwise comparisons are generated by the pairwise comparisons module <b>132</b>.</p><p id="p-0124" num="0113">Initially, pairwise comparisons module <b>132</b> selects <b>502</b> two documents from the document database. These documents can be selected at random, or selected in such a way as to ensure that the compared documents are a representative subset of all the documents in the document database. A representative subset may be a statistically significant sample of free images for example. For the purposes of the following example, a first document is referred to as i, a second document is referred to as j.</p><p id="p-0125" num="0114">For the first document i, the pairwise comparisons module <b>132</b> determines <b>504</b> parameters of the attractiveness model described using the click information. In the example attractiveness model described above, based on a Gaussian approximation of a random variable X, one equation that can be used to determine the result is</p><p id="p-0126" num="0000"><maths id="MATH-US-00012" num="00012"><math overflow="scroll"> <mrow>  <mi>&#x3d5;</mi>  <mo>(</mo>  <mfrac>   <mrow>    <mi>y</mi>    <mo>-</mo>    <mrow>     <mo>(</mo>     <mrow>      <msub>       <mi>&#xb5;</mi>       <mn>1</mn>      </msub>      <mo>+</mo>      <msub>       <mi>&#xb5;</mi>       <mn>2</mn>      </msub>     </mrow>     <mo>)</mo>    </mrow>   </mrow>   <mrow>    <msubsup>     <mi>&#x3c3;</mi>     <mn>1</mn>     <mn>2</mn>    </msubsup>    <mo>+</mo>    <msubsup>     <mi>&#x3c3;</mi>     <mn>2</mn>     <mn>2</mn>    </msubsup>   </mrow>  </mfrac>  <mo>)</mo> </mrow></math></maths></p><p id="p-0127" num="0000">where &#x3bc; is the mean of X and &#x3c3; is the variance of X.</p><p id="p-0128" num="0115">For the second document j, the pairwise comparisons module <b>132</b> determines <b>506</b> parameters of the attractiveness model described using the click information. Again the same equation that can be used to determine the result:</p><p id="p-0129" num="0000"><maths id="MATH-US-00013" num="00013"><math overflow="scroll"> <mrow>  <mi>&#x3d5;</mi>  <mo>(</mo>  <mfrac>   <mrow>    <mi>y</mi>    <mo>-</mo>    <mrow>     <mo>(</mo>     <mrow>      <msub>       <mi>&#xb5;</mi>       <mn>1</mn>      </msub>      <mo>+</mo>      <msub>       <mi>&#xb5;</mi>       <mn>2</mn>      </msub>     </mrow>     <mo>)</mo>    </mrow>   </mrow>   <mrow>    <msubsup>     <mi>&#x3c3;</mi>     <mn>1</mn>     <mn>2</mn>    </msubsup>    <mo>+</mo>    <msubsup>     <mi>&#x3c3;</mi>     <mn>2</mn>     <mn>2</mn>    </msubsup>   </mrow>  </mfrac>  <mo>)</mo> </mrow></math></maths></p><p id="p-0130" num="0000">where &#x3bc; is the mean of X and &#x3c3; is the variance of X.</p><p id="p-0131" num="0116">For the pair of documents (i,j), the pairwise comparisons module <b>132</b> determines <b>508</b> the relative attractive score of the pair, that is, probability based on the attractiveness model that one document is more attractive than the other document. This may involve the calculation of a probability value (referred to as p-value) that, in an example, represents the probability that a hypothesis, such as document i is more attractive than document j, is wrong or false. That is, the p-value can be considered representative of the probability of error in a hypothesis being true.</p><p id="p-0132" num="0117">The pairwise comparisons module <b>132</b> then determines <b>510</b> the significance of the relative attractiveness of the pair of documents.</p><p id="p-0133" num="0118">If the p-value or error in a hypothesis of attractiveness is significant (that is, the the p-value is above or below a specified significance level&#x2014;above or below depending on the significance calculation used) then pairwise comparisons module <b>132</b> stores the comparison in the pairwise comparisons database <b>116</b>. Any appropriate level of significance may be adopted, however by way of example in certain implementations the threshold significance is 0.05. For example, if the p-value is lower than the significance threshold 0.05, a hypothesis that document 1 is more attractive than document 2 would be accepted (that is, stored in the pairwise comparisons database <b>116</b>). To be very confident that all stored pairwise comparisons are correct, a high threshold significance value, like 0.001 could be used. Each pairwise comparison therefore represents a relative attractiveness documents within a pair of documents.</p><p id="p-0134" num="0119">An example data structure for records stored in the pairwise comparisons dataset is:</p><p id="p-0135" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="77pt" align="left"/><colspec colname="1" colwidth="140pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>[</entry></row><row><entry/><entry>&#x2003;document id 1,</entry></row><row><entry/><entry>&#x2003;document id 2,</entry></row><row><entry/><entry>&#x2003;query id,</entry></row><row><entry/><entry>&#x2003;p-value</entry></row><row><entry/><entry>]</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><heading id="h-0016" level="2">Neural Network</heading><p id="p-0136" num="0120">Calculating pairwise comparisons as described above is potentially a computationally expensive process since pairs of documents need to be compared for each query.</p><p id="p-0137" num="0121">In certain embodiments, a neural network is used to model how documents are ranked and thereby reduce computation at a query level. The neural network, referred to in this disclosure as an adjustment model <b>118</b>, is trained on the dataset of pairwise relevance comparisons discussed above. A process <b>600</b> for training the neural network to learn how to adjust scores is illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><heading id="h-0017" level="2">Document Features</heading><p id="p-0138" num="0122">In this example, inputs to the neural network include document features. Document features are metrics of characteristics of a document. Relevantly, document features include characteristics of the content of the document, such as the average intensity value of each pixel in an image, as well as characteristics of the metadata of the document such as the author or photographer. Notably, document features can be query-invariant, that is, the feature value (or score) in respect of a document is independent of other documents and does not depend on a search query.</p><p id="p-0139" num="0123">In some embodiments all features of features can be used as inputs to the neural network. In other embodiments, a subset of features of documents can be specified. By way of example, for image-type documents, specified features contemplated include: whether an image is a cutout; whether an object in an image is a single, isolated object; whether the image supports being placed in a photoholder; whether an image can be recoloured; whether an image should be rescaled or repeated if enlarged; whether an image is free or paid; whether the image is a raster or vector; and a score indicative of the quality of the image (for example a value from 1 to 4). Other examples may be contributor brand or artist. Document features are typically numeric or symbolic for ease of processing. For some features, types such as string can be converted to numerical categorical values.</p><p id="p-0140" num="0124">Features can be centered and whitened to aid with convergence during training. Data pre-processing can be performed to determine if values of the features are missing. In some embodiments, missing feature values can be imputed using, for example, an average value.</p><p id="p-0141" num="0125">In other embodiments, a deep learning neural network may be used. In those embodiments, it is not necessary to select features as the relevant features will be determined by deep learning neural network itself. The deep learning neural network in addition automatically calibrates the relevant features. These embodiments will be described in more detail below.</p><heading id="h-0018" level="2">Adjustment Model <b>118</b></heading><p id="p-0142" num="0126">In the following example, the adjustment model <b>118</b> is a neural network that takes in one or more inputs. The inputs for a given document are features of that document (as described above) and may be described/input to the model in, for example, a feature vector. In some embodiments, the adjustment model <b>118</b> is a Siamese neural network. A Siamese neural network uses the same weights while taking in two different input vectors to compute an output vector. In this case, there are two documents involved in a comparison so features of both documents are used as inputs to the Siamese neural network.</p><p id="p-0143" num="0127">Typically, the inputs to the adjustment model <b>118</b> are features of the document. Therefore for each pair of documents d<sub>i </sub>and d<sub>j</sub>, the features of d<sub>i </sub>will be one input and the features of d<sub>j </sub>will be another input. The weights in the neural network can be adjusted through optimization procedures. Optimization procedures contemplated in this disclosure include stochastic gradient descent.</p><p id="p-0144" num="0128">The neural network of the adjustment model <b>118</b> can be trained by adjusting the weights to minimize a loss function. The loss function can be, for example, a cross entropy loss function. An example cross entropy loss function can be defined as:</p><p id="p-0145" num="0000"><maths id="MATH-US-00014" num="00014"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>L</mi>    <mrow>     <mi>i</mi>     <mo>,</mo>     <mi>j</mi>    </mrow>   </msub>   <mo>(</mo>   <mrow>    <msub>     <mi>b</mi>     <mi>i</mi>    </msub>    <mo>,</mo>    <msub>     <mi>b</mi>     <mi>j</mi>    </msub>   </mrow>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mrow>    <mo>-</mo>    <msub>     <mover>      <mi>p</mi>      <mo>~</mo>     </mover>     <mrow>      <mi>i</mi>      <mo>,</mo>      <mi>j</mi>     </mrow>    </msub>    <mo>&#x2062;</mo>    <mi>log</mi>    <mo>&#x2062;</mo>    <msub>     <mi>p</mi>     <mrow>      <mi>i</mi>      <mo>,</mo>      <mi>j</mi>     </mrow>    </msub>   </mrow>   <mo>-</mo>   <mrow>    <mrow>     <mo>(</mo>     <mrow>      <mn>1</mn>      <mo>-</mo>      <msub>       <mover>        <mi>p</mi>        <mo>~</mo>       </mover>       <mrow>        <mi>i</mi>        <mo>,</mo>        <mi>j</mi>       </mrow>      </msub>     </mrow>     <mo>)</mo>    </mrow>    <mo>&#x2062;</mo>    <mrow>     <mi>log</mi>     <mo>&#x2061;</mo>     <mo>(</mo>     <mrow>      <mn>1</mn>      <mo>-</mo>      <msub>       <mi>p</mi>       <mrow>        <mi>i</mi>        <mo>,</mo>        <mi>j</mi>       </mrow>      </msub>     </mrow>     <mo>)</mo>    </mrow>   </mrow>  </mrow> </mrow></math></maths><maths id="MATH-US-00014-2" num="00014.2"><math overflow="scroll"> <mi>Where</mi></math></maths><maths id="MATH-US-00014-3" num="00014.3"><math overflow="scroll"> <mrow>  <msub>   <mi>p</mi>   <mrow>    <mi>i</mi>    <mo>,</mo>    <mi>j</mi>   </mrow>  </msub>  <mo>=</mo>  <mfrac>   <mn>1</mn>   <mrow>    <mn>1</mn>    <mo>+</mo>    <mrow>     <mi>exp</mi>     <mo>&#x2061;</mo>     <mo>(</mo>     <mrow>      <mo>-</mo>      <mrow>       <mi>&#x3b3;</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mrow>         <msub>          <mi>b</mi>          <mi>i</mi>         </msub>         <mo>&#x2062;</mo>         <msub>          <mi>s</mi>          <mi>i</mi>         </msub>        </mrow>        <mo>-</mo>        <mrow>         <msub>          <mi>b</mi>          <mi>j</mi>         </msub>         <mo>&#x2062;</mo>         <msub>          <mi>s</mi>          <mi>j</mi>         </msub>        </mrow>       </mrow>       <mo>)</mo>      </mrow>     </mrow>     <mo>)</mo>    </mrow>   </mrow>  </mfrac> </mrow></math></maths></p><p id="p-0146" num="0129">This can be simplified to &#x3b3;(1&#x2212;p<sub>i,j</sub>)(b<sub>i</sub>s<sub>i</sub>&#x2212;b<sub>j</sub>s<sub>j</sub>)+log (1+exp(&#x2212;&#x3b3;(b<sub>i</sub>s<sub>i</sub>&#x2212;b<sub>j</sub>s<sub>j</sub>)))</p><p id="p-0147" num="0130">In this example loss function, &#x3b3; is a steepness parameter, p<sub>i,j </sub>is the target probability of document pair i and j and b<sub>i </sub>and b<sub>j </sub>are the score adjustment values for the corresponding documents i and j (the score adjustment values being based on the query invariant features for the documents). In this example, s<sub>i </sub>and s<sub>j </sub>are the base scores from the search engine for documents i and j.</p><p id="p-0148" num="0131">If a Siamese neural network is used, then it is possible to train the network with pairs such that p<sub>i,j</sub>=1 if document i is more relevant than document j and zero otherwise, effectively generating a binary label/classification for the stored pairwise comparisons dataset in pairwise comparisons database <b>116</b>. Because a Siamese neural network is symmetrical, cases where p<sub>i,j</sub>=1 need only be considered. This can simplify the loss function. The adjustment model <b>118</b> then seeks to minimise the average loss over all the document pairs. The average loss over all document pairs in a dataset D can be represented as</p><p id="p-0149" num="0000"><maths id="MATH-US-00015" num="00015"><math overflow="scroll"> <mrow>  <mrow>   <mi>L</mi>   <mo>&#x2061;</mo>   <mo>(</mo>      <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <mrow>     <semantics definitionURL="">      <mo>&#x2758;</mo>      <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>     </semantics>          <semantics definitionURL="">      <mo>&#x2758;</mo>      <annotation encoding="Mathematica">"\[RightBracketingBar]"</annotation>     </semantics>    </mrow>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <munder>     <mo>&#x2211;</mo>     <mrow>      <mi>i</mi>      <mo>,</mo>      <mrow>       <mi>j</mi>       <mo>&#x2208;</mo>             </mrow>     </mrow>    </munder>    <mrow>     <msub>      <mi>L</mi>      <mrow>       <mi>i</mi>       <mo>,</mo>       <mi>j</mi>      </mrow>     </msub>     <mo>(</mo>     <mrow>      <msub>       <mi>b</mi>       <mi>i</mi>      </msub>      <mo>,</mo>      <msub>       <mi>b</mi>       <mi>j</mi>      </msub>     </mrow>     <mo>)</mo>    </mrow>   </mrow>  </mrow> </mrow></math></maths></p><heading id="h-0019" level="1">Training an Adjustment Model <b>118</b> (Embodiment 1)</heading><p id="p-0150" num="0132">This section describes a first embodiment of a computer implemented method for training an adjustment model <b>118</b>. The process <b>600</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0151" num="0133">The process <b>600</b>, which is performed by the adjustment model <b>118</b> module <b>134</b>, starts whenever the adjustment model <b>118</b> needs to be initialised. In the present example the adjustment model <b>118</b> is a Siamese neural network.</p><p id="p-0152" num="0134">In order to train the adjustment model <b>118</b>, as a first step, the adjustment model module <b>134</b> determines <b>602</b> document features that will be used to assess attractiveness. As above, document features include metrics of the documents, document type, category, metadata and potentially distinguishing aspects of the document.</p><p id="p-0153" num="0135">The next step <b>604</b> is to determine a pair of documents (i, j) that will be compared, for example by selecting two documents at random from the document database <b>112</b>. In some cases, the adjustment model module <b>134</b> may take into account documents that have previously been selected and preference documents that have not been selected for comparison.</p><p id="p-0154" num="0136">At <b>606</b>, a feature vector is determined for each of the documents (i,j) set of the relevant feature values for that document.</p><p id="p-0155" num="0137">In this example, the neural network of the adjustment model <b>118</b> is a Siamese neural network as described above. This means there are two subcomponents to the neural network: subnetwork A and subnetwork B. At <b>608</b>, a first feature vector, that is the feature vector corresponding to a first document i, is input into subnetwork A. The second feature vector, that is the feature vector corresponding to a second document j, is input into subnetwork B.</p><p id="p-0156" num="0138">The adjustment model module <b>134</b> then estimates <b>610</b> the boost of document i from the output of subnetwork A based on the feature vector of document i. Similarly, the adjustment model module <b>134</b> estimates <b>612</b> the boost of document j from the output of subnetwork B based on the feature vector of document j.</p><p id="p-0157" num="0139">The adjustment model module <b>134</b> then determines <b>614</b> the attractiveness of document i from the document attractiveness model <b>126</b> described above. Similarly, the adjustment model module <b>134</b> then determines <b>616</b> the attractiveness of document j from the document attractiveness model <b>126</b> described above. Steps <b>614</b> and <b>616</b> may simply involve determining the entries for document pair i, j in the pairwise comparisons dataset stored in the pairwise comparisons database <b>116</b> where the pairwise comparison dataset has been previously determined.</p><p id="p-0158" num="0140">Based on this, the adjustment model module <b>134</b> can determine <b>620</b> a target probability that document i is more attractive than document j (as described above).</p><p id="p-0159" num="0141">The adjustment model module <b>134</b> then determines <b>620</b> the loss based on the loss function, which in this example (and as described above) is a cross-entropy loss, taking into account the target probability determined earlier.</p><p id="p-0160" num="0142">The weights of subnetwork A and B are then updated <b>622</b> based on the cross-entropy loss according to a gradient.</p><p id="p-0161" num="0143">The gradient for each document pair i,j is given by</p><p id="p-0162" num="0000"><maths id="MATH-US-00016" num="00016"><math overflow="scroll"> <mrow>  <mfrac>   <mrow>    <mo>&#x2202;</mo>    <msub>     <mi>L</mi>     <mrow>      <mi>i</mi>      <mo>,</mo>      <mi>j</mi>     </mrow>    </msub>   </mrow>   <mrow>    <mo>&#x2202;</mo>    <msub>     <mi>b</mi>     <mi>i</mi>    </msub>   </mrow>  </mfrac>  <mo>=</mo>  <mrow>   <mi>&#x3b3;</mi>   <mo>&#x2062;</mo>   <mrow>    <msub>     <mi>s</mi>     <mi>i</mi>    </msub>    <mo>(</mo>    <mrow>     <mrow>      <mo>(</mo>      <mrow>       <mn>1</mn>       <mo>-</mo>       <msub>        <mover>         <mi>p</mi>         <mo>~</mo>        </mover>        <mrow>         <mi>i</mi>         <mo>,</mo>         <mi>j</mi>        </mrow>       </msub>      </mrow>      <mo>)</mo>     </mrow>     <mo>-</mo>     <mfrac>      <mn>1</mn>      <mrow>       <mn>1</mn>       <mo>+</mo>       <mrow>        <mi>exp</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mo>-</mo>         <mrow>          <mi>&#x3b3;</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mrow>            <msub>             <mi>b</mi>             <mi>i</mi>            </msub>            <mo>&#x2062;</mo>            <msub>             <mi>s</mi>             <mi>i</mi>            </msub>           </mrow>           <mo>-</mo>           <mrow>            <msub>             <mi>b</mi>             <mi>j</mi>            </msub>            <mo>&#x2062;</mo>            <msub>             <mi>s</mi>             <mi>j</mi>            </msub>           </mrow>          </mrow>          <mo>)</mo>         </mrow>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mfrac>    </mrow>    <mo>)</mo>   </mrow>  </mrow> </mrow></math></maths><maths id="MATH-US-00016-2" num="00016.2"><math overflow="scroll"> <mi>And</mi></math></maths><maths id="MATH-US-00016-3" num="00016.3"><math overflow="scroll"> <mrow>  <mfrac>   <mrow>    <mo>&#x2202;</mo>    <msub>     <mi>L</mi>     <mrow>      <mi>i</mi>      <mo>,</mo>      <mi>j</mi>     </mrow>    </msub>   </mrow>   <mrow>    <mo>&#x2202;</mo>    <msub>     <mi>b</mi>     <mi>j</mi>    </msub>   </mrow>  </mfrac>  <mo>=</mo>  <mrow>   <mo>-</mo>   <mi>&#x3b3;</mi>   <mo>&#x2062;</mo>   <mrow>    <msub>     <mi>s</mi>     <mi>j</mi>    </msub>    <mo>(</mo>    <mrow>     <mrow>      <mo>(</mo>      <mrow>       <mn>1</mn>       <mo>-</mo>       <msub>        <mover>         <mi>p</mi>         <mo>~</mo>        </mover>        <mrow>         <mi>i</mi>         <mo>,</mo>         <mi>j</mi>        </mrow>       </msub>      </mrow>      <mo>)</mo>     </mrow>     <mo>-</mo>     <mfrac>      <mn>1</mn>      <mrow>       <mn>1</mn>       <mo>+</mo>       <mrow>        <mi>exp</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mo>-</mo>         <mrow>          <mi>&#x3b3;</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mrow>            <msub>             <mi>b</mi>             <mi>i</mi>            </msub>            <mo>&#x2062;</mo>            <msub>             <mi>s</mi>             <mi>i</mi>            </msub>           </mrow>           <mo>-</mo>           <mrow>            <msub>             <mi>b</mi>             <mi>j</mi>            </msub>            <mo>&#x2062;</mo>            <msub>             <mi>s</mi>             <mi>j</mi>            </msub>           </mrow>          </mrow>          <mo>)</mo>         </mrow>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mfrac>    </mrow>    <mo>)</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0163" num="0144">The learning rule of the Siamese network using a single training pair, with learning parameter n can then be expressed using stochastic gradient descent as</p><p id="p-0164" num="0000"><maths id="MATH-US-00017" num="00017"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>w</mi>    <mi>k</mi>   </msub>   <mo>&#x2190;</mo>   <mrow>    <msub>     <mi>w</mi>     <mi>k</mi>    </msub>    <mo>-</mo>    <mrow>     <mi>&#x3b7;</mi>     <mo>&#x2062;</mo>     <mfrac>      <mrow>       <mo>&#x2202;</mo>       <msub>        <mi>L</mi>        <mrow>         <mi>i</mi>         <mo>,</mo>         <mi>j</mi>        </mrow>       </msub>      </mrow>      <mrow>       <mo>&#x2202;</mo>       <msub>        <mi>w</mi>        <mi>k</mi>       </msub>      </mrow>     </mfrac>    </mrow>   </mrow>  </mrow>  <mo>=</mo>  <mrow>   <msub>    <mi>w</mi>    <mi>k</mi>   </msub>   <mo>-</mo>   <mrow>    <mi>&#x3b7;</mi>    <mo>(</mo>    <mrow>     <mrow>      <mfrac>       <mrow>        <mo>&#x2202;</mo>        <msub>         <mi>L</mi>         <mrow>          <mi>i</mi>          <mo>,</mo>          <mi>j</mi>         </mrow>        </msub>       </mrow>       <mrow>        <mo>&#x2202;</mo>        <msub>         <mi>b</mi>         <mi>i</mi>        </msub>       </mrow>      </mfrac>      <mo>&#x2062;</mo>      <mfrac>       <mrow>        <mo>&#x2202;</mo>        <msub>         <mi>b</mi>         <mi>i</mi>        </msub>       </mrow>       <mrow>        <mo>&#x2202;</mo>        <msub>         <mi>w</mi>         <mi>k</mi>        </msub>       </mrow>      </mfrac>     </mrow>     <mo>+</mo>     <mrow>      <mfrac>       <mrow>        <mo>&#x2202;</mo>        <msub>         <mi>L</mi>         <mrow>          <mi>i</mi>          <mo>,</mo>          <mi>j</mi>         </mrow>        </msub>       </mrow>       <mrow>        <mo>&#x2202;</mo>        <msub>         <mi>b</mi>         <mi>j</mi>        </msub>       </mrow>      </mfrac>      <mo>&#x2062;</mo>      <mfrac>       <mrow>        <mo>&#x2202;</mo>        <msub>         <mi>b</mi>         <mi>j</mi>        </msub>       </mrow>       <mrow>        <mo>&#x2202;</mo>        <msub>         <mi>w</mi>         <mi>k</mi>        </msub>       </mrow>      </mfrac>     </mrow>    </mrow>    <mo>)</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0165" num="0145">This can be expressed as</p><p id="p-0166" num="0000"><maths id="MATH-US-00018" num="00018"><math overflow="scroll"> <mrow>  <mfrac>   <mrow>    <mo>&#x2202;</mo>    <msub>     <mi>L</mi>     <mrow>      <mi>i</mi>      <mo>,</mo>      <mi>j</mi>     </mrow>    </msub>   </mrow>   <mrow>    <mo>&#x2202;</mo>    <msub>     <mi>w</mi>     <mi>k</mi>    </msub>   </mrow>  </mfrac>  <mo>=</mo>  <mrow>   <mrow>    <mi>&#x3b3;</mi>    <mo>(</mo>    <mrow>     <mrow>      <mo>(</mo>      <mrow>       <mn>1</mn>       <mo>-</mo>       <msub>        <mover>         <mi>p</mi>         <mo>~</mo>        </mover>        <mrow>         <mi>i</mi>         <mo>,</mo>         <mi>j</mi>        </mrow>       </msub>      </mrow>      <mo>)</mo>     </mrow>     <mo>-</mo>     <mfrac>      <mn>1</mn>      <mrow>       <mn>1</mn>       <mo>+</mo>       <mrow>        <mi>exp</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mo>-</mo>         <mrow>          <mi>&#x3b3;</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mrow>            <msub>             <mi>b</mi>             <mi>i</mi>            </msub>            <mo>&#x2062;</mo>            <msub>             <mi>s</mi>             <mi>i</mi>            </msub>           </mrow>           <mo>-</mo>           <mrow>            <msub>             <mi>b</mi>             <mi>j</mi>            </msub>            <mo>&#x2062;</mo>            <msub>             <mi>s</mi>             <mi>j</mi>            </msub>           </mrow>          </mrow>          <mo>)</mo>         </mrow>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mfrac>    </mrow>    <mo>)</mo>   </mrow>   <mo>&#x2062;</mo>   <mrow>    <mo>(</mo>    <mrow>     <mrow>      <msub>       <mi>s</mi>       <mi>i</mi>      </msub>      <mo>&#x2062;</mo>      <mfrac>       <mrow>        <mo>&#x2202;</mo>        <msub>         <mi>b</mi>         <mi>i</mi>        </msub>       </mrow>       <mrow>        <mo>&#x2202;</mo>        <msub>         <mi>w</mi>         <mi>k</mi>        </msub>       </mrow>      </mfrac>     </mrow>     <mo>-</mo>     <mrow>      <msub>       <mi>s</mi>       <mi>j</mi>      </msub>      <mo>&#x2062;</mo>      <mfrac>       <mrow>        <mo>&#x2202;</mo>        <msub>         <mi>b</mi>         <mi>j</mi>        </msub>       </mrow>       <mrow>        <mo>&#x2202;</mo>        <msub>         <mi>w</mi>         <mi>k</mi>        </msub>       </mrow>      </mfrac>     </mrow>    </mrow>    <mo>)</mo>   </mrow>  </mrow> </mrow></math></maths><maths id="MATH-US-00018-2" num="00018.2"><math overflow="scroll"> <mi>or</mi></math></maths><maths id="MATH-US-00018-3" num="00018.3"><math overflow="scroll"> <mrow>  <mo>=</mo>  <mrow>   <msub>    <mi>&#x3bb;</mi>    <mrow>     <mi>i</mi>     <mo>,</mo>     <mi>j</mi>    </mrow>   </msub>   <mo>(</mo>   <mrow>    <mrow>     <msub>      <mi>s</mi>      <mi>i</mi>     </msub>     <mo>&#x2062;</mo>     <mfrac>      <mrow>       <mo>&#x2202;</mo>       <msub>        <mi>b</mi>        <mi>i</mi>       </msub>      </mrow>      <mrow>       <mo>&#x2202;</mo>       <msub>        <mi>w</mi>        <mi>k</mi>       </msub>      </mrow>     </mfrac>    </mrow>    <mo>-</mo>    <mrow>     <msub>      <mi>s</mi>      <mi>j</mi>     </msub>     <mo>&#x2062;</mo>     <mfrac>      <mrow>       <mo>&#x2202;</mo>       <msub>        <mi>b</mi>        <mi>j</mi>       </msub>      </mrow>      <mrow>       <mo>&#x2202;</mo>       <msub>        <mi>w</mi>        <mi>k</mi>       </msub>      </mrow>     </mfrac>    </mrow>   </mrow>   <mo>)</mo>  </mrow> </mrow></math></maths></p><p id="p-0167" num="0146">That is,</p><p id="p-0168" num="0000"><maths id="MATH-US-00019" num="00019"><math overflow="scroll"> <mrow>  <msub>   <mi>&#x3bb;</mi>   <mrow>    <mi>i</mi>    <mo>,</mo>    <mi>j</mi>   </mrow>  </msub>  <mo>:=</mo>  <mrow>   <mi>&#x3b3;</mi>   <mo>(</mo>   <mrow>    <mrow>     <mo>(</mo>     <mrow>      <mn>1</mn>      <mo>-</mo>      <msub>       <mover>        <mi>p</mi>        <mo>~</mo>       </mover>       <mrow>        <mi>i</mi>        <mo>,</mo>        <mi>j</mi>       </mrow>      </msub>     </mrow>     <mo>)</mo>    </mrow>    <mo>-</mo>    <mfrac>     <mn>1</mn>     <mrow>      <mn>1</mn>      <mo>+</mo>      <mrow>       <mi>exp</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mo>-</mo>        <mrow>         <mi>&#x3b3;</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <mrow>           <msub>            <mi>b</mi>            <mi>i</mi>           </msub>           <mo>&#x2062;</mo>           <msub>            <mi>s</mi>            <mi>i</mi>           </msub>          </mrow>          <mo>-</mo>          <mrow>           <msub>            <mi>b</mi>            <mi>j</mi>           </msub>           <mo>&#x2062;</mo>           <msub>            <mi>s</mi>            <mi>j</mi>           </msub>          </mrow>         </mrow>         <mo>)</mo>        </mrow>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mfrac>   </mrow>   <mo>)</mo>  </mrow> </mrow></math></maths></p><p id="p-0169" num="0147">The adjustment model module <b>134</b> may train over a single pair of documents, or a group of documents. This is referred to as a minibatch. A minibatch may improve the speed at which the model can be trained. That is, it is possible to use a process of mini-batched stochastic gradient descent to speed up training the adjustment model <b>118</b>. Document pairs in a minibatch I can be summed up as follows</p><p id="p-0170" num="0000"><maths id="MATH-US-00020" num="00020"><math overflow="scroll"> <mrow>  <mrow>   <munder>    <mo>&#x2211;</mo>    <mrow>     <mi>i</mi>     <mo>,</mo>     <mi>j</mi>    </mrow>   </munder>   <mrow>    <msub>     <mi>&#x3bb;</mi>     <mrow>      <mi>i</mi>      <mo>,</mo>      <mi>j</mi>     </mrow>    </msub>    <mo>(</mo>    <mrow>     <mrow>      <msub>       <mi>s</mi>       <mi>i</mi>      </msub>      <mo>&#x2062;</mo>      <mfrac>       <mrow>        <mo>&#x2202;</mo>        <msub>         <mi>b</mi>         <mi>i</mi>        </msub>       </mrow>       <mrow>        <mo>&#x2202;</mo>        <msub>         <mi>w</mi>         <mi>k</mi>        </msub>       </mrow>      </mfrac>     </mrow>     <mo>-</mo>     <mrow>      <msub>       <mi>s</mi>       <mi>j</mi>      </msub>      <mo>&#x2062;</mo>      <mfrac>       <mrow>        <mo>&#x2202;</mo>        <msub>         <mi>b</mi>         <mi>j</mi>        </msub>       </mrow>       <mrow>        <mo>&#x2202;</mo>        <msub>         <mi>w</mi>         <mi>k</mi>        </msub>       </mrow>      </mfrac>     </mrow>    </mrow>    <mo>)</mo>   </mrow>  </mrow>  <mo>=</mo>  <mrow>   <munder>    <mo>&#x2211;</mo>    <mi>i</mi>   </munder>   <mrow>    <msub>     <mi>&#x3bb;</mi>     <mi>i</mi>    </msub>    <mo>&#x2062;</mo>    <msub>     <mi>s</mi>     <mi>i</mi>    </msub>    <mo>&#x2062;</mo>    <mfrac>     <mrow>      <mo>&#x2202;</mo>      <msub>       <mi>b</mi>       <mi>i</mi>      </msub>     </mrow>     <mrow>      <mo>&#x2202;</mo>      <msub>       <mi>w</mi>       <mi>k</mi>      </msub>     </mrow>    </mfrac>   </mrow>  </mrow> </mrow></math></maths><maths id="MATH-US-00020-2" num="00020.2"><math overflow="scroll"> <mi>where</mi></math></maths><maths id="MATH-US-00020-3" num="00020.3"><math overflow="scroll"> <mrow>  <msub>   <mi>&#x3bb;</mi>   <mi>i</mi>  </msub>  <mo>=</mo>  <mrow>   <mrow>    <munder>     <mo>&#x2211;</mo>     <mrow>      <mrow>       <mi>j</mi>       <mo>:</mo>       <mrow>        <mo>(</mo>        <mrow>         <mi>i</mi>         <mo>,</mo>         <mi>j</mi>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mo>&#x2208;</mo>      <mi>&#x2110;</mi>     </mrow>    </munder>    <msub>     <mi>&#x3bb;</mi>     <mrow>      <mi>i</mi>      <mo>,</mo>      <mi>j</mi>     </mrow>    </msub>   </mrow>   <mo>-</mo>   <mrow>    <munder>     <mo>&#x2211;</mo>     <mrow>      <mrow>       <mi>j</mi>       <mo>:</mo>       <mrow>        <mo>(</mo>        <mrow>         <mi>j</mi>         <mo>,</mo>         <mi>i</mi>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mo>&#x2208;</mo>      <mi>&#x2110;</mi>     </mrow>    </munder>    <mrow>     <msub>      <mi>&#x3bb;</mi>      <mrow>       <mi>i</mi>       <mo>,</mo>       <mi>j</mi>      </mrow>     </msub>     <mo>.</mo>    </mrow>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0171" num="0148">At step <b>624</b>, the adjustment model module <b>134</b> will determine whether the training should continue. Typically, training may occur over a small to substantial percentage of the documents in the document database. For example, the adjustment model <b>134</b> may determine that there is a statistically significant sample of document pairs that have been analysed. Although training could be performed over all the documents in the document database doing so may not be desirable as it may result in over-fitting, which may in turn cause in spurious results in new documents.</p><heading id="h-0020" level="2">Neural Network</heading><p id="p-0172" num="0149"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example adjustment model <b>700</b>. The adjustment model in this example is based on a neural network that is trained as described in this disclosure such as according to process <b>600</b>. In this example, a feature vector <b>702</b> in input into an input layer <b>704</b>, which is a layer with the same number of nodes as there are features. There is a second layer of nodes, hidden layer <b>706</b>, which is fully connected with the input layer. This is followed by a sigmoid layer, which is connected to the hidden layer.</p><p id="p-0173" num="0150">The sigmoid layer produces the document score adjustment of a document i based on the corresponding input feature vector for document i. A sigmoid layer is a layer with a non-linear function (such as</p><p id="p-0174" num="0000"><maths id="MATH-US-00021" num="00021"><math overflow="scroll"> <mrow>  <mfrac>   <mn>1</mn>   <mrow>    <mn>1</mn>    <mo>+</mo>    <msup>     <mi>e</mi>     <mrow>      <mo>-</mo>      <mi>x</mi>     </mrow>    </msup>   </mrow>  </mfrac>  <mo>)</mo> </mrow></math></maths></p><p id="p-0175" num="0000">that produces an output between two defined values (in this example, 0 and 1) but also has a smooth gradient. In this case, the output can be scaled and capped to a range of 1 to 1.5 for the purposes of a multiplicative score adjustment. As above, the multiplicative score adjustment is a multiplier that is applied to the initial document score, so this can significantly increase the score of a document with a score adjustment of 1.5, but leave documents with a score adjustment of 1 unchanged.</p><p id="p-0176" num="0151">Notably once the adjustment model <b>118</b> has been trained, the score adjustment function such as <b>700</b> can be used to score or index all the documents in the document database based on a base score generated by a search engine. Each document therefore is associated with a score adjustment (that is, for example, a multiplier value) from the score adjustment function. This means the documents can be evaluated offline (i.e. without requiring a network connection to a user). A new document can be added to the document database and evaluated to determine the corresponding score adjustment value. In some embodiments, it is possible to select from the variants described above as to how the score adjustment will be determined.</p><heading id="h-0021" level="1">Training an Adjustment Model <b>118</b> (Embodiment 2)</heading><p id="p-0177" num="0152">This section describes a second embodiment of a computer implemented method for training an adjustment model. The process <b>800</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>. In this embodiment, the adjustment model <b>118</b> is a deep learning neural network.</p><p id="p-0178" num="0153">The process <b>800</b>, which is performed by the adjustment model module <b>134</b>, similarly to process <b>600</b> starts whenever the document score adjustment function needs to be initialised. Unlike process <b>600</b>, all of the features of a document can be concatenated into a single vector which is then input into the deep learning neural network. This is because the adjustment model <b>118</b>, being a deep learning neural network, will determine which of the features are relevant for the purpose of determining a score adjustment.</p><p id="p-0179" num="0154">In order to train the neural network, as a first step, the adjustment model module <b>134</b> determines <b>802</b> a brand embedding, where the brand is the contributor brand of a document. As a second step, the adjustment model module <b>134</b> determines <b>804</b> a quality embedding, where quality is a score of the quality which may be determined by a document reviewer or may be automatically determined based on attributes of the document (such as image size and resolution). For example a score of the quality of an image may range from 1 to 4. The adjustment model module <b>134</b> then determines <b>806</b> an embedding for other features. Other features include the features mentioned above including</p><p id="p-0180" num="0155">The adjustment model module <b>134</b> will then concatenate these three embeddings to determine a dense feature vector which is an input to the first layer, an embedding layer, of the adjustment model <b>118</b>. For illustration, the following example describes an embedding layer used for text processing. Assuming an input contains two sentences:</p><p id="p-0181" num="0156">Hope to see you soon</p><p id="p-0182" num="0157">Nice meeting you</p><p id="p-0183" num="0158">These can be encoded as dense vectors [1, 2, 3, 4, 5] and [6, 7, 4, 0, 0], where each word is assigned a unique integer. In this example, each input has a size limit of five, so the second dense vector is padded out with zeroes. The embedding layer is a fixed size (for example 3 nodes).</p><p id="p-0184" num="0159">The structure of the neural network is illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0185" num="0160">In this example, the neural network of the adjustment model <b>118</b> is a Siamese neural network as described above. This means there are two subcomponents to the neural network: subnetwork A and subnetwork B. Each subnetwork reflects the structure as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, however, one aspect to note is that the subnetworks, being a Siamese neural network, share weights.</p><p id="p-0186" num="0161">At <b>808</b>, a first dense feature vector, that is the dense feature vector corresponding to a first document i, is input into subnetwork A. The second dense vector, that is the dense vector corresponding to a second document j, is input into subnetwork B.</p><p id="p-0187" num="0162">The adjustment model module <b>134</b> then estimates <b>810</b> the score adjustment of document i from the output of subnetwork A based on the dense feature vector of document i. Similarly, the adjustment model module <b>134</b> estimates <b>812</b> the score adjustment of document j from the output of subnetwork B based on the dense feature vector of document j.</p><p id="p-0188" num="0163">The adjustment model module <b>134</b> then determines <b>814</b> the attractiveness of document i from the document attractiveness model <b>126</b> described above. Similarly, the adjustment model module <b>134</b> then determines <b>816</b> the attractiveness of document j from the document attractiveness model <b>126</b> described above. Steps <b>814</b> and <b>816</b> may simply involve determining the entries for document pair i, j in the pairwise comparisons dataset stored in the pairwise comparisons database <b>116</b> where the pairwise comparison dataset has previously been determined.</p><p id="p-0189" num="0164">Based on this, the adjustment model module <b>134</b> can determine <b>820</b> a target probability that document i is more attractive than document j.</p><p id="p-0190" num="0165">The adjustment model module <b>134</b> then determines <b>820</b> the loss based on the loss function, which in this example is a cross-entropy loss, taking into account the target probability determined earlier.</p><p id="p-0191" num="0166">The weights of subnetwork A and B are then updated <b>822</b> based on the cross-entropy loss (as described above).</p><p id="p-0192" num="0167">At step <b>824</b>, the adjustment model module <b>134</b> will determine whether the training should continue. Typically, training may occur over a small to substantial percentage of the documents in the document database. However, although possible, training may not occur over all the documents in the document database to prevent over-fitting, which may cause in spurious results in new documents.</p><p id="p-0193" num="0168">The adjustment model module <b>134</b> may train over a single pair of documents, or a group of documents. This is referred to as a minibatch. As above, a minibatch may improve the speed at which the model can be trained.</p><heading id="h-0022" level="2">Deep Learning Neural Network</heading><p id="p-0194" num="0169"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example deep learning neural network. In this disclosure the deep learning neural network is a Siamese neural network so the illustration and description below applies to one subnetwork of the neural network. The other subnetwork is structured the same and therefore operates in the same way.</p><p id="p-0195" num="0170">The features brand embedding, quality embedding <b>904</b> and other features <b>906</b>, in this example represent an embedded layer and are the features that are determined for the documents in the document database. These features are typically determined for all the documents in the document database. Example other features are those described above, including quality of an image and whether a document (typically an image in the examples described in this disclosure) is free or paid.</p><p id="p-0196" num="0171">As above, the features are input into a concatenator <b>908</b>. This concatenator <b>908</b> generates a vector that represents all of the features are input into it.</p><p id="p-0197" num="0172">The vector is then input into a first fully connected layer <b>910</b>. This first fully connected layer <b>910</b> is much like the hidden layer <b>706</b> in the example neural network in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. Each node in the fully connected layer produces a first intermediate value, each of which is input into an activation function called a rectifier linear unit <b>912</b> (often referred to as ReLU). It is possible that other activation functions (such as linear, sigmoid and hyperbolic tangent) can be used each of which has specific advantages and disadvantages but in this example the ReLU is used.</p><p id="p-0198" num="0173">The ReLU <b>912</b> is a component that activates nodes according to an activation function that returns 0 if it receives any negative input, but for any positive input it returns that input. That is, the ReLU activation function can be considered to be the function: f(x)=max(0,x). In this sense a node in the first fully connected layer <b>910</b> that produces a first intermediate value that is negative will be, in effect, deactivated in that the value passed on to the remaining layers of the neural network is 0. Similarly, a node in the first fully connected layer <b>910</b> that produces a first intermediate value that is positive will be activated in that the first intermediate value will be passed on to the remaining layers. The use of the ReLU <b>912</b> helps the model deal with non-linear effects. That is, the effect of increasing a variable by one is different at different values of that variable.</p><p id="p-0199" num="0174">The output from the ReLU activated nodes is input into a second fully connected layer <b>914</b>. Again, this is much like the previous fully connected layer <b>910</b> (and similarly the hidden layer in the example neural network in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). Each node in the second fully connected layer produces a second intermediate value, each of which is input into an activation function called a sigmoid <b>916</b>. In a sigmoid activation function the input to the function is transformed into a value between 0.0 and 1.0. Inputs that are much larger than 1.0 are capped at the upper limit of 1.0, similarly, values below 0.0 are capped at the lower limit of 0.0. In this example the sigmoid layer <b>916</b> can be scaled to the range 1.0 to 1.5 by a scaling layer (not illustrated).</p><p id="p-0200" num="0175">The end result output from the adjustment model <b>118</b> is a document score adjustment <b>918</b>. As per above, during training of the adjustment model <b>118</b>, this document score adjustment is input into a loss function (in this disclosure a cross-entropy loss function is used) in order to determine how to adjust the weights in the deep learning neural network.</p><p id="p-0201" num="0176">Once the training of the adjustment model <b>118</b> is complete, it is possible to determine a score adjustment of a document by inputting the features of the document into the adjustment model <b>118</b>.</p><heading id="h-0023" level="2">Dampener Parameters</heading><p id="p-0202" num="0177">As a result of utilising the user interaction database <b>114</b>, some documents may be rated as more attractive than they should be or would be expected to be. That is, free user browsing may produce user interactions that are not aligned with specific objectives (which may be for example to encourage use of specific documents such as documents with higher quality ratings). As a result, a dampener parameter can be used. This dampener parameter acts to reduce the overall contribution of a result to the weight adjustment in the network. This dampener parameter can be applied in both embodiments described above.</p><p id="p-0203" num="0178">An example of where a dampener parameter can be used is with free and paid images. This is because the majority of users would not be subscribers to paid content and therefore would click on free images significantly more often. In this case the dampener parameter is used to effectively penalise free content to account for the fact that a majority of users would not utilise paid content and therefore a document that is, or includes, paid content would likely be determined to be less relevant in a pairwise comparison.</p><p id="p-0204" num="0179">This is done by changing the target probability if document i is free and document j is not. A dampener parameter k is defined, which can be multiplied with the target probability, to dampen the proportion of free to paid images. There are different values of k (for example, 0.1 to 0.5) that can be used to balance results between free and paid documents.</p><p id="p-0205" num="0180">This accounts for the fact that free images tend to dominate paid images in attractiveness; most users prefer free images not because they are truly attractive compared to a paid image, but more so because they cost nothing. Dampener parameters can be determined for other data biases or other similar discrepancies in attractiveness. The tables below illustrates a document score adjustment for free images (table 1) and paid images (table 2) where the dampener parameter is 0.5. Note that the paid images have lower document score adjustment values on average as their base scores are higher.</p><p id="p-0206" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 1</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Free</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="28pt" align="left"/><colspec colname="1" colwidth="98pt" align="center"/><colspec colname="2" colwidth="91pt" align="center"/><tbody valign="top"><row><entry/><entry>Document score adjustment value</entry><entry>Count</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="28pt" align="left"/><colspec colname="1" colwidth="98pt" align="center"/><colspec colname="2" colwidth="91pt" align="char" char="."/><tbody valign="top"><row><entry/><entry>1.0001863241195679</entry><entry>230</entry></row><row><entry/><entry>1.0005956888198853</entry><entry>33</entry></row><row><entry/><entry>1.0023924112319946</entry><entry>121</entry></row><row><entry/><entry>1.0075739622116089</entry><entry>4</entry></row><row><entry/><entry>1.0291855335235596</entry><entry>112</entry></row><row><entry/><entry>1.0827466249465942</entry><entry>8</entry></row><row><entry/><entry>1.2221081256866455</entry><entry>46</entry></row><row><entry/><entry>1.2667508125305176</entry><entry>551</entry></row><row><entry/><entry>1.3594299554824830</entry><entry>11</entry></row><row><entry/><entry>1.4557728767395020</entry><entry>4221</entry></row><row><entry/><entry>1.4682446718215942</entry><entry>905</entry></row><row><entry/><entry>1.4852802753448486</entry><entry>5631</entry></row><row><entry/><entry>1.4910130500793457</entry><entry>4</entry></row><row><entry/><entry>1.4973838329315186</entry><entry>1066</entry></row><row><entry/><entry>1.4986631870269775</entry><entry>605</entry></row><row><entry/><entry>1.4997961521148682</entry><entry>56765</entry></row><row><entry/><entry>1.4998960494995117</entry><entry>1</entry></row><row><entry/><entry>1.4999842643737793</entry><entry>38638</entry></row><row><entry/><entry>1.4999949932098389</entry><entry>33</entry></row><row><entry/><entry>1.4999970197677612</entry><entry>53</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0207" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 2</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Paid</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="35pt" align="left"/><colspec colname="1" colwidth="98pt" align="center"/><colspec colname="2" colwidth="84pt" align="center"/><tbody valign="top"><row><entry/><entry>Document score adjustment value</entry><entry>Count</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="35pt" align="left"/><colspec colname="1" colwidth="98pt" align="center"/><colspec colname="2" colwidth="84pt" align="char" char="."/><tbody valign="top"><row><entry/><entry>1.0000001192092896</entry><entry>32</entry></row><row><entry/><entry>1.0000002384185790</entry><entry>226</entry></row><row><entry/><entry>1.0000008344650269</entry><entry>31</entry></row><row><entry/><entry>1.0000013113021850</entry><entry>3</entry></row><row><entry/><entry>1.0000025033950806</entry><entry>83</entry></row><row><entry/><entry>1.0000101327896118</entry><entry>29</entry></row><row><entry/><entry>1.0000283718109130</entry><entry>86</entry></row><row><entry/><entry>1.0000323057174683</entry><entry>28</entry></row><row><entry/><entry>1.0001302957534790</entry><entry>62242</entry></row><row><entry/><entry>1.0001863241195679</entry><entry>6098</entry></row><row><entry/><entry>1.0002158880233765</entry><entry>50</entry></row><row><entry/><entry>1.0003656148910522</entry><entry>191</entry></row><row><entry/><entry>1.0004165172576904</entry><entry>303241</entry></row><row><entry/><entry>1.0006898641586304</entry><entry>214</entry></row><row><entry/><entry>1.0016744136810303</entry><entry>374648</entry></row><row><entry/><entry>1.0023924112319946</entry><entry>31484</entry></row><row><entry/><entry>1.0027688741683960</entry><entry>2429</entry></row><row><entry/><entry>1.0046726465225220</entry><entry>17401</entry></row><row><entry/><entry>1.0053175687789917</entry><entry>910976</entry></row><row><entry/><entry>1.0087512731552124</entry><entry>8406</entry></row><row><entry/><entry>1.0291855335235596</entry><entry>372231</entry></row><row><entry/><entry>1.0542206764221191</entry><entry>48870</entry></row><row><entry/><entry>1.2221081256866455</entry><entry>2398044</entry></row><row><entry/><entry>1.3053148984909058</entry><entry>1216</entry></row><row><entry/><entry>1.4557728767395020</entry><entry>3645724</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0208" num="0181">The flowcharts illustrated in the figures and described above define operations in particular orders to explain various features. In some cases the operations described and illustrated may be able to be performed in a different order to that shown/described, one or more operations may be combined into a single operation, a single operation may be divided into multiple separate operations, and/or the function(s) achieved by one or more of the described/illustrated operations may be achieved by one or more alternative operations</p><p id="p-0209" num="0182">In the foregoing specification, embodiments of the invention have been described with reference to numerous specific details that may vary from implementation to implementation. Thus, the sole and exclusive indicator of what is the invention, and is intended by the applicants to be the invention, is the set of claims that issue from this application, in the specific form in which such claims issue, including any subsequent correction. Any definitions expressly set forth herein for terms contained in such claims shall govern the meaning of such terms as used in the claims. Hence, no limitation, element, property, feature, advantage or attribute that is not expressly recited in a claim should limit the scope of such claim in any way. The specification and drawings are, accordingly, to be regarded in an illustrative rather than a restrictive sense</p><p id="p-0210" num="0183">As used herein the terms &#x201c;include&#x201d; and &#x201c;comprise&#x201d; (and variations of those terms, such as &#x201c;including&#x201d;, &#x201c;includes&#x201d;, &#x201c;comprising&#x201d;, &#x201c;comprises&#x201d;, &#x201c;comprised&#x201d; and the like) are intended to be inclusive and are not intended to exclude further features, components, integers or steps.</p><p id="p-0211" num="0184">Various features of the disclosure have been described using flowcharts. Although these flowcharts define steps in particular orders to explain various features, in some cases the steps may be able to be performed in a different order. Furthermore, in some cases one or more steps may be combined into a single step, a single step may be divided into multiple separate steps, and/or the function(s) achieved by one or more of the described/illustrated steps may be achieved by one or more alternative steps. Still further, the functionality/processing of a given flowchart step could potentially be performed by various different systems or applications.</p><p id="p-0212" num="0185">It will be understood that the invention disclosed and defined in this specification extends to all alternative combinations of two or more of the individual features mentioned or evident from the text or drawings. All of these different combinations constitute various alternative aspects of the invention.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230004570A1-20230105-M00001.NB"><img id="EMI-M00001" he="10.92mm" wi="76.20mm" file="US20230004570A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230004570A1-20230105-M00002.NB"><img id="EMI-M00002" he="5.25mm" wi="76.20mm" file="US20230004570A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230004570A1-20230105-M00003.NB"><img id="EMI-M00003" he="6.01mm" wi="76.20mm" file="US20230004570A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004 MATH-US-00004-2" nb-file="US20230004570A1-20230105-M00004.NB"><img id="EMI-M00004" he="14.48mm" wi="76.20mm" file="US20230004570A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230004570A1-20230105-M00005.NB"><img id="EMI-M00005" he="6.69mm" wi="76.20mm" file="US20230004570A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006" nb-file="US20230004570A1-20230105-M00006.NB"><img id="EMI-M00006" he="6.69mm" wi="76.20mm" file="US20230004570A1-20230105-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00007" nb-file="US20230004570A1-20230105-M00007.NB"><img id="EMI-M00007" he="7.37mm" wi="76.20mm" file="US20230004570A1-20230105-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00008" nb-file="US20230004570A1-20230105-M00008.NB"><img id="EMI-M00008" he="7.79mm" wi="76.20mm" file="US20230004570A1-20230105-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00009" nb-file="US20230004570A1-20230105-M00009.NB"><img id="EMI-M00009" he="11.60mm" wi="76.20mm" file="US20230004570A1-20230105-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00010" nb-file="US20230004570A1-20230105-M00010.NB"><img id="EMI-M00010" he="7.03mm" wi="76.20mm" file="US20230004570A1-20230105-M00010.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00011 MATH-US-00011-2 MATH-US-00011-3" nb-file="US20230004570A1-20230105-M00011.NB"><img id="EMI-M00011" he="15.83mm" wi="76.20mm" file="US20230004570A1-20230105-M00011.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00012" nb-file="US20230004570A1-20230105-M00012.NB"><img id="EMI-M00012" he="6.35mm" wi="76.20mm" file="US20230004570A1-20230105-M00012.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00013" nb-file="US20230004570A1-20230105-M00013.NB"><img id="EMI-M00013" he="6.35mm" wi="76.20mm" file="US20230004570A1-20230105-M00013.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00014 MATH-US-00014-2 MATH-US-00014-3" nb-file="US20230004570A1-20230105-M00014.NB"><img id="EMI-M00014" he="15.49mm" wi="76.20mm" file="US20230004570A1-20230105-M00014.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00015" nb-file="US20230004570A1-20230105-M00015.NB"><img id="EMI-M00015" he="8.13mm" wi="76.20mm" file="US20230004570A1-20230105-M00015.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00016 MATH-US-00016-2 MATH-US-00016-3" nb-file="US20230004570A1-20230105-M00016.NB"><img id="EMI-M00016" he="19.05mm" wi="76.20mm" file="US20230004570A1-20230105-M00016.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00017" nb-file="US20230004570A1-20230105-M00017.NB"><img id="EMI-M00017" he="6.35mm" wi="76.20mm" file="US20230004570A1-20230105-M00017.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00018 MATH-US-00018-2 MATH-US-00018-3" nb-file="US20230004570A1-20230105-M00018.NB"><img id="EMI-M00018" he="18.37mm" wi="76.20mm" file="US20230004570A1-20230105-M00018.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00019" nb-file="US20230004570A1-20230105-M00019.NB"><img id="EMI-M00019" he="6.35mm" wi="76.20mm" file="US20230004570A1-20230105-M00019.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00020 MATH-US-00020-2 MATH-US-00020-3" nb-file="US20230004570A1-20230105-M00020.NB"><img id="EMI-M00020" he="19.73mm" wi="76.20mm" file="US20230004570A1-20230105-M00020.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00021" nb-file="US20230004570A1-20230105-M00021.NB"><img id="EMI-M00021" he="6.35mm" wi="76.20mm" file="US20230004570A1-20230105-M00021.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method for determining a score adjustment for a search document, comprising:<claim-text>determining a first attractiveness model of a first document from one or more documents based on one or more user interactions associated with the first document;</claim-text><claim-text>determining a second attractiveness model of a second document from one or more documents based on one or more user interactions associated with the second document;</claim-text><claim-text>determining one or more pairwise comparisons of documents based on the first and second attractiveness models of the first and second documents;</claim-text><claim-text>training an adjustment model based on the pairwise comparisons of documents; and</claim-text><claim-text>inputting the search document into the adjustment model to determine the score adjustment.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the adjustment model is a neural network.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computer-implemented method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the neural network is a symmetrical neural network.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the one or more pairwise comparisons comprises determining a probability of a hypothesis of a comparison between an attractiveness of the first document based on the first attractiveness model and an attractiveness of the second document based on the second attractiveness model.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. (canceled)</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. (canceled)</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computer-implemented method of <claim-ref idref="CLM-00003">claim 3</claim-ref> wherein the symmetrical neural network is a Siamese neural network comprising a first subnetwork and a second subnetwork and wherein each of the first subnetwork and second subnetwork share weights.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The computer-implemented method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein training the neural network comprises adjusting the weights of the neural network using a cross-entropy loss function.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computer-implemented method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein training the Siamese neural network comprises inputting a first document into the first subnetwork and inputting a second document into the second network to calculate the cross-entropy loss based on the cross-entropy loss function.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. (canceled)</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. (canceled)</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first attractiveness model is based on clicks of the first document in a search result produced by a query.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second attractiveness model is based on clicks of the second document in a search result produced by a query.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the attractiveness model is based on a beta random variable.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The computer-implemented method of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the attractiveness model is based on the Beta-Bernoulli model.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. (canceled)</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the attractiveness model is based on a Gaussian random variable.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. (canceled)</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein inputting the search document into the adjustment model comprises determining one of more document features of the search document.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computer-implemented method of <claim-ref idref="CLM-00019">claim 19</claim-ref> wherein the search document is an image and wherein the document features comprise one or more of:<claim-text>whether the image is a cutout;</claim-text><claim-text>whether an object in the image is a single, isolated object;</claim-text><claim-text>whether the image supports being placed in a photoholder;</claim-text><claim-text>whether the image can be recoloured;</claim-text><claim-text>whether the image should be rescaled or repeated if enlarged;</claim-text><claim-text>whether the image is free or paid;</claim-text><claim-text>whether the image is a raster image or a vector image; and</claim-text><claim-text>a measurement of the quality of the image.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the adjustment model is a deep learning neural network.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The computer-implemented method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the adjustment model comprises a first fully connected layer and a second fully connected layer.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The computer-implemented method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the adjustment model comprises a rectifier linear unit activation function between the first fully connected layer and the second fully connected layer.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The computer-implemented method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the adjustment model comprises a sigmoid layer connected to the second fully connected layer.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the adjustment model comprises a dampening parameter.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. (canceled)</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. A computer processing system comprising:<claim-text>a processing unit;</claim-text><claim-text>a communication interface;</claim-text><claim-text>a non-transient storage medium readable by a processor, the storage medium storing instructions executable by one or more processors to cause the one or more processors to:</claim-text><claim-text>determine a first attractiveness model of a first document from one or more documents based on one or more user interactions associated with the first document;</claim-text><claim-text>determine a second attractiveness model of a second document from one or more documents based on one or more user interactions associated with the second document;</claim-text><claim-text>determine one or more pairwise comparisons of documents based on the first and second attractiveness models of the first and second documents;</claim-text><claim-text>train an adjustment model based on the pairwise comparisons of documents; and</claim-text><claim-text>input a search document into the adjustment model to determine a score adjustment for the search document.</claim-text></claim-text></claim></claims></us-patent-application>