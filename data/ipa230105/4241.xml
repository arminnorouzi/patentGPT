<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004242A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004242</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17756593</doc-number><date>20210528</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>AU</country><doc-number>2020901767</doc-number><date>20200529</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>042</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>042</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>013</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>74</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>012</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>016</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2203</main-group><subgroup>04108</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10012</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30201</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">A CONTACTLESS TOUCHSCREEN INTERFACE</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only"><addressbook><last-name>VAN DER MERWE</last-name><first-name>Marthinus</first-name><address><city>Mullaloo</city><country>AU</country></address></addressbook><residence><country>AU</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>VAN DER MERWE</last-name><first-name>Marthinus</first-name><address><city>Mullaloo</city><country>AU</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/AU2021/050522</doc-number><date>20210528</date></document-id><us-371c12-date><date>20220527</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A contactless touchscreen interface has a digital display to display digital information; the proximity detector and a proximity detector comprising an image sensor to detect user interaction at a virtual touch intersection plane offset a distance from the digital display and to resolve the interaction into XY offset-plane interaction coordinates with reference to the digital display. A gaze determining imaging system comprising an image sensor determines a gaze relative offset with respect to the digital display using facial image data captured by the image sensor. An interface controller comprising a parallax adjustment controller to convert the XY offset-plane interaction coordinates to XY on-screen apparent coordinates using the gaze relative offset and the distance and an input controller generates an input at the XY on-screen apparent coordinates accordingly.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="151.38mm" wi="151.98mm" file="US20230004242A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="169.42mm" wi="154.01mm" file="US20230004242A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="184.40mm" wi="118.62mm" orientation="landscape" file="US20230004242A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="177.80mm" wi="123.61mm" file="US20230004242A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="200.49mm" wi="124.63mm" orientation="landscape" file="US20230004242A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD OF THE INVENTION</heading><p id="p-0002" num="0001">This invention relates generally to touchscreen displays. More particularly, this invention relates to a contactless touchscreen interface used to provide touchscreen interaction without physical contact with a digital display thereof.</p><heading id="h-0002" level="1">SUMMARY OF THE DISCLOSURE</heading><p id="p-0003" num="0002">There is provided herein a contactless touchscreen interface comprising a digital display to display digital information. A proximity detector comprising an image sensor detects user interaction at a virtual touch intersection plane offset a distance from the digital display and to resolve the interaction into XY offset-plane interaction coordinates with reference to the digital display.</p><p id="p-0004" num="0003">A gaze determining imaging system comprising an image sensor determines a gaze relative offset with respect to the digital display using facial image data captured by the image sensor.</p><p id="p-0005" num="0004">An interface controller uses a parallax adjustment controller to convert the XY offset-plane interaction coordinates to XY on-screen apparent coordinates using the gaze relative offset and the distance and an input controller generates an input at the XY on-screen apparent coordinates.</p><p id="p-0006" num="0005">As such, the present system may provide a touchless screen interface for a variety of applications, such as ATMs, medical equipment and or the like.</p><p id="p-0007" num="0006">The proximity detector may comprise a plurality of image sensors located around the digital display. These image sensors may be arranged to provide coverage across the entire surface area of the digital display within tight bezel confines despite limited field of view thereof wherein image sensors are located at opposite sides of the digital display and each sensor captures image data from an opposite region of the digital display.</p><p id="p-0008" num="0007">According to a preferred arrangement, the gaze determining imaging system comprises a single image sensor at a top of the bezel, thereby providing an unobstructed view to capture the facial image data and the proximity detector comprises a pair of image sensors close in at either side of the digital display achieving a field of view across the entire surface area thereof despite the space confines of the bezel.</p><p id="p-0009" num="0008">The gaze determining image subsystem may employ facial recognition to determine at least one of a facial centroid and eye location which requires less processing power as would be required for determining eye orientation whilst yet providing reasonably accurate parallax adjustment.</p><p id="p-0010" num="0009">The interface may further comprise a feedback interface comprising a plurality of ultrasonic transducers which emit ultrasound which induces mid-air tactile feedback. The ultrasound transducers may be located around an edge of the digital display and may emit ultrasound in a frequency range of between 20 kHz-60 kHz, preferably approximately 40 kHz. The ultrasound transducers may be orientated in towards the screen and may be recessed beneath an upper surface plane of the screen for flush mount application or alternatively extend above the upper surface plane for enhanced ultrasound transmission.</p><p id="p-0011" num="0010">The ultrasound transducers may be located at opposite sides of the screen so that ultrasound generated thereby coincides substantially in time or correlates in phase from opposite directions at the fingertip. In embodiments, the timing of pulses or phase of the ultrasound transducers at opposite ends of the screen may be controlled according to the XY offset-plane interaction coordinates taking into account the speed of sound so that a focal point coincides with a user's fingertip.</p><p id="p-0012" num="0011">In further embodiments, the feedback interface may emit different ultrasound frequencies for which the phase is controlled to create a standing wave coinciding at the XY offset-plane interaction coordinates.</p><p id="p-0013" num="0012">The interface may comprise an interaction depth indicator indicating to a user whether the user is interacting with the intersection plane wherein the depth indicator may indicate to the user that the user is interacting too close if the proximity detector detects continuous intersection of the intersection plane and indicate to the user that the user is interacting at the appropriate depth if the proximity detector detects intermittent interaction with the virtual touch intersection plane, the depth indicator may indicate to the user that the user is interacting at the appropriate depth.</p><p id="p-0014" num="0013">Other aspects of the invention are also disclosed.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0015" num="0014">Notwithstanding any other forms which may fall within the scope of the present invention, preferred embodiments of the disclosure will now be described, by way of example only, with reference to the accompanying drawings in which:</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a contactless touchscreen interface in accordance with an embodiment;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a side elevation view of relative positions of the interface in accordance with an embodiment;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a front elevation view of the relative positions of the interface;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a side elevation view of a digital display of the interface of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in accordance with an embodiment;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a top plan view of the digital display of <figref idref="DRAWINGS">FIG. <b>4</b></figref>; and</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a side elevation view of a digital display of the interface of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in accordance with a further embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0022" num="0021">A contactless touchscreen interface <b>100</b> may comprise an application computer <b>110</b> displaying information in a user interface <b>118</b> on a digital display <b>119</b>. The digital display <b>118</b> may interface with the application computer <b>110</b> via HDMI or similar video interface. The application computer <b>111</b> may be for various applications, including ATMs, medical equipment, point-of-sale systems, airline check-in interfaces and the like.</p><p id="p-0023" num="0022">The interface <b>100</b> may comprise an interface controller <b>101</b> which interfaces with the proximity detector <b>115</b> overlaid the digital display <b>119</b>.</p><p id="p-0024" num="0023">The interface controller <b>101</b> comprises a processor <b>110</b> for processing digital data. In operable communication with the processor <b>110</b> across a system bus <b>109</b> is a memory device <b>108</b>. The memory device <b>108</b> is configured for storing digital data including computer program code instructions which may be logically divided into various computer program code controllers <b>107</b> and associated data <b>103</b>. In use, the processor <b>110</b> fetches these computer program code instructions from the memory device <b>108</b> for interpretation and execution for the implementation of the functionality described herein.</p><p id="p-0025" num="0024">The controllers <b>107</b> may comprise an image processing controller <b>106</b>, parallax adjustment controller <b>105</b> and an input device (HRD) controller <b>104</b>.</p><p id="p-0026" num="0025">The interface controller <b>101</b> may comprise an I/O interface <b>124</b> for interfacing with the proximity detector <b>115</b>.</p><p id="p-0027" num="0026">With reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the proximity detector <b>115</b> detects user interaction at a virtual touch intersection plane <b>122</b>. The virtual touch intersection plane <b>122</b> is set at a distance d from the digital display <b>119</b>. In embodiments, the distance d may be controlled according to an offset setting <b>102</b> within the data <b>103</b> of the interface controller <b>101</b>.</p><p id="p-0028" num="0027">In one form, the proximity detector <b>115</b> may take the form of a screen bezel which has light beam interrupt sensors casting an orthogonal arrangement of horizontal and vertical parallel light beams thereacross. The light beams may be infrared. Interruption of an orthogonal pair of light beams is detected by the sensors to determine XY offset-plane interaction coordinates <b>120</b> at the XY virtual touch intersection plane <b>122</b>. If more than two parallel light beams are interrupted, the proximity detector <b>115</b> may determine the centre thereof.</p><p id="p-0029" num="0028">In another form, the proximity detector <b>115</b> takes the form of a transparent capacitive sensitive overlay configured to detect capacitive coupling of a user's hand or finger when near the capacitive sensitive overlay. The capacitive sensitive overlay may comprise a matrix of transparent conductive plates, each of which acts as a capacitive plate to detect capacitive coupling with a user's hand or finger. The XY offset-plane interaction coordinates <b>120</b> may be determined by a region of capacitive plates having greatest capacitive coupling. The capacitive plates may be coupled to an operational amplifier wherein the gain thereof may be used to virtually adjust the distance d from the digital display <b>119</b> to the virtual touch intersection plane <b>122</b>.</p><p id="p-0030" num="0029">In further embodiments the proximity detector <b>115</b> may detect proximity using visual sensing using at least one an image sensor <b>116</b>. The image sensor <b>116</b> may capture visible image data of the user's hand in relation to the digital display <b>119</b> to determine the relative positioning thereof.</p><p id="p-0031" num="0030">In embodiments, the proximity detector <b>115</b> is configured to determine a plurality of relative spatial points (point cloud) lying on contours or extremities of a user's hand or finger to determine the XY offset plane interaction coordinates <b>120</b>.</p><p id="p-0032" num="0031">For example, the proximity detector <b>115</b> may use the image processing controller <b>106</b> to map the visual boundaries of the user's hand or finger to determine the plurality of relative spatial points. The most extreme point may be determined therefrom indicative of the position of the user's fingertip.</p><p id="p-0033" num="0032">In embodiments, the image sensor <b>116</b> may be a stereoscopic image sensor <b>116</b> and wherein the plurality of relative spatial points are determined from differential comparison from stereoscopic image data obtained from the stereoscopic image sensor <b>116</b>. In accordance with this embodiment, the plurality of relative spatial points may further map the contours of the hand. Using a stereoscopic image sensor <b>116</b> may allow the utilisation of a single image sensor <b>116</b> to determine the relative position of the user's hand or finger.</p><p id="p-0034" num="0033">As shown in <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>5</b></figref>, the image sensor <b>116</b> of the proximity detector <b>115</b> may be located right at an edge <b>130</b> of the digital display <b>119</b>. In embodiments, the digital display <b>119</b> may be surrounded by a screen bezel and wherein the image sensor <b>116</b> is located on or within the bezel.</p><p id="p-0035" num="0034">As is further illustrated in <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>5</b></figref>, so as to be able to obtain a field of view across the entire surface of the digital display <b>119</b>, the proximity detector <b>115</b> may comprise a plurality of image sensors <b>116</b> around the digital display <b>119</b>.</p><p id="p-0036" num="0035">As is further shown in <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>5</b></figref>, to account for limited field of view of the image sensors <b>116</b> within the confines of the bezel, the image sensors <b>116</b> may capture image data from opposite regions <b>129</b>. For example, the plurality of image sensors <b>116</b> may comprise a first image sensor <b>116</b>A operable to detect interactions at a first region <b>129</b>A opposite the first image sensor <b>116</b>A. Furthermore, a second image sensor <b>116</b>B opposite the first image sensor <b>116</b>A is operable to detect interactions at a second region <b>129</b>B of the digital display <b>119</b> opposite the second image sensor <b>116</b>B.</p><p id="p-0037" num="0036">The interface <b>100</b> further comprises a gaze determining imaging system to determine a gaze relative offset with respect to the digital display <b>119</b>. In the embodiment shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the gaze determining imaging system comprises an image sensor <b>116</b> which captures facial image data of a user's face in front of the digital display <b>119</b> and wherein the image processing controller <b>106</b> determines the gaze relative offset using the facial image data.</p><p id="p-0038" num="0037">The image processing controller <b>106</b> may use facial detection to detect a position of a face within the field of view of the image sensor <b>116</b>. The relative gaze offset may be calculated in accordance with a centroid of a facial area detected by the image processing controller <b>106</b> or, in further embodiments, the image processing controller <b>106</b> may further recognise locations of eyes with an official region. The determination of a facial area centroid or locations of the eyes may require less processing power as compared to detecting the actual orientation of the eyes whilst yet providing a relatively accurate parallax adjustment.</p><p id="p-0039" num="0038">The parallax adjustment controller <b>105</b> is configured to convert the offset-plane interaction coordinates <b>120</b> Xi and Yi to on-screen apparent coordinates <b>123</b> Xa and Ya.</p><p id="p-0040" num="0039">In embodiments, the gaze determining image system may comprise an image sensor <b>116</b> at a top of the bezel whereas the proximity detector <b>115</b> may comprise a pair of image sensors <b>116</b> either side of the bezel. The superior location of the gaze determining image system image sensor <b>116</b> allows unobstructed view of the user's face to determining the gaze relative offset whereas the side-by-side location of the image sensors <b>116</b> of the proximity detector <b>115</b> allows for comprehensive coverage of the surface of the display <b>119</b> within the confines of a tight bezel therearound.</p><p id="p-0041" num="0040">In other embodiments, the image sensor <b>116</b> may be an infrared image sensor to detect a heat signature of the hand to determine the relative positioning thereof. In embodiments, the infrared image sensor <b>116</b> may locate behind the digital display <b>119</b> to detect infrared through the digital display.</p><p id="p-0042" num="0041">The controllers <b>107</b> may comprise a human input device (HID) controller <b>104</b> which converts the on-screen apparent coordinates <b>123</b> to an HID input via the USB interface <b>112</b> or other HID input of the application computer <b>111</b>. As such, in effect, the HID controller <b>104</b> may emulate a mouse input device from the perspective of the application computer <b>101</b>. As such, the application computer <b>111</b> may display a mouse cursor <b>117</b> or other interaction indication at the calculated on-screen apparent coordinates <b>123</b>.</p><p id="p-0043" num="0042">With reference to <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref>, there is shown the digital display <b>119</b> and the virtual touch intersection plane <b>122</b> offset a distance d from the digital display <b>119</b>.</p><p id="p-0044" num="0043">As alluded to above, the distance d may be physically set by the construction of the proximity detector <b>115</b> (such as the positioning of the light beam interrupts with respect to the digital display <b>119</b>) or alternatively virtually adjusted, such as by adjusting the gain of an operational amplifier interfacing the aforedescribed capacitive touch sensitive overlay according to the offset setting <b>102</b> of the interface controller <b>101</b>.</p><p id="p-0045" num="0044">In embodiments, the interface controller <b>101</b> may dynamically adjust the offset setting <b>102</b>. For example, in one manner, for colder temperatures, the interface controller <b>101</b> may increase the offset setting <b>102</b> to account for when users wear bulkier gloves in cold weather.</p><p id="p-0046" num="0045">In alternative embodiments, the interface controller <b>101</b> may dynamically adjust the offset setting <b>102</b> according to user specific interactions. For example, using the aforedescribed capacitive touch sensor, the interface controller <b>101</b> may detect an offset at which the user prefers to virtually tap the virtual touch intersection plane <b>122</b>, such as by determining peak values of measured capacitive coupling of the capacitive plates.</p><p id="p-0047" num="0046">For example, some users may inherently prefer to tap the virtual touch intersection plane closer to the digital display <b>119</b> as compared to others. As such, in this way, the interface controller <b>101</b> dynamically adjusts the positioning of the virtual touch intersection plane according to the specific user behaviour.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref> show the image sensor <b>116</b> capturing image data of the user's face <b>125</b> wherein the image processing controller <b>106</b> may determine relative angles thereof both in the horizontal and vertical plane. <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates these angles being resolved into gaze relative offset Xg and Yg. The gaze relative offsets may be determined with respect to a reference point of the digital display <b>119</b>, such as a bottom left-hand corner thereof.</p><p id="p-0049" num="0048">Furthermore, <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref> show the interaction point coordinates <b>120</b> Xi and Yi, being the position at which the user's forefinger intersects the virtual touch intersection plane <b>122</b>. Where the digital display <b>119</b> is in a relatively low position, the interaction point coordinates <b>120</b> may be beneath the gaze of the user at the virtual touch intersection plane <b>122</b> given that the trajectory of the user's finger and the gaze are nonparallel but coincident at the on-screen apparent coordinates <b>123</b>. The parallax adjustment controller <b>105</b> may adjust the offset between the interaction point coordinates <b>120</b> and the on-screen apparent coordinates <b>123</b> depending on the angle of the user gaze.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref> furthermore display on-screen apparent coordinates <b>123</b> Xa and Ya calculated by the parallax adjustment controller <b>105</b>.</p><p id="p-0051" num="0050">In embodiments, the interface <b>100</b> comprises a feedback interface <b>114</b> to provide feedback when the users forefinger intersects the virtual touch intersection plane <b>122</b>. The feedback interface <b>114</b> may generate an audible output, such as a beep sound every time the user's forefinger intersects the virtual touch intersection plane <b>122</b>.</p><p id="p-0052" num="0051">Alternatively, the feedback interface <b>114</b> may display a pointer indication <b>117</b> within the user interface <b>118</b> when the user's forefinger intersects the virtual touch intersection plane <b>122</b>.</p><p id="p-0053" num="0052">In further embodiments, the feedback interface <b>114</b> generates haptic feedback.</p><p id="p-0054" num="0053">In embodiment, the feedback interface <b>140</b> may comprise a plurality of ultrasonic transducers <b>128</b> which emit ultrasound which induces mid-air tactile feedback of the user's finger when intersecting the virtual touch intersection plane <b>122</b>.</p><p id="p-0055" num="0054">As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the ultrasonic transducers <b>128</b> may be located at an edge of the digital display <b>119</b>. In this regard, the ultrasonic transducers <b>128</b> may be located at or within a bezel of the digital display <b>119</b>.</p><p id="p-0056" num="0055">Furthermore, the ultrasonic transducers <b>128</b> may be orientated in towards the digital display <b>119</b> to direct ultrasound inwardly.</p><p id="p-0057" num="0056">As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the ultrasonic transducers <b>128</b> may comprise ultrasonic transducers <b>128</b>B which are recessed beneath a surface plane of the digital display <b>119</b>, being advantageous for flush mounted application.</p><p id="p-0058" num="0057">However, for enhanced ultrasound transmission, the ultrasonic transducers <b>128</b> may comprise ultrasonic transducers <b>128</b>A which extend above the surface plane of the digital display <b>119</b>.</p><p id="p-0059" num="0058">The ultrasonic transducers <b>128</b> may emit ultrasound at between 20 kHz-60 kHz, preferably at approximately 40 kHz.</p><p id="p-0060" num="0059">As is further shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the ultrasonic transducers <b>128</b> may be located at opposite edges of the digital display <b>119</b> so that ultrasound admitted thereby coincides from opposite directions.</p><p id="p-0061" num="0060">In embodiments, the feedback interface <b>114</b> may control the timing or phase of the operation of the ultrasonic transducers <b>128</b> so that ultrasound admitted thereby coincides substantially simultaneously at a focal point at the XY offset-plane interaction coordinates <b>120</b>.</p><p id="p-0062" num="0061">Specifically, the ultrasonic transducers may comprise a first set of ultrasonic transducers <b>128</b>A located at a first side of the digital display <b>119</b> and a second set of ultrasonic transducers <b>1288</b> located at an opposite side of the digital display <b>119</b>. The feedback interface <b>114</b> may control the timing of the operation of the transducers <b>128</b>A and <b>128</b>B taking into account the speed of sound so that ultrasonic pressure waves <b>131</b> generated by the transducers <b>128</b> coincide at a focal point at the XY offset-plane interaction coordinates <b>120</b>. For example, with reference to the orientation of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the first set of ultrasonic transducers <b>128</b>A on the left would fire just before the ultrasonic transducers <b>1288</b> on the right so as to coincide simultaneously at the XY offset-plane interaction coordinates <b>120</b>.</p><p id="p-0063" num="0062">Alternatively, the feedback interface <b>114</b> may control the phase of the ultrasound generated by the transducers <b>128</b>A and <b>128</b>B so that their maximum amplitudes coincide at the focal point at the XY offset plane interaction corners <b>120</b>.</p><p id="p-0064" num="0063">In embodiments, the feedback interface <b>114</b> controls the frequency of ultrasound generated by the ultrasonic transducers <b>128</b> to create a standing wave at the XY offset-plane interaction coordinates <b>120</b>. For example, the feedback interface <b>114</b> may generate a 40 kHz signal and a 60 kHz signal which coincide to generate a standing wave at the XY offset-plane interaction coordinates.</p><p id="p-0065" num="0064">In embodiments, the feedback interface <b>114</b> may provide continuous feedback whilst the user's finger penetrates the virtual touch intersection plane <b>122</b>.</p><p id="p-0066" num="0065">Furthermore, the feedback interface <b>114</b> may generate different types of haptic feedback depending on the on-screen gestures. For example, a mouseclick may be signified by a high amplitude ultrasonic pulse whereas a drag gesture may be signified by a continuous train of lower amplitude pulses. Further haptic feedback may be provided to signify key click gestures and the like.</p><p id="p-0067" num="0066">In embodiments, the interface may comprise an interaction depth indicator indicating to a user whether the user is interacting with the intersection plane <b>122</b> at an appropriate depth. In accordance with this embodiment, the digital display may comprise a visual indicator, such as one located at a bezel of the digital display which, for example, may display green when the user is interacting at the appropriate depth.</p><p id="p-0068" num="0067">If the proximity detector detects continuous intersection of the intersection plane <b>122</b>, the depth indicator may indicate to the user that the user is interacting too close. Conversely, if the proximity detector detects intermittent interaction with the virtual touch intersection plane, the depth indicator may indicate to the user that the user is interacting at the appropriate depth.</p><p id="p-0069" num="0068">In embodiments, the interface <b>100</b> may utilise a redundant touch interface such as the aforedescribed capacitive touch interface and/or a haptic overlay which detects physical touches on the digital display <b>119</b> in the event that the proximity detector <b>115</b> is non-functional or wherein the redundant touch interface is used in combination with the detection of approximately detector <b>115</b>.</p><p id="p-0070" num="0069">The foregoing description, for purposes of explanation, used specific nomenclature to provide a thorough understanding of the invention. However, it will be apparent to one skilled in the art that specific details are not required in order to practise the invention. Thus, the foregoing descriptions of specific embodiments of the invention are presented for purposes of illustration and description. They are not intended to be exhaustive or to limit the invention to the precise forms disclosed as obviously many modifications and variations are possible in view of the above teachings. The embodiments were chosen and described in order to best explain the principles of the invention and its practical applications, thereby enabling others skilled in the art to best utilize the invention and various embodiments with various modifications as are suited to the particular use contemplated. It is intended that the following claims and their equivalents define the scope of the invention.</p><p id="p-0071" num="0070">The term &#x201c;approximately&#x201d; or similar as used herein should be construed as being within 10% of the value stated unless otherwise indicated.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A contactless touchscreen interface comprising:<claim-text>a digital display to display digital information;</claim-text><claim-text>a proximity detector comprising a plurality of image sensors located adjacent an edge of the digital display to detect user interaction at a virtual touch intersection plane offset a distance from the digital display and to resolve the interaction into XY offset-plane interaction coordinates with reference to the digital display;</claim-text><claim-text>a gaze determining imaging system comprising an image sensor which determines a gaze relative offset with respect to the digital display using facial image data captured by the image sensor;</claim-text><claim-text>an interface controller comprising a parallax adjustment controller to convert the XY offset-plane interaction coordinates to XY on-screen apparent coordinates using the gaze relative offset and the distance; and</claim-text><claim-text>an input controller to generate an input at the XY on-screen apparent coordinates, wherein the plurality of image sensors of the proximity detector comprise a-first and second image sensors configured to separately ascertain respective opposite regions of the digital display.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The interface as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the proximity detector is configured to determine a plurality of relative spatial points lying on at least one contours and extremities of user's hand or finger and wherein the XY offset-plane interaction coordinates are determined from the plurality of relative spatial points.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The interface as claimed in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the plurality of image sensors are stereoscopic image sensors and wherein the plurality of relative spatial points are determined from differential comparison of stereoscopic image data obtained from the stereoscopic image sensors.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The interface as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the interface controller comprises an image processing controller which determines the gaze relative offset using the facial image data using facial detection to detect the position of a face within a field of view of the image sensor.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The interface as claimed in <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the image processing controller determines at least one of a facial centroid and eye location using the facial image data.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The interface as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a feedback interface comprising a plurality of ultrasonic transducers which emit ultrasound inducing mid-air tactile feedback.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The interface as claimed in <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the ultrasonic transducers are located at an edge of the digital display.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The interface as claimed in <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the ultrasonic transducers are orientated in towards the digital display.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The interface as claimed in <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the ultrasonic transducers are recessed beneath the surface plane of the digital display.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The interface as claimed in <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the ultrasonic transducers extend above a surface plane of the digital display.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The interface as claimed in <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the ultrasonic transducers emit ultrasound at approximately 40 kHz.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The interface as claimed in <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the ultrasonic transducers are located at opposite edges of the digital display.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The interface as claimed in <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the ultrasonic transducers comprise a first set of ultrasonic transducers located at a first side of the digital display and a second set of ultrasonic transducers located at an opposite side of the digital display and wherein at least one of the timing of the operation and phase of the transducers is adjusted according to the XY offset plane interaction coordinates so that ultrasound from the first and second set of transducers coincides or correlates at the XY offset plane interaction coordinates.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The interface as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the interface comprises a single image sensor for the gaze determining imaging system at a top of a bezel of the digital display and the proximity detector comprises a pair of image sensors at either side of the bezel.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The interface as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the feedback interface generates two frequencies of ultrasound using the ultrasonic transducers and controls the phase thereof to create a standing wave at the XY offset-plane interaction coordinates.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The interface as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the interface comprises an interaction depth indicator indicating to a user whether the user is interacting with the intersection plane wherein the depth indicator indicates to the user that the user is interacting too close if the proximity detector detects continuous intersection of the intersection plane and indicates to the user that the user is interacting at the appropriate depth if the proximity detector detects intermittent interaction with the virtual touch intersection plane.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. An interface as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the interface controller further comprises a HID controller which converts the on-screen apparent coordinates to an HID input for an application computer operably coupled to the interface controller, wherein the HID input is a mouse input and wherein the application computer displays a mouse cursor at the on-screen apparent coordinates.</claim-text></claim></claims></us-patent-application>