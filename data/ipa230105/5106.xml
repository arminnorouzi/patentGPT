<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005107A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005107</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17363253</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>11</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>11</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>194</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>005</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>11</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>11</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>194</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">MULTI-TASK TEXT INPAINTING OF DIGITAL IMAGES</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Palo Alto Research Center Incorporated</orgname><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Gopalkrishna</last-name><first-name>Vijay Kumar Baikampady</first-name><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Bala</last-name><first-name>Raja</first-name><address><city>Pittsford</city><state>NY</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A multi-task text infilling system receives a digital image and identifies a region of interest of the image that contains original text. The system uses a machine learning model to determine, in parallel: a foreground image that includes the original text; a background image that omits the original text; and a binary mask that distinguishes foreground pixels from background pixels, The system receives a target mask that contains replacement text. The system then applies the target mask to blend the background image with the foreground layer image and yield a modified digital image that includes the replacement text and omits the original text.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="169.50mm" wi="82.21mm" file="US20230005107A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="198.20mm" wi="130.47mm" orientation="landscape" file="US20230005107A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="199.14mm" wi="131.32mm" file="US20230005107A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="184.74mm" wi="84.07mm" file="US20230005107A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="142.58mm" wi="98.04mm" orientation="landscape" file="US20230005107A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="175.68mm" wi="129.54mm" orientation="landscape" file="US20230005107A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">There are many situations in which the removal of scene text from an image or sequence of video image frames is desirable. For example, video sequences may contain images of storefronts with names, street signs, license plates, and other scenes that contain text. In some situations, it is desirable to remove the scene text from the images to prevent disclosure of personally identifiable information, such as street names or vehicle plate numbers, to others who view the images. In other situations, it is desirable to remove scene text to avoid creating images that contain a logo or product name that conflicts with obligations to advertisers or sponsors of the image or video sequence. In other situations, it us desirable to remove scene text and replace it with personalized messages or messages that suit the storyline that the images are presenting. Other applications, such as translation of scene text, can also benefit from text removal and replacement processes.</p><p id="p-0003" num="0002">The removal of scene text from digital images is sometimes referred to as &#x201c;text infilling&#x201d; or &#x201c;text inpainting.&#x201d; Manual text infilling is very time consuming, and it can be imprecise. Therefore, several automated text infilling methods are known. However, the existing methods often yield results in which the tone or texture of the infilled text is still distinguishable from the original background, and/or results in which the background tone or texture has been altered to match the infilled area. This is an especially challenging problem when an image's background exhibits texture or some other form of varying intensity and/or color. In other situations, the background immediately adjacent to the text may be changed to white or some other color or texture that does is not consistent with that of the rest of the background. This results in images which do not appear natural, and it is not acceptable in applications such as video personalization that require higher fidelity results.</p><p id="p-0004" num="0003">This document describes methods and systems that are directed to addressing the issues listed above.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0005" num="0004">Methods and systems for editing digital image frames are described in this document. The system will include a processor and programming instructions that, when executed, will cause the processor to receive a digital image frame, process the digital image frame to define a region of interest (ROI) that contains original text, and process the ROI through a multi-task machine learning model such as a deep neural network. Processing the ROI through the model will determine, in parallel processes: (i) a foreground image of the ROI, wherein the foreground image comprises the original text; (ii) a background image of the ROI, wherein the background image omits the original text; and (iii) a binary mask that distinguishes foreground image pixels from background image pixels in the ROI. Then, when the system receives a target mask that contains replacement text, the system will apply the target mask to blend the background image with the foreground image and yield a modified digital image that includes the replacement text and omits the original text.</p><p id="p-0006" num="0005">In some embodiments, the multi-task machine learning model may include a single deep neural encoder for receiving the ROI, along with separate deep neural decoders for predicting each of the foreground image, the background image and the binary mask.</p><p id="p-0007" num="0006">Optionally, the system also may include a dataset of ground truth foreground images, ground truth background images and ground truth mask images. The system may use the dataset to train the machine learning model by allowing the dataset to process the images of the dataset with one or more of the following losses: background reconstruction loss; background adversarial loss; foreground reconstruction loss; mask reconstruction loss; end to end reconstruction loss; and/or end to end adversarial loss.</p><p id="p-0008" num="0007">In some embodiments, before applying the target mask, the system may generate a residual correction signal, and it may apply the residual correction signal to the background image. For example, the system may: (a) use the binary mask to extract an average background signal from the ROI of the digital image frame; (b) identify an average background signal of the background image; (c) generate a residual signal as a difference between the average background signal extracted by the binary mask for the ROI of the digital image frame and the average background signal of the background image; and (d) modify the background image by adding the residual signal to substantially every pixel in the background image.</p><p id="p-0009" num="0008">In some embodiments, the system may use the binary mask to extract an average foreground signal from the foreground image. The system may then modify the foreground image by adding the extracted average foreground signal to the foreground image.</p><p id="p-0010" num="0009">In some embodiments, when applying the target mask to blend the background image with the foreground image and yield the modified digital image, the system may generate a modified ROI by applying the target mask to blend the background image with the foreground image, and it may replace the ROI of the received digital image frame with the modified ROI. Optionally, the blending action may include assigning a color value of the foreground image to foreground pixels of the target mask, and assigning a color value of the background image to background pixels of the target mask. Optionally, the blending action also may include using a generative neural network to blend the foreground image with the background image through the target mask.</p><p id="p-0011" num="0010">In some embodiments, to process the digital image frame to define the ROI the system may apply a text detector to the digital image frame to return bounding box coordinates, and the system may then define the ROI according to the bounding box coordinates.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example text infilling process for an image or video sequence.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates the steps of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in flowchart format.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates various layers and masks that may generated by various tasks of the text infilling processes of this document.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b></figref> compares example images, before and after processing, using a prior art method and methods described in this document.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates example hardware elements of a video processing system.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0017" num="0016">As used in this document, the singular forms &#x201c;a,&#x201d; &#x201c;an,&#x201d; and &#x201c;the&#x201d; include plural references unless the context clearly dictates otherwise. Unless defined otherwise, all technical and scientific terms used in this document have the same meanings as commonly understood by one of ordinary skill in the art. As used in this document, the term &#x201c;comprising&#x201d; (or &#x201c;comprises&#x201d;) means &#x201c;including (or includes), but not limited to.&#x201d; When used in this document, the term &#x201c;exemplary&#x201d; is intended to mean &#x201c;by way of example&#x201d; and is not intended to indicate that a particular exemplary item is preferred or required.</p><p id="p-0018" num="0017">In this document, when terms such &#x201c;first&#x201d; and &#x201c;second&#x201d; are used to modify a noun, such use is simply intended to distinguish one item from another, and it is not intended to require a sequential order unless specifically stated.</p><p id="p-0019" num="0018">The term &#x201c;approximately,&#x201d; when used in connection with a numeric value, is intended to include values that are close to, but not exactly, the number. For example, in some embodiments, the term &#x201c;approximately&#x201d; may include values that are within +/&#x2212;10 percent of the value.</p><p id="p-0020" num="0019">In this document, the term &#x201c;average&#x201d; or &#x201c;averaging&#x201d; refers to the calculation of a mean value of a group of values. In addition, unless specifically stated otherwise, the terms can also apply to the calculation of some similar middle value of a set such as a median, or the result of a process that first eliminates high and/or low outliers before calculating a middle value.</p><p id="p-0021" num="0020">In this document, the term &#x201c;substantially every&#x201d;, when referring to a particular parameter, means that a great majority of such parameters satisfies a condition. In various embodiments, the majority may be at least 80%, at least 85%, at least 90%, or at least 95% of the parameters.</p><p id="p-0022" num="0021">In this document, the term &#x201c;text&#x201d; will be used to refer to a sequence of one or more characters, and it may include alphanumeric characters such as letters and numbers, characters that represent words or portions of words in various languages such as kanji and hanzi, as well as punctuation marks, mathematical symbols, emojis, and other symbols.</p><p id="p-0023" num="0022">Additional terms that are relevant to this disclosure will be defined at the end of this Detailed Description section.</p><p id="p-0024" num="0023">As noted in the Background section above, the removal of scene text from digital images is sometimes referred to as &#x201c;text infilling&#x201d; or &#x201c;text inpainting&#x201d;. There are several known methods of text infilling, including methods that use deep neural networks. One known method uses methods known as &#x201c;pix2pix&#x201d;, in which conditional generative adversarial network (GAN) transforms a first image to a second image. The pix2pix method is described in numerous publications, including Wang et al., &#x201c;High Resolution Image Synthesis and Semantic Manipulation with Conditional GANs&#x201d; (arxiv:1711.11585 [cs.CV], 2018). The conditional GAN of pix2pix is trained on a dataset of image pairs [s, x] in which each s is a semantic label map (i.e., images in which labels have been assigned to text) and each x is a corresponding natural photo. The pix2pix method uses a generator G and a discriminator D. The role of the generator is to translate semantic label maps into realistic-looking images. The role of the discriminator is to take the output of the generator and try to distinguish real images from translated ones. Training enables the conditional GAN to map images that contain text to inpainted versions of those images, from which the text has been removed. In the pix2pix method, a generator such as the U-Net encoder-decoder network may be used, and a discriminator such as a patch-based fully convolutional network may be used. This is known and has been described in more detail in, for example, Hu et al., &#x201c;Image-to-Image Translation with Conditional-GAN&#x201d; (Stanford University 2018). The pix-to-pix method uses a single encoder and decoder in its generator network.</p><p id="p-0025" num="0024">The pix2pix method can be used for text infilling of individual images, but it may be time-consuming and inefficient to apply to a sequence of video images. In addition, even if used, in current applications it can result in image frames that have the issues described in the Background section above.</p><p id="p-0026" num="0025">To address this, this document describes a novel method of applying text infilling to a digital image or a video sequence of digital image frames. A text region of interest (ROI) is represented in terms of three elements: a foreground (text) image, a background image, and a mask that specifies the foreground vs. background pixels. The system uses a multi-task network architecture that may simultaneously predict these three images. This enables the inpainting network to determine the location of the text and extract precise color and texture for the foreground and background, and thus improve the quality of the text inpainting process. In addition, segregating the input image into background, foreground and mask images can help the system define specific loss functions for the individual components, as well as on the new image that the system creates by combining these components.</p><p id="p-0027" num="0026">An example workflow is shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates the steps of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in flowchart format. At <b>101</b> an image processing system will receive a digital image frame. (The process described in this document may be repeated and performed on multiple image frames of a video sequence, but to simplify the description we will explain the process as applied to one of the frames.) The system may do this by receiving the image from a remote computing device via a communication link, by retrieving it from a local or cloud-based memory device, or in real-time as a camera of the system captures digital image frames as a video.</p><p id="p-0028" num="0027">The image frames received at step <b>101</b> may be pre-processed, in that they have been cropped to identify a defined region ROI that contains text. If not, then at <b>102</b> the system may process the image frames with a text detector that returns bounding box coordinates for cropping the text ROI in each frame. A text detector is a set of programming instructions that a computing device uses to process an image with a feature detector such as the tesseract algorithm, a maximally stable extremal regions (MSER) algorithm detector, a convolutional neural network (CNN) or any now or hereafter known optical character recognition (OCR) process. <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example ROI <b>301</b> that contains a foreground region that exhibits a first color and/or texture, along with a background region that exhibits of a second color and/or texture. The foreground region includes the pixels that include text, while the background region includes the pixels that do not contain text.</p><p id="p-0029" num="0028">Returning to <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, at <b>103</b> the system then applies the ROI to an encoder of a neural network or other machine learning model which converts the ROI image into a format that the network may then process, such as a multidimensional tensor. The encoder used at <b>103</b> may be (as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) a single, deep neural encoder that is shared by multiple decoders. The output of the encoder is then simultaneously passed to three decoders: a foreground image decoder, a background image decoder, and a binary image decoder, which perform a parallel process to simultaneously predict the three constituent image that make up the single ROI input. Optionally, some or all of the decoders may be deep neural network decoders. At <b>104</b> the foreground image decoder of the neural network will predict a foreground image of the ROI, in which the foreground image includes color values for the text of the ROI. At <b>105</b> the background image decoder of the network will determine a background image of the ROI, in which the background image includes color values for areas of the ROI that do not contain the text of the ROI. This process, also referred to as text inpainting, will be described in more detail below.</p><p id="p-0030" num="0029">At <b>106</b> the binary mask decoder of the network will predict a binary mask that distinguishes the foreground pixels from the background pixels by assigning each pixel a value of either 0 or 1, as shown by way of example in mask <b>302</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In reality, the neural decoder predicts the probability of each pixel being a foreground color, taking on values between 0 and 1. This probability map is then converted to a binary mask with a suitable thresholding operator. In mask <b>302</b>, foreground pixels have a value of 1 while background pixels have a value of 0, but in practice this assignment could also be reversed such that foreground pixels have a value of 0 while background pixels have a value of 1. Boundary pixels may be assigned strictly 1 or 0, or they may be assigned intermediate values in order to ensure smooth transitions from foreground to background. The encoder may be trained jointly with the three decoders (foreground, background, mask) as is described later in this document.</p><p id="p-0031" num="0030">Returning again to <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, ideally the predicted background and foreground images produced by the decoders should exactly match the input ROI in the background and foreground regions respectively. However the decoders are likely to introduce errors. To reduce the magnitude of these errors, a residual post-correction step is introduced. First the system applies the binary mask to the input ROI to extract the average color of the input foreground pixels and the average color of the input background pixels, respectively, at <b>107</b>. The system may use the binary mask to extract a foreground average color by averaging the color values of pixels of the input image that correspond to foreground pixels in the mask. Similarly, the system may use the binary mask to extract a background average color by averaging the color values of input image pixels that correspond to background pixels in the mask. Optionally, the average value may be a weighted average. If the mask is a continuous-valued map indicating probability of foreground, then these probabilities are treated as foreground weights. A weighted foreground average may be computed by performing pixelwise multiplication of foreground weights and foreground colors, summing the products, and dividing by the sum of foreground weights. The process is repeated to compute a weighted background average, except that background weight is computed as 1-{foreground weight}.</p><p id="p-0032" num="0031">Next the system uses the binary mask to determine an average background color signal (step <b>108</b>) from the output image predicted by the background decoder using the same set of steps described above. This average predicted background color signal is subtracted channelwise for each of R, G, B from the average input background color signal from <b>107</b>, and the difference will form a residual correction signal [&#x394;R, &#x394;G, &#x394;B] at <b>109</b>. At <b>111</b> the system will then add the residual signal to substantially every pixel of the predicted background image. This will result in a predicted background image as shown by area <b>303</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in which pixels of the ROI that contain text have been infilled with the predicted background color. At <b>112</b> the system will add the average input foreground color computed from the input ROI to substantially every pixel in the predicted foreground image. This will result in a corrected foreground image as shown by area <b>304</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in which pixels of the ROI that do not contain text have been infilled with the predicted foreground color.</p><p id="p-0033" num="0032">At <b>113</b> the system will receive a target mask, which is a binary mask that contains a new replacement text string. <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example target mask <b>305</b>, in which, as with the binary mask <b>302</b> for the input image, foreground pixels continue to have a value of 1 while background pixels have a value of 0.</p><p id="p-0034" num="0033">At <b>114</b> the system will then use the target mask to blend the predicted background image and the predicted foreground image to yield a modified ROI. The blending process may be performed by simple binary switching, in which background pixels of the target mask are assigned color values of the predicted background image while foreground pixels of the target mask are assigned color values of the predicted foreground image. Alternatively, the blending may be performed using any now or hereafter known blending process such as those that use a generative adversarial network. (See, for example, Zhang et. Al., &#x201c;Deep Image Blending&#x201d;, arXiv: 1910.11495 [cs.cv] 2020.)</p><p id="p-0035" num="0034">At <b>115</b> the system will the replace the ROI of the input image with the modified ROI to yield a modified image. The modified image is then output at <b>120</b>.</p><p id="p-0036" num="0035">In the process described above, &#x201c;color&#x201d; has been described as a single value for the purpose of describing the process. However, in practice, each pixel of the ROI will have a color value in an RGB color model, which means that the color value will include three channels (one for a red component R, one for a blue component G, and one for a blue component B). Pixels may include additional channels for additional characteristics, such as luminance or chrominance. Thus, the process described above may be performed for each available channel. In such situations, the average foreground color, average background color, and residual signal will be C-dimensional vectors, in which C represents the number of available channels of the network. When determining averages, or when adding or blending, some channels may be given different weights than others. For example, the system may assign equal weights to RGB channels, but other channels such as those for characteristics such as luminance and chrominance may have unequal weights in various processes.</p><p id="p-0037" num="0036">As noted in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, before the image process begins the single encoder and separate decoders may be jointly trained at <b>99</b>. Therefore, in various embodiments, the neural networks used for the encoding and decoding may be trained to minimize one or more of the following loss terms:</p><p id="p-0038" num="0037">For the Background Image:</p><p id="p-0039" num="0038">Reconstruction loss: This is the error between a predicted background image (i.e. inpainted ROI) and ground truth. This prediction includes the residual correction that would be performed at step <b>108</b>. This loss may be defined in a pixel space (i.e., L1 norm, which is the sum of absolute pixel values, or L2 norm, which is the square root of the sum of squared pixel values), spatially filtered pixel values, or feature space (such as perceptual loss in a VGG convolutional neural network architecture).</p><p id="p-0040" num="0039">Spatial adversarial loss: The decoder must be trained on a dataset that includes ground truth foreground, background and mask images with one or more of the following losses to produce realistic inpainted images that fool a discriminator trained to distinguish real from fake inpainted images.</p><p id="p-0041" num="0040">For the Foreground image:</p><p id="p-0042" num="0041">Reconstruction loss: This is the error between the foreground pixels in the input ROI and pixels of the predicted foreground image. The foreground pixels are selected using the predicted binary mask. Reconstruction loss in a foreground image may include, for example, L1, L2 and/or perceptual losses.</p><p id="p-0043" num="0042">Energy loss: This loss minimizes the total energy of the predicted foreground signal to encourage smoothly varying color. Energy loss in a foreground image may consider energy and/or smoothness.</p><p id="p-0044" num="0043">For the predicted Binary Mask:</p><p id="p-0045" num="0044">Reconstruction loss: This loss minimizes the error between the predicted binary mask and the ground truth binary mask. Reconstruction loss in a mask may include, for example, L1, L2, and/or perceptual losses.</p><p id="p-0046" num="0045">For the whole image (in which the predicted foreground, background and binary mask are used to reconstruct the image):</p><p id="p-0047" num="0046">Reconstruction loss: This loss minimizes the error between the input ROI and the reconstructed output ROI. During training, the target mask <b>113</b> contains the same text string as the input ROI. End-to-end reconstruction loss in a mask may include, for example, L1, L2, and/or perceptual losses.</p><p id="p-0048" num="0047">Adversarial loss: The reconstructed image should be realistic enough to fool a discriminator trained to distinguish real images from altered (i.e., inpainted) images.</p><p id="p-0049" num="0048">Other losses, such as the difference between foreground and background pixel statistics (such as mean or variance), or the difference in background regions between predicted output and input. Each of these losses, when available, will be input into the model with the training images during the training process.</p><p id="p-0050" num="0049">Ground truth for inpainted regions may be determined in any number of ways, such as using a graphics rendering program to create a dataset of image pairs, in which each pair has the same scene but only one pair has text.</p><p id="p-0051" num="0050">The data set may be manually selected or synthetically generated, with varied colors for foreground or background images. The process illustrated in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> provide a multi-tasking process in which the network is tasked to jointly produce naturalistic inpainted background regions that preserve texture and other characteristics, a foreground image that preserves the text color, and a binary mask that distinguishes foreground regions from background regions. This disentangling enables the system to define loss functions for each region separately.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example comparison of masks generated by prior art systems as compared to those of the processes described in this document. In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, column <b>401</b> shows cropped ROIs from original image frame. Column <b>402</b> shows binary masks generated using mask RCNN as known in the prior art. Column <b>403</b> shows binary masks generated by an example of the present invention. As can be seen by comparing columns <b>402</b> and <b>403</b>, the prior art missed some of the characters of the original image when predicting its mask, while the present invention accurately captured all of the characters from the original image.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts an example of internal hardware that may be included in an image processing system, such as a local or remote computing device that processes image frames as described above. An electrical bus <b>500</b> serves as an information highway interconnecting the other illustrated components of the hardware. Processor <b>505</b> is a central processing device of the system, configured to perform calculations and logic operations required to execute programming instructions. As used in this document and in the claims, the terms &#x201c;processor&#x201d; and &#x201c;processing device&#x201d; may refer to a single processor or any number of processors in a set of processors that collectively perform a set of operations, such as a central processing unit (CPU), a graphics processing unit (GPU), a remote server, or a combination of these. The programming instructions may be configured to cause the processor to perform any or all of the methods described above. Read only memory (ROM), random access memory (RAM), flash memory, hard drives and other devices capable of storing electronic data constitute examples of memory devices <b>525</b>. A memory device may include a single device or a collection of devices across which data and/or instructions are stored.</p><p id="p-0054" num="0053">An optional display interface <b>530</b> may permit information from the bus <b>500</b> to be displayed on a display device <b>535</b> in visual, graphic or alphanumeric format. An audio interface and audio output (such as a speaker) also may be provided. Communication with external devices may occur using various communication devices <b>540</b> such as a wireless antenna, an RFID tag and/or short-range or near-field communication transceiver, each of which may optionally communicatively connect with other components of the device via one or more communication system. The communication device <b>540</b> may be configured to be communicatively connected to a communications network, such as the Internet, a local area network or a cellular telephone data network.</p><p id="p-0055" num="0054">The hardware may also include a user interface sensor <b>545</b> that allows for receipt of data from input devices <b>550</b> such as a keyboard, a mouse, a joystick, a touchscreen, a touch pad, a remote control, a pointing device and/or microphone. Such devices may be used to help label images in training the model. Digital image frames also may be received from a camera <b>520</b> that can capture video and/or still images.</p><p id="p-0056" num="0055">Terminology that is relevant to this disclosure includes:</p><p id="p-0057" num="0056">An &#x201c;electronic device&#x201d; or a &#x201c;computing device&#x201d; refers to a device or system that includes a processor and memory. Each device may have its own processor and/or memory, or the processor and/or memory may be shared with other devices as in a virtual machine or container arrangement. The memory will contain or receive programming instructions that, when executed by the processor, cause the electronic device to perform one or more operations according to the programming instructions. Examples of electronic devices include personal computers, servers, mainframes, virtual machines, containers, gaming systems, televisions, digital home assistants and mobile electronic devices such as smartphones, personal digital assistants, cameras, tablet computers, laptop computers, media players and the like. Electronic devices also may include components of vehicles such as dashboard entertainment and navigation systems, as well as on-board vehicle diagnostic and operation systems. In a client-server arrangement, the client device and the server are electronic devices, in which the server contains instructions and/or data that the client device accesses via one or more communications links in one or more communications networks. In a virtual machine arrangement, a server may be an electronic device, and each virtual machine or container also may be considered an electronic device. In the discussion above, a client device, server device, virtual machine or container may be referred to simply as a &#x201c;device&#x201d; for brevity. Additional elements that may be included in electronic devices are discussed above in the context of <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0058" num="0057">The terms &#x201c;processor&#x201d; and &#x201c;processing device&#x201d; refer to a hardware component of an electronic device that is configured to execute programming instructions. Except where specifically stated otherwise, the singular terms &#x201c;processor&#x201d; and &#x201c;processing device&#x201d; are intended to include both single-processing device embodiments and embodiments in which multiple processing devices together or collectively perform a process.</p><p id="p-0059" num="0058">The terms &#x201c;memory,&#x201d; &#x201c;memory device,&#x201d; &#x201c;data store,&#x201d; &#x201c;data storage facility,&#x201d; &#x201c;computer-readable medium&#x201d; and the like each refer to a non-transitory device on which computer-readable data, programming instructions or both are stored. Except where specifically stated otherwise, the terms &#x201c;memory,&#x201d; &#x201c;memory device,&#x201d; &#x201c;data store,&#x201d; &#x201c;data storage facility&#x201d; and the like are intended to include single device embodiments, embodiments in which multiple memory devices together or collectively store a set of data or instructions, as well as individual sectors within such devices. A &#x201c;computer program product&#x201d; is a computer-readable medium with programming instructions stored on it.</p><p id="p-0060" num="0059">A &#x201c;machine learning model&#x201d; or a &#x201c;model&#x201d; refers to a set of algorithmic routines and parameters that can predict an output(s) of a real-world process (e.g., replacement of scene text) based on a set of input features, without being explicitly programmed. A structure of the software routines (e.g., number of subroutines and relation between them) and/or the values of the parameters can be determined in a training process, which can use actual results of the real-world process that is being modeled. Such systems or models are understood to be necessarily rooted in computer technology, and in fact, cannot be implemented or even exist in the absence of computing technology. While machine learning systems utilize various types of statistical analyses, machine learning systems are distinguished from statistical analyses by virtue of the ability to learn without explicit programming and being rooted in computer technology. Example machine learning models include neural networks such as convolutional neural networks (CNNs), generative adversarial networks (GANs), and other trained networks that exhibit artificial intelligence.</p><p id="p-0061" num="0060">The term &#x201c;bounding box&#x201d; refers to a rectangular box that represents the location of an object. A bounding box may be represented in data by x- and y-axis coordinates [x<sub>max</sub>, y<sub>max</sub>] that correspond to a first corner of the box (such as the upper right corner), along with x- and y-axis coordinates [x<sub>min</sub>, y<sub>min</sub>] that correspond to the corner of the rectangle that is opposite the first corner (such as the lower left corner). It may be calculated as the smallest rectangle that contains all of the points of an object, optionally plus an additional space to allow for a margin of error. The points of the object may be those detected by one or more sensors, such as pixels of an image captured by a camera.</p><p id="p-0062" num="0061">The features and functions described above, as well as alternatives, may be combined into many other different systems or applications. Various alternatives, modifications, variations or improvements may be made by those skilled in the art, each of which is also intended to be encompassed by the disclosed embodiments.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A digital image frame editing method comprising, by a processor:<claim-text>receiving a digital image frame;</claim-text><claim-text>processing the digital image frame to define a region of interest (ROI) that contains original text;</claim-text><claim-text>processing the ROI through a multi-task machine learning model to predict, in parallel processes:<claim-text>a foreground image of the ROI, wherein the foreground image comprises the original text,</claim-text><claim-text>a background image of the ROI, wherein the background image omits the original text, and</claim-text><claim-text>a binary mask that distinguishes foreground image pixels from background image pixels in the ROI;</claim-text></claim-text><claim-text>receiving a target mask that contains replacement text; and</claim-text><claim-text>applying the target mask to blend the background image with the foreground image and yield a modified digital image that includes the replacement text and omits the original text.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the multi-task machine learning model comprises:<claim-text>a single deep neural encoder that receives the ROI; and</claim-text><claim-text>separate deep neural decoders for predicting each of the foreground image, the background image and the binary mask.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref> further comprising, before receiving the digital image frame, training the multi-task machine learning model on a dataset comprising ground truth foreground images, ground truth background images and ground truth mask images with one or more of the following losses<claim-text>background reconstruction loss;</claim-text><claim-text>background adversarial loss;</claim-text><claim-text>foreground reconstruction loss;</claim-text><claim-text>mask reconstruction loss;</claim-text><claim-text>end to end reconstruction loss; or</claim-text><claim-text>end to end adversarial loss.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref> further comprising, before applying the target mask:<claim-text>using the binary mask to extract an average background signal of the ROI; and</claim-text><claim-text>using the average background signal to modify the background image produced by the decoder that predicted the background image.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref> wherein using the average background signal to modify the background image comprises:<claim-text>generating a residual signal as a difference between the average background signal extracted by the binary mask and the average background signal of the predicted background image; and</claim-text><claim-text>modifying the predicted background image by adding the residual signal to substantially every pixel in the predicted background image.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref> further comprising, before applying the target mask:<claim-text>using the binary mask to extract an average foreground signal from the foreground image; and</claim-text><claim-text>using the average foreground signal to modify the foreground image produced by the decoder that predicted the foreground image.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein using the average foreground signal to modify the foreground image comprises modifying the foreground image by adding the extracted average foreground signal to the foreground image.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein applying the target mask to blend the background image with the foreground image and yield the modified digital image further comprises:<claim-text>generating a modified ROI by applying the target mask to blend the background image with the foreground image; and</claim-text><claim-text>replacing the ROI of the received digital image frame with the modified ROI.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the blending comprises:<claim-text>assigning a color value of the foreground image to foreground pixels of the target mask; and</claim-text><claim-text>assigning a color value of the background image to background pixels of the target mask.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the blending comprises using a generative neural network to blend the foreground image with the background image through the target mask.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the machine learning model comprises a deep neural network.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein processing the digital image frame to define the ROI comprises:<claim-text>applying a text detector to the digital image frame to return bounding box coordinates; and</claim-text><claim-text>defining the ROI according to the bounding box coordinates.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A digital image frame editing system comprising:<claim-text>a processor; and</claim-text><claim-text>a memory device containing programming instructions that are configured to cause the processor to:<claim-text>receive a digital image frame,</claim-text><claim-text>process the digital image frame to define a region of interest (ROI) that contains original text,</claim-text><claim-text>process the ROI through a multi-task machine learning model to determine, in parallel processes:<claim-text>a foreground image of the ROI, wherein the foreground image comprises the original text;</claim-text><claim-text>a background image of the ROI, wherein the background image omits the original text; and</claim-text><claim-text>a binary mask that distinguishes foreground image pixels from background image pixels in the ROI,</claim-text></claim-text><claim-text>receive a target mask that contains replacement text, and</claim-text><claim-text>apply the target mask to blend the background image with the foreground image and yield a modified digital image that includes the replacement text and omits the original text.</claim-text></claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref> wherein the multi-task machine learning model comprises:<claim-text>a single deep neural encoder for receiving the ROI; and</claim-text><claim-text>separate deep neural decoders for predicting each of the foreground image, the background image and the binary mask.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref> further comprising:<claim-text>a dataset comprising ground truth foreground images, ground truth background images and ground truth mask images; and</claim-text><claim-text>additional programming instructions that are configured to cause the processor to, before receiving the digital image frame, train the multi-task machine learning model on the dataset with one or more of the following losses<claim-text>background reconstruction loss,</claim-text><claim-text>background adversarial loss,</claim-text><claim-text>foreground reconstruction loss,</claim-text><claim-text>mask reconstruction loss,</claim-text><claim-text>end to end reconstruction loss, or</claim-text><claim-text>end to end adversarial loss.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising additional instructions configured to cause the processor to, before applying the target mask:<claim-text>generate a residual correction signal; and</claim-text><claim-text>apply the residual correction signal to the background image.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising additional instructions configured to cause the processor to:<claim-text>use the binary mask to extract an average foreground signal from the foreground image; and</claim-text><claim-text>modify the foreground image by adding the extracted average foreground signal to the foreground image.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising additional instructions configured to cause the processor to:<claim-text>use the binary mask to extract an average background signal from the ROI of the digital image frame;</claim-text><claim-text>identify an average background signal of the background image;</claim-text><claim-text>generate a residual signal as a difference between the average background signal extracted by the binary mask for the ROI of the digital image frame and the average background signal of the background image; and</claim-text><claim-text>modify the background image by adding the residual signal to substantially every pixel in the background image.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the instructions to apply the target mask to blend the background image with the foreground image and yield the modified digital image further comprise instructions to:<claim-text>generate a modified ROI by applying the target mask to blend the background layer with the foreground layer; and</claim-text><claim-text>replace the ROI of the received digital image frame with the modified ROI.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the instructions to blend comprise instructions to:<claim-text>assign a color value of the foreground image to foreground pixels of the target mask; and</claim-text><claim-text>assign a color value of the background image to background pixels of the target mask.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the instructions to blend comprise instructions to use a generative neural network to blend the foreground image with the background image through the target mask.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the machine learning model comprises a deep neural network.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the instructions to process the digital image frame to define the ROI comprise instructions to:<claim-text>apply a text detector to the digital image frame to return bounding box coordinates; and</claim-text><claim-text>define the ROI according to the bounding box coordinates.</claim-text></claim-text></claim></claims></us-patent-application>