<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005494A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005494</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17845214</doc-number><date>20220621</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202110734412.8</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>21</main-group><subgroup>0232</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>R</subclass><main-group>3</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>R</subclass><main-group>29</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>R</subclass><main-group>1</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>R</subclass><main-group>1</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>P</subclass><main-group>1</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>P</subclass><main-group>15</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>21</main-group><subgroup>0232</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>3</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>29</main-group><subgroup>001</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>1</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>1</main-group><subgroup>025</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>P</subclass><main-group>1</main-group><subgroup>023</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>P</subclass><main-group>15</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>2021</main-group><subgroup>02163</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">Method and Apparatus for Removing Noise from Sound Signal from Microphone</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Robert Bosch GmbH</orgname><address><city>Stuttgart</city><country>DE</country></address></addressbook><residence><country>DE</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Du</last-name><first-name>Bin</first-name><address><city>Shanghai</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Jiao</last-name><first-name>Tongtong</first-name><address><city>Shanghai</city><country>CN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method for removing noise from a sound signal received by a microphone is provided. The method includes receiving a vibration signal from a vibration monitoring device mechanically connected to a loudspeaker, the vibration signal indicating vibration caused by a sound emitted by the loudspeaker. The method further includes receiving a sound signal received by the microphone. In addition, the method includes removing the vibration signal from the sound signal so as to remove noise from the sound signal. With the vibration signal from the vibration monitoring device, noise can be removed from the sound signal received by the microphone so as to achieve a satisfactory audio effect or accurate sound recognition.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="53.09mm" wi="113.20mm" file="US20230005494A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="78.99mm" wi="115.23mm" file="US20230005494A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="150.54mm" wi="126.49mm" file="US20230005494A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="106.43mm" wi="115.23mm" file="US20230005494A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="136.82mm" wi="137.24mm" file="US20230005494A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><p id="p-0002" num="0001">This application claims priority under 35 U.S.C. &#xa7; 119 to patent application no. CN 10 202110734412.8, filed on Jun. 30, 2021 in China, the disclosure of which is incorporated herein by reference in its entirety.</p><p id="p-0003" num="0002">The present disclosure relates to media communication and processing, and in particular to sound denoising.</p><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0004" num="0003">In many scenarios, a microphone and a loudspeaker need to work at the same time, for example, when a karaoke device, a smart sound box, or a sound system including a microphone and a loudspeaker in a conference room is used. In some application scenarios, noise from the microphone is amplified and played by the loudspeaker, and then captured by the microphone, which cycles back and forth, making it difficult to get a satisfactory audio effect. Especially in the conference room or a karaoke box, such repeated noise is often unbearable to a user.</p><p id="p-0005" num="0004">In the process of using the smart sound box, when the user issues a voice command while the loudspeaker plays some media content, as a sound signal received by the microphone contains both the sound signal corresponding to the played media content and also a voice command signal of the user, the voice command is often difficult to be accurately recognized, making the smart sound box unable to perform the corresponding operation in time.</p><p id="p-0006" num="0005">At present, a microphone array and a monitoring microphone are arranged, and microphone beamforming and active noise reduction technologies are used to remove noise from the sound signal received by the microphone, such as removing a signal corresponding to a sound emitted by the loudspeaker. However, in such an arrangement, it is difficult to obtain an ideal denoising effect because the monitoring microphone tends to be supersaturated at a high sound volume.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0007" num="0006">It is expected to provide an improved technique for removing noise from a signal from a microphone to achieve a satisfactory audio effect or accurate sound recognition. With this technique, it is possible to remove a signal corresponding to a sound emitted by a loudspeaker that is included in the signal from the microphone.</p><p id="p-0008" num="0007">According to one aspect, a method for removing noise from a sound signal received by a microphone is provided. The method includes: receiving a vibration signal from a vibration monitoring device mechanically connected to a loudspeaker, the vibration signal indicating vibration caused by a sound emitted by the loudspeaker; receiving a sound signal received by the microphone; and removing the vibration signal from the sound signal so as to remove noise from the sound signal.</p><p id="p-0009" num="0008">According to another aspect, an apparatus for removing noise from a sound signal received by a microphone is provided. The apparatus includes: a receiving unit, configured to receive a vibration signal from a vibration monitoring device mechanically connected to a loudspeaker, and a sound signal received by the microphone, the vibration signal indicating vibration caused by a sound emitted by the loudspeaker; and a processing unit, configured to remove the vibration signal from the sound signal to remove noise from the sound signal.</p><p id="p-0010" num="0009">According to another aspect, a sound apparatus is provided. The sound apparatus includes: a loudspeaker; a microphone; a vibration monitoring device, mechanically connected to the loudspeaker and being capable of generating a vibration signal indicating vibration caused by a sound emitted by the loudspeaker; and a processor configured to execute the method according to the embodiment of the present disclosure as described above.</p><p id="p-0011" num="0010">According to yet another aspect, a computer program product is provided, which includes computer program instructions which, when run, enable the processor to execute the method according to the embodiments of the present disclosure.</p><p id="p-0012" num="0011">According to the embodiments of various aspects of the present disclosure, vibration caused by a sound emitted by a loudspeaker is monitored using a vibration monitoring device mechanically coupled to the loudspeaker, then a sound signal received by a microphone is processed on the basis of the monitored vibration signal, and in particular, the signal corresponding to the sound emitted by the loudspeaker is removed. This eliminates the need to use a monitoring microphone, solves the problem of supersaturation of the monitoring microphone, and can remove noise at the same time, so as to achieve a satisfactory audio effect or obtain accurate sound recognition.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0013" num="0012">In the accompanying drawings, embodiments are described only by way of example rather than limitation. Similar reference numerals in the drawings refer to similar elements.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an apparatus for sound collection and playback according to an embodiment of the present disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an apparatus for sound collection and playback according to another embodiment of the present disclosure;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a method for removing noise from a signal from a microphone according to an embodiment of the present disclosure; and</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a method for voice recognition according to an embodiment of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0018" num="0017">Various aspects and features of various embodiments of the present disclosure are described with reference to the above drawings. The above drawings are only schematic rather than limiting. Without departing from the essence of the present disclosure, the dimensions, shape, reference numeral, or appearance of each element in the above drawings may be changed. In addition, various parts of the apparatus in the embodiments of the present disclosure are not fully marked with reference numerals in the above drawings. In some drawings, only related components are shown, but this does not limit such various parts to those shown in the drawings of this specification.</p><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0019" num="0018">In many scenarios, a microphone and a loudspeaker need to work at the same time. In such a scenario, when a user uses the microphone while the loudspeaker plays audio content, a signal received by the microphone includes not only a sound signal emitted by the user but also a signal corresponding to a sound emitted by the loudspeaker, that is, the loudspeaker may become an environmental noise source for the microphone. It is expected to remove the signal corresponding to the sound emitted by the loudspeaker from the signal from the microphone, that is, to remove a noise signal, so as to better restore the sound signal emitted by the user for subsequent playback or sound recognition.</p><p id="p-0020" num="0019">According to the embodiments of the present disclosure, vibration caused by a sound emitted by the loudspeaker is monitored using a vibration monitoring device, and then the sound signal received by the microphone is processed on the basis of the monitored vibration signal so as to obtain the sound signal emitted by the user.</p><p id="p-0021" num="0020">Scenarios requiring simultaneous operation of a microphone and a loudspeaker are various, and only two specific applications will be described below, but it may be understood that the scope of protection of the present disclosure is not limited thereto.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an apparatus <b>10</b> for sound collection and playback according to an embodiment of the present disclosure. The apparatus <b>10</b> may be a sound apparatus. Alternatively, the apparatus <b>10</b> may be some media devices having sound playback and collection functions, such as a smart television. Specifically, in this embodiment, the apparatus <b>10</b> is a smart sound box including a loudspeaker <b>11</b>, a microphone <b>12</b>, a vibration monitoring device <b>13</b>, a processor <b>14</b>, and an (optional) housing <b>15</b>.</p><p id="p-0023" num="0022">The loudspeaker <b>11</b> is configured to play a predetermined audio content, which may be a sound received by the microphone <b>12</b>, processed by the processor <b>14</b> and then played by the loudspeaker <b>11</b>, or may be a predetermined audio content, such as music, crosstalk or a story, that is played, for example, by a user-designated smart sound box.</p><p id="p-0024" num="0023">The microphone <b>12</b> is configured to receive surrounding sound signals. In the case of a smart sound box, it needs to receive a voice signal from the user. The received voice signal from the user is transmitted to the processor <b>14</b> for voice recognition, and when a voice from the user is recognized, the smart sound box may perform the corresponding operation. For example, when the user sends out the voice &#x201c;What's the weather like today?&#x201d;, the microphone <b>12</b> may receive the voice signal, and then the processor <b>14</b> recognizes the voice signal, determines that the user is asking about the weather, initiates a weather inquiry operation to inquire about the weather, and sends an inquiry result to the loudspeaker <b>11</b>, which plays the weather conditions of the day to the user. Sometimes, the user is using a playback function of the smart sound box to play predetermined audio content, such as music, crosstalk or a story, through the loudspeaker.</p><p id="p-0025" num="0024">At this point, he wants to communicate with the smart sound box to inquire about certain contents or use other functions thereof. For example, when the smart sound box is playing crosstalk, the user sends out the voice &#x201c;Remind me in 20 minutes&#x201d;. At this time, the microphone <b>12</b> of the smart sound box receives not only a sound signal from the user but also an audio signal emitted from the loudspeaker <b>11</b>. This makes it difficult for the processor <b>14</b> to recognize the user's voice according to the signal from the microphone. In this case, the user often has to send out a voice signal at a higher volume again, which deteriorates the experience of using the smart sound box.</p><p id="p-0026" num="0025">The smart sound box shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> further includes a vibration monitoring device <b>13</b> arranged on the housing <b>15</b> and used for monitoring vibration caused by a sound emitted by the loudspeaker <b>11</b>. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the loudspeaker <b>11</b> is accommodated in the housing <b>15</b>. When the audio signal is played through the loudspeaker <b>11</b>, the sound thus emitted causes the corresponding vibration in the housing <b>15</b>, and the vibration of the housing <b>15</b> is monitored by the vibration monitoring device <b>13</b>. In this way, the vibration signal from the vibration monitoring device <b>13</b> can indicate the vibration caused by the sound emitted by the loudspeaker <b>11</b>. Although as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the vibration monitoring device <b>13</b> is arranged on an outer surface of the housing <b>15</b>, it is not restrictive, and it may also be arranged on an inner surface of the housing <b>15</b>. The vibration monitoring device <b>13</b> includes an acceleration sensor.</p><p id="p-0027" num="0026">The vibration signal monitored by the vibration monitoring device <b>13</b> is transmitted to the processor <b>14</b>. As described above, the professor <b>14</b> also receives a sound signal received by the microphone <b>12</b>. The processor <b>14</b> is capable of processing the sound signal received by the microphone <b>12</b> on the basis of the received vibration signal so as to remove noise from the sound signal received by the microphone <b>12</b>. Specifically, the received vibration signal is removed from the sound signal received by the microphone. In an embodiment, the processor <b>14</b> includes a receiving unit <b>141</b> and a processing unit <b>142</b>. The receiving unit <b>141</b> receives a vibration signal from the vibration monitoring device <b>13</b> and a sound signal from the microphone <b>12</b>. The processing unit <b>142</b> processes the sound signal on the basis of the received vibration signal.</p><p id="p-0028" num="0027">In an embodiment, the processing unit <b>142</b> may adjust the amplitude, phase and/or bandwidth of the vibration signal with reference to the amplitude, phase and/or bandwidth of the sound signal from the microphone <b>12</b> to convert the vibration signal into the corresponding sound signal. Thereafter, the processing unit <b>142</b> removes the adjusted vibration signal from the sound signal received by the microphone <b>12</b>, i.e., removes the corresponding sound signal obtained after the conversion, so as to obtain the sound signal from the user in the case of the smart sound box. The sound signal from the user may be further subjected to voice recognition processing to initiate the corresponding operation, such as making a timing of 20 minutes.</p><p id="p-0029" num="0028">In an embodiment, the smart sound box may also include a controller (not shown) that controls various operations of the smart sound box. For example, the controller controls the operation of the processor <b>14</b> according to the use state of the loudspeaker <b>11</b>. When the controller determines that the loudspeaker <b>11</b> is in use, the receiving unit <b>141</b> receives both the vibration signal from the vibration monitoring device <b>13</b> and the sound signal from the microphone <b>12</b>, and the processing unit <b>142</b> processes the sound signal from the microphone <b>12</b> on the basis of the received vibration signal. Otherwise, when the controller determines that the loudspeaker <b>11</b> is not in use, the receiving unit <b>141</b> does not receive the vibration signal from the vibration monitoring device <b>13</b> but only receives the sound signal from the microphone <b>12</b>. Therefore, voice recognition can be directly performed on the basis of the sound signal from the microphone.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an apparatus <b>20</b> for sound collection and playback according to another embodiment of the present disclosure. The apparatus <b>20</b>, for example, is a conference room sound apparatus including a loudspeaker <b>21</b>, a microphone <b>22</b>, a vibration monitoring device <b>23</b>, a processor <b>24</b>, and an (optional) housing <b>25</b>. Similar to the embodiment shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the processor <b>24</b> may include a receiving unit and a processing unit. It may also be expected that the processor as a whole implements the functions of the receiving unit and the processing unit described above.</p><p id="p-0031" num="0030">Different from the embodiment described above with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in the embodiment shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the vibration monitoring device <b>23</b> is arranged on an inner surface of the housing <b>25</b>.</p><p id="p-0032" num="0031">In the process of a conference, the microphone <b>22</b> receives sound signals. The received sound signals are processed by the processor <b>24</b> and then played by the loudspeaker <b>21</b>. In this case, a small noise from the microphone is amplified into a large noise, and is played out by the loudspeaker <b>21</b>, and the microphone <b>22</b> further receives the signal from the loudspeaker, which cycles back and forth, making the noise unbearable.</p><p id="p-0033" num="0032">In order to overcome the influence of noise, similarly to the embodiment shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in the embodiment shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the processor <b>24</b> receives a vibration signal from the vibration monitoring device <b>23</b> and the sound signal from the microphone <b>22</b>, and processes the sound signal on the basis of the vibration signal, after which the processed sound signal is amplified and output via the loudspeaker <b>21</b>. Therefore, the noise is removed and a satisfactory audio effect is obtained.</p><p id="p-0034" num="0033">In <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, similar components are denoted by similar reference numerals and these similar components perform similar functions. Therefore, similar components are not described in detail with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0035" num="0034">Although the locations of the vibration monitoring devices <b>13</b>, <b>23</b> are shown only with reference to <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, it may be understood that this is not restrictive. The vibration monitoring device <b>13</b>, <b>23</b> may be arranged at any location in the vicinity of the loudspeaker and is mechanically connected to the loudspeaker. Mechanical connection refers to the physical connection between two components to enable a force transmission between the two components, so as to realize a vibration transmission between them. The connection includes direct connection and indirect connection. The vibration monitoring device <b>13</b>, <b>23</b> is arranged and connected at a location relative to the loudspeaker in such a manner that they can be configured to monitor vibration caused by a sound emitted by the loudspeaker.</p><p id="p-0036" num="0035">It is conceivable that, by mechanically connecting the vibration monitoring device to the loudspeaker, including a rigid connection, the vibration caused by the sound emitted by the loudspeaker can be coupled from the loudspeaker to the vibration monitoring device by mechanical coupling for monitoring thereby.</p><p id="p-0037" num="0036">In an embodiment, the vibration monitoring device is indirectly connected to the loudspeaker. For example, the sound emitted by the loudspeaker may cause vibration of a component mechanically coupled to the loudspeaker, and in this case, the vibration monitoring device may monitor the vibration of such a component. For example, as shown in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, the vibration monitoring device <b>13</b>, <b>23</b> and the loudspeaker <b>11</b>, <b>21</b> are arranged on the housing <b>15</b>, <b>25</b> at the same time. When the loudspeaker <b>11</b>, <b>21</b> emits a sound, vibration of the housing <b>15</b>, <b>25</b> is caused, and the vibration monitoring device <b>13</b>, <b>23</b> monitors the vibration of the housing <b>15</b>, <b>25</b>. In one case, the vibration monitoring device includes an acceleration sensor and may monitor the vibration caused by the sound emitted by the loudspeaker.</p><p id="p-0038" num="0037">In an embodiment, both the vibration monitoring device and the loudspeaker are arranged inside the housing, while the microphone is arranged outside the housing. Thus, the vibration monitoring device can more accurately monitor the vibration caused by the sound emitted by the loudspeaker, and the voice signal emitted from the outside of the housing, for example, by the user to the microphone, is limited due to the presence of the housing.</p><p id="p-0039" num="0038">In an embodiment, the vibration monitoring device is closer to the loudspeaker than to the microphone in order to accurately detect the vibration caused by the sound from the loudspeaker so as to avoid being affected by the sound signal emitted by the user to the microphone.</p><p id="p-0040" num="0039">Additionally, although the apparatus according to the present disclosure has been described with reference to the smart sound box and the conference room sound apparatus, this is not restrictive, and the apparatus may also include a karaoke device. Although the housings <b>15</b>, <b>25</b> are shown in the embodiments of <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, it is not restrictive, and it is also conceivable that no such housing is provided. For example, the vibration monitoring device is arranged directly on the loudspeaker, or the vibration monitoring device is arranged on another component connected to the loudspeaker and in the vicinity of the loudspeaker.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a method <b>100</b> for removing noise from a signal from a microphone according to an embodiment. The method <b>100</b> may be performed by the processor <b>14</b> and <b>24</b> as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> to remove the noise from the signal received by the microphone while the microphone and the loudspeaker are simultaneously operating.</p><p id="p-0042" num="0041">According to the method <b>100</b>, in step <b>110</b>, a vibration signal from a vibration monitoring device <b>13</b>, <b>23</b> is received, which vibration signal indicates vibration caused by a sound emitted by a loudspeaker <b>11</b>, <b>21</b>. In step <b>120</b>, a sound signal received by the microphone <b>12</b>, <b>22</b> is received. In step <b>130</b>, the vibration signal is removed from the sound signal received by the microphone <b>12</b>, <b>22</b> so as to remove, from the sound signal received by the microphone, noise corresponding to the sound emitted by the loudspeaker.</p><p id="p-0043" num="0042">It may be understood that in order to make the vibration signal correspond to the sound signal from the microphone, the time delay between respective signals of the microphone and the loudspeaker may be substantially negligible, considering that the distance between the microphone and the loudspeaker is very close in the application environment, and therefore steps <b>110</b> and <b>120</b> may be performed simultaneously to obtain the vibration signal and the sound signal corresponding to each other. Of course the time delay between them may also be taken into account when the sound signal is processed on the basis of the vibration signal.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a method <b>200</b> for voice recognition according to an embodiment of the present disclosure. The method <b>200</b> may be performed by the smart sound box as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> to recognize the voice signal from the user.</p><p id="p-0045" num="0044">According to the method <b>200</b> as previously described, it is first determined in step <b>205</b> whether the loudspeaker is in use. When it is determined in step <b>205</b> that the loudspeaker is not in use, the method proceeds to step <b>240</b>, where only the sound signal from the microphone is received, and optionally the sound signal is processed, and then voice recognition is performed on the basis of the sound signal from the microphone in step <b>250</b>. For example, when the smart sound box does not play any audio content through the loudspeaker, and the user sends a voice to the smart sound box, &#x201c;What's the weather like today?&#x201d;, the smart sound box will directly receive the voice information and perform voice recognition on same.</p><p id="p-0046" num="0045">When it is determined in step <b>205</b> that the loudspeaker is in use, the method proceeds to steps <b>210</b> and <b>220</b>, where both the sound signal from the microphone and the vibration signal from the vibration monitoring device are received, and the sound signal is processed on the basis of the received vibration signal in step <b>230</b> to remove noise generated by the sound emitted by the loudspeaker from the sound signal. The operation of steps <b>210</b>-<b>230</b> is similar to the steps of the method described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0047" num="0046">Next, in step <b>250</b>, voice recognition is performed on the basis of the processed sound signal, whereby the smart sound box can obtain an accurate voice command from the user.</p><p id="p-0048" num="0047">Although not shown in <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>4</b></figref>, it may also be expected that first the vibration signal is processed such that same is converted into the corresponding sound signal, and then the sound signal from the microphone is processed on the basis of the converted sound signal. For example, the amplitude and phase of the vibration signal can be adjusted with reference to the amplitude and phase of the sound signal from the microphone to obtain the amplitude and phase of the corresponding sound signal, and then the adjusted vibration signal is removed from the sound signal received by the microphone, thereby removing noise in the sound signal.</p><p id="p-0049" num="0048">The embodiments of the method and apparatus of the present disclosure have been described above with reference to <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>4</b></figref>, and these embodiments can be combined with each other to achieve different effects regardless of the subject matter type. Furthermore, the various units/steps/processing mentioned above are not restrictive, and the functions thereof may be incorporated/combined/changed/modified to obtain corresponding effects.</p><p id="p-0050" num="0049">In the present disclosure, the functions of the units of the apparatus for removing noise from the signal received by the microphone may be implemented by software or corresponding hardware, or by means of a processor. For example, the processor may read computer programs in a memory and run these computer programs to implement the functions of the above units. In an embodiment, the apparatus of the present disclosure may also be implemented by a memory and a processor.</p><p id="p-0051" num="0050">In an embodiment, the functions of the apparatus may be implemented in a processor of a mobile device, or at a remote location relative to the mobile device.</p><p id="p-0052" num="0051">It may be understood that the method and apparatus according to the embodiments of the present disclosure may be implemented by computer programs/software. Such software may be loaded into a working memory of a microprocessor and configured to execute, when being run, the method according to the embodiments of the present disclosure.</p><p id="p-0053" num="0052">The exemplary embodiments of the present disclosure cover both of the following: computer programs/software of the present disclosure created/used from the very beginning, and available programs/software that have been converted for use as the computer programs/software of the present disclosure by means of updating.</p><p id="p-0054" num="0053">According to another embodiment of the present disclosure, a computer program product, for example, a machine (such as a computer)-readable medium, such as a CD-ROM, is provided, where computer program code is included, and the computer program code, when executed, enables a computer or a processor to perform the method according to the embodiments of the present disclosure. The machine-readable medium is, for example, an optical storage medium or a solid-state medium supplied with other hardware or as part of other hardware.</p><p id="p-0055" num="0054">The computer programs for performing the method according to the embodiments of the present disclosure may alternatively be published in another form, for example, via the Internet or other wired or wireless telecommunication systems.</p><p id="p-0056" num="0055">The computer programs may alternatively be provided on a network such as the World Wide Web, and can be downloaded from such a network to a working computer of a microprocessor.</p><p id="p-0057" num="0056">It must be pointed out that the embodiments of the present disclosure are described with reference to different subject matters. In particular, some embodiments are described with reference to method-type claims, while other embodiments are described with reference to device-type claims. However, those skilled in the art will learn from the above and following descriptions that, unless otherwise specified, in addition to any combination of features belonging to one type of subject matter, any combination of features related to different subject matters is also deemed to be disclosed by the present application. Moreover, all the features can be combined to provide a synergistic effect that is greater than the simple addition of the features.</p><p id="p-0058" num="0057">Specific embodiments of the present disclosure have been described above. Other embodiments are within the scope of the appended claims. In some cases, actions or steps described in the claims may be performed in an order different from that in the embodiments and desired results can still be achieved. In addition, the processes described in the accompanying drawings do not necessarily require the specific order or sequential order shown to achieve the desired results. In some embodiments, multitasking and parallel processing are also possible or may be advantageous.</p><p id="p-0059" num="0058">The present disclosure is described above with reference to specific embodiments. Those skilled in the art should understand that the technical solutions of the present disclosure can be implemented in various ways without departing from the spirit and basic features of the present disclosure. Specific embodiments are only schematic rather than limiting. In addition, these embodiments can be combined arbitrarily to achieve the purpose of the present disclosure. The scope of protection of the present disclosure is defined by the appended claims.</p><p id="p-0060" num="0059">The word &#x201c;include/comprise&#x201d; in the specification and the claims does not exclude the existence of other elements or steps, and the presentation of various &#x201c;steps&#x201d; and the order of the various steps shown in the drawings limit neither the sequence nor the number thereof. The functions of the elements recorded in this specification or in the claims may alternatively be divided or combined, and implemented by a plurality of corresponding elements or a single element.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for removing noise from a sound signal received by a microphone, comprising:<claim-text>receiving a vibration signal from a vibration monitoring device mechanically connected to a loudspeaker, the vibration signal indicating vibration caused by a sound emitted by the loudspeaker;</claim-text><claim-text>receiving a sound signal received by the microphone; and</claim-text><claim-text>removing the vibration signal from the sound signal so as to remove noise from the sound signal.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>adjusting the amplitude, phase and/or bandwidth of the vibration signal,</claim-text><claim-text>wherein removing the vibration signal from the sound signal comprises removing the adjusted vibration signal from the sound signal.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. An apparatus for removing noise from a sound signal received by a microphone, comprising:<claim-text>a receiving unit, configured to receive a vibration signal from a vibration monitoring device mechanically connected to a loudspeaker, and a sound signal received by the microphone, the vibration signal indicating vibration caused by a sound emitted by the loudspeaker; and</claim-text><claim-text>a processing unit, configured to remove the vibration signal from the sound signal so as to remove noise from the sound signal.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the processing unit is configured to adjust the amplitude, phase and/or bandwidth of the vibration signal and remove the adjusted vibration signal from the sound signal.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. An apparatus for sound collection and playback, comprising:<claim-text>a loudspeaker;</claim-text><claim-text>a microphone;</claim-text><claim-text>a vibration monitoring device, mechanically connected to the loudspeaker and being configured to generate a vibration signal indicating vibration caused by a sound emitted by the loudspeaker; and</claim-text><claim-text>a processor, configured to perform the method of <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The apparatus according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the vibration monitoring device is mechanically connected directly or indirectly to the loudspeaker to monitor vibration coupled from the loudspeaker to the vibration monitoring device by way of mechanical vibration coupling.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The apparatus according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the vibration monitoring device comprises an acceleration sensor.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The apparatus according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising a housing for accommodating the loudspeaker, the vibration monitoring device being arranged inside the housing to monitor vibration of the housing caused by the sound emitted by the loudspeaker,<claim-text>wherein the microphone is arranged outside the housing.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The apparatus according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the distance between the vibration monitoring device and the microphone is greater than that between the vibration monitoring device and the loudspeaker.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The apparatus according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the apparatus is a smart sound box or a smart television, and the processor is further configured to perform voice recognition on the basis of the noise-removed sound signal.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The apparatus according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising a karaoke device and a conference room sound device, wherein the processor is further configured to additionally process the noise-removed sound signal so as to emit the corresponding sound through the loudspeaker.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A computer program product, comprising computer instructions that, when executed by a processor, run the method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim></claims></us-patent-application>