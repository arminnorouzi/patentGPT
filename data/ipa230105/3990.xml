<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230003991A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230003991</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17939041</doc-number><date>20220907</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>02</class><subclass>B</subclass><main-group>23</main-group><subgroup>24</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>B</subclass><main-group>11</main-group><subgroup>03</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>B</subclass><main-group>11</main-group><subgroup>24</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>B</subclass><main-group>11</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>593</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>23</main-group><subgroup>2415</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>B</subclass><main-group>11</main-group><subgroup>03</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>B</subclass><main-group>11</main-group><subgroup>24</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>B</subclass><main-group>11</main-group><subgroup>026</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0012</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>593</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SURFACE ESTIMATION METHOD, SURFACE ESTIMATION DEVICE, AND RECORDING MEDIUM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/JP2020/009973</doc-number><date>20200309</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17939041</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>Evident Corporation</orgname><address><city>Nagano</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>YAMAMOTO</last-name><first-name>Naoki</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Evident Corporation</orgname><role>03</role><address><city>Nagano</city><country>JP</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A surface estimation method includes a region-setting step and an estimation step. In the region-setting step, a reference region that is one of a three-dimensional region and a two-dimensional region is set. The three-dimensional region includes three or more points and is set in a three-dimensional space. The three-dimensional space includes three-dimensional coordinates of three or more points on a subject calculated on the basis of a two-dimensional image of the subject. The three-dimensional coordinates of the three or more points are included in three-dimensional image data. The two-dimensional region includes three or more points and is set in the two-dimensional image. In the estimation step, a reference surface that approximates a surface of the subject is estimated on the basis of three or more points of the three-dimensional image data corresponding to the three or more points included in the reference region.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="99.74mm" wi="122.43mm" file="US20230003991A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="192.02mm" wi="80.09mm" file="US20230003991A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="224.45mm" wi="106.17mm" file="US20230003991A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="92.12mm" wi="53.34mm" file="US20230003991A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="151.98mm" wi="111.25mm" file="US20230003991A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="221.83mm" wi="155.53mm" orientation="landscape" file="US20230003991A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="235.46mm" wi="124.46mm" file="US20230003991A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="231.90mm" wi="108.88mm" file="US20230003991A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="222.17mm" wi="108.71mm" file="US20230003991A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="220.73mm" wi="114.30mm" file="US20230003991A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="166.03mm" wi="113.54mm" file="US20230003991A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="168.99mm" wi="113.54mm" file="US20230003991A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="125.39mm" wi="134.96mm" file="US20230003991A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="165.78mm" wi="124.46mm" file="US20230003991A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="180.51mm" wi="95.84mm" file="US20230003991A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="199.14mm" wi="94.66mm" file="US20230003991A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="216.24mm" wi="127.08mm" file="US20230003991A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="217.42mm" wi="136.82mm" file="US20230003991A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="213.02mm" wi="74.25mm" file="US20230003991A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="211.07mm" wi="130.47mm" file="US20230003991A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="165.78mm" wi="124.46mm" file="US20230003991A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="154.52mm" wi="98.21mm" file="US20230003991A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="231.39mm" wi="111.08mm" file="US20230003991A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="110.07mm" wi="111.00mm" file="US20230003991A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="154.52mm" wi="98.21mm" file="US20230003991A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="110.15mm" wi="126.07mm" file="US20230003991A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="167.56mm" wi="98.21mm" file="US20230003991A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="222.00mm" wi="139.19mm" file="US20230003991A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="222.25mm" wi="93.56mm" file="US20230003991A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="152.74mm" wi="124.46mm" file="US20230003991A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00030" num="00030"><img id="EMI-D00030" he="143.51mm" wi="98.21mm" file="US20230003991A1-20230105-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00031" num="00031"><img id="EMI-D00031" he="110.07mm" wi="141.90mm" file="US20230003991A1-20230105-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00032" num="00032"><img id="EMI-D00032" he="149.44mm" wi="124.54mm" file="US20230003991A1-20230105-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00033" num="00033"><img id="EMI-D00033" he="231.48mm" wi="139.19mm" file="US20230003991A1-20230105-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00034" num="00034"><img id="EMI-D00034" he="234.53mm" wi="124.46mm" file="US20230003991A1-20230105-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00035" num="00035"><img id="EMI-D00035" he="211.41mm" wi="73.32mm" file="US20230003991A1-20230105-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00036" num="00036"><img id="EMI-D00036" he="152.91mm" wi="124.46mm" file="US20230003991A1-20230105-D00036.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00037" num="00037"><img id="EMI-D00037" he="147.91mm" wi="98.21mm" file="US20230003991A1-20230105-D00037.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00038" num="00038"><img id="EMI-D00038" he="204.13mm" wi="124.54mm" file="US20230003991A1-20230105-D00038.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00039" num="00039"><img id="EMI-D00039" he="153.67mm" wi="124.46mm" file="US20230003991A1-20230105-D00039.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00040" num="00040"><img id="EMI-D00040" he="114.05mm" wi="124.46mm" file="US20230003991A1-20230105-D00040.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00041" num="00041"><img id="EMI-D00041" he="133.52mm" wi="98.55mm" file="US20230003991A1-20230105-D00041.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00042" num="00042"><img id="EMI-D00042" he="218.52mm" wi="138.85mm" file="US20230003991A1-20230105-D00042.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00043" num="00043"><img id="EMI-D00043" he="208.28mm" wi="145.29mm" file="US20230003991A1-20230105-D00043.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00044" num="00044"><img id="EMI-D00044" he="218.27mm" wi="93.30mm" file="US20230003991A1-20230105-D00044.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00045" num="00045"><img id="EMI-D00045" he="210.65mm" wi="100.08mm" file="US20230003991A1-20230105-D00045.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00046" num="00046"><img id="EMI-D00046" he="172.38mm" wi="142.83mm" file="US20230003991A1-20230105-D00046.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><p id="p-0002" num="0001">The present application is a continuation application based on International Patent Application No. PCT/JP2020/009973 filed on Mar. 9, 2020, the content of which is incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading><heading id="h-0002" level="1">Field of the Invention</heading><p id="p-0003" num="0002">The present invention relates to a surface estimation method, a surface estimation device, and a recording medium.</p><heading id="h-0003" level="1">Description of Related Art</heading><p id="p-0004" num="0003">Industrial endoscope devices have been used for observation and inspection of internal damage, corrosion, and the like of boilers, pipes, aircraft engines, and the like. In such an endoscope device, multiple types of optical adapters for observing and inspecting various objects to be observed are prepared. Optical adapters are attached to the distal ends of endoscopes and are exchangeable. In an inspection using such an endoscope device, there is a desire to quantitatively measure the size of a defect, damage, or the like of a subject. To meet such a desire, there is an endoscope device provided with a three-dimensional measurement function.</p><p id="p-0005" num="0004">For example, an endoscope device has a function of measuring geometric sizes of a subject on the basis of information of a point designated on an image by a user. For example, in a surface-based measurement, a reference surface is estimated on the basis of three or more points designated by a user, and the three-dimensional distance between a measurement point designated by a user and the reference surface is measured. The reference surface approximates the surface of a subject.</p><p id="p-0006" num="0005">The device disclosed in Japanese Unexamined Patent Application, First Publication No. 2017-162452 uses a plane or a curved surface as a reference surface and executes the surface-based measurement. <figref idref="DRAWINGS">FIG. <b>67</b></figref> and <figref idref="DRAWINGS">FIG. <b>68</b></figref> show an example in which three or more points used for estimating a reference surface in Japanese Unexamined Patent Application, First Publication No. 2017-162452 are set in an image G<b>301</b> of a subject.</p><p id="p-0007" num="0006">In <figref idref="DRAWINGS">FIG. <b>67</b></figref>, a point P<b>301</b>, a point P<b>302</b>, a point P<b>303</b>, and a point P<b>304</b> are set in the image G<b>301</b>. Each of the four points is disposed at the vertex of a square. In <figref idref="DRAWINGS">FIG. <b>68</b></figref>, two or more points P<b>311</b> on a line L<b>311</b> and two or more points P<b>312</b> on a line L<b>312</b> are set in the image G<b>301</b>.</p><heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0008" num="0007">According to a first aspect of the present invention, a surface estimation method executed by a processor includes a region-setting step and an estimation step. In the region-setting step, a reference region that is one of a three-dimensional region and a two-dimensional region is set. The three-dimensional region includes three or more points and is set in a three-dimensional space. The three-dimensional space includes three-dimensional coordinates of three or more points on a subject calculated on the basis of a two-dimensional image of the subject. The three-dimensional coordinates of the three or more points are included in three-dimensional image data. The two-dimensional region includes three or more points and is set in the two-dimensional image. The three or more points of the reference region include one or more combinations, each of which is constituted by three points that form a triangle. In the estimation step, a reference surface that approximates a surface of the subject is estimated on the basis of three or more points of the three-dimensional image data corresponding to the three or more points included in the reference region.</p><p id="p-0009" num="0008">According to a second aspect of the present invention, in the first aspect, the reference region that is the three-dimensional region may be set in the three-dimensional space in the region-setting step.</p><p id="p-0010" num="0009">According to a third aspect of the present invention, in the first aspect, the reference region that is the two-dimensional region may be set in the two-dimensional image in the region-setting step.</p><p id="p-0011" num="0010">According to a fourth aspect of the present invention, in the second aspect, continuity of the three or more points corresponding to the three-dimensional coordinates included in the three-dimensional image data may be determined in the region-setting step. The reference region may include only the three or more points determined to be continuous.</p><p id="p-0012" num="0011">According to a fifth aspect of the present invention, in the first aspect, the reference region may include a region that is not a convex set.</p><p id="p-0013" num="0012">According to a sixth aspect of the present invention, in the first aspect, the reference region may include two or more regions.</p><p id="p-0014" num="0013">According to a seventh aspect of the present invention, in the first aspect, the surface estimation method may further include an image display step, a position input step, and a state determination step. In the image display step, one of an image of the three-dimensional image data and the two-dimensional image may be displayed on a display. In the position input step, position information input through an input device may be accepted. The position information may indicate a position on the image of the three-dimensional image data or the two-dimensional image displayed on the display. In the state determination step, a state of the subject may be determined. In the region-setting step, a boundary of the reference region may be determined on the basis of both the position indicated by the position information and the state.</p><p id="p-0015" num="0014">According to an eighth aspect of the present invention, in the fifth aspect, a boundary of the reference region may include a first boundary and a second boundary that is on an inner side of the first boundary.</p><p id="p-0016" num="0015">According to a ninth aspect of the present invention, in the first aspect, the three or more points corresponding to the three-dimensional coordinates included in the three-dimensional image data may be divided into two or more regions in the region-setting step. A boundary of the reference region may include a boundary of one or more regions included in the two or more regions.</p><p id="p-0017" num="0016">According to a tenth aspect of the present invention, in the sixth aspect, region information indicating at least one of a size of the reference region and a position of the reference region may be recorded on a recording medium in advance. At least one of the size of the reference region and the position of the reference region may be set on the basis of the region information in the region-setting step.</p><p id="p-0018" num="0017">According to an eleventh aspect of the present invention, in the sixth aspect, the surface estimation method may further include an image display step of displaying one of an image of the three-dimensional image data and the two-dimensional image on a display. In the region-setting step, three or more points on the image of the three-dimensional image data or the two-dimensional image may be input through an input device. In the region-setting step, the reference region including the input three or more points may be set.</p><p id="p-0019" num="0018">According to a twelfth aspect of the present invention, in the eleventh aspect, the reference region may be set on the basis of line segments connecting the input three or more points together in the region-setting step.</p><p id="p-0020" num="0019">According to a thirteenth aspect of the present invention, in the sixth aspect, the surface estimation method may further include an image display step of displaying one of an image of the three-dimensional image data and the two-dimensional image on a display. In the region-setting step, three or more points on a line designated in the image of the three-dimensional image data or the two-dimensional image by a user may be input through an input device. In the region-setting step, the reference region including the three or more points on the line may be set.</p><p id="p-0021" num="0020">According to a fourteenth aspect of the present invention, in the sixth aspect, the three or more points may be selected from four or more points included in the reference region in the estimation step. In the estimation step, the reference surface may be estimated on the basis of three or more points of the three-dimensional image data corresponding to the three or more selected points.</p><p id="p-0022" num="0021">According to a fifteenth aspect of the present invention, in the sixth aspect, the surface estimation method may further include an image display step of displaying one of an image of the three-dimensional image data and the two-dimensional image on a display and displaying the reference region on the image of the three-dimensional image data or the two-dimensional image.</p><p id="p-0023" num="0022">According to a sixteenth aspect of the present invention, in the sixth aspect, the surface estimation method may further include an image display step of displaying an image of the three-dimensional image data on a display and displaying, on the image of the three-dimensional image data, the three or more points of the three-dimensional image data used for estimating the reference surface.</p><p id="p-0024" num="0023">According to a seventeenth aspect of the present invention, in the sixth aspect, the surface estimation method may further include an image display step of displaying the two-dimensional image on a display and displaying, on the two-dimensional image, three or more points corresponding to the three or more points of the three-dimensional image data used for estimating the reference surface.</p><p id="p-0025" num="0024">According to an eighteenth aspect of the present invention, in the first aspect, the surface estimation method may further include a division step and an image display step. In the division step, three or more points corresponding to the three-dimensional coordinates included in the three-dimensional image data may be divided into two or more regions. In the image display step, one of an image of the three-dimensional image data and the two-dimensional image may be displayed on a display and an image of the two or more regions may be displayed on the display.</p><p id="p-0026" num="0025">According to a nineteenth aspect of the present invention, in the sixth aspect, the surface estimation method may further include a map generation step and an image display step. In the map generation step, a curvature map indicating distribution of curvatures in a three-dimensional shape of the subject indicated by the three-dimensional image data may be generated. In the image display step, one of an image of the three-dimensional image data and the two-dimensional image may be displayed on a display and an image of the curvature map may be displayed on the display.</p><p id="p-0027" num="0026">According to a twentieth aspect of the present invention, in the sixth aspect, the surface estimation method may further include an image display step of displaying one of an image of the three-dimensional image data and the two-dimensional image on a display and displaying a region corresponding to the reference surface on the image of the three-dimensional image data or the two-dimensional image.</p><p id="p-0028" num="0027">According to a twenty-first aspect of the present invention, in the sixth aspect, the surface estimation method may further include an abnormality detection step of detecting an abnormal region on the surface of the subject on the basis of one of an image of the three-dimensional image data and the two-dimensional image. In the region-setting step, the reference region excluding a region corresponding to the abnormal region may be set.</p><p id="p-0029" num="0028">According to a twenty-second aspect of the present invention, in the sixth aspect, the surface estimation method may further include a measurement step of measuring a size of the subject on the basis of the reference surface.</p><p id="p-0030" num="0029">According to a twenty-third aspect of the present invention, in the twenty-second aspect, a three-dimensional distance between the reference surface and a point on the surface of the subject may be measured in the measurement step.</p><p id="p-0031" num="0030">According to a twenty-fourth aspect of the present invention, in the sixth aspect, a geometric feature of the reference surface may be estimated in the estimation step.</p><p id="p-0032" num="0031">According to a twenty-fifth aspect of the present invention, a surface estimation device includes a processor. The processor is configured to set a reference region that is one of a three-dimensional region and a two-dimensional region. The three-dimensional region includes three or more points and is set in a three-dimensional space. The three-dimensional space includes three-dimensional coordinates of three or more points on a subject calculated on the basis of a two-dimensional image of the subject. The three-dimensional coordinates of the three or more points are included in three-dimensional image data. The two-dimensional region includes three or more points and is set in the two-dimensional image. The three or more points of the reference region include one or more combinations, each of which is constituted by three points that form a triangle. The processor is configured to estimate a reference surface that approximates a surface of the subject on the basis of three or more points of the three-dimensional image data corresponding to the three or more points included in the reference region.</p><p id="p-0033" num="0032">According to a twenty-sixth aspect of the present invention, a non-transitory computer-readable recording medium saves a program causing a computer to execute a region-setting step and an estimation step. In the region-setting step, a reference region that is one of a three-dimensional region and a two-dimensional region is set. The three-dimensional region includes three or more points and is set in a three-dimensional space. The three-dimensional space includes three-dimensional coordinates of three or more points on a subject calculated on the basis of a two-dimensional image of the subject. The three-dimensional coordinates of the three or more points are included in three-dimensional image data. The two-dimensional region includes three or more points and is set in the two-dimensional image. The three or more points of the reference region include one or more combinations, each of which is constituted by three points that form a triangle. In the estimation step, a reference surface that approximates a surface of the subject is estimated on the basis of three or more points of the three-dimensional image data corresponding to the three or more points included in the reference region.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram showing a configuration of a surface estimation device according to a first embodiment of the present invention.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flow chart showing a procedure of surface estimation processing in the first embodiment of the present invention.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram showing an example of a three-dimensional image of point-cloud data in the first embodiment of the present invention.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram schematically showing the point-cloud data in the first embodiment of the present invention.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram showing an example of a 2D image in the first embodiment of the present invention.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a perspective view showing an entire configuration of an endoscope device according to a second embodiment of the present invention.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram showing an internal configuration of the endoscope device according to the second embodiment of the present invention.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a block diagram showing a functional configuration of a CPU included in the endoscope device according to the second embodiment of the present invention.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flow chart showing a procedure of surface estimation processing in the second embodiment of the present invention.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram showing an example of an image displayed on a display unit in the second embodiment of the present invention.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram showing an example of an image displayed on the display unit in the second embodiment of the present invention.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram showing an example of an image displayed on the display unit in the second embodiment of the present invention.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram showing an example of an image displayed on the display unit in the second embodiment of the present invention.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram showing an example of an image displayed on a display unit in a first modified example of the second embodiment of the present invention.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a diagram showing an example of an image displayed on the display unit in the first modified example of the second embodiment of the present invention.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a diagram showing an example of an image displayed on the display unit in the first modified example of the second embodiment of the present invention.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a diagram showing an example of a reference region in a second modified example of the second embodiment of the present invention.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram showing an example of the reference region in the second modified example of the second embodiment of the present invention.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a diagram showing an example of an image displayed on a display unit in a third modified example of the second embodiment of the present invention.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a block diagram showing a functional configuration of a CPU included in an endoscope device according to a third embodiment of the present invention.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a flow chart showing a procedure of three-dimensional measurement in the third embodiment of the present invention.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a diagram showing an example of a reference region in a fourth embodiment of the present invention.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a diagram showing an example of the reference region in the fourth embodiment of the present invention.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a diagram showing an example of a reference region in the fourth embodiment of the present invention.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a diagram showing an example of the reference region in the fourth embodiment of the present invention.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a diagram showing an example of the reference region in the fourth embodiment of the present invention.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a diagram showing an example of the reference region in the fourth embodiment of the present invention.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is a diagram showing an example of points used for estimating a reference surface in the fourth embodiment of the present invention.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a diagram showing an example of points used for estimating the reference surface in the fourth embodiment of the present invention.</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>30</b></figref> is a diagram showing an example of a reference region in a fifth embodiment of the present invention.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>31</b></figref> is a diagram showing an example of the reference region in the fifth embodiment of the present invention.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>32</b></figref> is a block diagram showing a functional configuration of a CPU included in an endoscope device according to a sixth embodiment of the present invention.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>33</b></figref> is a flow chart showing a procedure of surface estimation processing in the sixth embodiment of the present invention.</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>34</b></figref> is a diagram showing an example of an image displayed on a display unit in the sixth embodiment of the present invention.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>35</b></figref> is a diagram showing two or more regions on a three-dimensional image of point-cloud data in the sixth embodiment of the present invention.</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>36</b></figref> is a diagram showing an example of an image displayed on the display unit in the sixth embodiment of the present invention.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>37</b></figref> is a flow chart showing a procedure of surface estimation processing in a first modified example of the sixth embodiment of the present invention.</p><p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. <b>38</b></figref> is a diagram showing an example of an image displayed on a display unit in the first modified example of the sixth embodiment of the present invention.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>39</b></figref> is a flow chart showing a procedure of surface estimation processing in a third modified example of the sixth embodiment of the present invention.</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. <b>40</b></figref> is a diagram showing an example of an image displayed on a display unit in the third modified example of the sixth embodiment of the present invention.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>41</b></figref> is a diagram showing an example of an image displayed on a display unit in a seventh embodiment of the present invention.</p><p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. <b>42</b></figref> is a diagram showing an example of an image displayed on the display unit in the seventh embodiment of the present invention.</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. <b>43</b></figref> is a diagram showing an example of an image displayed on a display unit in an eighth embodiment of the present invention.</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>44</b></figref> is a block diagram showing a functional configuration of a CPU included in an endoscope device according to a ninth embodiment of the present invention.</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>45</b></figref> is a flow chart showing a procedure of surface estimation processing in the ninth embodiment of the present invention.</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>46</b></figref> is a diagram showing an example of an image displayed on a display unit in the ninth embodiment of the present invention.</p><p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. <b>47</b></figref> is a block diagram showing a functional configuration of a CPU included in an endoscope device according to a modified example of the ninth embodiment of the present invention.</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>48</b></figref> is a flow chart showing a procedure of surface estimation processing in the modified example of the ninth embodiment of the present invention.</p><p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. <b>49</b></figref> is a diagram showing an example of an image displayed on a display unit in the modified example of the ninth embodiment of the present invention.</p><p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. <b>50</b></figref> is a block diagram showing a functional configuration of a CPU included in an endoscope device according to a tenth embodiment of the present invention.</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>51</b></figref> is a flow chart showing a procedure of surface estimation processing in the tenth embodiment of the present invention.</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>52</b></figref> is a diagram showing an example of an image displayed on a display unit in the tenth embodiment of the present invention.</p><p id="p-0086" num="0085"><figref idref="DRAWINGS">FIG. <b>53</b></figref> is a diagram showing an example of an image displayed on the display unit in the tenth embodiment of the present invention.</p><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. <b>54</b></figref> is a block diagram showing a functional configuration of a CPU included in an endoscope device according to an eleventh embodiment of the present invention.</p><p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. <b>55</b></figref> is a flow chart showing a procedure of surface estimation processing in the eleventh embodiment of the present invention.</p><p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. <b>56</b></figref> is a diagram showing an example of an image displayed on a display unit in the eleventh embodiment of the present invention.</p><p id="p-0090" num="0089"><figref idref="DRAWINGS">FIG. <b>57</b></figref> is a block diagram showing a configuration of a personal computer according to a twelfth embodiment of the present invention.</p><p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. <b>58</b></figref> is a block diagram showing a functional configuration of a CPU included in the personal computer according to the twelfth embodiment of the present invention.</p><p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. <b>59</b></figref> is a block diagram showing a functional configuration of a CPU included in an endoscope device according to an embodiment of an invention related to the present invention.</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. <b>60</b></figref> is a flow chart showing a procedure of processing of detecting a region in which a factor of interest has occurred in the embodiment of the invention related to the present invention.</p><p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. <b>61</b></figref> is a diagram showing an example of a stereo image in the embodiment of the invention related to the present invention.</p><p id="p-0095" num="0094"><figref idref="DRAWINGS">FIG. <b>62</b></figref> is a diagram showing an example of an image in which a triangle is set in the embodiment of the invention related to the present invention.</p><p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. <b>63</b></figref> is a diagram showing a triangle that is set in each of two images included in a stereo image in the embodiment of the invention related to the present invention.</p><p id="p-0097" num="0096"><figref idref="DRAWINGS">FIG. <b>64</b></figref> is a diagram showing an example of an image included in a stereo image in the embodiment of the invention related to the present invention.</p><p id="p-0098" num="0097"><figref idref="DRAWINGS">FIG. <b>65</b></figref> is a diagram showing an example of an image included in a stereo image in the embodiment of the invention related to the present invention.</p><p id="p-0099" num="0098"><figref idref="DRAWINGS">FIG. <b>66</b></figref> is a diagram showing an example of a region in which a factor of interest has occurred in the embodiment of the invention related to the present invention.</p><p id="p-0100" num="0099"><figref idref="DRAWINGS">FIG. <b>67</b></figref> is a diagram showing an example in which a point used for estimating a reference surface is set in prior art.</p><p id="p-0101" num="0100"><figref idref="DRAWINGS">FIG. <b>68</b></figref> is a diagram showing an example in which a point used for estimating the reference surface is set in the prior art.</p><p id="p-0102" num="0101"><figref idref="DRAWINGS">FIG. <b>69</b></figref> is a diagram showing an example in which a point used for estimating the reference surface is set in the prior art.</p><p id="p-0103" num="0102"><figref idref="DRAWINGS">FIG. <b>70</b></figref> is a diagram showing an example in which a point used for estimating the reference surface is set in the prior art.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading><p id="p-0104" num="0103">In a case in which the surface of a subject is a curved surface, there is a possibility that the device disclosed in Japanese Unexamined Patent Application, First Publication No. 2017-162452 cannot accurately estimate a reference surface. <figref idref="DRAWINGS">FIG. <b>69</b></figref> and <figref idref="DRAWINGS">FIG. <b>70</b></figref> show an example in which an erroneous reference surface is estimated.</p><p id="p-0105" num="0104">In <figref idref="DRAWINGS">FIG. <b>69</b></figref>, a point P<b>321</b>, a point P<b>322</b>, and a point P<b>323</b> are set on the surface of a sphere SP<b>301</b>. When the sphere SP<b>301</b> is seen in a parallel direction to the Z-axis, each of the three points is disposed at the vertex of a triangle. In a case in which the heights of the three points in the Z-direction are the same, the three points are on a plane. Therefore, the plane is erroneously estimated as a reference surface.</p><p id="p-0106" num="0105">In <figref idref="DRAWINGS">FIG. <b>70</b></figref>, a line L<b>331</b> and a line L<b>332</b> are set on the surface of a cylinder CY<b>301</b>, and two or more points P<b>331</b> on the line L<b>331</b> and two or more points P<b>332</b> on the line L<b>332</b> are set. In a case in which the line L<b>331</b> and the line L<b>332</b> are parallel to the center axis of the cylinder CY<b>301</b>, the two or more points P<b>331</b> and the two or more points P<b>332</b> are on a plane. Therefore, the plane is erroneously estimated as a reference surface.</p><p id="p-0107" num="0106">Hereinafter, embodiments of the present invention will be described with reference to the drawings.</p><heading id="h-0007" level="1">First Embodiment</heading><p id="p-0108" num="0107"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a configuration of a surface estimation device <b>7</b> according to a first embodiment of the present invention. The surface estimation device <b>7</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> includes a generation unit <b>70</b>, a region-setting unit <b>71</b>, and a surface estimation unit <b>72</b>.</p><p id="p-0109" num="0108">The generation unit <b>70</b> calculates three-dimensional coordinates of three or more points on a subject on the basis of a two-dimensional image of the subject and generates point-cloud data (three-dimensional image data) including the three-dimensional coordinates of the three or more points (generation step). The region-setting unit <b>71</b> sets a reference region that is one of a three-dimensional region and a two-dimensional region (region-setting step). The three-dimensional region includes three or more points and is set in a three-dimensional space. The three-dimensional space includes the three-dimensional coordinates included in the point-cloud data. The two-dimensional region includes three or more points and is set in the two-dimensional image of the subject. The three or more points of the reference region include one or more combinations, each of which is constituted by three points that form a triangle. The surface estimation unit <b>72</b> estimates a reference surface that approximates the surface of the subject on the basis of three or more points of the point-cloud data corresponding to the three or more points included in the reference region (estimation step).</p><p id="p-0110" num="0109">Each unit shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> may be constituted by at least one of a processor and a logic circuit. For example, the processor is at least one of a central processing unit (CPU), a digital signal processor (DSP), and a graphics-processing unit (GPU). For example, the logic circuit is at least one of an application-specific integrated circuit (ASIC) and a field-programmable gate array (FPGA). Each unit shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> may include one or a plurality of processors. Each unit shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> may include one or a plurality of logic circuits.</p><p id="p-0111" num="0110">A computer of the surface estimation device <b>7</b> may read a program and execute the read program. The program includes commands defining the operations of the generation unit <b>70</b>, the region-setting unit <b>71</b>, and the surface estimation unit <b>72</b>. In other words, the functions of the generation unit <b>70</b>, the region-setting unit <b>71</b>, and the surface estimation unit <b>72</b> may be realized by software.</p><p id="p-0112" num="0111">The program described above may be recorded on a computer-readable recording medium. The program may be transmitted from a computer storing the program to the surface estimation device <b>7</b> through a transmission medium or transmission waves in a transmission medium. The &#x201c;transmission medium&#x201d; transmitting the program is a medium having a function of transmitting information. The medium having the function of transmitting information includes a network (communication network) such as the Internet and a communication circuit line (communication line) such as a telephone line. The program described above may realize some of the functions described above. In addition, the program described above may be a differential file (differential program). The functions described above may be realized by a combination of a program that has already been recorded in a computer and a differential program.</p><p id="p-0113" num="0112">Surface estimation processing in the first embodiment will be described by using <figref idref="DRAWINGS">FIG. <b>2</b></figref>. <figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a procedure of the surface estimation processing.</p><p id="p-0114" num="0113">The generation unit <b>70</b> calculates three-dimensional coordinates (3D coordinates) of three or more points on a subject on the basis of a two-dimensional image (2D image) of the subject and generates point-cloud data including the 3D coordinates of the three or more points (Step S<b>1</b>). Step S<b>1</b> corresponds to the generation step.</p><p id="p-0115" num="0114">In a case in which the 2D image of the subject is a stereo image, one 2D image includes an image of the subject seen from a first viewpoint and an image of the subject seen from a second viewpoint different from the first viewpoint. The generation unit <b>70</b> calculates 3D coordinates corresponding to each pixel of the 2D image. The generation unit <b>70</b> generates the point-cloud data including the 3D coordinates of the three or more points on the subject. The 3D coordinates of each of the three or more points in the point-cloud data are associated with a point on the 2D image. Specifically, the 3D coordinates in the point-cloud data are associated with a pixel on the 2D image. For example, the point-cloud data include the 3D coordinates and position information of the pixel on the 2D image.</p><p id="p-0116" num="0115">The generation unit <b>70</b> may calculate 3D coordinates of three or more points on a subject by using two or more images and by applying structure-from-motion (SfM). The generation unit <b>70</b> may calculate 3D coordinates of three or more points on a subject by using two or more 2D images of the subject on which two or more stripe patterns having different spatial phases are projected and by applying a phase-shift method. The generation unit <b>70</b> may calculate 3D coordinates of three or more points on a subject by using one 2D image of the subject on which patterned light having randomly disposed bright and dark parts is emitted. A method of generating the point-cloud data is not limited to the above-described methods.</p><p id="p-0117" num="0116">The generation unit <b>70</b> may generate a three-dimensional image (3D image) for displaying the point-cloud data on a display. The 3D image is an image of a three-dimensional shape (3D shape) indicated by the point-cloud data. The 3D image includes color data of each pixel. Each pixel of the 3D image is associated with the 3D coordinates. The generation unit <b>70</b> may generate a 3D image corresponding to each of two or more different viewpoints.</p><p id="p-0118" num="0117">After Step S<b>1</b>, the region-setting unit <b>71</b> sets a reference region that is one of a three-dimensional region and a two-dimensional region (Step S<b>2</b>). Step S<b>2</b> corresponds to the region-setting step.</p><p id="p-0119" num="0118"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an example of a 3D image of the point-cloud data. The generation unit <b>70</b> converts the point-cloud data into mesh data and adds texture to the mesh data. The generation unit <b>70</b> generates a 3D image G<b>1</b> by executing this processing. The generation unit <b>70</b> may generate a 3D image without converting the point-cloud data into the mesh data.</p><p id="p-0120" num="0119">In the 3D image G<b>1</b>, point-cloud data PD<b>1</b> to which texture is added are shown. In the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the region-setting unit <b>71</b> sets a three-dimensional reference region R<b>1</b>. The reference region R<b>1</b> is a cube and has volume. The reference region R<b>1</b> may be a cuboid, a sphere, a cylinder, or the like. The shape of the reference region R<b>1</b> is not limited to these examples.</p><p id="p-0121" num="0120">The generation unit <b>70</b> sets at least one of the position of the reference region R<b>1</b> and the size of the reference region R<b>1</b> on the basis of information input through an input device. Alternatively, the generation unit <b>70</b> automatically sets at least one of the position of the reference region R<b>1</b> and the size of the reference region R<b>1</b>. In the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the 3D image G<b>1</b> is used to show the reference region R<b>1</b>. The generation unit <b>70</b> does not need to generate the 3D image G<b>1</b>.</p><p id="p-0122" num="0121"><figref idref="DRAWINGS">FIG. <b>4</b></figref> schematically shows the point-cloud data in a three-dimensional space (3D space). The 3D coordinates in the point-cloud data are defined in a 3D space SP<b>1</b> and include an X-coordinate, a Y-coordinate, and a Z-coordinate. Two or more points CP<b>1</b> in the point-cloud data are shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In the example shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the region-setting unit <b>71</b> sets a three-dimensional reference region R<b>2</b> in the 3D space SP<b>1</b>. The reference region R<b>2</b> includes three or more points CP<b>1</b> in the point-cloud data. The three or more points CP<b>1</b> in the reference region R<b>2</b> three-dimensionally expand.</p><p id="p-0123" num="0122">The three or more points CP<b>1</b> in the reference region R<b>2</b> include one or more combinations, each of which is constituted by three points that form a triangle. For example, a point CP<b>1</b><i>a</i>, a point CP<b>1</b><i>b</i>, and a point CP<b>1</b><i>c </i>in the reference region R<b>2</b> form a triangle T<b>1</b>. In the reference region R<b>2</b>, there are many combinations, each of which is constituted by three points that form a triangle.</p><p id="p-0124" num="0123">Even when an arbitrary virtual straight line in the 3D space SP<b>1</b> passes through one or more points CP<b>1</b> in the reference region R<b>2</b>, there are always one or more points CP<b>1</b> that the virtual straight line does not pass through in the reference region R<b>2</b>. In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a virtual straight line L<b>1</b> and a virtual straight line L<b>2</b> are shown. Each of the virtual straight line L<b>1</b> and the virtual straight line L<b>2</b> passes through only some of the three or more points CP<b>1</b> in the reference region R<b>2</b>. An arbitrary virtual straight line passing through the reference region R<b>2</b> passes through only some of the three or more points CP<b>1</b> in the reference region R<b>2</b>.</p><p id="p-0125" num="0124"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an example of a 2D image of a subject. In the example shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the region-setting unit <b>71</b> sets a two-dimensional reference region R<b>3</b> in a 2D image G<b>2</b> of a subject. The reference region R<b>3</b> is a square and has area. The reference region R<b>3</b> may be a polygon having three or more vertices. The reference region R<b>3</b> may be a circle, an ellipse, or the like. The shape of the reference region R<b>3</b> is not limited to these examples.</p><p id="p-0126" num="0125">The generation unit <b>70</b> sets at least one of the position of the reference region R<b>3</b> and the size of the reference region R<b>3</b> on the basis of information input through an input device. Alternatively, the generation unit <b>70</b> automatically sets at least one of the position of the reference region R<b>3</b> and the size of the reference region R<b>3</b>.</p><p id="p-0127" num="0126">The reference region R<b>3</b> includes three or more points on the surface of a subject. A point on a 2D image of a subject is associated with the 3D coordinates of a point in the point-cloud data. Therefore, each of the three or more points in the reference region R<b>3</b> is associated with the 3D coordinates in the point-cloud data. The three or more points in the reference region R<b>3</b> two-dimensionally expand. Therefore, three or more points of the point-cloud data corresponding to the three or more points in the reference region R<b>3</b> three-dimensionally expand.</p><p id="p-0128" num="0127">After Step S<b>2</b>, the surface estimation unit <b>72</b> estimates a reference surface that approximates the surface of the subject on the basis of three or more points of the point-cloud data corresponding to the three or more points included in the reference region (Step S<b>3</b>). Step S<b>3</b> corresponds to the estimation step. When Step S<b>3</b> is executed, the surface estimation processing is completed.</p><p id="p-0129" num="0128">In a case in which a reference region is a three-dimensional region, the reference region includes three or more points in the point-cloud data. The surface estimation unit <b>72</b> acquires 3D coordinates of each of the three or more points from the point-cloud data. The surface estimation unit <b>72</b> estimates a reference surface on the basis of the 3D coordinates of the three or more points in the reference region.</p><p id="p-0130" num="0129">In a case in which a reference region is a two-dimensional region, the reference region includes three or more points on a 2D image of a subject. A point on a 2D image of a subject is associated with the 3D coordinates of a point in the point-cloud data. The surface estimation unit <b>72</b> acquires 3D coordinates associated with each of the three or more points in the reference region from the point-cloud data. The surface estimation unit <b>72</b> estimates a reference surface on the basis of the 3D coordinates of the three or more points of the point-cloud data corresponding to the three or more points in the reference region.</p><p id="p-0131" num="0130">For example, the surface estimation unit <b>72</b> estimates a reference surface that is a plane by using a least-squares method. The surface estimation unit <b>72</b> may estimate a reference surface by using an algorithm of robust estimation such as random sample consensus (RANSAC). The surface estimation unit <b>72</b> may estimate a reference surface by using machine learning such as deep learning. A reference surface may be a spherical surface, a cylindrical surface, a quadric surface, or the like. In a case in which the surface estimation unit <b>72</b> estimates a plane, the reference region includes three or more points. In a case in which the surface estimation unit <b>72</b> estimates a spherical surface, the reference region includes four or more points. In a case in which the surface estimation unit <b>72</b> estimates a cylindrical surface, the reference region includes five or more points. The shape of a reference surface is not limited to the above-described examples.</p><p id="p-0132" num="0131">The surface estimation device <b>7</b> may include a reading unit that reads point-cloud data including three-dimensional coordinates of three or more points on a subject calculated on the basis of a two-dimensional image of the subject instead of the generation unit <b>70</b>. The reading unit may read the point-cloud data from a recording medium without executing Step S<b>1</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> (reading step). A processing unit having a similar function to that of the generation unit <b>70</b> generates the point-cloud data and records the point-cloud data on the recording medium. A different device from the surface estimation device <b>7</b> may include the processing unit.</p><p id="p-0133" num="0132">In the first embodiment, the surface estimation device <b>7</b> sets a three-dimensional or two-dimensional reference region. The reference region includes three or more points that three-dimensionally or two-dimensionally expand. Therefore, three or more points of the point-cloud data used for estimating a reference surface are less likely to be biased on a plane or a straight line. Accordingly, the surface estimation device <b>7</b> can improve the accuracy of a reference surface.</p><heading id="h-0008" level="1">Second Embodiment</heading><p id="p-0134" num="0133">A second embodiment of the present invention will be described. Hereinafter, an example in which the surface estimation device is an endoscope device will be described. The surface estimation device has only to be a device having a surface estimation function and is not limited to an endoscope device. The surface estimation device may be built-in equipment mounted on a specific device or a system. The surface estimation device may operate in a cloud environment. A subject is an industrial product.</p><p id="p-0135" num="0134"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows an external appearance of an endoscope device <b>1</b> according to the second embodiment. <figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an internal configuration of the endoscope device <b>1</b>. The endoscope device <b>1</b> images a subject and generates an image. In order to observe various subjects, an inspector can perform replacement of an optical adaptor mounted at a distal end of an insertion unit <b>2</b>, selection of a built-in video-processing program, and addition of a video-processing program.</p><p id="p-0136" num="0135">The endoscope device <b>1</b> shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> includes the insertion unit <b>2</b>, a main body unit <b>3</b>, an operation unit <b>4</b>, and a display unit <b>5</b>.</p><p id="p-0137" num="0136">The insertion unit <b>2</b> is inserted into the inside of a subject. The insertion unit <b>2</b> has a long and thin bendable tube shape from a distal end <b>20</b> to a base end portion. The insertion unit <b>2</b> images a subject and outputs an imaging signal to the main body unit <b>3</b>. An optical adapter is mounted on the distal end <b>20</b> of the insertion unit <b>2</b>. For example, a single-eye optical adapter is mounted on the distal end <b>20</b> of the insertion unit <b>2</b>. The main body unit <b>3</b> is a control device including a housing unit that houses the insertion unit <b>2</b>. The operation unit <b>4</b> accepts an operation for the endoscope device <b>1</b> from a user. The display unit <b>5</b> includes a display screen and displays an image of a subject acquired by the insertion unit <b>2</b>, an operation menu, and the like on the display screen.</p><p id="p-0138" num="0137">The operation unit <b>4</b> is a user interface (input device). For example, the operation unit <b>4</b> is at least one of a button, a switch, a key, a mouse, a joystick, a touch pad, a track ball, and a touch panel. The display unit <b>5</b> is a monitor (display) such as a liquid crystal display (LCD). The display unit <b>5</b> may be a touch panel. In such a case, the operation unit <b>4</b> and the display unit <b>5</b> are integrated. A user touches the screen of the display unit <b>5</b> by using a part (for example, a finger) of the body or a tool.</p><p id="p-0139" num="0138">The main body unit <b>3</b> shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref> includes an endoscope unit <b>8</b>, a camera control unit (CCU) <b>9</b>, and a control device <b>10</b>. The endoscope unit <b>8</b> includes a light source device and a bending device not shown in the drawing. The light source device supplies illumination light that is necessary for observation. The bending device bends a bending mechanism built in the insertion unit <b>2</b>. An imaging device <b>28</b> is built in the distal end <b>20</b> of the insertion unit <b>2</b>. The imaging device <b>28</b> is an image sensor. The imaging device <b>28</b> photo-electrically converts an optical image of a subject formed by an optical adaptor and generates an imaging signal. The CCU <b>9</b> drives the imaging device <b>28</b>. The imaging signal output from the imaging device <b>28</b> is input into the CCU <b>9</b>. The CCU <b>9</b> performs preprocessing including amplification, noise elimination, and the like for the imaging signal acquired by the imaging device <b>28</b>. The CCU <b>9</b> converts the imaging signal on which the preprocessing is performed into a video signal such as an NTSC signal.</p><p id="p-0140" num="0139">The control device <b>10</b> includes a video-signal-processing circuit <b>12</b>, a read-only memory (ROM) <b>13</b>, a random-access memory (RAM) <b>14</b>, a card interface <b>15</b>, an external device interface <b>16</b>, a control interface <b>17</b>, and a central processing unit (CPU) <b>18</b>. The video-signal-processing circuit <b>12</b> performs predetermined video processing on the video signal output from the CCU <b>9</b>. For example, the video-signal-processing circuit <b>12</b> performs video processing related to improvement of visibility. For example, the video processing is color reproduction, gray scale correction, noise suppression, contour enhancement, and the like. The video-signal-processing circuit <b>12</b> combines the video signal output from the CCU <b>9</b> and a graphic image signal generated by the CPU <b>18</b>. The graphic image signal includes an image of the operation screen, measurement information, and the like. The measurement information includes a 3D image of the point-cloud data, a measurement result, or the like. The video-signal-processing circuit <b>12</b> outputs a combined video signal to the display unit <b>5</b>. In addition, the video-signal-processing circuit <b>12</b> outputs image data on the basis of the video signal output from the CCU <b>9</b> to the CPU <b>18</b>.</p><p id="p-0141" num="0140">The ROM <b>13</b> is a nonvolatile recording medium on which a program for the CPU <b>18</b> to control the operation of the endoscope device <b>1</b> is recorded. The RAM <b>14</b> is a volatile recording medium that temporarily stores information used by the CPU <b>18</b> for controlling the endoscope device <b>1</b>. The CPU <b>18</b> controls the operation of the endoscope device <b>1</b> on the basis of the program recorded on the ROM <b>13</b>.</p><p id="p-0142" num="0141">A memory card <b>42</b>, which is a removable recording medium, is connected to the card interface <b>15</b>. The card interface <b>15</b> inputs control-processing information, image information, and the like stored on the memory card <b>42</b> into the control device <b>10</b>. In addition, the card interface <b>15</b> records control-processing information, image information, and the like generated by the endoscope device <b>1</b> on the memory card <b>42</b>.</p><p id="p-0143" num="0142">An external device such as a USB device is connected to the external device interface <b>16</b>. For example, a personal computer (PC) <b>41</b> is connected to the external device interface <b>16</b>. The external device interface <b>16</b> transmits information to the PC <b>41</b> and receives information from the PC <b>41</b>. In this way, the monitor of the PC <b>41</b> can display information. In addition, by inputting an instruction into the PC <b>41</b>, a user can perform an operation related to control of the endoscope device <b>1</b>.</p><p id="p-0144" num="0143">The control interface <b>17</b> performs communication with the operation unit <b>4</b>, the endoscope unit <b>8</b>, and the CCU <b>9</b> for operation control. The control interface <b>17</b> notifies the CPU <b>18</b> of an instruction input into the operation unit <b>4</b> by a user. The control interface <b>17</b> outputs control signals used for controlling the light source device and the bending device to the endoscope unit <b>8</b>. The control interface <b>17</b> outputs a control signal used for controlling the imaging device <b>28</b> to the CCU <b>9</b>.</p><p id="p-0145" num="0144">A program executed by the CPU <b>18</b> may be recorded on a computer-readable recording medium. The program recorded on this recording medium may be read and executed by a computer other than the endoscope device <b>1</b>. For example, the program may be read and executed by the PC <b>41</b>. The PC <b>41</b> may control the endoscope device <b>1</b> by transmitting control information used for controlling the endoscope device <b>1</b> to the endoscope device <b>1</b> in accordance with the program. Alternatively, the PC <b>41</b> may acquire a video signal from the endoscope device <b>1</b> and may process the acquired video signal.</p><p id="p-0146" num="0145">As described above, the endoscope device <b>1</b> includes the imaging device <b>28</b> and the CPU <b>18</b>. The imaging device <b>28</b> images a subject and generates an imaging signal. The imaging signal includes an image of the subject. Accordingly, the imaging device <b>28</b> acquires the image of the subject generated by imaging the subject. The image acquired by the imaging device <b>28</b> is input into the CPU <b>18</b> via the video-signal-processing circuit <b>12</b>.</p><p id="p-0147" num="0146">The imaging device <b>28</b> has a function of an image acquisition unit that acquires an image of a subject. The image acquisition unit may be an image input device. For example, in a case in which the PC <b>41</b> operates as a surface estimation device, the image acquisition unit is a communication interface (communicator) that performs communication with the endoscope device <b>1</b>. The image acquisition unit may be a wireless communicator. The image acquisition unit may be a reading circuit that reads an image from a recording medium on which the image is recorded.</p><p id="p-0148" num="0147"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a functional configuration of the CPU <b>18</b>. The CPU <b>18</b> has functional units including a control unit <b>180</b>, a generation unit <b>181</b>, a region-setting unit <b>182</b>, a display control unit <b>183</b>, a position calculation unit <b>184</b>, a point-setting unit <b>185</b>, and a surface estimation unit <b>186</b>. At least one of the blocks shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> may be constituted by a different circuit from the CPU <b>18</b>.</p><p id="p-0149" num="0148">Each unit shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> may be constituted by at least one of a processor and a logic circuit. Each unit shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> may include one or a plurality of processors. Each unit shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> may include one or a plurality of logic circuits.</p><p id="p-0150" num="0149">The control unit <b>180</b> acquires a 2D image (image data) of a subject from the video-signal-processing circuit <b>12</b> and controls processing executed by each unit shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0151" num="0150">The generation unit <b>181</b> has the same function as that of the generation unit <b>70</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The generation unit <b>181</b> calculates 3D coordinates of three or more points on a subject on the basis of a 2D image of the subject and generates point-cloud data including the 3D coordinates of the three or more points (generation step). The generation unit <b>181</b> can generate the point-cloud data by using the same method as that shown in the first embodiment. In addition, the generation unit <b>181</b> generates a 3D image for displaying the point-cloud data on the display unit <b>5</b>.</p><p id="p-0152" num="0151">The region-setting unit <b>182</b> has the same function as that of the region-setting unit <b>71</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The region-setting unit <b>182</b> sets a reference region that is one of a three-dimensional region and a two-dimensional region (region-setting step). The region-setting unit <b>182</b> can set a reference region by using the same method as that shown in the first embodiment.</p><p id="p-0153" num="0152">The display control unit <b>183</b> displays the 2D image of the subject acquired by the imaging device <b>28</b> and the 3D image of the point-cloud data generated by the generation unit <b>181</b> on the display unit <b>5</b>. For example, the display control unit <b>183</b> controls processing executed by the video-signal-processing circuit <b>12</b>. The display control unit <b>183</b> causes the video signal processed by the video-signal-processing circuit <b>12</b> to be output from the video-signal-processing circuit <b>12</b> to the display unit <b>5</b>. The video signal includes color data of each pixel of the 2D image of the subject. The display unit <b>5</b> displays the 2D image of the subject on the basis of the video signal output from the video-signal-processing circuit <b>12</b>.</p><p id="p-0154" num="0153">Alternatively, the display control unit <b>183</b> outputs a 3D video signal to the display unit <b>5</b> via the video-signal-processing circuit <b>12</b>. The 3D video signal includes color data of each pixel of the 3D image of the point-cloud data. The display unit <b>5</b> displays the 3D image of the point-cloud data on the basis of the 3D video signal output from the video-signal-processing circuit <b>12</b>. Specifically, the display control unit <b>183</b> generates a graphic image signal for displaying the 3D image of the point-cloud data. The display control unit <b>183</b> outputs the generated graphic image signal to the video-signal-processing circuit <b>12</b>. The video-signal-processing circuit <b>12</b> combines the video signal output from the CCU <b>9</b> and the graphic image signal output from the CPU <b>18</b>. The video-signal-processing circuit <b>12</b> outputs the combined video signal to the display unit <b>5</b>. The display unit <b>5</b> displays the 2D image of the subject and the 3D image of the point-cloud data.</p><p id="p-0155" num="0154">The display control unit <b>183</b> displays various kinds of information on the display unit <b>5</b>. In other words, the display control unit <b>183</b> displays various kinds of information on an image. The various kinds of information include a measurement result or the like. The various kinds of information may include a cursor. The cursor is a mark used by a user to designate a specific point on an image.</p><p id="p-0156" num="0155">For example, the display control unit <b>183</b> generates a graphic image signal of the various kinds of information. The display control unit <b>183</b> outputs the generated graphic image signal to the video-signal-processing circuit <b>12</b>. The video-signal-processing circuit <b>12</b> combines the video signal output from the CCU <b>9</b> and the graphic image signal output from the CPU <b>18</b>. In this way, the various kinds of information are superimposed on an image. The video-signal-processing circuit <b>12</b> outputs the combined video signal to the display unit <b>5</b>. The display unit <b>5</b> displays an image on which the various kinds of information are superimposed.</p><p id="p-0157" num="0156">A user inputs position information indicating a position on an image into the operation unit <b>4</b> by operating the operation unit <b>4</b>. The operation unit <b>4</b> outputs the position information input into the operation unit <b>4</b> by a user. The position information input into the operation unit <b>4</b> is input into the control interface <b>17</b>, which is an input unit. The position information is output from the control interface <b>17</b> to the CPU <b>18</b>. The position calculation unit <b>184</b> calculates a position on an image on the basis of the position information input into the operation unit <b>4</b>.</p><p id="p-0158" num="0157">For example, in a case in which the cursor is displayed on an image, the position information indicates a position at which the cursor is displayed. The display control unit <b>183</b> displays a cursor at the position calculated by the position calculation unit <b>184</b>.</p><p id="p-0159" num="0158">In a case in which the display unit <b>5</b> and the operation unit <b>4</b> are constituted as a touch panel, a user may input a position on an image into the operation unit <b>4</b> by touching the screen of the display unit <b>5</b>. In such a case, the operation unit <b>4</b> outputs position information indicating the position.</p><p id="p-0160" num="0159">The point-setting unit <b>185</b> accepts a point on a subject through the operation unit <b>4</b>. For example, a user moves a cursor to an intended position on an image and performs a predetermined operation. At this time, the point-setting unit <b>185</b> accepts a point corresponding to the position. In a case in which the display unit <b>5</b> and the operation unit <b>4</b> are constituted as a touch panel, a user inputs a point on the image into the operation unit <b>4</b> by touching the screen of the display unit <b>5</b>. The point-setting unit <b>185</b> accepts the point. The point-setting unit <b>185</b> generates point information indicating the accepted point.</p><p id="p-0161" num="0160">In a case in which the point-setting unit <b>185</b> accepts a point on the 2D image of the subject, the point information includes coordinates of a pixel corresponding to the point. The coordinates of the pixel are associated with the 3D coordinates in the point-cloud data. In a case in which the point-setting unit <b>185</b> accepts a point on the 3D image of the point-cloud data, the point information includes the 3D coordinates of the point.</p><p id="p-0162" num="0161">The point-setting unit <b>185</b> accepts a reference point. The reference point indicates a reference position for setting a reference region.</p><p id="p-0163" num="0162">In a case in which the point-setting unit <b>185</b> accepts a point on the 2D image of the subject, the point-setting unit <b>185</b> sets the accepted point in the 2D image. In a case in which the point-setting unit <b>185</b> accepts a point on the 3D image of the point-cloud data, the point-setting unit <b>185</b> sets the accepted point in the 3D image. The position information of the point set by the point-setting unit <b>185</b> is held on the RAM <b>14</b>. The point is set by associating the point with a specific image.</p><p id="p-0164" num="0163">The surface estimation unit <b>186</b> estimates a reference surface that approximates the surface of the subject on the basis of three or more points of the point-cloud data corresponding to the three or more points included in the reference region (estimation step). The surface estimation unit <b>186</b> can estimate a reference surface by using the same method as that shown in the first embodiment.</p><p id="p-0165" num="0164">Surface estimation processing in the second embodiment will be described by using <figref idref="DRAWINGS">FIG. <b>9</b></figref>. <figref idref="DRAWINGS">FIG. <b>9</b></figref> shows a procedure of the surface estimation processing.</p><p id="p-0166" num="0165">The generation unit <b>181</b> calculates 3D coordinates of three or more points on a subject on the basis of the 2D image of the subject and generates point-cloud data including the 3D coordinates of the three or more points. In addition, the generation unit <b>181</b> generates a 3D image of the point-cloud data (Step S<b>101</b>). Step S<b>101</b> corresponds to the generation step.</p><p id="p-0167" num="0166">After Step S<b>101</b>, the display control unit <b>183</b> displays the 2D image of the subject and the 3D image of the point-cloud data on the display unit <b>5</b> (Step S<b>102</b>). The display control unit <b>183</b> may display only the 2D image of the subject on the display unit <b>5</b>.</p><p id="p-0168" num="0167"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows an example of an image displayed on the display unit <b>5</b> in Step S<b>102</b>. A 2D image G<b>11</b> of a subject and a 3D image G<b>12</b> of the point-cloud data are displayed on the display unit <b>5</b>. When an instruction to change the viewpoint of the 3D image G<b>12</b> is input through the operation unit <b>4</b>, the generation unit <b>181</b> may generate a new 3D image of the point-cloud data corresponding to the subject seen from the changed viewpoint. The display control unit <b>183</b> may display the new 3D image on the display unit <b>5</b> instead of the 3D image G<b>12</b>.</p><p id="p-0169" num="0168">In the example shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the 2D image G<b>11</b> and the 3D image G<b>12</b> are arranged in the horizontal direction of the screen of the display unit <b>5</b>. The disposition of the 2D image G<b>11</b> and the 3D image G<b>12</b> is not limited to this example. For example, the 2D image G<b>11</b> and the 3D image G<b>12</b> may be arranged in the vertical direction of the screen of the display unit <b>5</b>. At this time, the 2D image G<b>11</b> and the 3D image G<b>12</b> may rotate by 90 degrees. Part of the 2D image G<b>11</b> and part of the 3D image G<b>12</b> may overlap each other.</p><p id="p-0170" num="0169">In a case in which the 2D image of the subject is a stereo image, the 2D image includes a first image of the subject seen from a first viewpoint and a second image of the subject seen from a second viewpoint different from the first viewpoint. The display control unit <b>183</b> may display the first image and the second image on the display unit <b>5</b>. Alternatively, the display control unit <b>183</b> may display only one of the first image and the second image on the display unit <b>5</b>.</p><p id="p-0171" num="0170">After Step S<b>102</b>, the point-setting unit <b>185</b> accepts two reference points on the 2D image of the subject through the operation unit <b>4</b> and generates point information indicating each of the two accepted reference points (Step S<b>103</b>).</p><p id="p-0172" num="0171"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows an example of an image displayed on the display unit <b>5</b> in Step S<b>103</b>. The same 2D image G<b>11</b> as that shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref> and the same 3D image G<b>12</b> as that shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref> are displayed on the display unit <b>5</b>.</p><p id="p-0173" num="0172">In an example in which a cursor is displayed on the 2D image G<b>11</b>, a user moves the cursor to an intended position on the 2D image G<b>11</b> and performs a predetermined operation. At this time, the point-setting unit <b>185</b> accepts a point corresponding to the position. In an example in which the display unit <b>5</b> and the operation unit <b>4</b> are constituted as a touch panel, a user touches the screen of the display unit <b>5</b>. At this time, the point-setting unit <b>185</b> accepts a point corresponding to the touched position.</p><p id="p-0174" num="0173">In the example shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the point-setting unit <b>185</b> accepts a reference point P<b>11</b> and a reference point P<b>12</b>. A mark indicating the reference point P<b>11</b> and a mark indicating the reference point P<b>12</b> may be displayed on the 2D image G<b>11</b>.</p><p id="p-0175" num="0174">After Step S<b>103</b>, the region-setting unit <b>182</b> sets a reference region on the basis of the two reference points indicated by the point information (Step S<b>104</b>). Step S<b>104</b> corresponds to the region-setting step.</p><p id="p-0176" num="0175">The region-setting unit <b>182</b> sets a two-dimensional reference region on the 2D image of the subject. For example, the region-setting unit <b>182</b> sets a reference region R<b>11</b> on the 2D image G<b>11</b> on the basis of the reference point P<b>11</b> and the reference point P<b>12</b> shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. In the example shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the shape of the reference region R<b>11</b> is a rectangle. The reference point P<b>11</b> and the reference point P<b>12</b> are vertices on the diagonal line of the reference region R<b>11</b>.</p><p id="p-0177" num="0176">Information of the reference region set by the region-setting unit <b>182</b> is held on the RAM <b>14</b>. The information includes the position of the reference region and the size of the reference region. The reference region is set by associating the reference region with the 2D image of the subject.</p><p id="p-0178" num="0177">After Step S<b>104</b>, the display control unit <b>183</b> displays the reference region on the 2D image of the subject (Step S<b>105</b>).</p><p id="p-0179" num="0178">Specifically, the display control unit <b>183</b> generates a graphic image signal for displaying the reference region. The display control unit <b>183</b> outputs the generated graphic image signal to the video-signal-processing circuit <b>12</b>. The video-signal-processing circuit <b>12</b> combines the video signal output from the CCU <b>9</b> and the graphic image signal output from the CPU <b>18</b>. The video-signal-processing circuit <b>12</b> outputs the combined video signal to the display unit <b>5</b>. The display unit <b>5</b> displays the 2D image of the subject on which the reference region is superimposed.</p><p id="p-0180" num="0179"><figref idref="DRAWINGS">FIG. <b>12</b></figref> shows an example of an image displayed on the display unit <b>5</b> in Step S<b>105</b>. The same 2D image G<b>11</b> as that shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref> and the same 3D image G<b>12</b> as that shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref> are displayed on the display unit <b>5</b>. A reference region R<b>11</b> is displayed on the 2D image G<b>11</b>.</p><p id="p-0181" num="0180">In the example shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, a line (frame) indicating the outline of the reference region R<b>11</b> is displayed on the 2D image G<b>11</b>. In the example shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the surface of the subject in the reference region R<b>11</b> is visible. The inside of the reference region R<b>11</b> may be displayed in a predetermined color or pattern. At this time, the surface of the subject in the reference region R<b>11</b> is not necessarily visible. As long as a user can check the position and the size of the reference region R<b>11</b>, a method of displaying the reference region R<b>11</b> is not limited to the above-described methods.</p><p id="p-0182" num="0181">The display control unit <b>183</b> displays the 2D image of the subject on the display unit <b>5</b> in Step S<b>102</b> and displays the reference region on the 2D image in Step S<b>105</b> (image display step). A user can check whether or not the reference region is set at a position intended by the user.</p><p id="p-0183" num="0182">After Step S<b>105</b>, the surface estimation unit <b>186</b> estimates a reference surface that approximates the surface of the subject on the basis of three or more points of the point-cloud data corresponding to the three or more points included in the reference region (Step S<b>106</b>). Step S<b>106</b> corresponds to the estimation step.</p><p id="p-0184" num="0183">The surface estimation unit <b>186</b> extracts all the pixels in the reference region of the 2D image of the subject. The pixels in the 2D image of the subject are associated with the 3D coordinates in the point-cloud data. The surface estimation unit <b>186</b> acquires 3D coordinates of each of the extracted pixels from the point-cloud data. The surface estimation unit <b>186</b> estimates a reference surface by using the acquired 3D coordinates.</p><p id="p-0185" num="0184">The surface estimation unit <b>186</b> may select three or more points from four or more points included in the reference region in the estimation step. The surface estimation unit <b>186</b> may estimate a reference surface that approximates the surface of the subject on the basis of three or more points of the point-cloud data corresponding to the three or more selected points in the estimation step.</p><p id="p-0186" num="0185">In other words, the surface estimation unit <b>186</b> may extract some of all the pixels in the reference region of the 2D image of the subject. For example, the surface estimation unit <b>186</b> may extract greater than or equal to a predetermined rate (for example, 50%) of all the pixels in the reference region of the 2D image. The extracted pixels include one or more combinations, each of which is constituted by three pixels that form a triangle.</p><p id="p-0187" num="0186">After Step S<b>106</b>, the display control unit <b>183</b> displays the reference surface on the 2D image of the subject (Step S<b>107</b>). When Step S<b>107</b> is executed, the surface estimation processing is completed.</p><p id="p-0188" num="0187">Specifically, the display control unit <b>183</b> generates a graphic image signal for displaying the reference surface. The display control unit <b>183</b> outputs the generated graphic image signal to the video-signal-processing circuit <b>12</b>. The video-signal-processing circuit <b>12</b> combines the video signal output from the CCU <b>9</b> and the graphic image signal output from the CPU <b>18</b>. The video-signal-processing circuit <b>12</b> outputs the combined video signal to the display unit <b>5</b>. The display unit <b>5</b> displays the 2D image of the subject on which the reference surface is superimposed.</p><p id="p-0189" num="0188"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows an example of an image displayed on the display unit <b>5</b> in Step S<b>107</b>. The same 2D image G<b>11</b> as that shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref> and the same 3D image G<b>12</b> as that shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref> are displayed on the display unit <b>5</b>. A reference surface F<b>11</b> displayed on the 2D image G<b>11</b>.</p><p id="p-0190" num="0189">For example, the display control unit <b>183</b> acquires 3D coordinates associated with each pixel in the 2D image G<b>11</b> from the point-cloud data. The display control unit <b>183</b> calculates the three-dimensional distance (3D distance) between the point having the 3D coordinates and the reference surface. When the 3D distance is less than a predetermined distance, the display control unit <b>183</b> extracts a pixel on the 2D image G<b>11</b> corresponding to the point. The display control unit <b>183</b> repeats the above-described processing by using all or some of the pixels of the 2D image G<b>11</b>. The display control unit <b>183</b> generates a graphic image signal for displaying a region including the extracted pixels as a reference surface.</p><p id="p-0191" num="0190">In the example shown in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, a line (frame) indicating the outline of the region including the extracted pixels is displayed as the reference surface F<b>11</b> on the 2D image G<b>11</b>. In the example shown in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the surface of the subject within the line indicating the reference surface F<b>11</b> is visible. The inside of the line may be displayed in a predetermined color or pattern. At this time, the surface of the subject within the line is not necessarily visible. As long as a user can check the position of the reference surface F<b>11</b>, a method of displaying the reference surface F<b>11</b> is not limited to the above-described methods. A user can check whether or not the reference surface F<b>11</b> accurately approximates the surface of the subject.</p><p id="p-0192" num="0191">In the example shown in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the reference region R<b>11</b> shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref> is not displayed. The reference region R<b>11</b> and the reference surface F<b>11</b> may be displayed on the 2D image G<b>11</b>.</p><p id="p-0193" num="0192">The display control unit <b>183</b> displays the 2D image of the subject on the display unit <b>5</b> in Step S<b>102</b> and displays the region corresponding to the reference surface on the 2D image in Step S<b>107</b> (image display step). A user can check whether or not the reference surface accurately approximates the surface of the subject.</p><p id="p-0194" num="0193">After the point-setting unit <b>185</b> accepts the reference point in Step S<b>103</b>, a user may change the reference point. For example, a user inputs position information indicating a new position of the reference point into the operation unit <b>4</b>. The position information is output to the CPU <b>18</b> through the control interface <b>17</b>. The region-setting unit <b>182</b> changes the reference point on the basis of the position information.</p><p id="p-0195" num="0194">Region information indicating at least one of the size of the reference region and the position of the reference region may be recorded on a recording medium in advance. The region-setting unit <b>182</b> may set at least one of the size of the reference region and the position of the reference region on the basis of the region information in Step S<b>104</b>.</p><p id="p-0196" num="0195">For example, the memory card <b>42</b> stores the region information. The region information is transferred from the memory card <b>42</b> to the RAM <b>14</b> via the card interface <b>15</b>. The region-setting unit <b>182</b> reads the region information from the RAM <b>14</b>.</p><p id="p-0197" num="0196">An example in which the region-setting unit <b>182</b> sets the size of the reference region will be described. A user designates a reference point on the 2D image of the subject, and the point-setting unit <b>185</b> accepts the reference point. The region-setting unit <b>182</b> sets a rectangle of which the center is at the reference point accepted by the point-setting unit <b>185</b> in the 2D image. The region information indicates the number of pixels of the reference region in the horizontal direction and the number of pixels of the reference region in the vertical direction. The region-setting unit <b>182</b> sets the number of pixels of the rectangle in the horizontal direction and the number of pixels of the rectangle in the vertical direction on the basis of the region information. Each time a user designates the size of the reference region, region information indicating the size may be recorded on a recording medium.</p><p id="p-0198" num="0197">An example in which the region-setting unit <b>182</b> sets the position of the reference region will be described. The region information indicates a predetermined position on the 2D image of the subject. For example, the predetermined position is the center of the 2D image. The region-setting unit <b>182</b> sets the reference region at the position indicated by the region information. At this time, the region-setting unit <b>182</b> sets the size of the reference region to that designated by a user. Each time a user designates a position (reference point) of the reference region, region information indicating the position may be recorded on a recording medium. The region-setting unit <b>182</b> may set the size of the reference region and the position of the reference region on the basis of the region information.</p><p id="p-0199" num="0198">Two or more pieces of the region information may be recorded on a recording medium in advance. The region-setting unit <b>182</b> may select one of the two or more pieces of the region information on the basis of information input through the operation unit <b>4</b> in Step S<b>104</b>. The region-setting unit <b>182</b> may set one of the size of the reference region and the position of the reference region on the basis of the selected region information in Step S<b>104</b>.</p><p id="p-0200" num="0199">The two or more pieces of the region information indicate different sizes. For example, first region information indicates a first size, and second region information indicates a second size different from the first size. Alternatively, the two or more pieces of the region information indicate different positions. For example, first region information indicates a first position, and second region information indicates a second position different from the first position. A user selects one of the first region information and the second region information and inputs a selection result into the operation unit <b>4</b>. The selection result is output to the CPU <b>18</b> via the control interface <b>17</b>. The region-setting unit <b>182</b> sets the size of the reference region or the position of the reference region on the basis of the region information selected by a user.</p><p id="p-0201" num="0200">After the region-setting unit <b>182</b> sets the reference region in Step S<b>104</b>, a user may change at least one of the size of the reference region and the position of the reference region at any timing. The timing may come after the reference region is displayed in Step S<b>105</b> or after the reference surface is displayed in Step S<b>107</b>. For example, a user inputs information indicating at least one of the size and the position into the operation unit <b>4</b>. The information is output to the CPU <b>18</b> via the control interface <b>17</b>. The region-setting unit <b>182</b> changes at least one of the size of the reference region and the position of the reference region on the basis of the information.</p><p id="p-0202" num="0201">In the surface estimation processing shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the region-setting unit <b>182</b> sets the reference region on the basis of the two reference points. The number of reference points is not limited to two. As described above, the region-setting unit <b>182</b> may set a reference region of which the center is at one reference point.</p><p id="p-0203" num="0202">The region-setting unit <b>182</b> may set a reference region without using a reference point. Accordingly, Step S<b>103</b> does not need to be executed. The CPU <b>18</b> does not need to have the functions of the position calculation unit <b>184</b> and the point-setting unit <b>185</b>. A method of setting a reference region without using a reference point will be described in the tenth embodiment.</p><p id="p-0204" num="0203">The display control unit <b>183</b> may display the 3D image of the point-cloud data on the display unit <b>5</b> and may display the three or more points of the point-cloud data used for estimating the reference surface on the 3D image (image display step). Three or more points included in the three-dimensional reference region are used for estimating the reference surface. The display control unit <b>183</b> extracts the three or more points from the point-cloud data and displays the three or more points on the 3D image. For example, pixels corresponding to the points are displayed in a predetermined color.</p><p id="p-0205" num="0204">The display control unit <b>183</b> may display the 2D image of the subject on the display unit <b>5</b> and may display three or more points corresponding to the three or more points of the point-cloud data used for estimating the reference surface on the 2D image (image display step). The three or more points of the point-cloud data corresponding to the three or more points included in the two-dimensional reference region are used for estimating the reference surface. The display control unit <b>183</b> extracts the three or more points from the point-cloud data. The display control unit <b>183</b> extracts, from the reference region, points corresponding to the respective three or more points extracted from the point-cloud data and displays the points on the 2D image. For example, pixels corresponding to the points are displayed in a predetermined color.</p><p id="p-0206" num="0205">The reference region does not need to be displayed. Accordingly, Step S<b>105</b> does not need to be executed. The reference surface does not need to be displayed. Accordingly, Step S<b>107</b> does not need to be executed.</p><p id="p-0207" num="0206">In a case in which setting the reference point, displaying the reference region, and displaying the reference surface are unnecessary, an image does not need to be displayed. Accordingly, Step S<b>102</b> does not need to be executed. The CPU <b>18</b> does not need to have the function of the display control unit <b>183</b>.</p><p id="p-0208" num="0207">The surface estimation unit <b>186</b> may estimate geometric features of the reference surface in Step S<b>106</b>. For example, in a case in which the reference surface is a cylindrical surface, the surface estimation unit <b>186</b> may estimate the diameter of the cylinder. In a case in which the reference surface is a spherical surface, the surface estimation unit <b>186</b> may estimate the diameter of the sphere.</p><p id="p-0209" num="0208">The surface estimation unit <b>186</b> may estimate the gradient of the reference surface. The gradient is expressed as an angle between a predetermined plane and the reference surface. For example, the predetermined plane is vertical to the visual line of the endoscope.</p><p id="p-0210" num="0209">The surface estimation unit <b>186</b> may estimate the type of the reference surface. The type indicates one of a plane, a cylindrical surface, a spherical surface, and the like. The surface estimation unit <b>186</b> estimates a reference surface by assuming that the reference surface is a plane. At this time the surface estimation unit <b>186</b> calculates an estimation error on the basis of the distance between the reference surface and the surface of the subject or the like. Similarly, the surface estimation unit <b>186</b> estimates a reference surface by assuming that the reference surface is a cylindrical surface, a spherical surface, or the like and calculates an estimation error. The surface estimation unit <b>186</b> obtains a reference surface having the smallest estimation error as a final reference surface. At this time, the surface estimation unit <b>186</b> obtains the type of the reference surface.</p><p id="p-0211" num="0210">The estimation result of the reference surface may include a parameter of an expression of the reference surface.</p><p id="p-0212" num="0211">The display control unit <b>183</b> may display an estimation result of geographic features of the reference surface on the display unit <b>5</b>. The estimation result is included in the graphic image signal generated by the display control unit <b>183</b>.</p><p id="p-0213" num="0212">The order of processing in the surface estimation processing is not limited to that shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. For example, the 2D image of the subject may be displayed on the display unit <b>5</b> before Step S<b>101</b> is executed.</p><p id="p-0214" num="0213">In the second embodiment, the endoscope device <b>1</b> sets a two-dimensional reference region on the 2D image of the subject. The reference region includes three or more points that two-dimensionally expand. Therefore, three or more points of the point-cloud data used for estimating a reference surface are less likely to be biased on a plane or a straight line. Accordingly, the endoscope device <b>1</b> can improve the accuracy of a reference surface.</p><heading id="h-0009" level="1">First Modified Example of Second Embodiment</heading><p id="p-0215" num="0214">A first modified example of the second embodiment of the present invention will be described. In the examples shown in <figref idref="DRAWINGS">FIGS. <b>10</b> to <b>13</b></figref>, the 2D image of the subject and the 3D image of the point-cloud data are displayed on the display unit <b>5</b>, and the 2D image is used for setting a two-dimensional reference region. A combination of an image displayed on the display unit <b>5</b> and an image used for setting a reference region is not limited to these examples. In the first modified example of the second embodiment, the 3D image of the point-cloud data is displayed on the display unit <b>5</b> and is used for setting a three-dimensional reference region.</p><p id="p-0216" num="0215">Surface estimation processing in the first modified example of the second embodiment will be described. The surface estimation processing in the first modified example of the second embodiment is executed in accordance with <figref idref="DRAWINGS">FIG. <b>9</b></figref>. Hereinafter, the same processing as the surface estimation processing in the second embodiment will not be described.</p><p id="p-0217" num="0216">The display control unit <b>183</b> displays the 3D image of the point-cloud data on the display unit <b>5</b> in Step S<b>102</b> (image display step). In Step S<b>102</b>, the 2D image of the subject is not displayed. The point-setting unit <b>185</b> accepts three reference points on the 3D image of the point-cloud data through the operation unit <b>4</b> and generates point information indicating each of the three accepted reference points in Step S<b>103</b>.</p><p id="p-0218" num="0217"><figref idref="DRAWINGS">FIG. <b>14</b></figref> shows an example of an image displayed on the display unit <b>5</b> in Step S<b>103</b>. The same 3D image G<b>12</b> as that shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref> is displayed on the display unit <b>5</b>.</p><p id="p-0219" num="0218">In an example in which a cursor is displayed on the 3D image G<b>12</b>, a user moves the cursor to an intended position on the 3D image G<b>12</b> and performs a predetermined operation. At this time, the point-setting unit <b>185</b> accepts a point corresponding to the position. In an example in which the display unit <b>5</b> and the operation unit <b>4</b> are constituted as a touch panel, a user touches the screen of the display unit <b>5</b>. At this time, the point-setting unit <b>185</b> accepts a point corresponding to the touched position.</p><p id="p-0220" num="0219">In the example shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the point-setting unit <b>185</b> accepts a reference point P<b>21</b> and a reference point P<b>22</b>. A mark indicating the reference point P<b>21</b> and a mark indicating the reference point P<b>22</b> may be displayed on the 3D image G<b>12</b>.</p><p id="p-0221" num="0220">The region-setting unit <b>182</b> sets a three-dimensional reference region in the 3D space defining the 3D coordinates of the point-cloud data in Step S<b>104</b>. In the example shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the region-setting unit <b>182</b> calculates a rectangle RC<b>21</b> on the basis of the reference point P<b>21</b> and the reference point P<b>22</b>. The reference point P<b>21</b> and the reference point P<b>22</b> are vertices on the diagonal line of the rectangle RC<b>21</b>.</p><p id="p-0222" num="0221">After two reference points are set, a user designates a third reference point. <figref idref="DRAWINGS">FIG. <b>15</b></figref> shows an example of an image displayed on the display unit <b>5</b> when the third reference point is designated. A 3D image G<b>13</b> of the point-cloud data is displayed on the display unit <b>5</b>. In an example in which the display unit <b>5</b> and the operation unit <b>4</b> are constituted as a touch panel, a user performs a swipe operation on the 3D image G<b>12</b> shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref> and changes the viewpoint of the 3D image G<b>12</b>. The 3D image G<b>13</b> shows a cross-section of the subject seen in the 3D image G<b>12</b> shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref>.</p><p id="p-0223" num="0222">A user designates a reference point P<b>23</b> by using a cursor displayed on the 3D image G<b>13</b> or by touching the screen of the display unit <b>5</b>. The point-setting unit <b>185</b> accepts the reference point P<b>23</b>. A mark indicating the reference point P<b>23</b> may be displayed on the 3D image G<b>13</b>. In an example in which the display unit <b>5</b> and the operation unit <b>4</b> are constituted as a touch panel, a user may magnify or reduce the 3D image G<b>13</b> by performing a pinch operation.</p><p id="p-0224" num="0223">In the examples shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref> and <figref idref="DRAWINGS">FIG. <b>15</b></figref>, the region-setting unit <b>182</b> calculates a reference region R<b>21</b> on the basis of the reference point P<b>21</b>, the reference point P<b>22</b>, and the reference point P<b>23</b>. The region-setting unit <b>182</b> sets the reference region R<b>21</b> in the 3D space defining the 3D coordinates of the point-cloud data. In the example shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref>, the shape of the reference region R<b>21</b> is a cuboid. One of the surfaces of the cuboid is defined by the rectangle RC<b>21</b> shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref>. The height of the cuboid is the same as the 3D distance between the rectangle RC<b>21</b> and the reference point P<b>23</b>.</p><p id="p-0225" num="0224">Information of the reference region set by the region-setting unit <b>182</b> is held on the RAM <b>14</b>. The information includes the position of the reference region and the size of the reference region. The reference region is set by associating the reference region with the point-cloud data.</p><p id="p-0226" num="0225">The display control unit <b>183</b> displays the reference region on the 3D image of the point-cloud data in Step S<b>105</b> (image display step). The display unit <b>5</b> displays the 3D image on which the reference region is superimposed. A user can check whether or not the reference region is set at a position intended by the user.</p><p id="p-0227" num="0226">The surface estimation unit <b>186</b> estimates a reference surface on the basis of three or more points of the point-cloud data corresponding to three or more points included in the reference region in Step S<b>106</b>. For example, the surface estimation unit <b>186</b> extracts all the points in the reference region. The surface estimation unit <b>186</b> estimates a reference surface by using the 3D coordinates of each of the extracted points. The surface estimation unit <b>186</b> may extract some of the points in the reference region. For example, the surface estimation unit <b>186</b> may extract greater than or equal to a predetermined rate (for example, 50%) of all the points in the reference region. The extracted points include one or more combinations, each of which is constituted by three points that form a triangle. There is no straight line passing through all the extracted points.</p><p id="p-0228" num="0227">The display control unit <b>183</b> displays the reference surface on the 3D image of the point-cloud data in Step S<b>107</b>. The display unit <b>5</b> displays the 3D image on which the reference surface is superimposed. A user can check whether or not the reference surface accurately approximates the surface of the subject.</p><p id="p-0229" num="0228"><figref idref="DRAWINGS">FIG. <b>16</b></figref> shows an example of an image displayed on the display unit <b>5</b> in Step S<b>107</b>. A 3D image G<b>14</b> of the point-cloud data is displayed on the display unit <b>5</b>.</p><p id="p-0230" num="0229">For example, the display control unit <b>183</b> calculates the 3D distance between each point in the point-cloud data and the reference surface. When the 3D distance between the point and the reference surface is less than a predetermined distance, the display control unit <b>183</b> extracts the point. The display control unit <b>183</b> repeats the above-described processing by using points in all or part of the point-cloud data. The display control unit <b>183</b> generates a graphic image signal for displaying a region including the extracted points as the reference surface.</p><p id="p-0231" num="0230">In the example shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref>, a region including the extracted points is displayed as a reference surface F<b>21</b> on the 3D image G<b>14</b>. For example, the reference surface F<b>21</b> is displayed in a different color from that of the surface of the subject. As long as a user can check the position of the reference surface F<b>21</b>, a method of displaying the reference surface F<b>21</b> is not limited to the above-described example. A user can check whether or not the reference surface F<b>21</b> accurately approximates the surface of the subject.</p><p id="p-0232" num="0231">In the examples shown in <figref idref="DRAWINGS">FIGS. <b>14</b> to <b>16</b></figref>, the 3D image of the point-cloud data is displayed on the display unit <b>5</b> and is used for setting a reference region. The 2D image of the subject and the 3D image of the point-cloud data may be displayed on the display unit <b>5</b>, and the 3D image may be used for setting a reference region.</p><p id="p-0233" num="0232">There is a case in which a recessed portion or a projection portion is formed on the surface of a subject and a reference surface approximating a part excluding the recessed portion or the projection portion is required. In a case in which a reference region is set in the 2D image of the subject, the reference region may include a pixel of the recessed portion or the projection portion. Therefore, the accuracy of a reference surface may deteriorate. Even when a step is formed on the surface of a subject and a reference region includes a pixel of the step, the accuracy of a reference surface may deteriorate.</p><p id="p-0234" num="0233">In the first modified example of the second embodiment, the region-setting unit <b>182</b> sets a three-dimensional reference region. In the example shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref>, the region-setting unit <b>182</b> sets the reference region R<b>21</b> that does not include many points corresponding to the recessed portion of the subject. Therefore, the endoscope device <b>1</b> can improve the accuracy of a reference surface.</p><heading id="h-0010" level="1">Second Modified Example of Second Embodiment</heading><p id="p-0235" num="0234">A second modified example of the second embodiment of the present invention will be described. In the second modified example of the second embodiment, a three-dimensional reference region is set. The reference region is a set of three or more points having continuity.</p><p id="p-0236" num="0235">Surface estimation processing in the second modified example of the second embodiment will be described. The surface estimation processing in the second modified example of the second embodiment is executed in accordance with <figref idref="DRAWINGS">FIG. <b>9</b></figref>. Hereinafter, the same processing as the surface estimation processing in the first modified example of the second embodiment will not be described.</p><p id="p-0237" num="0236">The region-setting unit <b>182</b> determines continuity of three or more points corresponding to the three-dimensional coordinates included in the point-cloud data in Step S<b>104</b>. The region-setting unit <b>182</b> sets a reference region including only three or more points determined to be continuous.</p><p id="p-0238" num="0237">Specifically, the region-setting unit <b>182</b> sets a three-dimensional determination region. A method of setting a determination region is the same as the method of setting a reference region shown in the first modified example of the second embodiment. The region-setting unit <b>182</b> determines continuity of points in the determination region. For example, the region-setting unit <b>182</b> uses the 3D distance between two adjacent points. The region-setting unit <b>182</b> classifies each point in the determination region on the basis of the 3D distance.</p><p id="p-0239" num="0238">Segmentation is known as a simple method of classifying each point corresponding to the point-cloud data. For example, the region-setting unit <b>182</b> can use Euclidean cluster extraction in segmentation. This is a function installed in a point cloud library (PCL), which is open source software.</p><p id="p-0240" num="0239">The region-setting unit <b>182</b> determines a point within a predetermined distance of each point as a near-point by using this function. One point and a near-point thereof are on the same object. For example, in a case in which the subject includes a first object and a second object apart from each other, each point corresponding to the point-cloud data is classified into any one of a point on the first object and a point on the second object. The region-setting unit <b>182</b> assigns each of the three or more points corresponding to the three-dimensional coordinates included in the point-cloud data to one of two or more objects. In this way, the region-setting unit <b>182</b> divides (classifies) the three or more points in the point-cloud data into two or more regions. In the above-described example, the three or more points in the point-cloud data are divided into a first region constituted by points on a first object and a second region constituted by points on a second object. In a case in which the subject is constituted by only one object, the region-setting unit <b>182</b> assigns all the points in the point-cloud data to the object.</p><p id="p-0241" num="0240">The region-setting unit <b>182</b> executes the above-described segmentation in the determination region. In this way, the region-setting unit <b>182</b> assigns each point in the determination region to any one of one or more objects. The region-setting unit <b>182</b> determines whether or not a point in the determination region constitutes a continuous surface on the basis of the result of the segmentation. Two or more points assigned to one object constitute a continuous surface of the subject. In other words, the two or more points have continuity. The surface of the subject is discontinuous between two objects. In other words, points lack continuity between the two objects. The region-setting unit <b>182</b> selects one object and extracts three or more points included in the selected object from the determination region. The region-setting unit <b>182</b> sets a reference region including the three or more extracted points. The reference region includes all or some of the points in the determination region.</p><p id="p-0242" num="0241"><figref idref="DRAWINGS">FIG. <b>17</b></figref> and <figref idref="DRAWINGS">FIG. <b>18</b></figref> show examples of a reference region. Hereinafter, a reference region will be described by referring to the 3D image of the point-cloud data.</p><p id="p-0243" num="0242">The same 3D image G<b>12</b> as that shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref> is shown in <figref idref="DRAWINGS">FIG. <b>17</b></figref>. A 3D image G<b>15</b> is shown in <figref idref="DRAWINGS">FIG. <b>17</b></figref>. The 3D image G<b>15</b> shows a cross-section of the subject seen in the 3D image G<b>12</b>. The region-setting unit <b>182</b> assigns all the points in and around the recessed portion of the subject to an object OB<b>31</b>. The region-setting unit <b>182</b> sets a reference region R<b>31</b> including points in the object OB<b>31</b>.</p><p id="p-0244" num="0243">A 3D image G<b>12</b><i>a </i>and a 3D image G<b>16</b> are shown in <figref idref="DRAWINGS">FIG. <b>18</b></figref>. The 3D image G<b>16</b> shows a cross-section of the subject seen in the 3D image G<b>12</b><i>a</i>. The region-setting unit <b>182</b> assigns points in and around the recessed portion of the subject to an object OB<b>32</b> or an object OB<b>33</b>. The object OB<b>32</b> and the object OB<b>33</b> are not connected to each other. The region-setting unit <b>182</b> sets a reference region R<b>32</b> including points in the object OB<b>32</b>. The reference region R<b>32</b> does not include points in the object OB<b>33</b>. Points in the object OB<b>33</b> are not used for estimating a reference surface.</p><p id="p-0245" num="0244">In a method of generating the point-cloud data by using two or more 2D images, there is a case in which a region seen in one 2D image is not seen in the other 2D images. Therefore, there is a case in which 3D coordinates of a point in the region cannot be calculated. In such a case, the surface of the subject indicated by the point-cloud data is discontinuous as shown in <figref idref="DRAWINGS">FIG. <b>18</b></figref>.</p><p id="p-0246" num="0245">In the second modified example of the second embodiment, a reference surface is estimated on the basis of only points corresponding to a continuous surface of a subject. Therefore, the endoscope device <b>1</b> can improve the accuracy of a reference surface.</p><heading id="h-0011" level="1">Third Modified Example of Second Embodiment</heading><p id="p-0247" num="0246">A third modified example of the second embodiment of the present invention will be described. In the third modified example of the second embodiment, a 3D image of the point-cloud data seen from each of two or more viewpoints is displayed on the display unit <b>5</b>, and a reference surface is displayed on the 3D image.</p><p id="p-0248" num="0247">Surface estimation processing in the third modified example of the second embodiment will be described. The surface estimation processing in the third modified example of the second embodiment is executed in accordance with <figref idref="DRAWINGS">FIG. <b>9</b></figref>. Hereinafter, the same processing as the surface estimation processing in the first modified example of the second embodiment will not be described.</p><p id="p-0249" num="0248">The generation unit <b>181</b> generates a 3D image of the point-cloud data seen from each of two or more viewpoints in Step S<b>101</b>. Hereinafter, an example in which the generation unit <b>181</b> generates three 3D images will be described. Three directions of visual lines for generating three 3D images may be vertical to each other. For example, the generation unit <b>181</b> may generate a 3D image of the point-cloud data seen in a parallel direction to the X-axis, a 3D image of the point-cloud data seen in a parallel direction to the Y-axis, and a 3D image of the point-cloud data seen in a parallel direction to the Z-axis.</p><p id="p-0250" num="0249">The display control unit <b>183</b> displays three 3D images on the display unit <b>5</b> in Step S<b>102</b>. The display control unit <b>183</b> displays a reference surface on each 3D image in Step S<b>107</b>. The display unit <b>5</b> displays the 3D image on which the reference surface is superimposed.</p><p id="p-0251" num="0250"><figref idref="DRAWINGS">FIG. <b>19</b></figref> shows an example of an image displayed on the display unit <b>5</b> in Step S<b>107</b>. A 3D image G<b>17</b>, a 3D image G<b>18</b>, and a 3D image G<b>19</b> are displayed on the display unit <b>5</b>. The 3D image G<b>17</b> is a 3D image of the point-cloud data seen from a first viewpoint. The 3D image G<b>18</b> is a 3D image of the point-cloud data seen from a second viewpoint different from the first viewpoint. The 3D image G<b>19</b> is a 3D image of the point-cloud data seen from a third viewpoint different from the first viewpoint and the second viewpoint.</p><p id="p-0252" num="0251">The display control unit <b>183</b> executes similar processing to that for displaying the reference surface F<b>21</b> shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref>. The display control unit <b>183</b> displays a reference surface F<b>41</b> on the 3D image G<b>17</b>, the 3D image G<b>18</b>, and the 3D image G<b>19</b>.</p><p id="p-0253" num="0252">The display control unit <b>183</b> may display a 3D image of the point-cloud data seen in a parallel direction to the surface of the subject or the reference surface on the display unit <b>5</b>. In this way, a user can easily check the reference surface.</p><p id="p-0254" num="0253">The generation unit <b>181</b> may extract some of the points in the point-cloud data and may generate a 3D image including the extracted points. The display control unit <b>183</b> may display the 3D image on the display unit <b>5</b>.</p><p id="p-0255" num="0254">For example, the generation unit <b>181</b> generates a first 3D image of all the points in the point-cloud data. The first 3D image is a 3D image of the point-cloud data seen from a first viewpoint. The generation unit <b>181</b> generates a second 3D image including the points extracted from the point-cloud data. The second 3D image is a 3D image of the point-cloud data seen from a second viewpoint different from the first viewpoint. The display control unit <b>183</b> displays the first 3D image and the second 3D image on the display unit <b>5</b>. At this time, the display control unit <b>183</b> may display the magnified second 3D image on the display unit <b>5</b>. The display control unit <b>183</b> displays the reference surface on the first 3D image and the second 3D image.</p><p id="p-0256" num="0255">In the third modified example of the second embodiment, a reference surface is displayed on the 3D image of the point-cloud data seen from each of two or more viewpoints. A user can check whether or not the reference surface accurately approximates the surface of the subject.</p><heading id="h-0012" level="1">Third Embodiment</heading><p id="p-0257" num="0256">A third embodiment of the present invention will be described. The endoscope device <b>1</b> according to the third embodiment has a three-dimensional measurement function.</p><p id="p-0258" num="0257">In the third embodiment, the CPU <b>18</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> is changed to a CPU <b>18</b><i>a </i>shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref>. <figref idref="DRAWINGS">FIG. <b>20</b></figref> shows a functional configuration of the CPU <b>18</b><i>a</i>. The CPU <b>18</b><i>a </i>has functional units including a control unit <b>180</b>, a generation unit <b>181</b>, a region-setting unit <b>182</b>, a display control unit <b>183</b>, a position calculation unit <b>184</b>, a point-setting unit <b>185</b>, a surface estimation unit <b>186</b>, and a measurement unit <b>187</b>. At least one of the blocks shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref> may be constituted by a different circuit from the CPU <b>18</b><i>a</i>. The same configuration as that shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> will not be described.</p><p id="p-0259" num="0258">Each unit shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref> may be constituted by at least one of a processor and a logic circuit. Each unit shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref> may include one or a plurality of processors. Each unit shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref> may include one or a plurality of logic circuits.</p><p id="p-0260" num="0259">The measurement unit <b>187</b> measures the size of a subject on the basis of the reference surface (measurement step). For example, the measurement unit <b>187</b> executes the surface-based measurement. In other words, the measurement unit <b>187</b> measures the 3D distance between the reference surface and a point on the surface of the subject in the measurement step.</p><p id="p-0261" num="0260">The point-setting unit <b>185</b> accepts a reference point as in the second embodiment. The reference point indicates a reference position for setting a reference region. In addition, the point-setting unit <b>185</b> accepts a measurement point and generates point information indicating the accepted measurement point. The measurement point indicates a position at which the size of the subject is measured. A user designates a measurement point by using a cursor displayed on an image or by touching the screen of the display unit <b>5</b>.</p><p id="p-0262" num="0261">The measurement unit <b>187</b> calculates the 3D distance between the reference surface and the measurement point indicated by the point information. In this way, the measurement unit <b>187</b> can measure the depth of a recessed portion on the surface of the subject or can measure the height of a projection portion on the surface of the subject.</p><p id="p-0263" num="0262">Three-dimensional measurement (3D measurement) in the third embodiment will be described by using <figref idref="DRAWINGS">FIG. <b>21</b></figref>. <figref idref="DRAWINGS">FIG. <b>21</b></figref> shows a procedure of the 3D measurement. The same processing as that shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref> will not be described.</p><p id="p-0264" num="0263">After Step S<b>107</b>, the point-setting unit <b>185</b> accepts one measurement point on the 2D image of the subject through the operation unit <b>4</b> and generates point information indicating the accepted measurement point (Step S<b>108</b>).</p><p id="p-0265" num="0264">After Step S<b>108</b>, the measurement unit <b>187</b> measures the size of the subject on the basis of the reference surface estimated in Step S<b>106</b> and the measurement point indicated by the point information (Step S<b>109</b>). Step S<b>109</b> corresponds to the measurement step.</p><p id="p-0266" num="0265">After Step S<b>109</b>, the display control unit <b>183</b> displays a measurement result on the 2D image of the subject (Step S<b>110</b>). The measurement result indicates the size measured in Step S<b>109</b>. When Step S<b>110</b> is executed, the 3D measurement is completed.</p><p id="p-0267" num="0266">Specifically, the display control unit <b>183</b> generates a graphic image signal for displaying the measurement result. The display control unit <b>183</b> outputs the generated graphic image signal to the video-signal-processing circuit <b>12</b>. The video-signal-processing circuit <b>12</b> combines the video signal output from the CCU <b>9</b> and the graphic image signal output from the CPU <b>18</b><i>a</i>. The video-signal-processing circuit <b>12</b> outputs the combined video signal to the display unit <b>5</b>. The display unit <b>5</b> displays the 2D image of the subject on which the measurement result is superimposed. For example, a character indicating the size of the subject is displayed on the 2D image. The display control unit <b>183</b> may display the measurement result on the 3D image of the point-cloud data.</p><p id="p-0268" num="0267">The measurement unit <b>187</b> may measure the 3D distance between the reference surface and each of two or more points on the surface of the subject in Step S<b>109</b>. The display control unit <b>183</b> may display the maximum value of the measured 3D distance on the display unit <b>5</b> in Step S<b>110</b> (measurement result display step).</p><p id="p-0269" num="0268">Specifically, the measurement unit <b>187</b> extracts all or some of the points in a circle circumscribing the two-dimensional reference region from the 2D image of the subject. A square, a rectangle, or the like may be used instead of a circle. Points on the 2D image of the subject are associated with the 3D coordinates of points in the point-cloud data. The measurement unit <b>187</b> acquires the 3D coordinates associated with each of the extracted points from the point-cloud data.</p><p id="p-0270" num="0269">Alternatively, the measurement unit <b>187</b> extracts all or some of the points in a sphere circumscribing the three-dimensional reference region from the point-cloud data. A cube, a cylinder, or the like may be used instead of a sphere. The measurement unit <b>187</b> acquires the 3D coordinates of each of the extracted points from the point-cloud data.</p><p id="p-0271" num="0270">The measurement unit <b>187</b> calculates the 3D distance between the reference surface and each of the extracted points. The measurement unit <b>187</b> obtains the maximum value of the calculated 3D distance as a measurement result. In this way, the measurement unit <b>187</b> can accurately measure the depth of a recessed portion on the surface of the subject or can accurately measure the height of a projection portion on the surface of the subject.</p><p id="p-0272" num="0271">A user may designate a measurement mode. For example, before Step S<b>103</b> is executed, a user inputs mode information indicating a measurement mode into the operation unit <b>4</b> by operating the operation unit <b>4</b>. The mode information is output to the CPU <b>18</b><i>a </i>through the control interface <b>17</b>. The control unit <b>180</b> sets the measurement mode on the basis of the mode information. In the example shown in the third embodiment, the mode information indicates the surface-based measurement. The mode information may indicate a mode that does not include measurement. In such a case, the surface estimation processing shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref> is executed.</p><p id="p-0273" num="0272">After the point-setting unit <b>185</b> accepts the reference point in Step S<b>103</b>, a user may change the reference point.</p><p id="p-0274" num="0273">Region information indicating at least one of the size of the reference region and the position of the reference region may be recorded on a recording medium in advance. The region-setting unit <b>182</b> may set at least one of the size of the reference region and the position of the reference region on the basis of the region information in Step S<b>104</b>.</p><p id="p-0275" num="0274">Two or more pieces of the region information may be recorded on a recording medium in advance. The region-setting unit <b>182</b> may select one of the two or more pieces of the region information on the basis of information input through the operation unit <b>4</b> in Step S<b>104</b>. The region-setting unit <b>182</b> may set one of the size of the reference region and the position of the reference region on the basis of the selected region information in Step S<b>104</b>.</p><p id="p-0276" num="0275">After the region-setting unit <b>182</b> sets the reference region in Step S<b>104</b>, a user may change at least one of the size of the reference region and the position of the reference region at any timing. The timing may come after the reference region is displayed in Step S<b>105</b> or after the reference surface is displayed in Step S<b>107</b>.</p><p id="p-0277" num="0276">The region-setting unit <b>182</b> may set a reference region without using a reference point. Accordingly, Step S<b>103</b> does not need to be executed. In a case in which the measurement unit <b>187</b> extracts points used for measurement from the 2D image of the subject or the point-cloud data, Step S<b>108</b> does not need to be executed. The CPU <b>18</b><i>a </i>does not need to have the functions of the position calculation unit <b>184</b> and the point-setting unit <b>185</b>.</p><p id="p-0278" num="0277">The display control unit <b>183</b> may display the 3D image of the point-cloud data on the display unit <b>5</b> and may display the three or more points of the point-cloud data used for estimating the reference surface on the 3D image. The display control unit <b>183</b> may display the 2D image of the subject on the display unit <b>5</b> and may display three or more points corresponding to the three or more points of the point-cloud data used for estimating the reference surface on the 2D image.</p><p id="p-0279" num="0278">The reference region does not need to be displayed. Accordingly, Step S<b>105</b> does not need to be executed. The reference surface does not need to be displayed. Accordingly, Step S<b>107</b> does not need to be executed. The measurement result does not need to be displayed. Accordingly, Step S<b>110</b> does not need to be executed.</p><p id="p-0280" num="0279">In a case in which setting the reference point, displaying the reference region, displaying the reference surface, and displaying the measurement result are unnecessary, an image does not need to be displayed. Accordingly, Step S<b>102</b> does not need to be executed. The CPU <b>18</b><i>a </i>does not need to have the function of the display control unit <b>183</b>.</p><p id="p-0281" num="0280">The order of processing in the 3D measurement is not limited to that shown in <figref idref="DRAWINGS">FIG. <b>21</b></figref>. For example, the 2D image of the subject may be displayed on the display unit <b>5</b> before Step S<b>101</b> is executed.</p><p id="p-0282" num="0281">In the third embodiment, the endoscope device <b>1</b> measures the size of the subject. Since the reference surface is accurately estimated, the endoscope device <b>1</b> can obtain an accurate measurement result.</p><heading id="h-0013" level="1">Fourth Embodiment</heading><p id="p-0283" num="0282">A fourth embodiment of the present invention will be described. The endoscope device <b>1</b> according to the fourth embodiment includes the CPU <b>18</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> or the CPU <b>18</b><i>a </i>shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref>.</p><p id="p-0284" num="0283">There is a case in which the height or the depth on the surface of a subject is different between a predetermined region and a region around the predetermined region. The predetermined region is a recessed portion, a projection portion, a step, or the like. In a case in which the calculated reference surface approximates the surface including the predetermined region and the region around the predetermined region, the accuracy of the reference surface deteriorates. In the fourth embodiment, the surface estimation unit <b>186</b> estimates a reference surface that approximates a surface excluding the predetermined region.</p><p id="p-0285" num="0284">The region-setting unit <b>182</b> sets a reference region including a region that is not a convex set. An object that is convex in Euclidean space is defined as follows. When a line segment connecting any two points included in the object together is defined and any point on the line segment is included in the object, the object is convex.</p><p id="p-0286" num="0285">A reference region in the fourth embodiment will be described by using <figref idref="DRAWINGS">FIG. <b>22</b></figref> and <figref idref="DRAWINGS">FIG. <b>23</b></figref>. A reference region R<b>51</b> shown in <figref idref="DRAWINGS">FIG. <b>22</b></figref> includes a point P<b>51</b>, a point P<b>52</b>, and a point P<b>53</b>. A line segment L<b>51</b> connects the point P<b>51</b> and the point P<b>52</b> together. The point P<b>53</b> is on the line segment L<b>51</b>. The point P<b>53</b> is included in the reference region R<b>51</b>. Points on the line segment L<b>51</b> other than the point P<b>53</b> are not shown in <figref idref="DRAWINGS">FIG. <b>22</b></figref>. All the points on the line segment L<b>51</b> are included in the reference region R<b>51</b>. Points other than the point P<b>51</b>, the point P<b>52</b>, or the point P<b>53</b> are not shown in <figref idref="DRAWINGS">FIG. <b>22</b></figref>. Points on a line segment connecting any two points in the reference region R<b>51</b> together are always included in the reference region R<b>51</b>. In other words, a line segment connecting any two points in the reference region R<b>51</b> together does not pass through a region outside the reference region R<b>51</b>. Accordingly, the reference region R<b>51</b> is a convex set.</p><p id="p-0287" num="0286">A reference region R<b>52</b> shown in <figref idref="DRAWINGS">FIG. <b>23</b></figref> includes a point P<b>54</b>, a point P<b>55</b>, and a point P<b>56</b>. A line segment L<b>52</b> connects the point P<b>54</b> and the point P<b>55</b> together. The point P<b>56</b> is on the line segment L<b>52</b>. The point P<b>56</b> is not included in the reference region R<b>52</b>. Some of the points on the line segment L<b>52</b> are not included in the reference region R<b>52</b>. In other words, the line segment L<b>52</b> passes through a region outside the reference region R<b>52</b>. Accordingly, the reference region R<b>52</b> is not a convex set. The region-setting unit <b>182</b> sets a reference region like the reference region R<b>52</b>. A region that is a convex set is not limited to a two-dimensional region and may be a three-dimensional region.</p><p id="p-0288" num="0287">Hereinafter, a reference region will be described by referring to the 3D image of the point-cloud data. <figref idref="DRAWINGS">FIG. <b>24</b></figref> shows a first example of a reference region. A reference region in the first example is a region (spherical shell) between two spheres. The centers of the two spheres are the same, and the diameters of the two spheres are different from each other.</p><p id="p-0289" num="0288">A 3D image G<b>61</b> is shown in <figref idref="DRAWINGS">FIG. <b>24</b></figref>. A user designates a reference point P<b>61</b> and a reference point P<b>62</b> by using a cursor displayed on the 3D image G<b>61</b> or by touching the screen of the display unit <b>5</b>. The point-setting unit <b>185</b> accepts the reference point P<b>61</b> and the reference point P<b>62</b>.</p><p id="p-0290" num="0289">The region-setting unit <b>182</b> calculates a first sphere having a line segment connecting the reference point P<b>61</b> and the reference point P<b>62</b> together as the diameter. The region-setting unit <b>182</b> calculates a second sphere. The center of the first sphere and the center of the second sphere are the same. The diameter of the second sphere is less than that of the first sphere. The region-setting unit <b>182</b> sets a region between the first sphere and the second sphere as a reference region R<b>61</b>. The surface estimation unit <b>186</b> estimates a reference surface on the basis of three or more points included in the reference region R<b>61</b>.</p><p id="p-0291" num="0290">The boundaries of the reference region R<b>61</b> include a first boundary R<b>61</b><i>a </i>and a second boundary R<b>61</b><i>b </i>that is on the inner side of the first boundary R<b>61</b><i>a</i>. The first boundary R<b>61</b><i>a </i>is on the relatively outer side, and the second boundary R<b>61</b><i>b </i>is on the relatively inner side. The first boundary R<b>61</b><i>a </i>surrounds the second boundary R<b>61</b><i>b</i>. The first boundary R<b>61</b><i>a </i>is the same as the first sphere. The second boundary R<b>61</b><i>b </i>is the same as the second sphere.</p><p id="p-0292" num="0291">A region R<b>62</b> is shown in <figref idref="DRAWINGS">FIG. <b>24</b></figref>. The height or the depth of the region R<b>62</b> is different from that of a region around the region R<b>62</b>. The first boundary R<b>61</b><i>a </i>and the second boundary R<b>61</b><i>b </i>are around the region R<b>62</b>. The reference region R<b>61</b> surrounds the region R<b>62</b>. Points in the region R<b>62</b> are not included in the reference region R<b>61</b>. Therefore, the surface estimation unit <b>186</b> can accurately estimate a reference surface.</p><p id="p-0293" num="0292">A user may correct the size of the first sphere or the second sphere. In an example in which the display unit <b>5</b> and the operation unit <b>4</b> are constituted as a touch panel, a user may correct the size of the first sphere or the second sphere by performing a drag operation.</p><p id="p-0294" num="0293">In the example shown in <figref idref="DRAWINGS">FIG. <b>24</b></figref>, a user designates two reference points. <figref idref="DRAWINGS">FIG. <b>25</b></figref> shows an example in which a user designates one reference point.</p><p id="p-0295" num="0294">The same 3D image G<b>61</b> as that shown in <figref idref="DRAWINGS">FIG. <b>24</b></figref> is shown in <figref idref="DRAWINGS">FIG. <b>25</b></figref>. A user designates a reference point P<b>63</b> by using a cursor displayed on the 3D image G<b>61</b> or by touching the screen of the display unit <b>5</b>. The point-setting unit <b>185</b> accepts the reference point P<b>63</b>.</p><p id="p-0296" num="0295">The region-setting unit <b>182</b> calculates a first sphere and a second sphere, each of which has the reference points P<b>63</b> at the center. The diameter of the second sphere is less than that of the first sphere. The region-setting unit <b>182</b> sets a region between the first sphere and the second sphere as a reference region R<b>63</b>.</p><p id="p-0297" num="0296">After the region-setting unit <b>182</b> sets the reference region by using the method shown in <figref idref="DRAWINGS">FIG. <b>24</b></figref> or <figref idref="DRAWINGS">FIG. <b>25</b></figref>, the region-setting unit <b>182</b> may exclude a partial region from the reference region. <figref idref="DRAWINGS">FIG. <b>26</b></figref> shows a method of excluding a partial region from the reference region.</p><p id="p-0298" num="0297">The same 3D image G<b>12</b> as that shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref> is displayed on the display unit <b>5</b>. After a reference region R<b>64</b> is set, a user designates a region to be excluded from the reference region R<b>64</b>. In an example in which the display unit <b>5</b> and the operation unit <b>4</b> are constituted as a touch panel, a user may exclude a partial region from the reference region R<b>64</b> by performing a pinch operation. The boundaries of the reference region R<b>64</b> include a first boundary R<b>64</b><i>a </i>and a second boundary R<b>64</b><i>b </i>that is on the inner side of the first boundary R<b>64</b><i>a. </i></p><p id="p-0299" num="0298"><figref idref="DRAWINGS">FIG. <b>27</b></figref> shows a second example of a reference region. A reference region in the second example includes only a region around a side of a cuboid. A 3D image G<b>63</b> is shown in <figref idref="DRAWINGS">FIG. <b>27</b></figref>. A user designates a reference point P<b>64</b> and a reference point P<b>65</b> by using a cursor displayed on the 3D image G<b>63</b> or by touching the screen of the display unit <b>5</b>. The point-setting unit <b>185</b> accepts the reference point P<b>64</b> and the reference point P<b>65</b>.</p><p id="p-0300" num="0299">The region-setting unit <b>182</b> calculates a rectangle on the basis of the reference point P<b>64</b> and the reference point P<b>65</b>. The reference point P<b>64</b> and the reference point P<b>65</b> are vertices on the diagonal line of the rectangle. After the reference point P<b>64</b> and the reference point P<b>65</b> are set, a user designates a reference point P<b>66</b> by using a cursor displayed on the 3D image G<b>63</b> or by touching the screen of the display unit <b>5</b>. The point-setting unit <b>185</b> accepts the reference point P<b>66</b>.</p><p id="p-0301" num="0300">The region-setting unit <b>182</b> calculates a cuboid on the basis of the reference point P<b>64</b>, the reference point P<b>65</b>, and the reference point P<b>66</b>. One of the surfaces of the cuboid is defined by a rectangle calculated on the basis of the reference point P<b>64</b> and the reference point P<b>65</b>. The height of the cuboid is the same as the 3D distance between the rectangle and the reference point P<b>66</b>.</p><p id="p-0302" num="0301">The region-setting unit <b>182</b> calculates a cylinder having each of the sides of the cuboid as a center axis. Since the cuboid has six sides, the region-setting unit <b>182</b> calculates six cylinders. The region-setting unit <b>182</b> sets a reference region R<b>65</b> that is a set of regions in the six cylinders. The shape of a region included in the reference region R<b>65</b> is not limited to a cylinder. The surface estimation unit <b>186</b> estimates a reference surface on the basis of three or more points included in the reference region R<b>65</b>.</p><p id="p-0303" num="0302">The region-setting unit <b>182</b> may set a two-dimensional reference region that is not a convex set. For example, the region-setting unit <b>182</b> may calculate a first circle and a second circle on the 2D image of the subject. The center of the first circle and the center of the second circle are the same. The diameter of the second circle is less than that of the first circle. The region-setting unit <b>182</b> may set a region between the first circle and the second circle as a reference region. A square, a rectangle, or the like may be used instead of a circle.</p><p id="p-0304" num="0303">The surface estimation unit <b>186</b> estimates a reference surface on the basis of three or more points in the point-cloud data corresponding to three or more points included in the reference region. For example, the surface estimation unit <b>186</b> extracts all the points in the reference region. The surface estimation unit <b>186</b> estimates a reference surface by using the 3D coordinates of each of the extracted points. The surface estimation unit <b>186</b> may extract some of the points in the reference region.</p><p id="p-0305" num="0304">Specifically, the surface estimation unit <b>186</b> selects three or more of the four or more points included in the reference region in Step S<b>106</b> shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref> or <figref idref="DRAWINGS">FIG. <b>21</b></figref>. The surface estimation unit <b>186</b> estimates a reference surface in Step S<b>106</b> on the basis of three or more points of the point-cloud data corresponding to the three or more selected points. Since the number of points used for estimating a reference surface is reduced, the surface estimation unit <b>186</b> can reduce the processing load for estimating a reference surface.</p><p id="p-0306" num="0305">In order for the surface estimation unit <b>186</b> to select three or more points, a reference region needs to include four or more points. Four or more points of a reference region include one or more combinations, each of which is constituted by three points that form a triangle. There is no straight line passing through all the four or more points of a reference region.</p><p id="p-0307" num="0306"><figref idref="DRAWINGS">FIG. <b>28</b></figref> and <figref idref="DRAWINGS">FIG. <b>29</b></figref> show examples of points used for estimating a reference surface. In order to simplify the drawings, an example in which a two-dimensional reference region is used will be described. The following descriptions may be applied to a case in which a three-dimensional reference region is used.</p><p id="p-0308" num="0307">A reference region R<b>71</b> is shown in <figref idref="DRAWINGS">FIG. <b>28</b></figref>. The surface estimation unit <b>186</b> extracts some points (pixels of the 2D image) arranged regularly out of all the points (pixels of the 2D image) in the reference region R<b>71</b>. In <figref idref="DRAWINGS">FIG. <b>28</b></figref>, only points to be extracted are shown, and points not to be extracted are not shown. In the example shown in <figref idref="DRAWINGS">FIG. <b>28</b></figref>, the surface estimation unit <b>186</b> can extract points arranged uniformly in a wide range by executing simple processing.</p><p id="p-0309" num="0308">A reference region R<b>72</b> is shown in <figref idref="DRAWINGS">FIG. <b>29</b></figref>. The surface estimation unit <b>186</b> extracts some points (pixels of the 2D image) arranged randomly out of all the points (pixels of the 2D image) in the reference region R<b>72</b>. In <figref idref="DRAWINGS">FIG. <b>29</b></figref>, only points to be extracted are shown, and points not to be extracted are not shown. In a case in which a periodical uneven pattern exists on the surface of a subject, there is a possibility that the interval of the pattern and the interval of points to be extracted match each other in the example shown in <figref idref="DRAWINGS">FIG. <b>29</b></figref>. Therefore, there is a possibility that only points of a region matching a specific pattern are extracted and the accuracy of a reference surface deteriorates. In the example shown in <figref idref="DRAWINGS">FIG. <b>29</b></figref>, since points of a region not matching a specific pattern are also extracted, the accuracy of a reference surface is improved.</p><p id="p-0310" num="0309">The surface estimation unit <b>186</b> may mainly extract points of a part near the boundary of the reference region. A first density is greater than a second density. The first density indicates a density at which points are extracted in a part near the boundary of the reference region. The second density indicates a density at which points are extracted in a part far from the boundary of the reference region. In this way, the surface estimation unit <b>186</b> can use many points in a part near the boundary of the reference region. In addition, the surface estimation unit <b>186</b> can restrict the total number of points used for estimating a reference surface and can extract points in a wide range.</p><p id="p-0311" num="0310">The surface estimation unit <b>186</b> may extract points inside the boundary of the reference region or may extract all or some of the points on the boundary of the reference region by using the above-described method.</p><p id="p-0312" num="0311">In the fourth embodiment, a reference region including a region that is not a convex set is set. The endoscope device <b>1</b> can exclude, from the reference region, points in a region having a different shape from that of a surrounding region. Therefore, the endoscope device <b>1</b> can improve the accuracy of a reference surface.</p><heading id="h-0014" level="1">Fifth Embodiment</heading><p id="p-0313" num="0312">A fifth embodiment of the present invention will be described. The endoscope device <b>1</b> according to the fifth embodiment includes the CPU <b>18</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> or the CPU <b>18</b><i>a </i>shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref>.</p><p id="p-0314" num="0313">In the fifth embodiment, a reference region includes two or more regions. The two or more regions are apart from each other. Each of the two or more regions included in the reference region includes three or more points. The reference region may include a region including three or more points and a region including one or two points. Three or more points included in each of the regions include one or more combinations, each of which is constituted by three points that form a triangle. Alternatively, three or more points in the entire reference region include one or more combinations, each of which is constituted by three points that form a triangle.</p><p id="p-0315" num="0314">In each of the regions included in the reference region, there is no straight line passing through all the three or more points. Alternatively, in the entire reference region including two or more regions, there is no straight line passing through all the points. In other words, even when all the points are on a straight line in one region included in the reference region, the straight line does not pass through one or more points in another region included in the reference region.</p><p id="p-0316" num="0315"><figref idref="DRAWINGS">FIG. <b>30</b></figref> shows an example of a reference region. In order to simplify the drawings, an example in which a two-dimensional reference region is used will be described. The following descriptions may be applied to a case in which a three-dimensional reference region is used.</p><p id="p-0317" num="0316">A 2D image G<b>81</b> of a subject is shown in <figref idref="DRAWINGS">FIG. <b>30</b></figref>. A user designates a reference point P<b>81</b>, a reference point P<b>82</b>, a reference point P<b>83</b>, and a reference point P<b>84</b> by using a cursor displayed on the 2D image G<b>81</b> or by touching the screen of the display unit <b>5</b>. The point-setting unit <b>185</b> accepts the reference point P<b>81</b>, the reference point P<b>82</b>, the reference point P<b>83</b>, and the reference point P<b>84</b>.</p><p id="p-0318" num="0317">The region-setting unit <b>182</b> sets a region R<b>81</b> having the reference point P<b>81</b> at the center. The region-setting unit <b>182</b> sets a circle having the diameter of a predetermined value as the region R<b>81</b>. A square, a rectangle, or the like may be used instead of a circle. Similarly, the region-setting unit <b>182</b> sets a region R<b>82</b> having the reference point P<b>82</b> at the center, sets a region R<b>83</b> having the reference point P<b>83</b> at the center, and sets a region R<b>84</b> having the reference point P<b>84</b> at the center. The reference region includes the region R<b>81</b>, the region R<b>82</b>, the region R<b>83</b>, and the region R<b>84</b>. The region R<b>81</b>, the region R<b>82</b>, the region R<b>83</b>, and the region R<b>84</b> are apart from each other.</p><p id="p-0319" num="0318">A region R<b>85</b> is shown in <figref idref="DRAWINGS">FIG. <b>30</b></figref>. The height or the depth of the region R<b>85</b> is different from that of a region around the region R<b>85</b>. The region R<b>81</b>, the region R<b>82</b>, the region R<b>83</b>, and the region R<b>84</b> are disposed at positions surrounding the region R<b>85</b>. Points in the region R<b>85</b> are not included in the reference region. Therefore, the surface estimation unit <b>186</b> can accurately estimate a reference surface.</p><p id="p-0320" num="0319">In the example shown in <figref idref="DRAWINGS">FIG. <b>30</b></figref>, a user designates four reference points. <figref idref="DRAWINGS">FIG. <b>31</b></figref> shows an example in which a user designates one reference point.</p><p id="p-0321" num="0320">The same 2D image G<b>81</b> as that shown in <figref idref="DRAWINGS">FIG. <b>30</b></figref> is shown in <figref idref="DRAWINGS">FIG. <b>31</b></figref>. The same region R<b>85</b> as that shown in <figref idref="DRAWINGS">FIG. <b>30</b></figref> is shown in <figref idref="DRAWINGS">FIG. <b>31</b></figref>. A user designates a reference point P<b>85</b> by using a cursor displayed on the 2D image G<b>81</b> or by touching the screen of the display unit <b>5</b>. The point-setting unit <b>185</b> accepts the reference point P<b>85</b>.</p><p id="p-0322" num="0321">The region-setting unit <b>182</b> sets a region R<b>86</b>, a region R<b>87</b>, a region R<b>88</b>, and a region R<b>89</b>, each of which has a point around the reference point P<b>85</b> at the center. For example, the region-setting unit <b>182</b> extracts four points that are a predetermined distance apart from the reference point P<b>85</b>. The region-setting unit <b>182</b> sets a region having each of the extracted points at the center.</p><p id="p-0323" num="0322">In the fifth embodiment, a reference region including two or more regions is set. The endoscope device <b>1</b> can set a reference region that does not include a region having a different shape from that of a surrounding region. Therefore, the endoscope device <b>1</b> can improve the accuracy of a reference surface.</p><heading id="h-0015" level="1">Sixth Embodiment</heading><p id="p-0324" num="0323">A sixth embodiment of the present invention will be described. In the sixth embodiment, the boundary of a reference region is set on the basis of the state of the surface of a subject.</p><p id="p-0325" num="0324">In the sixth embodiment, the CPU <b>18</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> is changed to a CPU <b>18</b><i>b </i>shown in <figref idref="DRAWINGS">FIG. <b>32</b></figref>. <figref idref="DRAWINGS">FIG. <b>32</b></figref> shows a functional configuration of the CPU <b>18</b><i>b</i>. The CPU <b>18</b><i>b </i>has functional units including a control unit <b>180</b>, a generation unit <b>181</b>, a region-setting unit <b>182</b>, a display control unit <b>183</b>, a position calculation unit <b>184</b>, a point-setting unit <b>185</b>, a surface estimation unit <b>186</b>, and a state determination unit <b>188</b>. At least one of the blocks shown in <figref idref="DRAWINGS">FIG. <b>32</b></figref> may be constituted by a different circuit from the CPU <b>18</b><i>b</i>. The same configuration as that shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> will not be described.</p><p id="p-0326" num="0325">Each unit shown in <figref idref="DRAWINGS">FIG. <b>32</b></figref> may be constituted by at least one of a processor and a logic circuit. Each unit shown in <figref idref="DRAWINGS">FIG. <b>32</b></figref> may include one or a plurality of processors. Each unit shown in <figref idref="DRAWINGS">FIG. <b>32</b></figref> may include one or a plurality of logic circuits.</p><p id="p-0327" num="0326">The display control unit <b>183</b> displays one of a 3D image of the point-cloud data and a 2D image of a subject on the display unit <b>5</b> (image display step). The point-setting unit <b>185</b> accepts position information input through the operation unit <b>4</b> (position input step). The position information indicates a position on the 3D image of the point-cloud data or the 2D image of the subject displayed on the display unit <b>5</b>. The state determination unit <b>188</b> determines a state of the subject (state determination step). The region-setting unit <b>182</b> determines a boundary of a reference region on the basis of both the position indicated by the position information and the state of the subject in a region-setting step.</p><p id="p-0328" num="0327">Surface estimation processing in the sixth embodiment will be described by using <figref idref="DRAWINGS">FIG. <b>33</b></figref>. <figref idref="DRAWINGS">FIG. <b>33</b></figref> shows a procedure of the surface estimation processing. The same processing as that shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref> will not be described.</p><p id="p-0329" num="0328">The display control unit <b>183</b> displays the 3D image of the point-cloud data on the display unit <b>5</b> in Step S<b>102</b>. Step S<b>102</b> corresponds to the image display step.</p><p id="p-0330" num="0329">The point-setting unit <b>185</b> accepts two reference points on the 3D image of the point-cloud data through the operation unit <b>4</b> and generates point information indicating each of the two accepted reference points in Step S<b>103</b>. Step S<b>103</b> corresponds to the position input step.</p><p id="p-0331" num="0330">The two reference points input in Step S<b>103</b> indicate pixel positions on the 3D image of the point-cloud data. Accordingly, the position information is input in Step S<b>103</b>, and the point-setting unit <b>185</b> accepts the position information. The point-setting unit <b>185</b> generates point information including the 3D coordinates of the two reference points indicated by the position information.</p><p id="p-0332" num="0331">After Step S<b>103</b>, the state determination unit <b>188</b> determines a state of the subject (Step S<b>111</b>). Step S<b>111</b> corresponds to the state determination step.</p><p id="p-0333" num="0332">The state determination unit <b>188</b> determines a state of the subject by using at least one of the point-cloud data, the 3D image of the point-cloud data, and the 2D image of the subject. The state determination unit <b>188</b> divides a region on the 3D image into two or more regions (segments) on the basis of the state.</p><p id="p-0334" num="0333">For example, the state determination unit <b>188</b> determines a feature of the 3D shape of the subject by using the point-cloud data. The state determination unit <b>188</b> divides (classifies) three or more points corresponding to the three-dimensional coordinates included in the point-cloud data into two or more regions on the basis of the feature. For example, the state determination unit <b>188</b> assigns each of the three or more points to one of two or more regions by using Euclidean cluster extraction. The state determination unit <b>188</b> associates each of the regions with a region on the 3D image of the point-cloud data. In this way, the state determination unit <b>188</b> divides the region on the 3D image into two or more regions.</p><p id="p-0335" num="0334">The state determination unit <b>188</b> may calculate a normal line perpendicular to the surface of the subject on the basis of the point-cloud data and may detect an edge or a step of the subject on the basis of the change in the direction of the normal line. The state determination unit <b>188</b> may divide three or more points in the point-cloud data into a first region and a second region. For example, the first region is constituted by points on the edge or the step. The second region is constituted by points on a part other than the edge or the step. The state determination unit <b>188</b> may divide a region on the 3D image of the point-cloud data into two regions by associating each of the regions of the point-cloud data with the region on the 3D image.</p><p id="p-0336" num="0335">The state determination unit <b>188</b> may determine a state of the subject by using the 2D image of the subject or the 3D image of the point-cloud data. For example, the state determination unit <b>188</b> may detect an edge of the subject by performing image processing on the 2D image of the subject or the 3D image of the point-cloud data. The state determination unit <b>188</b> may divide three or more points in the 2D image of the subject into a first region and a second region. For example, the first region is constituted by points on the edge. The second region is constituted by points on a part other than the edge. The state determination unit <b>188</b> may divide a region on the 3D image of the point-cloud data into two regions by associating each of the regions on the 2D image of the subject with the region on the 3D image. Similarly, the state determination unit <b>188</b> may divide three or more points in the 3D image of the point-cloud data into a first region and a second region.</p><p id="p-0337" num="0336">The state determination unit <b>188</b> may determine a feature of the subject on the basis of the brightness or the color of the 2D image of the subject or the 3D image of the point-cloud data. The state determination unit <b>188</b> may perform matching processing on a stereo image of the subject and may determine a feature of the subject on the basis of the correlation value obtained in the matching processing. The state determination unit <b>188</b> may determine a feature of the subject by using a watershed algorithm, deep learning, or the like.</p><p id="p-0338" num="0337">After Step S<b>111</b>, the region-setting unit <b>182</b> sets a reference region on the basis of the positions of the two reference points and the state of the subject (Step S<b>104</b><i>a</i>). Step S<b>104</b><i>a </i>corresponds to the region-setting step.</p><p id="p-0339" num="0338">The region-setting unit <b>182</b> sets a three-dimensional reference region in the 3D space defining the 3D coordinates of the point-cloud data. For example, the region-setting unit <b>182</b> sets an outer boundary of the reference region on the basis of the positions of the two reference points. The region-setting unit <b>182</b> sets an inner boundary of the reference region on the basis of the state of the subject. For example, there is a case in which there is an abnormal region on the surface of the subject. The height or the depth of the abnormal region is different from that of a region around the abnormal region. The region-setting unit <b>182</b> excludes the abnormal region from the reference region by setting the inner boundary of the reference region. In this way, the region-setting unit <b>182</b> can set the reference region that does not include the abnormal region. After Step S<b>104</b><i>a</i>, Step S<b>105</b> is executed.</p><p id="p-0340" num="0339"><figref idref="DRAWINGS">FIG. <b>34</b></figref> shows an example of an image displayed on the display unit <b>5</b>. A 3D image G<b>91</b> is displayed on the display unit <b>5</b>. A user designates a reference point P<b>91</b> and a reference point P<b>92</b> by using a cursor displayed on the 3D image G<b>91</b> or by touching the screen of the display unit <b>5</b>. The point-setting unit <b>185</b> accepts the reference point P<b>91</b> and the reference point P<b>92</b>.</p><p id="p-0341" num="0340">The region-setting unit <b>182</b> calculates a sphere SP<b>91</b> having a line segment connecting the reference point P<b>91</b> and the reference point P<b>92</b> together as the diameter. The region-setting unit <b>182</b> sets the sphere SP<b>91</b> as the outer boundary of a reference region.</p><p id="p-0342" num="0341"><figref idref="DRAWINGS">FIG. <b>35</b></figref> shows two or more regions on the 3D image of the point-cloud data. The state determination unit <b>188</b> divides a region on the 3D image into two or more regions on the basis of the state of the subject. <figref idref="DRAWINGS">FIG. <b>35</b></figref> shows such two or more regions. In the example shown in <figref idref="DRAWINGS">FIG. <b>35</b></figref>, a region of the subject on the 3D image is divided into a region R<b>91</b>, a region R<b>92</b>, and a region R<b>93</b>.</p><p id="p-0343" num="0342">The region-setting unit <b>182</b> sets a region having the greatest volume among the two or more regions in the sphere SP<b>91</b> as a reference region. Regions other than the region set as the reference region are not included in the reference region. Part of the region R<b>91</b> and the entire region R<b>92</b> are in the sphere SP<b>91</b>. The region R<b>91</b> in the sphere SP<b>91</b> is included in the reference region. The region R<b>92</b> is not included in the reference region. The region-setting unit <b>182</b> sets the boundary of the region R<b>92</b> as the inner boundary of the reference region.</p><p id="p-0344" num="0343">In the above-described example, the state determination unit <b>188</b> divides three or more points corresponding to the three-dimensional coordinates included in the point-cloud data into two or more regions. The boundary of the reference region includes boundaries of one or more regions included in the two or more regions.</p><p id="p-0345" num="0344">Each of the above-described two or more regions includes three or more points. The above-described two or more regions may include a region including three or more points and a region including one or two points. The reference region includes a region including three or more points.</p><p id="p-0346" num="0345">In the example shown in <figref idref="DRAWINGS">FIG. <b>35</b></figref>, the state determination unit <b>188</b> divides three or more points included in the point-cloud data into three regions. The boundary of the reference region includes the boundary of the region R<b>92</b>. The state determination unit <b>188</b> may set a reference region surrounding the region R<b>92</b> and the region R<b>93</b>, and the boundary of the reference region may include the boundary of the region R<b>92</b> and the boundary of the region R<b>93</b>.</p><p id="p-0347" num="0346">The outer boundary of the reference region may be a cuboid, a cube, a cylinder, or the like. The shape of the reference region is not limited to these examples.</p><p id="p-0348" num="0347">In the example shown in <figref idref="DRAWINGS">FIG. <b>34</b></figref>, a user designates two reference points. <figref idref="DRAWINGS">FIG. <b>36</b></figref> shows an example in which a user designates one reference point.</p><p id="p-0349" num="0348">The same 3D image G<b>91</b> as that shown in <figref idref="DRAWINGS">FIG. <b>34</b></figref> is shown in <figref idref="DRAWINGS">FIG. <b>36</b></figref>. A user designates a reference point P<b>93</b> by using a cursor displayed on the 3D image G<b>91</b> or by touching the screen of the display unit <b>5</b>. The point-setting unit <b>185</b> accepts the reference point P<b>93</b>.</p><p id="p-0350" num="0349">The region-setting unit <b>182</b> calculates a sphere SP<b>92</b> having the reference point P<b>93</b> at the center. The diameter of the sphere SP<b>92</b> is a predetermined length. The region-setting unit <b>182</b> sets the sphere SP<b>92</b> as the outer boundary of a reference region. The state determination unit <b>188</b> sets the boundary of the region R<b>92</b> shown in <figref idref="DRAWINGS">FIG. <b>35</b></figref> as the inner boundary of the reference region.</p><p id="p-0351" num="0350">After the region-setting unit <b>182</b> sets the outer boundary of the reference region, the state determination unit <b>188</b> may divide a region only inside the boundary into two or more regions. In the example shown in <figref idref="DRAWINGS">FIG. <b>34</b></figref>, after the region-setting unit <b>182</b> sets the sphere SP<b>91</b> as the outer boundary of the reference region, the state determination unit <b>188</b> may divide a region inside the boundary into two or more regions. The processing load is reduced, compared to a case in which all of the point-cloud data are processed.</p><p id="p-0352" num="0351">The 2D image of the subject may be used instead of the 3D image of the point-cloud data. Hereinafter, an example in which the 2D image of the subject is used will be described.</p><p id="p-0353" num="0352">The display control unit <b>183</b> displays the 2D image of the subject on the display unit <b>5</b> in Step S<b>102</b>. The point-setting unit <b>185</b> accepts one or more reference points on the 2D image of the subject through the operation unit <b>4</b> and generates point information indicating each of the one or more accepted reference points in Step S<b>103</b>.</p><p id="p-0354" num="0353">The state determination unit <b>188</b> determines a state of the subject by using the 2D image of the subject in Step S<b>111</b>. The state determination unit <b>188</b> divides a region on the 2D image into two or more regions on the basis of the result of the determination.</p><p id="p-0355" num="0354">The region-setting unit <b>182</b> sets a two-dimensional reference region on the 2D image of the subject in Step S<b>104</b><i>a</i>. For example, the region-setting unit <b>182</b> sets the outer boundary of the reference region on the basis of the positions of the reference points. The region-setting unit <b>182</b> sets the inner boundary of the reference region on the basis of the state of the subject.</p><p id="p-0356" num="0355">The CPU <b>18</b><i>b </i>may include the measurement unit <b>187</b> shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref>. After Step S<b>107</b> is executed, Step S<b>108</b>, Step S<b>109</b>, and Step S<b>110</b> shown in <figref idref="DRAWINGS">FIG. <b>21</b></figref> may be executed.</p><p id="p-0357" num="0356">After the point-setting unit <b>185</b> accepts the reference points in Step S<b>103</b>, a user may change the reference points. The number of reference points set in Step S<b>103</b> is not limited to two.</p><p id="p-0358" num="0357">Region information indicating at least one of the size of the reference region and the position of the reference region may be recorded on a recording medium in advance. The region-setting unit <b>182</b> may set at least one of the size of the reference region and the position of the reference region on the basis of the region information in Step S<b>104</b><i>a</i>. For example, the region-setting unit <b>182</b> may set at least one of the size of the outer boundary of the reference region and the position of the outer boundary of the reference region on the basis of the region information.</p><p id="p-0359" num="0358">Two or more pieces of the region information may be recorded on a recording medium in advance. The region-setting unit <b>182</b> may select one of the two or more pieces of the region information on the basis of information input through the operation unit <b>4</b> in Step S<b>104</b><i>a</i>. The region-setting unit <b>182</b> may set one of the size of the reference region and the position of the reference region on the basis of the selected region information in Step S<b>104</b><i>a. </i></p><p id="p-0360" num="0359">After the region-setting unit <b>182</b> sets the reference region in Step S<b>104</b><i>a</i>, a user may change at least one of the size of the reference region and the position of the reference region at any timing. The timing may come after the reference region is displayed in Step S<b>105</b> or after the reference surface is displayed in Step S<b>107</b>.</p><p id="p-0361" num="0360">The display control unit <b>183</b> may display the 3D image of the point-cloud data on the display unit <b>5</b> and may display the three or more points of the point-cloud data used for estimating the reference surface on the 3D image. The display control unit <b>183</b> may display the 2D image of the subject on the display unit <b>5</b> and may display three or more points corresponding to the three or more points of the point-cloud data used for estimating the reference surface on the 2D image.</p><p id="p-0362" num="0361">The reference region does not need to be displayed. Accordingly, Step S<b>105</b> does not need to be executed. The reference surface does not need to be displayed. Accordingly, Step S<b>107</b> does not need to be executed.</p><p id="p-0363" num="0362">The order of processing in the surface estimation processing is not limited to that shown in <figref idref="DRAWINGS">FIG. <b>33</b></figref>. For example, Step S<b>111</b> may be executed before Step S<b>102</b> or Step S<b>103</b> is executed. Step S<b>111</b> may be executed by using the 2D image of the subject before Step S<b>101</b> is executed.</p><p id="p-0364" num="0363">In the sixth embodiment, a reference region is set on the basis of the state of the surface of a subject. The endoscope device <b>1</b> can easily set a reference region that does not include an abnormal region. Accordingly, the endoscope device <b>1</b> can improve the accuracy of a reference surface.</p><heading id="h-0016" level="1">First Modified Example of Sixth Embodiment</heading><p id="p-0365" num="0364">A first modified example of the sixth embodiment of the present invention will be described. In the first modified example of the sixth embodiment, the state determination unit <b>188</b> divides (classifies) three or more points corresponding to the three-dimensional coordinates included in the point-cloud data into two or more regions. The display control unit <b>183</b> displays an image of the two or more regions on the display unit <b>5</b>, and the point-setting unit <b>185</b> accepts a reference point on the image.</p><p id="p-0366" num="0365">Surface estimation processing in the first modified example of the sixth embodiment will be described by using <figref idref="DRAWINGS">FIG. <b>37</b></figref>. <figref idref="DRAWINGS">FIG. <b>37</b></figref> shows a procedure of the surface estimation processing. The same processing as that shown in <figref idref="DRAWINGS">FIG. <b>33</b></figref> will not be described.</p><p id="p-0367" num="0366">After Step S<b>101</b>, the state determination unit <b>188</b> determines a state of the subject in Step S<b>111</b>. In this way, the state determination unit <b>188</b> divides three or more points corresponding to the three-dimensional coordinates included in the point-cloud data into two or more regions.</p><p id="p-0368" num="0367">After Step S<b>111</b>, the display control unit <b>183</b> displays the 3D image of the point-cloud data and an image of the two or more regions on the display unit <b>5</b> (Step S<b>102</b><i>a</i>). Specifically, the display control unit <b>183</b> generates a graphic image signal for displaying the 3D image of the point-cloud data and the image of the two or more regions. The image of the two or more regions includes color data of each pixel. The display control unit <b>183</b> outputs the generated graphic image signal to the display unit <b>5</b> through the video-signal-processing circuit <b>12</b>. The display unit <b>5</b> displays the 3D image of the point-cloud data and the image of the two or more regions.</p><p id="p-0369" num="0368">After Step S<b>102</b><i>a</i>, the point-setting unit <b>185</b> accepts two reference points on the image of the two or more regions through the operation unit <b>4</b> and generates point information indicating the accepted reference points (Step S<b>103</b><i>a</i>). After Step S<b>103</b><i>a</i>, Step S<b>104</b><i>a </i>is executed.</p><p id="p-0370" num="0369"><figref idref="DRAWINGS">FIG. <b>38</b></figref> shows an example of an image displayed on the display unit <b>5</b>. The same 3D image G<b>91</b> as that shown in <figref idref="DRAWINGS">FIG. <b>34</b></figref> and an image G<b>92</b> of the two or more regions are displayed on the display unit <b>5</b>. In the example shown in <figref idref="DRAWINGS">FIG. <b>38</b></figref>, the region of the image G<b>92</b> is divided into a region R<b>94</b>, a region R<b>95</b>, and a region R<b>96</b>.</p><p id="p-0371" num="0370">A user refers to the image G<b>92</b> and determines a position of a reference point. A user can determine a feature of a subject by comparing the 3D image G<b>91</b> with the image G<b>92</b>. For example, a user can determine that the region R<b>95</b> and the region R<b>96</b> are recessed portions or projection portions. A user determines a position of a reference point for setting a reference region that does not include regions corresponding to the region R<b>95</b> and the region R<b>96</b>. For example, a user designates a reference point on the largest region R<b>94</b>.</p><p id="p-0372" num="0371">A user designates a reference point P<b>94</b> and a reference point P<b>95</b> by using a cursor displayed on the image G<b>92</b> or by touching the screen of the display unit <b>5</b>. The point-setting unit <b>185</b> accepts the reference point P<b>94</b> and the reference point P<b>95</b>.</p><p id="p-0373" num="0372">The region-setting unit <b>182</b> calculates a sphere SP<b>93</b> having a line segment connecting the reference point P<b>94</b> and the reference point P<b>95</b> together as the diameter. The region-setting unit <b>182</b> sets the sphere SP<b>93</b> as the outer boundary of a reference region.</p><p id="p-0374" num="0373">The region-setting unit <b>182</b> sets a region having the greatest volume among two or more regions in the sphere SP<b>93</b> as a reference region. Regions other than the region set as the reference region are not included in the reference region. Part of the region R<b>94</b> and the entire region R<b>95</b> are in the sphere SP<b>93</b>. The region R<b>94</b> in the sphere SP<b>93</b> is included in the reference region. The region R<b>95</b> is not included in the reference region. The region-setting unit <b>182</b> sets the boundary of the region R<b>95</b> as the inner boundary of the reference region.</p><p id="p-0375" num="0374">The outer boundary of the reference region may be a cuboid, a cube, a cylinder, or the like. The shape of the reference region is not limited to these examples.</p><p id="p-0376" num="0375">The display control unit <b>183</b> may display the 2D image of the subject on the display unit <b>5</b> instead of the 3D image of the point-cloud data in Step S<b>102</b><i>a. </i></p><p id="p-0377" num="0376">The CPU <b>18</b><i>b </i>may include the measurement unit <b>187</b> shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref>. After Step S<b>107</b> is executed, Step S<b>108</b>, Step S<b>109</b>, and Step S<b>110</b> shown in <figref idref="DRAWINGS">FIG. <b>21</b></figref> may be executed.</p><p id="p-0378" num="0377">After the point-setting unit <b>185</b> accepts the reference points in Step S<b>103</b><i>a</i>, a user may change the reference points. The number of reference points set in Step S<b>103</b><i>a </i>is not limited to two.</p><p id="p-0379" num="0378">After the region-setting unit <b>182</b> sets the reference region in Step S<b>104</b><i>a</i>, a user may change at least one of the size of the reference region and the position of the reference region at any timing. The timing may come after the reference region is displayed in Step S<b>105</b> or after the reference surface is displayed in Step S<b>107</b>.</p><p id="p-0380" num="0379">The display control unit <b>183</b> may display the 3D image of the point-cloud data on the display unit <b>5</b> and may display the three or more points of the point-cloud data used for estimating the reference surface on the 3D image. The display control unit <b>183</b> may display the 2D image of the subject on the display unit <b>5</b> and may display three or more points corresponding to the three or more points of the point-cloud data used for estimating the reference surface on the 2D image.</p><p id="p-0381" num="0380">The display control unit <b>183</b> may display an image of the two or more regions generated by dividing the three or more points in the point-cloud data on the display unit <b>5</b>. The display control unit <b>183</b> may display three or more points on the image corresponding to the three or more points of the point-cloud data used for estimating the reference surface.</p><p id="p-0382" num="0381">The reference region does not need to be displayed. Accordingly, Step S<b>105</b> does not need to be executed. The reference surface does not need to be displayed. Accordingly, Step S<b>107</b> does not need to be executed.</p><p id="p-0383" num="0382">In the first modified example of the sixth embodiment, a reference region is set on the basis of the state of the surface of a subject. The endoscope device <b>1</b> can easily set a reference region that does not include an abnormal region. Accordingly, the endoscope device <b>1</b> can improve the accuracy of a reference surface.</p><heading id="h-0017" level="1">Second Modified Example of Sixth Embodiment</heading><p id="p-0384" num="0383">A second modified example of the sixth embodiment of the present invention will be described. In the second modified example of the sixth embodiment, a curvature map is used for setting a reference region. The curvature map indicates the distribution of curvatures in the 3D shape of a subject indicated by the point-cloud data. For example, the curvature map includes 3D coordinates of a point and a curvature at the point. A reference region includes a region in which curvatures fall within a predetermined range.</p><p id="p-0385" num="0384">Surface estimation processing in the second modified example of the sixth embodiment will be described. The surface estimation processing in the second modified example of the sixth embodiment is executed in accordance with <figref idref="DRAWINGS">FIG. <b>33</b></figref>. Hereinafter, the same processing as the surface estimation processing in the sixth embodiment will not be described.</p><p id="p-0386" num="0385">The state determination unit <b>188</b> calculates a curvature at each point of the point-cloud data and generates a curvature map in Step S<b>111</b>. The state determination unit <b>188</b> determines a state of a subject on the basis of the curvature map. Specifically, the state determination unit <b>188</b> determines a curvature of the surface of the subject by using the following method.</p><p id="p-0387" num="0386">The state determination unit <b>188</b> sets, in the 3D space, a three-dimensional determination region having a reference point on the 3D image of the point-cloud data at the center. The state determination unit <b>188</b> acquires a curvature associated with each point in the determination region from the curvature map. The state determination unit <b>188</b> calculates the difference between the curvature at the reference point and the curvature at each point in the determination region.</p><p id="p-0388" num="0387">The region-setting unit <b>182</b> extracts a point at which the difference falls within a predetermined range from the determination region in Step S<b>104</b><i>a</i>. The region-setting unit <b>182</b> extracts three or more points from the determination region. The region-setting unit <b>182</b> sets a reference region including the three or more extracted points. The reference region includes all or some of the points in the determination region.</p><p id="p-0389" num="0388">The 2D image of the subject may be used instead of the 3D image of the point-cloud data. Hereinafter, an example in which the 2D image of the subject is used will be described.</p><p id="p-0390" num="0389">The state determination unit <b>188</b> calculates a curvature at each point of the point-cloud data and generates a curvature map in Step S<b>111</b>. The state determination unit <b>188</b> sets, in the 2D image of the subject, a two-dimensional determination region having the reference point accepted by the point-setting unit <b>185</b> at the center. Thereafter, similar processing to that in the example in which the 3D image of the point-cloud data is used is executed, and a reference region is set in the 2D image of the subject.</p><p id="p-0391" num="0390">The display control unit <b>183</b> may display an image of the curvature map on the display unit <b>5</b> and may display three or more points corresponding to the three or more points of the point-cloud data used for estimating the reference surface on the image.</p><p id="p-0392" num="0391">In the second modified example of the sixth embodiment, a reference region includes three or more points on a surface having a stable curvature. The endoscope device <b>1</b> can set a reference region that does not include an abnormal region. Accordingly, the endoscope device <b>1</b> can improve the accuracy of a reference surface.</p><heading id="h-0018" level="1">Third Modified Example of Sixth Embodiment</heading><p id="p-0393" num="0392">A third modified example of the sixth embodiment of the present invention will be described. In the third modified example of the sixth embodiment, the display control unit <b>183</b> displays an image of a curvature map on the display unit <b>5</b>, and the point-setting unit <b>185</b> accepts a reference point on the image.</p><p id="p-0394" num="0393">Surface estimation processing in the third modified example of the sixth embodiment will be described by using <figref idref="DRAWINGS">FIG. <b>39</b></figref>. <figref idref="DRAWINGS">FIG. <b>39</b></figref> shows a procedure of the surface estimation processing. The same processing as that shown in <figref idref="DRAWINGS">FIG. <b>33</b></figref> will not be described.</p><p id="p-0395" num="0394">After Step S<b>101</b>, the state determination unit <b>188</b> calculates a curvature at each point of the point-cloud data and generates a curvature map (Step S<b>112</b>).</p><p id="p-0396" num="0395">After Step S<b>112</b>, the display control unit <b>183</b> displays the 3D image of the point-cloud data and an image of the curvature map on the display unit <b>5</b> (Step S<b>102</b><i>b</i>). Specifically, the display control unit <b>183</b> generates a graphic image signal for displaying the 3D image of the point-cloud data and the image of the curvature map. The image of the curvature map includes color data of each pixel. The display control unit <b>183</b> outputs the generated graphic image signal to the display unit <b>5</b> through the video-signal-processing circuit <b>12</b>. The display unit <b>5</b> displays the 3D image of the point-cloud data and the image of the curvature map.</p><p id="p-0397" num="0396">After Step S<b>102</b><i>b</i>, the point-setting unit <b>185</b> accepts one reference point on the image of the curvature map through the operation unit <b>4</b> and generates point information indicating the accepted reference point (Step S<b>103</b><i>b</i>).</p><p id="p-0398" num="0397"><figref idref="DRAWINGS">FIG. <b>40</b></figref> shows an example of an image displayed on the display unit <b>5</b> in Step S<b>103</b><i>b</i>. A 3D image G<b>93</b> of the point-cloud data and an image G<b>94</b> of the curvature map are displayed on the display unit <b>5</b>. A pixel of the image G<b>94</b> is displayed in a color corresponding to the curvature at the pixel. For example, a pixel having a large curvature is displayed in a dark color, and a pixel having a small curvature is displayed in a light color.</p><p id="p-0399" num="0398">A user refers to the image G<b>94</b> and determines a position of a reference point. A user can determine a feature of a subject by comparing the 3D image G<b>93</b> with the image G<b>94</b>. For example, the curvatures of a region R<b>97</b> and a region R<b>98</b> on the image G<b>94</b> of the curvature map are higher than those of regions around the region R<b>97</b> and the region R<b>98</b>. Therefore, a user can determine that the region R<b>97</b> and the region R<b>98</b> are recessed portions or projection portions. A user determines a position of a reference point for setting a reference region that does not include regions corresponding to the region R<b>97</b> and the region R<b>98</b>.</p><p id="p-0400" num="0399">A user designates a reference point P<b>96</b> by using a cursor displayed on the image G<b>94</b> or by touching the screen of the display unit <b>5</b>. The point-setting unit <b>185</b> accepts the reference point P<b>96</b>.</p><p id="p-0401" num="0400">After Step S<b>103</b><i>b</i>, the state determination unit <b>188</b> calculates a point of the point-cloud data corresponding to the reference point accepted by the point-setting unit <b>185</b>. The state determination unit <b>188</b> sets, in the 3D space, a three-dimensional determination region having the calculated point at the center. The state determination unit <b>188</b> acquires a curvature associated with each point in the determination region from the curvature map. The state determination unit <b>188</b> calculates the difference between the curvature at the reference point and the curvature at each point in the determination region (Step S<b>111</b><i>b</i>). After Step S<b>111</b><i>b</i>, Step S<b>104</b><i>a </i>is executed.</p><p id="p-0402" num="0401">The 2D image of the subject may be used instead of the 3D image of the point-cloud data. Hereinafter, an example in which the 2D image of the subject is used will be described.</p><p id="p-0403" num="0402">After the point-setting unit <b>185</b> accepts the reference point on the image of the curvature map, the state determination unit <b>188</b> calculates a point on the 2D image of the subject corresponding to the reference point. The state determination unit <b>188</b> sets, in the 2D image of the subject, a two-dimensional determination region having the calculated point at the center. Thereafter, similar processing to that in the example in which the 3D image of the point-cloud data is used is executed, and a reference region is set in the 2D image of the subject.</p><p id="p-0404" num="0403">In the third modified example of the sixth embodiment, a reference region includes three or more points on a surface having a stable curvature. The endoscope device <b>1</b> can set a reference region that does not include an abnormal region. Accordingly, the endoscope device <b>1</b> can improve the accuracy of a reference surface.</p><heading id="h-0019" level="1">Seventh Embodiment</heading><p id="p-0405" num="0404">A seventh embodiment of the present invention will be described. The endoscope device <b>1</b> according to the seventh embodiment includes the CPU <b>18</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> or the CPU <b>18</b><i>a </i>shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref>.</p><p id="p-0406" num="0405">In the seventh embodiment, a user designates three or more points on the 3D image of the point-cloud data or the 2D image of the subject. A reference region is set on the basis of line segments connecting the three or more points together.</p><p id="p-0407" num="0406">The display control unit <b>183</b> displays one of the 3D image of the point-cloud data and the 2D image of the subject on the display unit <b>5</b> (image display step). Three or more points on the 3D image of the point-cloud data or the 2D image of the subject are input through the operation unit <b>4</b> (region-setting step). The region-setting unit <b>182</b> sets a reference region including the input three or more points (region-setting step).</p><p id="p-0408" num="0407">The region-setting unit <b>182</b> sets a reference region on the basis of the line segments connecting the input three or more points together in the region-setting step.</p><p id="p-0409" num="0408">Surface estimation processing in the seventh embodiment will be described. The surface estimation processing in the seventh embodiment is executed in accordance with <figref idref="DRAWINGS">FIG. <b>9</b></figref> or <figref idref="DRAWINGS">FIG. <b>21</b></figref>. Hereinafter, the same processing as the surface estimation processing in the second embodiment or the third embodiment will not be described.</p><p id="p-0410" num="0409">The display control unit <b>183</b> displays the 3D image of the point-cloud data on the display unit <b>5</b> in Step S<b>102</b>. Step S<b>102</b> corresponds to the image display step.</p><p id="p-0411" num="0410">The point-setting unit <b>185</b> accepts three or more reference points on the 3D image of the point-cloud data through the operation unit <b>4</b> and generates point information indicating each of the three or more accepted reference points in Step S<b>103</b>. Step S<b>103</b> corresponds to the region-setting step.</p><p id="p-0412" num="0411">The region-setting unit <b>182</b> sets a reference region on the basis of the three or more reference points indicated by the point information in Step S<b>104</b>. Step S<b>104</b> corresponds to the region-setting step.</p><p id="p-0413" num="0412"><figref idref="DRAWINGS">FIG. <b>41</b></figref> shows an example of an image displayed on the display unit <b>5</b>. A 3D image G<b>101</b> is displayed on the display unit <b>5</b>. A user designates a reference point P<b>101</b>, a reference point P<b>102</b>, and a reference point P<b>103</b> by using a cursor displayed on the 3D image G<b>101</b> or by touching the screen of the display unit <b>5</b>. The point-setting unit <b>185</b> accepts the reference point P<b>101</b>, the reference point P<b>102</b>, and the reference point P<b>103</b>.</p><p id="p-0414" num="0413">The region-setting unit <b>182</b> calculates a line segment connecting two reference points together. In the example shown in <figref idref="DRAWINGS">FIG. <b>41</b></figref>, the region-setting unit <b>182</b> calculates a line segment L<b>101</b>, a line segment L<b>102</b>, and a line segment L<b>103</b>. The line segment L<b>101</b> connects the reference point P<b>101</b> and the reference point P<b>102</b> together. The line segment L<b>102</b> connects the reference point P<b>102</b> and the reference point P<b>103</b> together. The line segment L<b>103</b> connects the reference point P<b>103</b> and the reference point P<b>101</b> together. Each of the line segment L<b>101</b>, the line segment L<b>102</b>, and the line segment L<b>103</b> may be displayed on the 3D image G<b>101</b>.</p><p id="p-0415" num="0414">The region-setting unit <b>182</b> calculates three cylinders, each of which has one of the line segment L<b>101</b>, the line segment L<b>102</b>, and the line segment L<b>103</b> as the center axis. The region-setting unit <b>182</b> sets a reference region R<b>101</b> that is a set of regions in the three cylinders. The shape of regions included in the reference region R<b>101</b> is not limited to a cylinder. The surface estimation unit <b>186</b> estimates a reference surface on the basis of three or more points included in the reference region R<b>101</b>.</p><p id="p-0416" num="0415"><figref idref="DRAWINGS">FIG. <b>42</b></figref> shows another example of an image displayed on the display unit <b>5</b>. The same 3D image G<b>101</b> as that shown in <figref idref="DRAWINGS">FIG. <b>41</b></figref> is displayed on the display unit <b>5</b>. A user designates a reference point P<b>104</b>, a reference point P<b>105</b>, a reference point P<b>106</b>, and a reference point P<b>107</b> by using a cursor displayed on the 3D image G<b>101</b> or by touching the screen of the display unit <b>5</b>. The point-setting unit <b>185</b> accepts the reference point P<b>104</b>, the reference point P<b>105</b>, the reference point P<b>106</b>, and the reference point P<b>107</b>.</p><p id="p-0417" num="0416">The region-setting unit <b>182</b> calculates a line segment connecting two reference points together. In the example shown in <figref idref="DRAWINGS">FIG. <b>42</b></figref>, the region-setting unit <b>182</b> calculates a line segment L<b>104</b>, a line segment L<b>105</b>, a line segment L<b>106</b>, and a line segment L<b>107</b>. The line segment L<b>104</b> connects the reference point P<b>104</b> and the reference point P<b>105</b> together. The line segment L<b>105</b> connects the reference point P<b>105</b> and the reference point P<b>106</b> together. The line segment L<b>106</b> connects the reference point P<b>106</b> and the reference point P<b>107</b> together. The line segment L<b>107</b> connects the reference point P<b>107</b> and the reference point P<b>104</b> together. Each of the line segment L<b>104</b>, the line segment L<b>105</b>, the line segment L<b>106</b>, and the line segment L<b>107</b> may be displayed on the 3D image G<b>101</b>.</p><p id="p-0418" num="0417">The region-setting unit <b>182</b> calculates four cylinders, each of which has one of the line segment L<b>104</b>, the line segment L<b>105</b>, the line segment L<b>106</b>, and the line segment L<b>107</b> as the center axis. The region-setting unit <b>182</b> sets a reference region R<b>102</b> that is a set of regions in the four cylinders. The shape of regions included in the reference region R<b>102</b> is not limited to a cylinder. The surface estimation unit <b>186</b> estimates a reference surface on the basis of three or more points included in the reference region R<b>102</b>.</p><p id="p-0419" num="0418">The 2D image of the subject may be used instead of the 3D image of the point-cloud data. Hereinafter, an example in which the 2D image of the subject is used will be described.</p><p id="p-0420" num="0419">The display control unit <b>183</b> displays the 2D image of the subject on the display unit <b>5</b> in Step S<b>102</b>. The point-setting unit <b>185</b> accepts three or more reference points on the 2D image of the subject through the operation unit <b>4</b> and generates point information indicating each of the three or more accepted reference points in Step S<b>103</b>. The region-setting unit <b>182</b> sets a reference region on the basis of the three or more reference points indicated by the point information in Step S<b>104</b>.</p><p id="p-0421" num="0420">After the point-setting unit <b>185</b> accepts the reference points in Step S<b>103</b>, a user may change the reference points. The number of reference points set in Step S<b>103</b> is not limited to three or four.</p><p id="p-0422" num="0421">The display control unit <b>183</b> may display the 3D image of the point-cloud data on the display unit <b>5</b> and may display the three or more points of the point-cloud data used for estimating the reference surface on the 3D image. The display control unit <b>183</b> may display the 2D image of the subject on the display unit <b>5</b> and may display three or more points corresponding to the three or more points of the point-cloud data used for estimating the reference surface on the 2D image.</p><p id="p-0423" num="0422">The reference region does not need to be displayed. Accordingly, Step S<b>105</b> does not need to be executed. The reference surface does not need to be displayed. Accordingly, Step S<b>107</b> does not need to be executed.</p><p id="p-0424" num="0423">In the seventh embodiment, a reference region including three or more points designated by a user is set. Therefore, the degree of freedom of the reference region increases in accordance with the shape of the subject or the composition of an image. The endoscope device <b>1</b> can easily set a reference region.</p><heading id="h-0020" level="1">Eighth Embodiment</heading><p id="p-0425" num="0424">An eighth embodiment of the present invention will be described. The endoscope device <b>1</b> according to the eighth embodiment includes the CPU <b>18</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> or the CPU <b>18</b><i>a </i>shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref>.</p><p id="p-0426" num="0425">In the eighth embodiment, a user designates a line on an image by tracing the 3D image of the point-cloud data or the 2D image of the subject. A reference region is set on the basis of three or more points on the designated line.</p><p id="p-0427" num="0426">The display control unit <b>183</b> displays one of the 3D image of the point-cloud data and the 2D image of the subject on the display unit <b>5</b> (image display step). Three or more points on the line designated in the 3D image of the point-cloud data or the 2D image of the subject by a user are input through the operation unit <b>4</b> (region-setting step). The region-setting unit <b>182</b> sets a reference region including three or more points on the line (region-setting step).</p><p id="p-0428" num="0427">Surface estimation processing in the eighth embodiment will be described. The surface estimation processing in the eighth embodiment is executed in accordance with <figref idref="DRAWINGS">FIG. <b>9</b></figref> or <figref idref="DRAWINGS">FIG. <b>21</b></figref>. Hereinafter, the same processing as the surface estimation processing in the second embodiment or the third embodiment will not be described.</p><p id="p-0429" num="0428">The display control unit <b>183</b> displays the 3D image of the point-cloud data on the display unit <b>5</b> in Step S<b>102</b>. Step S<b>102</b> corresponds to the image display step.</p><p id="p-0430" num="0429">A user draws a line by tracing the 3D image of the point-cloud data. The point-setting unit <b>185</b> accepts three or more reference points on the line through the operation unit <b>4</b> and generates point information indicating each of the three or more accepted reference points in Step S<b>103</b>. Step S<b>103</b> corresponds to the region-setting step.</p><p id="p-0431" num="0430">The region-setting unit <b>182</b> sets a reference region on the basis of the three or more reference points indicated by the point information in Step S<b>104</b>. Step S<b>104</b> corresponds to the region-setting step.</p><p id="p-0432" num="0431"><figref idref="DRAWINGS">FIG. <b>43</b></figref> shows an example of an image displayed on the display unit <b>5</b>. The same 3D image G<b>101</b> as that shown in <figref idref="DRAWINGS">FIG. <b>41</b></figref> is displayed on the display unit <b>5</b>. In an example in which the display unit <b>5</b> and the operation unit <b>4</b> are constituted as a touch panel, a user draws a line L<b>111</b> by tracing the screen of the display unit <b>5</b> so that the trace forms a line. In an example in which a cursor on the 3D image G<b>101</b> is displayed, a user may draw the line L<b>111</b> by moving the cursor on the 3D image G<b>101</b> so that the trace forms a line. The point-setting unit <b>185</b> accepts all the reference points on the line L<b>111</b>. The point-setting unit <b>185</b> may accept some of the reference points on the line L<b>111</b>. The line L<b>111</b> is not necessarily a closed line.</p><p id="p-0433" num="0432">The region-setting unit <b>182</b> sets a reference region R<b>111</b> having the line L<b>111</b> as the center axis and having a circle as the cross-section. The shape of the cross-section of the reference region R<b>111</b> is not limited to a circle. The surface estimation unit <b>186</b> estimates a reference surface on the basis of three or more points included in the reference region R<b>111</b>.</p><p id="p-0434" num="0433">The 2D image of the subject may be used instead of the 3D image of the point-cloud data. Hereinafter, an example in which the 2D image of the subject is used will be described.</p><p id="p-0435" num="0434">The display control unit <b>183</b> displays the 2D image of the subject on the display unit <b>5</b> in Step S<b>102</b>. A user draws a line by tracing the 2D image of the subject. The point-setting unit <b>185</b> accepts three or more reference points on the line through the operation unit <b>4</b> and generates point information indicating each of the three or more accepted reference points in Step S<b>103</b>. The region-setting unit <b>182</b> sets a reference region on the basis of the three or more reference points indicated by the point information in Step S<b>104</b>.</p><p id="p-0436" num="0435">The display control unit <b>183</b> may display the 3D image of the point-cloud data on the display unit <b>5</b> and may display the three or more points of the point-cloud data used for estimating the reference surface on the 3D image. The display control unit <b>183</b> may display the 2D image of the subject on the display unit <b>5</b> and may display three or more points corresponding to the three or more points of the point-cloud data used for estimating the reference surface on the 2D image.</p><p id="p-0437" num="0436">The reference region does not need to be displayed. Accordingly, Step S<b>105</b> does not need to be executed. The reference surface does not need to be displayed. Accordingly, Step S<b>107</b> does not need to be executed.</p><p id="p-0438" num="0437">In the eighth embodiment, a reference region including three or more points on a line designated by a user by tracing an image is set. Therefore, the degree of freedom of the reference region increases in accordance with the shape of the subject or the composition of an image. The endoscope device <b>1</b> can easily set a reference region.</p><heading id="h-0021" level="1">Ninth Embodiment</heading><p id="p-0439" num="0438">A ninth embodiment of the present invention will be described. In the ninth embodiment, the 3D image of the point-cloud data or the 2D image of the subject is displayed on the display unit <b>5</b>, and an image of two or more regions in the point-cloud data is displayed on the display unit <b>5</b>. A user refers to the image of the two or more regions and designates a reference point on the 3D image of the point-cloud data.</p><p id="p-0440" num="0439">In the ninth embodiment, the CPU <b>18</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> is changed to a CPU <b>18</b><i>c </i>shown in <figref idref="DRAWINGS">FIG. <b>44</b></figref>. <figref idref="DRAWINGS">FIG. <b>44</b></figref> shows a functional configuration of the CPU <b>18</b><i>c</i>. The CPU <b>18</b><i>c </i>has functional units including a control unit <b>180</b>, a generation unit <b>181</b>, a region-setting unit <b>182</b>, a display control unit <b>183</b>, a position calculation unit <b>184</b>, a point-setting unit <b>185</b>, a surface estimation unit <b>186</b>, and a division unit <b>189</b>. At least one of the blocks shown in <figref idref="DRAWINGS">FIG. <b>44</b></figref> may be constituted by a different circuit from the CPU <b>18</b><i>c</i>. The same configuration as that shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> will not be described.</p><p id="p-0441" num="0440">Each unit shown in <figref idref="DRAWINGS">FIG. <b>44</b></figref> may be constituted by at least one of a processor and a logic circuit. Each unit shown in <figref idref="DRAWINGS">FIG. <b>44</b></figref> may include one or a plurality of processors. Each unit shown in <figref idref="DRAWINGS">FIG. <b>44</b></figref> may include one or a plurality of logic circuits.</p><p id="p-0442" num="0441">The division unit <b>189</b> has some of the functions of the state determination unit <b>188</b> shown in <figref idref="DRAWINGS">FIG. <b>32</b></figref>. The division unit <b>189</b> divides (classifies) three or more points corresponding to the three-dimensional coordinates included in the point-cloud data into two or more regions (division step). The display control unit <b>183</b> displays one of the image of the point-cloud data and the 2D image of the subject on the display unit <b>5</b> and displays an image of the two or more regions on the display unit <b>5</b> (image display step).</p><p id="p-0443" num="0442">Surface estimation processing in the ninth embodiment will be described by using <figref idref="DRAWINGS">FIG. <b>45</b></figref>. <figref idref="DRAWINGS">FIG. <b>45</b></figref> shows a procedure of the surface estimation processing. The same processing as that shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref> will not be described.</p><p id="p-0444" num="0443">After Step S<b>101</b>, the division unit <b>189</b> divides three or more points corresponding to the three-dimensional coordinates included in the point-cloud data into two or more regions (Step S<b>121</b>). Step S<b>121</b> corresponds to the division step. The division unit <b>189</b> divides three or more points in the point-cloud data into two or more regions by using a similar method to that in the sixth embodiment.</p><p id="p-0445" num="0444">After Step S<b>121</b>, the display control unit <b>183</b> displays the 3D image of the point-cloud data and an image of the two or more regions on the display unit <b>5</b> (Step S<b>102</b><i>c</i>). Step S<b>102</b><i>c </i>corresponds to the image display step. Step S<b>102</b><i>c </i>is the same as Step S<b>102</b><i>a </i>shown in <figref idref="DRAWINGS">FIG. <b>37</b></figref>. After Step S<b>102</b><i>c</i>, Step S<b>103</b> is executed.</p><p id="p-0446" num="0445"><figref idref="DRAWINGS">FIG. <b>46</b></figref> shows an example of an image displayed on the display unit <b>5</b>. The same 3D image G<b>91</b> as that shown in <figref idref="DRAWINGS">FIG. <b>34</b></figref> and the same image G<b>92</b> as that shown in <figref idref="DRAWINGS">FIG. <b>38</b></figref> are displayed on the display unit <b>5</b>. In the example shown in <figref idref="DRAWINGS">FIG. <b>46</b></figref>, the region of the image G<b>92</b> is divided into a region R<b>94</b>, a region R<b>95</b>, and a region R<b>96</b>.</p><p id="p-0447" num="0446">A user refers to the image G<b>92</b> and determines a position of a reference point. A user can determine a feature of a subject by comparing the 3D image G<b>91</b> with the image G<b>92</b>. For example, a user can determine that the region R<b>95</b> and the region R<b>96</b> are recessed portions or projection portions. A user determines a position of a reference point for setting a reference region that does not include regions corresponding to the region R<b>95</b> and the region R<b>96</b>. For example, a user designates a reference point on a region of the 3D image G<b>91</b> corresponding to the largest region R<b>94</b>.</p><p id="p-0448" num="0447">A user designates a reference point P<b>121</b> and a reference point P<b>122</b> by using a cursor displayed on the 3D image G<b>91</b> or by touching the screen of the display unit <b>5</b>. The point-setting unit <b>185</b> accepts the reference point P<b>121</b> and the reference point P<b>122</b>.</p><p id="p-0449" num="0448">The region-setting unit <b>182</b> calculates a sphere SP<b>121</b> having a line segment connecting the reference point P<b>121</b> and the reference point P<b>122</b> together as the diameter. The region-setting unit <b>182</b> sets the sphere SP<b>121</b> as the boundary of a reference region. The region-setting unit <b>182</b> may exclude a region of the 3D image G<b>91</b> corresponding to the region R<b>95</b> from the reference region.</p><p id="p-0450" num="0449">The reference region may be a cuboid, a cube, a cylinder, or the like. The shape of the reference region is not limited to these examples.</p><p id="p-0451" num="0450">The display control unit <b>183</b> may display the 2D image of the subject on the display unit <b>5</b> instead of the 3D image of the point-cloud data in Step S<b>102</b><i>c. </i></p><p id="p-0452" num="0451">The CPU <b>18</b><i>c </i>may include the measurement unit <b>187</b> shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref>. After Step S<b>107</b> is executed, Step S<b>108</b>, Step S<b>109</b>, and Step S<b>110</b> shown in <figref idref="DRAWINGS">FIG. <b>21</b></figref> may be executed.</p><p id="p-0453" num="0452">After the point-setting unit <b>185</b> accepts the reference points in Step S<b>103</b>, a user may change the reference points. The number of reference points set in Step S<b>103</b> is not limited to two.</p><p id="p-0454" num="0453">After the region-setting unit <b>182</b> sets the reference region in Step S<b>104</b>, a user may change at least one of the size of the reference region and the position of the reference region at any timing. The timing may come after the reference region is displayed in Step S<b>105</b> or after the reference surface is displayed in Step S<b>107</b>.</p><p id="p-0455" num="0454">The display control unit <b>183</b> may display the 3D image of the point-cloud data on the display unit <b>5</b> and may display the three or more points of the point-cloud data used for estimating the reference surface on the 3D image. The display control unit <b>183</b> may display the 2D image of the subject on the display unit <b>5</b> and may display three or more points corresponding to the three or more points of the point-cloud data used for estimating the reference surface on the 2D image.</p><p id="p-0456" num="0455">The reference region does not need to be displayed. Accordingly, Step S<b>105</b> does not need to be executed. The reference surface does not need to be displayed. Accordingly, Step S<b>107</b> does not need to be executed.</p><p id="p-0457" num="0456">In the ninth embodiment, an image of two or more regions in the point-cloud data is displayed on the display unit <b>5</b>. A user can determine a suitable region for designating a reference point by referring to the image.</p><heading id="h-0022" level="1">Modified Example of Ninth Embodiment</heading><p id="p-0458" num="0457">A modified example of the ninth embodiment of the present invention will be described. In the modified example of the ninth embodiment, the 3D image of the point-cloud data or the 2D image of the subject is displayed on the display unit <b>5</b>, and an image of a curvature map is displayed on the display unit <b>5</b>. The curvature map indicates the distribution of curvatures in the 3D shape of the subject indicated by the point-cloud data. For example, the curvature map includes 3D coordinates of a point and a curvature at the point. A user refers to the image of the curvature map and designates a reference point on the 3D image of the point-cloud data.</p><p id="p-0459" num="0458">In the modified example of the ninth embodiment, the CPU <b>18</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> is changed to a CPU <b>18</b><i>d </i>shown in <figref idref="DRAWINGS">FIG. <b>47</b></figref>. <figref idref="DRAWINGS">FIG. <b>47</b></figref> shows a functional configuration of the CPU <b>18</b><i>d</i>. The CPU <b>18</b><i>d </i>has functional units including a control unit <b>180</b>, a generation unit <b>181</b>, a region-setting unit <b>182</b>, a display control unit <b>183</b>, a position calculation unit <b>184</b>, a point-setting unit <b>185</b>, a surface estimation unit <b>186</b>, and a curvature calculation unit <b>190</b>. At least one of the blocks shown in <figref idref="DRAWINGS">FIG. <b>47</b></figref> may be constituted by a different circuit from the CPU <b>18</b><i>d</i>. The same configuration as that shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> will not be described.</p><p id="p-0460" num="0459">Each unit shown in <figref idref="DRAWINGS">FIG. <b>47</b></figref> may be constituted by at least one of a processor and a logic circuit. Each unit shown in <figref idref="DRAWINGS">FIG. <b>47</b></figref> may include one or a plurality of processors. Each unit shown in <figref idref="DRAWINGS">FIG. <b>47</b></figref> may include one or a plurality of logic circuits.</p><p id="p-0461" num="0460">The curvature calculation unit <b>190</b> has some of the functions of the state determination unit <b>188</b> shown in <figref idref="DRAWINGS">FIG. <b>32</b></figref>. The curvature calculation unit <b>190</b> calculates a curvature at each point of the point-cloud data and generates a curvature map (map generation step). The display control unit <b>183</b> displays one of the 3D image of the point-cloud data and the 2D image of the subject on the display unit <b>5</b> and displays an image of the curvature map on the display unit <b>5</b> (image display step).</p><p id="p-0462" num="0461">Surface estimation processing in the modified example of the ninth embodiment will be described by using <figref idref="DRAWINGS">FIG. <b>48</b></figref>. <figref idref="DRAWINGS">FIG. <b>48</b></figref> shows a procedure of the surface estimation processing. The same processing as that shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref> will not be described.</p><p id="p-0463" num="0462">After Step S<b>101</b>, the curvature calculation unit <b>190</b> calculates a curvature at each point of the point-cloud data and generates a curvature map (Step S<b>122</b>). Step S<b>122</b> corresponds to the map generation step. The curvature calculation unit <b>190</b> generates the curvature map by using a similar method to that in the second modified example of the sixth embodiment.</p><p id="p-0464" num="0463">After Step S<b>122</b>, the display control unit <b>183</b> displays the 3D image of the point-cloud data and an image of the curvature map on the display unit <b>5</b> (Step S<b>102</b><i>d</i>). Step S<b>102</b><i>d </i>corresponds to the image display step. Step S<b>102</b><i>d </i>is the same as Step S<b>102</b><i>b </i>shown in <figref idref="DRAWINGS">FIG. <b>39</b></figref>. After Step S<b>102</b><i>d</i>, Step S<b>103</b> is executed.</p><p id="p-0465" num="0464"><figref idref="DRAWINGS">FIG. <b>49</b></figref> shows an example of an image displayed on the display unit <b>5</b>. The same 3D image G<b>93</b> as that shown in <figref idref="DRAWINGS">FIG. <b>40</b></figref> and the same image G<b>94</b> as that shown in <figref idref="DRAWINGS">FIG. <b>40</b></figref> are displayed on the display unit <b>5</b>.</p><p id="p-0466" num="0465">A user refers to the image G<b>94</b> and determines a position of a reference point. A user can determine a feature of a subject by comparing the 3D image G<b>93</b> with the image G<b>94</b>. For example, the curvatures of a region R<b>97</b> and a region R<b>98</b> on the image G<b>94</b> of the curvature map are greater than those of regions around the region R<b>97</b> and the region R<b>98</b>. Therefore, a user can determine that the region R<b>97</b> and the region R<b>98</b> are recessed portions or projection portions. A user determines a position of a reference point for setting a reference region that does not include regions corresponding to the region R<b>97</b> and the region R<b>98</b>.</p><p id="p-0467" num="0466">A user designates a reference point P<b>123</b> and a reference point P<b>124</b> by using a cursor displayed on the 3D image G<b>93</b> or by touching the screen of the display unit <b>5</b>. The point-setting unit <b>185</b> accepts the reference point P<b>123</b> and the reference point P<b>124</b>.</p><p id="p-0468" num="0467">The region-setting unit <b>182</b> calculates a sphere SP<b>122</b> having a line segment connecting the reference point P<b>123</b> and the reference point P<b>124</b> together as the diameter. The region-setting unit <b>182</b> sets the sphere SP<b>122</b> as the boundary of a reference region.</p><p id="p-0469" num="0468">The reference region may be a cuboid, a cube, a cylinder, or the like. The shape of the reference region is not limited to these examples.</p><p id="p-0470" num="0469">The display control unit <b>183</b> may display the 2D image of the subject on the display unit <b>5</b> instead of the 3D image of the point-cloud data in Step S<b>102</b><i>d. </i></p><p id="p-0471" num="0470">The CPU <b>18</b><i>d </i>may include the measurement unit <b>187</b> shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref>. After Step S<b>107</b> is executed, Step S<b>108</b>, Step S<b>109</b>, and Step S<b>110</b> shown in <figref idref="DRAWINGS">FIG. <b>21</b></figref> may be executed.</p><p id="p-0472" num="0471">After the point-setting unit <b>185</b> accepts the reference points in Step S<b>103</b>, a user may change the reference points. The number of reference points set in Step S<b>103</b> is not limited to two.</p><p id="p-0473" num="0472">After the region-setting unit <b>182</b> sets the reference region in Step S<b>104</b>, a user may change at least one of the size of the reference region and the position of the reference region at any timing. The timing may come after the reference region is displayed in Step S<b>105</b> or after the reference surface is displayed in Step S<b>107</b>.</p><p id="p-0474" num="0473">The display control unit <b>183</b> may display the 3D image of the point-cloud data on the display unit <b>5</b> and may display the three or more points of the point-cloud data used for estimating the reference surface on the 3D image. The display control unit <b>183</b> may display the 2D image of the subject on the display unit <b>5</b> and may display three or more points corresponding to the three or more points of the point-cloud data used for estimating the reference surface on the 2D image.</p><p id="p-0475" num="0474">The reference region does not need to be displayed. Accordingly, Step S<b>105</b> does not need to be executed. The reference surface does not need to be displayed. Accordingly, Step S<b>107</b> does not need to be executed.</p><p id="p-0476" num="0475">In the modified example of the ninth embodiment, an image of a curvature map is displayed on the display unit <b>5</b>. A user can determine a suitable region for designating a reference point by referring to the image.</p><heading id="h-0023" level="1">Tenth Embodiment</heading><p id="p-0477" num="0476">A tenth embodiment of the present invention will be described. In the tenth embodiment, an abnormal region on the surface of a subject is automatically detected, and a reference region excluding a region corresponding to the abnormal region is set. A user does not need to designate a reference point.</p><p id="p-0478" num="0477">In the tenth embodiment, the CPU <b>18</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> is changed to a CPU <b>18</b><i>e </i>shown in <figref idref="DRAWINGS">FIG. <b>50</b></figref>. <figref idref="DRAWINGS">FIG. <b>50</b></figref> shows a functional configuration of the CPU <b>18</b><i>e</i>. The CPU <b>18</b><i>e </i>has functional units including a control unit <b>180</b>, a generation unit <b>181</b>, a region-setting unit <b>182</b>, a display control unit <b>183</b>, a surface estimation unit <b>186</b>, and an abnormality detection unit <b>191</b>. At least one of the blocks shown in <figref idref="DRAWINGS">FIG. <b>50</b></figref> may be constituted by a different circuit from the CPU <b>18</b><i>e</i>. The same configuration as that shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> will not be described.</p><p id="p-0479" num="0478">Each unit shown in <figref idref="DRAWINGS">FIG. <b>50</b></figref> may be constituted by at least one of a processor and a logic circuit. Each unit shown in <figref idref="DRAWINGS">FIG. <b>50</b></figref> may include one or a plurality of processors. Each unit shown in <figref idref="DRAWINGS">FIG. <b>50</b></figref> may include one or a plurality of logic circuits.</p><p id="p-0480" num="0479">The abnormality detection unit <b>191</b> detects an abnormal region on the surface of a subject on the basis of one of the 3D image of the point-cloud data and the 2D image of the subject (abnormality detection step). The region-setting unit <b>182</b> sets a reference region excluding a region corresponding to the abnormal region (region-setting step).</p><p id="p-0481" num="0480">Surface estimation processing in the tenth embodiment will be described by using <figref idref="DRAWINGS">FIG. <b>51</b></figref>. <figref idref="DRAWINGS">FIG. <b>51</b></figref> shows a procedure of the surface estimation processing. The same processing as that shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref> will not be described.</p><p id="p-0482" num="0481">After Step S<b>101</b>, the display control unit <b>183</b> displays the 2D image of the subject on the display unit <b>5</b> (Step S<b>102</b><i>e</i>).</p><p id="p-0483" num="0482">After Step S<b>102</b><i>e</i>, the abnormality detection unit <b>191</b> detects an abnormal region on the surface of the subject on the basis of the 2D image of the subject (Step S<b>131</b>). Step S<b>131</b> corresponds to the abnormality detection step.</p><p id="p-0484" num="0483">The abnormality detection unit <b>191</b> detects an abnormal region from the 2D image of the subject by using a recognition model of an abnormality. For example, a recognition model of an abnormality is acquired through machine learning such as deep learning. An abnormality is a defect, damage, or the like. The abnormality detection unit <b>191</b> obtains a type of an abnormality and a position of the abnormal region.</p><p id="p-0485" num="0484">After Step S<b>131</b>, the region-setting unit <b>182</b> sets a reference region excluding a region corresponding to the abnormal region on the 2D image of the subject (Step S<b>104</b><i>e</i>). Step S<b>104</b><i>e </i>corresponds to the region-setting step. After Step S<b>104</b><i>e</i>, Step S<b>105</b> is executed.</p><p id="p-0486" num="0485"><figref idref="DRAWINGS">FIG. <b>52</b></figref> and <figref idref="DRAWINGS">FIG. <b>53</b></figref> show examples of an image displayed on the display unit <b>5</b>. A 2D image G<b>131</b> is displayed on the display unit <b>5</b>. The abnormality detection unit <b>191</b> detects a convex abnormal object AO<b>131</b> by processing the 2D image G<b>131</b>.</p><p id="p-0487" num="0486">The region-setting unit <b>182</b> calculates a rectangle RC<b>131</b> around the abnormal object AO<b>131</b>. The region-setting unit <b>182</b> sets a first boundary that is a predetermined distance away from the rectangle RC<b>131</b> outside the rectangle RC<b>131</b>. In addition, the region-setting unit <b>182</b> sets a second boundary that is a predetermined distance away from the rectangle RC<b>131</b> inside the rectangle RC<b>131</b>. In this way, the region-setting unit <b>182</b> sets a reference region R<b>131</b> including the first boundary and the second boundary in the 2D image G<b>131</b>.</p><p id="p-0488" num="0487">The boundary of the reference region R<b>131</b> may be a polygon having three or more vertices. The boundary of the reference region R<b>131</b> may be a circle, an ellipse, or the like. The shape of the reference region R<b>131</b> is not limited to these examples.</p><p id="p-0489" num="0488">The 3D image of the point-cloud data may be used instead of the 2D image of the subject. Hereinafter, an example in which the 3D image of the point-cloud data is used will be described.</p><p id="p-0490" num="0489">The display control unit <b>183</b> displays the 3D image of the point-cloud data on the display unit <b>5</b> in Step S<b>102</b><i>e</i>. The abnormality detection unit <b>191</b> detects an abnormal region on the surface of the subject on the basis of the 3D image of the point-cloud data in Step S<b>131</b>. The region-setting unit <b>182</b> sets, in the 3D space, a reference region excluding a region corresponding to the abnormal region in Step S<b>104</b><i>e. </i></p><p id="p-0491" num="0490">The CPU <b>18</b><i>e </i>may include the measurement unit <b>187</b> shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref>. After Step S<b>107</b> is executed, Step S<b>108</b>, Step S<b>109</b>, and Step S<b>110</b> shown in <figref idref="DRAWINGS">FIG. <b>21</b></figref> may be executed.</p><p id="p-0492" num="0491">The control unit <b>180</b> may set the measurement mode on the basis of the type of the abnormality detected by the abnormality detection unit <b>191</b>. For example, when a convex or concave abnormal object is detected, the control unit <b>180</b> may set the measurement mode to the surface-based measurement in order to measure the height or the depth.</p><p id="p-0493" num="0492">Region information indicating the size of the reference region may be recorded on a recording medium in advance. The region-setting unit <b>182</b> may set the size of the reference region on the basis of the region information in Step S<b>104</b><i>e. </i></p><p id="p-0494" num="0493">Two or more pieces of the region information may be recorded on a recording medium in advance. The region-setting unit <b>182</b> may select one of the two or more pieces of the region information on the basis of information input through the operation unit <b>4</b> in Step S<b>104</b><i>e</i>. The region-setting unit <b>182</b> may set the size of the reference region on the basis of the selected region information in Step S<b>104</b><i>e. </i></p><p id="p-0495" num="0494">After the region-setting unit <b>182</b> sets the reference region in Step S<b>104</b><i>e</i>, a user may change at least one of the size of the reference region and the position of the reference region at any timing. The timing may come after the reference region is displayed in Step S<b>105</b> or after the reference surface is displayed in Step S<b>107</b>.</p><p id="p-0496" num="0495">The display control unit <b>183</b> may display the 3D image of the point-cloud data on the display unit <b>5</b> and may display the three or more points of the point-cloud data used for estimating the reference surface on the 3D image. The display control unit <b>183</b> may display the 2D image of the subject on the display unit <b>5</b> and may display three or more points corresponding to the three or more points of the point-cloud data used for estimating the reference surface on the 2D image.</p><p id="p-0497" num="0496">The reference region does not need to be displayed. Accordingly, Step S<b>105</b> does not need to be executed. The reference surface does not need to be displayed. Accordingly, Step S<b>107</b> does not need to be executed.</p><p id="p-0498" num="0497">In a case in which displaying the reference region and displaying the reference surface are unnecessary, an image does not need to be displayed. Accordingly, Step S<b>102</b><i>e </i>does not need to be executed. The CPU <b>18</b><i>e </i>does not need to have the function of the display control unit <b>183</b>.</p><p id="p-0499" num="0498">In the tenth embodiment, the endoscope device <b>1</b> can set a reference region that does not include an abnormal region. Therefore, the endoscope device <b>1</b> can improve the accuracy of a reference surface.</p><heading id="h-0024" level="1">Eleventh Embodiment</heading><p id="p-0500" num="0499">An eleventh embodiment of the present invention will be described. In the eleventh embodiment, a deviation map is displayed on the display unit <b>5</b>. The deviation map indicates the distribution of the 3D distance between a reference surface and a point corresponding to the 3D coordinates included in the point-cloud data.</p><p id="p-0501" num="0500">In the eleventh embodiment, the CPU <b>18</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> is changed to a CPU <b>18</b><i>f </i>shown in <figref idref="DRAWINGS">FIG. <b>54</b></figref>. <figref idref="DRAWINGS">FIG. <b>54</b></figref> shows a functional configuration of the CPU <b>18</b><i>f</i>. The CPU <b>18</b><i>f </i>has functional units including a control unit <b>180</b>, a generation unit <b>181</b>, a region-setting unit <b>182</b>, a display control unit <b>183</b>, a position calculation unit <b>184</b>, a point-setting unit <b>185</b>, a surface estimation unit <b>186</b>, and a deviation calculation unit <b>192</b>. At least one of the blocks shown in <figref idref="DRAWINGS">FIG. <b>54</b></figref> may be constituted by a different circuit from the CPU <b>18</b><i>f</i>. The same configuration as that shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> will not be described.</p><p id="p-0502" num="0501">Each unit shown in <figref idref="DRAWINGS">FIG. <b>54</b></figref> may be constituted by at least one of a processor and a logic circuit. Each unit shown in <figref idref="DRAWINGS">FIG. <b>54</b></figref> may include one or a plurality of processors. Each unit shown in <figref idref="DRAWINGS">FIG. <b>54</b></figref> may include one or a plurality of logic circuits.</p><p id="p-0503" num="0502">The deviation calculation unit <b>192</b> calculates a deviation by calculating the 3D distance between a reference surface and each point in the point-cloud data. The deviation calculation unit <b>192</b> generates a deviation map indicating the distribution of deviations (map generation step). The display control unit <b>183</b> displays one of the 3D image of the point-cloud data and the 2D image of the subject on the display unit <b>5</b> and displays the deviation map on the 3D image of the point-cloud data or the 2D image of the subject (image display step).</p><p id="p-0504" num="0503">Surface estimation processing in the eleventh embodiment will be described by using <figref idref="DRAWINGS">FIG. <b>55</b></figref>. <figref idref="DRAWINGS">FIG. <b>55</b></figref> shows a procedure of the surface estimation processing. The same processing as that shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref> will not be described.</p><p id="p-0505" num="0504">After Step S<b>101</b>, the display control unit <b>183</b> displays the 3D image of the point-cloud data on the display unit <b>5</b> (Step S<b>102</b><i>f</i>). After Step S<b>102</b><i>f</i>, Step S<b>103</b> is executed.</p><p id="p-0506" num="0505">After Step S<b>106</b>, the deviation calculation unit <b>192</b> calculates a deviation at each point of the point-cloud data and generates a deviation map (Step S<b>141</b>). Step S<b>141</b> corresponds to the map generation step.</p><p id="p-0507" num="0506">After Step S<b>141</b>, the display control unit <b>183</b> displays an image of the deviation map on the display unit <b>5</b> (Step S<b>142</b>). Specifically, the display control unit <b>183</b> generates a graphic image signal for displaying the 3D image of the point-cloud data on which the image of the deviation map is superimposed. The image of the deviation map includes color data of each pixel. The display control unit <b>183</b> outputs the generated graphic image signal to the display unit <b>5</b> through the video-signal-processing circuit <b>12</b>. The display unit <b>5</b> displays the 3D image of the point-cloud data on which the image of the deviation map is superimposed.</p><p id="p-0508" num="0507">Step S<b>142</b> corresponds to the image display step. When Step S<b>142</b> is executed, the surface estimation processing is completed.</p><p id="p-0509" num="0508"><figref idref="DRAWINGS">FIG. <b>56</b></figref> shows an example of an image displayed on the display unit <b>5</b>. A 3D image G<b>141</b> of the point-cloud data is displayed on the display unit <b>5</b> and an image G<b>142</b> of the deviation map is displayed on the 3D image G<b>141</b>. A pixel of the image G<b>142</b> is displayed in a color corresponding to the deviation at the pixel. For example, a pixel having a large deviation is displayed in a dark color, and a pixel having a small deviation is displayed in a light color. In <figref idref="DRAWINGS">FIG. <b>56</b></figref>, a reference region and a reference surface are not shown.</p><p id="p-0510" num="0509">A user can check whether or not a reference surface matches the surface of a subject by referring to the image G<b>142</b>. In a case in which a region having a large deviation is included in a reference region, the accuracy of a reference surface may deteriorate. When a reference region and the image G<b>142</b> are displayed on the 3D image G<b>141</b>, a user can check whether or not a region having a large deviation is included in the reference region.</p><p id="p-0511" num="0510">The display control unit <b>183</b> may display the 2D image of the subject on the display unit <b>5</b> instead of the 3D image of the point-cloud data in Step S<b>102</b><i>f. </i></p><p id="p-0512" num="0511">The CPU <b>18</b><i>f </i>may include the measurement unit <b>187</b> shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref>. After Step S<b>107</b>, Step S<b>141</b>, or Step S<b>142</b> is executed, Step S<b>108</b>, Step S<b>109</b>, and Step S<b>110</b> shown in <figref idref="DRAWINGS">FIG. <b>21</b></figref> may be executed.</p><p id="p-0513" num="0512">After the point-setting unit <b>185</b> accepts the reference points in Step S<b>103</b>, a user may change the reference points.</p><p id="p-0514" num="0513">Region information indicating at least one of the size of the reference region and the position of the reference region may be recorded on a recording medium in advance. The region-setting unit <b>182</b> may set at least one of the size of the reference region and the position of the reference region on the basis of the region information in Step S<b>104</b>. For example, the region-setting unit <b>182</b> may set at least one of the size of the outer boundary of the reference region and the position of the outer boundary of the reference region on the basis of the region information.</p><p id="p-0515" num="0514">Two or more pieces of the region information may be recorded on a recording medium in advance. The region-setting unit <b>182</b> may select one of the two or more pieces of the region information on the basis of information input through the operation unit <b>4</b> in Step S<b>104</b>. The region-setting unit <b>182</b> may set one of the size of the reference region and the position of the reference region on the basis of the selected region information in Step S<b>104</b>.</p><p id="p-0516" num="0515">After the region-setting unit <b>182</b> sets the reference region in Step S<b>104</b>, a user may change at least one of the size of the reference region and the position of the reference region at any timing. The timing may come after the reference region is displayed in Step S<b>105</b> or after the deviation map is displayed in Step S<b>142</b>.</p><p id="p-0517" num="0516">The region-setting unit <b>182</b> may set a reference region without using a reference point. Accordingly, Step S<b>103</b> does not need to be executed. The CPU <b>18</b><i>f </i>does not need to have the functions of the position calculation unit <b>184</b> and the point-setting unit <b>185</b>.</p><p id="p-0518" num="0517">The display control unit <b>183</b> may display the 3D image of the point-cloud data on the display unit <b>5</b> and may display the three or more points of the point-cloud data used for estimating the reference surface on the 3D image. The display control unit <b>183</b> may display the 2D image of the subject on the display unit <b>5</b> and may display three or more points corresponding to the three or more points of the point-cloud data used for estimating the reference surface on the 2D image.</p><p id="p-0519" num="0518">The reference region does not need to be displayed. Accordingly, Step S<b>105</b> does not need to be executed.</p><p id="p-0520" num="0519">In the eleventh embodiment, an image of a deviation map is displayed on the display unit <b>5</b>. A user can check whether or not the reference surface accurately approximates the surface of the subject by referring to the image.</p><heading id="h-0025" level="1">Twelfth Embodiment</heading><p id="p-0521" num="0520">A twelfth embodiment of the present invention will be described. Hereinafter, an example in which the PC <b>41</b> shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a surface estimation device will be described. The PC <b>41</b> acquires a 2D image of a subject from the endoscope device <b>1</b> and executes surface estimation processing.</p><p id="p-0522" num="0521">The external device interface <b>16</b> of the endoscope device <b>1</b> performs communication with the PC <b>41</b>. Specifically, the external device interface <b>16</b> transmits one or more 2D images of a subject to the PC <b>41</b>. The PC <b>41</b> receives the 2D images from the endoscope device <b>1</b>.</p><p id="p-0523" num="0522">For example, the external device interface <b>16</b> is connected to the PC <b>41</b> wirelessly or by a cable. The communication between the external device interface <b>16</b> and the PC <b>41</b> may be performed via a local area network (LAN) or the Internet.</p><p id="p-0524" num="0523"><figref idref="DRAWINGS">FIG. <b>57</b></figref> shows a configuration of the PC <b>41</b>. The PC <b>41</b> shown in <figref idref="DRAWINGS">FIG. <b>57</b></figref> includes a communication unit <b>43</b>, a CPU <b>44</b>, and a display unit <b>45</b>.</p><p id="p-0525" num="0524">The communication unit <b>43</b> performs communication with the external device interface <b>16</b> of the endoscope device <b>1</b>. Specifically, the communication unit <b>43</b> receives one or more 2D images of a subject from the external device interface <b>16</b>. The CPU <b>44</b> executes surface estimation processing. The display unit <b>45</b> is a monitor (display) such as an LCD. The display unit <b>45</b> includes a display screen and displays an image, an operation menu, and the like on the display screen.</p><p id="p-0526" num="0525"><figref idref="DRAWINGS">FIG. <b>58</b></figref> shows a functional configuration of the CPU <b>44</b>. The CPU <b>44</b> has functional units including a control unit <b>440</b>, a generation unit <b>441</b>, a region-setting unit <b>442</b>, a display control unit <b>443</b>, a position calculation unit <b>444</b>, a point-setting unit <b>445</b>, a surface estimation unit <b>446</b>, and a communication control unit <b>447</b>. At least one of the blocks shown in <figref idref="DRAWINGS">FIG. <b>58</b></figref> may be constituted by a different circuit from the CPU <b>44</b>.</p><p id="p-0527" num="0526">Each unit shown in <figref idref="DRAWINGS">FIG. <b>58</b></figref> may be constituted by at least one of a processor and a logic circuit. Each unit shown in <figref idref="DRAWINGS">FIG. <b>58</b></figref> may include one or a plurality of processors. Each unit shown in <figref idref="DRAWINGS">FIG. <b>58</b></figref> may include one or a plurality of logic circuits.</p><p id="p-0528" num="0527">The control unit <b>440</b> controls processing executed by each unit. The generation unit <b>441</b> has the same function as that of the generation unit <b>181</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The region-setting unit <b>442</b> has the same function as that of the region-setting unit <b>182</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The display control unit <b>443</b> has the same function as that of the display control unit <b>183</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The position calculation unit <b>444</b> has the same function as that of the position calculation unit <b>184</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The point-setting unit <b>445</b> has the same function as that of the point-setting unit <b>185</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The surface estimation unit <b>446</b> has the same function as that of the surface estimation unit <b>186</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The communication control unit <b>447</b> performs communication with the external device interface <b>16</b> of the endoscope device <b>1</b> by controlling the communication unit <b>43</b>.</p><p id="p-0529" num="0528">The CPU <b>44</b> executes the surface estimation processing shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. The CPU <b>44</b> may have the function of the measurement unit <b>187</b> shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref>. In a case in which the CPU <b>44</b> has the function of the measurement unit <b>187</b>, the CPU <b>44</b> may execute the 3D measurement shown in <figref idref="DRAWINGS">FIG. <b>21</b></figref>. The CPU <b>44</b> may have the function of the state determination unit <b>188</b> shown in <figref idref="DRAWINGS">FIG. <b>32</b></figref>. In a case in which the CPU <b>44</b> has the function of the state determination unit <b>188</b>, the CPU <b>44</b> may execute the surface estimation processing shown in <figref idref="DRAWINGS">FIG. <b>33</b></figref>, <figref idref="DRAWINGS">FIG. <b>37</b></figref>, or <figref idref="DRAWINGS">FIG. <b>39</b></figref>.</p><p id="p-0530" num="0529">The CPU <b>44</b> may have the function of the division unit <b>189</b> shown in <figref idref="DRAWINGS">FIG. <b>44</b></figref>. In a case in which the CPU <b>44</b> has the function of the division unit <b>189</b>, the CPU <b>44</b> may execute the surface estimation processing shown in <figref idref="DRAWINGS">FIG. <b>45</b></figref>. The CPU <b>44</b> may have the function of the curvature calculation unit <b>190</b> shown in <figref idref="DRAWINGS">FIG. <b>47</b></figref>. In a case in which the CPU <b>44</b> has the function of the curvature calculation unit <b>190</b>, the CPU <b>44</b> may execute the surface estimation processing shown in <figref idref="DRAWINGS">FIG. <b>48</b></figref>.</p><p id="p-0531" num="0530">The CPU <b>44</b> may have the function of the abnormality detection unit <b>191</b> shown in <figref idref="DRAWINGS">FIG. <b>50</b></figref>. In a case in which the CPU <b>44</b> has the function of the abnormality detection unit <b>191</b>, the CPU <b>44</b> may execute the surface estimation processing shown in <figref idref="DRAWINGS">FIG. <b>51</b></figref>. The CPU <b>44</b> may have the function of the deviation calculation unit <b>192</b> shown in <figref idref="DRAWINGS">FIG. <b>54</b></figref>. In a case in which the CPU <b>44</b> has the function of the deviation calculation unit <b>192</b>, the CPU <b>44</b> may execute the surface estimation processing shown in <figref idref="DRAWINGS">FIG. <b>55</b></figref>.</p><p id="p-0532" num="0531">The CPU <b>44</b> may read a program including commands defining the operations of the CPU <b>44</b> and may execute the read program. In other words, the function of the CPU <b>44</b> may be realized by software.</p><p id="p-0533" num="0532">The endoscope device <b>1</b> may generate point-cloud data on the basis of a 2D image of a subject, and the external device interface <b>16</b> of the endoscope device <b>1</b> may transmit the 2D image and the point-cloud data to the PC <b>41</b>. The communication unit <b>43</b> of the PC <b>41</b> may receive the 2D image and the point-cloud data from the external device interface <b>16</b>. Therefore, the CPU <b>44</b> does not need to include the generation unit <b>181</b>.</p><p id="p-0534" num="0533">In the twelfth embodiment, the PC <b>41</b> can improve the accuracy of a reference surface.</p><heading id="h-0026" level="2">(Related Technique)</heading><p id="p-0535" num="0534">The 3D measurement is executed in an inspection using an industrial endoscope. In the 3D measurement, a user adjusts the composition between the distal end of an endoscope and a measurement target captured in the visual field of the endoscope, and then acquires an image. A user inputs intended multiple points (coordinates) on the acquired image and obtains a measurement result.</p><p id="p-0536" num="0535">Stereo measurement can be used as a method for the 3D measurement. In the stereo measurement, a stereo image including a first image of a subject seen from a first viewpoint and a second image of the subject seen from a second viewpoint different from the first viewpoint is used. In the stereo measurement, matching processing is executed, and a point on the second image corresponding to a point on the first image is detected.</p><p id="p-0537" num="0536">There is a case in which an image on which a user inputs a point includes various factors that obstruct accurate measurement. These factors are collectively called matching-obstruction factors. In addition, a region on an image including a matching-obstruction factor is called a matching-obstruction region.</p><p id="p-0538" num="0537">In a case in which a user accidently inputs a point in a matching-obstruction region, a device cannot calculate accurate 3D coordinates of the point and cannot obtain an accurate measurement result. Therefore, it is important to notify a user of a region for which measurement can be executed and a matching-obstruction region for which measurement cannot be executed when the user inputs a point. At this time, a device can use a method such as a method of visualizing these regions.</p><p id="p-0539" num="0538">As a specific example of the matching-obstruction factor, there is a phenomenon called occlusion, an unnecessary object attached to an optical system, or the like. In addition, there is a case in which specular reflection of illumination light emitted to the surface of a subject occurs and an optical image of reflected light is seen in an image. This phenomenon caused by the specular reflection of the illumination light is also an example of the matching-obstruction factor. Hereinafter, this phenomenon is called a factor of interest.</p><p id="p-0540" num="0539">The occurrence frequency of a factor of interest is higher than that of the other matching-obstruction factors. In addition, in a case in which only one of two images included in a stereo image is displayed and a factor of interest occurs in the other of the two images included in the stereo image, a user is unlikely to notice the factor of interest in the image that is not displayed. Therefore, it is important to detect a region in which a factor of interest has occurred and notify a user of the region.</p><p id="p-0541" num="0540">In order for a user to accurately input a point even when a factor of interest occurs, the following two methods are applicable.</p><p id="p-0542" num="0541">A first method is to apply the matching processing to the entire measurement-possible region in a stereo image. The measurement-possible region is a region having a common visual field between the first image and the second image. For example, the first method includes a method of visualizing the reliability of a point input by a user. The reliability is obtained through the matching processing applied to the entire measurement-possible region in a stereo image. Alternatively, the first method includes a method or the like of displaying a 3D shape of a subject obtained through the matching processing.</p><p id="p-0543" num="0542">However, in these methods, a processing time is generally long since the matching processing at all the points in a measurement-possible region is required. Therefore, there is a shortcoming in that a waiting time is long from when an image is acquired until a user is notified of a region in which a factor of interest has occurred.</p><p id="p-0544" num="0543">A second method is to devise an algorithm so that accurate matching processing can be executed even when a factor of interest occurs. In the second method, it is ideal to calculate accurate 3D coordinates corresponding to an input point. In the matching processing, calculation is performed by using information of the brightness or color of a first image and a second image of a stereo image in many cases. In a case in which the pattern of a bright part caused by a factor of interest is stronger than that of a subject, it is generally difficult to avoid a mistake of the matching. Even if there is an algorithm that can execute accurate matching processing on the basis of a point in a region in which a factor of interest has occurred, it is difficult to balance the accuracy and a processing time.</p><p id="p-0545" num="0544">Therefore, it is necessary to detect a region in which a factor of interest has occurred in a short processing time by using a different method from the first method and the second method. In particular, calculation resources are limited in built-in equipment such as an industrial endoscope. Therefore, it is highly necessary to improve a processing time. In addition, it is preferable to detect a matching-obstruction region at regular time intervals in an image displayed live. In this way, a device can notify a user of whether or not an image is suitable for measurement in a stage at which the composition is adjusted.</p><p id="p-0546" num="0545">In order to meet the above-described request for a processing time, there is a method of detecting a region in which a factor of interest has occurred in a short processing time by using a 2D image of a subject without using a calculation result of 3D coordinates. For example, the method is disclosed in Japanese Patent No. 6253380 (reference document).</p><p id="p-0547" num="0546">The above-described reference document discloses a method of using a first image and a second image in a stereo image so as to detect an unnecessary component included in each of the images. An unnecessary component does not necessarily match a matching-obstruction region. Specifically, the following method is disclosed as a method of detecting an unnecessary component.</p><p id="p-0548" num="0547">A plurality of images having mutually different parallax are acquired. Each image included in the plurality of images is set as a reference image, and a relative difference, which is the difference between the reference image and one or more of the other images, is calculated. An unnecessary component included in each image is detected by using information of the relative difference.</p><p id="p-0549" num="0548">In addition to the above-described method, the reference document discloses that processing to match the positions of two images included in a stereo image may be executed before the relative difference is calculated.</p><heading id="h-0027" level="2">(Problem of Related Technique)</heading><p id="p-0550" num="0549">In the technique disclosed in the reference document, each image included in the plurality of images having mutually different parallax is set as a reference image, and the difference between the reference image and one or more of the other images is calculated. If this method is applied to an industrial endoscope, there is a possibility that the performance of detecting a matching-obstruction region deteriorates in terms of a processing time and the detection accuracy. Hereinafter, these points will be described.</p><heading id="h-0028" level="2">(1) Terms of Point of Processing Time</heading><p id="p-0551" num="0550">Calculation resources are limited in an industrial endoscope that is built-in equipment. In addition, it is preferable to detect a matching-obstruction region at regular time intervals in an image displayed live. Therefore, a short processing time is preferable.</p><p id="p-0552" num="0551">In the method disclosed in the reference document, each of two images included in a stereo image is set as a reference image, and the same processing is repeated twice. Therefore, the processing is redundant and requires a long processing time.</p><heading id="h-0029" level="2">(2) Terms of Point of Detection Accuracy</heading><p id="p-0553" num="0552">In the technique disclosed in the reference document, it is not considered to detect a region in which a factor of interest caused by specular reflection of illumination light has occurred. In an industrial endoscope, since the distance between an endoscope and a subject is different between imaging timings, the processing to match the positions of two images disclosed in the reference document is suitable.</p><p id="p-0554" num="0553">In an industrial endoscope, an illumination optical system and a stereo-observation optical system are physically close to each other. Therefore, the difference in brightness between two images included in a stereo image is likely to occur, especially when a subject is close to an endoscope. When it is assumed that processing to match positions with high accuracy, which is executed in a long processing time, is applied to the two images, the difference in brightness between the two images remains after the processing is executed. Therefore, there is a possibility that a region having the difference in brightness between the two images is erroneously detected as a matching-obstruction region.</p><p id="p-0555" num="0554">A factor of interest does not always occur in the entire region having the difference in brightness. In order to detect a region in which a factor of interest has occurred, the entire region having the difference in brightness does not need to be detected.</p><p id="p-0556" num="0555">In a specific imaging condition of a subject, there is a possibility that the positional deviation occurs between two images for which the processing to match positions has been executed. In a region in which this deviation has occurred, the difference in brightness occurs between the two images. Therefore, there is a possibility that a region having the difference in brightness is erroneously detected as a matching-obstruction region.</p><p id="p-0557" num="0556">The purpose of a related invention is to provide a method that can detect a region in which a factor of interest caused by specular reflection of illumination light has occurred in a short processing time with high accuracy.</p><heading id="h-0030" level="1">Embodiment of Related Invention</heading><p id="p-0558" num="0557">An embodiment of the related invention will be described. The endoscope device <b>1</b> in the embodiment of the related invention includes a CPU <b>18</b><i>g </i>shown in <figref idref="DRAWINGS">FIG. <b>59</b></figref> instead of the CPU <b>18</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. <figref idref="DRAWINGS">FIG. <b>59</b></figref> shows a functional configuration of the CPU <b>18</b><i>g</i>. The CPU <b>18</b><i>g </i>has functional units including a control unit <b>200</b>, a feature-point-processing unit <b>201</b>, a division unit <b>202</b>, a difference calculation unit <b>203</b>, a determination unit <b>204</b>, a region detection unit <b>205</b>, and a display control unit <b>206</b>. At least one of the blocks shown in <figref idref="DRAWINGS">FIG. <b>59</b></figref> may be constituted by a different circuit from the CPU <b>18</b><i>g. </i></p><p id="p-0559" num="0558">Each unit shown in <figref idref="DRAWINGS">FIG. <b>59</b></figref> may be constituted by at least one of a processor and a logic circuit. Each unit shown in <figref idref="DRAWINGS">FIG. <b>59</b></figref> may include one or a plurality of processors. Each unit shown in <figref idref="DRAWINGS">FIG. <b>59</b></figref> may include one or a plurality of logic circuits.</p><p id="p-0560" num="0559">The control unit <b>200</b> acquires a stereo image (image data) of a subject from the video-signal-processing circuit <b>12</b> and controls processing executed by each unit shown in <figref idref="DRAWINGS">FIG. <b>59</b></figref>. The stereo image includes a first image of the subject seen from a first viewpoint and a second image of the subject seen from a second viewpoint different from the first viewpoint.</p><p id="p-0561" num="0560">The feature-point-processing unit <b>201</b> detects three or more feature points on the first image and detects a point on the second image corresponding to each of the feature points. The feature-point-processing unit <b>201</b> associates the feature point on the first image and the point on the second image with each other.</p><p id="p-0562" num="0561">The division unit <b>202</b> performs Delaunay triangulation on the second image on the basis of the point on the second image associated with the feature point on the first image. In this way, the division unit <b>202</b> sets one or more triangular regions in the second image. The division unit <b>202</b> sets one or more triangular regions in the first image on the basis of the relationship of points associated between the two images. In addition, the division unit <b>202</b> deforms the triangle in the second image in order to match the shape of the triangle in the first image and the shape of the triangle in the second image.</p><p id="p-0563" num="0562">The difference calculation unit <b>203</b> calculates the difference between a pixel value of the first image and a pixel value of the second image for each pixel. In this way, the difference calculation unit <b>203</b> calculates the difference in brightness between the two images. The difference calculation unit <b>203</b> extracts, from the first image, a region including a pixel at which the difference exceeds a predetermined threshold value. The extracted region is a candidate of a region in which a factor of interest caused by specular reflection of illumination light has occurred.</p><p id="p-0564" num="0563">The determination unit <b>204</b> determines whether or not there is a pair of two regions having a similar feature among the extracted regions. Hereinafter, this determination is called pairing determination. In a case in which a factor of interest occurs in a stereo image, there is a feature that this pair exists. In a case in which the pair exists, the determination unit <b>204</b> detects a region included in the pair as a region in which a factor of interest has occurred.</p><p id="p-0565" num="0564">In addition, the determination unit <b>204</b> detects a region having brightness exceeding a reference value of brightness that is set in advance. When at least part of the region overlaps the two regions detected in the pairing determination, the determination unit <b>204</b> determines the two regions detected in the pairing determination and the region having high brightness as regions in which a factor of interest has occurred.</p><p id="p-0566" num="0565">The display control unit <b>206</b> displays a stereo image on the display unit <b>5</b> and displays the regions in which a factor of interest has occurred on the stereo image.</p><p id="p-0567" num="0566">Processing to detect a region in which a factor of interest has occurred will be described by using <figref idref="DRAWINGS">FIG. <b>60</b></figref>. <figref idref="DRAWINGS">FIG. <b>60</b></figref> shows a procedure of the processing.</p><p id="p-0568" num="0567">The feature-point-processing unit <b>201</b> detects feature points on the first image and detects a point on the second image corresponding to each of the feature points (Step S<b>201</b>).</p><p id="p-0569" num="0568">Details of Step S<b>201</b> will be described. <figref idref="DRAWINGS">FIG. <b>61</b></figref> shows an example of a stereo image. The stereo image shown in <figref idref="DRAWINGS">FIG. <b>61</b></figref> includes a first image G<b>201</b> and a second image G<b>202</b>. The feature-point-processing unit <b>201</b> detects a feature point P<b>201</b>, a feature point P<b>202</b>, and a feature point P<b>203</b> on the first image G<b>201</b>. The feature-point-processing unit <b>201</b> detects a point P<b>211</b> on the second image G<b>202</b> and associates the point P<b>211</b> with the feature point P<b>201</b>. The feature-point-processing unit <b>201</b> detects a point P<b>212</b> on the second image G<b>202</b> and associates the point P<b>212</b> with the feature point P<b>202</b>. The feature-point-processing unit <b>201</b> detects a point P<b>213</b> on the second image G<b>202</b> and associates the point P<b>213</b> with the feature point P<b>203</b>. In the example shown in <figref idref="DRAWINGS">FIG. <b>61</b></figref>, three feature points are detected. The number of feature points is not limited to three.</p><p id="p-0570" num="0569">The feature-point-processing unit <b>201</b> detects, as a feature point, a point of which an error in the matching processing, which uses the first image and the second image, is expected to be small. The feature-point-processing unit <b>201</b> determines the adequacy of the feature point by checking the following index for that purpose. For example, the feature-point-processing unit <b>201</b> uses a uniqueness ratio, continuity of parallax, consistency of an image, a reprojection error, and the like as an index.</p><p id="p-0571" num="0570">The uniqueness ratio indicates the similarity between two points on the second image. Specifically, the uniqueness ratio indicates the similarity between a first point and a second point. The first point is the most similar to the feature point on the first image. The second point is the second most similar to the feature point. The continuity of parallax indicates the relationship of parallax that is allowed in a localized area. When the parallax at the feature point and the parallax at a point around the feature point are almost the same, there is continuity of parallax. When the parallax at the feature point and the parallax at a point around the feature point are greatly different from each other, there is not continuity of parallax.</p><p id="p-0572" num="0571">The consistency of an image indicates the relationship between two points associated with each other through the matching processing. Specifically, the matching processing is executed twice. A point on the second image corresponding to a point on the first image is detected through first matching processing. In addition, a point on the first image corresponding to the point on the second image is detected through second matching processing. The consistency of an image indicates how well the two points match each other. One of the two points is the point on the first image used for the first matching processing. The other of the two points is the point on the first image detected through the second matching processing. In the matching processing executed twice, the entire image does not need to be used.</p><p id="p-0573" num="0572">The reprojection error indicates the amount of shift between the point detected through the matching processing and an epipolar line.</p><p id="p-0574" num="0573">The feature-point-processing unit <b>201</b> does not need to use all the above-described indices. The feature-point-processing unit <b>201</b> determines the adequacy of the feature point on the basis of one or more indices.</p><p id="p-0575" num="0574">The feature-point-processing unit <b>201</b> may detect a feature point by detecting a point having any feature from the first image. Alternatively, the first image may be divided in a lattice shape in advance, and the feature-point-processing unit <b>201</b> may detect an intersection point of two boundary lines as a candidate of a feature point.</p><p id="p-0576" num="0575">After Step S<b>201</b>, the division unit <b>202</b> sets one or more triangular regions in the second image and sets one or more triangular regions in the first image on the basis of the relationship of points between the two images (Step S<b>202</b>).</p><p id="p-0577" num="0576">Details of Step S<b>202</b> will be described. The division unit <b>202</b> sets one or more triangular regions in the second image by performing Delaunay triangulation on the second image. The vertices of each triangle are associated with the feature points on the first image detected in Step S<b>201</b>. <figref idref="DRAWINGS">FIG. <b>62</b></figref> shows an example of an image in which triangles are set. The region of the second image G<b>203</b> includes two or more triangular regions.</p><p id="p-0578" num="0577">The division unit <b>202</b> identifies three feature points on the first image corresponding to three vertices of one triangle on the second image. The division unit <b>202</b> sets a triangle having the three feature points as vertices in the first image. The division unit <b>202</b> associates the triangle on the second image and the triangle on the first image with each other. The division unit <b>202</b> executes the above-described processing in which all the triangles on the second image are targeted.</p><p id="p-0579" num="0578"><figref idref="DRAWINGS">FIG. <b>63</b></figref> shows triangles set in each of the first image and the second image. An image G<b>204</b>, which is part of the first image, and an image G<b>205</b>, which is part of the second image, are shown in <figref idref="DRAWINGS">FIG. <b>63</b></figref>. A triangle T<b>201</b> on the image G<b>204</b> and a triangle T<b>202</b> on the image G<b>205</b> are associated with each other. The division unit <b>202</b> deforms the shape of the triangle T<b>202</b> and causes the shape of the triangle T<b>201</b> and the shape of the triangle T<b>202</b> to match each other. The division unit <b>202</b> performs this processing on all the triangles on the second image, thus generating a deformed image of the second image.</p><p id="p-0580" num="0579">After Step S<b>202</b>, the difference calculation unit <b>203</b> calculates the difference in brightness between the first image and the second image (Step S<b>203</b>).</p><p id="p-0581" num="0580">Details of Step S<b>203</b> will be described. The difference calculation unit <b>203</b> calculates the difference in brightness by using the first image and the deformed image of the second image generated in Step S<b>202</b>. The difference calculation unit <b>203</b> extracts, from the first image, a region including a pixel at which the difference exceeds a predetermined threshold value. For example, the difference calculation unit <b>203</b> extracts a pixel value of the deformed image of the second image from a pixel value of the first image, thus calculating the difference. The difference calculation unit <b>203</b> extracts, from the first image, a region including a pixel at which the positive difference is greater than a predetermined threshold value. In addition, the difference calculation unit <b>203</b> extracts, from the first image, a region including a pixel at which the negative difference is less than a predetermined threshold value.</p><p id="p-0582" num="0581"><figref idref="DRAWINGS">FIG. <b>64</b></figref> and <figref idref="DRAWINGS">FIG. <b>65</b></figref> show examples of the first image. The region of a first image G<b>206</b> shown in <figref idref="DRAWINGS">FIG. <b>64</b></figref> includes a region R<b>201</b> and a region R<b>202</b>. The region R<b>201</b> includes a pixel at which the positive difference is greater than a predetermined threshold value. The region R<b>202</b> includes a pixel at which the negative difference is less than a predetermined threshold value. The region of a first image G<b>207</b> shown in <figref idref="DRAWINGS">FIG. <b>65</b></figref> includes a region R<b>203</b> and a region R<b>204</b>. The region R<b>203</b> includes a pixel at which the positive difference is greater than a predetermined threshold value. The region R<b>204</b> includes a pixel at which the negative difference is less than a predetermined threshold value.</p><p id="p-0583" num="0582">After Step S<b>203</b>, the determination unit <b>204</b> executes the pairing determination. In this way, the determination unit <b>204</b> determines whether or not there is a pair of two regions having a similar feature among the regions extracted in Step S<b>203</b> (Step S<b>204</b>).</p><p id="p-0584" num="0583">Details of Step S<b>204</b> will be described. In the example shown in <figref idref="DRAWINGS">FIG. <b>64</b></figref>, the determination unit <b>204</b> determines whether or not the feature of the region R<b>201</b> and the feature of the region R<b>202</b> are similar to each other. In the example shown in <figref idref="DRAWINGS">FIG. <b>65</b></figref>, the determination unit <b>204</b> determines whether or not the feature of the region R<b>203</b> and the feature of the region R<b>204</b> are similar to each other. The determination unit <b>204</b> executes the pairing determination by using the following indices.</p><p id="p-0585" num="0584">The determination unit <b>204</b> determines whether or not the signs of the differences in brightness of the two regions are different from each other. When the difference in brightness of one region is positive and the difference in brightness of the other region is negative, the determination unit <b>204</b> determines that the features of the two regions are similar to each other.</p><p id="p-0586" num="0585">The determination unit <b>204</b> determines the distance between the two regions. When the two regions are close to each other, the determination unit <b>204</b> determines that the features of the two regions are similar to each other.</p><p id="p-0587" num="0586">The determination unit <b>204</b> compares the shapes of the two regions. When the shapes of the two regions are similar to each other, the determination unit <b>204</b> determines that the features of the two regions are similar to each other.</p><p id="p-0588" num="0587">The determination unit <b>204</b> compares the directions (angles) of the two regions with each other. For example, the determination unit <b>204</b> determines a rectangle, which approximates and circumscribes each of the regions, and compares the angles of the long and short axes of the rectangle with each other. When the directions of the two regions are similar to each other, the determination unit <b>204</b> determines that the features of the two regions are similar to each other.</p><p id="p-0589" num="0588">The determination unit <b>204</b> compares the areas of the two regions. When the areas of the two regions are similar to each other, the determination unit <b>204</b> determines that the features of the two regions are similar to each other.</p><p id="p-0590" num="0589">The determination unit <b>204</b> does not need to use all the above-described indices. The determination unit <b>204</b> determines whether or not the features of the two regions are similar to each other on the basis of one or more indices. The determination unit <b>204</b> detects a region included in the pair as a region in which a factor of interest has occurred.</p><p id="p-0591" num="0590">After Step S<b>204</b>, the determination unit <b>204</b> detects, in the first image, a region having brightness exceeding a reference value of brightness that is set in advance (Step S<b>205</b>).</p><p id="p-0592" num="0591">Details of Step S<b>205</b> will be described. For example, the determination unit <b>204</b> determines whether or not a pixel value exceeds the reference value for each pixel. An example of the reference value is 250. The determination unit <b>204</b> detects a region including a pixel having a pixel value exceeding the reference value. The detected region has high brightness. The determination unit <b>204</b> determines whether or not at least part of the region having high brightness overlaps the two regions detected in the pairing determination. If the region having high brightness overlaps the two regions, the determination unit <b>204</b> detects a union of the region having high brightness and the two regions detected in the pairing determination as a region in which a factor of interest has occurred. If the region having high brightness does not overlap the two regions, the determination unit <b>204</b> detects only the two regions detected in the pairing determination as a region in which a factor of interest has occurred.</p><p id="p-0593" num="0592"><figref idref="DRAWINGS">FIG. <b>66</b></figref> shows an example of a region in which a factor of interest has occurred. A region R<b>205</b> includes a pixel having brightness exceeding the reference value. A region R<b>206</b> and a region R<b>207</b> constitute the pair detected in the pairing determination. Part of the region R<b>205</b> overlaps the region R<b>206</b>, and other part of the region R<b>205</b> overlaps the region R<b>207</b>. Therefore, the determination unit <b>204</b> detects the union of the region R<b>205</b>, the region R<b>206</b>, and the region R<b>207</b> as a region in which a factor of interest has occurred.</p><p id="p-0594" num="0593">After Step S<b>205</b>, the determination unit <b>204</b> puts a convex curve (closed convex curve) around the region in which a factor of interest has occurred. The determination unit <b>204</b> eventually detects the region surrounded by the convex curve as a region in which a factor of interest has occurred (Step S<b>206</b>).</p><p id="p-0595" num="0594">In the example shown in <figref idref="DRAWINGS">FIG. <b>66</b></figref>, the region R<b>205</b>, the region R<b>206</b>, and the region R<b>207</b> are surrounded by a convex curve L<b>201</b>. The determination unit <b>204</b> detects all the regions surrounded by the convex curve L<b>201</b> as a region in which a factor of interest has occurred. The regions surrounded by the convex curve L<b>201</b> include the region R<b>205</b>, the region R<b>206</b>, and the region R<b>207</b>. In addition, the regions surrounded by the convex curve L<b>201</b> include a region other than the region R<b>205</b>, the region R<b>206</b>, or the region R<b>207</b>.</p><p id="p-0596" num="0595">Step S<b>206</b> may be executed without executing Step S<b>205</b>. In a case in which there is a region having high brightness, a region surrounded by a convex curve includes at least part of the region having high brightness.</p><p id="p-0597" num="0596">After Step S<b>206</b>, the display control unit <b>206</b> displays a stereo image on the display unit <b>5</b> and displays the region in which a factor of interest has occurred on the stereo image (Step S<b>207</b>). When Step S<b>207</b> is executed, the processing shown in <figref idref="DRAWINGS">FIG. <b>60</b></figref> is completed.</p><p id="p-0598" num="0597">Specifically, the display control unit <b>206</b> generates a graphic image signal for displaying the region in which a factor of interest has occurred. The display control unit <b>206</b> outputs the generated graphic image signal to the video-signal-processing circuit <b>12</b>. The video-signal-processing circuit <b>12</b> combines the video signal output from the CCU <b>9</b> and the graphic image signal output from the CPU <b>18</b><i>g</i>. The video-signal-processing circuit <b>12</b> outputs the combined video signal to the display unit <b>5</b>. The display unit <b>5</b> displays a stereo image on which the region in which a factor of interest has occurred is superimposed. For example, the first image included in the stereo image is displayed on the display unit <b>5</b>, and the region in which a factor of interest has occurred is displayed on the first image.</p><p id="p-0599" num="0598">The region in which a factor of interest has occurred is displayed on the stereo image. Therefore, a user can avoid inputting a point in the region in which a factor of interest has occurred. Since inputting a point in a matching-obstruction region is avoided, the accuracy of stereo measurement is improved.</p><p id="p-0600" num="0599">In many cases, a region having high brightness is not suitable for the matching processing. Even when a region having high brightness does not overlap the two regions detected in the pairing determination, the display control unit <b>206</b> may display the region having high brightness on the stereo image in order to draw a user's attention. A user can avoid inputting a point in the region having high brightness.</p><p id="p-0601" num="0600">In the embodiment of the related invention, each of two images included in a stereo image does not need to be set as a reference image, and the matching processing does not need to be executed twice. Therefore, the endoscope device <b>1</b> can detect a region in which a factor of interest has occurred in a short processing time. In the embodiment of the related invention, the pairing determination is executed. Therefore, the endoscope device <b>1</b> can detect a region in which a factor of interest has occurred with high accuracy.</p><p id="p-0602" num="0601">While preferred embodiments of the invention have been described and shown above, it should be understood that these are examples of the invention and are not to be considered as limiting. Additions, omissions, substitutions, and other modifications can be made without departing from the spirit or scope of the present invention. Accordingly, the invention is not to be considered as being limited by the foregoing description, and is only limited by the scope of the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A surface estimation method executed by a processor, the surface estimation method comprising:<claim-text>a region-setting step of setting a reference region that is one of a three-dimensional region and a two-dimensional region,<claim-text>wherein the three-dimensional region includes three or more points and is set in a three-dimensional space,</claim-text><claim-text>wherein the three-dimensional space includes three-dimensional coordinates of three or more points on a subject calculated on the basis of a two-dimensional image of the subject,</claim-text><claim-text>wherein the three-dimensional coordinates of the three or more points are included in three-dimensional image data,</claim-text><claim-text>wherein the two-dimensional region includes three or more points and is set in the two-dimensional image, and</claim-text><claim-text>wherein the three or more points of the reference region include one or more combinations, each of which is constituted by three points that form a triangle; and</claim-text></claim-text><claim-text>an estimation step of estimating a reference surface that approximates a surface of the subject on the basis of three or more points of the three-dimensional image data corresponding to the three or more points included in the reference region.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The surface estimation method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the reference region that is the three-dimensional region is set in the three-dimensional space in the region-setting step.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The surface estimation method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the reference region that is the two-dimensional region is set in the two-dimensional image in the region-setting step.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The surface estimation method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>,<claim-text>wherein continuity of the three or more points corresponding to the three-dimensional coordinates included in the three-dimensional image data is determined in the region-setting step, and</claim-text><claim-text>wherein the reference region includes only the three or more points determined to be continuous.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The surface estimation method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the reference region includes a region that is not a convex set.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The surface estimation method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the reference region includes two or more regions.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The surface estimation method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>an image display step of displaying one of an image of the three-dimensional image data and the two-dimensional image on a display;</claim-text><claim-text>a position input step of accepting position information input through an input device,<claim-text>wherein the position information indicates a position on the image of the three-dimensional image data or the two-dimensional image displayed on the display; and</claim-text></claim-text><claim-text>a state determination step of determining a state of the subject,</claim-text><claim-text>wherein a boundary of the reference region is determined on the basis of both the position indicated by the position information and the state in the region-setting step.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The surface estimation method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>,<claim-text>wherein a boundary of the reference region includes a first boundary and a second boundary that is on an inner side of the first boundary.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The surface estimation method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the three or more points corresponding to the three-dimensional coordinates included in the three-dimensional image data are divided into two or more regions in the region-setting step, and</claim-text><claim-text>wherein a boundary of the reference region includes a boundary of one or more regions included in the two or more regions.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The surface estimation method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>,<claim-text>wherein region information indicating at least one of a size of the reference region and a position of the reference region is recorded on a recording medium in advance, and</claim-text><claim-text>wherein at least one of the size of the reference region and the position of the reference region is set on the basis of the region information in the region-setting step.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The surface estimation method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising an image display step of displaying one of an image of the three-dimensional image data and the two-dimensional image on a display,<claim-text>wherein three or more points on the image of the three-dimensional image data or the two-dimensional image are input through an input device in the region-setting step, and</claim-text><claim-text>the reference region including the input three or more points is set in the region-setting step.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The surface estimation method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>,<claim-text>wherein the reference region is set on the basis of line segments connecting the input three or more points together in the region-setting step.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The surface estimation method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising an image display step of displaying one of an image of the three-dimensional image data and the two-dimensional image on a display,<claim-text>wherein three or more points on a line designated in the image of the three-dimensional image data or the two-dimensional image by a user are input through an input device in the region-setting step, and</claim-text><claim-text>the reference region including the three or more points on the line is set in the region-setting step.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The surface estimation method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>,<claim-text>wherein the three or more points are selected from four or more points included in the reference region in the estimation step, and</claim-text><claim-text>the reference surface is estimated on the basis of three or more points of the three-dimensional image data corresponding to the three or more selected points in the estimation step.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The surface estimation method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising an image display step of displaying one of an image of the three-dimensional image data and the two-dimensional image on a display and displaying the reference region on the image of the three-dimensional image data or the two-dimensional image.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The surface estimation method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising an image display step of displaying an image of the three-dimensional image data on a display and displaying, on the image of the three-dimensional image data, the three or more points of the three-dimensional image data used for estimating the reference surface.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The surface estimation method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising an image display step of displaying the two-dimensional image on a display and displaying, on the two-dimensional image, three or more points corresponding to the three or more points of the three-dimensional image data used for estimating the reference surface.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The surface estimation method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a division step of dividing three or more points corresponding to the three-dimensional coordinates included in the three-dimensional image data into two or more regions; and</claim-text><claim-text>an image display step of displaying one of an image of the three-dimensional image data and the two-dimensional image on a display and displaying an image of the two or more regions on the display.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The surface estimation method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:<claim-text>a map generation step of generating a curvature map indicating distribution of curvatures in a three-dimensional shape of the subject indicated by the three-dimensional image data; and</claim-text><claim-text>an image display step of displaying one of an image of the three-dimensional image data and the two-dimensional image on a display and displaying an image of the curvature map on the display.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The surface estimation method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising an image display step of displaying one of an image of the three-dimensional image data and the two-dimensional image on a display and displaying a region corresponding to the reference surface on the image of the three-dimensional image data or the two-dimensional image.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The surface estimation method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising an abnormality detection step of detecting an abnormal region on the surface of the subject on the basis of one of an image of the three-dimensional image data and the two-dimensional image,<claim-text>wherein the reference region excluding a region corresponding to the abnormal region is set in the region-setting step.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The surface estimation method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising a measurement step of measuring a size of the subject on the basis of the reference surface.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The surface estimation method according to <claim-ref idref="CLM-00022">claim 22</claim-ref>,<claim-text>wherein a three-dimensional distance between the reference surface and a point on the surface of the subject is measured in the measurement step.</claim-text></claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The surface estimation method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>,<claim-text>wherein a geometric feature of the reference surface is estimated in the estimation step.</claim-text></claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. A surface estimation device, comprising a processor configured to:<claim-text>set a reference region that is one of a three-dimensional region and a two-dimensional region,<claim-text>wherein the three-dimensional region includes three or more points and is set in a three-dimensional space,</claim-text><claim-text>wherein the three-dimensional space includes three-dimensional coordinates of three or more points on a subject calculated on the basis of a two-dimensional image of the subject,</claim-text><claim-text>wherein the three-dimensional coordinates of the three or more points are included in three-dimensional image data,</claim-text><claim-text>wherein the two-dimensional region includes three or more points and is set in the two-dimensional image, and</claim-text><claim-text>wherein the three or more points of the reference region include one or more combinations, each of which is constituted by three points that form a triangle; and</claim-text></claim-text><claim-text>estimate a reference surface that approximates a surface of the subject on the basis of three or more points of the three-dimensional image data corresponding to the three or more points included in the reference region.</claim-text></claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. A non-transitory computer-readable recording medium saving a program causing a computer to execute:<claim-text>a region-setting step of setting a reference region that is one of a three-dimensional region and a two-dimensional region,<claim-text>wherein the three-dimensional region includes three or more points and is set in a three-dimensional space,</claim-text><claim-text>wherein the three-dimensional space includes three-dimensional coordinates of three or more points on a subject calculated on the basis of a two-dimensional image of the subject,</claim-text><claim-text>wherein the three-dimensional coordinates of the three or more points are included in three-dimensional image data,</claim-text><claim-text>wherein the two-dimensional region includes three or more points and is set in the two-dimensional image, and</claim-text><claim-text>wherein the three or more points of the reference region include one or more combinations, each of which is constituted by three points that form a triangle; and</claim-text></claim-text><claim-text>an estimation step of estimating a reference surface that approximates a surface of the subject on the basis of three or more points of the three-dimensional image data corresponding to the three or more points included in the reference region.</claim-text></claim-text></claim></claims></us-patent-application>