<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005273A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005273</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17783767</doc-number><date>20201210</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2019-227970</doc-number><date>20191218</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>56</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>56</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">INFORMATION PROCESSING APPARATUS, INFORMATION PROCESSING METHOD, PROGRAM, AND MOVABLE OBJECT</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SONY GROUP CORPORATION</orgname><address><city>TOKYO</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>IRIE</last-name><first-name>DAISUKE</first-name><address><city>TOKYO</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>TAKAHASHI</last-name><first-name>SHUICHI</first-name><address><city>TOKYO</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/046163</doc-number><date>20201210</date></document-id><us-371c12-date><date>20220609</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">To enhance identification accuracy for the external environment of a movable object.</p><p id="p-0002" num="0000">Acquired is image data having an image feature (such as area, date and time, and weather) corresponding to a movement scene of the movable object. Learning with the image data is performed to acquire an inference DNN coefficient for identification of the external environment of the movable object from the image data of the movement scene. For example, the external environment is identified on the basis of, for example, semantic segmentation or depth. The inference DNN to which the inference DNN coefficient is set enables accurate identification of the external environment of the movable object from the image data of the movement scene.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="144.10mm" wi="99.40mm" file="US20230005273A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="108.71mm" wi="122.17mm" file="US20230005273A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="221.32mm" wi="120.23mm" orientation="landscape" file="US20230005273A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="223.10mm" wi="84.41mm" orientation="landscape" file="US20230005273A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="218.95mm" wi="123.53mm" orientation="landscape" file="US20230005273A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="227.16mm" wi="113.62mm" orientation="landscape" file="US20230005273A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="200.49mm" wi="155.19mm" orientation="landscape" file="US20230005273A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="234.70mm" wi="149.27mm" orientation="landscape" file="US20230005273A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="225.89mm" wi="100.67mm" file="US20230005273A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="214.46mm" wi="143.00mm" file="US20230005273A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="151.55mm" wi="160.36mm" file="US20230005273A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0001">The present technology relates to an information processing apparatus, an information processing method, a program, and a movable object, and particularly, to an information processing apparatus and others for enhancing the identification accuracy for the external environment of the movable object.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0004" num="0002">Conventionally, an automated driving vehicle is equipped with an in-vehicle camera, identifies an external environment on the basis of image data of a traveling scene, and automatically controls driving using the result of the identification. For example, the identification result is based on semantic segmentation or depth. Because it is directly linked to safety, very high accuracy is required for identifying the external environment.</p><p id="p-0005" num="0003">In order to identify image data of a traveling scene, it is known to use a deep neural network (DNN) as a machine learning technique. In this case, learning is performed with image data of a traveling scene actually captured by the in-vehicle camera, so that inference DNN coefficients are acquired in advance.</p><p id="p-0006" num="0004">In identification of an external environment based on the image data of the traveling scene using the DNN, a large gap between the image data of the traveling scene and the image data of the traveling scene used for the learning results in a decrease in the accuracy of the identification result. Use of image data of as many scenes as possible in learning results in an increase in the accuracy of the identification result to some extent. However, it is impossible to perform learning covering image data of various scenes in the whole world.</p><p id="p-0007" num="0005">For example, Patent Document 1 discloses that elimination of the deviation in the amount of learning data for each capturing position enables acquisition of a general-purpose learning coefficient.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Document</heading><p id="p-0008" num="0006">Patent Document 1: Japanese Patent Application Laid-Open No. 2018-195237</p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><heading id="h-0006" level="1">Problems to be Solved by the Invention</heading><p id="p-0009" num="0007">An object of the present technology is to improve identification accuracy for the external environment of a movable object.</p><heading id="h-0007" level="1">Solutions to Problems</heading><p id="p-0010" num="0008">According to a concept of the present technology, provided is an information processing apparatus including:</p><p id="p-0011" num="0009">an image-data acquisition unit configured to acquire image data having an image feature corresponding to a movement scene of a movable object; and</p><p id="p-0012" num="0010">a learning DNN unit configured to perform learning with the image data acquired by the image-data acquisition unit to acquire an inference DNN coefficient for identification of an external environment of the movable object from the image data of the movement scene.</p><p id="p-0013" num="0011">In the present technology, the image-data acquisition unit acquires the image data having the image feature corresponding to the movement scene of the movable object. For example, the image feature may include a position element. In this case, for example, the image feature may further include a weather element or a date-and-time element.</p><p id="p-0014" num="0012">The learning DNN unit performs learning with the image data acquired by the image-data acquisition unit to acquire the inference DNN coefficient for identification of the external environment of the movable object from the image data of the movement scene. For example, on the basis of the inference DNN coefficient in a first time zone, the learning DNN unit may perform transfer learning with the image data acquired by the image-data acquisition unit to acquire the inference DNN coefficient to be used in a second time zone following the first time zone.</p><p id="p-0015" num="0013">As described above, in the present technology, learning is performed with image data having an image feature corresponding to a movement scene of a movable object, and an inference DNN coefficient for identification of the external environment of the movable object from the image data of the movement scene is acquired. The inference DNN to which the inference DNN coefficient is set enables accurate identification of the external environment of the movable object from the image data of the movement scene.</p><p id="p-0016" num="0014">Note that in the present technology, for example, the information processing apparatus may further include an image-data reception unit configured to receive, from the movable object, the image data of the movement scene with position information and date-and-time information added to the image data. Further, in the present technology, for example, the information processing apparatus may further include a coefficient transmission unit configured to transmit, to the movable object, the inference DNN coefficient acquired by the learning DNN unit. In this case, for example, when an evaluation value of the inference DNN coefficient acquired by the learning DNN unit is higher than an evaluation value of a general-purpose coefficient, the coefficient transmission unit may transmit, to the movable object, the inference DNN coefficient acquired by the learning DNN unit. This arrangement enables the movable object to use a coefficient having a higher evaluation value.</p><p id="p-0017" num="0015">Further, according to another concept of the present technology, provided is a movable object including:</p><p id="p-0018" num="0016">an inference DNN unit configured to identify an external environment from image data of a movement scene;</p><p id="p-0019" num="0017">a control unit configured to control movement on the basis of an identification result from the inference DNN unit; and</p><p id="p-0020" num="0018">a coefficient reception unit configured to receive, from a cloud server, an inference DNN coefficient to be used by the inference DNN unit,</p><p id="p-0021" num="0019">in which the inference DNN coefficient has been acquired by performing learning with the image data having an image feature corresponding to the movement scene.</p><p id="p-0022" num="0020">In the present technology, the movable object includes the inference DNN unit for identification of the external environment from the image data of the movement scene. The control unit controls the movement on the basis of the identification result from the inference DNN unit. Further, the coefficient reception unit receives, from the cloud server, the inference DNN coefficient to be used by the inference DNN unit. Here, the inference DNN coefficient has been acquired by performing the learning with the image data having the image feature corresponding to the movement scene.</p><p id="p-0023" num="0021">As described above, in the present technology, an inference DNN coefficient has been acquired by performing learning with image data having an image feature corresponding to a movement scene, and the inference DNN coefficient to be used by the inference DNN unit is received from the cloud server. This arrangement enables the inference DNN unit to identify accurately the external environment of the movable object from the image data of the movement scene.</p><p id="p-0024" num="0022">Note that in the present technology, for example, the movable object may further include an image-data transmission unit configured to transmit, to the cloud server, the image data of the movement scene with position information and date-and-time information added to the image data. This arrangement enables provision of the image data of the movement scene to the cloud server. Further, the position information regarding this movable object can be provided to the cloud server, and an inference DNN coefficient corresponding to an area in which this movable object is moving can be easily received from the cloud server.</p><p id="p-0025" num="0023">Furthermore, in the present technology, for example, the movable object may further include a learning DNN unit configured to perform learning with the image data of the movement scene to acquire the inference DNN coefficient; and a coefficient transmission unit configured to transmit, to the cloud server, the inference DNN coefficient acquired by the learning DNN unit. With this arrangement, in a case where an inference DNN coefficient cannot be acquired by performing learning due to insufficient collection of image data by the cloud server, the inference DNN coefficient acquired by the learning DNN unit can used as a substitute.</p><p id="p-0026" num="0024">Still furthermore, in the present technology, for example, with the movable object moving in an overlap region between a first area and a second area toward the second area side, when the coefficient reception unit receives, from the cloud server, the inference DNN coefficient corresponding to the second area, the inference DNN unit may make a switch from the inference DNN coefficient corresponding to the first area to the inference DNN coefficient corresponding to the second area. With this arrangement, even in a case where the area in which the movable object is moving changes, it enables to cause the inference DNN with an appropriate coefficient set thereto to function without being affected by a delay in transmission.</p><p id="p-0027" num="0025">Still furthermore, in the present technology, the inference DNN unit may include a first inference DNN and a second inference DNN, the coefficient reception unit may receive, with the movable object moving in a first area, the inference DNN coefficient corresponding to a second area to which the movable object moves next, the inference DNN coefficient corresponding to the first area may be set to the first inference DNN and the inference DNN coefficient corresponding to the second area may be set to the second inference DNN, and when the movable object moves from the first area into the second area, the inference DNN unit may make a switch from the first inference DNN in use to the second inference DNN to be used. With this arrangement, even in a case where the area in which the movable object is moving changes, it enables to cause the inference DNN with an appropriate coefficient set thereto to function without being affected by a delay in transmission.</p><p id="p-0028" num="0026">Still furthermore, in the present technology, the movable object may further include a storage configured to hold the inference DNN coefficient corresponding to an area in which the movable object is moving and the inference DNN coefficient corresponding to another area around the area, the inference DNN coefficients being each received by the coefficient reception unit, and when the movable object moves from a first area into a second area, the inference DNN unit may extract, from the storage, the inference DNN coefficient corresponding to the second area and may use the inference DNN coefficient extracted. With this arrangement, even in a case where the area in which the movable object is moving changes, it enables to cause the inference DNN with an appropriate coefficient set thereto to function without being affected by a delay in transmission.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0008" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0029" num="0027"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating a configuration example of an automated driving system as an embodiment.</p><p id="p-0030" num="0028"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating configuration examples of an automated driving vehicle and a cloud server.</p><p id="p-0031" num="0029"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates exemplary image data of a traveling scene and an exemplary identification result based on semantic segmentation for the image data of the traveling scene.</p><p id="p-0032" num="0030"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram illustrating a detailed configuration example of a cloud server.</p><p id="p-0033" num="0031"><figref idref="DRAWINGS">FIG. <b>5</b></figref> explanatorily illustrates processing of an image database unit.</p><p id="p-0034" num="0032"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram illustrating a detailed configuration example of a learning DNN unit.</p><p id="p-0035" num="0033"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram illustrating a detailed configuration example of a coefficient database unit.</p><p id="p-0036" num="0034"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart illustrating an exemplary processing procedure of the image database unit.</p><p id="p-0037" num="0035"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating an exemplary processing procedure of the learning DNN unit.</p><p id="p-0038" num="0036"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart illustrating an exemplary processing procedure of the coefficient database unit.</p><p id="p-0039" num="0037"><figref idref="DRAWINGS">FIG. <b>11</b></figref> explanatorily illustrates an exemplary method of handling a delay in transmission of a DNN coefficient.</p><p id="p-0040" num="0038"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a block diagram illustrating a hardware configuration example of the cloud server.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0009" level="1">MODE FOR CARRYING OUT THE INVENTION</heading><p id="p-0041" num="0039">Hereinafter, a mode for carrying out the invention (hereinafter referred to as an &#x201c;embodiment&#x201d;) will be described. Note that the description will be given in the following order.</p><p id="p-0042" num="0040">1. Embodiment</p><p id="p-0043" num="0041">2. Modifications</p><heading id="h-0010" level="1">1. Embodiment</heading><p id="p-0044" num="0042">[Configuration of Automated Driving System]</p><p id="p-0045" num="0043"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a configuration example of an automated driving system <b>10</b> as an embodiment. The automated driving system <b>10</b> includes an automobile (hereinafter, appropriately referred to as &#x201c;automated driving vehicle&#x201d;) <b>100</b> that has a plurality of automated driving functions and is connected to a cloud server <b>200</b> through an Internet <b>300</b>.</p><p id="p-0046" num="0044">In a plurality of areas, namely, areas <b>1</b>, <b>2</b>, . . . , and N in the illustrated example, automated driving vehicles are traveling. Each automated driving vehicle <b>100</b> periodically acquires image data of a scene having an image feature corresponding to the traveling scene, and transmits the image data of the scene to the server <b>200</b> through the Internet <b>300</b>. Here, the image feature corresponding to the traveling scene includes a position element of the traveling scene, for example, information regarding the area in traveling, and includes a weather element, a date-and-time element, and others of the traveling scene.</p><p id="p-0047" num="0045">Each automated driving vehicle <b>100</b> includes an inference deep neural network (DNN) unit <b>101</b> that identifies an external environment from the image data of the traveling scene. The external environment identified by the inference DNN unit <b>101</b> is based on, for example, semantic segmentation or depth. In each automated driving vehicle <b>100</b>, the power, braking, and others in automated driving are controlled on the basis of the identification result of the external environment by the inference DNN unit <b>101</b>.</p><p id="p-0048" num="0046">The cloud server <b>200</b> includes a learning DNN unit <b>201</b>. On the basis of the image data transmitted from each automated driving vehicle <b>100</b>, the learning DNN unit <b>201</b> periodically acquires, on an area and weather basis, a DNN coefficient to be set to the inference DNN unit <b>101</b> of the automated driving vehicle <b>100</b> described above. Then, the cloud server <b>200</b> periodically transmits, to each automated driving vehicle <b>100</b> through the Internet <b>300</b>, the DNN coefficient corresponding to the area in which the automated driving vehicle <b>100</b> is traveling and corresponding to the weather at that time.</p><p id="p-0049" num="0047">In such a manner, the DNN coefficient corresponding to the area in which each automated driving vehicle <b>100</b> is traveling and corresponding to the weather at that time is transmitted from the cloud server <b>200</b> to the automated driving vehicle <b>100</b>. This arrangement enables to enhance the accuracy of the identification result of the external environment by the inference DNN unit <b>101</b> of each automated driving vehicle <b>100</b>. Thus, the power, braking, and others in automated driving can be controlled more accurately.</p><p id="p-0050" num="0048">&#x201c;Configuration Examples of Automated Driving Vehicle and Cloud Server&#x201d;</p><p id="p-0051" num="0049"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates configuration examples of the automated driving vehicle <b>100</b> and the cloud server <b>200</b>. The automated driving vehicle <b>100</b> includes the inference DNN unit <b>101</b>, a capturing unit <b>102</b>, a position/date-and-time acquisition unit <b>103</b>, an image data memory <b>104</b>, a data transmission unit <b>105</b>, a data reception unit <b>106</b>, a DNN coefficient memory <b>107</b>, a control unit <b>108</b>, and a learning DNN unit <b>109</b>.</p><p id="p-0052" num="0050">The capturing unit <b>102</b> includes a lens, a capturing element such as a CCD image sensor or a CMOS image sensor, and others, and periodically acquires image data corresponding to a traveling scene. The position/date-and-time acquisition unit <b>103</b> acquires information regarding the current position using, for example, a global positioning system (GPS). Further, the position/date-and-time acquisition unit <b>104</b> acquires information regarding the current date and time from a clock unit (not illustrated).</p><p id="p-0053" num="0051">The image data memory <b>104</b> temporarily holds the image data of the traveling scene acquired by the capturing unit <b>102</b> with the position information and the date-and-time information acquired by the position/date-and-time acquisition unit <b>103</b> added to the image data. The data transmission unit <b>105</b> transmits, to the cloud server <b>200</b> through the Internet <b>300</b>, the image data (with the position information and the date-and-time information added to the image data) held in the image data memory <b>104</b>.</p><p id="p-0054" num="0052">The data reception unit <b>106</b> receives a DNN coefficient transmitted from the cloud server <b>200</b> through the Internet <b>300</b>. The DNN coefficient memory <b>107</b> temporarily stores the DNN coefficient received by the data reception unit <b>106</b>. The DNN coefficient stored in the DNN coefficient memory <b>107</b> is extracted and set to the inference DNN unit <b>101</b>.</p><p id="p-0055" num="0053">Then, the inference DNN unit <b>101</b> identifies an external environment from the image data of the traveling scene acquired by the capturing unit <b>102</b>. For example, the external environment is identified on the basis of, for example, semantic segmentation or depth. For example, <figref idref="DRAWINGS">FIG. <b>3</b>(<i>a</i>)</figref> illustrates exemplary image data of a traveling scene. <figref idref="DRAWINGS">FIG. <b>3</b>(<i>b</i>)</figref> illustrates an exemplary identification result based on semantic segmentation for the image data of the traveling scene. On the basis of the identification result of the external environment from the inference DNN unit <b>101</b>, the control unit <b>108</b> controls the power, braking, and others in automated driving.</p><p id="p-0056" num="0054">For example, in a case where communication with the cloud server <b>200</b> cannot be established due to no communication network, the learning DNN unit <b>109</b> performs learning with the image data stored in the memory <b>104</b> as learning data, and then acquires a DNN coefficient. In this case, for example, transfer learning based on the DNN coefficient set to and used by the inference DNN unit <b>101</b> is performed in a certain time zone (first time zone), and a DNN coefficient to be used in the next time zone (second time zone) is acquired. This DNN coefficient is a specialization coefficient corresponding to the zone (position) and the weather of the traveling scene.</p><p id="p-0057" num="0055">In a case where communication with the cloud server <b>200</b> becomes enabled, the DNN coefficient acquired by the learning DNN unit <b>109</b> is transmitted from the data transmission unit <b>105</b> to the cloud server <b>200</b> through the Internet <b>300</b>. Alternatively, the DNN coefficient acquired by the learning DNN unit <b>109</b> is set to and used by the inference DNN unit <b>101</b> in the next time zone.</p><p id="p-0058" num="0056">The cloud server <b>200</b> includes the learning DNN unit <b>201</b>, a data reception unit <b>202</b>, an image database unit <b>203</b>, a coefficient database unit <b>204</b>, and a data transmission unit <b>205</b>.</p><p id="p-0059" num="0057">&#x201c;Detailed Configuration Example of Cloud Server&#x201d;</p><p id="p-0060" num="0058"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a detailed configuration example of the cloud server <b>200</b>. The data reception unit <b>202</b> receives the image data of the traveling scene (with position information and date-and-time information added to the image data) transmitted from the automated driving vehicle <b>100</b> through the Internet <b>300</b> by communication such as <b>5</b>G. Further, the data reception unit <b>202</b> receives the DNN coefficient (specialization coefficient) transmitted from the automated driving vehicle <b>100</b>.</p><p id="p-0061" num="0059">&#x201c;Description of Image Database Unit&#x201d;</p><p id="p-0062" num="0060">The image database unit <b>203</b> saves, on an area, date and time, and weather basis, the image data of the traveling scene received by the data reception unit <b>202</b>, on the basis of the position information and the date-and time-information, and the weather information added to the image data. In this case, the weather information can be acquired from a weather information server or can be acquired by analysis of image data. Note that the position information and the date-and-time information are added to the image data of the traveling scene transmitted from the automated driving vehicle <b>100</b> in the above description; however, weather information may be further added to the image data.</p><p id="p-0063" num="0061">Further, the image database unit <b>203</b> configures and acquires, in a certain time zone, a learning data set on an area and weather basis in order to acquire a DNN coefficient to be used by the inference DNN unit <b>101</b> of the automated driving vehicle <b>100</b> in the next time zone.</p><p id="p-0064" num="0062"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a configuration example of a learning data set to be used for learning, in the time zone of 00:30 to 01:00 (time zone in traveling), of a DNN coefficient corresponding to sunny weather in a certain area to be used in the next time zone of 01:00 to 01:30. In this example, the rate of the image data for 00:00 to 00:30 today (sunny) is &#x201c;3&#x201d;, the rate of the image data for 01:00 to 01:30 on June 9th (sunny) is &#x201c;5&#x201d;, and the rate of the image data for 01:00 to 01:30 on June 10th (cloudy) is &#x201c;3&#x201d;. The image data for 01:00 to 01:30 on June 8th (rainy) is not used because the weather is totally different.</p><p id="p-0065" num="0063">Note that in this example, the image data for the time zone in traveling today (sunny) is not included in the configuration of the learning data set; however, it is also conceivable to include the image data. Further, it is also conceivable to refer to the date and time in units of years. For example, it is effective in configuration of a learning data set to be used for learning of a DNN coefficient corresponding to the weather on this day (snowy) in an area in which it rarely snows.</p><p id="p-0066" num="0064">Further, in this example, the time zones are set at intervals of 30 minutes. However, the length of such a time zone can be defined in accordance with the calculation speed of learning. Furthermore, in this example, the learning data set includes only the image data of the same area. However, in a case where the number of pieces of image data is small because of including only the image data of the same area, for example, it is also conceivable to refer to image data of the adjacent area.</p><p id="p-0067" num="0065">&#x201c;Description of Learning DNN Unit&#x201d;</p><p id="p-0068" num="0066">Referring back to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, on the basis of the area-and-weather-based learning data set acquired by the image database unit <b>203</b>, the learning DNN unit <b>201</b> performs learning with a learning DNN on an area and weather basis in a certain time zone, and then acquires a DNN coefficient to be set to and used by the inference DNN unit <b>101</b> of the automated driving vehicle <b>100</b> in the next time zone.</p><p id="p-0069" num="0067">In this case, transfer learning (unsupervised) based on the DNN coefficient in the certain time zone is performed, and a DNN coefficient to be used in the next time zone is acquired. The change between the traveling scene in the certain time zone and the traveling scene in the next time zone is not so large. Thus, sequential transfer learning enables efficient learning having a higher accuracy in a short time and with a small number of pieces of image data. The learning end condition is determined on the basis of, for example, a predefined period of time for learning or the predefined number of epochs (the number of times of updating the coefficient).</p><p id="p-0070" num="0068"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a detailed configuration example of the learning DNN unit <b>201</b>. The learning DNN unit <b>201</b> includes the area-and-weather-based learning DNN, and performs distributed learning in which area-and-weather-based learning is performed in parallel. This distributed learning enables an increase in the overall calculation speed. The illustrated example indicates a case where the areas are 1 to N and the types of weather is 1 to n. It is conceivable that the types of weather include sunny, cloudy, rainy, snowy, and others.</p><p id="p-0071" num="0069">Note that in the above description, in a certain time zone today, performed is learning of the DNN coefficients of each area corresponding to all the types of weather in the next time zone. However, it is also conceivable to omit learning of the DNN coefficients for the types of weather not corresponding at all in each area today. For example, in a case where the corresponding type of weather is only sunny in each area today, learning of the DNN coefficients corresponding to the other types of weather such as cloudy, rainy, and snowy is meaningless, and thus may be omitted.</p><p id="p-0072" num="0070">&#x201c;Description of Coefficient Database Unit&#x201d;</p><p id="p-0073" num="0071">Referring back to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the coefficient database unit <b>204</b> temporarily stores such an area-and-weather-based DNN coefficient (specialization coefficient) to be set to and used by the inference DNN unit <b>101</b> of the automated driving vehicle <b>100</b> next in a time zone, which is acquired by the learning DNN unit <b>201</b> in a certain time zone. Further, the coefficient database unit <b>204</b> also temporarily stores the DNN coefficient (specialization coefficient) transmitted from the automated driving vehicle <b>100</b>.</p><p id="p-0074" num="0072">Furthermore, the coefficient database unit <b>204</b> determines and transmits a DNN coefficient to be transmitted to each automated driving vehicle <b>100</b>. In this case, basically, the area-and-weather-based DNN coefficient, that is, the specialization coefficient is determined as the DNN coefficient to be transmitted. However, in a case where evaluation is performed on the basis of the loss function of the DNN and the evaluation value of the specialization coefficient is lower than that of the general-purpose coefficient, the general-purpose coefficient is determined as the DNN coefficient to be transmitted instead of the specialization coefficient. Here, the general-purpose coefficient is a DNN coefficient obtained by learning in advance with image data of a traveling scene satisfying a wide variety of conditions (position, weather, date and time, and others), and is a DNN coefficient that can correspond to the wide variety of conditions.</p><p id="p-0075" num="0073"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a detailed configuration example of the coefficient database unit <b>204</b>. The coefficient database unit <b>204</b> includes a storage unit <b>241</b> that stores such DNN coefficients, and a DNN-coefficient determination unit <b>242</b> that determines and outputs a DNN coefficient to be transmitted to each automated driving vehicle <b>100</b>. In addition, the storage unit <b>241</b> includes a storage unit <b>241</b><i>a </i>that temporarily stores the area-and-weather-based DNN coefficient (specialization coefficient) acquired by the learning DNN unit <b>201</b>, and a storage unit <b>241</b><i>b </i>that temporarily stores the area-and-weather-based DNN coefficient (specialization coefficient) received by the data reception unit <b>202</b>.</p><p id="p-0076" num="0074">Further, the DNN-coefficient determination unit <b>242</b> basically determines the area-and-weather-based DNN coefficient as a DNN coefficient in the next time zone to be transmitted to each automated driving vehicle <b>100</b>, extracts the determined DNN coefficient from the storage unit <b>241</b>, and then outputs the extracted DNN coefficient as the DNN coefficient to be transmitted. In this case, the determined DNN coefficient is basically extracted from the storage unit <b>241</b><i>a. </i>However, in a case where the DNN coefficient is absent in the storage unit <b>241</b><i>a </i>(corresponding to a case where learning cannot be performed due to a system failure or the like), if the DNN coefficient is present in the storage unit <b>241</b><i>b, </i>the DNN coefficient is extracted from the storage unit <b>241</b><i>b. </i></p><p id="p-0077" num="0075">Note that in a case where the determined DNN coefficient is present in both the storage unit <b>241</b><i>a </i>and the storage unit <b>241</b><i>b, </i>it is also conceivable that the DNN-coefficient determination unit <b>242</b> is configured to output the DNN coefficient higher in evaluation value, as the DNN coefficient to be transmitted.</p><p id="p-0078" num="0076">In this embodiment, actually, the DNN-coefficient determination unit <b>242</b> outputs, as a DNN coefficient to be transmitted, the DNN coefficient extracted from the storage unit <b>241</b>. In other words, only when the evaluation value of the specialization coefficient is higher than that of the general-purpose coefficient, the DNN-coefficient determination unit <b>242</b> outputs the specialization coefficient as a DNN coefficient to be transmitted. When the evaluation value of the specialization coefficient is lower than that of the general-purpose coefficient, the general-purpose coefficient is output as a DNN coefficient to be transmitted.</p><p id="p-0079" num="0077">As a case where the evaluation value of the specialization coefficient is lower than that of the general-purpose coefficient, for example, assumed is a case where the number of pieces of image data for learning is insufficient and sufficient learning cannot be performed. As a result, when the specialization coefficient is an inappropriate DNN coefficient, it can be avoided that the DNN coefficient is used on the automated driving vehicle <b>100</b> side.</p><p id="p-0080" num="0078">Further, the DNN-coefficient determination unit <b>242</b> may be configured to output the general-purpose coefficient as the DNN coefficient to be transmitted in a case where the determined DNN coefficient is absent in the storage unit <b>241</b> (storage unit <b>241</b><i>a </i>or <b>241</b><i>b</i>).</p><p id="p-0081" num="0079">Referring back to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the data transmission unit <b>205</b> transmits the DNN coefficient (specialization coefficient or general-purpose coefficient) determined by the coefficient database unit <b>204</b> to each automated driving vehicle <b>100</b> through the Internet <b>300</b>. In this case, the DNN coefficient to be used in the next time zone is transmitted to each automated driving vehicle <b>100</b> in a certain time zone.</p><p id="p-0082" num="0080">Note that in the above description, the coefficient database unit <b>204</b> outputs the general-purpose coefficient as the DNN coefficient to be transmitted when the evaluation value of the specialization coefficient is lower and the data transmission unit <b>205</b> transmits the general-purpose coefficient to the automated driving vehicle <b>100</b>. However, it is also conceivable that the coefficient database unit <b>204</b> is configured to output a command for instructing use of the general-purpose coefficient, when the evaluation value of the specialization coefficient is lower, and the data transmission unit <b>205</b> transmits the command to the automated driving vehicle <b>100</b>. In that case, according to the command, the automated driving vehicle <b>100</b> uses a general-purpose coefficient that the automated driving vehicle <b>100</b> has.</p><p id="p-0083" num="0081">It is also conceivable that the coefficient database unit <b>204</b> is configured to constantly output the specialization coefficient without comparing the evaluation value of the specialization coefficient with that of the general-purpose coefficient, the data transmission unit <b>205</b> is configured to transmit the specialization coefficient to the automated driving vehicle <b>100</b>, and the automated driving vehicle <b>100</b> is configured to compare the evaluation value of the specialization coefficient with that of the general-purpose coefficient to determine which to use.</p><p id="p-0084" num="0082">Note that in a case where the coefficient database unit <b>204</b> outputs the general-purpose coefficient as a DNN coefficient of certain area and weather, it is conceivable that the learning DNN unit <b>201</b> performs transfer learning based on the general-purpose coefficient in a case where the DNN coefficient in the next time zone of the area and weather is acquired.</p><p id="p-0085" num="0083">&#x201c;Exemplary Processing Procedures of Image database Unit, Learning DNN Unit, and Coefficient Database Unit&#x201d;</p><p id="p-0086" num="0084">The flowchart of <figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an exemplary processing procedure of the image database unit <b>203</b>. In step ST<b>1</b>, the image database unit <b>203</b> acquires, from the data reception unit <b>202</b>, image data of a traveling scene (with position information and date-and-time information added to the image data) of each automated driving vehicle <b>100</b>. Next, in step ST<b>2</b>, the image database unit <b>203</b> adds weather information to the acquired image data of the traveling scene (with position information and date-and-time information added to the image data) of each automated driving vehicle <b>100</b>.</p><p id="p-0087" num="0085">Next, in step ST<b>3</b>, the image database unit <b>203</b> saves, on an image feature (area, date and time, and weather) basis, the acquired image data of the traveling scene of each automated driving vehicle <b>100</b>. Next, in step ST<b>4</b>, the image database unit <b>203</b> determines, on an image feature (area and weather) basis, an image data set for learning of a DNN coefficient in the next time zone (see <figref idref="DRAWINGS">FIG. <b>5</b></figref>).</p><p id="p-0088" num="0086">Next, in step ST<b>5</b>, the image database unit <b>203</b> performs preprocessing on each image-feature-(area and weather)-based image data set and transmits each image-feature-(area and weather)-based image data set to the learning DNN unit <b>201</b>. For example, the preprocessing includes processing of cutting image data into patches, processing of normalizing pixel values, and processing of shuffling the order. This is preprocessing typically performed for learning regarding image data.</p><p id="p-0089" num="0087">The flowchart of <figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an exemplary processing procedure of the learning DNN unit <b>201</b>. In step ST<b>11</b>, the learning DNN unit <b>201</b> acquires the image-feature-(area and weather)-based image data set from the image database unit <b>203</b>. Next, in step ST<b>12</b>, with the image data set, the learning DNN unit <b>201</b> learns (transfer-learns) the DNN coefficient with the learning DNN, on an image feature (area and weather) basis.</p><p id="p-0090" num="0088">Next, in step ST<b>13</b>, the learning DNN unit <b>201</b> ends the learning on the basis of a predefined period of time for learning or the number of epochs. Next, in step ST<b>14</b>, the learning DNN unit <b>201</b> transmits, to the coefficient database unit <b>204</b>, the DNN coefficient learned on the image feature (area and weather) basis.</p><p id="p-0091" num="0089">The flowchart of <figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an exemplary processing procedure of the coefficient database unit <b>204</b>. In step ST<b>21</b>, the coefficient database unit <b>204</b> acquires, from the learning DNN unit <b>201</b>, the DNN coefficient learned on the image feature (area and weather) basis. Next, in step ST<b>22</b>, the coefficient database unit <b>204</b> saves the acquired DNN coefficient to which information regarding the image feature (area and weather) is added.</p><p id="p-0092" num="0090">Next, in step ST<b>23</b>, the coefficient database unit <b>204</b> evaluates whether or not the acquired DNN coefficient (specialization coefficient) is higher in accuracy than the general-purpose coefficient. This evaluation is performed on the basis of a loss function. Next, in step ST<b>24</b>, the coefficient database unit <b>204</b> transmits the acquired DNN coefficient (specialization coefficient) to the data transmission unit <b>205</b> if the accuracy is higher in accuracy, otherwise transmits the general-purpose coefficient to the data transmission unit <b>205</b> if the accuracy is not higher or transmits a command for use of the in-vehicle general-purpose coefficient to the data transmission unit <b>205</b>.</p><p id="p-0093" num="0091">&#x201c;Method of Handling Delay in Transmission of DNN Coefficient&#x201d;</p><p id="p-0094" num="0092">In a case where a delay occurs in transmission of a DNN coefficient from the cloud server <b>200</b> to the automated driving vehicle <b>100</b>, there are areas between which the automated driving vehicle <b>100</b> cannot receive the DNN coefficient corresponding to the traveling area. As a method of handling such a delay in transmission, for example, the following (1) to (3) are conceivable. With these methods of handling, even in a case where the area in which the automated driving vehicle <b>100</b> is traveling changes, it enables to cause the inference DNN with an appropriate DNN coefficient set thereto to function without being affected by a delay in transmission.</p><p id="p-0095" num="0093">(1) In this method, an overlap region is provided between an area and another area, and a switch is made between DNN coefficients within this region. In this case, with the automated driving vehicle <b>100</b> moving in the overlap region between a first area and a second area toward the second area side, when the data reception unit <b>106</b> receives, from the cloud server <b>200</b>, a coefficient corresponding to the second area, the inference DNN unit <b>101</b> of the automated driving vehicle <b>100</b> makes a switch from a DNN coefficient corresponding to the first area to the DNN coefficient corresponding to the second area.</p><p id="p-0096" num="0094">(2) In this method, an area (region) as a movement destination is predicted on the basis of a travel direction of the automated driving vehicle <b>100</b>, a coefficient corresponding to the area as the movement destination is applied in advance to two separately prepared inference DNNs, and a switch is made to the inference DNN to be used, when the automated driving vehicle <b>100</b> crosses the area or travels in the overlap region. In this case, the inference DNN unit <b>101</b> includes a first inference DNN and a second inference DNN, and the data reception unit <b>106</b> receives, with the automated driving vehicle <b>100</b> moving in the first area, a DNN coefficient corresponding to the second area to which the automated driving vehicle <b>100</b> moves next. Then, the DNN coefficient of the first area is set to the first inference DNN, and the DNN coefficient of the second area is set to the second inference DNN. When the automated driving vehicle <b>100</b> moves from the first area into the second area, the inference DNN unit <b>101</b> makes a switch from the first inference DNN in use to the second inference DNN to be used.</p><p id="p-0097" num="0095">(3) In this method, on the automated driving vehicle <b>100</b> side, DNN coefficients of a wide range including an area in traveling are held in a storage in advance. When the automated driving vehicle <b>100</b> crosses the area or travels in the overlap region, a switch is made to a DNN coefficient to be used by the inference DNN unit <b>101</b>. In this case, the automated driving vehicle <b>100</b> includes a storage that holds a DNN coefficient corresponding to an area in which the automated driving vehicle <b>100</b> is moving and a DNN coefficient corresponding to another area around the area, and the inference DNN coefficients are each received by the data reception unit <b>106</b>. When the automated driving vehicle <b>100</b> moves into the second area from the first area in which the automated driving vehicle <b>100</b> is traveling, the inference DNN unit <b>101</b> extracts the DNN coefficient of the second area from the storage and uses the DNN coefficient extracted.</p><p id="p-0098" num="0096">In this case, the range of the DNN coefficients held in the storage changes in accordance with traveling of the automated driving vehicle <b>100</b>. In <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the ellipse with a broken line indicates an area in which the automated driving vehicle <b>100</b> is traveling, and each ellipse with a solid line indicates an area around the area of which the DNN coefficient is held in the storage in advance. Note that in the illustrated example, the overlap regions between the areas are present; however, it is also conceivable that no overlap region is present.</p><p id="p-0099" num="0097">&#x201c;Hardware Configuration Example of Cloud Server&#x201d;</p><p id="p-0100" num="0098"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a block diagram illustrating a hardware configuration example of the cloud server <b>200</b>. In the cloud server <b>200</b>, a central processing unit (CPU) <b>501</b>, a read only memory (ROM) <b>502</b>, and a random access memory (RAM) <b>503</b> are mutually connected through a bus <b>504</b> Further, an input/output interface <b>505</b> is connected to the bus <b>504</b>. An input unit <b>506</b>, an output unit <b>507</b>, a storage unit <b>508</b>, a communication unit <b>509</b>, and a drive <b>510</b> are connected to the input/output interface <b>505</b>.</p><p id="p-0101" num="0099">The input unit <b>506</b> includes a keyboard, a mouse, and a microphone. The output unit <b>507</b> includes a display and a speaker. The storage unit <b>508</b> includes a hard disk or a non-volatile memory. The communication unit <b>509</b> includes a network interface. The drive <b>510</b> drives a removable medium <b>511</b> such as a magnetic disk, an optical disk, a magneto-optical disk, or a semiconductor memory.</p><p id="p-0102" num="0100">In the cloud server <b>200</b> having the configuration as above, the CPU <b>501</b> loads, for example, a program stored in the storage unit <b>508</b>, into the RAM <b>503</b> through the input/output interface <b>505</b> and the bus <b>504</b> to execute the program, so that the series of processing described above is performed.</p><p id="p-0103" num="0101">The program executed by the CPU <b>501</b> can be provided by being recorded on, for example, the removable medium <b>511</b> as a package medium or the like. Alternatively, the program can be provided through a wired or wireless transmission medium such as a local area network, an Internet, or digital satellite broadcasting.</p><p id="p-0104" num="0102">In the cloud server <b>200</b>, the program can be installed in the storage unit <b>508</b> through the input/output interface <b>505</b> by attachment of the removable medium <b>511</b> to the drive <b>510</b>. Alternatively, the program can be received by the communication unit <b>509</b> through the wired or wireless transmission medium and can be installed in the storage unit <b>508</b>. In addition, the program can be preinstalled in the ROM <b>502</b> or the storage unit <b>508</b>.</p><p id="p-0105" num="0103">Note that the program executed by the CPU <b>501</b> may be a program for chronologically performing the processing in accordance with the order described in the present specification, may be a program for parallelly performing the processing or a program for performing the processing with necessary timing, for example, when a call is made.</p><p id="p-0106" num="0104">As above, in the automated driving system <b>10</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the DNN coefficient (specialization coefficient) corresponding to the area in which each automated driving vehicle <b>100</b> is traveling and corresponding to the weather at that time is transmitted from the cloud server <b>200</b> to the automated driving vehicle <b>100</b>. This arrangement enables to enhance the accuracy of the identification result of the external environment by the inference DNN unit <b>101</b> of each automated driving vehicle <b>100</b>. Thus, the power, braking, and others in automated driving can be controlled more accurately.</p><heading id="h-0011" level="1">2. Modifications</heading><p id="p-0107" num="0105">Note that in the above embodiment, an area is not particularly mentioned; however, for a dangerous area, it is also conceivable to perform learning with the area narrowed in range. Learning with an area narrowed in range in such a manner enables an increase in the accuracy of the learned DNN coefficient.</p><p id="p-0108" num="0106">Further, in the above embodiment, the example in which the movable object is the automobile <b>100</b> is given. The present technology, however, is similarly applicable even if the movable object is, for example, an autonomous traveling robot, a flight object such as a drone, or the like. For example, in the case of a flight object such as a drone, it is also conceivable to define an image feature regarding the altitude of flight. For example, an altitude of 0 to 2 m is close to a human's viewpoint, and an altitude of not less than several tens of meters is a scene of aerial capturing.</p><p id="p-0109" num="0107">The preferred embodiment of the present disclosure has been described in detail with reference to the accompanying drawings; however, the technical scope of the present disclosure is not limited to the examples. It is obvious that persons having ordinary knowledge in the technical field of the present disclosure can conceive various types of alternations or modifications within the scope of the technical idea described in the claims, and thus it is also naturally understood that such alternations or modifications belong to the technical scope of the present disclosure.</p><p id="p-0110" num="0108">Further, the effects described in the present specification are merely explanatory or exemplary, and thus are not limitative. That is, the technology according to the present disclosure can exhibit other effects obvious to those skilled in the art from the description of the present specification, together with or instead of the above effects.</p><p id="p-0111" num="0109">Note that the present technology can also adopt the following configurations.</p><p id="p-0112" num="0110">(1) An information processing apparatus including:</p><p id="p-0113" num="0111">an image-data acquisition unit configured to acquire image data having an image feature corresponding to a movement scene of a movable object; and</p><p id="p-0114" num="0112">a learning DNN unit configured to perform learning with the image data acquired by the image-data acquisition unit to acquire an inference DNN coefficient for identification of an external environment of the movable object from the image data of the movement scene.</p><p id="p-0115" num="0113">(2) The information processing apparatus according to (1) described above,</p><p id="p-0116" num="0114">in which the image feature includes a position element.</p><p id="p-0117" num="0115">(3) The information processing apparatus according to (2) described above,</p><p id="p-0118" num="0116">in which the image feature further includes a weather element.</p><p id="p-0119" num="0117">(4) The information processing apparatus according to (2) or (3) described above,</p><p id="p-0120" num="0118">in which the image feature further includes a date-and-time element.</p><p id="p-0121" num="0119">(5) The information processing apparatus according to any of (1) to (4) described above,</p><p id="p-0122" num="0120">in which on the basis of the inference DNN coefficient in a first time zone, the learning DNN unit performs transfer learning with the image data acquired by the image-data acquisition unit to acquire the inference DNN coefficient to be used in a second time zone following the first time zone.</p><p id="p-0123" num="0121">(6) The information processing apparatus according to any of (1) to (5) described above, further including:</p><p id="p-0124" num="0122">an image-data reception unit configured to receive, from the movable object, the image data of the movement scene with position information and date-and-time information added to the image data.</p><p id="p-0125" num="0123">(7) The information processing apparatus according to any of (1) to (6) described above, further including:</p><p id="p-0126" num="0124">a coefficient transmission unit configured to transmit, to the movable object, the inference DNN coefficient acquired by the learning DNN unit.</p><p id="p-0127" num="0125">(8) The information processing apparatus according to (7) described above,</p><p id="p-0128" num="0126">in which when an evaluation value of the inference DNN coefficient acquired by the learning DNN unit is higher than an evaluation value of a general-purpose coefficient, the coefficient transmission unit transmits, to the movable object, the inference DNN coefficient acquired by the learning DNN unit.</p><p id="p-0129" num="0127">(9) An information processing method including:</p><p id="p-0130" num="0128">a procedure of acquiring image data having an image feature corresponding to a movement scene of a movable object; and</p><p id="p-0131" num="0129">a procedure of performing learning with the image data acquired to acquire an inference DNN coefficient for identification of an external environment of the movable object from the image data of the movement scene.</p><p id="p-0132" num="0130">(10) A program for causing a computer to function as:</p><p id="p-0133" num="0131">image-data acquisition means configured to acquire image data having an image feature corresponding to a movement scene of a movable object; and</p><p id="p-0134" num="0132">learning DNN means configured to perform learning with the image data acquired by the image-data acquisition means to acquire an inference DNN coefficient for identification of an external environment of the movable object from the image data of the movement scene.</p><p id="p-0135" num="0133">(11) A movable object including:</p><p id="p-0136" num="0134">an inference DNN unit configured to identify an external environment from image data of a movement scene;</p><p id="p-0137" num="0135">a control unit configured to control movement on the basis of an identification result from the inference DNN unit; and</p><p id="p-0138" num="0136">a coefficient reception unit configured to receive, from a cloud server, an inference DNN coefficient to be used by the inference DNN unit,</p><p id="p-0139" num="0137">in which the inference DNN coefficient has been acquired by performing learning with the image data having an image feature corresponding to the movement scene.</p><p id="p-0140" num="0138">(12) The movable object according to (11) described above, further including:</p><p id="p-0141" num="0139">an image-data transmission unit configured to transmit, to the cloud server, the image data of the movement scene with position information and date-and-time information added to the image data.</p><p id="p-0142" num="0140">(13) The movable object according to (11) or (12) described above, further including:</p><p id="p-0143" num="0141">a learning DNN unit configured to perform learning with the image data of the movement scene to acquire the inference DNN coefficient; and</p><p id="p-0144" num="0142">a coefficient transmission unit configured to transmit, to the cloud server, the inference DNN coefficient acquired by the learning DNN unit.</p><p id="p-0145" num="0143">(14) The movable object according to any of (11) to (13) described above,</p><p id="p-0146" num="0144">in which with the movable object moving in an overlap region between a first area and a second area toward the second area side, when the coefficient reception unit receives, from the cloud server, the inference DNN coefficient corresponding to the second area, the inference DNN unit makes a switch from the inference DNN coefficient corresponding to the first area to the inference DNN coefficient corresponding to the second area.</p><p id="p-0147" num="0145">(15) The movable object according to any of (11) to (13) described above,</p><p id="p-0148" num="0146">in which the inference DNN unit includes a first inference DNN and a second inference DNN,</p><p id="p-0149" num="0147">the coefficient reception unit receives, with the movable object moving in a first area, the inference DNN coefficient corresponding to a second area to which the movable object moves next,</p><p id="p-0150" num="0148">the inference DNN coefficient corresponding to the first area is set to the first inference DNN and the inference DNN coefficient corresponding to the second area is set to the second inference DNN, and</p><p id="p-0151" num="0149">when the movable object moves from the first area into the second area, the inference DNN unit makes a switch from the first inference DNN in use to the second inference DNN to be used.</p><p id="p-0152" num="0150">(16) The movable object according to any of (11) to (13), further including:</p><p id="p-0153" num="0151">a storage configured to hold the inference DNN coefficient corresponding to an area in which the movable object is moving and the inference DNN coefficient corresponding to another area around the area, the inference DNN coefficients being each received by the coefficient reception unit,</p><p id="p-0154" num="0152">in which when the movable object moves from a first area into a second area, the inference DNN unit extracts, from the storage, the inference DNN coefficient corresponding to the second area and uses the inference DNN coefficient extracted.</p><heading id="h-0012" level="1">REFERENCE SIGNS LIST</heading><p id="p-0155" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0153"><b>10</b> Automated driving system</li>    <li id="ul0001-0002" num="0154"><b>100</b> Automated driving vehicle</li>    <li id="ul0001-0003" num="0155"><b>101</b> Inference DNN unit</li>    <li id="ul0001-0004" num="0156"><b>102</b> Capturing unit</li>    <li id="ul0001-0005" num="0157"><b>103</b> Position/date-and-time acquisition unit</li>    <li id="ul0001-0006" num="0158"><b>104</b> Image data memory</li>    <li id="ul0001-0007" num="0159"><b>105</b> Data transmission unit</li>    <li id="ul0001-0008" num="0160"><b>106</b> Data reception unit</li>    <li id="ul0001-0009" num="0161"><b>107</b> DNN coefficient memory</li>    <li id="ul0001-0010" num="0162"><b>108</b> Control unit</li>    <li id="ul0001-0011" num="0163"><b>109</b> Learning DNN unit</li>    <li id="ul0001-0012" num="0164"><b>200</b> Cloud server</li>    <li id="ul0001-0013" num="0165"><b>201</b> Learning DNN unit</li>    <li id="ul0001-0014" num="0166"><b>202</b> Data reception unit</li>    <li id="ul0001-0015" num="0167"><b>203</b> Image database unit</li>    <li id="ul0001-0016" num="0168"><b>204</b> Coefficient database unit</li>    <li id="ul0001-0017" num="0169"><b>205</b> Data transmission unit</li>    <li id="ul0001-0018" num="0170"><b>241</b>, <b>241</b><i>a, </i><b>241</b><i>b </i>Storage unit</li>    <li id="ul0001-0019" num="0171"><b>242</b> Output-DNN-coefficient determination unit</li>    <li id="ul0001-0020" num="0172"><b>300</b> Internet</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An information processing apparatus comprising:<claim-text>an image-data acquisition unit configured to acquire image data having an image feature corresponding to a movement scene of a movable object; and</claim-text><claim-text>a learning DNN unit configured to perform learning with the image data acquired by the image-data acquisition unit to acquire an inference DNN coefficient for identification of an external environment of the movable object from the image data of the movement scene.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the image feature includes a position element.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The information processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>,<claim-text>wherein the image feature further includes a weather element.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The information processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>,<claim-text>wherein the image feature further includes a date-and-time element.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein on a basis of the inference DNN coefficient in a first time zone, the learning DNN unit performs transfer learning with the image data acquired by the image-data acquisition unit to acquire the inference DNN coefficient to be used in a second time zone following the first time zone.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>an image-data reception unit configured to receive, from the movable object, the image data of the movement scene with position information and date-and-time information added to the image data.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a coefficient transmission unit configured to transmit, to the movable object, the inference DNN coefficient acquired by the learning DNN unit.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The information processing apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>,<claim-text>wherein when an evaluation value of the inference DNN coefficient acquired by the learning DNN unit is higher than an evaluation value of a general-purpose coefficient, the coefficient transmission unit transmits, to the movable object, the inference DNN coefficient acquired by the learning DNN unit.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. An information processing method comprising:<claim-text>a procedure of acquiring image data having an image feature corresponding to a movement scene of a movable object; and</claim-text><claim-text>a procedure of performing learning with the image data acquired to acquire an inference DNN coefficient for identification of an external environment of the movable object from the image data of the movement scene.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A program for causing a computer to function as:<claim-text>image-data acquisition means configured to acquire image data having an image feature corresponding to a movement scene of a movable object; and</claim-text><claim-text>learning DNN means configured to perform learning with the image data acquired by the image-data acquisition means to acquire an inference DNN coefficient for identification of an external environment of the movable object from the image data of the movement scene.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A movable object comprising:<claim-text>an inference DNN unit configured to identify an external environment from image data of a movement scene;</claim-text><claim-text>a control unit configured to control movement on a basis of an identification result from the inference DNN unit; and</claim-text><claim-text>a coefficient reception unit configured to receive, from a cloud server, an inference DNN coefficient to be used by the inference DNN unit,</claim-text><claim-text>wherein the inference DNN coefficient has been acquired by performing learning with the image data having an image feature corresponding to the movement scene.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The movable object according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>an image-data transmission unit configured to transmit, to the cloud server, the image data of the movement scene with position information and date-and-time information added to the image data.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The movable object according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>a learning DNN unit configured to perform learning with the image data of the movement scene to acquire the inference DNN coefficient; and</claim-text><claim-text>a coefficient transmission unit configured to transmit, to the cloud server, the inference DNN coefficient acquired by the learning DNN unit.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The movable object according to <claim-ref idref="CLM-00011">claim 11</claim-ref>,<claim-text>wherein with the movable object moving in an overlap region between a first area and a second area toward the second area side, when the coefficient reception unit receives, from the cloud server, the inference DNN coefficient corresponding to the second area, the inference DNN unit makes a switch from the inference DNN coefficient corresponding to the first area to the inference DNN coefficient corresponding to the second area.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The movable object according to <claim-ref idref="CLM-00011">claim 11</claim-ref>,<claim-text>wherein the inference DNN unit includes a first inference DNN and a second inference DNN,</claim-text><claim-text>the coefficient reception unit receives, with the movable object moving in a first area, the inference DNN coefficient corresponding to a second area to which the movable object moves next,</claim-text><claim-text>the inference DNN coefficient corresponding to the first area is set to the first inference DNN and the inference DNN coefficient corresponding to the second area is set to the second inference DNN, and</claim-text><claim-text>when the movable object moves from the first area into the second area, the inference DNN unit makes a switch from the first inference DNN in use to the second inference DNN to be used.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The movable object according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>a storage configured to hold the inference DNN coefficient corresponding to an area in which the movable object is moving and the inference DNN coefficient corresponding to another area around the area, the inference DNN coefficients being each received by the coefficient reception unit,</claim-text><claim-text>wherein when the movable object moves from a first area into a second area, the inference DNN unit extracts, from the storage, the inference DNN coefficient corresponding to the second area and uses the inference DNN coefficient extracted.</claim-text></claim-text></claim></claims></us-patent-application>