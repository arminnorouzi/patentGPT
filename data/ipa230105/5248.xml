<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005249A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005249</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17785282</doc-number><date>20201215</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2019-233594</doc-number><date>20191224</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>94</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>7747</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>95</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>87</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">INFORMATION PROCESSING APPARATUS, INFORMATION PROCESSING SYSTEM, INFORMATION PROCESSING METHOD, AND INFORMATION PROCESSING PROGRAM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SONY GROUP CORPORATION</orgname><address><city>TOKYO</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>SATOH</last-name><first-name>RYUTA</first-name><address><city>TOKYO</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/046778</doc-number><date>20201215</date></document-id><us-371c12-date><date>20220614</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An object of the present disclosure is to provide an information processing apparatus, an information processing system, an information processing method, and an information processing program capable of achieving efficient use of training data. An information processing apparatus according to the present disclosure includes: a recognition unit (<b>101</b>) that performs object recognition processing using sensor information acquired by a sensor, the object recognition processing being performed by a first recognizer that has been pretrained; and a training data application determination unit (<b>22</b><i>d</i>) that determines whether the sensor information is applicable as training data to a second recognizer different from the first recognizer.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="71.46mm" wi="95.76mm" file="US20230005249A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="185.17mm" wi="137.67mm" orientation="landscape" file="US20230005249A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="230.21mm" wi="136.48mm" orientation="landscape" file="US20230005249A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="214.12mm" wi="155.79mm" file="US20230005249A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="230.21mm" wi="136.48mm" orientation="landscape" file="US20230005249A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="198.29mm" wi="91.44mm" orientation="landscape" file="US20230005249A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="206.16mm" wi="129.88mm" file="US20230005249A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="232.24mm" wi="91.52mm" orientation="landscape" file="US20230005249A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="227.41mm" wi="129.88mm" file="US20230005249A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="198.37mm" wi="91.52mm" orientation="landscape" file="US20230005249A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="85.17mm" wi="97.79mm" file="US20230005249A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="208.28mm" wi="129.88mm" file="US20230005249A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="230.21mm" wi="136.48mm" orientation="landscape" file="US20230005249A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="225.21mm" wi="87.71mm" orientation="landscape" file="US20230005249A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="216.83mm" wi="129.88mm" file="US20230005249A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="229.95mm" wi="92.46mm" orientation="landscape" file="US20230005249A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="234.78mm" wi="129.88mm" file="US20230005249A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="216.75mm" wi="106.34mm" orientation="landscape" file="US20230005249A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="150.45mm" wi="129.88mm" file="US20230005249A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="236.56mm" wi="154.69mm" orientation="landscape" file="US20230005249A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="215.05mm" wi="172.30mm" file="US20230005249A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="230.21mm" wi="136.48mm" orientation="landscape" file="US20230005249A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="225.21mm" wi="106.26mm" orientation="landscape" file="US20230005249A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="232.24mm" wi="163.58mm" file="US20230005249A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="225.21mm" wi="87.63mm" orientation="landscape" file="US20230005249A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="235.37mm" wi="171.62mm" file="US20230005249A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="225.21mm" wi="113.88mm" orientation="landscape" file="US20230005249A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="237.24mm" wi="173.91mm" file="US20230005249A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="186.44mm" wi="155.87mm" orientation="landscape" file="US20230005249A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="197.95mm" wi="155.53mm" file="US20230005249A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD</heading><p id="p-0002" num="0001">The present disclosure relates to an information processing apparatus, an information processing system, an information processing method, and an information processing program.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">There has been proposed an information processing system that analyzes a captured image, specifically, an image captured when a vehicle travels, using an in-vehicle camera mounted on the vehicle, and determines whether an obstacle such as a person exists in a traveling direction of the vehicle. Furthermore, there is proposed a technique of detecting in such an information processing system, an obstacle based on a captured image using a recognition model trained by machine learning. In addition, in a known configuration, a recognition model trained by machine learning is built by a server on a network, for example.</p><p id="p-0004" num="0003">Meanwhile, when using machine learning to detect an obstacle or the like based on a captured image, it is preferable to occasionally collect training data and perform retraining on the recognition model to update the recognition model. For example, the server acquires and collects, via communication, data obtained by the vehicle and performs retraining on the recognition model using the collected data as training data, and updates the recognition model. The updated recognition model is transmitted to the vehicle by communication. This makes it possible, on the vehicle side, to detect an obstacle or the like with higher accuracy by using the updated recognition model.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Literature</heading><p id="p-0005" num="0004">Patent Literature 1: JP 2018-202595 A</p><heading id="h-0005" level="1">SUMMARY</heading><heading id="h-0006" level="1">Technical Problem</heading><p id="p-0006" num="0005">However, data transmitted for retraining has various attributes depending on a situation of acquisition of the data such as an area where the data transmission has been performed, and the data utilization has been limited to the range in which the attributes are effective.</p><p id="p-0007" num="0006">An object of the present disclosure is to provide an information processing apparatus, an information processing system, an information processing method, and an information processing program capable of achieving efficient use of training data.</p><heading id="h-0007" level="1">Solution to Problem</heading><p id="p-0008" num="0007">For solving the problem described above, an information processing apparatus according to one aspect of the present disclosure has a recognition unit that performs object recognition processing using sensor information acquired by a sensor, the object recognition processing being performed by a first recognizer that has been pretrained; and a training data application determination unit that determines whether the sensor information is applicable as training data to a second recognizer different from the first recognizer.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0008" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram of an example schematically illustrating an overall picture of a driving assistance system related to each embodiment.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a configuration of an example of a driving assistance system according to each embodiment.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating an example of a hardware configuration of a terminal device applicable to each embodiment.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram illustrating an example of a hardware configuration of a server system applicable to an embodiment.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram illustrating a configuration of an example of a driving assistance system according to a first embodiment.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a functional block diagram of an example illustrating functions of a server system according to the first embodiment.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of an example illustrating processing in the server system according to the first embodiment.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a functional block diagram of an example illustrating functions of a server system according to a first modification of the first embodiment.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart of an example illustrating processing in the server system according to the first embodiment.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a functional block diagram of an example illustrating functions of a server system according to a second modification of the first embodiment.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a schematic diagram illustrating an example of an image based on image data included in sensor information.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart of an example illustrating processing in the server system according to the second modification of the first embodiment.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a block diagram illustrating a configuration of an example of a driving assistance system according to a second embodiment.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a functional block diagram of an example illustrating functions of a terminal device according to the second embodiment.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flowchart of an example illustrating processing in the terminal device according to the second embodiment.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a functional block diagram of an example illustrating functions of a terminal device according to a first modification of the second embodiment.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a flowchart of an example illustrating processing in the terminal device according to the first modification of the second embodiment.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a functional block diagram of an example illustrating functions of a terminal device according to a second modification of the second embodiment.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a flowchart of an example illustrating processing in the terminal device according to the second modification of the second embodiment.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a functional block diagram of an example illustrating functions of a terminal device according to a third modification of the second embodiment.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a flowchart of an example illustrating processing in the terminal device according to the third modification of the second embodiment.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a block diagram illustrating a configuration of an example of a driving assistance system according to a third embodiment.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a functional block diagram of an example illustrating functions of a terminal device according to the third embodiment.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a flowchart of an example illustrating processing in the terminal device according to the third embodiment.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a functional block diagram of an example illustrating functions of a terminal device according to a first modification of the third embodiment.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a flowchart of an example illustrating processing in the terminal device according to the first modification of the third embodiment.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a functional block diagram of an example illustrating functions of a terminal device according to a second modification of the third embodiment.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is a flowchart of an example illustrating processing in the terminal device according to the second modification of the third embodiment.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a block diagram illustrating a schematic configuration example of a vehicle control system, which is an example of a moving body control system to which the technology according to the present disclosure is applicable.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>30</b></figref> is a diagram illustrating an example of an installation position of an imaging unit. Description of Embodiments</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0039" num="0038">Embodiments of the present disclosure will be described below in detail with reference to the drawings. In each of the following embodiments, the same parts are denoted by the same reference symbols, and a repetitive description thereof will be omitted.</p><p id="p-0040" num="0039">Hereinafter, embodiments of the present disclosure will be described in the following order.</p><p id="p-0041" num="0040">1. Technology Applicable to Each Embodiment</p><p id="p-0042" num="0041">1-1. Overall Picture of System Applicable to Each Embodiment</p><p id="p-0043" num="0042">1-2. Outline of system according to each embodiment</p><p id="p-0044" num="0043">1-3. Hardware configuration example</p><p id="p-0045" num="0044">2. First embodiment</p><p id="p-0046" num="0045">2-1. First modification</p><p id="p-0047" num="0046">2-2. Second modification</p><p id="p-0048" num="0047">3. Second embodiment</p><p id="p-0049" num="0048">3-1. First modification</p><p id="p-0050" num="0049">3-2. Second modification</p><p id="p-0051" num="0050">3-3. Third modification</p><p id="p-0052" num="0051">4. Third embodiment</p><p id="p-0053" num="0052">4-1. First modification</p><p id="p-0054" num="0053">4-2. Second modification</p><p id="p-0055" num="0054">5. Fourth embodiment</p><heading id="h-0009" level="1">1. Technology Applicable to Each Embodiment</heading><p id="p-0056" num="0055">(1-1. Overall Picture of System Applicable to Each Embodiment)</p><p id="p-0057" num="0056">First, a technology applicable to each embodiment of the present disclosure will be described. <figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram of an example schematically illustrating an overall picture of a driving assistance system related to each embodiment. In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the driving assistance system includes a server system <b>2</b> and one or more vehicles <b>10</b>, with the server system <b>2</b> and the vehicles <b>10</b> being connected to a network <b>1</b>. The network <b>1</b> is, for example, a V2X network for performing vehicle-to-vehicle (V2X) communication.</p><p id="p-0058" num="0057">The server system <b>2</b> can be actualized by using a cloud system including a plurality of computers and storage devices that are connected to each other via a network and operate in cooperation. The server system <b>2</b> is not limited thereto, and may be configured by a server device using a single computer. By machine learning, the server system <b>2</b> can generate a recognizer that performs object recognition based on sensor information.</p><p id="p-0059" num="0058">The vehicle <b>10</b> is equipped with a terminal device <b>11</b> connected to the network <b>1</b>. The terminal device <b>11</b> has a configuration as an information processing apparatus including, for example, a central processing unit (CPU), read only memory (ROM), random access memory (RAM), and various interfaces, and further includes a recognizer that performs object recognition processing based on an output of a sensor <b>12</b>. An example of the sensor <b>12</b> is a camera. Hereinafter, the sensor <b>12</b> will be described as a camera <b>12</b>. For example, the recognizer generated by the server system <b>2</b> is downloaded via the network <b>1</b> to be mounted on the terminal device <b>11</b>. Installation of the recognizer is not limited thereto, and the recognizer may be mounted on the terminal device <b>11</b> in advance.</p><p id="p-0060" num="0059">The terminal device <b>11</b> performs object recognition processing by a recognizer based on the captured image which is acquired by the camera <b>12</b>. The terminal device <b>11</b> can perform driving assistance in the vehicle <b>10</b> by the object recognition processing. Here, it is preferable to occasionally update the recognizer using new training data so as to achieve improvement of the performance of the recognition processing. In view of this, the terminal device <b>11</b> transmits image data based on the captured image acquired by the camera <b>12</b> to the server system <b>2</b> via the network <b>1</b> (step S<b>1</b> and step S<b>2</b>).</p><p id="p-0061" num="0060">The server system <b>2</b> accumulates the image data transmitted from the terminal device <b>11</b> in a storage unit <b>20</b>. The storage unit <b>20</b> can be actualized by adopting a storage device such as a hard disk drive or large-scale flash memory, and further accumulates a recognizer and parameter information for generating a recognizer that performs object recognition processing. The server system <b>2</b> uses, in a training unit <b>3</b>, each data stored in the storage unit <b>20</b> including the image data transmitted from the terminal device <b>11</b> as training data (step S<b>3</b>), and executes optimization and retraining of the recognizer (step S<b>4</b>). This optimization and retraining generates a post-retraining recognizer <b>21</b>, which is an updated recognizer (step S<b>5</b>).</p><p id="p-0062" num="0061">The server system <b>2</b> transmits the generated post-retraining recognizer <b>21</b> to the terminal device <b>11</b> via the network <b>1</b> (step S<b>6</b> and step S<b>7</b>). The terminal device <b>11</b> updates an existing recognizer by the post-retraining recognizer <b>21</b> transmitted via the network <b>1</b>. In this manner, by retraining the existing recognizer using the newly collected training data, it is possible to improve the performance of the recognition processing performed by the recognizer.</p><p id="p-0063" num="0062">In such a driving assistance system, while it is conceivable to improve the machine learning method to perform retraining on the recognizer, it will also be effective to perform retraining using new training data. In order to improve the performance of the recognizer by retraining using new training data, it is desirable to continue to collect training data (for example, image data) contributing to performance improvement.</p><p id="p-0064" num="0063">At this time, the time-series data and the image data transmitted from the terminal device <b>11</b> to the server system <b>2</b> for use in retraining are generally large in size, which might increase communication cost. In addition, the time-series data and the image data have various attributes, and in addition, the attributes might be different for various factors such as for each vehicle <b>10</b> and for each area, for example. As an example, the recognition data regarding the traffic road sign varies in each country, for example, and image data including the traffic road sign acquired in country A might not be usable as training data for recognizing the traffic road sign in country B.</p><p id="p-0065" num="0064">An object of each embodiment of the present disclosure is to enable efficient use of new training data for use in retraining.</p><p id="p-0066" num="0065">(1-2. Outline of System According to Each Embodiment)</p><p id="p-0067" num="0066">Next, an outline of a driving assistance system according to each embodiment will be described. <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a configuration of an example of a driving assistance system according to each embodiment. In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, portions deeply associated with individual embodiments and their modifications are indicated with thick frames.</p><p id="p-0068" num="0067">In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the server system <b>2</b> includes a training data application determination unit <b>22</b> in addition to the training unit <b>3</b> and the storage unit <b>20</b> described above. The storage unit <b>20</b> accumulates training data and stores a plurality of recognizers. As will be described in detail below, based on the sensor information transmitted from the terminal device <b>11</b>, the training data application determination unit <b>22</b> designates a recognizer to be retrained using the sensor information from among the recognizers stored in the storage unit <b>20</b>. The server system <b>2</b> transmits information indicating the recognizer designated by the training data application determination unit <b>22</b> to the terminal device <b>11</b>.</p><p id="p-0069" num="0068">In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the terminal device <b>11</b> includes a recognition unit <b>101</b>, a control unit <b>102</b>, an accumulation determination unit <b>103</b>, an accumulation unit <b>104</b>, a storage unit <b>105</b>, an accumulated information output unit <b>106</b>, a transmission determination unit <b>107</b>, and a communication unit <b>110</b>. The terminal device <b>11</b> can further include a training data application determination unit <b>108</b>.</p><p id="p-0070" num="0069">The recognition unit <b>101</b> includes a recognizer downloaded from the server system <b>2</b>, for example, and performs object recognition processing on the basis of sensor information including image data based on a captured image, that is, an image captured by the camera <b>12</b>.</p><p id="p-0071" num="0070">At this time, the recognition unit <b>101</b> can use RAW data as a captured image, that is, an image captured by the camera <b>12</b>. Specifically, image data captured by the camera <b>12</b> and used for visual recognition is generally subjected to image processing such as demosaic processing or compression processing on data of each color of RGB to <b>8</b> bits, for example. In contrast, recognition processing does not need such image processing in usual cases and in the case of night or distant recognition, in particular, a higher recognition rate (recognition score) can be obtained by using RAW data, which uses output of the image sensor of the camera <b>12</b> in a substantially unprocessed state. The recognition score is a value indicating the degree of recognition, and takes a range of <b>0</b> or more and <b>1</b> or less, for example. The larger the recognition score, the higher the degree of recognition.</p><p id="p-0072" num="0071">Furthermore, in particular, in consideration of recognition of a distant object, it is preferable that the resolution of the camera <b>12</b> (the resolution of the image sensor) be high.</p><p id="p-0073" num="0072">An object recognition result obtained by the recognition unit <b>101</b> is passed to the control unit <b>102</b> that performs driving control of the vehicle <b>10</b>. The control unit <b>102</b> performs automated driving control such as obstacle avoidance based on the object recognition result passed from the recognition unit <b>101</b>.</p><p id="p-0074" num="0073">Here, the sensor information includes not only image data but also metadata, which is attribute information regarding the camera <b>12</b> and regarding imaging using the camera <b>12</b>. The metadata can include, for example, information regarding camera performance such as a model number, resolution, and frame rate of the camera <b>12</b>, and information indicating image processing (white balance processing, gain adjustment processing, and the like) executed in the camera <b>12</b>. The metadata can further include information regarding the vehicle <b>10</b>, such as information indicating the vehicle type and destination of the vehicle <b>10</b>, information related to the current location of the vehicle <b>10</b>, and the traveling speed of the vehicle <b>10</b>.</p><p id="p-0075" num="0074">The accumulation determination unit <b>103</b> determines whether to accumulate sensor information output from the camera <b>12</b> in the storage unit <b>105</b> being a storage device, for example, by the accumulation unit <b>104</b>. Although a specific example will be described below, the accumulation determination unit <b>103</b> determines whether to accumulate the sensor information in the storage unit <b>105</b> by the accumulation unit <b>104</b> based on image data or metadata included in the sensor information, for example. The accumulated information output unit <b>106</b> outputs the sensor information accumulated in the storage unit <b>105</b> to the outside. The accumulated information output unit <b>106</b> may output the sensor information to the outside by wired communication or can output the sensor information to the outside by wireless communication such as Wi-Fi (registered trademark) or Bluetooth (registered trademark).</p><p id="p-0076" num="0075">The transmission determination unit <b>107</b> determines whether to transmit the sensor information output from the camera <b>12</b> to the server system <b>2</b>. For example, the transmission determination unit <b>107</b> determines whether to transmit the sensor information to the server system <b>2</b> based on the recognition result by the recognition unit <b>101</b> and the sensor information. Having determined to transmit the sensor information to the server system <b>2</b>, the transmission determination unit <b>107</b> adds information indicating the recognizer, which has been transmitted from the training data application determination unit <b>22</b> in the server system <b>2</b>, to the sensor information to be transmitted. The sensor information determined not to be transmitted to the server system <b>2</b> by the transmission determination unit <b>107</b> will be accumulated in the storage unit <b>105</b> by the accumulation unit <b>104</b>.</p><p id="p-0077" num="0076">According to this driving assistance system, the transmission determination unit <b>107</b> determines whether to transmit the sensor information in the terminal device <b>11</b>, making it possible to suppress the communication cost related to the transmission of sensor information. Furthermore, the server system <b>2</b> causes the training data application determination unit <b>22</b> to determine the recognizer to which the sensor information transmitted from the terminal device <b>11</b> is to be applied, making it possible to apply the sensor information to a recognizer that performs recognition processing different from that of the recognizer included in the terminal device <b>11</b>, leading to achievement of efficient use of sensor information.</p><p id="p-0078" num="0077">(1-3. Hardware Configuration Example)</p><p id="p-0079" num="0078">Next, an example of a hardware configuration of the server system <b>2</b> and the terminal device <b>11</b> applicable to each embodiment will be described. <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating an example of a hardware configuration of the terminal device <b>11</b> applicable to each embodiment. In <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the terminal device <b>11</b> includes a CPU <b>1001</b>, ROM <b>1002</b>, RAM <b>1003</b>, interfaces (I/F) <b>1004</b> and <b>1005</b>, a communication interface <b>1006</b>, and a GNSS receiver <b>1007</b>, which are communicably connected to each other via a bus <b>1030</b>.</p><p id="p-0080" num="0079">The CPU <b>1001</b> controls the entire operation of the terminal device <b>11</b> by using the RAM <b>1003</b> as work memory according to a program stored in advance in the ROM <b>1002</b>. Note that the ROM <b>1002</b> is formed to have stored data and programs rewritable under the control of the CPU <b>1001</b>, and can store and update, for example, the program constituting the recognizer downloaded from the server system <b>2</b> described above. Memory devices are not limited thereto, and the RAM <b>1003</b> may include a nonvolatile region, and a program constituting the recognizer may be stored in the nonvolatile region of the RAM <b>1003</b>.</p><p id="p-0081" num="0080">The interface <b>1004</b> is an interface for communicating with the camera <b>12</b>. Image data generated from a captured image, that is, an image captured by the camera <b>12</b> and sensor information including the image data are input from the interface <b>1004</b> to the terminal device <b>11</b>. Furthermore, the CPU <b>1001</b> can send an instruction such as imaging to the camera <b>12</b> via the interface <b>1004</b>.</p><p id="p-0082" num="0081">The interface <b>1005</b> is an interface for a control system of the vehicle <b>10</b> on which the terminal device <b>11</b> is mounted. Information such as an object recognition result by the recognizer is passed to the control system of the vehicle <b>10</b> via the interface <b>1005</b>. The interface <b>1005</b> controls communication with the network <b>1</b> according to an instruction of the CPU <b>1001</b>.</p><p id="p-0083" num="0082">The GNSS receiver <b>1007</b> receives a signal based on a global navigation satellite system (GNSS) and acquires positional information indicating a current position. Furthermore, the GNSS receiver <b>1007</b> can acquire the current altitude and time together with the current position. Incidentally, when the vehicle <b>10</b> equipped with the terminal device <b>11</b> separately includes a positional information acquisition means, by acquiring the positional information from the positional information acquisition means, it is possible to omit the GNSS receiver <b>1007</b> in the terminal device <b>11</b>.</p><p id="p-0084" num="0083">In this manner, the terminal device <b>11</b> includes the CPU <b>1001</b>, the ROM <b>1002</b>, the RAM <b>1003</b>, and various interfaces, and functions as a computer (information processing apparatus).</p><p id="p-0085" num="0084">Note that the recognition unit <b>101</b>, the accumulation determination unit <b>103</b>, the accumulation unit <b>104</b>, the accumulated information output unit <b>106</b>, the transmission determination unit <b>107</b>, and the communication unit <b>110</b> included in the terminal device <b>11</b> described above are actualized by an information processing program operating on the CPU <b>1001</b>. Not limited to this, for example, some or all of the accumulation determination unit <b>103</b>, the accumulation unit <b>104</b>, the accumulated information output unit <b>106</b>, the transmission determination unit <b>107</b>, and the communication unit <b>110</b> may be configured by hardware circuits that operate in cooperation with each other.</p><p id="p-0086" num="0085"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram illustrating an example of a hardware configuration of the server system <b>2</b> applicable to the embodiment. In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the server system <b>2</b> is illustrated as a single computer (information processing apparatus). In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the server system <b>2</b> includes a CPU <b>2000</b>, ROM <b>2001</b>, RAM <b>2002</b>, a storage device <b>2004</b>, and a communication interface <b>2005</b>, which are communicably connected to each other via a bus <b>2010</b>.</p><p id="p-0087" num="0086">The CPU <b>2000</b> controls the entire operation of the server system <b>2</b> using the RAM <b>2002</b> as work memory according to a program stored in advance in the ROM <b>2001</b> or the storage device <b>2004</b>. Under the CPU <b>2000</b>, the program and each parameter constituting the recognizer generated by machine learning according to the program are stored in the storage device <b>2004</b>, for example.</p><p id="p-0088" num="0087">The communication interface <b>2005</b> is an interface that controls communication with the network <b>1</b>.</p><p id="p-0089" num="0088">Note that the training unit <b>3</b> and the training data application determination unit <b>22</b> included in the server system <b>2</b> described above are actualized by an information processing program operating on the CPU <b>2000</b>. Not limited to this, some or all of the training unit <b>3</b> and the training data application determination unit <b>22</b> may be configured by hardware circuits that operate in cooperation with each other.</p><heading id="h-0010" level="1">2. First Embodiment</heading><p id="p-0090" num="0089">Next, a first embodiment of the present disclosure will be described. In each embodiment of the present disclosure, it is possible to designate a recognizer to which the sensor information acquired in the terminal device <b>11</b> is to be applied as training data for retraining, from among a plurality of recognizers. In the first embodiment, the designation of the recognizer is performed in the server system <b>2</b>. <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram illustrating a configuration of an example of a driving assistance system according to the first embodiment. As illustrated as a colored area in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the first embodiment uses a training data application determination unit <b>22</b><i>a </i>on the server system <b>2</b> side without using the training data application determination unit <b>108</b> on the terminal device <b>11</b> side illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0091" num="0090">In the first embodiment, regarding sensor information transmitted from the terminal device <b>11</b>, a scene at the time of capturing image data included in the sensor information is determined based on a predetermined rule or learning. Subsequently, a recognizer to which the sensor information is to be applied as training data is designated for each determined scene, for example.</p><p id="p-0092" num="0091">Here, the scene indicates a situation when sensing, that is, imaging by the camera <b>12</b> is performed, and includes a place and environmental conditions regarding the imaging. More specifically, the scene information indicating the scene may include the following information.</p><p id="p-0093" num="0092">Information acquired from the outside of the camera <b>12</b>, such as a current position acquired by the GNSS receiver <b>1007</b> or the like, information indicating a current time, and a system status of an in-vehicle system mounted on the vehicle <b>10</b>.</p><p id="p-0094" num="0093">Sensor internal information such as exposure time and white balance processing at the time of imaging stored in the camera <b>12</b>.</p><p id="p-0095" num="0094">Information estimated based on a predetermined rule from information obtained by sensing (for example, image data based on a captured image) or information estimated by learning.</p><p id="p-0096" num="0095">The training data application determination unit <b>22</b><i>a </i>estimates an applicable recognizer by a rule-based method or a database search based on the scene information acquired as described above. The recognizer is not limited thereto, and the training data application determination unit <b>22</b><i>a </i>can directly designate an applicable recognizer based on the scene information by learning.</p><p id="p-0097" num="0096"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a functional block diagram of an example illustrating functions of the server system <b>2</b> according to the first embodiment. In <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the server system <b>2</b> includes a communication unit <b>200</b>, a training data application determination unit <b>22</b><i>aa, </i>a training data accumulation unit <b>201</b>, a training unit <b>202</b>, and a learning result utilization unit <b>203</b>. In addition, the training data application determination unit <b>22</b><i>a </i>includes a metadata analysis unit <b>221</b> and a training data determination unit <b>222</b>. The communication unit <b>200</b>, the training data application determination unit <b>22</b><i>a </i>(the metadata analysis unit <b>221</b> and the training data determination unit <b>222</b>), the training data accumulation unit <b>201</b>, the training unit <b>202</b>, and the learning result utilization unit <b>203</b> are actualized by an information processing program operating on the CPU <b>2000</b>.</p><p id="p-0098" num="0097">Here, the server system <b>2</b> includes a plurality of recognizers <b>210</b><sub>1</sub>, <b>210</b><sub>2</sub>, <b>210</b><sub>3</sub>, <b>210</b><sub>4</sub>, . . . . Note that, in the following description, in a case where it is not necessary to distinguish the recognizers <b>2101</b>, <b>2102</b>, <b>2103</b>, <b>2104</b>, . . . , these recognizers are represented by the recognizer <b>210</b>. The training data accumulation unit <b>201</b> accumulates training data for generating the plurality of recognizers <b>210</b>, with the accumulation performed for each of the recognizers <b>210</b>.</p><p id="p-0099" num="0098">The communication unit <b>200</b> controls the communication interface <b>2005</b> to receive sensor information transmitted from the terminal device <b>11</b>, for example. The metadata analysis unit <b>221</b> extracts the metadata from the sensor information, and analyzes the extracted metadata. The training data determination unit <b>222</b> designates the recognizer <b>210</b> to which the sensor information is applicable among the plurality of recognizers <b>210</b> based on the analysis result of the metadata.</p><p id="p-0100" num="0099">At this time, the training data determination unit <b>222</b> can designate a recognizer different from the recognizer included in the terminal device <b>11</b> that has transmitted the sensor information, as the recognizer <b>210</b> to which the sensor information is applicable. For example, the training data determination unit <b>222</b> determines which of the above-described scene information the sensor information corresponds to, and designates the recognizer <b>210</b> to which the sensor information is applicable according to the scene information to which the sensor information corresponds.</p><p id="p-0101" num="0100">The training data determination unit <b>222</b> adds information indicating the designated recognizer <b>210</b> to the sensor information, for example, and accumulates the obtained sensor information in the training data accumulation unit <b>201</b>.</p><p id="p-0102" num="0101">The training unit <b>202</b> retrains the recognizer <b>210</b> by using the training data accumulated for each recognizer <b>210</b> designated in the training data accumulation unit <b>201</b>, and then updates the recognizer <b>210</b>. The learning result utilization unit <b>203</b> performs utilization processing on the recognizer <b>210</b> retrained and updated by the training unit <b>202</b>. For example, the learning result utilization unit <b>203</b> transmits the updated recognizer <b>210</b> to the corresponding terminal device <b>11</b>.</p><p id="p-0103" num="0102"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of an example illustrating processing in the server system <b>2</b> according to the first embodiment. In the first step S<b>100</b>, the server system <b>2</b> performs communication processing by the communication unit <b>200</b>, and receives the sensor information transmitted from the terminal device <b>11</b> via the network <b>1</b>. The processing in step S<b>100</b> includes processing of receiving each piece of sensor information transmitted from each of the plurality of terminal devices <b>11</b>.</p><p id="p-0104" num="0103">In the next step S<b>101</b>, the server system <b>2</b> causes the metadata analysis unit <b>221</b> to acquire image data and metadata from each piece of sensor information acquired in the communication processing in step S<b>100</b>. In the next step S<b>102</b>, the server system <b>2</b> causes the metadata analysis unit <b>221</b> to execute analysis processing of each piece of metadata acquired in step S<b>101</b>.</p><p id="p-0105" num="0104">The next step S<b>103</b><i>st </i>represents a start of loop processing indicating execution of processing of step S<b>103</b><i>st </i>to step S<b>103</b><i>ed </i>for all the sensor information acquired in the communication processing of step S<b>100</b>. The next step S<b>104</b><i>st </i>represents a start of loop processing indicating execution of processing of step S<b>104</b><i>st </i>to step S<b>104</b><i>ed </i>for all the application targets of each piece of sensor information, that is, for each of the recognizers <b>210</b>.</p><p id="p-0106" num="0105">In step S<b>105</b>, the server system <b>2</b> causes the training data determination unit <b>222</b> to execute metadata application determination processing. That is, in step S<b>105</b>, the training data determination unit <b>222</b> examines, based on the metadata, whether one piece of sensor information (referred to as target sensor information) among the pieces of sensor information acquired in the communication processing in step S<b>100</b> is applicable to the target recognizer <b>210</b> among the individual recognizers <b>210</b>.</p><p id="p-0107" num="0106">In the next step S<b>106</b>, when the training data determination unit <b>222</b> has determined that the target sensor information is applicable to the target recognizer <b>210</b> (step S<b>106</b>, &#x201c;Yes&#x201d;), the processing proceeds to step S<b>107</b>. In step S<b>107</b>, the server system <b>2</b> causes the training data accumulation unit <b>201</b> to execute accumulation processing of the training data of the application target. That is, in step S<b>107</b>, the training data accumulation unit <b>201</b> accumulates the target sensor information as training data for the target recognizer <b>210</b> determined to be applicable in step S<b>106</b>.</p><p id="p-0108" num="0107">After the processing of step S<b>107</b>, the server system <b>2</b> returns the processing from step S<b>104</b><i>ed </i>to step S<b>104</b><i>st, </i>and executes the processing of steps S<b>105</b> and S<b>106</b> on the next recognizer <b>210</b> set as the target recognizer <b>210</b> among the individual recognizers <b>210</b>.</p><p id="p-0109" num="0108">In contrast, when it is determined in step S<b>106</b> that the target sensor information is not applicable to the target recognizer <b>210</b> (step S<b>106</b>, &#x201c;No&#x201d;), the processing returns from step S<b>104</b><i>ed </i>to step S<b>104</b><i>st. </i></p><p id="p-0110" num="0109">After completion of the processing of steps S<b>104</b><i>st </i>to S<b>104</b><i>ed </i>for each of the recognizers <b>210</b> regarding the target sensor information, the server system <b>2</b> returns the processing from step S<b>103</b><i>ed </i>to step S<b>103</b><i>st, </i>and iterates the processing of steps S<b>103</b><i>st </i>to S<b>103</b><i>ed </i>on the next sensor information as target sensor information among the pieces of sensor information acquired in the communication processing of step S<b>100</b>.</p><p id="p-0111" num="0110">After completion of the processing of steps S<b>103</b><i>st </i>to S<b>103</b><i>ed </i>for each piece of sensor information, the server system <b>2</b> proceeds to the processing of step S<b>108</b>. In step S<b>108</b>, the server system <b>2</b> causes the training unit <b>202</b> to perform retraining processing on the corresponding recognizer <b>210</b> by using each piece of sensor information accumulated in the training data accumulation unit <b>201</b> by the above-described processing, as training data. After completion of the processing of step S<b>108</b>, the server system <b>2</b> proceeds to the processing of step S<b>109</b>, and causes the learning result utilization unit <b>203</b> to execute the utilization processing of each recognizer <b>210</b> retrained in step S<b>108</b>.</p><p id="p-0112" num="0111">In this manner, in the first embodiment, each piece of sensor information transmitted from each terminal device <b>11</b> is applied as training data for the recognizer <b>210</b> determined applicable based on the metadata. Therefore, each piece of sensor information transmitted from each of the terminal devices <b>11</b> is applicable as the training data to a recognition machine different from the recognizer used for the sensor information by the terminal device <b>11</b> being the transmission source, making is possible to utilize the sensor information as training data with higher efficiency.</p><p id="p-0113" num="0112">(2-1. First Modification)</p><p id="p-0114" num="0113">Next, a first modification of the first embodiment will be described. The first modification of the first embodiment uses a configuration in which designation of the recognizer is performed in the server system <b>2</b>, in which the server system <b>2</b> designates a recognizer <b>210</b> capable of converting the sensor information transmitted from the terminal device <b>11</b> to applicable information from among the plurality of recognizers <b>210</b> included in the server system <b>2</b> as the recognizer to which the sensor information is to be applied as training data. In the first modification of the first embodiment, the configuration of the driving assistance system according to the first embodiment described with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref> is applicable.</p><p id="p-0115" num="0114"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a functional block diagram of an example illustrating functions of the server system <b>2</b> according to the first modification of the first embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the first modification of the first embodiment uses a training data application determination unit <b>22</b><i>b </i>on the server system <b>2</b> side without using the training data application determination unit <b>108</b> on the terminal device <b>11</b> side illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Furthermore, unlike the configuration of the training data application determination unit <b>22</b><i>a </i>of the first embodiment described above, the training data application determination unit <b>22</b><i>b </i>includes a domain analysis unit <b>230</b>, a domain evaluation unit <b>231</b>, and a training data determination unit <b>232</b>.</p><p id="p-0116" num="0115">In the first modification of the first embodiment, the server system <b>2</b> converts the domain of the sensor information using the applicability obtained by domain adaptation. Here, the domain indicates a region (category) for specifying the target, such as an area (country and the like) and an environment (day/night, highway/local road, and so on). The domain adaptation is a technique of converting a region (object, country, task, . . . ) to be a target of a recognizer. As an example, domain adaptation allows a recognizer trained with Japanese data to be recognizable with US data. In this case, two domains of Japan and the US are converted.</p><p id="p-0117" num="0116">That is, the training data application determination unit <b>22</b><i>b </i>according to the first modification of the first embodiment sets a region to be a target of the recognizer <b>210</b> to be adapted to a combination rule of domains capable of domain adaptation. As a specific example, assumed domains are N domains, namely, domains #1, #2, #3, . . . , #N. In the N domains, a combination of transformable domains represented by a directed graph is assumed to exist, for example, as follows.</p><p id="p-0118" num="0117">.Domain #1-&#x3e;Domain #2, Domain #5, Domain #7, . . .</p><p id="p-0119" num="0118">.Domain #2-&#x3e;Domain #1, Domain #3, . . .</p><p id="p-0120" num="0119">The number of steps indicating the number of stages necessary for conversion into domain #j when domain #i has been input is defined as S_{ij}. In addition, an evaluation value per hop count is defined as &#x3b1;(&#x3c;1), and an evaluation value when domain #i is converted to domain #j is defined as &#x3b2;{ij}(&#x3c;1).</p><p id="p-0121" num="0120">The evaluation value Ev from domain #a to domain #b of the input sensor information is obtained by the following Formula (1).</p><p id="p-0122" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Ev=&#x3b1;</i><sup>(s_{ij})</sup>+&#x3b2;_{<i>ij</i>}&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0123" num="0121">A threshold is set for the evaluation value Ev, and the recognizer <b>210</b> having the evaluation value Ev higher than the threshold is set as the applicable recognizer <b>210</b>.</p><p id="p-0124" num="0122">The domain analysis unit <b>230</b> analyzes the sensor information received by the communication unit <b>200</b> from the terminal device <b>11</b>, and obtains a domain related to the sensor information. The domain analysis unit <b>230</b> may obtain the domain from the image data included in the sensor information or may obtain the domain from the metadata. Based on the domain obtained by the domain analysis unit <b>230</b>, the domain evaluation unit <b>231</b> calculates the evaluation value Ev for each recognizer <b>210</b> included in the server system <b>2</b>, for example, by the above-described Formula (1).</p><p id="p-0125" num="0123">The training data determination unit <b>232</b> compares each evaluation value Ev calculated by the domain evaluation unit <b>231</b> with a threshold, and designates the recognizer <b>210</b> having the evaluation value Ev higher than the threshold as the recognizer <b>210</b> to which the sensor information is applicable.</p><p id="p-0126" num="0124"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart of an example illustrating processing in the server system <b>2</b> according to the first modification of the first embodiment. In the first step S<b>200</b>, the server system <b>2</b> performs communication processing by the communication unit <b>200</b>, and receives the sensor information transmitted from the terminal device <b>11</b> via the network <b>1</b>. The processing in step S<b>200</b> includes processing of receiving each piece of sensor information transmitted from each of the plurality of terminal devices <b>11</b>.</p><p id="p-0127" num="0125">In the next step S<b>201</b>, the domain analysis unit <b>230</b> in the server system <b>2</b> acquires the sensor information received in the communication processing in step S<b>200</b>. In the next step S<b>202</b>, the domain analysis unit <b>230</b> analyzes the sensor information acquired in step S<b>201</b> to obtain a domain related to the sensor information.</p><p id="p-0128" num="0126">The next step S<b>203</b><i>st </i>represents a start of loop processing indicating execution of processing of step S<b>203</b><i>st </i>to step S<b>203</b><i>ed </i>for all the sensor information acquired in the communication processing of step S<b>200</b>. The next step S<b>204</b><i>st </i>represents a start of loop processing indicating execution of processing of step S<b>204</b><i>st </i>to step S<b>204</b><i>ed </i>for all the application targets of each piece of sensor information, that is, for each of the domain conversion targets.</p><p id="p-0129" num="0127">In step S<b>205</b>, the domain analysis unit <b>230</b> in the server system <b>2</b> calculates a conversion path from the domain obtained from the target sensor information for the target domain as the conversion target. In the next step S<b>206</b>, in the server system <b>2</b>, the domain evaluation unit <b>231</b> calculates the evaluation value Ev for the conversion path of the domain calculated in step S<b>205</b> according to the above-described Formula (1).</p><p id="p-0130" num="0128">In the next step S<b>207</b>, the training data determination unit <b>232</b> determines whether the evaluation value Ev calculated in step S<b>206</b> exceeds a threshold.</p><p id="p-0131" num="0129">When the training data determination unit <b>232</b> has determined in step S<b>207</b> that the evaluation value Ev exceeds the threshold (step S<b>207</b>, &#x201c;Yes&#x201d;), the processing proceeds to step S<b>208</b>. In step S<b>208</b>, the training data accumulation unit <b>201</b> in the server system <b>2</b> executes accumulation processing of the training data of the application target. That is, in step S<b>208</b>, the training data accumulation unit <b>201</b> accumulates the target sensor information as training data for the target recognizer <b>210</b> determined to be applicable in step S<b>206</b>.</p><p id="p-0132" num="0130">After the processing of step S<b>208</b>, the server system <b>2</b> returns the processing from step S<b>204</b><i>ed </i>to step S<b>204</b><i>st, </i>and executes the processing of steps S<b>205</b> to S<b>207</b> with the next domain among the individual domains as the target domain of the conversion target.</p><p id="p-0133" num="0131">In contrast, when the training data determination unit <b>232</b> determines in step S<b>207</b> that the evaluation value Ev is a threshold or less (step S<b>207</b>, &#x201c;No&#x201d;), the processing returns from step S<b>204</b><i>ed </i>to step S<b>204</b><i>st. </i></p><p id="p-0134" num="0132">After completion of the processing of steps S<b>204</b><i>st </i>to S<b>204</b><i>ed </i>for each of domains regarding the target sensor information, the server system <b>2</b> returns the processing from step S<b>203</b><i>ed </i>to step S<b>203</b><i>st, </i>and iterates the processing of steps S<b>203</b><i>st </i>to S<b>203</b><i>ed </i>on the next sensor information as target sensor information among the pieces of sensor information acquired in the communication processing of step S<b>200</b>.</p><p id="p-0135" num="0133">After completion of the processing of steps S<b>203</b><i>st </i>to S<b>203</b><i>ed </i>for each piece of sensor information, the server system <b>2</b> proceeds to the processing of step S<b>209</b>. In step S<b>209</b>, the server system <b>2</b> causes the training unit <b>202</b> to perform retraining processing on the corresponding recognizer <b>210</b> by using each piece of sensor information accumulated in the training data accumulation unit <b>201</b> by the above-described processing, as training data. After completion of the processing of step S<b>208</b>, the server system <b>2</b> proceeds to the processing of step S<b>210</b>, and causes the learning result utilization unit <b>203</b> to execute the utilization processing of each recognizer <b>210</b> retrained in step S<b>209</b>.</p><p id="p-0136" num="0134">In this manner, according to the first modification of the first embodiment, the sensor information transmitted from the terminal device <b>11</b> is applicable to a domain different from the domain at the acquisition of the sensor information. This makes it possible to use the sensor information with higher efficiency.</p><p id="p-0137" num="0135">(2-2. Second Modification)</p><p id="p-0138" num="0136">Next, a second modification of the first embodiment will be described. The second modification of the first embodiment uses a configuration in which designation of the recognizer is performed in the server system <b>2</b>, in which the server system <b>2</b> designates, from among the plurality of recognizers <b>210</b> included in the server system <b>2</b>, a recognizer based on an object included in image data included in the sensor information transmitted from the terminal device <b>11</b>, as the recognizer to which the sensor information is to be applied as training data. In the second modification of the first embodiment, the configuration of the driving assistance system according to the first embodiment described with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref> is applicable.</p><p id="p-0139" num="0137"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a functional block diagram of an example illustrating functions of the server system <b>2</b> according to the second modification of the first embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the second modification of the first embodiment uses a training data application determination unit <b>22</b><i>c </i>on the server system <b>2</b> side without using the training data application determination unit <b>108</b> on the terminal device <b>11</b> side illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Furthermore, unlike the configuration of the training data application determination unit <b>22</b><i>a </i>of the first embodiment described above, the training data application determination unit <b>22</b><i>c </i>includes a sensing analysis unit <b>240</b> and a training data determination unit <b>241</b>.</p><p id="p-0140" num="0138">In <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the sensing analysis unit <b>240</b> analyzes image data included in the sensor information received by the communication unit <b>200</b>. The training data determination unit <b>241</b> designates the recognizer <b>210</b> to which the sensor information is applicable among the plurality of recognizers <b>210</b> based on the information analyzed by the sensing analysis unit <b>240</b>.</p><p id="p-0141" num="0139">This point will be described more specifically. The second modification of the first embodiment analyzes image data included in sensor information transmitted from the terminal device <b>11</b> so as to obtain an object or an object class included in the image data. Subsequently, for each obtained object or object class, a database is used to search whether the sensor information is applicable as training data. It is possible to apply, as a constituent of the database, a pair of an application candidate recognizer and a target object. In this case, the constituent of the database is not limited to a positive list that lists applicable objects but may be a negative list (list of inapplicable objects) or a combination of a positive list and a negative list.</p><p id="p-0142" num="0140">Examples of the database for the recognizer applicable to the second modification of the first embodiment include the following.</p><p id="p-0143" num="0141">Sign recognizer: Negative list of signs in other countries when used in Japan</p><p id="p-0144" num="0142">Pedestrian detector: positive list of pedestrians when used in Japan</p><p id="p-0145" num="0143"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a schematic diagram illustrating an example of an image <b>50</b> based on image data included in sensor information. The image <b>50</b> illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref> includes a pedestrian <b>51</b>, a traffic road sign <b>52</b>, and a vehicle <b>53</b> as objects.</p><p id="p-0146" num="0144">The sensing analysis unit <b>240</b> analyzes the image <b>50</b> to detect each object (the pedestrian <b>51</b>, the traffic road sign <b>52</b> and the vehicle <b>53</b>). Here, a case where the detected pedestrian <b>51</b> and the traffic road sign <b>52</b> are those of the US as targets for the recognizer used in Japan will be discussed. Since a foreign sign is present in the image <b>50</b> based on the analysis result of the sensing analysis unit <b>240</b>, the training data determination unit <b>241</b> unselects the sign recognizer as a recognizer to be applied to the sensor information. On the other hand, since a pedestrian is present in the image <b>50</b>, the pedestrian detector is selected as a recognizer to be applied to the sensor information.</p><p id="p-0147" num="0145"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart of an example illustrating processing in the server system <b>2</b> according to the second modification of the first embodiment. In the first step S<b>300</b>, the server system <b>2</b> performs communication processing by the communication unit <b>200</b>, and receives the sensor information transmitted from the terminal device <b>11</b> via the network <b>1</b>. The processing in step S<b>300</b> includes processing of receiving each piece of sensor information transmitted from each of the plurality of terminal devices <b>11</b>.</p><p id="p-0148" num="0146">In the next step S<b>301</b>, the sensing analysis unit <b>240</b> in the server system <b>2</b> acquires the sensor information received in the communication processing in step S<b>300</b>. In the next step S<b>302</b>, the sensing analysis unit <b>240</b> analyzes the sensor information acquired in step S<b>301</b> to obtain an object or an object class included in the image data included in the sensor information. Hereinafter, an object will be described as an example among objects and object classes analyzed from image data.</p><p id="p-0149" num="0147">The next step S<b>303</b><i>st </i>represents a start of loop processing indicating execution of processing of step S<b>303</b><i>st </i>to step S<b>303</b><i>ed </i>for all the sensor information acquired in the communication processing of step S<b>300</b>. The next step S<b>304</b><i>st </i>represents a start of loop processing indicating execution of processing of step S<b>304</b><i>st </i>to step S<b>304</b><i>ed </i>for all the application targets of each piece of sensor information, that is, for each of the recognizers <b>210</b>.</p><p id="p-0150" num="0148">In step S<b>305</b>, in the server system <b>2</b>, based on each object included in the image data included in the target sensor information in step S<b>302</b>, the training data determination unit <b>241</b> examines whether the sensor information is applicable to the target recognizer <b>210</b> among the individual recognizers <b>210</b>.</p><p id="p-0151" num="0149">When the training data determination unit <b>241</b> has determined in step S<b>306</b> that the target sensor information is applicable to the target recognizer <b>210</b> (step S<b>306</b>, &#x201c;Yes&#x201d;), the processing proceeds to step S<b>307</b>. In step S<b>307</b>, the training data accumulation unit <b>201</b> in the server system <b>2</b> executes accumulation processing of the training data of the application target. That is, in step S<b>307</b>, the training data accumulation unit <b>201</b> accumulates the target sensor information as training data for the target recognizer <b>210</b> determined to be applicable in step S<b>306</b>.</p><p id="p-0152" num="0150">After the processing of step S<b>307</b>, the server system <b>2</b> returns the processing from step S<b>304</b><i>ed </i>to step S<b>304</b><i>st, </i>and the next recognizer <b>210</b> among the individual recognizers <b>210</b> is set as the target recognizer <b>210</b>, and the processing of steps S<b>305</b> and S<b>306</b> is executed.</p><p id="p-0153" num="0151">In contrast, when the training data determination unit <b>232</b> has determined in step S<b>306</b> that the target sensor information is not applicable to the target recognizer <b>210</b> (step S<b>306</b>, &#x201c;No&#x201d;), the processing returns from step S<b>304</b><i>ed </i>to step S<b>304</b><i>st. </i></p><p id="p-0154" num="0152">After completion of the processing of steps S<b>304</b><i>st </i>to S<b>304</b><i>ed </i>for each of the recognizers <b>210</b> regarding the target sensor information, the server system <b>2</b> returns the processing from step S<b>303</b><i>ed </i>to step S<b>303</b><i>st, </i>and iterates the processing of steps S<b>303</b><i>st </i>to S<b>303</b><i>ed </i>on the next sensor information as target sensor information among the pieces of sensor information acquired in the communication processing of step S<b>300</b>.</p><p id="p-0155" num="0153">After completion of the processing of steps S<b>303</b><i>st </i>to S<b>303</b><i>ed </i>for each piece of sensor information, the server system <b>2</b> proceeds to the processing of step S<b>308</b>. In step S<b>308</b>, the server system <b>2</b> causes the training unit <b>202</b> to perform retraining processing on the corresponding recognizer <b>210</b> by using each piece of sensor information accumulated in the training data accumulation unit <b>201</b> by the above-described processing, as training data. After completion of the processing of step S<b>308</b>, the server system <b>2</b> proceeds to the processing of step S<b>309</b>, and causes the learning result utilization unit <b>203</b> to execute the utilization processing of each recognizer <b>210</b> retrained in step S<b>308</b>.</p><p id="p-0156" num="0154">In this manner, in the second modification of the first embodiment, the recognizer <b>210</b> to which the sensor information is applicable is selected based on the object or the object class included in the image data included in the sensor information transmitted from the terminal device <b>11</b>. This makes it possible to use the sensor information with higher efficiency.</p><heading id="h-0011" level="1">3. Second Embodiment</heading><p id="p-0157" num="0155">Next, a second embodiment of the present disclosure will be described. In the first embodiment and its modifications described above, the server system <b>2</b> designates the recognizer <b>210</b> to which the sensor information transmitted from the terminal device <b>11</b> is to be applied. In contrast, in the second embodiment, the terminal device <b>11</b> designates the recognizer <b>210</b> to which the sensor information to be transmitted is to be applied. Furthermore, in the second embodiment, the terminal device <b>11</b> determines whether to transmit acquired sensor information to the server system <b>2</b>.</p><p id="p-0158" num="0156"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a block diagram illustrating a configuration of an example of a driving assistance system according to the second embodiment. The driving assistance system illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref> uses the training data application determination unit <b>108</b> illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> in the terminal device <b>11</b>. In this case, for example, the training data application determination unit <b>22</b> on the server system <b>2</b> side transmits information indicating the recognizer <b>210</b> included in the server system <b>2</b> to the training data application determination unit <b>108</b> on the terminal device <b>11</b> side. In addition, in the second embodiment and its modifications described below, a transmission determination unit <b>107</b><i>a </i>plays a central role as illustrated as a colored area in the figure.</p><p id="p-0159" num="0157">For example, similarly to the first embodiment described above, the training data application determination unit <b>108</b> determines to which of the recognizers <b>210</b> included in the server system <b>2</b> the sensor information is to be applied based on the metadata included in the sensor information acquired from the camera <b>12</b>. The transmission determination unit <b>107</b><i>a </i>adds information indicating the recognizer <b>210</b> to which the sensor information is determined to be applied by the training data application determination unit <b>108</b> to the sensor information to be transmitted to the server system <b>2</b>, and transmits the obtained sensor information to the server system <b>2</b>. At this time, the transmission determination unit <b>107</b><i>a </i>determines whether to transmit the sensor information to the server system <b>2</b> based on metadata included in the sensor information, for example. This makes it possible to achieve efficient use of the sensor information as well as suppression of the communication cost for transmitting the sensor information.</p><p id="p-0160" num="0158"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a functional block diagram of an example illustrating functions of the terminal device <b>11</b> according to the second embodiment. In <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the terminal device <b>11</b> includes an imaging unit <b>300</b>, a recognition unit <b>101</b>, a recognition result utilization unit <b>301</b>, a training data application determination unit <b>108</b><i>a, </i>a transmission determination unit <b>107</b><i>a, </i>and a communication unit <b>110</b>. The imaging unit <b>300</b>, the recognition unit <b>101</b>, the training data application determination unit <b>108</b><i>a, </i>the transmission determination unit <b>107</b><i>a, </i>and the communication unit <b>110</b> are actualized by an information processing program operating on the CPU <b>1001</b>.</p><p id="p-0161" num="0159">The imaging unit <b>300</b> controls the camera <b>12</b> to acquire image data based on a captured image and also acquires metadata related to imaging. The imaging unit <b>300</b> outputs the acquired image data and metadata, as sensor information. The recognition unit <b>101</b> executes the object recognition processing on the image data included in the sensor information output from the imaging unit <b>300</b> by using the recognizer <b>210</b> acquired from the server system <b>2</b> via the network <b>1</b>, for example. The recognition result utilization unit <b>301</b> is included in a control system of the vehicle <b>10</b> equipped with the terminal device <b>11</b>, and performs control such as obstacle avoidance according to a result of the object recognition processing, for example.</p><p id="p-0162" num="0160">A metadata analysis unit <b>250</b> and a training data determination unit <b>222</b> are functionally equivalent to the metadata analysis unit <b>221</b> and the training data determination unit <b>222</b> described in the first embodiment, respectively, for example. For example, the metadata analysis unit <b>250</b> extracts metadata from the sensor information, and analyzes the extracted metadata. Based on the analysis result on the metadata, the training data determination unit <b>251</b> designates the recognizer <b>210</b> to which the sensor information is applicable among the plurality of recognizers <b>210</b> included in the server system <b>2</b>.</p><p id="p-0163" num="0161">The transmission determination unit <b>107</b><i>a </i>determines whether to transmit the sensor information acquired by the imaging unit <b>300</b> to the server system <b>2</b>. For example, the transmission determination unit <b>107</b><i>a </i>determines whether to transmit the sensor information based on the sensor information. Transmission determination is not limited thereto, and the transmission determination unit <b>107</b><i>a </i>can determine whether to transmit the sensor information according to a recognition result of the recognition unit <b>101</b> based on the sensor information.</p><p id="p-0164" num="0162">The communication unit <b>110</b> transmits the sensor information determined to be transmitted by the transmission determination unit <b>107</b><i>a, </i>to the server system <b>2</b> via the network <b>1</b>.</p><p id="p-0165" num="0163"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flowchart of an example illustrating processing in the terminal device <b>11</b> according to the second embodiment. In the first step S<b>400</b>, the terminal device <b>11</b> performs imaging processing by the imaging unit <b>300</b>, and receives, from the camera <b>12</b>, sensor information including image data based on the captured image and metadata related to imaging performed by the camera <b>12</b>.</p><p id="p-0166" num="0164">In the next step S<b>401</b>, the terminal device <b>11</b> causes the recognition unit <b>101</b> to perform object recognition processing based on the sensor information acquired in the imaging processing in step S<b>400</b>. In the next step S<b>402</b>, the terminal device <b>11</b> causes the recognition result utilization unit <b>301</b> to execute processing using the recognition result of the object recognition processing performed by the recognition unit <b>101</b>.</p><p id="p-0167" num="0165">In the next step S<b>403</b>, the metadata analysis unit <b>250</b> acquires metadata from the sensor information acquired in the imaging processing in step S<b>400</b>, and then executes analysis processing on the acquired metadata.</p><p id="p-0168" num="0166">The next step S<b>404</b><i>st </i>represents a start of loop processing indicating execution of processing of step S<b>404</b><i>st </i>to step S<b>404</b><i>ed </i>for all the application targets of the sensor information, that is, for each of the recognizers <b>210</b> included in the server system <b>2</b>.</p><p id="p-0169" num="0167">In step S<b>405</b>, in the terminal device <b>11</b>, the metadata application determination processing is executed by the training data determination unit <b>251</b>. That is, in step S<b>405</b>, the training data determination unit <b>251</b> examines, based on the metadata, whether the sensor information acquired in the imaging processing in step S<b>400</b> is applicable to the target recognizer <b>210</b> among the individual recognizers <b>210</b>. Here, the determination method described in the first embodiment is applicable to the training data determination unit <b>251</b>. That is, the training data determination unit <b>251</b> determines a scene at the time of capturing the image data included in the sensor information regarding the sensor information based on a predetermined rule or learning, and designates the recognizer <b>210</b> to which the sensor information is to be applied as the training data for each determined scene, for example.</p><p id="p-0170" num="0168">When the training data determination unit <b>251</b> has determined in the next step S<b>406</b> that the sensor information is applicable to the target recognizer <b>210</b> (step S<b>406</b>, &#x201c;Yes&#x201d;), the processing proceeds to step S<b>407</b>. In step S<b>407</b>, the accumulation determination unit <b>103</b> in the terminal device <b>11</b> executes accumulation processing of the training data of the application target. That is, in step S<b>407</b>, the accumulation determination unit <b>103</b> determines to accumulate the sensor information as training data for the target recognizer <b>210</b> determined to be applicable in step S<b>406</b>. In accordance with this determination, the accumulation unit <b>104</b> stores and accumulates the sensor information in the storage unit <b>105</b>.</p><p id="p-0171" num="0169">After the processing of step S<b>407</b>, the terminal device <b>11</b> returns the processing from step S<b>404</b><i>ed </i>to step S<b>404</b><i>st, </i>and executes the processing of steps S<b>405</b> to S<b>407</b> on the next recognizer <b>210</b> as the target recognizer <b>210</b> among the individual recognizers <b>210</b>.</p><p id="p-0172" num="0170">In contrast, when it is determined in step S<b>406</b> that the target sensor information is not applicable to the target recognizer <b>210</b> (step S<b>406</b>, &#x201c;No&#x201d;), the processing returns from step S<b>404</b><i>ed </i>to step S<b>404</b><i>st. </i></p><p id="p-0173" num="0171">After completion of the processing of steps S<b>404</b><i>st </i>to S<b>404</b><i>ed </i>for each of the recognizers <b>210</b> regarding the sensor information, the terminal device <b>11</b> proceeds to the processing of step S<b>408</b>. In step S<b>408</b>, the terminal device <b>11</b> determines whether the number of recognizers <b>210</b> determined by the transmission determination unit <b>107</b><i>a </i>as application targets of the sensor information in the processing of steps S<b>404</b><i>st </i>to S<b>404</b><i>ed </i>is sufficient (for example, whether the number exceeds a threshold). When the transmission determination unit <b>107</b><i>a </i>determines that the number of the recognizers <b>210</b> determined as the application targets of the sensor information is not sufficient, the transmission determination unit <b>107</b><i>a </i>ends a series of processing according to the flowchart of <figref idref="DRAWINGS">FIG. <b>15</b></figref> without transmitting the sensor information to the server system <b>2</b>.</p><p id="p-0174" num="0172">In contrast, when the transmission determination unit <b>107</b><i>a </i>has determined in step S<b>408</b> that the number of the recognizers <b>210</b> determined as the application targets of the sensor information is sufficient, the processing proceeds to step S<b>409</b>. In step S<b>409</b>, the transmission determination unit <b>107</b><i>a </i>performs transmission processing of the sensor information. For example, the transmission determination unit <b>107</b><i>a </i>adds information indicating each of the recognizers <b>210</b> determined to be applicable by the processing of steps S<b>404</b><i>st </i>to S<b>404</b><i>ed </i>to the sensor information stored in the storage unit <b>105</b> in step S<b>407</b>.</p><p id="p-0175" num="0173">In the next step S<b>410</b>, the sensor information to which the information indicating each recognizer <b>210</b> has been added is read from the storage unit <b>105</b> and then transmitted by the communication unit <b>110</b> to the server system <b>2</b> via the network <b>1</b>. The server system <b>2</b> stores the sensor information transmitted from the terminal device <b>11</b> in the storage unit <b>20</b> in association with each of the recognizers <b>210</b> as training data for performing retraining of each of the recognizers <b>210</b> based on the information indicating each of the recognizers <b>210</b> added to the sensor information.</p><p id="p-0176" num="0174">In this manner, the second embodiment makes a determination as to whether the sensor information including the image data based on the captured image and including the metadata, output from the camera <b>12</b>, is applicable to each recognizer <b>210</b> of the server system <b>2</b> based on the metadata. Therefore, the terminal device <b>11</b> can apply the sensor information acquired from the camera <b>12</b> as the training data to a recognition machine different from the recognizer used for the sensor information by the terminal device <b>11</b>, making is possible to utilize the sensor information as training data with higher efficiency. Furthermore, the terminal device <b>11</b> causes the transmission determination unit <b>107</b><i>a </i>to determine whether to transmit the sensor information to the server system <b>2</b> based on the metadata included in the sensor information, for example. This makes it possible to suppress the communication cost for transmitting the sensor information.</p><p id="p-0177" num="0175">Although the above description is an exemplary case where the transmission determination unit <b>107</b><i>a </i>determines whether to transmit the sensor information to the server system <b>2</b> based on the metadata included in the sensor information, determination is not limited to this example. For example, the transmission determination unit <b>107</b><i>a </i>may determine whether to transmit the sensor information to the server system <b>2</b> based on a recognition score included in a recognition result for image data included in the sensor information, obtained by the recognition unit <b>101</b>. In this case, for example, it is conceivable that the transmission determination unit <b>107</b><i>a </i>transmits the sensor information to the server system <b>2</b> when the recognition score is a predetermined value or less. In the server system <b>2</b>, the recognition performance can be further improved by implementing training based on the sensor information having such a low recognition score.</p><p id="p-0178" num="0176">(3-1. First Modification)</p><p id="p-0179" num="0177">Next, a first modification of the second embodiment will be described. The first modification of the second embodiment is an example in which the terminal device <b>11</b> determines whether to transmit the acquired sensor information to the server system <b>2</b> based on the immediacy of the sensor information. In the first modification of the second embodiment, the configuration of the driving assistance system according to the first embodiment described with reference to <figref idref="DRAWINGS">FIG. <b>13</b></figref> is applicable.</p><p id="p-0180" num="0178">This point will be described more specifically. The sensor information acquired by the camera <b>12</b> sometimes include pieces of information having significantly different orders of time constants at the same time. As an example, there is a great temporal difference from the occurrence of the information until the point where a response is necessary between information regarding lens aging degradation correction included in the metadata of sensor information and the pedestrian recognition processing performed based on image data. In view of this, the first modification of the second embodiment obtains the degree of necessity of immediate transmission for each purpose of use regarding the target sensor information, and transmits sensor information to the server system <b>2</b> with priority given the sensor information with immediate necessity. Sensor information with less immediacy is accumulated and is not transmitted at the point of acquisition.</p><p id="p-0181" num="0179"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a functional block diagram of an example illustrating functions of the terminal device <b>11</b> according to the first modification of the second embodiment. In <figref idref="DRAWINGS">FIG. <b>16</b></figref>, compared to the configuration of the terminal device <b>11</b> according to the second embodiment described with reference to <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the terminal device <b>11</b> has a configuration in which a training data immediacy calculation unit <b>152</b> and a training data immediacy determination unit <b>153</b> are inserted between the training data application determination unit <b>108</b><i>a </i>and the transmission determination unit <b>107</b><i>a. </i></p><p id="p-0182" num="0180">The training data immediacy calculation unit <b>151</b> calculates the immediacy of the sensor information designated to be applied to the recognizer <b>210</b> by the training data determination unit <b>251</b>. For example, the training data immediacy calculation unit <b>151</b> calculates the immediacy of the sensor information according to the purpose of use of the recognizer <b>210</b> to which application of the sensor information has been designated by the training data determination unit <b>251</b>. The training data immediacy determination unit <b>153</b> determines the immediacy of the sensor information based on the calculated immediacy.</p><p id="p-0183" num="0181">In the above-described example, the recognizer <b>210</b> performs recognition related to lens aging correction. When the span of change in the recognized content is long, the training data immediacy calculation unit <b>152</b> obtains the next correction timing based on metadata of the sensor information, for example, and calculates a difference between the timing and the current time. When the calculated difference is longer than a threshold, the training data immediacy determination unit <b>153</b> determines that the sensor information has low immediacy. In another example described above, in a case where the recognizer <b>210</b> performs pedestrian recognition, the training data immediacy determination unit <b>153</b> determines that the immediacy of the sensor information is high.</p><p id="p-0184" num="0182">A transmission determination unit <b>107</b><i>b </i>determines whether to transmit the sensor information to the server system <b>2</b> according to the immediacy determined by the training data immediacy determination unit <b>153</b>. That is, the transmission determination unit <b>107</b><i>b </i>determines that the sensor information determined to have high immediacy will be transmitted to the server system <b>2</b> at that point. In contrast, the transmission determination unit <b>107</b><i>b </i>determines that the sensor information determined to have low immediacy does not need to be transmitted at that point, and accumulates the sensor information in the storage unit <b>105</b> by the accumulation unit <b>104</b>, for example.</p><p id="p-0185" num="0183">The communication unit <b>110</b> immediately transmits the sensor information determined to be transmitted by the transmission determination unit <b>107</b><i>b </i>to the server system <b>2</b>.</p><p id="p-0186" num="0184"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a flowchart of an example illustrating processing in the terminal device <b>11</b> according to the first modification of the second embodiment. In the first step S<b>500</b>, the terminal device <b>11</b> performs imaging processing by the imaging unit <b>300</b>, and acquires, from the camera <b>12</b>, sensor information including image data based on a captured image and including metadata related to imaging by the camera <b>12</b>.</p><p id="p-0187" num="0185">In the next step S<b>501</b>, the terminal device <b>11</b> causes the recognition unit <b>101</b> to perform object recognition processing based on the sensor information acquired in the imaging processing of step S<b>500</b>. In the next step S<b>502</b>, the terminal device <b>11</b> causes the recognition result utilization unit <b>301</b> to execute processing using the recognition result obtained in the object recognition processing performed by the recognition unit <b>101</b>.</p><p id="p-0188" num="0186">In the next step S<b>503</b>, the metadata analysis unit <b>250</b> acquires metadata from the sensor information acquired in the imaging processing in step S<b>500</b>, and executes analysis processing on the acquired metadata.</p><p id="p-0189" num="0187">The next step S<b>504</b><i>st </i>represents a start of loop processing indicating execution of processing of step S<b>504</b><i>st </i>to step S<b>504</b><i>ed </i>for all the application targets of the sensor information, that is, for each of the recognizers <b>210</b> included in the server system <b>2</b>.</p><p id="p-0190" num="0188">In step S<b>505</b>, in the terminal device <b>11</b>, the metadata application determination processing is executed by the training data determination unit <b>251</b>. That is, in step S<b>505</b>, the training data determination unit <b>251</b> examines, based on the metadata, whether the sensor information acquired in the imaging processing in step S<b>500</b> is applicable to the target recognizer <b>210</b> among the individual recognizers <b>210</b>. Here, the determination method described in the first embodiment is applicable to the training data determination unit <b>251</b>. That is, the training data determination unit <b>251</b> determines a scene at the time of capturing the image data included in the sensor information regarding the sensor information based on a predetermined rule or learning, and designates the recognizer <b>210</b> to which the sensor information is to be applied as the training data for each determined scene, for example.</p><p id="p-0191" num="0189">When the training data determination unit <b>251</b> has determined in the next step S<b>506</b> that the sensor information is applicable to the target recognizer <b>210</b> (step S<b>506</b>, &#x201c;Yes&#x201d;), the processing proceeds to step S<b>507</b>. In step S<b>507</b>, the accumulation determination unit <b>103</b> in the terminal device <b>11</b> executes accumulation processing of the training data of the application target. That is, in step S<b>507</b>, the accumulation determination unit <b>103</b> determines to accumulate the sensor information as training data for the target recognizer <b>210</b> determined to be applicable in step S<b>506</b>. In accordance with this determination, the accumulation unit <b>104</b> stores and accumulates the sensor information in the storage unit <b>105</b>.</p><p id="p-0192" num="0190">After the processing of step S<b>507</b>, the terminal device <b>11</b> returns the processing from step S<b>504</b><i>ed </i>to step S<b>504</b><i>st, </i>and executes the processing of steps S<b>505</b> to S<b>507</b> on the next recognizer <b>210</b> as the target recognizer <b>210</b> among the individual recognizers <b>210</b>.</p><p id="p-0193" num="0191">In contrast, when it is determined in step S<b>506</b> that the target sensor information is not applicable to the target recognizer <b>210</b> (step S<b>506</b>, &#x201c;No&#x201d;), the processing returns from step S<b>504</b><i>ed </i>to step S<b>504</b><i>st. </i></p><p id="p-0194" num="0192">After completion of the processing of steps S<b>504</b><i>st </i>to S<b>504</b><i>ed </i>for each of the recognizers <b>210</b> regarding the sensor information, the terminal device <b>11</b> proceeds to the processing of step S<b>508</b>. In step S<b>508</b>, the terminal device <b>11</b> calculates the immediacy of the sensor information by the training data immediacy calculation unit <b>152</b>. In the next step S<b>509</b>, the terminal device <b>11</b> causes the training data immediacy determination unit <b>153</b> to determine the immediacy of the sensor information.</p><p id="p-0195" num="0193">In the next step S<b>510</b>, the terminal device <b>11</b> causes the transmission determination unit <b>107</b><i>b </i>to determine whether to transmit the sensor information to the server system <b>2</b> based on the immediacy determined in step S<b>509</b>. When it is determined that the immediacy of the sensor information is low, the transmission determination unit <b>107</b><i>b </i>determines not to transmit the sensor information to the server system <b>2</b> (step S<b>510</b>, &#x201c;No&#x201d;), and ends the series of processing of the flowchart of <figref idref="DRAWINGS">FIG. <b>17</b></figref>.</p><p id="p-0196" num="0194">In contrast, when it is determined in step S<b>510</b> that the immediacy of the sensor information is high, the transmission determination unit <b>107</b><i>b </i>determines to transmit the sensor information to the server system <b>2</b> (step S<b>510</b>, &#x201c;Yes&#x201d;), and proceeds to the processing of step S<b>511</b>. In step S<b>511</b>, the transmission determination unit <b>107</b><i>b </i>performs transmission processing of the sensor information. For example, the transmission determination unit <b>107</b><i>b </i>adds, to the sensor information, information indicating the recognizer <b>210</b> determined to be applicable by the processing of steps S<b>504</b><i>st </i>to S<b>504</b><i>ed. </i></p><p id="p-0197" num="0195">In the next step S<b>512</b>, the sensor information to which the information indicating each recognizer <b>210</b> has been added is transmitted to the server system <b>2</b> via the network <b>1</b> by the communication unit <b>110</b>. The server system <b>2</b> stores the sensor information transmitted from the terminal device <b>11</b> in association with the recognizer <b>210</b> in the storage unit <b>20</b> as training data for performing retraining of the recognizer <b>210</b> based on the information indicating the recognizer <b>210</b> added to the sensor information.</p><p id="p-0198" num="0196">In this manner, in the first modification of the second embodiment, whether to transmit the sensor information to the server system <b>2</b> is determined based on the immediacy of the sensor information. This makes it possible to suppress the communication cost for transmitting the sensor information.</p><p id="p-0199" num="0197">(3-2. Second Modification)</p><p id="p-0200" num="0198">Next, a second modification of the second embodiment will be described. The second modification of the second embodiment is an example in which the terminal device <b>11</b> determines whether to transmit the acquired sensor information to the server system <b>2</b> based on the rarity of the sensor information. In the second modification of the second embodiment, the configuration of the driving assistance system according to the first embodiment described with reference to <figref idref="DRAWINGS">FIG. <b>13</b></figref> can be applied.</p><p id="p-0201" num="0199">This point will be described more specifically. Some sensor information acquired by the camera <b>12</b> is rare and needs to be collected early. For example, sensor information regarding a newly introduced road or traffic system is preferably reflected in training data as early as possible. In order to collect such sensor information, a region involving occurrence of a situation having a need of early collection is set in advance or occasionally by communication such as V2X, and the sensor information acquired in the region is determined as information to be transmitted to the server system <b>2</b>.</p><p id="p-0202" num="0200"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a functional block diagram of an example illustrating functions of the terminal device <b>11</b> according to the second modification of the second embodiment. In <figref idref="DRAWINGS">FIG. <b>18</b></figref>, as compared with the configuration of the terminal device <b>11</b> according to the second embodiment described with reference to <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the terminal device <b>11</b> has a configuration in which the training data application determination unit <b>108</b><i>a </i>is omitted and a rarity analysis unit <b>310</b> and a rarity determination unit <b>311</b> have been added.</p><p id="p-0203" num="0201">The sensor information acquired by the imaging unit <b>300</b> is input to a transmission determination unit <b>107</b><i>e </i>via the recognition unit <b>101</b>. On the other hand, the rarity analysis unit <b>310</b> analyzes the rarity of an event in a range captured by the camera <b>12</b>. The rarity determination unit <b>311</b> determines the rarity of the event analyzed by the rarity analysis unit <b>310</b>.</p><p id="p-0204" num="0202">For example, the rarity analysis unit <b>310</b> performs processing of setting a region including a rare event by communication with the outside, user setting, or the like. Furthermore, the rarity analysis unit <b>310</b> acquires positional information indicating the current position from the GNSS receiver <b>1007</b> included in the terminal device <b>11</b>, for example. Based on the analysis result of the rarity analysis unit <b>310</b>, the rarity determination unit <b>311</b> determines whether an imaging range captured by the camera <b>12</b> includes a set region. For example, based on the positional information indicating the current position acquired by the rarity analysis unit <b>310</b> and the set region, the rarity determination unit <b>311</b> determines that there is rarity in the sensor information acquired from the camera <b>12</b> when the acquired positional information corresponds to the set region.</p><p id="p-0205" num="0203">The transmission determination unit <b>107</b><i>c </i>determines whether to transmit the sensor information acquired from the camera <b>12</b> to the server system <b>2</b> in accordance with the determination result of the rarity determination unit <b>311</b>. The communication unit <b>110</b> transmits the sensor information determined to be transmitted by the transmission determination unit <b>107</b><i>c, </i>to the server system <b>2</b> via the network <b>1</b>.</p><p id="p-0206" num="0204"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a flowchart of an example illustrating processing in the terminal device <b>11</b> according to the second modification of the second embodiment. In the first step S<b>600</b>, the terminal device <b>11</b> performs imaging processing by the imaging unit <b>300</b>, and receives, from the camera <b>12</b>, sensor information including image data based on the captured image and metadata related to imaging performed by the camera <b>12</b>.</p><p id="p-0207" num="0205">In the next step S<b>601</b>, the terminal device <b>11</b> causes the recognition unit <b>101</b> to perform object recognition processing based on the sensor information acquired in the imaging processing of step S<b>600</b>. In the next step S<b>602</b>, the terminal device <b>11</b> causes the recognition result utilization unit <b>301</b> to execute processing using the recognition result obtained in the object recognition processing performed by the recognition unit <b>101</b>.</p><p id="p-0208" num="0206">In the next step S<b>603</b>, the rarity analysis unit <b>310</b> acquires metadata from the set sensor information acquired in the imaging processing in step S<b>600</b>, and executes analysis processing on the acquired metadata. In the next step S<b>604</b>, the rarity analysis unit <b>310</b> analyzes the rarity. For example, the rarity analysis unit <b>310</b> acquires positional information indicating the current position and information regarding the set region.</p><p id="p-0209" num="0207">In the next step S<b>605</b>, based on the analysis result in step S<b>604</b>, the rarity determination unit <b>311</b> determines whether the sensor information is applicable to the recognizer <b>210</b> trained by using the sensor information having rarity. For example, when the current position is a position corresponding to the region based on the positional information indicating the current position and the information indicating the set region, the rarity determination unit <b>311</b> can determine that the sensor information acquired from the camera <b>12</b> is sensor information having rarity.</p><p id="p-0210" num="0208">In the next step S<b>606</b>, when the rarity determination unit <b>311</b> has determined that the sensor information is applicable to the recognizer <b>210</b> trained by using the sensor information having rarity (step S<b>606</b>, &#x201c;Yes&#x201d;), the processing proceeds to step S<b>607</b>. In step S<b>607</b>, the terminal device <b>11</b> performs transmission processing of the sensor information by the transmission determination unit <b>107</b><i>c. </i>For example, the transmission determination unit <b>107</b><i>c </i>adds information indicating that there is rarity to the sensor information. The sensor information having the added information is transmitted to the server system <b>2</b> by the communication unit <b>110</b> via the network <b>1</b>.</p><p id="p-0211" num="0209">On the other hand, in a case where it is determined in step S<b>606</b> that the sensor information is not applicable to the recognizer <b>210</b> trained by using the sensor information having rarity, the series of processing according to the flowchart of <figref idref="DRAWINGS">FIG. <b>19</b></figref> is terminated without transmitting the sensor information to the server system <b>2</b>.</p><p id="p-0212" num="0210">In this manner, in the second modification of the second embodiment, it is determined whether to transmit the sensor information to the server system <b>2</b> based on the rarity of the sensor information. This makes it possible to suppress the communication cost for transmitting the sensor information.</p><p id="p-0213" num="0211">(3-3. Third Modification)</p><p id="p-0214" num="0212">Next, a third modification of the second embodiment will be described. The third modification of the second embodiment is an example having a capability of avoiding the transmission of duplicated sensor information from a plurality of vehicles <b>10</b>. In the third modification of the second embodiment, the configuration of the driving assistance system according to the first embodiment described with reference to <figref idref="DRAWINGS">FIG. <b>13</b></figref> is applicable.</p><p id="p-0215" num="0213">That is, when a plurality of vehicles <b>10</b> having a sensor information transmission function exists in relatively close proximity, for example, there is a possibility that a plurality of pieces of similar sensor information will be transmitted from the plurality of vehicles <b>10</b> to the server system <b>2</b>. Therefore, in the third modification of the second embodiment, a plurality of vehicles <b>10</b> existing in proximity has a capability to transmit mutually different data to the server system <b>2</b>. More specifically, in the third modification of the second embodiment, the target vehicle <b>10</b> (hereinafter, target vehicle) transmits and receives metadata of the sensor information to be transmitted to and from the surrounding vehicles <b>10</b> so as to suppress transmission of sensor information similar to the sensor information already transmitted by another vehicle <b>10</b> from the target vehicle.</p><p id="p-0216" num="0214"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a functional block diagram of an example illustrating functions of the terminal device <b>11</b> according to the third modification of the second embodiment. In <figref idref="DRAWINGS">FIG. <b>20</b></figref>, the terminal device <b>11</b> of the target vehicle includes an imaging unit <b>300</b>, a recognition unit <b>101</b>, a recognition result utilization unit <b>301</b>, a transmission determination unit <b>107</b><i>d, </i>a communication unit <b>110</b><i>a, </i>a non-target-vehicle data accumulation unit <b>320</b>, and a storage unit <b>321</b>. In addition, as compared to the target vehicle, other vehicles <b>10</b><sub>1</sub>, <b>10</b><sub>2</sub>, <b>10</b><sub>3</sub>, . . . each include an imaging unit <b>300</b>, a recognition unit <b>101</b>, a recognition result utilization unit <b>301</b>, a transmission determination unit <b>107</b>, and a communication unit <b>110</b>. Vehicle configuration is not limited to this, and each of the other vehicles <b>10</b><sub>1</sub>, <b>10</b><sub>2</sub>, <b>10</b><sub>3</sub>, . . . may have a configuration similar to the target vehicle.</p><p id="p-0217" num="0215">The imaging unit <b>300</b>, the recognition unit <b>101</b>, and the recognition result utilization unit <b>301</b> have functions equivalent to those of the imaging unit <b>300</b>, the recognition unit <b>101</b>, and the recognition result utilization unit <b>301</b> illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, for example, and thus, description thereof is omitted here.</p><p id="p-0218" num="0216">In <figref idref="DRAWINGS">FIG. <b>20</b></figref>, the communication unit <b>110</b><i>a </i>communicates with the communication unit <b>110</b> of the other vehicles <b>10</b><sub>1</sub>, <b>10</b><sub>2</sub>, <b>10</b><sub>3</sub>, . . . , and acquires information indicating sensor information already transmitted to the server system <b>2</b> by the other vehicles <b>10</b><sub>1</sub>, <b>10</b><sub>2</sub>, <b>10</b><sub>3</sub>, . . . .</p><p id="p-0219" num="0217">For example, in the target vehicle, the recognition unit <b>101</b> performs object recognition processing on image data included in the sensor information acquired from the camera <b>12</b>, and passes a recognition result to the transmission determination unit <b>107</b><i>d. </i>Similarly, in each of the vehicles <b>10</b><sub>1</sub>, <b>10</b><sub>2</sub>, <b>10</b><sub>3</sub>, . . . , the recognition unit <b>101</b> performs object recognition processing on the image data included in the sensor information acquired from the camera <b>12</b>, and passes the recognition result to each of the transmission determination units <b>107</b><i>d. </i></p><p id="p-0220" num="0218">The communication unit <b>110</b><i>a </i>communicates with each of the vehicles <b>10</b><sub>1</sub>, <b>10</b><sub>2</sub>, <b>10</b><sub>3</sub>, . . . , and requests information indicating sensor information to be transmitted to each of the vehicles <b>10</b><sub>1</sub>, <b>10</b><sub>2</sub>, <b>10</b><sub>3</sub>, . . . . In response to this request, each of the transmission determination units <b>107</b> of the vehicles <b>10</b><sub>1</sub>, <b>10</b><sub>2</sub>, <b>10</b><sub>3</sub>, . . . transmits, for example, metadata included in the sensor information and the recognition result to the target vehicle via the network <b>1</b>.</p><p id="p-0221" num="0219">In the terminal device <b>11</b> mounted on the target vehicle, the communication unit <b>110</b><i>a </i>passes the metadata and the recognition result received from each of the vehicles <b>10</b><sub>1</sub>, <b>10</b><sub>2</sub>, <b>10</b><sub>3</sub>, . . . to the non-target-vehicle data accumulation unit <b>320</b>. The non-target-vehicle data accumulation unit <b>320</b> accumulates the received metadata and the recognition result in the storage unit <b>321</b>.</p><p id="p-0222" num="0220">Based on the recognition result passed from the recognition unit <b>101</b> and the metadata included in the sensor information, the transmission determination unit <b>107</b><i>d </i>determines whether the already-transmitted data corresponding to the recognition result and the metadata is accumulated in the storage unit <b>321</b>. When the data is not accumulated, the transmission determination unit <b>107</b><i>d </i>transmits the recognition result and the metadata to the server system <b>2</b> via the communication unit <b>110</b><i>a. </i>In contrast, when the data is accumulated, the recognition result and the metadata will not be transmitted.</p><p id="p-0223" num="0221">In this manner, by avoiding transmission of duplicated sensor information among the plurality of vehicles <b>10</b>, it is possible to reduce the communication cost.</p><p id="p-0224" num="0222"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a flowchart of an example illustrating processing in the terminal device <b>11</b> according to the third modification of the second embodiment. In <figref idref="DRAWINGS">FIG. <b>21</b></figref>, the processing of steps S<b>700</b> to S<b>709</b> illustrated in section (a) and the processing of steps S<b>720</b> to S<b>721</b> illustrated in section (b) are processing that operates independently of each other by mutually different threads, for example. Furthermore, the processing in sections (a) and (b) is processing executed by the terminal device <b>11</b> mounted on the target vehicle.</p><p id="p-0225" num="0223">First, the processing of section (b) will be described. In step S<b>720</b>, the communication unit <b>110</b><i>a </i>performs reception processing for receiving non-target-vehicle transmission information. For example, the communication unit <b>110</b><i>a </i>requests metadata of the transmitted sensor information and the recognition result based on the sensor information to each of the other vehicles <b>10</b><sub>1</sub>, <b>10</b><sub>2</sub>, <b>10</b><sub>3</sub>, . . . . In the next step S<b>721</b>, the communication unit <b>110</b><i>a </i>performs accumulation processing of accumulating the non-target-vehicle transmission information according to the request in step S<b>720</b> in the storage unit <b>321</b> by the non-target-vehicle data accumulation unit <b>320</b>. For example, in response to the request in step S<b>720</b>, the communication unit <b>110</b><i>a </i>receives sets of the metadata and the recognition result transmitted from the other vehicles <b>10</b><sub>1</sub>, <b>10</b><sub>2</sub>, <b>10</b><sub>3</sub>, . . . , and passes the received sets of the metadata and the recognition result to the non-target-vehicle data accumulation unit <b>320</b>. The non-target-vehicle data accumulation unit <b>320</b> stores and accumulates the sets of the metadata and the recognition result passed from the communication unit <b>110</b><i>a </i>in the storage unit <b>321</b>.</p><p id="p-0226" num="0224">The processing in steps S<b>720</b> and S<b>721</b> in section (b) is executed occasionally.</p><p id="p-0227" num="0225">Next, the processing of section (a) will be described. In the first step S<b>700</b>, the terminal device <b>11</b> performs imaging processing by the imaging unit <b>300</b>, and acquires, from the camera <b>12</b>, sensor information including image data based on a captured image and including metadata related to imaging by the camera <b>12</b>.</p><p id="p-0228" num="0226">In the next step S<b>701</b>, the terminal device <b>11</b> causes the recognition unit <b>101</b> to perform object recognition processing based on the sensor information acquired in the imaging processing of step S<b>700</b>. In the next step S<b>702</b>, the terminal device <b>11</b> causes the recognition result utilization unit <b>301</b> to execute processing using the recognition result obtained in the object recognition processing performed by the recognition unit <b>101</b>. Furthermore, the recognition unit <b>101</b> passes the recognition result and the metadata included in the sensor information to the transmission determination unit <b>107</b><i>d. </i></p><p id="p-0229" num="0227">The next step S<b>703</b><i>st </i>indicates the start of loop processing indicating that the processing is to be executed in steps S<b>703</b><i>st </i>to S<b>703</b><i>ed </i>for all the non-target-vehicle data (the recognition result and the metadata) accumulated in the storage unit <b>321</b> in step S<b>721</b> of section (b).</p><p id="p-0230" num="0228">In step S<b>704</b>, the transmission determination unit <b>107</b><i>d </i>requests the non-target-vehicle data accumulation unit <b>320</b> to read one set of the metadata and the recognition result accumulated in the storage unit <b>321</b>, and acquires the set of the metadata and the recognition result read from the storage unit <b>321</b> in response to the request.</p><p id="p-0231" num="0229">In the next step S<b>705</b>, the transmission determination unit <b>107</b><i>d </i>determines whether the set of the metadata and the recognition result read from the storage unit <b>321</b> in step S<b>704</b> is data close to the set of the metadata and the recognition result passed from the recognition unit <b>101</b> in step S<b>702</b>. For example, the transmission determination unit <b>107</b><i>d </i>obtains the similarity between the individual recognition results, and when the obtained similarity is a threshold or less, the transmission determination unit <b>107</b><i>d </i>can determine that the set of the metadata and the recognition result read from the storage unit <b>321</b> is data close to the set of the metadata and the recognition result passed from the recognition unit <b>101</b>. The comparison method is not limited to this and the determination may be made by comparing the pieces of metadata.</p><p id="p-0232" num="0230">Having determined that the two sets are data close to each other (step S<b>706</b>, &#x201c;Yes&#x201d;), the transmission determination unit <b>107</b><i>d </i>proceeds to processing of step S<b>706</b>, and sets a non-transmit flag in the set of the metadata and the recognition result read from the storage unit <b>321</b>. After the processing of step S<b>706</b>, the transmission determination unit <b>107</b><i>d </i>returns the processing from step S<b>703</b><i>ed </i>to step S<b>703</b><i>st, </i>and executes the processing from step S<b>704</b> on the next set of the metadata and the recognition result accumulated in the storage unit <b>321</b>.</p><p id="p-0233" num="0231">In contrast, when having determined that the sets of data are not data close to each other (step S<b>705</b>, &#x201c;No&#x201d;), the transmission determination unit <b>107</b><i>d </i>returns the processing from step S<b>703</b><i>ed </i>to step S<b>703</b><i>st, </i>and executes the processing from step S<b>704</b> on the next set of the metadata and the recognition result accumulated in the storage unit <b>321</b>.</p><p id="p-0234" num="0232">After completion of the processing of step S<b>704</b> and step S<b>705</b> for all the sets of the metadata and the recognition result accumulated in the storage unit <b>321</b>, the transmission determination unit <b>107</b><i>d </i>proceeds to the processing of step S<b>707</b>.</p><p id="p-0235" num="0233">In step S<b>707</b>, the transmission determination unit <b>107</b><i>d </i>determines the presence or absence of a set of the metadata and the recognition result to which the non-transmit flag is set in step S<b>706</b> among all sets of the metadata and the recognition result accumulated in the storage unit <b>321</b>. When the transmission determination unit <b>107</b><i>d </i>has determined that the non-transmit flag is not set in all the sets of the metadata and the recognition result accumulated in the storage unit <b>321</b> (step S<b>707</b>, &#x201c;Yes&#x201d;), the processing proceeds to step S<b>708</b>.</p><p id="p-0236" num="0234">In step S<b>708</b>, the transmission determination unit <b>107</b><i>d </i>passes the sensor information corresponding to the set of the metadata and the recognition result passed from the recognition unit <b>101</b> in step S<b>702</b> to the communication unit <b>110</b><i>a, </i>and performs transmission processing for transmitting the sensor information to the server system <b>2</b>. In the next step S<b>709</b>, the communication unit <b>110</b><i>a </i>transmits the sensor information passed from the transmission determination unit <b>107</b><i>d </i>to the server system <b>2</b> via the network <b>1</b>.</p><p id="p-0237" num="0235">In contrast, when having determined, in step S<b>707</b>, that there is at least one set of the metadata and the recognition result to which the non-transmit flag is set in step S<b>706</b> among all the sets of the metadata and the recognition result accumulated in the storage unit <b>321</b> (step S<b>707</b>, &#x201c;No&#x201d;), the transmission determination unit <b>107</b><i>d </i>ends the series of processing of the flowchart in section (a) in <figref idref="DRAWINGS">FIG. <b>21</b></figref> without performing the transmission processing of the sensor information.</p><p id="p-0238" num="0236">In this manner, the third modification of the second embodiment suppress transmission of sensor information close to the sensor information already transmitted by the other vehicle <b>10</b>, making it possible to reduce the communication cost for transmitting the sensor information.</p><heading id="h-0012" level="1">4. Third Embodiment</heading><p id="p-0239" num="0237">Next, a third embodiment will be described. The third embodiment relates to sensor information that has not been transmitted to the server system <b>2</b> in the second embodiment described above, and is applicable to any of the second embodiment and their modifications. Here, for the sake of explanation, the processing according to the third embodiment will be described as being applied to the second embodiment described above.</p><p id="p-0240" num="0238"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a block diagram illustrating a configuration of an example of a driving assistance system according to the third embodiment. The configuration illustrated in <figref idref="DRAWINGS">FIG. <b>22</b></figref> is equivalent to the configuration described in the second embodiment with reference to <figref idref="DRAWINGS">FIG. <b>13</b></figref>. However, as illustrated in <figref idref="DRAWINGS">FIG. <b>22</b></figref> as a colored area, an accumulation determination unit <b>103</b>, an accumulation unit <b>104</b>, and an accumulated information output unit <b>106</b> play main roles.</p><p id="p-0241" num="0239">Specifically, there may be a case where sensor information that has not been transmitted in the above-described second embodiment and the like is also necessary. Therefore, in the third embodiment, the sensor information that has not been transmitted is stored and accumulated in the storage unit <b>105</b> by the accumulation unit <b>104</b>. Thereafter, at an arbitrary timing, the sensor information is output from the accumulated information output unit <b>106</b> by direct connection which is not via the Internet or the like, such as cable connection or a local area network (LAN).</p><p id="p-0242" num="0240"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a functional block diagram of an example illustrating functions of the terminal device <b>11</b> according to the third embodiment. Since the basic configuration is similar to the configuration of the terminal device <b>11</b> according to the second embodiment described with reference to <figref idref="DRAWINGS">FIG. <b>14</b></figref>, a detailed description thereof will be omitted. In the terminal device <b>11</b> according to the third embodiment, the transmission determination unit <b>107</b><i>e </i>passes the sensor information determined not to be transmitted, to the accumulation unit <b>104</b>, and the accumulation unit <b>104</b> stores and accumulates the received sensor information in the storage unit <b>105</b>. When an external device capable of collecting sensor information is connected, the accumulated information output unit <b>106</b> requests the accumulation unit <b>104</b> for the sensor information accumulated in the storage unit <b>105</b>, and outputs the sensor information read from the storage unit <b>105</b> by the accumulation unit <b>104</b> to the external device.</p><p id="p-0243" num="0241"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a flowchart illustrating an example of processing in the terminal device <b>11</b> according to the third embodiment. In <figref idref="DRAWINGS">FIG. <b>24</b></figref>, the processing of steps S<b>820</b> to S<b>821</b> illustrated in section (b) is processing to be started when an external device is connected to the accumulated information output unit <b>106</b>, for example.</p><p id="p-0244" num="0242">First, the processing of section (a) will be described. The processing of section (a) is substantially the same as the processing according to the second embodiment described with reference to <figref idref="DRAWINGS">FIG. <b>15</b></figref>. That is, in the first step S<b>800</b>, the terminal device <b>11</b> performs the imaging processing by the imaging unit <b>300</b>, and receives, from the camera <b>12</b>, sensor information including the image data based on the captured image and the metadata related to the imaging by the camera <b>12</b>. In the next step S<b>801</b>, the terminal device <b>11</b> causes the recognition unit <b>101</b> to perform object recognition processing based on the sensor information acquired in the imaging processing in step S<b>800</b>. In the next step S<b>802</b>, the terminal device <b>11</b> causes the recognition result utilization unit <b>301</b> to execute processing using the recognition result by the object recognition processing of the recognition unit <b>101</b>.</p><p id="p-0245" num="0243">In the next step S<b>803</b>, the metadata analysis unit <b>250</b> acquires metadata from the sensor information acquired in the imaging processing in step S<b>800</b>, and executes analysis processing on the acquired metadata.</p><p id="p-0246" num="0244">The next step S<b>804</b><i>st </i>represents a start of loop processing indicating execution of processing of step S<b>804</b><i>st </i>to step S<b>804</b><i>ed </i>for all the application targets of the sensor information, that is, for each of the recognizers <b>210</b> included in the server system <b>2</b>.</p><p id="p-0247" num="0245">In step S<b>805</b>, in the terminal device <b>11</b>, the metadata application determination processing is executed by the training data determination unit <b>251</b>. That is, in step S<b>805</b>, the training data determination unit <b>251</b> examines, based on the metadata, whether the sensor information acquired in the imaging processing in step S<b>800</b> is applicable to the target recognizer <b>210</b> among the individual recognizers <b>210</b> by using the determination method described in the first embodiment, for example</p><p id="p-0248" num="0246">When the training data determination unit <b>251</b> has determined in the next step S<b>806</b> that the sensor information is applicable to the target recognizer <b>210</b> (step S<b>806</b>, &#x201c;Yes&#x201d;), the processing proceeds to step S<b>807</b>. In step S<b>807</b>, the accumulation determination unit <b>103</b> in the terminal device <b>11</b> executes accumulation processing of the training data of the application target. That is, in step S<b>807</b>, the accumulation determination unit <b>103</b> determines to accumulate the sensor information as training data for the target recognizer <b>210</b> determined to be applicable in step S<b>806</b>. In accordance with this determination, the accumulation unit <b>104</b> stores and accumulates the sensor information in the storage unit <b>105</b>.</p><p id="p-0249" num="0247">After the processing of step S<b>807</b>, the terminal device <b>11</b> returns the processing from step S<b>804</b><i>ed </i>to step S<b>804</b><i>st, </i>and executes the processing of steps S<b>805</b> to S<b>807</b> on the next recognizer <b>210</b> as the target recognizer <b>210</b> among the individual recognizers <b>210</b>.</p><p id="p-0250" num="0248">In contrast, when it is determined in step S<b>806</b> that the target sensor information is not applicable to the target recognizer <b>210</b> (step S<b>806</b>, &#x201c;No&#x201d;), the processing returns from step S<b>804</b><i>ed </i>to step S<b>804</b><i>st. </i></p><p id="p-0251" num="0249">After completion of the processing of steps S<b>804</b><i>st </i>to S<b>804</b><i>ed </i>for each of the recognizers <b>210</b> regarding the sensor information, the terminal device <b>11</b> proceeds to the processing of step S<b>808</b>. In step S<b>808</b>, the terminal device <b>11</b> determines whether the number of recognizers <b>210</b> determined by the transmission determination unit <b>107</b><i>e </i>as application targets of the sensor information in the processing of steps S<b>804</b><i>st </i>to S<b>804</b><i>ed </i>is sufficient (for example, whether the number exceeds a threshold).</p><p id="p-0252" num="0250">Having determined that the number of the recognizers <b>210</b> determined as the application targets of the sensor information is not sufficient, the transmission determination unit <b>107</b><i>e </i>proceeds to the processing of step S<b>811</b>. In step S<b>811</b>, the transmission determination unit <b>107</b><i>e </i>performs accumulation processing of sensor information. For example, the transmission determination unit <b>107</b><i>e </i>instructs the accumulation unit <b>104</b> to continue to hold the sensor information accumulated in the storage unit <b>105</b> in step S<b>807</b>. The end of the accumulation processing completes the series of processing according to the flowchart of section (a) of <figref idref="DRAWINGS">FIG. <b>24</b></figref>.</p><p id="p-0253" num="0251">In contrast, when the transmission determination unit <b>107</b><i>e </i>has determined in step S<b>808</b> that the number of the recognizers <b>210</b> determined as the application targets of the sensor information is sufficient, the processing proceeds to step S<b>809</b>. In step S<b>809</b>, the transmission determination unit <b>107</b><i>e </i>performs transmission processing of the sensor information. For example, the transmission determination unit <b>107</b><i>e </i>adds information indicating each of the recognizers <b>210</b> determined to be applicable by the processing of steps S<b>804</b><i>st </i>to S<b>804</b><i>ed </i>to the sensor information accumulated in the storage unit <b>105</b> in step S<b>807</b>.</p><p id="p-0254" num="0252">In the next step S<b>810</b>, the sensor information to which the information indicating each recognizer <b>210</b> has been added is read from the storage unit <b>105</b> and then transmitted by the communication unit <b>110</b> to the server system <b>2</b> via the network <b>1</b>. The accumulation unit <b>104</b> deletes the transmitted sensor information from the storage unit <b>105</b>. The server system <b>2</b> stores the sensor information transmitted from the terminal device <b>11</b> in the storage unit <b>20</b> in association with each of the recognizers <b>210</b> as training data for performing retraining of each of the recognizers <b>210</b> based on the information indicating each of the recognizers <b>210</b> added to the sensor information.</p><p id="p-0255" num="0253">The processing of section (b) will be described. When the external device is directly connected by a cable or the like, the accumulated information output unit <b>106</b> executes processing of connecting with the external device in step S<b>820</b>. When the connection with the external device is established, the accumulated information output unit <b>106</b> issues an instruction to the accumulation unit <b>104</b> to read the accumulated sensor information from the storage unit <b>105</b>. The accumulated information output unit <b>106</b> transmits each piece of sensor information read from the storage unit <b>105</b> by the accumulation unit <b>104</b> to the external device.</p><p id="p-0256" num="0254">In this manner, in the third embodiment, the sensor information that has not been transmitted to the server system <b>2</b> by the transmission determination unit <b>107</b><i>e </i>can be output to the external device at an arbitrary timing. With this configuration, it is possible to efficiently use the sensor information without increasing a communication cost by the network <b>1</b>.</p><p id="p-0257" num="0255">(4-1. First Modification)</p><p id="p-0258" num="0256">Next, a first modification of the third embodiment will be described. The first modification of the third embodiment relates to sensor information that has not been transmitted to the server system <b>2</b> in the second embodiment and their modifications described above, and is applicable to any of the second embodiment and their modifications. Here, for the sake of explanation, the processing according to the third embodiment will be described as being applied to the second embodiment described above.</p><p id="p-0259" num="0257">The communication cost at transmission of sensor information from the terminal device <b>11</b> to the server system <b>2</b> via the network <b>1</b> varies in cases, for example, where a Wi-Fi (registered trademark) hot spot is used to connect to the network <b>1</b> or where communication is performed at a timing with a low communication load. Therefore, in the first modification of the third embodiment, a network communication fee plan (hourly communication fee information) and a communication plan (hourly communication amount plan) are set in advance, or set by communication such as V2X. In addition, accumulated sensor information is to be transmitted at a timing when the communication cost is low, in a time zone when the communication load is not high, in a case where the communication amount in the own vehicle is small, and the like.</p><p id="p-0260" num="0258"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a functional block diagram of an example illustrating functions of the terminal device <b>11</b> according to the first modification of the third embodiment. Since the basic configuration is similar to the configuration of the terminal device <b>11</b> according to the second embodiment described with reference to <figref idref="DRAWINGS">FIG. <b>14</b></figref>, a detailed description thereof will be omitted. In the terminal device <b>11</b> according to the first modification of the third embodiment, the transmission determination unit <b>107</b><i>e </i>passes the sensor information determined not to be transmitted, to the accumulation unit <b>104</b>, and the accumulation unit <b>104</b> stores and accumulates the sensor information in the storage unit <b>105</b>. At a timing when the communication cost is low, in a time zone where the communication load is not high, when the communication amount in the own vehicle is small, or the like, the transmission determination unit <b>107</b><i>e </i>reads the accumulated sensor information from the storage unit <b>105</b> and transmits the sensor information to the server system <b>2</b> via the network <b>1</b>.</p><p id="p-0261" num="0259"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a flowchart of an example illustrating processing in the terminal device <b>11</b> according to the first modification of the third embodiment. In <figref idref="DRAWINGS">FIG. <b>26</b></figref>, the processing of steps S<b>900</b> to S<b>911</b> illustrated in section (a) and the processing of steps S<b>920</b> to S<b>924</b> illustrated in section (b) are processing that operates independently of each other by mutually different threads, for example.</p><p id="p-0262" num="0260">First, the processing of section (a) will be described. The processing of section (a) is substantially the same as the processing according to the second embodiment described with reference to <figref idref="DRAWINGS">FIG. <b>15</b></figref>. That is, in the first step S<b>900</b>, the terminal device <b>11</b> performs the imaging processing by the imaging unit <b>300</b>, and receives, from the camera <b>12</b>, sensor information including the image data based on the captured image and the metadata related to the imaging by the camera <b>12</b>. In the next step S<b>901</b>, the terminal device <b>11</b> causes the recognition unit <b>101</b> to perform object recognition processing based on the sensor information acquired in the imaging processing in step S<b>900</b>. In the next step S<b>902</b>, the terminal device <b>11</b> causes the recognition result utilization unit <b>301</b> to execute processing using the recognition result by the object recognition processing of the recognition unit <b>101</b>.</p><p id="p-0263" num="0261">In the next step S<b>903</b>, the metadata analysis unit <b>250</b> acquires metadata from the sensor information acquired in the imaging processing in step S<b>900</b>, and executes analysis processing on the acquired metadata.</p><p id="p-0264" num="0262">The next step S<b>904</b><i>st </i>represents a start of loop processing indicating execution of processing of step S<b>904</b><i>st </i>to step S<b>904</b><i>ed </i>for all the application targets of the sensor information, that is, for each of the recognizers <b>210</b> included in the server system <b>2</b>.</p><p id="p-0265" num="0263">In step S<b>905</b>, in the terminal device <b>11</b>, the metadata application determination processing is executed by the training data determination unit <b>251</b>. That is, in step S<b>905</b>, the training data determination unit <b>251</b> examines, based on the metadata, whether the sensor information acquired in the imaging processing in step S<b>900</b> is applicable to the target recognizer <b>210</b> among the individual recognizers <b>210</b> by using the determination method described in the first embodiment, for example.</p><p id="p-0266" num="0264">When the training data determination unit <b>251</b> has determined in the next step S<b>906</b> that the sensor information is applicable to the target recognizer <b>210</b> (step S<b>906</b>, &#x201c;Yes&#x201d;), the processing proceeds to step S<b>907</b>. In step S<b>907</b>, the accumulation determination unit <b>103</b> in the terminal device <b>11</b> executes accumulation processing of the training data of the application target. That is, in step S<b>907</b>, the accumulation determination unit <b>103</b> determines to accumulate the sensor information as training data for the target recognizer <b>210</b> determined to be applicable in step S<b>906</b>. In accordance with this determination, the accumulation unit <b>104</b> stores and accumulates the sensor information in the storage unit <b>105</b>.</p><p id="p-0267" num="0265">After the processing of step S<b>907</b>, the terminal device <b>11</b> returns the processing from step S<b>904</b><i>ed </i>to step S<b>904</b><i>st, </i>and executes the processing of steps S<b>905</b> to S<b>907</b> on the next recognizer <b>210</b> as the target recognizer <b>210</b> among the individual recognizers <b>210</b>.</p><p id="p-0268" num="0266">In contrast, when it is determined in step S<b>906</b> that the target sensor information is not applicable to the target recognizer <b>210</b> (step S<b>906</b>, &#x201c;No&#x201d;), the processing returns from step S<b>904</b><i>ed </i>to step S<b>904</b><i>st. </i></p><p id="p-0269" num="0267">After completion of the processing of steps S<b>904</b><i>st </i>to S<b>904</b><i>ed </i>for each of the recognizers <b>210</b> regarding the sensor information, the terminal device <b>11</b> proceeds to the processing of step S<b>908</b>. In step S<b>908</b>, the terminal device <b>11</b> determines whether the number of recognizers <b>210</b> determined by the transmission determination unit <b>107</b><i>e </i>as application targets of the sensor information in the processing of steps S<b>904</b><i>st </i>to S<b>904</b><i>ed </i>is sufficient (for example, whether the number exceeds a threshold).</p><p id="p-0270" num="0268">When the transmission determination unit <b>107</b><i>e </i>has determined in step S<b>908</b> that the number of the recognizers <b>210</b> determined as the application targets of the sensor information is not sufficient (step S<b>908</b>, &#x201c;No&#x201d;), the processing proceeds to step S<b>911</b>. In step S<b>911</b>, the transmission determination unit <b>107</b><i>e </i>performs accumulation processing of sensor information. For example, the transmission determination unit <b>107</b><i>e </i>instructs the accumulation unit <b>104</b> to continue to hold the sensor information accumulated in the storage unit <b>105</b> in step S<b>907</b>. The end of the accumulation processing completes the series of processing according to the flowchart of section (a) of <figref idref="DRAWINGS">FIG. <b>26</b></figref>.</p><p id="p-0271" num="0269">In contrast, when the transmission determination unit <b>107</b><i>e </i>has determined in step S<b>908</b> that the number of the recognizers <b>210</b> determined as the application targets of the sensor information is sufficient (step S<b>908</b>, &#x201c;Yes&#x201d;), the processing proceeds to step S<b>909</b>. In step S<b>909</b>, the transmission determination unit <b>107</b><i>e </i>performs transmission processing of the sensor information. For example, the transmission determination unit <b>107</b><i>e </i>adds information indicating each of the recognizers <b>210</b> determined to be applicable by the processing of steps S<b>904</b><i>st </i>to S<b>904</b><i>ed </i>to the sensor information accumulated in the storage unit <b>105</b> in step S<b>907</b>.</p><p id="p-0272" num="0270">In the next step S<b>910</b>, the sensor information to which the information indicating each recognizer <b>210</b> has been added is read from the storage unit <b>105</b> and then transmitted by the communication unit <b>110</b> to the server system <b>2</b> via the network <b>1</b>. The accumulation unit <b>104</b> deletes the transmitted sensor information from the storage unit <b>105</b>. The server system <b>2</b> stores the sensor information transmitted from the terminal device <b>11</b> in the storage unit <b>20</b> in association with each of the recognizers <b>210</b> as training data for performing retraining of each of the recognizers <b>210</b> based on the information indicating each of the recognizers <b>210</b> added to the sensor information.</p><p id="p-0273" num="0271">Next, the processing of section (b) will be described. In step S<b>920</b>, the transmission determination unit <b>107</b><i>e </i>performs connection processing of connecting to the network <b>1</b> by the communication unit <b>110</b>. In the next step S<b>921</b>, the transmission determination unit <b>107</b><i>e </i>performs estimation processing of estimating the communication cost in the network <b>1</b>. For example, the transmission determination unit <b>107</b><i>e </i>estimates the communication cost based on a communication plan, a fee plan, a time zone, traffic of the network <b>1</b>, and the like regarding the connection to the network <b>1</b>.</p><p id="p-0274" num="0272">In the next step S<b>922</b>, the transmission determination unit <b>107</b><i>e </i>determines whether the communication cost estimated in step S<b>921</b> is low, for example, whether the communication cost is less than a predetermined value. In a case where the transmission determination unit <b>107</b><i>e </i>determines that the estimated communication cost is high (step S<b>922</b>, &#x201c;No&#x201d;), the transmission determination unit <b>107</b><i>e </i>ends the series of processing in the flowchart of this section (b) without transmitting the sensor information accumulated in the storage unit <b>105</b>.</p><p id="p-0275" num="0273">In contrast, when having determined, in step S<b>922</b>, that the communication cost estimated in step S<b>921</b> is low (step S<b>922</b>, &#x201c;Yes&#x201d;), the transmission determination unit <b>107</b><i>e </i>proceeds to the processing of step S<b>923</b>. In step S<b>923</b>, the transmission determination unit <b>107</b><i>e </i>performs transmission processing of the sensor information accumulated in the storage unit <b>105</b>. For example, the transmission determination unit <b>107</b><i>e </i>instructs the accumulation unit <b>104</b> to read the sensor information accumulated in the storage unit <b>105</b>.</p><p id="p-0276" num="0274">In the next step S<b>924</b>, the transmission determination unit <b>107</b><i>e </i>transmits the sensor information read from the storage unit <b>105</b> by the accumulation unit <b>104</b> in response to the instruction in step S<b>923</b> to the server system <b>2</b> via the network <b>1</b> by the communication unit <b>110</b>. The accumulation unit <b>104</b> deletes the transmitted sensor information from the storage unit <b>105</b>.</p><p id="p-0277" num="0275">In this manner, in the first modification of the third embodiment, the sensor information accumulated in the storage unit <b>105</b> without being transmitted to the server system <b>2</b> at the time of acquisition is to be transmitted to the server system <b>2</b> via the network <b>1</b> according to the communication cost. This makes it possible to reduce the communication cost related to the transmission of the sensor information.</p><p id="p-0278" num="0276">(4-2. Second modification)</p><p id="p-0279" num="0277">Next, a second modification of the third embodiment will be described. The second modification of the third embodiment relates to sensor information that has not been transmitted to the server system <b>2</b> in the second embodiment and their modifications described above, and is applicable to any of the second embodiment and their modifications. Here, for the sake of explanation, the processing according to the second modification of the third embodiment will be described as being applied to the second embodiment described above.</p><p id="p-0280" num="0278">In the second modification of the third embodiment, the sensor information that has not been transmitted to the server system <b>2</b> at the time of acquisition is prioritized, and accumulation of the sensor information is controlled according to the priority. Specifically, the priority of the sensor information sometimes differs depending on the content included, the situation at the time of acquisition, and the like. Accordingly, at accumulation of the sensor information that has not been transmitted to the server system <b>2</b>, the accumulation determination unit <b>103</b> prioritizes the sensor information and accumulates the prioritized sensor information in the storage unit <b>105</b>. When accumulating the sensor information exceeding the amount of information that can be accumulated in the accumulation unit <b>104</b> (storage unit <b>105</b>), the accumulation determination unit <b>103</b> sequentially deletes the sensor information in order from the lowest priority data out of the accumulated sensor information while accumulating the sensor information with higher priority.</p><p id="p-0281" num="0279"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a functional block diagram of an example illustrating functions of the terminal device <b>11</b> according to the second modification of the third embodiment. Since the basic configuration is similar to the configuration of the terminal device <b>11</b> according to the second embodiment described with reference to <figref idref="DRAWINGS">FIG. <b>14</b></figref>, a detailed description thereof will be omitted. In the terminal device <b>11</b> according to the second modification of the third embodiment, the sensor information determined not to be transmitted is passed to the accumulation determination unit <b>103</b>, and the accumulation determination unit <b>103</b> prioritizes the sensor information. The accumulation unit <b>104</b> stores and accumulates the prioritized sensor information in the storage unit <b>105</b>. At this time, when the storage unit <b>105</b> has no capacity, the accumulation determination unit <b>103</b> deletes the sensor information of the lowest priority among the pieces of sensor information stored in the storage unit <b>105</b>.</p><p id="p-0282" num="0280">In addition, at a timing when the communication cost is low, in a time zone where the communication load is not high, when the communication amount in the own vehicle is small, or the like, the transmission determination unit <b>107</b><i>e </i>reads the accumulated sensor information from the storage unit <b>105</b> and transmits the sensor information to the server system <b>2</b> via the network <b>1</b>.</p><p id="p-0283" num="0281"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is a flowchart of an example illustrating processing in the terminal device <b>11</b> according to the second modification of the third embodiment. Note that, in <figref idref="DRAWINGS">FIG. <b>28</b></figref>, the processing of steps S<b>1000</b> to S<b>1017</b> illustrated in section (a) and the processing of steps S<b>1030</b> to S<b>1034</b> illustrated in section (b) are processing that operate independently of each other by mutually different threads.</p><p id="p-0284" num="0282">First, the processing of section (a) will be described. In the processing of section (a), the processing of steps S<b>1000</b> to S<b>1010</b> is substantially the same as the processing of steps S<b>400</b> to S<b>410</b> in the flowchart of <figref idref="DRAWINGS">FIG. <b>15</b></figref>. That is, in the first step S<b>1000</b>, the terminal device <b>11</b> performs the imaging processing by the imaging unit <b>300</b>, and receives, from the camera <b>12</b>, sensor information including the image data based on the captured image and the metadata related to the imaging by the camera <b>12</b>. In the next step S<b>1001</b>, the terminal device <b>11</b> causes the recognition unit <b>101</b> to perform object recognition processing based on the sensor information acquired in the imaging processing in step S<b>1000</b>. In the next step S<b>1002</b>, the terminal device <b>11</b> causes the recognition result utilization unit <b>301</b> to execute processing using the recognition result by the object recognition processing of the recognition unit <b>101</b>.</p><p id="p-0285" num="0283">In the next step S<b>1003</b>, the metadata analysis unit <b>250</b> acquires metadata from the sensor information acquired in the imaging processing in step S<b>1000</b>, and executes analysis processing on the acquired metadata.</p><p id="p-0286" num="0284">The next step S<b>1004</b><i>st </i>represents a start of loop processing indicating execution of processing of step S<b>1004</b><i>st </i>to step S<b>1004</b><i>ed </i>for all the application targets of the sensor information, that is, for each of the recognizers <b>210</b> included in the server system <b>2</b>.</p><p id="p-0287" num="0285">In step S<b>1005</b>, in the terminal device <b>11</b>, the metadata application determination processing is executed by the training data determination unit <b>251</b>. That is, in step S<b>1005</b>, the training data determination unit <b>251</b> examines, based on the metadata, whether the sensor information acquired in the imaging processing in step S<b>1000</b> is applicable to the target recognizer <b>210</b> among the individual recognizers <b>210</b> by using the determination method described in the first embodiment, for example.</p><p id="p-0288" num="0286">When the training data determination unit <b>251</b> has determined in the next step S<b>1006</b> that the sensor information is applicable to the target recognizer <b>210</b> (step S<b>1006</b>, &#x201c;Yes&#x201d;), the processing proceeds to step S<b>1007</b>. In step S<b>1007</b>, the accumulation determination unit <b>103</b> in the terminal device <b>11</b> executes accumulation processing of the training data of the application target. That is, in step S<b>1007</b>, the accumulation determination unit <b>103</b> determines to accumulate the sensor information as training data for the target recognizer <b>210</b> determined to be applicable in step S<b>1006</b>. In accordance with this determination, the accumulation unit <b>104</b> stores and accumulates the sensor information in the storage unit <b>105</b>.</p><p id="p-0289" num="0287">After the processing of step S<b>1007</b>, the terminal device <b>11</b> returns the processing from step S<b>1004</b><i>ed </i>to step S<b>1004</b><i>st, </i>and executes the processing of steps S<b>1005</b> to S<b>1007</b> on the next recognizer <b>210</b> as the target recognizer <b>210</b> among the individual recognizers <b>210</b>.</p><p id="p-0290" num="0288">In contrast, when it is determined in step S<b>1006</b> that the target sensor information is not applicable to the target recognizer <b>210</b> (step S<b>1006</b>, &#x201c;No&#x201d;), the processing returns from step S<b>1004</b><i>ed </i>to step S<b>1004</b><i>st. </i></p><p id="p-0291" num="0289">After completion of the processing of steps S<b>1004</b><i>st </i>to S<b>1004</b><i>ed </i>for each of the recognizers <b>210</b> regarding the sensor information, the terminal device <b>11</b> proceeds to the processing of step S<b>1008</b>. In step S<b>1008</b>, the terminal device <b>11</b> determines whether the number of recognizers <b>210</b> determined by the transmission determination unit <b>107</b><i>e </i>as application targets of the sensor information in the processing of steps S<b>1004</b><i>st </i>to S<b>1004</b><i>ed </i>is sufficient (for example, whether the number exceeds a threshold).</p><p id="p-0292" num="0290">In contrast, when the transmission determination unit <b>107</b><i>e </i>has determined in step S<b>1008</b> that the number of the recognizers <b>210</b> determined as the application targets of the sensor information is sufficient (step S<b>1008</b>, &#x201c;Yes&#x201d;), the processing proceeds to step S<b>1009</b>. In step S<b>1009</b>, the transmission determination unit <b>107</b><i>e </i>performs transmission processing of the sensor information. For example, the transmission determination unit <b>107</b><i>e </i>adds information indicating each of the recognizers <b>210</b> determined to be applicable by the processing of steps S<b>1004</b><i>st </i>to S<b>1004</b><i>ed </i>to the sensor information accumulated in the storage unit <b>105</b> in step S<b>1007</b>.</p><p id="p-0293" num="0291">In the next step S<b>1010</b>, the sensor information to which the information indicating each recognizer <b>210</b> has been added is read from the storage unit <b>105</b> and then transmitted by the communication unit <b>110</b> to the server system <b>2</b> via the network <b>1</b>. The accumulation unit <b>104</b> deletes the transmitted sensor information from the storage unit <b>105</b>. The server system <b>2</b> stores the sensor information transmitted from the terminal device <b>11</b> in the storage unit <b>20</b> in association with each of the recognizers <b>210</b> as training data for performing retraining of each of the recognizers <b>210</b> based on the information indicating each of the recognizers <b>210</b> added to the sensor information.</p><p id="p-0294" num="0292">When the transmission determination unit <b>107</b><i>e </i>has determined in step S<b>1008</b> that the number of the recognizers <b>210</b> determined as the application targets of the sensor information is not sufficient (step S<b>1008</b>, &#x201c;No&#x201d;), the processing proceeds to step S<b>1011</b>.</p><p id="p-0295" num="0293">In step S<b>1011</b>, the accumulation determination unit <b>103</b> sets priority to the sensor information (referred to as new sensor information). For example, the accumulation determination unit <b>103</b> can set the priority based on the metadata included in the new sensor information. Furthermore, the accumulation determination unit <b>103</b> can set a priority to the new sensor information based on the recognition result of the recognition unit <b>101</b>. For example, the accumulation determination unit <b>103</b> can set priority to the new sensor information such that the higher the recognition rate indicated by the recognition result, the higher the priority given to the sensor information.</p><p id="p-0296" num="0294">In the next step S<b>1012</b>, the accumulation determination unit <b>103</b> determines whether the capacity in the storage unit <b>105</b> to store the new sensor information is insufficient. For example, the accumulation determination unit <b>103</b> inquires of the accumulation unit <b>104</b> about the capacity of the storage unit <b>105</b>.</p><p id="p-0297" num="0295">When it is determined in step S<b>1012</b> that there is sufficient capacity for storing new sensor information in the storage unit <b>105</b> (step S<b>1012</b>, &#x201c;No&#x201d;), the accumulation determination unit <b>103</b> proceeds to the processing of step S<b>1017</b>. In step S<b>1017</b>, the accumulation determination unit <b>103</b> instructs the accumulation unit <b>104</b> to store the new sensor information in the storage unit <b>105</b>. In response to this instruction, the storage unit <b>105</b> stores and accumulates the new sensor information in the storage unit <b>105</b>. Completion of the storage of the sensor information in the storage unit <b>105</b> will end the series of processing according to the flowchart of section (a).</p><p id="p-0298" num="0296">In contrast, when it is determined in step S<b>1012</b> that there is no sufficient capacity for storing new sensor information in the storage unit <b>105</b> (step S<b>1012</b>, &#x201c;Yes&#x201d;), the accumulation determination unit <b>103</b> proceeds to the processing of step S<b>1013</b>. In step S<b>1013</b>, the accumulation determination unit <b>103</b> instructs the accumulation unit <b>104</b> to search for the sensor information to which the lowest priority is set among the pieces of sensor information accumulated in the storage unit <b>105</b>.</p><p id="p-0299" num="0297">In the next step S<b>1014</b>, the accumulation determination unit <b>103</b> compares the priority of the sensor information obtained by the search in step S<b>1013</b> with the priority of the new sensor information. When it is determined, as a result of the comparison, that the priority of the sensor information (lowest-priority sensor information) obtained by the search in step S<b>1013</b> is higher than the priority of the new sensor information (step S<b>1014</b>, &#x201c;when the lowest priority data has higher priority&#x201d;), the series of processing according to the flowchart of section (a) will end. In this case, the new sensor information will be discarded, for example.</p><p id="p-0300" num="0298">In contrast, when the accumulation determination unit <b>103</b> has determined in step S<b>1014</b>, that the priority of the new sensor information is higher than the priority of the lowest-priority sensor information (step S<b>1014</b>, &#x201c;when the new priority data has higher priority&#x201d;), the processing proceeds to step S<b>1015</b>. In step S<b>1015</b>, the accumulation determination unit <b>103</b> instructs the accumulation unit <b>104</b> to delete the lowest-priority sensor information from the storage unit <b>105</b>. In response to this instruction, the accumulation unit <b>104</b> deletes the lowest-priority sensor information from the storage unit <b>105</b>.</p><p id="p-0301" num="0299">In the next step S<b>1016</b>, the accumulation determination unit <b>103</b> instructs the accumulation unit <b>104</b> to store the new sensor information in the storage unit <b>105</b>. In response to this instruction, the storage unit <b>105</b> stores and accumulates the new sensor information in the storage unit <b>105</b>. Completion of the storage of the sensor information in the storage unit <b>105</b> will end the series of processing according to the flowchart of section (a).</p><p id="p-0302" num="0300">The processing of section (b) will be described. The processing according to section (b) in <figref idref="DRAWINGS">FIG. <b>28</b></figref> is substantially the same as the processing according to section (b) in <figref idref="DRAWINGS">FIG. <b>26</b></figref>. Specifically, in step S<b>1030</b>, the transmission determination unit <b>107</b><i>e </i>performs connection processing of connecting to the network <b>1</b> by the communication unit <b>110</b>. In the next step S<b>1031</b>, the transmission determination unit <b>107</b><i>e </i>performs estimation processing of estimating the communication cost in the network <b>1</b>. For example, the transmission determination unit <b>107</b><i>e </i>estimates the communication cost based on a communication plan, a fee plan, a time zone, traffic of the network <b>1</b>, and the like regarding the connection to the network <b>1</b>.</p><p id="p-0303" num="0301">In the next step S<b>1032</b>, the transmission determination unit <b>107</b><i>e </i>determines whether the communication cost estimated in step S<b>1031</b> is low, for example, whether the communication cost is less than a predetermined value. In a case where the transmission determination unit <b>107</b><i>e </i>determines that the estimated communication cost is high (step S<b>1032</b>, &#x201c;No&#x201d;), the transmission determination unit <b>107</b><i>e </i>ends the series of processing in the flowchart of this section (b) without transmitting the sensor information accumulated in the storage unit <b>105</b>.</p><p id="p-0304" num="0302">In contrast, when having determined, in step S<b>1032</b>, that the communication cost estimated in step S<b>1031</b> is low (step S<b>1032</b>, &#x201c;Yes&#x201d;), the transmission determination unit <b>107</b><i>e </i>proceeds to the processing of step S<b>1033</b>. In step S<b>1033</b>, the transmission determination unit <b>107</b><i>e </i>performs transmission processing of the sensor information accumulated in the storage unit <b>105</b>. For example, the transmission determination unit <b>107</b><i>e </i>instructs the accumulation unit <b>104</b> to read the sensor information accumulated in the storage unit <b>105</b>.</p><p id="p-0305" num="0303">In the next step S<b>1034</b>, the transmission determination unit <b>107</b><i>e </i>transmits the sensor information read from the storage unit <b>105</b> by the accumulation unit <b>104</b> in response to the instruction in step S<b>1033</b> to the server system <b>2</b> via the network <b>1</b> by the communication unit <b>110</b>. The accumulation unit <b>104</b> deletes the transmitted sensor information from the storage unit <b>105</b>.</p><p id="p-0306" num="0304">In this manner, in the second modification of the third embodiment, priority is set to the sensor information that has not been transmitted to the server system <b>2</b>, and the sensor information with low priority is deleted when there is no more capacity for storing the sensor information in the storage unit <b>105</b>. This makes it possible to save the storage capacity of the storage unit <b>105</b>. In addition, since the sensor information with high priority is accumulated in the storage unit <b>105</b>, by reading the sensor information stored in the storage unit <b>105</b> at a predetermined timing and transmitting the sensor information to the server system <b>2</b>, it is possible to achieve efficient use of the sensor information, leading to the reduction of the communication cost.</p><heading id="h-0013" level="1">5. Fourth Embodiment</heading><p id="p-0307" num="0305">The technology according to the present disclosure (the present technology) is applicable to various products. The technology according to the present disclosure may be applied to devices mounted on any of moving objects such as automobiles, electric vehicles, hybrid electric vehicles, motorcycles, bicycles, personal mobility, airplanes, drones, ships, and robots.</p><p id="p-0308" num="0306"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a block diagram illustrating a schematic configuration example of a vehicle control system, which is an example of a moving body control system to which the technology according to the present disclosure is applicable.</p><p id="p-0309" num="0307">A vehicle control system <b>12000</b> includes a plurality of electronic control units connected via a communication network <b>12001</b>. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>29</b></figref>, the vehicle control system <b>12000</b> includes a drive system control unit <b>12010</b>, a body system control unit <b>12020</b>, a vehicle exterior information detection unit <b>12030</b>, a vehicle interior information detection unit <b>12040</b>, and an integrated control unit <b>12050</b>. Furthermore, as a functional configuration of the integrated control unit <b>12050</b>, a microcomputer <b>12051</b>, an audio image output unit <b>12052</b>, and an in-vehicle network interface (I/F) <b>12053</b> are illustrated.</p><p id="p-0310" num="0308">The drive system control unit <b>12010</b> controls the operation of the device related to the drive system of the vehicle in accordance with various programs. For example, the drive system control unit <b>12010</b> functions as a control device of a driving force generation device that generates a driving force of a vehicle such as an internal combustion engine or a driving motor, a driving force transmission mechanism that transmits a driving force to the wheels, a steering mechanism that adjusts steering angle of the vehicle, a braking device that generates a braking force of the vehicle, or the like.</p><p id="p-0311" num="0309">The body system control unit <b>12020</b> controls the operation of various devices mounted on the vehicle body in accordance with various programs. For example, the body system control unit <b>12020</b> functions as a control device for a keyless entry system, a smart key system, a power window device, or various lamps such as a head lamp, a back lamp, a brake lamp, a turn signal lamp, or a fog lamp. In this case, the body system control unit <b>12020</b> can receive input of radio waves transmitted from a portable device that substitutes for the key or signals from various switches. The body system control unit <b>12020</b> receives the input of these radio waves or signals and controls the door lock device, the power window device, the lamp, or the like, of the vehicle.</p><p id="p-0312" num="0310">The vehicle exterior information detection unit <b>12030</b> detects information outside the vehicle equipped with the vehicle control system <b>12000</b>. For example, an imaging unit <b>12031</b> is connected to the vehicle exterior information detection unit <b>12030</b>. The vehicle exterior information detection unit <b>12030</b> causes the imaging unit <b>12031</b> to capture an image of the exterior of the vehicle and receives the captured image. The vehicle exterior information detection unit <b>12030</b> may perform object detection processing or distance detection processing of people, vehicles, obstacles, signs, characters on the road surface, or the like based on the received image.</p><p id="p-0313" num="0311">The imaging unit <b>12031</b> is an optical sensor that receives light and outputs an electric signal corresponding to the amount of received light. The imaging unit <b>12031</b> can output the electric signal as an image and also as distance measurement information. Furthermore, the light received by the imaging unit <b>12031</b> may be visible light or invisible light such as infrared rays.</p><p id="p-0314" num="0312">The vehicle interior information detection unit <b>12040</b> detects vehicle interior information. The vehicle interior information detection unit <b>12040</b> is connected to a driver state detector <b>12041</b> that detects the state of the driver, for example. The driver state detector <b>12041</b> may include a camera that images the driver, for example. The vehicle interior information detection unit <b>12040</b> may calculate the degree of fatigue or degree of concentration of the driver or may determine whether the driver is dozing off based on the detection information input from the driver state detector <b>12041</b>.</p><p id="p-0315" num="0313">The microcomputer <b>12051</b> can calculate a control target value of the driving force generation device, the steering mechanism, or the braking device based on vehicle external/internal information obtained by the vehicle exterior information detection unit <b>12030</b> or the vehicle interior information detection unit <b>12040</b>, and can output a control command to the drive system control unit <b>12010</b>. For example, the microcomputer <b>12051</b> can perform cooperative control for the purpose of achieving a function of an advanced driver assistance system (ADAS) including collision avoidance or impact mitigation of vehicles, follow-up running based on an inter-vehicle distance, cruise control, vehicle collision warning, vehicle lane departure warning, or the like.</p><p id="p-0316" num="0314">Furthermore, it is allowable such that the microcomputer <b>12051</b> controls the driving force generation device, the steering mechanism, the braking device, or the like, based on the information regarding the surroundings of the vehicle obtained by the vehicle exterior information detection unit <b>12030</b> or the vehicle interior information detection unit <b>12040</b>, thereby performing cooperative control for the purpose of autonomous driving or the like, in which the vehicle performs autonomous traveling without depending on the operation of the driver.</p><p id="p-0317" num="0315">Furthermore, the microcomputer <b>12051</b> can output a control command to the body system control unit <b>12020</b> based on the vehicle exterior information acquired by the vehicle exterior information detection unit <b>12030</b>. For example, the microcomputer <b>12051</b> can control the head lamp in accordance with the position of the preceding vehicle or the oncoming vehicle sensed by the vehicle exterior information detection unit <b>12030</b>, and thereby can perform cooperative control aiming at antiglare such as switching the high beam to low beam.</p><p id="p-0318" num="0316">The audio image output unit <b>12052</b> transmits an output signal in the form of at least one of audio or image to an output device capable of visually or audibly notifying the occupant of the vehicle or the outside of the vehicle of information. In the example of <figref idref="DRAWINGS">FIG. <b>29</b></figref>, an audio speaker <b>12061</b>, a display unit <b>12062</b>, and an instrument panel <b>12063</b> are illustrated as exemplary output devices. The display unit <b>12062</b> may include, for example, at least one of an onboard display and a head-up display.</p><p id="p-0319" num="0317"><figref idref="DRAWINGS">FIG. <b>30</b></figref> is a diagram illustrating an example of an installation position of the imaging unit <b>12031</b>.</p><p id="p-0320" num="0318">In <figref idref="DRAWINGS">FIG. <b>30</b></figref>, a vehicle <b>12100</b> has imaging units <b>12101</b>, <b>12102</b>, <b>12103</b>, <b>12104</b>, and <b>12105</b> as the imaging units <b>12031</b>.</p><p id="p-0321" num="0319">For example, the imaging units <b>12101</b>, <b>12102</b>, <b>12103</b>, <b>12104</b>, and <b>12105</b> are installed at positions on the vehicle <b>12100</b>, including a front nose, a side mirror, a rear bumper, a back door, an upper portion of the windshield in a vehicle interior, or the like. The imaging unit <b>12101</b> provided on the front nose and the imaging unit <b>12105</b> provided on the upper portion of the windshield in the vehicle interior mainly acquire an image in front of the vehicle <b>12100</b>. The imaging units <b>12102</b> and <b>12103</b> provided in the side mirrors mainly acquire images of the side of the vehicle <b>12100</b>. The imaging unit <b>12104</b> provided on the rear bumper or the back door mainly acquires an image behind the vehicle <b>12100</b>. The images in front acquired by the imaging units <b>12101</b> and <b>12105</b> are mainly used for detecting a preceding vehicle or a pedestrian, an obstacle, a traffic light, a traffic sign, a lane, or the like.</p><p id="p-0322" num="0320">Note that <figref idref="DRAWINGS">FIG. <b>30</b></figref> illustrates an example of the imaging range of the imaging units <b>12101</b> to <b>12104</b>. An imaging range <b>12111</b> indicates an imaging range of the imaging unit <b>12101</b> provided on the front nose, imaging ranges <b>12112</b> and <b>12113</b> indicate imaging ranges of the imaging units <b>12102</b> and <b>12103</b> provided on the side mirrors, respectively, and an imaging range <b>12114</b> indicates an imaging range of the imaging unit <b>12104</b> provided on the rear bumper or the back door. For example, by superimposing pieces of image data captured by the imaging units <b>12101</b> to <b>12104</b>, it is possible to obtain a bird's-eye view image of the vehicle <b>12100</b> as viewed from above.</p><p id="p-0323" num="0321">At least one of the imaging units <b>12101</b> to <b>12104</b> may have a function of acquiring distance information. For example, at least one of the imaging units <b>12101</b> to <b>12104</b> may be a stereo camera including a plurality of imaging elements, or an imaging element having pixels for phase difference detection.</p><p id="p-0324" num="0322">For example, the microcomputer <b>12051</b> can obtain a distance to each of three-dimensional objects in the imaging ranges <b>12111</b> to <b>12114</b> and a temporal change (relative speed with respect to the vehicle <b>12100</b>) of the distance based on the distance information obtained from the imaging units <b>12101</b> to <b>12104</b>, and thereby can extract a three-dimensional object traveling at a predetermined speed (for example, 0 km/h or more) in substantially the same direction as the vehicle <b>12100</b> being the closest three-dimensional object on the traveling path of the vehicle <b>12100</b>, as a preceding vehicle. Furthermore, the microcomputer <b>12051</b> can set an inter-vehicle distance to be ensured in front of the preceding vehicle in advance, and can perform automatic brake control (including follow-up stop control), automatic acceleration control (including follow-up start control), or the like. In this manner, it is possible to perform cooperative control for the purpose of autonomous driving or the like, in which the vehicle autonomously travels without depending on the operation of the driver.</p><p id="p-0325" num="0323">For example, based on the distance information obtained from the imaging units <b>12101</b> to <b>12104</b>, the microcomputer <b>12051</b> can extract three-dimensional object data regarding the three-dimensional object with classification into three-dimensional objects, such as a two-wheeled vehicle, a regular vehicle, a large vehicle, a pedestrian, and other three-dimensional objects such as a utility pole, and can use the data for automatic avoidance of obstacles. For example, the microcomputer <b>12051</b> distinguishes obstacles around the vehicle <b>12100</b> into obstacles having high visibility to the driver of the vehicle <b>12100</b> and obstacles having low visibility to the driver. Subsequently, the microcomputer <b>12051</b> determines a collision risk indicating the risk of collision with each of obstacles. When the collision risk is a set value or more and there is a possibility of collision, the microcomputer <b>12051</b> can output an alarm to the driver via the audio speaker <b>12061</b> and the display unit <b>12062</b>, and can perform forced deceleration and avoidance steering via the drive system control unit <b>12010</b>, thereby achieving driving assistance for collision avoidance.</p><p id="p-0326" num="0324">At least one of the imaging units <b>12101</b> to <b>12104</b> may be an infrared camera that detects infrared rays. For example, the microcomputer <b>12051</b> can recognize a pedestrian by determining whether a pedestrian is present in the captured images of the imaging units <b>12101</b> to <b>12104</b>. Such pedestrian recognition is performed, for example, by a procedure of extracting feature points in a captured image of the imaging units <b>12101</b> to <b>12104</b> as an infrared camera, and by a procedure of performing pattern matching processing on a series of feature points indicating the contour of the object to discriminate whether it is a pedestrian. When the microcomputer <b>12051</b> determines that a pedestrian is present in the captured images of the imaging units <b>12101</b> to <b>12104</b> and recognizes a pedestrian, the audio image output unit <b>12052</b> controls the display unit <b>12062</b> to perform superimposing display of a rectangular contour line for emphasis to the recognized pedestrian. Furthermore, the audio image output unit <b>12052</b> may control the display unit <b>12062</b> to display an icon indicating a pedestrian or the like at a desired position.</p><p id="p-0327" num="0325">Hereinabove, an example of the vehicle control system to which the technology according to the present disclosure is applicable has been described. According to the technology of the present disclosure, the camera <b>12</b> is applicable to the imaging unit <b>12031</b>, and the terminal device <b>11</b> is applicable to the vehicle exterior information detection unit <b>12030</b> among the described configurations. By applying the technology of the present disclosure to the vehicle exterior information detection unit <b>12030</b>, it is possible to use the sensor information acquired by the camera <b>12</b> with higher efficiency, leading to improvement of the recognition rate of pedestrians and the like. Furthermore, by applying the second embodiment and its modifications and the third embodiment and its modifications to the vehicle exterior information detection unit <b>12030</b>, it is also possible to reduce the communication cost for transmitting sensor information.</p><p id="p-0328" num="0326">Note that the present technology can also have the following configurations.<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0327">(1) An information processing apparatus comprising:</li></ul></p><p id="p-0329" num="0328">a recognition unit that performs object recognition processing using sensor information acquired by a sensor, the object recognition processing being performed by a first recognizer that has been pretrained; and</p><p id="p-0330" num="0329">a training data application determination unit that determines whether the sensor information is applicable as training data to a second recognizer different from the first recognizer.<ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0330">(2) The information processing apparatus according to the above (1), further comprising</li></ul></p><p id="p-0331" num="0331">a transmission determination unit that determines, based on the sensor information, whether to transmit the sensor information to a server capable of generating the first recognizer in order to apply the sensor information to the second recognizer as the training data.<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0332">(3) The information processing apparatus according to the above (2),</li></ul></p><p id="p-0332" num="0333">wherein the transmission determination unit</p><p id="p-0333" num="0334">transmits, to the server, sensor information in which the number of recognizers capable of performing recognition processing, designated from among the second recognizers by the training data application determination unit, is a predetermined number or more, among pieces of the sensor information.<ul id="ul0004" list-style="none">    <li id="ul0004-0001" num="0335">(4) The information processing apparatus according to the above (2) or (3),</li></ul></p><p id="p-0334" num="0336">wherein the transmission determination unit</p><p id="p-0335" num="0337">transmits, to the server, the sensor information that is to be next updated in a time less than a predetermined time.<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0338">(5) The information processing apparatus according to any one of the above (2) to (4),</li></ul></p><p id="p-0336" num="0339">wherein the transmission determination unit</p><p id="p-0337" num="0340">transmits, to the server, the sensor information that occurs at a frequency less than a predetermined frequency.<ul id="ul0006" list-style="none">    <li id="ul0006-0001" num="0341">(6) The information processing apparatus according to any one of the above (2) to (5), further comprising</li></ul></p><p id="p-0338" num="0342">a communication unit capable of communicating with another information processing apparatus,</p><p id="p-0339" num="0343">wherein the transmission determination unit</p><p id="p-0340" num="0344">causes the communication unit to acquire information indicating sensor information to be transmitted to the server by the another information processing apparatus through communication with the another information processing apparatus, and determines whether to transmit the sensor information acquired by the sensor to the server based on the acquired information.<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0345">(7) The information processing apparatus according to any one of the above (2) to (6), further comprising</li></ul></p><p id="p-0341" num="0346">an accumulation determination unit that determines whether to accumulate the sensor information in an accumulation unit,</p><p id="p-0342" num="0347">wherein the accumulation determination unit</p><p id="p-0343" num="0348">accumulates the sensor information determined not to be transmitted to the server by the transmission determination unit, in the accumulation unit.<ul id="ul0008" list-style="none">    <li id="ul0008-0001" num="0349">(8) The information processing apparatus according to the above (7),</li></ul></p><p id="p-0344" num="0350">wherein the transmission determination unit</p><p id="p-0345" num="0351">transmits the sensor information accumulated in the accumulation unit to the server at a timing when a communication cost is low.<ul id="ul0009" list-style="none">    <li id="ul0009-0001" num="0352">(9) The information processing apparatus according to the above (7) or (8),</li></ul></p><p id="p-0346" num="0353">wherein the accumulation determination unit</p><p id="p-0347" num="0354">obtains a priority of the sensor information determined not to be transmitted to the server, and determines whether to accumulate the sensor information in the accumulation unit based on the priority. (10) An information processing system comprising:</p><p id="p-0348" num="0355">a server;</p><p id="p-0349" num="0356">an information processing apparatus capable of communicating with the server,</p><p id="p-0350" num="0357">wherein the server includes</p><p id="p-0351" num="0358">a training unit that generates a first recognizer and a second recognition unit different from the first recognizer by using machine learning based on training data, and</p><p id="p-0352" num="0359">the information processing apparatus includes:</p><p id="p-0353" num="0360">a recognition unit that performs, by the first recognizer, object recognition processing using sensor information acquired by a sensor; and</p><p id="p-0354" num="0361">a training data application determination unit that determines whether the sensor information is applicable as training data to a second recognizer different from the first recognizer.<ul id="ul0010" list-style="none">    <li id="ul0010-0001" num="0362">(11) An information processing method executed by a processor, the method comprising:</li></ul></p><p id="p-0355" num="0363">a recognition step of performing, by using a first recognizer that has been pretrained, object recognition processing using sensor information acquired by a sensor;</p><p id="p-0356" num="0364">a recognizer determination step of designating a recognizer capable of performing recognition processing using the sensor information out of a second recognizer different from the first recognizer based on the sensor information; and</p><p id="p-0357" num="0365">a training data application determination step of determining whether the sensor information is applicable as training data to the second recognizer different from the first recognizer.<ul id="ul0011" list-style="none">    <li id="ul0011-0001" num="0366">(12) An information processing program for causing a computer to execute processing, the processing comprising:</li></ul></p><p id="p-0358" num="0367">a recognition step of performing, by using a first recognizer that has been pretrained, object recognition processing using sensor information acquired by a sensor;</p><p id="p-0359" num="0368">a recognizer determination step of designating a recognizer capable of performing recognition processing using the sensor information out of a second recognizer different from the first recognizer based on the sensor information; and</p><p id="p-0360" num="0369">a training data application determination step of determining whether the sensor information is applicable as training data to the second recognizer different from the first recognizer.<ul id="ul0012" list-style="none">    <li id="ul0012-0001" num="0370">(13) An information processing apparatus comprising:</li></ul></p><p id="p-0361" num="0371">a training unit that generates a first recognizer and a second recognizer different from the first recognizer by using machine learning based on training data; and</p><p id="p-0362" num="0372">a training data application determination unit that determines whether sensor information transmitted from a terminal device including the first recognizer is applicable as training data to the second recognizer based on the sensor information,</p><p id="p-0363" num="0373">wherein, when the training data application determination unit has determined that the sensor information is applicable to the second recognizer as training data, the training unit retrains the second recognizer based on the sensor information and updates the second recognizer.<ul id="ul0013" list-style="none">    <li id="ul0013-0001" num="0374">(14) The information processing apparatus according to the above (13),</li></ul></p><p id="p-0364" num="0375">wherein the training data application determination unit</p><p id="p-0365" num="0376">determines whether the sensor information is applicable to the second recognizer as training data based on a scene indicated by the sensor information.<ul id="ul0014" list-style="none">    <li id="ul0014-0001" num="0377">(15) The information processing apparatus according to the above (13) or (14),</li></ul></p><p id="p-0366" num="0378">wherein the training data application determination unit</p><p id="p-0367" num="0379">determines whether conversion of the sensor information will make the sensor information applicable to the second recognizer as training data.<ul id="ul0015" list-style="none">    <li id="ul0015-0001" num="0380">(16) The information processing apparatus according to any one of the above (13) to (15),</li></ul></p><p id="p-0368" num="0381">wherein the training data application determination unit</p><p id="p-0369" num="0382">determines whether the sensor information is applicable to the second recognizer as training data according to an object detected by a sensor, the detected objected being indicated in the sensor information.<ul id="ul0016" list-style="none">    <li id="ul0016-0001" num="0383">(17) An information processing system comprising</li></ul></p><p id="p-0370" num="0384">an information processing apparatus; and</p><p id="p-0371" num="0385">a terminal device including a first recognizer that performs object recognition processing using sensor information acquired by a sensor, the terminal device being capable of communicating with the information processing apparatus;</p><p id="p-0372" num="0386">wherein the information processing apparatus includes:</p><p id="p-0373" num="0387">a training unit that generates the first recognizer and a second recognizer different from the first recognizer by using machine learning based on training data; and</p><p id="p-0374" num="0388">a training data application determination unit that determines whether sensor information transmitted from the terminal device including the first recognizer is applicable as training data to the second recognizer based on the sensor information, and</p><p id="p-0375" num="0389">when the training data application determination unit has determined that the sensor information is applicable to the second recognizer as training data, the training unit retrains the second recognizer based on the sensor information and updates the second recognizer.<ul id="ul0017" list-style="none">    <li id="ul0017-0001" num="0390">(18) An information processing method executed by a processor, the method comprising:</li></ul></p><p id="p-0376" num="0391">a training step of generating a first recognizer and a second recognizer different from the first recognizer by using machine learning based on training data; and</p><p id="p-0377" num="0392">a training data application determination step of determining whether sensor information transmitted from a terminal device including the first recognizer is applicable as training data to the second recognizer based on the sensor information,</p><p id="p-0378" num="0393">wherein, when the training data application determination step has determined that the sensor information is applicable to the second recognizer as training data, the training step retrains the second recognizer based on the sensor information and updates the second recognizer.<ul id="ul0018" list-style="none">    <li id="ul0018-0001" num="0394">(19) An information processing program that causes a computer to execute processing, the processing comprising:</li></ul></p><p id="p-0379" num="0395">a training step of generating a first recognizer and a second recognizer different from the first recognizer by using machine learning based on training data; and</p><p id="p-0380" num="0396">a training data application determination step of determining whether sensor information transmitted from a terminal device including the first recognizer is applicable as training data to the second recognizer based on the sensor information,</p><p id="p-0381" num="0397">wherein, when the training data application determination step has determined that the sensor information is applicable to the second recognizer as training data, the training step retrains the second recognizer based on the sensor information and updates the second recognizer.</p><heading id="h-0014" level="1">REFERENCE SIGNS LIST</heading><p id="p-0382" num="0398"><b>1</b> NETWORK</p><p id="p-0383" num="0399"><b>2</b> SERVER SYSTEM</p><p id="p-0384" num="0400"><b>3</b>, <b>202</b> TRAINING UNIT</p><p id="p-0385" num="0401"><b>10</b>, <b>10</b><sub>1</sub>, <b>10</b><sub>2</sub>, <b>10</b><sub>3 </sub>VEHICLE</p><p id="p-0386" num="0402"><b>11</b> TERMINAL DEVICE</p><p id="p-0387" num="0403"><b>12</b> CAMERA</p><p id="p-0388" num="0404"><b>20</b>, <b>105</b> STORAGE UNIT</p><p id="p-0389" num="0405"><b>22</b>, <b>22</b><i>a, </i><b>22</b><i>b, </i><b>22</b><i>c, </i><b>22</b><i>d, </i><b>108</b>, <b>108</b><i>a </i>TRAINING DATA APPLICATION DETERMINATION UNIT</p><p id="p-0390" num="0406"><b>101</b> RECOGNITION UNIT</p><p id="p-0391" num="0407"><b>103</b> ACCUMULATION DETERMINATION UNIT</p><p id="p-0392" num="0408"><b>104</b> ACCUMULATION UNIT</p><p id="p-0393" num="0409"><b>106</b> ACCUMULATED INFORMATION OUTPUT UNIT</p><p id="p-0394" num="0410"><b>107</b>, <b>107</b><i>a, </i><b>107</b><i>b, </i><b>107</b><i>c, </i><b>107</b><i>d, </i><b>107</b><i>e </i>TRANSMISSION DETERMINATION UNIT</p><p id="p-0395" num="0411"><b>110</b>, <b>110</b><i>a, </i><b>200</b> COMMUNICATION UNIT</p><p id="p-0396" num="0412"><b>152</b> TRAINING DATA IMMEDIACY CALCULATION UNIT</p><p id="p-0397" num="0413"><b>153</b> TRAINING DATA IMMEDIACY DETERMINATION UNIT</p><p id="p-0398" num="0414"><b>201</b> TRAINING DATA ACCUMULATION UNIT</p><p id="p-0399" num="0415"><b>210</b>, <b>210</b><sub>1</sub>, <b>210</b><sub>2</sub>, <b>210</b><sub>3</sub>, <b>210</b><sub>4 </sub>RECOGNIZER</p><p id="p-0400" num="0416"><b>221</b>, <b>250</b> METADATA ANALYSIS UNIT</p><p id="p-0401" num="0417"><b>230</b> DOMAIN ANALYSIS UNIT</p><p id="p-0402" num="0418"><b>231</b> DOMAIN EVALUATION UNIT</p><p id="p-0403" num="0419"><b>232</b>, <b>241</b>, <b>251</b> TRAINING DATA DETERMINATION UNIT</p><p id="p-0404" num="0420"><b>240</b> SENSING ANALYSIS UNIT</p><p id="p-0405" num="0421"><b>300</b> IMAGING UNIT</p><p id="p-0406" num="0422"><b>310</b> RARITY ANALYSIS UNIT</p><p id="p-0407" num="0423"><b>311</b> RARITY DETERMINATION UNIT</p><p id="p-0408" num="0424"><b>320</b> NON-TARGET-VEHICLE DATA ACCUMULATION UNIT</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An information processing apparatus comprising:<claim-text>a recognition unit that performs object recognition processing using sensor information acquired by a sensor, the object recognition processing being performed by a first recognizer that has been pretrained; and</claim-text><claim-text>a training data application determination unit that determines whether the sensor information is applicable as training data to a second recognizer different from the first recognizer.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising<claim-text>a transmission determination unit that determines, based on the sensor information, whether to transmit the sensor information to a server capable of generating the first recognizer in order to apply the sensor information to the second recognizer as the training data.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The information processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>,<claim-text>wherein the transmission determination unit</claim-text><claim-text>transmits, to the server, sensor information in which the number of recognizers capable of performing recognition processing, designated from among the second recognizers by the training data application determination unit, is a predetermined number or more, among pieces of the sensor information.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The information processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>,<claim-text>wherein the transmission determination unit</claim-text><claim-text>transmits, to the server, the sensor information that is to be next updated in a time less than a predetermined time.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The information processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>,<claim-text>wherein the transmission determination unit</claim-text><claim-text>transmits, to the server, the sensor information that occurs at a frequency less than a predetermined frequency.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The information processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising<claim-text>a communication unit capable of communicating with another information processing apparatus,</claim-text><claim-text>wherein the transmission determination unit</claim-text><claim-text>causes the communication unit to acquire information indicating sensor information to be transmitted to the server by the another information processing apparatus through communication with the another information processing apparatus, and determines whether to transmit the sensor information acquired by the sensor to the server based on the acquired information.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The information processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising<claim-text>an accumulation determination unit that determines whether to accumulate the sensor information in an accumulation unit,</claim-text><claim-text>wherein the accumulation determination unit</claim-text><claim-text>accumulates the sensor information determined not to be transmitted to the server by the transmission determination unit, in the accumulation unit.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The information processing apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>,<claim-text>wherein the transmission determination unit</claim-text><claim-text>transmits the sensor information accumulated in the accumulation unit to the server at a timing when a communication cost is low.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The information processing apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>,<claim-text>wherein the accumulation determination unit</claim-text><claim-text>obtains a priority of the sensor information determined not to be transmitted to the server, and determines whether to accumulate the sensor information in the accumulation unit based on the priority.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. An information processing system comprising:<claim-text>a server;</claim-text><claim-text>an information processing apparatus capable of communicating with the server,</claim-text><claim-text>wherein the server includes</claim-text><claim-text>a training unit that generates a first recognizer and a second recognition unit different from the first recognizer by using machine learning based on training data, and</claim-text><claim-text>the information processing apparatus includes:</claim-text><claim-text>a recognition unit that performs, by the first recognizer, object recognition processing using sensor information acquired by a sensor; and</claim-text><claim-text>a training data application determination unit that determines whether the sensor information is applicable as training data to a second recognizer different from the first recognizer.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. An information processing method executed by a processor, the method comprising:<claim-text>a recognition step of performing, by using a first recognizer that has been pretrained, object recognition processing using sensor information acquired by a sensor;</claim-text><claim-text>a recognizer determination step of designating a recognizer capable of performing recognition processing using the sensor information out of a second recognizer different from the first recognizer based on the sensor information; and</claim-text><claim-text>a training data application determination step of determining whether the sensor information is applicable as training data to the second recognizer different from the first recognizer.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. An information processing program for causing a computer to execute processing, the processing comprising:<claim-text>a recognition step of performing, by using a first recognizer that has been pretrained, object recognition processing using sensor information acquired by a sensor;</claim-text><claim-text>a recognizer determination step of designating a recognizer capable of performing recognition processing using the sensor information out of a second recognizer different from the first recognizer based on the sensor information; and</claim-text><claim-text>a training data application determination step of determining whether the sensor information is applicable as training data to the second recognizer different from the first recognizer.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. An information processing apparatus comprising:<claim-text>a training unit that generates a first recognizer and a second recognizer different from the first recognizer by using machine learning based on training data; and</claim-text><claim-text>a training data application determination unit that determines whether sensor information transmitted from a terminal device including the first recognizer is applicable as training data to the second recognizer based on the sensor information,</claim-text><claim-text>wherein, when the training data application determination unit has determined that the sensor information is applicable to the second recognizer as training data, the training unit retrains the second recognizer based on the sensor information and updates the second recognizer.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The information processing apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>,<claim-text>wherein the training data application determination unit</claim-text><claim-text>determines whether the sensor information is applicable to the second recognizer as training data based on a scene indicated by the sensor information.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The information processing apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>,<claim-text>wherein the training data application determination unit</claim-text><claim-text>determines whether conversion of the sensor information will make the sensor information applicable to the second recognizer as training data.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The information processing apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>,<claim-text>wherein the training data application determination unit</claim-text><claim-text>determines whether the sensor information is applicable to the second recognizer as training data according to an object detected by a sensor, the detected objected being indicated in the sensor information.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. An information processing system comprising<claim-text>an information processing apparatus; and</claim-text><claim-text>a terminal device including a first recognizer that performs object recognition processing using sensor information acquired by a sensor, the terminal device being capable of communicating with the information processing apparatus;</claim-text><claim-text>wherein the information processing apparatus includes:</claim-text><claim-text>a training unit that generates the first recognizer and a second recognizer different from the first recognizer by using machine learning based on training data; and</claim-text><claim-text>a training data application determination unit that determines whether sensor information transmitted from the terminal device including the first recognizer is applicable as training data to the second recognizer based on the sensor information, and</claim-text><claim-text>when the training data application determination unit has determined that the sensor information is applicable to the second recognizer as training data, the training unit retrains the second recognizer based on the sensor information and updates the second recognizer.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. An information processing method executed by a processor, the method comprising:<claim-text>a training step of generating a first recognizer and a second recognizer different from the first recognizer by using machine learning based on training data; and</claim-text><claim-text>a training data application determination step of determining whether sensor information transmitted from a terminal device including the first recognizer is applicable as training data to the second recognizer based on the sensor information,</claim-text><claim-text>wherein, when the training data application determination step has determined that the sensor information is applicable to the second recognizer as training data, the training step retrains the second recognizer based on the sensor information and updates the second recognizer.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. An information processing program that causes a computer to execute processing, the processing comprising:<claim-text>a training step of generating a first recognizer and a second recognizer different from the first recognizer by using machine learning based on training data; and</claim-text><claim-text>a training data application determination step of determining whether sensor information transmitted from a terminal device including the first recognizer is applicable as training data to the second recognizer based on the sensor information,</claim-text><claim-text>wherein, when the training data application determination step has determined that the sensor information is applicable to the second recognizer as training data, the training step retrains the second recognizer based on the sensor information and updates the second recognizer.</claim-text></claim-text></claim></claims></us-patent-application>