<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005156A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005156</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17903851</doc-number><date>20220906</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202010784956.0</doc-number><date>20200806</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>11</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>11</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0012</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20076</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30024</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">ARTIFICIAL INTELLIGENCE-BASED PATHOLOGICAL IMAGE PROCESSING METHOD AND APPARATUS, ELECTRONIC DEVICE, AND STORAGE MEDIUM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/CN2021/100910</doc-number><date>20210618</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17903851</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Tencent Technology (Shenzhen) Company Limited</orgname><address><city>Shenzhen</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>WANG</last-name><first-name>Liang</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>XIAO</last-name><first-name>Kaiwen</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>TIAN</last-name><first-name>Kuan</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>YAO</last-name><first-name>Jianhua</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">This application provides an artificial intelligence-based pathological image processing method performed by an electronic device. The method includes: determining a seed pixel of an immune cell region from a pathological image; obtaining a seed pixel mask image corresponding to the seed pixel of the immune cell region from the pathological image based on the seed pixel of the immune cell region; segmenting an epithelial cell region in the pathological image, to obtain an epithelial cell mask image of the pathological image; fusing the seed pixel mask image and the epithelial cell mask image of the pathological image, to obtain an effective seed pixel mask image corresponding to the immune cell region in the pathological image; and determining a ratio value of the immune cell region in the pathological image based on the effective seed pixel mask image.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="88.39mm" wi="138.18mm" file="US20230005156A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="196.68mm" wi="140.21mm" file="US20230005156A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="173.31mm" wi="149.44mm" file="US20230005156A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="203.45mm" wi="154.18mm" file="US20230005156A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="178.14mm" wi="155.45mm" file="US20230005156A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="198.80mm" wi="155.45mm" file="US20230005156A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="230.46mm" wi="101.35mm" file="US20230005156A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="108.97mm" wi="119.63mm" file="US20230005156A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="146.30mm" wi="123.11mm" file="US20230005156A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="218.78mm" wi="101.09mm" file="US20230005156A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="104.65mm" wi="97.54mm" file="US20230005156A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="231.39mm" wi="134.79mm" orientation="landscape" file="US20230005156A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="207.01mm" wi="92.46mm" orientation="landscape" file="US20230005156A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="208.03mm" wi="99.82mm" orientation="landscape" file="US20230005156A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="215.73mm" wi="98.64mm" orientation="landscape" file="US20230005156A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="209.38mm" wi="101.26mm" orientation="landscape" file="US20230005156A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="180.00mm" wi="149.69mm" orientation="landscape" file="US20230005156A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation application of PCT Patent Application No. PCT/CN2021/100910, entitled &#x201c;ARTIFICIAL INTELLIGENCE-BASED PATHOLOGICAL IMAGE PROCESSING METHOD AND APPARATUS, ELECTRONIC DEVICE, AND STORAGE MEDIUM&#x201d; filed on Jun. 18, 2021, which claims priority to Chinese Patent Application No. 202010784956.0, filed with the State Intellectual Property Office of the People's Republic of China on Aug. 6, 2020, and entitled &#x201c;PATHOLOGICAL IMAGE PROCESSING METHOD AND DEVICE BASED ON ARTIFICIAL INTELLIGENCE&#x201d;, all of which are incorporated herein by reference in their entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD OF THE TECHNOLOGY</heading><p id="p-0003" num="0002">This application relates to the field of artificial intelligence technologies, and in particular, to an artificial intelligence-based pathological image processing method and apparatus, an electronic device, and a computer-readable storage medium.</p><heading id="h-0003" level="1">BACKGROUND OF THE DISCLOSURE</heading><p id="p-0004" num="0003">Artificial intelligence (AI) is a comprehensive technology of computer science and deals with the design principles and implementation methods of various intelligent machines, so that the machines are capable of perceiving, reasoning, and decision-making. An AI technology is a comprehensive discipline, covering a wide range of fields including several major directions such as natural language processing and machine learning/deep learning. With the development of technologies, the AI technology will be applied in more fields and play an increasingly important role.</p><p id="p-0005" num="0004">In the related art, an effective solution of determining a ratio value (or referred to as an IC value or an immune cell score) of immune cells in a pathological image based on artificial intelligence mainly relies on manual determining on the pathological image.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">Embodiments of this application provide an artificial intelligence-based pathological image processing method and apparatus, an electronic device, and a computer-readable storage medium, which can quantify a ratio value of an immune cell in a pathological image, to improve accuracy of the ratio value of the immune cell in the pathological image.</p><p id="p-0007" num="0006">Technical solutions in the embodiments of this application are implemented as follows:</p><p id="p-0008" num="0007">An embodiment of this application provides an artificial intelligence-based pathological image processing method performed by an electronic device, the method including:</p><p id="p-0009" num="0008">determining a seed pixel corresponding to an immune cell region from a pathological image;</p><p id="p-0010" num="0009">obtaining a seed pixel mask image corresponding to the seed pixel of the immune cell region from the pathological image based on the seed pixel of the immune cell region;</p><p id="p-0011" num="0010">segmenting an epithelial cell region in the pathological image, to obtain an epithelial cell mask image of the pathological image;</p><p id="p-0012" num="0011">fusing the seed pixel mask image and the epithelial cell mask image of the pathological image, to obtain an effective seed pixel mask image corresponding to the immune cell region in the pathological image; and</p><p id="p-0013" num="0012">determining a ratio value of the immune cell region in the pathological image based on the effective seed pixel mask image.</p><p id="p-0014" num="0013">An embodiment of this application provides an artificial intelligence-based pathological image processing apparatus, including:</p><p id="p-0015" num="0014">a processing module, configured to determine a seed pixel corresponding to an immune cell region from a pathological image; and obtain a seed pixel mask image corresponding to the seed pixel of the immune cell region from the pathological image based on the seed pixel of the immune cell region;</p><p id="p-0016" num="0015">a segmentation module, configured to segment an epithelial cell region in the pathological image, to obtain an epithelial cell mask image of the pathological image;</p><p id="p-0017" num="0016">a fusion module, configured to fuse the seed pixel mask image and the epithelial cell mask image of the pathological image, to obtain an effective seed pixel mask image corresponding to the immune cell region in the pathological image; and</p><p id="p-0018" num="0017">a determining module, configured to determine a ratio value of the immune cell region in the pathological image based on the effective seed pixel mask image.</p><p id="p-0019" num="0018">An embodiment of this application provides an electronic device for processing a pathological image. The electronic device includes:</p><p id="p-0020" num="0019">a memory, configured to store executable instructions; and</p><p id="p-0021" num="0020">a processor, configured to implement the artificial intelligence-based pathological image processing method provided in the embodiments of this application when executing the executable instructions stored in the memory.</p><p id="p-0022" num="0021">An embodiment of this application provides a non-transitory computer-readable storage medium, storing executable instructions, the executable instructions, when executed by a processor of an electronic device, being used by the electronic device for implementing the artificial intelligence-based pathological image processing method provided in the embodiments of this application.</p><p id="p-0023" num="0022">The embodiments of this application have the following beneficial effects:</p><p id="p-0024" num="0023">The effective seed pixel mask image is obtained by combining the seed pixel mask image and the epithelial cell mask image, and the ratio value of the immune cell region in the pathological image is determined according to the effective seed pixel in the effective seed pixel mask image, to accurately recognize the ratio value of the immune cell region in the pathological image.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> are pathological images according to a related art.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram of an application scenario of a pathological image processing system <b>10</b> according to an embodiment of this application.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic structural diagram of an electronic device <b>500</b> for processing a pathological image according to an embodiment of this application.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> to <figref idref="DRAWINGS">FIG. <b>4</b>C</figref> are schematic flowcharts of an artificial intelligence-based pathological image processing method according to an embodiment of this application.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a seed pixel mask image according to an embodiment of this application.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is an epithelial cell mask image according to an embodiment of this application.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic structural diagram of a segmentation network according to an embodiment of this application.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a schematic structural diagram of an encoding layer according to an embodiment of this application.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic structural diagram of a decoding layer according to an embodiment of this application.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a schematic structural diagram of a segmentation network according to an embodiment of this application.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is an effective seed pixel mask image according to an embodiment of this application.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a schematic structural diagram of an artificial intelligence auxiliary model according to an embodiment of this application.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>13</b>A</figref> is an input image according to an embodiment of this application.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>13</b>B</figref> is a schematic diagram of an IC region according to an embodiment of this application.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>13</b>C</figref> is an epithelial cell region binary image according to an embodiment of this application.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>13</b>D</figref> is an IC region binary mask image according to an embodiment of this application.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>14</b>A</figref> is an input image according to an embodiment of this application.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>14</b>B</figref> is a schematic diagram of an IC region according to an embodiment of this application.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>14</b>C</figref> is an epithelial cell region binary image according to an embodiment of this application.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>14</b>D</figref> is an IC region binary mask image according to an embodiment of this application.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a schematic diagram of a consistency experiment according to an embodiment of this application.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0046" num="0045">To make the objectives, technical solutions, and advantages of this application clearer, the following describes this application in further detail with reference to the accompanying drawings. The described embodiments are not to be considered as a limitation to this application. All other embodiments obtained by a person of ordinary skill in the art without creative efforts shall fall within the protection scope of this application.</p><p id="p-0047" num="0046">In the following descriptions, the included term &#x201c;first/second&#x201d; is merely intended to distinguish similar objects but does not necessarily indicate a specific order of an object. It may be understood that &#x201c;first/second&#x201d; is interchangeable in terms of a specific order or sequence if permitted, so that the embodiments of this application described herein can be implemented in a sequence in addition to the sequence shown or described herein.</p><p id="p-0048" num="0047">Unless otherwise defined, meanings of all technical and scientific terms used in this specification are the same as those usually understood by a person skilled in the art to which this application belongs. Terms used in this specification are merely intended to describe objectives of the embodiments of this application, but are not intended to limit this application.</p><p id="p-0049" num="0048">Before the embodiments of this application are further described in detail, a description is made on terms in the embodiments of this application, and the terms in the embodiments of this application are applicable to the following explanations.</p><p id="p-0050" num="0049">(1) A convolutional neural network (CNN), as one of representative algorithms of deep learning, is a feedforward neural network (FNN) that contains convolution calculation and has a deep structure. The convolutional neural network has a representation learning capability and can perform shift-invariant classification on an input image according to a hierarchical structure thereof.</p><p id="p-0051" num="0050">(2) A programmed death 1 (PD-1), also referred to as a cluster of differentiation 279 (CD279), is a receptor protein on a surface of a T cell in an immune cell, that is, an important immunosuppressive molecule. An immune system is adjusted and tolerance of the immune system is improved by downward adjusting a reaction of the immune system to human cells and suppressing an inflammatory activity of the T cell. The PD-1 may prevents autoimmune diseases, but may also prevent the immune system from killing cancer cells.</p><p id="p-0052" num="0051">(3) A programmed cell death-ligand 1 (PD-L1) is a ligand for the PD-1 and is a first-type transmembrane protein with a size being 40 kDa. Normally, the immune system responds to foreign antigens that accumulate in the lymph nodes or spleen, promoting the proliferation of antigen-specific T cells. However, the combination of PD-1 and PD-L1 may conduct an inhibitory signal to reduce the proliferation of the T cells.</p><p id="p-0053" num="0052">(4) An immune cell value (an IC value, or referred to as an immune cell score), also referred to as an IC proportion value or an IC ratio value, is a percentage value of immune cells occupying a tumor region.</p><p id="p-0054" num="0053">(5) A patch image is an image of a region of interest (ROI) obtained from a whole slide imaging (WSI) image.</p><p id="p-0055" num="0054">(6) A pathological image is an image used for presenting a pathological morphology of organs, tissues, or cells of a body. A cause, pathogenesis and development of a disease may be explored based on the pathological image. The pathological image includes a lesion slice image (an image formed by cutting a lesion tissue with a specific size for observing a lesion change), an endoscopic image, and the like.</p><p id="p-0056" num="0055">In the related technology, evaluation of the immune efficacy of the PD-1/PD-L1 by using an immunohistochemical method has become a research focus and hotspot. The PD-L1 is related to tumor progression and poor prognosis, and the PD-L1 is considered to be an effective biomarker for predicting prognosis. For example, breast cancer is the most common malignant tumor in women, and precision medicine provides an opportunity for more refined and individualized treatment of the breast cancer. A humanized monoclonal antibody (atezolizumab) combined with a paclitaxel protein binding agent may be used for the treatment of unresectable locally advanced triple-negative breast cancer (TNBC) or metastatic TNBC, which is diagnosed with the PD-L1 (SP142). In the breast cancer treatment, a quantity of immune cells (an expression of the PD-L1) in a patient may be obtained by using the PD-L1 method, to assist the patient or a doctor in assessing the ability to resist cancer, that is, the doctor will estimate a proportion value of stained immune cells (IC) in a tumor region, and the proportion value is used for selecting a medication method for cancer treatment. Roche provides a method for interpreting an IC value (also referred to as an IC ratio value or an IC value) based on an SP142 stain. The Roche defines some image patterns with IC values being less than 1% and image patterns with IC values being greater than 1%. A patch example image (a lesion slice image) with an IC value being less than 1% is shown in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, and points <b>101</b> represent immune cells. In <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, the immune cells are sparse, indicating that a proportion of the immune cells is relatively small. A patch example image with an IC value being greater than 1% is shown in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, and points <b>102</b> represent immune cells. In <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, the immune cells are dense, indicating that a proportion of the immune cells is relatively large.</p><p id="p-0057" num="0056">However, the related art has the following problems: (1) An interpretation result is an overall result for the patch example image rather than a pixel-based result and has relatively low interpretation accuracy. (2) There are only two interpretation results, that is, greater than or equal to 1% or less than 1%, and a more precise specific IC value cannot be given. For example, a specific IC value is not given for a patch image greater than or equal to 1%.</p><p id="p-0058" num="0057">To resolve the problems, the embodiments of this application provide an artificial intelligence-based pathological image processing method and apparatus, an electronic device, and a non-transitory computer-readable storage medium, which can quantify a ratio value of an immune cell in a pathological image, to improve accuracy of the ratio value of the immune cell in the pathological image.</p><p id="p-0059" num="0058">The artificial intelligence-based pathological image processing method provided in the embodiments of this application may be implemented by a terminal/a server alone; or may be implemented by collaborating a terminal and a server. For example, the terminal independently performs the artificial intelligence-based pathological image processing method, or the terminal sends a pathological image to the server, and the server performs the artificial intelligence-based pathological image processing method according to the received pathological image.</p><p id="p-0060" num="0059">The electronic device for processing a pathological image provided in the embodiments of this application may be terminal devices or servers of various types. The server may be an independent physical server, or may be a server cluster including a plurality of physical servers or a distributed system, or may be a cloud server providing basic cloud computing services, such as a cloud service, a cloud database, cloud computing, a cloud function, cloud storage, a network service, cloud communication, a middleware service, a domain name service, a security service, a content delivery network (CDN), big data, and an artificial intelligence platform. The terminal may be a smartphone, a tablet computer, a notebook computer, a desktop computer, a smart speaker, a smart watch, or the like, but is not limited thereto. The terminal and the server may be directly or indirectly connected in a wired or wireless communication manner. This is not limited in this application.</p><p id="p-0061" num="0060">For example, the server may be a server cluster deployed on a cloud and opens an artificial intelligence cloud service (or referred to as an AI as a service (AIaaS)) to a user. An AIaaS platform may split several common AI services and provide independent or packaged services in the cloud. This service mode is similar to an AI theme store: all users may access one or more AI services provided by using the AIaaS platform through an API interface.</p><p id="p-0062" num="0061">For example, one AI service may be a pathological image processing service, that is, a pathological image processing program provided in this embodiment of this application is encapsulated in the cloud server. A user invokes the pathological image processing service in the cloud service by using the terminal (in which a client such as a medical research client or a health medical client runs), so that the server deployed on the cloud invokes the encapsulated pathological image processing program to fuse a seed pixel mask image and an epithelial cell mask image of a pathological image, to obtain an effective seed pixel mask image corresponding to an immune cell region in the pathological image, determine a ratio value of the immune cell region in the pathological image based on the effective seed pixel mask image to respond to a prediction request for the pathological image. For example, for a medical research application, an acquired pathological image is processed, to obtain a ratio value (an IC value) of an immune cell region in the pathological image, and the pathological image and the corresponding IC value are returned to the medical research client for doctors and research fellows to perform tumor treatment research based on the IC value. For a health medical application, an acquired pathological image is processed, to obtain a ratio value (an IC value) of an immune cell region in the pathological image, and required calculation continues to be performed by using the IC value as intermediate data, to assist the doctors, the research fellows, and patients in disease prediction and re-diagnosis.</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram of an application scenario of a pathological image processing system <b>10</b> according to an embodiment of this application. A terminal <b>200</b> is connected to a server <b>100</b> through a network <b>300</b>. The network <b>300</b> may be a wide area network or a local area network, or a combination of the two.</p><p id="p-0064" num="0063">The terminal <b>200</b> (in which a client such as a medical research client or a health medical client runs) may be configured to obtain a pathological image. For example, an image acquisition apparatus of the terminal <b>200</b> is configured to acquire the pathological image.</p><p id="p-0065" num="0064">In some embodiments, a pathological image processing plug-in may be implanted in the client running in the terminal and configured to implement the artificial intelligence-based pathological image processing method locally on the client. For example, after obtaining a prediction request for a pathological image (which also obtains the pathological image), the terminal <b>200</b> invokes the pathological image processing plug-in, to implement the artificial intelligence-based pathological image processing method. That is, a seed pixel mask image and an epithelial cell mask image of the pathological image are fused, to obtain an effective seed pixel mask image corresponding to an immune cell region in the pathological image, and a ratio value of the immune cell region in the pathological image is determined based on the effective seed pixel mask image to respond to the prediction request for the pathological image. For example, for a medical research application, an acquired pathological image is processed, to obtain a ratio value (an IC value) of an immune cell region in the pathological image, and the pathological image and the corresponding IC value are returned to the medical research client for research fellows to perform manual review on the pathological image with reference to the pathological image and the corresponding IC value to check whether the IC value is accurate. When it is determined that the IC value is not accurate, the IC value is corrected, and the pathological image and the corresponding corrected IC value are stored in a database, to expand the database for subsequent medical research based on abundant and accurate medical data in the database.</p><p id="p-0066" num="0065">In some embodiments, after obtaining a prediction request for a pathological image, the terminal <b>200</b> invokes a pathological image processing interface (which may be implemented in a form of a cloud service, that is, an image processing service) of the server <b>100</b>. The server <b>100</b> fuses a seed pixel mask image and an epithelial cell mask image of the pathological image, to obtain an effective seed pixel mask image corresponding to an immune cell region in the pathological image, and determines a ratio value of the immune cell region to the pathological image based on the effective seed pixel mask image to respond to the prediction request for the pathological image. For example, for a medical research application, after a research fellow inputs a pathological image into the medical research application, the medical research application automatically obtains a prediction request for the pathological image (which also obtains the pathological image), invokes the pathological image processing interface of the server <b>100</b> to process the pathological image, to obtain a ratio value (an IC value) of an immune cell region in the pathological image, queries another pathological image (a slice reference image) having an IC value the same as the IC value in a database based on the IC value, and returns the pathological image, the corresponding IC value, and the slice reference image to the medical research client. The research fellow performs manual review on the pathological image with reference to the slice reference image to check whether the IC value is accurate. When it is determined that the IC value is not accurate, the IC value is corrected, and the pathological image and the corresponding corrected IC value are stored in the database, to expand the database for subsequent medical research based on abundant and accurate medical data in the database.</p><p id="p-0067" num="0066">In some embodiments, the terminal or server may implement, by running a computer program, the artificial intelligence-based pathological image processing method provided in this embodiment of this application. For example, the computer program may be a native program or a software module in an operating system. The computer program may be a native application (APP), that is, a program that needs to be installed in the operating system before the program can run. Alternatively, the computer program may be an applet, that is, a program that can be run by just being downloaded into a browser environment; Alternatively, the computer program may be an applet that can be embedded in any APP. To sum up, the computer program may be any form of application, module or plug-in.</p><p id="p-0068" num="0067">In some embodiments, a plurality of servers may form a blockchain, and the server <b>100</b> is a node in the blockchain. There may be information connections among nodes in the blockchain, and the nodes may transmit information through the information connections. Data (for example a pathological image processing logic and an IC value) related to the artificial intelligence-based pathological image processing method provided in this embodiment of this application may be stored in the blockchain.</p><p id="p-0069" num="0068">The following describes a structure of the electronic device for processing a pathological image provided in this embodiment of this application. <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic structural diagram of an electronic device <b>500</b> for processing a pathological image according to an embodiment of this application. A description is made by using an example in which the electronic device <b>500</b> is a server. The electronic device <b>500</b> for image processing shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> includes at least one processor <b>510</b>, a memory <b>550</b>, at least one network interface <b>520</b>, and a user interface <b>530</b>. All the components in the electronic device <b>500</b> are coupled together by using a bus system <b>540</b>. It may be understood that the bus system <b>540</b> is configured to implement connection and communication between the components. In addition to a data bus, the bus system <b>540</b> further includes a power bus, a control bus, and a status signal bus. However, for ease of clear description, all types of buses are marked as the bus system <b>540</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0070" num="0069">The processor <b>510</b> may be an integrated circuit chip having a signal processing capability, for example, a general purpose processor, a digital signal processor (DSP), or another programmable logic device (PLD), discrete gate, transistor logical device, or discrete hardware component. The general purpose processor may be a microprocessor, any conventional processor, or the like.</p><p id="p-0071" num="0070">The memory <b>550</b> includes a volatile memory or a non-volatile memory, or may include a volatile memory and a non-volatile memory. The non-volatile memory may be a read only memory (ROM), or may be a random access memory (RAM). The memory <b>550</b> described in this embodiment of this application is to include any other suitable type of memories. The memory <b>550</b> includes one or more storage devices physically away from the processor <b>510</b>.</p><p id="p-0072" num="0071">In some embodiments, the memory <b>550</b> can store data to support various operations, and examples of the data include programs, modules, and data structures, or subsets or supersets thereof, as illustrated below.</p><p id="p-0073" num="0072">An operating system <b>551</b> includes a system program configured to process various basic system services and perform a hardware-related task, for example, a framework layer, a core library layer, and a driver layer, and is configured to implement various basic services and process a hardware-related task.</p><p id="p-0074" num="0073">A network communication module <b>552</b> is configured to reach another computing device through one or more (wired or wireless) network interfaces <b>520</b>. Exemplary network interfaces <b>520</b> include: Bluetooth, wireless compatible authentication (WiFi), a universal serial bus (USB), and the like.</p><p id="p-0075" num="0074">In some embodiments, the artificial intelligence-based pathological image processing apparatus provided in this embodiment of this application may be implemented by using software such as the pathological image processing plug-in in the terminal or the pathological image processing service in the server. Certainly, this is not limited thereto. The artificial intelligence-based pathological image processing apparatus provided in this embodiment of this application may be implemented as various software embodiments, which include various forms such as an application, software, a software module, a script, or code.</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an artificial intelligence-based pathological image processing apparatus <b>555</b> stored in the memory <b>550</b>. The artificial intelligence-based pathological image processing apparatus may be software in a form of a program, a plug-in, or the like, for example, a pathological image processing plug-in and include a series of modules such as a processing module <b>5551</b>, a segmentation module <b>5552</b>, a fusion module <b>5553</b>, and a determining module <b>5554</b>. The processing module <b>5551</b>, the segmentation module <b>5552</b>, the fusion module <b>5553</b>, and the determining module <b>5554</b> are configured to implement the pathological image processing function provided in this embodiment of this application.</p><p id="p-0077" num="0076">As described above, the artificial intelligence-based pathological image processing method provided in this embodiment of this application may be implemented by various electronic devices. <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is a schematic flowchart of an artificial intelligence-based pathological image processing method according to an embodiment of this application. Descriptions is provided with reference to steps shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>.</p><p id="p-0078" num="0077">Step <b>101</b>. Determine a seed pixel corresponding to an immune cell region from a pathological image.</p><p id="p-0079" num="0078">For example, a pathological image (a WSI image) is obtained. The pathological image is acquired by using an image acquisition apparatus of a terminal or the pathological image is acquired by using an image acquisition device (a camera) independent of a terminal. The pathological image is a stained image. The image acquisition device sends the acquired pathological image to the terminal. The terminal forwards the pathological image to a server. After the pathological image is stained, an immune cell in the pathological image is stained. Therefore, the server may determine a seed pixel corresponding to the immune cell from the pathological image based on a pixel value of each pixel in the pathological image. That is, a cell corresponding to the seed pixel may be the immune cell. After determining the seed pixel corresponding to the immune cell, the server may subsequently perform fusion based on the seed pixel.</p><p id="p-0080" num="0079">In some embodiments, the determining a seed pixel of an immune cell region from a pathological image includes: converting the pathological image from a first color space to a second color space, to obtain a pixel value of each pixel of the pathological image in the second color space; and determining, when a pixel value of a pixel in the second color space falls within a value range of the seed pixel, the pixel as the seed pixel corresponding to the immune cell region in the pathological image.</p><p id="p-0081" num="0080">For example, the first color space is an RGB color space, and the second color space is a YUV color space. To better implement color segmentation, the pathological image may be converted from the RGB color space to the YUV color space, to obtain a pixel value of each pixel of the pathological image in the YUV color space. For a pixel i, a pixel value of the pixel i is y, u, and v. When y&#x2208;[y<sub>1</sub>,y<sub>2</sub>], u&#x2208;[u<sub>1</sub>,u<sub>2</sub>], and v&#x2208;[v<sub>1</sub>,v<sub>2</sub>], the pixel i is a seed pixel corresponding to the immune cell region in the pathological image. The first color space is the YUV color space, and the second color space is the RGB color space. To better generalize color segmentation, the pathological image may be converted from the YUV color space to the RGB color space, to obtain a pixel value of each pixel of the pathological image in the RGB color space. For a pixel i, a pixel value of the pixel i is r, g, and b. When r&#x2208;[r<sub>1</sub>,r<sub>2</sub>], g&#x2208;[g<sub>1</sub>,g<sub>2</sub>], and b&#x2208;[b<sub>1</sub>,b<sub>2</sub>], the pixel i is a seed pixel corresponding to the immune cell region in the pathological image.</p><p id="p-0082" num="0081">Step <b>102</b>. Obtain a seed pixel mask image corresponding to the seed pixel of the immune cell region from the pathological image based on the seed pixel of the immune cell region.</p><p id="p-0083" num="0082">For example, after determining seed pixels in the pathological image, the server combines the seed pixels, to obtain a seed pixel region, and sets pixel values of all the seed pixels in the seed pixel region to 1 and pixel values of pixels (non-seed pixels) in a non-seed pixel region in the pathological image to 0. Therefore, the seed pixels and the non-seed pixels are combined, to form a seed pixel mask image corresponding to the immune cell region in the pathological image. As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, white points <b>401</b> in an image represent seed pixels, and block points in the image represent non-seed pixels. A seed pixel mask image includes the seed pixels and the non-seed pixels.</p><p id="p-0084" num="0083">Step <b>103</b>. Segment an epithelial cell region in the pathological image, to obtain an epithelial cell mask image of the pathological image.</p><p id="p-0085" num="0084">There is no obvious sequential order between step <b>101</b> and step <b>102</b> and step <b>103</b>. After obtaining the pathological image, the server segments an epithelial cell region in the pathological image based on a feature of an epithelial cell, to obtain epithelial cells in the pathological image, combines the epithelial cells, to obtain an epithelial cell region, and sets a pixel value of the epithelial cell region to 1 and a pixel value of a non-epithelial cell region in the pathological image to 0. Therefore, the epithelial cell region and the non-epithelial cell region are combined, to obtain an epithelial cell mask image of the pathological image. As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a region <b>601</b> (a white region) represents an epithelial cell region, and the other region (a black region) represents a non-epithelial cell region. An epithelial cell mask image includes the epithelial cell region and the non-epithelial cell region.</p><p id="p-0086" num="0085">In this embodiment of this application, the epithelial cell region in the pathological image may be segmented by using a segmentation network, to obtain the epithelial cell mask image of the pathological image. The segmentation network configured to segment the epithelial cell region includes an encoder and a decoder. <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is a schematic flowchart of an artificial intelligence-based pathological image processing method according to an embodiment of this application. Step <b>103</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> may be implemented by step <b>1031</b> and step <b>1032</b> shown in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>. Step <b>1031</b>. Perform feature space-based encoding on the pathological image by using the encoder, to obtain a hidden space feature of the epithelial cell region in the pathological image. Step <b>1032</b>. Perform mask space-based decoding on the hidden space feature by using the decoder, to obtain the epithelial cell mask image of the pathological image.</p><p id="p-0087" num="0086">For example, when the segmentation network includes an encoder and a decoder, feature space-based encoding processing may be performed on the pathological image by using the encoder, to implement feature extraction, so as to obtain a hidden space feature of the epithelial cell region in the pathological image. Subsequently, mask space-based decoding processing is performed on the hidden space feature of the epithelial cell region by using the decoder. Each pixel in the pathological image is mapped as a probability that a pixel is an epithelial cell pixel. When probabilities that pixels are the epithelial cell pixel are greater than an epithelial cell threshold, the pixels are determined as the epithelial cell pixels, and the determined epithelial cell pixels are combined to obtain the epithelial cell mask image of the pathological image.</p><p id="p-0088" num="0087">In some embodiments, to avoid losing a detailed feature of an epithelial cell during encoding, encoding may be performed hierarchically, that is, the encoder includes a plurality of cascaded encoding layers. the performing feature space-based encoding on the pathological image by using the encoder, to obtain a hidden space feature of the epithelial cell region in the pathological image includes: performing downsampling encoding on the pathological image by using a first encoding layer in the plurality of cascaded encoding layers; outputting an encoding result of the first encoding layer to subsequent cascaded encoding layers, and continuing to perform downsampling encoding by using the subsequent cascaded encoding layers and outputting encoding results until an encoding result is outputted to the last encoding layer; and using an encoding result outputted by the last encoding layer as the hidden space feature of the epithelial cell region in the pathological image.</p><p id="p-0089" num="0088">Continuing the foregoing example, as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, an encoder includes a plurality of cascaded encoding layers. A first encoding layer performs downsampling encoding on a pathological image and outputs an encoding result to a second encoding layer, and the second encoding layer continues to perform downsampling encoding and outputs an encoding result until an encoding result is outputted to an N<sup>th </sup>encoding layer. N is a total quantity of the plurality of cascaded encoding layers. The following processing operations are performed by using a j<sup>th </sup>encoding layer in the plurality of cascaded encoding layers, j being an increasing natural number and 2&#x3c;j&#x3c;N: performing encoding on an encoding result outputted by a (j&#x2212;1)<sup>th </sup>encoding layer, to obtain a first encoding result of the j<sup>th </sup>encoding layer; combining the first encoding result of the j<sup>th </sup>encoding layer and the encoding result outputted by the (j&#x2212;1)<sup>th </sup>encoding layer, to obtain a combination result of the j<sup>th </sup>encoding layer; performing encoding on the combination result of the j<sup>th </sup>encoding layer, to obtain a second encoding result of the j<sup>th </sup>encoding layer; combining the second encoding result of the j<sup>th </sup>encoding layer and the combination result of the j<sup>th </sup>encoding layer, to obtain an encoding result of the j<sup>th </sup>encoding layer; and outputting the encoding result of the j<sup>th </sup>encoding layer to a (j+1)<sup>th </sup>encoding layer.</p><p id="p-0090" num="0089">For example, as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, a (j&#x2212;1)<sup>th </sup>encoding layer outputs an encoding result of the (j&#x2212;1)<sup>th </sup>encoding layer to a j<sup>th </sup>encoding layer. The j<sup>th </sup>encoding layer first performs first encoding on the encoding result of the (j&#x2212;1)<sup>th </sup>encoding layer (the first encoding is used for implementing a downsampling function), to obtain a first encoding result of the j<sup>th </sup>encoding layer. Subsequently, the j<sup>th </sup>encoding layer combines the first encoding result of the j<sup>th </sup>encoding layer and the encoding result outputted by the (j&#x2212;1)<sup>th </sup>encoding layer, to obtain a combination result of the j<sup>th </sup>encoding layer. Next, the j<sup>th </sup>encoding layer performs second encoding processing (the second encoding is used for implementing a feature extraction function, for example, a convolution operation of which a convolution kernel is 3*3 is first performed, and then the convolution operation of which the convolution kernel is 3*3 is performed again) on the combination result of the j<sup>th </sup>encoding layer, to obtain a second encoding result of the j<sup>th </sup>encoding layer. Then, the j<sup>th </sup>encoding layer combines the second encoding result of the j<sup>th </sup>encoding layer and the combination result of the j<sup>th </sup>encoding layer, to obtain an encoding result of the j<sup>th </sup>encoding layer. Finally, the j<sup>th </sup>encoding layer outputs the encoding result of the j<sup>th </sup>encoding layer to a (j+1)<sup>th </sup>encoding layer.</p><p id="p-0091" num="0090">In some embodiments, the performing encoding on an encoding result outputted by a (j&#x2212;1)<sup>th </sup>encoding layer, to obtain a first encoding result of the j<sup>th </sup>encoding layer includes: performing downsampling processing on the encoding result outputted by the (j&#x2212;1)<sup>th </sup>encoding layer, to obtain a downsampling result of the j<sup>th </sup>encoding layer; and performing convolution processing on the downsampling result of the j<sup>th </sup>encoding layer, to obtain the first encoding result of the j<sup>th </sup>encoding layer.</p><p id="p-0092" num="0091">Continuing the foregoing example, as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the j<sup>th </sup>encoding layer performs downsampling processing on the encoding result outputted by the (j&#x2212;1)<sup>th </sup>encoding layer, to obtain a downsampling result of the j<sup>th </sup>encoding layer; and then performs a convolution operation of which a convolution kernel is 3*3 on the downsampling result of the j<sup>th </sup>encoding layer, to obtain the first encoding result of the j<sup>th </sup>encoding layer.</p><p id="p-0093" num="0092">In some embodiments, to avoid losing the hidden space feature of the epithelial cell region, decoding may be performed hierarchically, that is, the decoder includes a plurality of cascaded decoding layers. the performing mask space-based decoding on the hidden space feature by using the decoder, to obtain the epithelial cell mask image of the pathological image includes: performing upsampling decoding on the hidden space feature by using a first decoding layer in the plurality of cascaded decoding layers; outputting a decoding result of the first decoding layer to subsequent cascaded decoding layers, and continuing to perform upsampling decoding by using the subsequent cascaded decoding layers and outputting decoding results until a decoding result is outputted to the last decoding layer; and mapping a decoding result outputted by the last decoding layer to a mask space, and using an obtained mapping result as the epithelial cell mask image of the pathological image.</p><p id="p-0094" num="0093">Continuing the foregoing example, as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a decoder includes a plurality of cascaded decoding layers. A first decoding layer performs upsampling decoding on the hidden space feature and outputs a decoding result to a second decoding layer, and the second decoding layer continues to perform upsampling decoding and outputs a decoding result until a decoding result is outputted to an N<sup>th </sup>decoding layer. N is a total quantity of the plurality of cascaded decoding layers. The following processing operations are performed by using an i<sup>th </sup>decoding layer in the plurality of cascaded decoding layers, i being an increasing natural number and 2&#x2264;i&#x3c;N: performing upsampling processing on a decoding result outputted by an (i&#x2212;1)<sup>th </sup>decoding layer, to obtain an upsampling result of the i<sup>th </sup>decoding layer; performing convolution processing on the upsampling result of the i<sup>th </sup>decoding layer, to obtain a decoding result of the i<sup>th </sup>decoding layer; and outputting the decoding result of the i<sup>th </sup>decoding layer to an (i+1)<sup>th </sup>decoding layer.</p><p id="p-0095" num="0094">In some embodiments, the performing upsampling processing on a decoding result outputted by an (i&#x2212;1)<sup>th </sup>decoding layer, to obtain an upsampling result of the i<sup>th </sup>decoding layer includes: performing convolution processing on the decoding result outputted by the (i&#x2212;1)<sup>th </sup>decoding layer, to obtain a first convolution result of the i<sup>th </sup>decoding layer; and performing full convolution processing on the first convolution result of the i<sup>th </sup>decoding layer, to obtain the upsampling result of the i<sup>th </sup>decoding layer.</p><p id="p-0096" num="0095">Continuing to the foregoing example, as shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, an (i&#x2212;1)<sup>th </sup>decoding layer outputs a decoding result of the (i&#x2212;1)<sup>th </sup>decoding layer to an i<sup>th </sup>decoding layer. The i<sup>th </sup>decoding layer first performs convolution processing (for example, a convolution operation of which a convolution kernel is 1*1) on the decoding result outputted by the (i&#x2212;1)<sup>th </sup>decoding layer, to obtain a first convolution result of the i<sup>th </sup>decoding layer, and then performs full convolution processing (for example, upsampling of which a convolution kernel is 3*3 is first performed, and then a convolution operation of a convolution kernel is 1*1 is performed) on the first convolution result of the i<sup>th </sup>decoding layer, to obtain an upsampling result of the i<sup>th </sup>decoding layer. The i<sup>th </sup>decoding layer outputs a decoding result to an (i+1)<sup>th </sup>decoding layer.</p><p id="p-0097" num="0096">In some embodiments, when the decoder includes a plurality of cascaded decoding layers and a cross-layer connection exists between a decoding layer and an encoding layer that are in a same layer, the performing mask space-based decoding on the hidden space feature by using the decoder, to obtain the epithelial cell mask image of the pathological image includes: performing, by using a first decoding layer in the plurality of cascaded decoding layers, upsampling decoding on the hidden space feature outputted by an encoding layer that is in a cross-layer connection with the first decoding layer, and outputting a decoding result of the first decoding layer to a second decoding layer; splicing, by using the second decoding layer, the decoding result of the first decoding layer and an encoding result outputted by an encoding layer that is in a cross-layer connection with the second decoding layer, to obtain a splicing result of the second decoding layer; performing upsampling decoding on the splicing result of the second decoding layer by using the second decoding layer; outputting a decoding result of the second decoding layer to subsequent cascaded decoding layers, and continuing to perform splicing and upsampling decoding by using the subsequent cascaded decoding layers and outputting decoding results until a decoding result is outputted to the last decoding layer; and mapping a decoding result outputted by the last decoding layer to a mask space, and using an obtained mapping result as the epithelial cell mask image of the pathological image.</p><p id="p-0098" num="0097">For example, as shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, when the segmentation network has four encoding layers and four decoding layers, a first decoding layer performs upsampling decoding on a hidden space feature outputted by a fourth encoding layer that is in a cross-layer connection with the first decoding layer and outputs a decoding result of the first decoding layer to a second decoding layer. The decoding result of the first decoding layer and an encoding result outputted by a third encoding layer that is in a cross-layer connection with the second decoding layer are spliced by using the second decoding layer, to obtain a splicing result of the second decoding layer. Upsampling decoding is performed on the splicing result of the second decoding layer by using the second decoding layer, and a decoding result of the second decoding layer is outputted to a third decoding layer. The decoding result of the second decoding layer and an encoding result outputted by a second encoding layer that is in a cross-layer connection with the third decoding layer are spliced by using the third decoding layer, to obtain a splicing result of the third decoding layer. Upsampling decoding is performed on the splicing result of the third decoding layer by using the third decoding layer, and a decoding result of the third decoding layer is outputted to a fourth decoding layer. Finally, a decoding result outputted by the fourth decoding layer is mapped to a mask space, and an obtained mapping result is used as the epithelial cell mask image of the pathological image.</p><p id="p-0099" num="0098">Step <b>104</b>. Fuse the seed pixel mask image and the epithelial cell mask image of the pathological image, to obtain an effective seed pixel mask image corresponding to the immune cell region in the pathological image.</p><p id="p-0100" num="0099">The seed pixel mask image includes stained immune cells and stained epithelial cells, and the epithelial cell mask image includes the stained epithelial cells. Therefore, to remove the stained epithelial cells from the seed pixel mask image, after obtaining the seed pixel mask image and the epithelial cell mask image, the server may fuse the seed pixel mask image and the epithelial cell mask image and remove the epithelial cells in the epithelial cell mask image from the seed pixel mask image, to form an effective seed pixel mask image corresponding to the immune cell region in the pathological image.</p><p id="p-0101" num="0100">For example, as shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a seed pixel mask image and an epithelial cell mask image are fused, to obtain an effective seed pixel mask image. White points <b>1001</b> in <figref idref="DRAWINGS">FIG. <b>11</b></figref> are less than the white points <b>401</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, that is, pixels corresponding to epithelial cells are removed from the seed pixel mask image, to form the effective seed pixel mask image including only effective seed pixels (immune cell pixels).</p><p id="p-0102" num="0101"><figref idref="DRAWINGS">FIG. <b>4</b>C</figref> is a schematic flowchart of an artificial intelligence-based pathological image processing method according to an embodiment of this application. Step <b>104</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> may be implemented by step <b>1041</b> to step <b>1043</b> shown in <figref idref="DRAWINGS">FIG. <b>4</b>C</figref>. The following processing operations are performed for each pixel in the epithelial cell mask image of the pathological image. Step <b>1041</b>. Perform negation on a pixel value of a pixel, to obtain a negation value of the pixel. Step <b>1042</b>. Perform an AND logical operation on a pixel value of a pixel corresponding to the pixel in the seed pixel mask image and the negation value of the pixel, and use a result of the AND logical operation as an effective seed pixel of the immune cell region in the pathological image. Step <b>1043</b>. Obtain the effective seed pixel mask image corresponding to the immune cell region from the pathological image based on a plurality of effective seed pixels of the immune cell region in the pathological image.</p><p id="p-0103" num="0102">For example, after obtaining the seed pixel mask image and the epithelial cell mask image (a size of the seed pixel mask image is the same as a size of the epithelial cell mask image and a size of the pathological image), the server performs the following processing operations for any pixel k in the epithelial cell mask image of the pathological image: performing negation on a pixel value of the pixel k, to obtain a negation value of the pixel k, performing an AND logical operation on a pixel value of a pixel corresponding to the pixel k in the seed pixel mask image and the negation value of the pixel k, using the pixel k as an effective seed pixel of the immune cell region in the pathological image when a result of the AND logical operation is 1, that is, a pixel value of the effective seed pixel is 1, and combining the effective seed pixel in the pathological image and a pixel (a non-effective seed pixel) other than the effective seed pixel, to obtain an effective seed pixel mask image corresponding to the immune cell region in the pathological image.</p><p id="p-0104" num="0103">For example, when the pixel value of the pixel k in the epithelial cell mask image is 1 (which indicates that the corresponding pixel k in the pathological image is an epithelial cell pixel), and the pixel value of the pixel corresponding to the pixel k in the seed pixel mask image is 1 (which indicates that the corresponding pixel k in the pathological image may be a stained immune cell pixel), negation is performed on the pixel value of the pixel k in the epithelial cell mask image, and the AND logical operation is performed on the pixel value of the pixel corresponding to the pixel k in the seed pixel mask image and the negation value of the pixel k, to obtain a result of the AND logical operation of 0, indicating that the pixel k is a non-immune cell pixel in the pathological image. When the pixel value of the pixel k in the epithelial cell mask image is 0 (which indicates that the corresponding pixel k in the pathological image is a non-epithelial cell pixel), and the pixel value of the pixel corresponding to the pixel k in the seed pixel mask image is 1 (which indicates that the corresponding pixel k in the pathological image may be the stained immune cell pixel), negation is performed on the pixel value of the pixel k in the epithelial cell mask image, and the AND logical operation is performed on the pixel value of the pixel corresponding to the pixel k in the seed pixel mask image and the negation value of the pixel k, to obtain a result of the AND logical operation of 1, indicating that the pixel k is the immune cell pixel in the pathological image, that is, the effective seed pixel.</p><p id="p-0105" num="0104">In some embodiments, to avoid detecting whether all pixels in the pathological image are effective seed pixels, only some pixels are detected. Therefore, the fusing the seed pixel mask image and the epithelial cell mask image of the pathological image, to obtain an effective seed pixel mask image corresponding to the immune cell region in the pathological image includes: determining a seed pixel region in the seed pixel mask image; determining the epithelial cell region in the epithelial cell mask image of the pathological image; and removing the epithelial cell region from the seed pixel region, to obtain the effective seed pixel mask image corresponding to the immune cell region in the pathological image.</p><p id="p-0106" num="0105">For example, a seed pixel region (that is, a region formed by pixels of which corresponding pixel values are 1) in the seed pixel mask image includes stained immune cells and stained epithelial cells, a non-seed pixel region definitely does not include the stained immune cells, and an epithelial cell region in the epithelial cell mask image definitely includes epithelial cells. Therefore, only the seed pixel region needs to be detected, to remove the stained epithelial cells. That is, the epithelial cell region is removed from the seed pixel region, to obtain the effective seed pixel mask image corresponding to the immune cell region in the pathological image.</p><p id="p-0107" num="0106">In some embodiments, the removing the epithelial cell region from the seed pixel region, to obtain the effective seed pixel mask image corresponding to the immune cell region in the pathological image includes: performing the following processing operations for each pixel in the seed pixel region: performing negation on a pixel value of a corresponding pixel in the epithelial cell region, to obtain a negation value of the pixel; performing an AND logical operation on a pixel value of a pixel in the seed pixel region and the negation value of the pixel, and using a result of the AND logical operation as an effective seed pixel of the immune cell region in the pathological image; and obtaining the effective seed pixel mask image corresponding to the immune cell region from the pathological image based on a plurality of effective seed pixels of the immune cell region in the pathological image.</p><p id="p-0108" num="0107">For example, when a pixel value of a pixel g in the seed pixel region is 1 (which indicates that the corresponding pixel g in the pathological image may be a stained immune cell pixel), and a pixel value of a pixel corresponding to the pixel g in the epithelial cell region is 1 (which indicates that the corresponding pixel g in the pathological image is a stained epithelial cell pixel), negation is performed on a pixel value of a pixel g in the epithelial cell region, and an AND logical operation is performed on the pixel value of the corresponding pixel g in the seed pixel region and the negation value of the pixel g, to obtain a result of the AND logical operation of 0, indicating that the pixel g is a non-immune cell pixel in the pathological image. When the pixel value of the pixel g in the seed pixel region is 1 (which indicates that the corresponding pixel g in the pathological image may be the stained immune cell pixel), and the pixel value of the pixel corresponding to the pixel g in the epithelial cell region is 0 (which indicates that the corresponding pixel g in the pathological image is a non-epithelial cell pixel), negation is performed on the pixel value of the pixel g in the epithelial cell region, and the AND logical operation is performed on the pixel value of the corresponding pixel g in the seed pixel region and the negation value of the pixel g, to obtain a result of the AND logical operation of 1, indicating that the pixel g is an immune cell pixel in the pathological image, that is, the effective seed pixel.</p><p id="p-0109" num="0108">Step <b>105</b>. Determine a ratio value of the immune cell region in the pathological image based on the effective seed pixel mask image.</p><p id="p-0110" num="0109">For example, after obtaining the effective seed pixel mask image, the server determines a quantity of effective seed pixels in the effective seed pixel mask image and determines a quantity of all pixels in the effective seed pixel mask image, and finally uses a ratio of the quantity of effective seed pixels to the quantity of all pixels as a ratio value of the immune cell region in the pathological image, that is, an accurate IC value may be obtained, to quantify the IC value.</p><p id="p-0111" num="0110">After obtaining the IC value, the server may query another pathological image (a slice reference image) having an IC value the same as the IC value in a database based on the IC value and return the pathological image, the corresponding IC value, and the slice reference image to a health medical client, to assist a doctor and a patient in disease prediction and re-diagnosis. The artificial intelligence-based pathological image processing method provided in this embodiment of this application is not aimed at living or animal bodies, and is not for a direct purpose of obtaining disease diagnosis results or health conditions. Therefore, the disease diagnosis results or the health conditions cannot be directly obtained according to the IC value, and the IC value is only used for assisting the doctor and the patient in disease prediction and re-diagnosis.</p><p id="p-0112" num="0111">In some embodiments, to effectively remove noise of the effective seed pixel mask image, the determining a ratio value of the immune cell region in the pathological image based on the effective seed pixel mask image includes: performing morphological processing on the effective seed pixel mask image, to obtain a morphological effective seed pixel mask image; determining a first quantity of effective seed pixels in the morphological effective seed pixel mask image; determining a second quantity of all pixels in the morphological effective seed pixel mask image; and using a ratio of the first quantity of effective seed pixels to the second quantity of all pixels as the ratio value of the immune cell region in the pathological image.</p><p id="p-0113" num="0112">For example, an opening operation is first performed on the effective seed pixel mask image, to remove an isolated abnormal value greater than an intra-field point in the effective seed pixel mask image, and then a dilation operation is performed, to remove an isolated abnormal value less than the intra-field point in the effective seed pixel mask image, to obtain a morphological effective seed pixel mask image, thereby implementing a denoising function. After obtaining the morphological effective seed pixel mask image, the server determines a quantity of effective seed pixels in the morphological effective seed pixel mask image and determines a quantity of all pixels in the morphological effective seed pixel mask image, and finally uses a ratio of the quantity of effective seed pixels to the quantity of all pixels as the ratio value of the immune cell region in the pathological image, that is, an accurate IC value may be obtained, to quantify the IC value.</p><p id="p-0114" num="0113">The following describes an exemplary application of this embodiment of this application in an actual application scenario.</p><p id="p-0115" num="0114">This embodiment of this application is applicable to application scenarios of various medical research. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the terminal <b>200</b> is connected to the server <b>100</b> deployed in the cloud through the network <b>300</b>, and a medical research application is installed on the terminal <b>200</b>. After a prediction request for a pathological image (a WSI image) is obtained, a pathological image processing interface of the server <b>100</b> is invoked. The server <b>100</b> performs a series of processing operations on the pathological image according to the prediction request, to obtain an IC value for doctors and research fellows to perform tumor treatment research based on the IC value.</p><p id="p-0116" num="0115">The related art has the following problems: (1) An interpretation result is an overall result for the patch example image rather than a pixel-based result and has relatively low interpretation accuracy. (2) There are only two interpretation results, that is, greater than or equal to 1% or less than 1%, and a more precise specific IC value cannot be given. For example, a specific IC value is not given for a patch image greater than or equal to 1%.</p><p id="p-0117" num="0116">To resolve the problems, an embodiment of this application provides an immune cell auxiliary interpretation method (that is, the artificial intelligence-based pathological image processing method) for a breast PD-L1 method. Stained ICs in an epithelial cell region are filtered, to obtain effective IC seed pixels, the effective IC seed pixels are converted into a mask image for IC scoring, and an IC ratio value is obtained based on the mask image, to assist a doctor in interpreting the IC value and help doctor improve accuracy of IC value interpretation. Simultaneously, consistency between IC values interpreted by group doctors is improved.</p><p id="p-0118" num="0117">Medically, there is a clinically meaningful improvement in a PD-L1 expression in 1% or more of population. Therefore, an objective, reproducible, and accurate PD-L1 assessment method is required to assist in diagnosis of the doctor. In this embodiment of this application, a current situation may be improved based on digital image analysis. In this embodiment of this application, cell positions of ICs and a proportion of the ICs in a tumor region may be predicted, to obtain an IC value. During the diagnosis of pathologists, on the basis of viewing a patch image of WSI and the IC value estimated by using the auxiliary method at the same time, the IC value is scored, thereby improving the accuracy of IC scoring. Simultaneously, in this embodiment of this application, a problem that a plurality of doctors performs scoring inconsistently may further be resolved.</p><p id="p-0119" num="0118">As shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the auxiliary interpretation method (an AI auxiliary model) provided in this embodiment of this application includes several steps, which are as follows: Step (1) Detect pixels of stained ICs, that is, seed pixels of a to-be-detect complete IC region. Step (2) Segment an epithelial cell region, where stained ICs in the epithelial cell region are invalid according to a definition of an effective IC and need to be filtered, to form effective IC seed pixels. Step (3) Perform denoising on the effective IC seed pixels and convert the effective IC seed pixels into a mask image used for IC scoring. IC value fitting may be performed by using image sin a Roche guideline document as a training data set, that is, a morphological operation is iterated, and finally an IC region is obtained to further obtain an IC ratio value. The following three steps in the auxiliary interpretation method are explained:</p><p id="p-0120" num="0119">Step (1) Detection of Pixels of Stained Immune Cells (ICs)</p><p id="p-0121" num="0120">As shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, an input of this portion is a patch image of WSI, and an output is a binary mask image M<sub>seed</sub>(x,y), representing a seed pixel in an IC region.</p><p id="p-0122" num="0121">By learning the image in the Roche guideline document, a color range of an RGB color space of the stained IC pixels is obtained, that is, R&#x2208;[r<sub>1</sub>,r<sub>2</sub>], G&#x2208;[g<sub>1</sub>,g<sub>2</sub>], and B&#x2208;[b<sub>1</sub>,b<sub>2</sub>], r, g, and b being pixel values. In order to better implement color segmentation, the ICs may be converted from the RGB color space to a YUV color space to obtain a value range of the IC pixels in the YUV color space, that is, Y&#x2208;[y<sub>1</sub>,y<sub>2</sub>], U&#x2208;[u<sub>1</sub>,u<sub>2</sub>], and V&#x2208;[v<sub>1</sub>,v<sub>2</sub>] and in the patch image, pixels belonging to Y&#x2208;[y<sub>1</sub>,y<sub>2</sub>], U&#x2208;[u<sub>1</sub>,u<sub>2</sub>], and V&#x2208;[v<sub>1</sub>,v<sub>2</sub>] are used as white points (pixel values are 1) for outputting the binary mask image. Pixels that do not belong to Y&#x2208;[y<sub>1</sub>,y<sub>2</sub>], U&#x2208;[u<sub>1</sub>,u<sub>2</sub>], and V&#x2208;[v<sub>1</sub>,v<sub>2</sub>] are used as black points (pixel values are 0) for outputting the binary mask image. Herein, the outputted binary mask image of the seed pixels in the IC region is used as M<sub>seed</sub>(x,y) (a seed pixel mask image).</p><p id="p-0123" num="0122">Step (2) Epithelial Cell Region Segmentation</p><p id="p-0124" num="0123">As shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, an input of this portion is a patch image of WSI, and an output is an epithelial cell region binary image M<sub>epithelium</sub>(x,y) and an effective IC seed region M<sub>effitive</sub>(x,y).</p><p id="p-0125" num="0124">By using a pre-trained deep learning network (for example, an encoder representation-based high-efficient semantic segmentation network), pixel-level segmentation is performed on an epithelial cell region of the patch image, and the encoder representation-based high-efficient semantic segmentation network may well balance calculation efficiency and precision, and can quickly and effectively perform inference under the condition of ensuring reliable precision.</p><p id="p-0126" num="0125">Epithelial cell detection is performed by using the encoder representation-based high-efficient semantic segmentation network (a segmentation network model). A training data set used herein is 2700 or more labeled immunohistochemical images with image parameters being 832*832 pixels, and each pixel length is 0.848 micrometers. The segmentation network model is from labeling data of three modalities of an immunohistochemical estrogen receptor (ER), a progesterone receptor (PR), and an antigen KI-67. Image features of epithelial regions in the ER modality, the PR modality, and the KI-67 modality are similar to an image feature of an epithelial region in a PD-L1 modality from an image level. Therefore, in the absence of labeling and training data of the epithelial region in the PD-L1 modality, the epithelial region of PD-L1 modality data may be directly detected by using an epithelial segmentation model trained by using other immunohistochemical modalities, or the epithelial region of the PD-L1 may be well obtained.</p><p id="p-0127" num="0126">Herein, the output epithelial cell region binary image is used as M<sub>epithelium</sub>(x,y) (an epithelial cell mask image). Further, an intersection of the two mask images of M<sub>seed</sub>(x,y) and the negated M<sub>epithelium</sub>(x,y) is calculated, to obtain an effective IC seed region binary mask image M<sub>effitive</sub>(x,y) (an effective seed pixel mask image). A calculation formula of M<sub>effitive</sub>(x,y) is shown in formula (1):</p><p id="p-0128" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>M</i><sub>effitive</sub>(<i>x,y</i>)=<i>M</i><sub>seed</sub>(<i>x,y</i>)&#x2229;Inv(<i>M</i><sub>epithelium</sub>(<i>x,y</i>))&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0129" num="0127">where Inv[ ] represents a negation operation.</p><p id="p-0130" num="0128">Step (3) IC Region Calculation and IC Value Obtaining</p><p id="p-0131" num="0129">As shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, an input of this portion is an IC seed region binary mask image, and an output is an IC region binary mask image M<sub>IC</sub>(x,y) and an IC value.</p><p id="p-0132" num="0130">Based on M<sub>epithelium</sub>(x,y), image morphological operations (an opening operation and a dilation operation) are performed. The dilation operation is used for eliminating the isolated abnormal values less than the intra-field point, and the opening operation is used for eliminating the isolated abnormal value greater than the intra-field point. Then, a ratio of a white region on M<sub>epithelium</sub>(x,y) after the image morphological operations occupying the entire image is calculated. Herein, the entire image is regarded as a tumor region, and therefore the obtained ratio value is an estimated IC value. Step 1 and step 2 are performed by using the image in the Roche guideline document to obtain a preliminary IC value. If a difference between the IC value and the image in the Roche guideline document is relatively large (the difference is greater than i %), the image morphological operations (opening and dilation) are further iterated. It is determined that an iteration parameter used in this embodiment of this application is two times and a size of a kernel used for the morphological operations is 3*3. A final IC region binary mask image M<sub>IC</sub>(x,y) is obtained based on the parameter. Further, an IC value calculation formula is shown in formula (2):</p><p id="p-0133" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>IC</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mi>score</mi>     </mrow>     <mo>=</mo>     <mfrac>      <mrow>       <mi>sum_pixel</mi>       <mo>[</mo>       <mrow>        <mrow>         <msub>          <mi>M</mi>          <mi>IC</mi>         </msub>         <mo>(</mo>         <mrow>          <mi>x</mi>          <mo>,</mo>          <mi>y</mi>         </mrow>         <mo>)</mo>        </mrow>        <mo>=</mo>        <mn>1</mn>       </mrow>       <mo>]</mo>      </mrow>      <mrow>       <mi>sum_pixel</mi>       <mo>[</mo>       <mrow>        <msub>         <mi>M</mi>         <mi>IC</mi>        </msub>        <mo>(</mo>        <mrow>         <mi>x</mi>         <mo>,</mo>         <mi>y</mi>        </mrow>        <mo>)</mo>       </mrow>       <mo>]</mo>      </mrow>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0134" num="0131">where sum_pixel[ ] represents that a quantity of pixels in an image is accumulated and calculated and an IC score represents an IC value.</p><p id="p-0135" num="0132">The IC value result predicted in this embodiment of this application may be used for providing auxiliary information to a doctor, so as to calculate a final IC value result.</p><p id="p-0136" num="0133">Through some experiments and analysis, it is determined that this embodiment of this application can help the doctor improve IC value interpretation accuracy. Simultaneously, consistency between IC values interpreted by group doctors is improved. Beneficial effects are as follows:</p><p id="p-0137" num="0134">As shown in <figref idref="DRAWINGS">FIG. <b>13</b>A</figref>, a patch image is inputted. The patch image in <figref idref="DRAWINGS">FIG. <b>13</b>A</figref> is detected, to detect a black region <b>1301</b> shown in <figref idref="DRAWINGS">FIG. <b>13</b>B</figref>. The black region <b>1301</b> represents a detected IC region. The patch image in <figref idref="DRAWINGS">FIG. <b>13</b>A</figref> is segmented, to obtain an epithelial cell region binary image M<sub>epithelium</sub>(x,y) shown in <figref idref="DRAWINGS">FIG. <b>13</b>C</figref>. A white region <b>1302</b> represents an epithelial cell region. Image morphological operations are performed based on M<sub>epithelium</sub>(x,y), to obtain an IC region binary mask image M<sub>IC</sub>(x,y) shown in <figref idref="DRAWINGS">FIG. <b>13</b>D</figref>. A white region <b>1303</b> represents an IC region including seed pixels. According to the algorithm in this embodiment of this application, an estimated IC value is obtained as 0.14% (an actual IC value&#x3c;1%), that is, the estimation is accurate.</p><p id="p-0138" num="0135">As shown in <figref idref="DRAWINGS">FIG. <b>14</b>A</figref>, a patch image is inputted. The patch image in <figref idref="DRAWINGS">FIG. <b>14</b>A</figref> is detected, to detect a black region <b>1401</b> shown in <figref idref="DRAWINGS">FIG. <b>14</b>B</figref>. The black region <b>1401</b> represents a detected IC region. The patch image in <figref idref="DRAWINGS">FIG. <b>14</b>A</figref> is segmented, to obtain an epithelial cell region binary image M<sub>epithelium</sub>(x,y) shown in <figref idref="DRAWINGS">FIG. <b>14</b>C</figref>. A white region <b>1402</b> represents an epithelial cell region. Image morphological operations are performed based on M<sub>epithelium </sub>(x,y) to obtain an IC region binary mask image M<sub>IC</sub>(x,y) shown in <figref idref="DRAWINGS">FIG. <b>14</b>D</figref>. A white region <b>1403</b> represents an IC region including seed pixels. According to the algorithm in this embodiment of this application, an estimated IC value is obtained as 31.9% (an actual IC value&#x3e;10%), that is, the estimation is accurate.</p><p id="p-0139" num="0136">For accuracy analysis: in WSI data of 100 patients, 109 test patch images are used and gold standards, that is, two gold standards, are formulated for these images. The first gold standard is to classify the value of the IC score as four classes, and IC value intervals defined in the four classes are: class 1 [0%, 1%), class 2 [1%, 5%), class 3 [5%, 10%), and class 4 [10%, 100%]. The second gold standard is to classify the value of the IC score into two classes, and IC value intervals defined in the two classes are: [0%, 1%) and [1%, 100%]. The first gold standard is of medical research significance, and the second gold standard is of clinical diagnosis significance. In this embodiment of this application, the accuracy under the two gold standards is shown in Table 1:</p><p id="p-0140" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 1</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Accuracy of AI Auxiliary Model</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="5"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="77pt" align="left"/><colspec colname="2" colwidth="42pt" align="left"/><colspec colname="3" colwidth="21pt" align="left"/><colspec colname="4" colwidth="63pt" align="center"/><tbody valign="top"><row><entry/><entry/><entry/><entry/><entry>F1 score </entry></row><row><entry/><entry/><entry/><entry/><entry>(weights based </entry></row><row><entry/><entry/><entry/><entry/><entry>on a quantity</entry></row><row><entry/><entry/><entry>Accuracy</entry><entry>AUC</entry><entry>of classes)</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="5"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="77pt" align="left"/><colspec colname="2" colwidth="42pt" align="center"/><colspec colname="3" colwidth="21pt" align="center"/><colspec colname="4" colwidth="63pt" align="center"/><tbody valign="top"><row><entry/><entry>First gold standard:</entry><entry>0.752</entry><entry>0.797</entry><entry>0.764</entry></row><row><entry/><entry>four intervals</entry><entry/><entry/><entry/></row><row><entry/><entry>Second gold standard:</entry><entry>0.963</entry><entry>0.888</entry><entry>0.962</entry></row><row><entry/><entry>two intervals</entry><entry/><entry/><entry/></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0141" num="0137">In addition, the 109 images are scored by 31 doctors for three experiments. In Experiment 1 and experiment 2, the doctors directly perform IC scoring on the patches without assistance of AI. In Experiment 3, the doctors obtain IC results estimated based on AI, and meanwhile, IC region image results identified in <figref idref="DRAWINGS">FIG. <b>13</b>B</figref> and <figref idref="DRAWINGS">FIG. <b>14</b>B</figref> are also provided to the doctors as a reference. Experiments have found that, for the results based on the AI assistance, average accuracy of the 31 doctors is increased by 13.6% (which is increased from 0.679 (Experiment 1) and 0.71 (Experiment 2) to 0.815 (Experiment 3)). In the result of Experiment 3, accuracy of doctor+AI is higher than accuracy of AI. In the accuracy in the two intervals, the accuracy of the doctor is also increased by 3.9% (which is increased from 0.92 in Experiment 2 to 0.959 in Experiment 3).</p><p id="p-0142" num="0138">For consistency analysis, scoring consistency between the 31 doctors is also improved. A scoring difference between the 31 doctors for a same patch image is verified by using a Fleiss' kappa statistic (FKS) method. Scoring is performed based on the four intervals, and consistency results are shown in Table 2. The consistency between the 31 doctors under the assistance of AI reaches 0.78.</p><p id="p-0143" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 2</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Accuracy of AI Auxiliary Model</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="63pt" align="left"/><colspec colname="2" colwidth="56pt" align="center"/><colspec colname="3" colwidth="42pt" align="center"/><colspec colname="4" colwidth="56pt" align="center"/><tbody valign="top"><row><entry>Consistency results </entry><entry>Experiment </entry><entry>Experiment </entry><entry>Experiment </entry></row><row><entry>in four intervals</entry><entry>1</entry><entry>2</entry><entry>3</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row><row><entry>Fleiss' kappa statistic</entry><entry>0.43</entry><entry>0.47</entry><entry>0.78</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0144" num="0139">Scoring results in the four intervals are shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref>. A horizontal coordinate is a doctor identifier (ID), and a longitudinal coordinate is an image ID. Each color block in Experiment 1, Experiment 2, and Experiment 3 represents a score of a doctor for one image, that is, four scoring results of class 1, class 2, class 3, and class 4. Visually, the consistency between the scoring results in Experiment 3 is obviously superior to that in Experiment 1 and Experiment 2.</p><p id="p-0145" num="0140">The artificial intelligence-based pathological image processing method provided in the embodiments of this application is described with reference to an exemplary application and implementation of the server provided in the embodiments of the application. An embodiment of this application further provides a pathological image processing apparatus. During actual application, functional modules in the pathological image processing apparatus may be cooperatively implemented by hardware resources of an electronic device (for example, a terminal device, a server, or a server cluster), for example, a calculation resource such as a processor, a communication resource (for example, being used for supporting implementation of various types of communication such as optical cable communication and cellular communication), and a memory. <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a pathological image processing apparatus <b>555</b> stored in the memory <b>550</b>. The pathological image processing apparatus may be software in a form of a program, a plug-in, or the like, for example, a software module designed by a programming language such as C/C++ or Java, application software or a dedicated software module in a large-scale software system designed by a programming language such as C/C++ or Java, an application interface, a plug-in, and a cloud service. Different implementations are described by using examples.</p><p id="p-0146" num="0141">The pathological image processing apparatus <b>555</b> includes a series of modules such as a processing module <b>5551</b>, a segmentation module <b>5552</b>, a fusion module <b>5553</b>, and a determining module <b>5554</b>. The following continuously describes a solution for implementing the pathological image processing by cooperation of each module in the pathological image processing apparatus <b>555</b> provided by this embodiment of this application.</p><p id="p-0147" num="0142">The processing module <b>5551</b> is configured to determine a seed pixel corresponding to an immune cell region from a pathological image; and obtain a seed pixel mask image corresponding to the seed pixel of the immune cell region from the pathological image based on the seed pixel of the immune cell region. The segmentation module <b>5552</b> is configured to segment an epithelial cell region in the pathological image, to obtain an epithelial cell mask image of the pathological image. The fusion module <b>5553</b> is configured to fuse the seed pixel mask image and the epithelial cell mask image of the pathological image, to obtain an effective seed pixel mask image corresponding to the immune cell region in the pathological image. The determining module <b>5554</b> is configured to determine a ratio value of the immune cell region in the pathological image based on the effective seed pixel mask image.</p><p id="p-0148" num="0143">In some embodiments, the processing module <b>5551</b> is further configured to convert the pathological image from a first color space to a second color space, to obtain a pixel value of each pixel in the pathological image in the second color space; and determine, when the pixel value of the pixel in the second color space falls within a value range of the seed pixel, the pixel as the seed pixel of the immune cell region in the pathological image.</p><p id="p-0149" num="0144">In some embodiments, a segmentation network configured to segment an epithelial cell includes an encoder and a decoder. The segmentation module <b>5552</b> is further configured to perform feature space-based encoding on the pathological image by using the encoder, to obtain a hidden space feature of the epithelial cell region in the pathological image; and perform mask space-based decoding on the hidden space feature by using the decoder, to obtain the epithelial cell mask image of the pathological image.</p><p id="p-0150" num="0145">In some embodiments, the decoder includes a plurality of cascaded decoding layers. The segmentation module <b>5552</b> is further configured to perform upsampling decoding on the hidden space feature by using a first decoding layer in the plurality of cascaded decoding layers; output a decoding result of the first decoding layer to subsequent cascaded decoding layers, and continue to perform upsampling decoding by using the subsequent cascaded decoding layers and output decoding results until a decoding result is outputted to the last decoding layer; and map a decoding result outputted by the last decoding layer to a mask space, and use an obtained mapping result as the epithelial cell mask image of the pathological image.</p><p id="p-0151" num="0146">In some embodiments, the segmentation module <b>5552</b> is further configured to perform the following processing operations for an i<sup>th </sup>decoding layer in the plurality of cascaded decoding layers: performing upsampling processing on a decoding result outputted by an (i&#x2212;1)<sup>th </sup>decoding layer, to obtain an upsampling result of the i<sup>th </sup>decoding layer; performing convolution processing on the upsampling result of the i<sup>th </sup>decoding layer, to obtain a decoding result of the i<sup>th </sup>decoding layer; and outputting the decoding result of the i<sup>th </sup>decoding layer to an (i+1)<sup>th </sup>decoding layer, i being an increasing natural number and 2&#x2264;i&#x3c;N, and N being a total quantity of the plurality of cascaded decoding layers.</p><p id="p-0152" num="0147">In some embodiments, the segmentation module <b>5552</b> is further configured to perform convolution processing on the decoding result outputted by the (i&#x2212;1)th decoding layer, to obtain a first convolution result of the i<sup>th </sup>decoding layer; and perform full convolution processing on the first convolution result of the i<sup>th </sup>decoding layer, to obtain the upsampling result of the i<sup>th </sup>decoding layer.</p><p id="p-0153" num="0148">In some embodiments, the encoder includes a plurality of cascaded encoding layers. The segmentation module <b>5552</b> is further configured to perform downsampling encoding on the pathological image by using a first encoding layer in the plurality of cascaded encoding layers; output an encoding result of the first encoding layer to subsequent cascaded encoding layers, and continue to perform downsampling encoding by using the subsequent cascaded encoding layers and output encoding results until an encoding result is outputted to the last encoding layer; and use an encoding result outputted by the last encoding layer as the hidden space feature of the epithelial cell region in the pathological image.</p><p id="p-0154" num="0149">In some embodiments, when the decoder includes a plurality of cascaded decoding layers and a cross-layer connection exists between the decoding layer and the encoding layer that are in a same layer, the segmentation module <b>5552</b> is further configured to perform, by using a first decoding layer in the plurality of cascaded decoding layers, upsampling decoding on the hidden space feature outputted by an encoding layer that is in a cross-layer connection with the first decoding layer, and output a decoding result of the first decoding layer to a second decoding layer; splice, by using the second decoding layer, the decoding result of the first decoding layer and an encoding result outputted by an encoding layer that is in a cross-layer connection with the second decoding layer, to obtain a splicing result of the second decoding layer; perform upsampling decoding on the splicing result of the second decoding layer by using the second decoding layer; output a decoding result of the second decoding layer to subsequent cascaded decoding layers, and continue to perform splicing and upsampling decoding by using the subsequent cascaded decoding layers and output decoding results until a decoding result is outputted to the last decoding layer; and map a decoding result outputted by the last decoding layer to a mask space, and use an obtained mapping result as the epithelial cell mask image of the pathological image.</p><p id="p-0155" num="0150">In some embodiments, the segmentation module <b>5552</b> is further configured to perform the following processing operations for a j<sup>th </sup>encoding layer in the plurality of cascaded encoding layers: performing encoding on an encoding result outputted by a (j&#x2212;1)<sup>th </sup>encoding layer, to obtain a first encoding result of the j<sup>th </sup>encoding layer; combining the first encoding result of the j<sup>th </sup>encoding layer and the encoding result outputted by the (j&#x2212;1)<sup>th </sup>encoding layer, to obtain a combination result of the j<sup>th </sup>encoding layer; performing encoding on the combination result of the j<sup>th </sup>encoding layer, to obtain a second encoding result of the j<sup>th </sup>encoding layer; combining the second encoding result of the j<sup>th </sup>encoding layer and the combination result of the j<sup>th </sup>encoding layer, to obtain an encoding result of the j<sup>th </sup>encoding layer; and outputting the encoding result of the j<sup>th </sup>encoding layer to a (j+1)<sup>th </sup>encoding layer, j being an increasing natural number and 2&#x2264;j&#x3c;N, and N being a total quantity of the plurality of cascaded encoding layers.</p><p id="p-0156" num="0151">In some embodiments, the segmentation module <b>5552</b> is further configured to perform downsampling processing on the encoding result outputted by the (j&#x2212;1)<sup>th </sup>encoding layer, to obtain a downsampling result of the j<sup>th </sup>encoding layer; and perform convolution processing on the downsampling result of the j<sup>th </sup>encoding layer, to obtain the first encoding result of the j<sup>th </sup>encoding layer.</p><p id="p-0157" num="0152">In some embodiments, the fusion module <b>5553</b> is further configured to perform the following processing operations for each pixel in the epithelial cell mask image of the pathological image: performing negation processing on a pixel value of the pixel in the epithelial cell region, to obtain a negation value of the pixel; performing an AND logical operation on a pixel value of a pixel corresponding to the pixel in the seed pixel mask image and the negation value of the pixel, and using a result of the AND logical operation as an effective seed pixel of the immune cell region in the pathological image; and obtaining the effective seed pixel mask image corresponding to the immune cell region from the pathological image based on a plurality of effective seed pixels of the immune cell region in the pathological image.</p><p id="p-0158" num="0153">In some embodiments, the fusion module <b>5553</b> is further configured to determine a seed pixel region in the seed pixel mask image; determine the epithelial cell region in the epithelial cell mask image of the pathological image; and remove the epithelial cell region from the seed pixel region, to obtain the effective seed pixel mask image corresponding to the immune cell region in the pathological image.</p><p id="p-0159" num="0154">In some embodiments, the fusion module <b>5553</b> is further configured to perform the following processing operations for each pixel in the seed pixel region: performing negation processing on a pixel value of a pixel corresponding to the pixel in the epithelial cell region, to obtain a negation value of the pixel; performing an AND logical operation on a pixel value of the pixel in the seed pixel region and the negation value of the pixel, and using a result of the AND logical operation as an effective seed pixel of the immune cell region in the pathological image; and obtaining the effective seed pixel mask image corresponding to the immune cell region from the pathological image based on effective seed pixels of immune cell regions in a plurality of pathological images.</p><p id="p-0160" num="0155">In some embodiments, the determining module <b>5554</b> is further configured to perform morphological processing on the effective seed pixel mask image, to obtain a morphological effective seed pixel mask image; determine a first quantity of effective seed pixels in the morphological effective seed pixel mask image; determine a second quantity of all pixels in the morphological effective seed pixel mask image; and use a ratio of the quantity of effective seed pixels to the quantity of all pixels as the ratio value of the immune cell region in the pathological image.</p><p id="p-0161" num="0156">An embodiment of this application provides a computer program product or a computer program. The computer program product or the computer program includes computer instructions, and the computer instructions are stored in a non-transitory computer-readable storage medium. The processor of the computer device reads the computer instructions from the computer-readable storage medium, and the processor executes the computer instructions, to cause the computer device to perform the artificial intelligence-based pathological image processing method according to the embodiments of this application.</p><p id="p-0162" num="0157">An embodiment of this application provides a computer-readable storage medium storing executable instructions, the executable instructions, when executed by a processor, causing the processor to perform the artificial intelligence-based pathological image processing method, for example, the artificial intelligence-based pathological image processing method shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> to <figref idref="DRAWINGS">FIG. <b>4</b>C</figref>, provided in the embodiments of this application.</p><p id="p-0163" num="0158">In some embodiments, the computer-readable storage medium may be a memory such as a ferroelectric RAM (FRAM), a ROM, a programmable ROM (PROM), an electrically programmable ROM (EPROM), an electrically erasable PROM (EEPROM), a flash memory, a magnetic surface memory, an optical disk, or a CD-ROM, or may be any device including one of or any combination of the foregoing memories.</p><p id="p-0164" num="0159">In some embodiments, the executable instructions can be written in a form of a program, software, a software module, a script, or code and according to a programming language (including a compiler or interpreter language or a declarative or procedural language) in any form, and may be deployed in any form, including an independent program or a module, a component, a subroutine, or another unit suitable for use in a computing environment.</p><p id="p-0165" num="0160">In an example, the executable instructions may, but do not necessarily, correspond to a file in a file system, and may be stored in a part of a file that saves another program or other data, for example, be stored in one or more scripts in a hypertext markup language (HTML) file, stored in a file that is specially used for a program in discussion, or stored in the plurality of collaborative files (for example, be stored in files of one or modules, subprograms, or code parts).</p><p id="p-0166" num="0161">In an example, the executable instructions can be deployed for execution on one computing device, execution on a plurality of computing devices located at one location, or execution on a plurality of computing devices that are distributed at a plurality of locations and that are interconnected through a communication network. In this application, the term &#x201c;unit&#x201d; or &#x201c;module&#x201d; in this application refers to a computer program or part of the computer program that has a predefined function and works together with other related parts to achieve a predefined goal and may be all or partially implemented by using software, hardware (e.g., processing circuitry and/or memory configured to perform the predefined functions), or a combination thereof. Each unit or module can be implemented using one or more processors (or processors and memory). Likewise, a processor (or processors and memory) can be used to implement one or more modules or units. Moreover, each module or unit can be part of an overall module that includes the functionalities of the module or unit.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005156A1-20230105-M00001.NB"><img id="EMI-M00001" he="6.01mm" wi="76.20mm" file="US20230005156A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A pathological image processing method performed by an electronic device, comprising:<claim-text>determining a seed pixel of an immune cell region from a pathological image;</claim-text><claim-text>obtaining a seed pixel mask image corresponding to the seed pixel of the immune cell region from the pathological image based on the seed pixel of the immune cell region;</claim-text><claim-text>segmenting an epithelial cell region in the pathological image, to obtain an epithelial cell mask image of the pathological image;</claim-text><claim-text>fusing the seed pixel mask image and the epithelial cell mask image of the pathological image, to obtain an effective seed pixel mask image corresponding to the immune cell region in the pathological image; and</claim-text><claim-text>determining a ratio value of the immune cell region in the pathological image based on the effective seed pixel mask image.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining a seed pixel of an immune cell region from a pathological image comprises:<claim-text>converting the pathological image from a first color space to a second color space, to obtain a pixel value of each pixel of the pathological image in the second color space; and</claim-text><claim-text>determining, when the pixel value of the pixel in the second color space falls within a value range of the seed pixel, the pixel as the seed pixel of the immune cell region in the pathological image.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a segmentation network configured to segment an epithelial cell comprises an encoder and a decoder; and<claim-text>the segmenting an epithelial cell region in the pathological image, to obtain an epithelial cell mask image of the pathological image comprises:</claim-text><claim-text>performing feature space-based encoding on the pathological image by using the encoder, to obtain a hidden space feature of the epithelial cell region in the pathological image; and</claim-text><claim-text>performing mask space-based decoding on the hidden space feature by using the decoder, to obtain the epithelial cell mask image of the pathological image.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the decoder comprises a plurality of cascaded decoding layers; and<claim-text>the performing mask space-based decoding on the hidden space feature by using the decoder, to obtain the epithelial cell mask image of the pathological image comprises:</claim-text><claim-text>performing upsampling decoding on the hidden space feature by using a first decoding layer in the plurality of cascaded decoding layers;</claim-text><claim-text>outputting a decoding result of the first decoding layer to subsequent cascaded decoding layers, and continuing to perform upsampling decoding by using the subsequent cascaded decoding layers and outputting decoding results until a decoding result is outputted to the last decoding layer; and</claim-text><claim-text>mapping a decoding result outputted by the last decoding layer to a mask space, and using an obtained mapping result as the epithelial cell mask image of the pathological image.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the encoder comprises a plurality of cascaded encoding layers; and<claim-text>the performing feature space-based encoding on the pathological image by using the encoder, to obtain a hidden space feature of the epithelial cell region in the pathological image comprises:</claim-text><claim-text>performing downsampling encoding on the pathological image by using a first encoding layer in the plurality of cascaded encoding layers;</claim-text><claim-text>outputting an encoding result of the first encoding layer to subsequent cascaded encoding layers, and continuing to perform downsampling encoding by using the subsequent cascaded encoding layers and outputting encoding results until an encoding result is outputted to the last encoding layer; and</claim-text><claim-text>using an encoding result outputted by the last encoding layer as the hidden space feature of the epithelial cell region in the pathological image.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the fusing the seed pixel mask image and the epithelial cell mask image of the pathological image, to obtain an effective seed pixel mask image corresponding to the immune cell region in the pathological image comprises:<claim-text>performing the following processing operations for each pixel in the epithelial cell mask image of the pathological image:</claim-text><claim-text>performing negation processing on a pixel value of the pixel, to obtain a negation value of the pixel;</claim-text><claim-text>performing an AND logical operation on a pixel value of a pixel corresponding to the pixel in the seed pixel mask image and the negation value of the pixel, and using a result of the AND logical operation as an effective seed pixel of the immune cell region in the pathological image; and</claim-text><claim-text>obtaining the effective seed pixel mask image corresponding to the immune cell region from the pathological image based on a plurality of effective seed pixels of the immune cell region in the pathological image.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the fusing the seed pixel mask image and the epithelial cell mask image of the pathological image, to obtain an effective seed pixel mask image corresponding to the immune cell region in the pathological image comprises:<claim-text>determining a seed pixel region in the seed pixel mask image;</claim-text><claim-text>determining the epithelial cell region in the epithelial cell mask image of the pathological image; and</claim-text><claim-text>removing the epithelial cell region from the seed pixel region, to obtain the effective seed pixel mask image corresponding to the immune cell region in the pathological image.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining a ratio value of the immune cell region in the pathological image based on the effective seed pixel mask image comprises:<claim-text>performing morphological processing on the effective seed pixel mask image, to obtain a morphological effective seed pixel mask image;</claim-text><claim-text>determining a first quantity of effective seed pixels in the morphological effective seed pixel mask image;</claim-text><claim-text>determining a second quantity of all pixels in the morphological effective seed pixel mask image; and</claim-text><claim-text>using a ratio of the first quantity to the second quantity as the ratio value of the immune cell region in the pathological image.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. An electronic device, comprising:<claim-text>a memory, configured to store executable instructions; and</claim-text><claim-text>a processor, configured to implement a pathological image processing method by executing the executable instructions stored in the memory, the method including:</claim-text><claim-text>determining a seed pixel of an immune cell region from a pathological image;</claim-text><claim-text>obtaining a seed pixel mask image corresponding to the seed pixel of the immune cell region from the pathological image based on the seed pixel of the immune cell region;</claim-text><claim-text>segmenting an epithelial cell region in the pathological image, to obtain an epithelial cell mask image of the pathological image;</claim-text><claim-text>fusing the seed pixel mask image and the epithelial cell mask image of the pathological image, to obtain an effective seed pixel mask image corresponding to the immune cell region in the pathological image; and</claim-text><claim-text>determining a ratio value of the immune cell region in the pathological image based on the effective seed pixel mask image.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The electronic device according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the determining a seed pixel of an immune cell region from a pathological image comprises:<claim-text>converting the pathological image from a first color space to a second color space, to obtain a pixel value of each pixel of the pathological image in the second color space; and</claim-text><claim-text>determining, when the pixel value of the pixel in the second color space falls within a value range of the seed pixel, the pixel as the seed pixel of the immune cell region in the pathological image.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The electronic device according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein a segmentation network configured to segment an epithelial cell comprises an encoder and a decoder; and<claim-text>the segmenting an epithelial cell region in the pathological image, to obtain an epithelial cell mask image of the pathological image comprises:</claim-text><claim-text>performing feature space-based encoding on the pathological image by using the encoder, to obtain a hidden space feature of the epithelial cell region in the pathological image; and</claim-text><claim-text>performing mask space-based decoding on the hidden space feature by using the decoder, to obtain the epithelial cell mask image of the pathological image.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The electronic device according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the decoder comprises a plurality of cascaded decoding layers; and<claim-text>the performing mask space-based decoding on the hidden space feature by using the decoder, to obtain the epithelial cell mask image of the pathological image comprises:</claim-text><claim-text>performing upsampling decoding on the hidden space feature by using a first decoding layer in the plurality of cascaded decoding layers;</claim-text><claim-text>outputting a decoding result of the first decoding layer to subsequent cascaded decoding layers, and continuing to perform upsampling decoding by using the subsequent cascaded decoding layers and outputting decoding results until a decoding result is outputted to the last decoding layer; and</claim-text><claim-text>mapping a decoding result outputted by the last decoding layer to a mask space, and using an obtained mapping result as the epithelial cell mask image of the pathological image.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The electronic device according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the encoder comprises a plurality of cascaded encoding layers; and<claim-text>the performing feature space-based encoding on the pathological image by using the encoder, to obtain a hidden space feature of the epithelial cell region in the pathological image comprises:</claim-text><claim-text>performing downsampling encoding on the pathological image by using a first encoding layer in the plurality of cascaded encoding layers;</claim-text><claim-text>outputting an encoding result of the first encoding layer to subsequent cascaded encoding layers, and continuing to perform downsampling encoding by using the subsequent cascaded encoding layers and outputting encoding results until an encoding result is outputted to the last encoding layer; and</claim-text><claim-text>using an encoding result outputted by the last encoding layer as the hidden space feature of the epithelial cell region in the pathological image.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The electronic device according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the fusing the seed pixel mask image and the epithelial cell mask image of the pathological image, to obtain an effective seed pixel mask image corresponding to the immune cell region in the pathological image comprises:<claim-text>performing the following processing operations for each pixel in the epithelial cell mask image of the pathological image:</claim-text><claim-text>performing negation processing on a pixel value of the pixel, to obtain a negation value of the pixel;</claim-text><claim-text>performing an AND logical operation on a pixel value of a pixel corresponding to the pixel in the seed pixel mask image and the negation value of the pixel, and using a result of the AND logical operation as an effective seed pixel of the immune cell region in the pathological image; and</claim-text><claim-text>obtaining the effective seed pixel mask image corresponding to the immune cell region from the pathological image based on a plurality of effective seed pixels of the immune cell region in the pathological image.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The electronic device according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the fusing the seed pixel mask image and the epithelial cell mask image of the pathological image, to obtain an effective seed pixel mask image corresponding to the immune cell region in the pathological image comprises:<claim-text>determining a seed pixel region in the seed pixel mask image;</claim-text><claim-text>determining the epithelial cell region in the epithelial cell mask image of the pathological image; and</claim-text><claim-text>removing the epithelial cell region from the seed pixel region, to obtain the effective seed pixel mask image corresponding to the immune cell region in the pathological image.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The electronic device according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the determining a ratio value of the immune cell region in the pathological image based on the effective seed pixel mask image comprises:<claim-text>performing morphological processing on the effective seed pixel mask image, to obtain a morphological effective seed pixel mask image;</claim-text><claim-text>determining a first quantity of effective seed pixels in the morphological effective seed pixel mask image;</claim-text><claim-text>determining a second quantity of all pixels in the morphological effective seed pixel mask image; and</claim-text><claim-text>using a ratio of the first quantity to the second quantity as the ratio value of the immune cell region in the pathological image.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A non-transitory computer-readable storage medium, storing executable instructions, the executable instructions, when executed by a processor of an electronic device, causing the electronic device to implement a pathological image processing method including:<claim-text>determining a seed pixel of an immune cell region from a pathological image;</claim-text><claim-text>obtaining a seed pixel mask image corresponding to the seed pixel of the immune cell region from the pathological image based on the seed pixel of the immune cell region;</claim-text><claim-text>segmenting an epithelial cell region in the pathological image, to obtain an epithelial cell mask image of the pathological image;</claim-text><claim-text>fusing the seed pixel mask image and the epithelial cell mask image of the pathological image, to obtain an effective seed pixel mask image corresponding to the immune cell region in the pathological image; and</claim-text><claim-text>determining a ratio value of the immune cell region in the pathological image based on the effective seed pixel mask image.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the determining a seed pixel of an immune cell region from a pathological image comprises:<claim-text>converting the pathological image from a first color space to a second color space, to obtain a pixel value of each pixel of the pathological image in the second color space; and</claim-text><claim-text>determining, when the pixel value of the pixel in the second color space falls within a value range of the seed pixel, the pixel as the seed pixel of the immune cell region in the pathological image.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein a segmentation network configured to segment an epithelial cell comprises an encoder and a decoder; and<claim-text>the segmenting an epithelial cell region in the pathological image, to obtain an epithelial cell mask image of the pathological image comprises:</claim-text><claim-text>performing feature space-based encoding on the pathological image by using the encoder, to obtain a hidden space feature of the epithelial cell region in the pathological image; and</claim-text><claim-text>performing mask space-based decoding on the hidden space feature by using the decoder, to obtain the epithelial cell mask image of the pathological image.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the fusing the seed pixel mask image and the epithelial cell mask image of the pathological image, to obtain an effective seed pixel mask image corresponding to the immune cell region in the pathological image comprises:<claim-text>performing the following processing operations for each pixel in the epithelial cell mask image of the pathological image:</claim-text><claim-text>performing negation processing on a pixel value of the pixel, to obtain a negation value of the pixel;</claim-text><claim-text>performing an AND logical operation on a pixel value of a pixel corresponding to the pixel in the seed pixel mask image and the negation value of the pixel, and using a result of the AND logical operation as an effective seed pixel of the immune cell region in the pathological image; and</claim-text><claim-text>obtaining the effective seed pixel mask image corresponding to the immune cell region from the pathological image based on a plurality of effective seed pixels of the immune cell region in the pathological image.</claim-text></claim-text></claim></claims></us-patent-application>