<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005278A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005278</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17834501</doc-number><date>20220607</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>KR</country><doc-number>10-2021-0085339</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>56</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>11</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>11</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>48</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>588</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>11</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>11</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>4802</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10028</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30256</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20021</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20076</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20068</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2210</main-group><subgroup>32</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2210</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">LANE EXTRACTION METHOD USING PROJECTION TRANSFORMATION OF THREE-DIMENSIONAL POINT CLOUD MAP</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>MOBILTECH</orgname><address><city>Seoul</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KIM</last-name><first-name>Jae Seung</first-name><address><city>Seoul</city><country>KR</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>PARK</last-name><first-name>Yeon Soo</first-name><address><city>Seoul</city><country>KR</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A lane extraction method uses projection transformation of a 3D point cloud map, by which the amount of operations required to extract the coordinates of a lane is reduced by performing deep learning and lane extraction in a two-dimensional (2D) domain, and therefore, lane information is obtained in real time. In addition, black-and-white brightness, which is most important information for lane extraction on an image, is substituted by the reflection intensity of a light detection and ranging (LiDAR) sensor so that a deep learning model capable of accurately extracting a lane is provided. Therefore, reliability and competitiveness is enhanced in the field of autonomous driving, the field of road recognition, the field of lane recognition, and the field of HD road maps for autonomous driving, and the fields similar or related thereto, and more particularly, in the fields of road recognition and autonomous driving using LiDAR.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="109.14mm" wi="100.41mm" file="US20230005278A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="135.81mm" wi="102.45mm" file="US20230005278A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="122.85mm" wi="142.49mm" file="US20230005278A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="134.87mm" wi="149.86mm" file="US20230005278A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="137.24mm" wi="149.10mm" file="US20230005278A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="129.79mm" wi="148.84mm" file="US20230005278A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="104.39mm" wi="160.78mm" file="US20230005278A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="105.07mm" wi="162.90mm" file="US20230005278A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="103.72mm" wi="160.95mm" file="US20230005278A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="199.64mm" wi="107.19mm" file="US20230005278A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="103.97mm" wi="134.54mm" file="US20230005278A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?federal-research-statement description="Federal Research Statement" end="lead"?><p id="p-0002" num="0001">This work is supported by the Korea Agency for Infrastructure Technology Advancement (KAIA) grant funded by the Ministry of Land, Infrastructure and Transport (Grant 22AMDP-C161762-02).</p><?federal-research-statement description="Federal Research Statement" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATION OF THE INVENTION</heading><p id="p-0003" num="0002">The present application claims the benefit of Korean Patent Application No. 10-2021-0085339 filed in the Korean Intellectual Property Office on Jun. 30, 2021, the entire contents of which are incorporated herein by reference.</p><heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading><heading id="h-0003" level="1">Field of the Invention</heading><p id="p-0004" num="0003">The present invention relates to a lane extraction method of using projection transformation of a three-dimensional (3D) point cloud map, by which a lane region is extracted by converting a 3D high-definition (HD) map including a point cloud into a planar image and using a deep learning model.</p><p id="p-0005" num="0004">In particular, the present invention relates to a lane extraction method using projection transformation of a 3D point cloud map, by which the amount of operations required to extract the coordinates of a lane is reduced by performing deep learning and lane extraction in a two-dimensional (2D) domain, and therefore, lane information is obtained in real time.</p><heading id="h-0004" level="1">Background of the Related Art</heading><p id="p-0006" num="0005">Unmanned autonomy of vehicles (autonomous vehicles) may mainly includes recognizing a surrounding environment (cognitive domain), planning a driving route from the recognized environment (determination domain), and driving along the planned route (control domain)</p><p id="p-0007" num="0006">In particular, the cognitive domain relates to a basic technique initially performed for autonomous driving, and only when techniques in the cognitive domain is accurately performed, techniques in the subsequent determination and control domains may be accurately performed.</p><p id="p-0008" num="0007">Techniques of recognizing a surrounding environment of a vehicle vary with a target environment, in which an autonomous vehicle is intended to drive. In particular, to perform autonomous driving in a road environment designed and constructed for driving of existing manned vehicles, techniques of recognizing various rules of the road are essentially required.</p><p id="p-0009" num="0008">In particular, recognizing and moving along a lane may be considered as the most basic technique for safe driving together with manned vehicles.</p><p id="p-0010" num="0009">One of methods of recognizing a lane on the road is based on an image obtained through a camera. With the development of image processing technology, there has been a lot of research into extracting a lane from a camera image.</p><p id="p-0011" num="0010">Recently, most research into lane extraction has been based on deep learning and variously modified using a semantic segmentation model as a framework.</p><p id="p-0012" num="0011">Camera images are mostly obtained from a camera mounted on the front of a vehicle, and estimation accuracy significantly decreases at the edge or vanishing point of an image because of the perspective and viewing angle of the camera.</p><p id="p-0013" num="0012">In addition, due to the characteristics of a camera image, it may be hard to recognize a lane because of an environmental condition, such as rain or the amount of light incident to the road.</p><p id="p-0014" num="0013">To solve these problems, Korea Patent Publication 10-2083909 (referred to as prior art), entitled &#x201c;Automatic Extraction Method for Lane Data Information for Autonomous Driving Vehicles Based on Point Cloud Map&#x201d;, discloses a technique for obtaining 3D information of a lane from 3D point cloud data obtained using light detection and ranging (LiDAR).</p><p id="p-0015" num="0014">However, in the case of the prior art, it takes a lot of time to perform operations because the operations are performed using 3D point cloud data in a process of extracting lane data.</p><p id="p-0016" num="0015">In addition, because of the scarcity of a point cloud included in a LiDAR frame, it is more difficult to recognize a lane when the lane is farther away from a LiDAR sensor. In particular, there is a greater problem when there is damage to a lane or when lane marking lines are far apart from each other.</p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0017" num="0016">The present invention provides a lane extraction method using projection transformation of a three-dimensional (3D) point cloud map, by which a lane region is extracted by converting a 3D high-definition (HD) map including a point cloud into a planar image and using a deep learning model.</p><p id="p-0018" num="0017">The present invention also provides a lane extraction method using projection transformation of a 3D point cloud map, by which the amount of operations required to extract the coordinates of a lane is reduced by performing deep learning and lane extraction in a two-dimensional (2D) domain, and therefore, lane information is obtained in real time.</p><p id="p-0019" num="0018">The present invention also provides a lane extraction method using projection transformation of a 3D point cloud map, by which line extraction is effectively automated and performed at a high speed with high accuracy by minimizing the amount of operations required to extract the coordinates of a lane.</p><p id="p-0020" num="0019">According to an aspect of the present invention, there is provided a lane extraction method using projection transformation of a 3D point cloud map. The lane extraction method includes a deep learning step of projecting a training 3D HD map onto a planar image, extracting a lane on a 2D map resulting from the projection, matching the 2D map with a planar image of the lane, setting the matched 2D map and planar image of the lane as training data, and training a deep learning model of extracting a lane from a map, based on the training data; and a lane extraction step of projecting a target 3D HD map onto a planar map image, extracting a planar lane image from the planar map image using the deep learning model, and converting the planar lane image into 3D coordinates.</p><p id="p-0021" num="0020">The deep learning step may include a training map projection process of projecting and converting a training 3D point cloud map corresponding to the training 3D HD map into a top-view image; a lane projection process of extracting a lane point from the training 3D point cloud map and projecting and converting the lane point into the top-view image; a training data setting process of matching and setting the 2D map resulting from the projection and the lane as the training data; and a deep learning process of training the deep learning model based on the training data.</p><p id="p-0022" num="0021">The training map projection process may include a view conversion process of converting the training 3D point cloud map into the top-view image using reflection intensity; a map segmentation process of segmenting the 2D point cloud map into grids having a certain size; and a planar map imaging process of making the 2D point cloud map into an image grid by grid.</p><p id="p-0023" num="0022">The lane extraction method may further include, after the planar map imaging process, a meaningless image removing process of removing a grid planar map image having no roadway from the planar map image.</p><p id="p-0024" num="0023">The lane projection process may include a lane point extraction process of extracting a lane point from the training 3D point cloud map; a line fitting process of fitting the lane point to a line; and a planar lane imaging process of making the line into an image for each grid.</p><p id="p-0025" num="0024">The training data setting process may include an identification (ID) assignment process of assigning the global coordinate value to each grid as an ID; and an image matching process of matching and setting a planar map image and a planar lane image, which are assigned a same ID, as an image pair.</p><p id="p-0026" num="0025">The lane extraction step may include a target map projection process of projecting a target 3D point cloud map corresponding to the target 3D HD map onto a top-view image; a lane extraction process of extracting the planar lane image from the planar map image using the deep learning model; and a lane coordinate reconstruction process of reconstructing global coordinates from the planar lane image.</p><p id="p-0027" num="0026">The target map projection process may include segmenting a 2D point cloud map into grids having a certain size, the 2D point cloud map corresponding to the top-view image onto which the target 3D point cloud map is projected. The lane coordinate reconstruction process may include reconstructing a 3D coordinate point cloud from the planar lane image by using a global coordinate value assigned to each grid as the ID.</p><p id="p-0028" num="0027">The lane coordinate reconstruction process may include a planar coordinate assignment process of converting the planar lane image into a lane point by assigning an x-coordinate and a y-coordinate to the planar lane image using the global coordinate value assigned to each grid; a 3D coordinate determination process of determining coordinates of a point closest to the x- and y-coordinates on the target 3D HD map; and a z-coordinate assignment process of assigning a z-coordinate of the point on the target 3D HD map to the lane point.</p><p id="p-0029" num="0028">Alternatively, the lane coordinate reconstruction process may include a planar coordinate assignment process of converting the planar lane image into a lane point by assigning an x-coordinate and a y-coordinate to the planar lane image using the global coordinate value assigned to each grid; an adjacent point determination process of determining 3D points within a certain distance from the x- and y-coordinates on the target 3D HD map; and a z-coordinate assignment process of assigning an average of z-coordinates of the 3D points to the lane point.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0030" num="0029">The above and other objects, features and advantages of the present invention will be apparent from the following detailed description of the embodiments of the invention in conjunction with the accompanying drawings, in which:</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a flowchart of a lane extraction method using projection transformation of a three-dimensional (3D) point cloud map, according to an embodiment;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a detailed flowchart of a step S<b>100</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, according to an embodiment;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a detailed flowchart of a process S<b>110</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, according to an embodiment;</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a detailed flowchart of a process S<b>120</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, according to an embodiment;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a detailed flowchart of a process S<b>130</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, according to an embodiment;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a detailed flowchart of a step S<b>200</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, according to an embodiment;</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a detailed flowchart of a process S<b>230</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, according to an embodiment;</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a detailed flowchart of a process S<b>230</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, according to another embodiment;</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram of the structure of a leaning network used in the step S<b>100</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>; and</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram for describing a process of generating training data used in the step S<b>100</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0041" num="0040">Examples of a lane extraction method using projection transformation of a three-dimensional (3D) point cloud map may be variously used. Embodiments are described below with reference to the accompanying drawings.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a flowchart of a lane extraction method using projection transformation of a 3D point cloud map, according to an embodiment.</p><p id="p-0043" num="0042">Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the lane extraction method using projection transformation of a 3D point cloud map may include a deep learning step S<b>100</b> and a lane extraction step S<b>200</b>.</p><p id="p-0044" num="0043">In the deep learning step S<b>100</b>, a method of extracting lane information, e.g., 3D coordinates of point cloud data of a lane, from a 3D high-definition (HD) map may be trained.</p><p id="p-0045" num="0044">At this time, when lane information is directly extracted from a 3D point cloud map, i.e., a 3D HD map, as described above, the amount of data operations is huge. Therefore, in an embodiment of the present invention, a lane is extracted after 3D information is converted into two-dimensional (2D) information and then reconstructed in three dimensions.</p><p id="p-0046" num="0045">In detail, in the deep learning step S<b>100</b>, after a training 3D HD map is projected onto a planar image, a lane on a 2D map resulting from the projection is extracted. The 2D map and a planar image of the lane are matched with each other and set as training data, and a deep learning model of extracting a lane from a map is trained based on the training data.</p><p id="p-0047" num="0046">The lane extraction step S<b>200</b> is a process of extracting 3D coordinates of a lane included in a point cloud map from a 3D point cloud measured using light detection and ranging (LiDAR). In the lane extraction step S<b>200</b>, after a target 3D HD map is projected onto a planar map image, a planar lane image is extracted from the planar map image using the deep learning model that has been trained in the deep learning step S<b>100</b> and then converted into 3D coordinates.</p><p id="p-0048" num="0047">Hereinafter, each operation in <figref idref="DRAWINGS">FIG. <b>1</b></figref> is described in detail.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a detailed flowchart of deep learning operation S<b>100</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, according to an embodiment.</p><p id="p-0050" num="0049">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, deep learning operation S<b>100</b> may include a training map projection process S<b>110</b>, a lane projection process S<b>120</b>, a training data setting process S<b>130</b>, and a deep learning process S<b>140</b>.</p><p id="p-0051" num="0050">In the training map projection process S<b>110</b>, a training 3D point cloud map corresponding to the training 3D HD map may be projected and converted into a top-view image.</p><p id="p-0052" num="0051">For example, a 3D point cloud map may be converted into a 2D map by removing a z-coordinate from the 3D coordinates of each point in the 3D point cloud map.</p><p id="p-0053" num="0052">In the lane projection process S<b>120</b>, a lane point may be extracted from the training 3D point cloud map and projected and converted into the top-view image.</p><p id="p-0054" num="0053">For example, because lane point extraction for learning is performed once and unrelated to analysis of a 3D point cloud map actually measured afterward, the lane point extraction may be manually performed.</p><p id="p-0055" num="0054">In the training data setting process S<b>130</b>, the 2D map resulting from the projection into the top-view image and the lane are matched with each other and set as training data. The training data setting process S<b>130</b> may include a process of matching a question (a map) with an answer (a lane).</p><p id="p-0056" num="0055">In the deep learning process S<b>140</b>, a lane extraction deep learning model is trained based on the training data, in which a question and an answer are matched with each other. The deep learning process S<b>140</b> includes a process of learning an answer to a question to obtain the answer when the question is given.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a detailed flowchart of the training map projection process S<b>110</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, according to an embodiment.</p><p id="p-0058" num="0057">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the training map projection process S<b>110</b> includes a view conversion process S<b>111</b>, a map segmentation process S<b>112</b>, and a planar map imaging process S<b>113</b>.</p><p id="p-0059" num="0058">In the training map projection process S<b>110</b>, after the training 3D point cloud map may be converted into a top view using reflection intensity in the view conversion process S<b>111</b>, the 2D point cloud map may be segmented into grids having a certain size in the map segmentation process S<b>112</b>, and the 2D point cloud map may be made into an image grid by grid in the planar map imaging process S<b>113</b>.</p><p id="p-0060" num="0059">In detail, an image, which is appropriate to allow the deep learning model to recognize a lane characteristic, may be generated using the reflection intensity of 3D point cloud data.</p><p id="p-0061" num="0060">For example, after each grid having a certain size is generated using an x-coordinate and a y-coordinate of a 3D map, an image corresponding to the grid may be generated by projecting point cloud data of the grid onto an XY plane.</p><p id="p-0062" num="0061">At this time, each image may be generated as a gray image having the reflection intensity of a projected point as a pixel value.</p><p id="p-0063" num="0062">In addition, for the reconstruction of 3D coordinates described below, information on global coordinates may be preserved by giving each image, as an identification (ID), the coordinate value of the left top of a corresponding grid.</p><p id="p-0064" num="0063">After the planar map imaging process S<b>113</b>, a meaningless image removing process may be performed. In the meaningless image removing process, a grid planar map image having no roadway, e.g., an empty image including no points or an image including only a structure, such as a building, forest, or a parking lot, but not a roadway, may be removed.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a detailed flowchart of the lane projection process S<b>120</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, according to an embodiment.</p><p id="p-0066" num="0065">Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the lane projection process S<b>120</b> may include a lane point extraction process S<b>121</b>, a line fitting process S<b>122</b>, and a planar lane imaging process S<b>123</b>.</p><p id="p-0067" num="0066">In the lane projection process S<b>120</b>, after the lane point is extracted from the training 3D point cloud map in the lane point extraction process S<b>121</b>, the lane point may be fitted to a line in the line fitting process S<b>122</b>, and the line may be made into an image for each grid in the planar lane imaging process S<b>123</b>.</p><p id="p-0068" num="0067">In detail, the lane point may be manually and directly extracted from the training 3D point cloud map in the lane point extraction process S<b>121</b>, and ling fitting may be performed on a sample, i.e., the lane point, in the line fitting process S<b>122</b>.</p><p id="p-0069" num="0068">The point cloud data of a lane, which has been interpolated into a line through the process described above, may have coordinates that coincide with coordinates on the 3D HD map.</p><p id="p-0070" num="0069">In the planar lane imaging process S<b>123</b>, the point cloud data of a lane may be converted into a segmented image by using the grid that has been applied to the point cloud data in the map segmentation process S<b>112</b>.</p><p id="p-0071" num="0070">At this time, the segmented image of the lane may include a binary image, which has a maximum brightness of 255 for pixels corresponding to the lane and a minimum brightness of 0 for pixels corresponding to a background excluding the lane.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a detailed flowchart of the training data setting process S<b>130</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, according to an embodiment.</p><p id="p-0073" num="0072">Referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the training data setting process S<b>130</b> may include an ID assignment process S<b>131</b> and an image matching process S<b>132</b>.</p><p id="p-0074" num="0073">In the training data setting process S<b>130</b>, a coordinate value corresponding to global coordinates may be assigned to each grid as an ID. Here, the ID may include the coordinate value of the left top of the grid.</p><p id="p-0075" num="0074">Thereafter, a planar map image and a planar lane image, which are assigned the same ID, may be matched with each other and set as an image pair. Here, the planar lane image may be the ground truth of the planar map image.</p><p id="p-0076" num="0075">The deep learning process S<b>140</b> is described in detail below.</p><p id="p-0077" num="0076">A basic encoder-decoder structure additionally including skip connection is used as a base line. To reduce checkerboard artifacts often occurring in the upsampling of a decoder, not stride convolution but anti-aliasing, e.g., BlurPool, is used in the downsampling of an encoder. Even for the upsampling, linear interpolation and normal convolution is used instead of deconvolution (dilated convolution) that is a major cause of an artifact.</p><p id="p-0078" num="0077">A two-channel image, into which a map segment image and a result of performing Canny edge filtering on the map segment image are concatenated, is used as an input of the deep learning model.</p><p id="p-0079" num="0078">An output of the deep learning model is a lane mask predicted with respect to the input and has the same size as the input image.</p><p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a detailed flowchart of the lane extraction step S<b>200</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, according to an embodiment.</p><p id="p-0081" num="0080">Referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the lane extraction step S<b>200</b> may include a target map projection process S<b>210</b>, a lane extraction process S<b>220</b>, and a lane coordinate reconstruction process S<b>230</b>.</p><p id="p-0082" num="0081">In the lane extraction step S<b>200</b>, a target 3D point cloud map corresponding to the target 3D HD map may be projected and converted into a top-view image in the target map projection process S<b>210</b>, a planar lane image may be extracted from planar map image using the deep learning model in the lane extraction process S<b>220</b>, and the global coordinates may be reconstructed from the planar lane image in the lane coordinate reconstruction process S<b>230</b>.</p><p id="p-0083" num="0082">At this time, the target map projection process S<b>210</b> may include segmenting a 2D point cloud map, onto which a 3D point cloud map is projected, into grids having a certain size. The lane coordinate reconstruction process S<b>230</b> may include reconstructing a 3D coordinate point cloud from a planar lane image by using a global coordinate value assigned to each grid as an ID.</p><p id="p-0084" num="0083">In detail, when there is an entire map from which a lane is to be obtained, the map may be divided into segment images using grids, the segment images may be sequentially input to the deep learning model, and a lane image corresponding to each segment image may be obtained.</p><p id="p-0085" num="0084">At this time, the lane image obtained as an output may include a gray image, in which a pixel of the lane image has a pixel value between 0 and 1 that expresses the probability of corresponding to a lane.</p><p id="p-0086" num="0085">Pixel values may be binarized using a certain threshold, and the coordinates of a pixel having a value of 1 may be converted into global coordinates using a grid ID of the lane image.</p><p id="p-0087" num="0086">This process may be repeatedly performed on the lane image of every grid so that lane points for the entire map may be obtained.</p><p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a detailed flowchart of the lane coordinate reconstruction process S<b>230</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, according to an embodiment.</p><p id="p-0089" num="0088">Referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the lane coordinate reconstruction process S<b>230</b> may include a planar coordinate assignment process S<b>231</b> in which the planar lane image is converted into a lane point by assigning an x-coordinate and a y-coordinate to the planar lane image using a global coordinate value assigned to each grid, a 3D coordinate determination process S<b>232</b> in which the coordinates of a point closest to the x- and y-coordinates are determined on the target 3D HD map, and a z-coordinate assignment process S<b>233</b> in which a z-coordinate of the point on the target 3D HD map is assigned to the lane point.</p><p id="p-0090" num="0089">When the lane coordinate reconstruction process S<b>230</b> is completed, the lane point may be matched to a point closest thereto on the 3D HD map according to a distance based on the x- and y-coordinates of the lane point so that the z-coordinate of the point on the 3D HD map may be assigned as the z-coordinate of the lane point.</p><p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a detailed flowchart of the lane coordinate reconstruction process S<b>230</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, according to another embodiment.</p><p id="p-0092" num="0091">Referring to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the lane coordinate reconstruction process S<b>230</b> may include a planar coordinate assignment process S<b>231</b>&#x2032; in which a planar lane image is converted into a lane point by assigning an x-coordinate and a y-coordinate to the planar lane image using a global coordinate value assigned to each grid, an adjacent point determination process S<b>232</b>&#x2032; in which 3D points within a certain distance from the x- and y-coordinates on the target 3D HD map are determined, and a z-coordinate assignment process S<b>233</b>&#x2032; in which the average of z-coordinates of the 3D points is assigned to the lane point.</p><p id="p-0093" num="0092">In other words, the average of z-coordinate values of points within a certain radius around a point closest to the x- and y-coordinates on the target 3D HD map may be calculated and used as the z-coordinate of the lane point. Through this process, the influence of noise around the ground may be minimized.</p><p id="p-0094" num="0093">Consequently, x-, y-, and z-lane coordinates that coincide with a point cloud on a 3D map may be obtained, and accordingly, a lane may be recognized and marked on the 3D map.</p><p id="p-0095" num="0094"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram of the structure of a leaning network used in a learning process in a lane extraction method, according to an embodiment.</p><p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows an example of data produced from a process of generating a data set. The first column of <figref idref="DRAWINGS">FIG. <b>10</b></figref> shows top-view segment images of a map, the second column of <figref idref="DRAWINGS">FIG. <b>10</b></figref> shows edge images obtained by applying a Canny edge filter to the top-view segment images, and the third column of <figref idref="DRAWINGS">FIG. <b>10</b></figref> shows lane images of a grid.</p><p id="p-0097" num="0096">Here, the images in the first and second columns may be used as inputs of a lane extraction model, and the lane extraction model may be trained on the images in the third column.</p><p id="p-0098" num="0097">According to the embodiments of the present invention, a 3D HD map including a point cloud may be converted into a planar image, and a lane region may be extracted using a 2D image deep learning model.</p><p id="p-0099" num="0098">In particular, because deep learning and lane extraction is performed in a 2D domain, the amount of operations required to extract the coordinates of a lane may be minimized, and accordingly, lane information may be obtained in real time.</p><p id="p-0100" num="0099">Moreover, because the amount of operations required to extract the coordinates of a lane is minimized, a lane extraction process may be effectively automated and performed at a high speed with high accuracy.</p><p id="p-0101" num="0100">In addition, because black-and-white brightness, which is most important information for lane extraction on an image, is substituted by the reflection intensity of a LiDAR sensor, a deep learning model capable of accurately extracting a lane may be provided.</p><p id="p-0102" num="0101">Accordingly, during autonomous driving based on a 3D HD map, various kinds of information about a driving environment may be quickly and accurately determined through highly accurate real-time analysis.</p><p id="p-0103" num="0102">In addition, because accurate information about quickly changing and newly built roadways is obtained, an autonomous driving system may be updated in real time.</p><p id="p-0104" num="0103">Therefore, reliability and competitiveness may be enhanced in the field of autonomous driving, the field of road recognition, the field of lane recognition, and the field of HD road maps for autonomous driving, and the fields similar or related thereto, and more particularly, in the fields of road recognition and autonomous driving using LiDAR.</p><p id="p-0105" num="0104">While the present invention has been described with reference to the particular illustrative embodiments, it is not to be restricted by the embodiments but only by the appended claims. It is to be appreciated that those skilled in the art can change or modify the embodiments without departing from the scope and spirit of the present invention.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A lane extraction method using projection transformation of a three-dimensional (3D) point cloud map, the lane extraction method comprising:<claim-text>a deep learning step of projecting a training 3D high-definition (HD) map onto a planar image, extracting a lane on a two-dimensional (2D) map resulting from the projection, matching the 2D map with a planar image of the lane, setting the matched 2D map and planar image of the lane as training data, and training a deep learning model of extracting a lane from a map, based on the training data; and</claim-text><claim-text>a lane extraction step of projecting a target 3D HD map onto a planar map image, extracting a planar lane image from the planar map image using the deep learning model, and converting the planar lane image into 3D coordinates,</claim-text><claim-text>wherein the lane extraction step includes:</claim-text><claim-text>a target map projection process of projecting a target 3D point cloud map corresponding to the target 3D HD map onto a top-view image;</claim-text><claim-text>a lane extraction process of extracting the planar lane image from the planar map image using the deep learning model; and</claim-text><claim-text>a lane coordinate reconstruction process of reconstructing global coordinates from the planar lane image,</claim-text><claim-text>wherein the target map projection process includes segmenting a two-dimensional (2D) point cloud map into grids having a certain size, the 2D point cloud map corresponding to the top-view image onto which the target 3D point cloud map is projected,</claim-text><claim-text>the lane extraction process includes extracting the planar lane image including a gray image, in which a pixel of the planar lane image has a pixel value between 0 and 1 that expresses a probability of corresponding to the lane, and</claim-text><claim-text>the lane coordinate reconstruction process includes reconstructing a 3D coordinate point cloud from the planar lane image by using a global coordinate value assigned to each grid as an identification (ID), wherein pixel values of the planar lane image are binarized using a certain threshold, and coordinates of a pixel having a value of 1 is converted into global coordinates using the ID of each grid of the planar lane image.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The lane extraction method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the deep learning step includes:<claim-text>a training map projection process of projecting and converting a training 3D point cloud map corresponding to the training 3D HD map into a top-view image;</claim-text><claim-text>a lane projection process of extracting a lane point from the training 3D point cloud map and projecting and converting the lane point into the top-view image;</claim-text><claim-text>a training data setting process of matching and setting the 2D map resulting from the projection and the lane as the training data; and</claim-text><claim-text>a deep learning process of training the deep learning model based on the training data.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The lane extraction method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the training map projection process includes:<claim-text>a view conversion process of converting the training 3D point cloud map into the top-view image using reflection intensity;</claim-text><claim-text>a map segmentation process of segmenting the 2D point cloud map into grids having a certain size; and</claim-text><claim-text>a planar map imaging process of making the 2D point cloud map into an image grid by grid.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The lane extraction method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising, after the planar map imaging process, a meaningless image removing process of removing a grid planar map image having no roadway from the planar map image.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The lane extraction method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the lane projection process includes:<claim-text>a lane point extraction process of extracting a lane point from the training 3D point cloud map;</claim-text><claim-text>a line fitting process of fitting the lane point to a line; and</claim-text><claim-text>a planar lane imaging process of making the line into an image for each grid.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The lane extraction method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the training data setting process includes:<claim-text>an ID assignment process of assigning the global coordinate value to each grid as the ID; and</claim-text><claim-text>an image matching process of matching and setting a planar map image and a planar lane image, which are assigned a same ID, as an image pair.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The lane extraction method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the lane coordinate reconstruction process includes:<claim-text>a planar coordinate assignment process of converting the planar lane image into a lane point by assigning an x-coordinate and a y-coordinate to the planar lane image using the global coordinate value assigned to each grid;</claim-text><claim-text>a 3D coordinate determination process of determining coordinates of a point closest to the x- and y-coordinates on the target 3D HD map; and</claim-text><claim-text>a z-coordinate assignment process of assigning a z-coordinate of the point on the target 3D HD map to the lane point.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The lane extraction method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the lane coordinate reconstruction process includes:<claim-text>a planar coordinate assignment process of converting the planar lane image into a lane point by assigning an x-coordinate and a y-coordinate to the planar lane image using the global coordinate value assigned to each grid;</claim-text><claim-text>an adjacent point determination process of determining 3D points within a certain distance from the x- and y-coordinates on the target 3D HD map; and</claim-text><claim-text>a z-coordinate assignment process of assigning an average of z-coordinates of the 3D points to the lane point.</claim-text></claim-text></claim></claims></us-patent-application>