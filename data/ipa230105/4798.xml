<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004799A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004799</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17852954</doc-number><date>20220629</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc></classifications-cpc><invention-title id="d2e43">ONE-DIMENSIONAL-CONVOLUTION-BASED SIGNAL CLASSIFIER</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63217179</doc-number><date>20210630</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SRI International</orgname><address><city>Menlo Park</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Clymer</last-name><first-name>Bradley</first-name><address><city>Denver</city><state>CO</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An output module cooperates with a machine learning architecture to analyze parameter-varying signals. The signal-analyzing neural-network contains at least a one-dimensional-convolution layer to apply a series of i) a one-dimensional convolutional-based operation on the data of the parameter-varying signals ii) followed by a non-linear activation function on the data of the parameter-varying signals, under analysis, with multiple representations of the parameter-varying signals. Each representation of the parameter-varying signal is analyzed in a different domain in order to produce a classification of an entity into a specific category of an object corresponding to identifying features of the time-varying signals. Branches of the signal-analyzing neural-network are constructed to apply at least two or more successive layers of the one-dimensional-convolution layer followed by the non-linear activation function layer to values of time and frequency features of the parameter-varying signals, under analysis.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="207.86mm" wi="158.75mm" file="US20230004799A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="236.39mm" wi="173.65mm" file="US20230004799A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="253.32mm" wi="170.35mm" file="US20230004799A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="247.65mm" wi="166.79mm" file="US20230004799A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="216.83mm" wi="180.59mm" file="US20230004799A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="236.22mm" wi="175.85mm" file="US20230004799A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="229.87mm" wi="168.74mm" orientation="landscape" file="US20230004799A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="223.86mm" wi="159.17mm" orientation="landscape" file="US20230004799A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE</heading><p id="p-0002" num="0001">This application claims priority under 35 USC 119 to U.S. provisional patent application Ser. 63/217,179, titled &#x201c;one-dimensional-convolution-based signal classifier,&#x201d; filed 30 Jun. 2021, which the disclosure of such is incorporated herein by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">Embodiments of this disclosure relate generally to artificial intelligence using a network to classify signals.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Currently, identifying electromagnetic signals can be performed by manual or semi-automated observation of a given receiver's spectrum, and sometimes with a filter bank, or cyclostationary processing. A limit is imposed by the need to guess at what might already be there, as well as the computational complexity of performing the search in real time. Thus, there is a need to identify electromagnetic signals without any prior knowledge of their presence or type.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">Provided herein are various methods, apparatuses, and systems for an artificial intelligence architecture.</p><p id="p-0006" num="0005">In an embodiment, the output module can work with one or more processors to execute instructions and a memory to store data and instructions. The output module cooperates with a machine learning architecture to analyze parameter-varying signals. The machine learning architecture uses a signal-analyzing neural-network. The signal-analyzing neural-network is trained with one or more machine learning algorithms on sampled data of the parameter-varying signals. The signal-analyzing neural-network contains at least a one-dimensional-convolution layer to apply a series of i) a one-dimensional convolutional-based operation on the data of the parameter-varying signals ii) followed by a non-linear activation function on the data of the parameter-varying signals, under analysis, with multiple representations of the parameter-varying signals. Each representation of the parameter-varying signal is analyzed in a different domain. Each representation of the parameter-varying signal is analyzed in a different domain with the series of i) the one-dimensional convolutional-based operation on the data of the time-varying signals ii) followed by the non-linear activation function on the data of the parameter-varying signals, under analysis, in order to produce a classification of an entity into a specific category of an object corresponding to identifying features of the parameter-varying signals.</p><p id="p-0007" num="0006">One or more branches of the signal-analyzing neural-network can be constructed to apply at least two or more successive layers of the one-dimensional-convolution layer to apply the one-dimensional-convolution based operation followed by the non-linear activation function layer to apply the non-linear activation function to data values of time and frequency in the time-varying signals, under analysis.</p><p id="p-0008" num="0007">These and many more embodiments are discussed.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">DRAWINGS</heading><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a block diagram of an embodiment of an example device containing an output module configured to work with one or more machine learning architectures that use a signal-analyzing neural-network;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a diagram of a machine learning architecture with 1) a preprocessing section that expands the data of the parameter-varying signals, under analysis, from one dimension to multiple dimensions, 2) a feature extraction section that uses successive layers of a series of i) a one-dimensional convolutional-based operation on the data of the parameter-varying signals ii) followed by a non-linear activation function on the data of the parameter-varying signals, under analysis, with multiple representations of the parameter-varying signals, and 3) a classification section that classifies the parameter-varying signals under analysis into a specific category (from multiple different possible categories) based on the extracted features.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a block diagram of an embodiment of an example machine learning architecture that uses a signal-analyzing neural-network with at least a one-dimensional convolutional-based operation on the sampled data of the time-varying signals under analysis;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> illustrate block diagrams of an embodiment of another embodiment of an example machine learning architecture that uses a signal-analyzing neural-network to contain a one-dimensional-convolution layer to apply a one-dimensional convolutional-based operation on the sampled data of the time-varying signals followed by a non-linear activation function layer on the sampled data of the time-varying signals under analysis;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a diagram of a number of electronic systems and devices communicating with each other in a network environment in accordance with an embodiment of the device with the machine learning architecture that use a signal-analyzing neural-network; and</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a diagram of an embodiment of a computing device that can be a part of the systems associated with the device and its associated modules and the machine learning architecture discussed herein.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0015" num="0014">While the design is subject to various modifications, equivalents, and alternative forms, specific embodiments thereof have been shown by way of example in the drawings and will now be described in detail. It should be understood that the design is not limited to the particular embodiments disclosed, but&#x2014;on the contrary&#x2014;the intention is to cover all modifications, equivalents, and alternative forms using the specific embodiments.</p><heading id="h-0006" level="1">DESCRIPTION</heading><p id="p-0016" num="0015">In the following description, numerous specific details can be set forth, such as examples of specific data signals, named components, number of frames, etc., in order to provide a thorough understanding of the present design. It will be apparent, however, to one of ordinary skill in the art that the present design can be practiced without these specific details. In other instances, well known components or methods have not been described in detail but rather in a block diagram in order to avoid unnecessarily obscuring the present design. Further, specific numeric references such as the first server, can be made. However, the specific numeric reference should not be interpreted as a literal sequential order but rather interpreted that the first server is different than a second server. Thus, the specific details set forth can be merely exemplary. The specific details can be varied from and still be contemplated to be within the spirit and scope of the present design. The term &#x201c;coupled&#x201d; is defined as meaning connected either directly to the component or indirectly to the component through another component.</p><p id="p-0017" num="0016">Described herein includes machine learning architectures that can operate on signals, such as complex-valued Radio Frequency data, and provide a subsequent classification. The machine learning architectures use machine learning algorithms to automatically identify any time-varying signal types on which it is trained.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a block diagram of an embodiment of an example device containing an output module configured to work with one or more machine learning architectures that use a signal-analyzing neural-network.</p><p id="p-0019" num="0018">The device <b>100</b> may include a gather module, an output module, a formatting module, a data store, and one or more machine learning architectures trained with machine learning on time-varying signals. Note, the formatting module and the user interface may be implemented as one module or separate modules.</p><p id="p-0020" num="0019">The gather module can collect the data, such as time-varying signals, from sensors to be analyzed. The gather module can also get data from the data store to be used in the analysis.</p><p id="p-0021" num="0020">The output module can be configured to work with one or more processors to execute instructions and one or more memories to store data and instructions. The output module can be configured to analyze and assess the outputted results of one or more of the machine learning architectures trained with machine learning on time-varying signals using a convolution operation.</p><p id="p-0022" num="0021">The output module cooperating with the one or more machine learning architectures is configured to analyze a group of two or more time-varying signals (e.g. electromagnetic signals, sound waves, radio frequency waves, etc.) composed of complex-valued features/parameters. The complex-valued features/parameters include can include i) real and imaginary valued features/parameters, ii) real-valued features/parameters, iii) imaginary-valued features/parameters, and iv) any combination of these features/parameters. Complex numbers/valued data can be one-dimensional data with two-dimensional vectors whose components are the so-called real part and imaginary part. Note, a real-valued wave magnitude is being measured, but the measurement is captured in the real and imaginary parts of the number in equally real ways; the complex nature captures the phase relative to a reference oscillator, and is a very real thing.</p><p id="p-0023" num="0022">The data store cooperates with the data gather module to collect and maintain historical data of processes and their connections, including sensor data, meta data for the sensor data, etc., which is updated over time as the device <b>100</b> is in operation.</p><p id="p-0024" num="0023">The output module can be configured to analyze and assess the outputted results of one or more of the machine learning architectures trained with machine learning. The output module can be configured to cooperate with one or more of the machine learning architectures trained with machine learning to supply the initial data and/or any supplemental data subsequently requested by the artificial intelligence architecture.</p><p id="p-0025" num="0024">The data gather module with the output module cooperates to supply further data and/or metrics requested by the output module.</p><p id="p-0026" num="0025">The device <b>100</b> is configurable by its user interface, by a user, enabling the user to customize and configure various settings as well as supply input data for use by the output module and/or the machine learning architectures.</p><p id="p-0027" num="0026">The formatting module cooperates with the output module to output a formalized report as well as different parameters for a human user's consumption in a medium of any of 1) printable report, 2) presented digitally on a user interface, 3) in a machine-readable format for further use in machine-learning reinforcement and refinement, or 4) any combination of the three. The indications can be a classification of detected objects, various parameters, and other types of information. The formatting module can generate an output such as a printed or electronic report with the relevant data and calculations of how the classification was made, etc.</p><p id="p-0028" num="0027">The data store can store the data and meta data metrics for a period of time. In one embodiment, that can be at least 27 days. In other embodiments, this time can vary. This corpus of data is fully searchable. The data gather works with probes and/or sensors to monitor time-varying signal traffic and store and record the data and metadata associated with the time-varying signal traffic in the data store.</p><p id="p-0029" num="0028">The formatting module, cooperating with the output module, the gather module, and the machine learning architectures, can cooperate to present a representation of an output result from the machine learning architecture to be shown on a display screen indicating the specific category that the entity is classified to belong to from the sampled data of time-varying signals under analysis, without any prior knowledge of a presence or a type of the classified entity corresponding to the actually being contained or present within the radio frequency data, currently under analysis, in real time. In real time can be a time period of less than 500 microseconds from a time the probe/sensor receives the reflection of a time-varying signal, the analysis on the values and features of the reflection of a time-varying signal into a category, and then the display of that information onto a display screen. Note, the sampled data of time-varying signals under analysis is sampled at a fixed rate, such as one sample every two milliseconds, etc.</p><p id="p-0030" num="0029">A motivation for this approach is the speed of one-dimensional convolutions, owing directly to their single dimensionality. Similar to that of classical matched filter techniques, real one-dimensional convolutions require O(N<sup>2</sup>) operations, for a filter of length N. In comparison to two-dimensional convolutions&#x2014;which require O(N<sup>4</sup>) operations&#x2014;the appeal of reducing operational dimensionality is clear. The constructed machine learning architecture will, in an example implementation, use successive series of hundreds of filters of length N (which also can be hundreds or thousands) in the one-dimensional convolutions acting as a filter; and thus, the one-dimensional convolutions save a huge amount of computation cycles and thus corresponding lower lag time and lower power consumption compared to for example a two-dimensional implementation. The computations and effects from the one-dimensional convolution operations allow a network based upon this type of architecture to achieve high accuracy as well as computational efficiency. The disclosed methods and apparatuses can also rely on a unique multiple value data representation and corresponding architecture.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a diagram of a machine learning architecture with 1) a preprocessing section that expands the data of the parameter-varying signals, under analysis, from one dimension to multiple dimensions, 2) a feature extraction section that uses successive layers of a series of i) a one-dimensional convolutional-based operation on the data of the parameter-varying signals ii) followed by a non-linear activation function on the data of the parameter-varying signals, under analysis, with multiple representations of the parameter-varying signals, and 3) a classification section that classifies the parameter-varying signals under analysis into a specific category (from multiple different possible categories) based on the extracted features. <figref idref="DRAWINGS">FIG. <b>2</b></figref> essentially illustrates a high level overview of the operations being performed by the machine learning architecture. The parameter-varying signals can be time-varying, space-varying, spectral-line-varying (gas spectrometer), etc. Below time-varying signals under analysis will be used as an example of the parameter-varying signals under analysis. The preprocessing section takes in a one dimensional parameter varying signal, breaks that into multiple different representations that each is represented in multiple different dimensions and then puts the multi-dimensional representation into a given domain for analysis within that domain.</p><p id="p-0032" num="0031">In an embodiment, an output module works with one or more processors to execute instructions and one or more memories to store data and instructions. The output module cooperates with a machine learning architecture to analyze time-varying signals. The machine learning architecture uses a signal-analyzing neural-network. Prior to deployment of the machine learning architecture composed with the signal-analyzing neural-network, the signal-analyzing neural-network is trained with labeled data and one or more supervised machine learning algorithms on sampled data of time-varying signals. The signal-analyzing neural-network contains a one-dimensional-convolution layer to apply, during deployment, a one-dimensional convolutional-based operation on the sampled data of the time-varying signals followed by a non-linear activation function layer on the sampled data of the time-varying signals, under analysis, in order to produce a classification of an entity into a specific category of an object corresponding to identifying features of the time-varying signals. Two or more branches of the signal-analyzing neural-network can be constructed to apply at least two or more successive layers of the one-dimensional-convolution layer to apply the one-dimensional-convolution based operation followed by the non-linear activation function layer to values of time and frequency features of the time-varying signals, under analysis.</p><p id="p-0033" num="0032">The signal-analyzing neural-network is constructed to include a first branch where input values of the parameter-varying signals in a first domain are supplied into the first branch of the signal-analyzing neural-network. A first one-dimensional-convolution layer in the first branch is configured to apply the one-dimensional convolutional-based operation on the input values of the parameter-varying signals in the first domain. A second branch has input values of the parameter-varying signals in a second domain supplied into the second branch of the signal-analyzing neural-network. A second one-dimensional-convolution layer in the second branch is configured to apply the one-dimensional convolutional-based operation on the input values of the parameter-varying signals in a second domain at a same time with operations in the first branch. Again, below time-varying signals, such as Radio Frequency waves under analysis will be used as an example of the parameter-varying signals under analysis in the Time and Frequency domains expanded from one dimension to multiple dimensions such as eight dimensions and then classified into a specific object.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a block diagram of an embodiment of an example machine learning architecture that uses a signal-analyzing neural-network with at least a one-dimensional convolutional-based operation on the sampled data of the time-varying signals under analysis.</p><p id="p-0035" num="0034">In general, data for time-varying signals, such as Radio Frequency signals, are complex-valued, but very few machine learning networks are optimized for handling complex-valued data. To address this, in some embodiments, the disclosed apparatuses and methods implement a complexity tradeoff in the representation of input data, expanding a single 1-dimensional time signal into an 8-dimensional time-and-frequency representation. The intent is to learn increasingly non-linear features of the time and frequency domains and combine these representations in later portions of the network for classification of those input signals into a category or a reduction of input dimensionality for post-processing.</p><p id="p-0036" num="0035">The one-dimensional complex-valued signal can be represented as a real-valued 8-dimensional matrix where real-imaginary, magnitude, and phase values for both the time-domain representation of the signal, and its corresponding frequency-domain representation uses the Fast Fourier Transform. For the two-domain case, there are four values each; and thus, eight total dimensions total. The values for the features can be i) in both the time and frequency domain, ii) each with their real and imaginary parts, iii) along with corresponding magnitude and phase information.</p><p id="p-0037" num="0036">The series of one-dimensional convolution operations (applications of are performed on the time and frequency domains separately, but in parallel, and then these representations can be combined at the later layers of the network. Note, in mathematics, a convolution can be a mathematical operation on two functions (f and g) that produces a third function (f*g) that expresses how the shape of one is modified by the other. The convolution can be the integral of the product of the two functions after one is reversed and shifted. The integral is evaluated for all values of shift, producing the convolution function. One-dimensional convolutional neural networks (CNNs) can be considered as modified filter banks. Note, a convolutional neural network can be a class of artificial neural network which use multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. In the case of the one implementation of a signal classifier based on one-dimensional convolutional neural networks, a parallel set of these convolution-based filter banks are trained in separate branches, and then concatenated in the final layers, as seen in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. During deployment of the machine learning architecture, the machine learning architecture can analyze sampled data in different domains in separate branches, and then concatenate in the final portions of the network, as seen in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0038" num="0037">The one or more machine learning architectures are configured to use a signal-analyzing neural-network <b>100</b> composed of two or more branches. The machine learning architectures can be implemented as an Artificial Intelligence classifier, an Artificial Intelligence model, etc. Prior to deployment of the machine learning architecture composed of a signal-analyzing neural-network <b>100</b> with two or more branches, the signal-analyzing neural-network <b>100</b> is trained with labeled data of time varying signals and one or more supervised machine learning algorithms (linear regression, random forest, support vector machines, etc. on the sampled data. The time-varying signals have values associated with the features for the time-varying signals. The signal-analyzing neural-network <b>100</b> is trained to recognize the values and features for the time-varying signals associated with different objects. During deployment, the signal-analyzing neural-network <b>100</b> is configured to contain a one-dimensional-convolution layer to apply a one-dimensional convolutional-based operation on the sampled data of the time-varying signals followed by a non-linear activation function layer on the sampled data of the time-varying signals, such as complex-valued radio frequency data, in order to produce a classification of an entity into a specific category of radio frequency source [e.g. radar reflection, digital radio signal] corresponding to identifying values and features of the time-varying signals. Note, the classification is derived from the training data on the values and features of time-varying signals corresponding to different known sources such as objects&#x2014;drone, rocket, bird, etc.</p><p id="p-0039" num="0038">The classification is derived from the training data on the values and features of time-varying signals with the one or more supervised machine learning algorithms objects from a group of two or more possible categories of objects.</p><p id="p-0040" num="0039">The measured values of the sampled data of time-varying signals under analysis can be supplied to the machine learning architectures using the signal-analyzing neural-network <b>100</b> through a multiple value data structure (e.g. matrix) with real and imaginary values, a set of vectors, etc. The signal-analyzing neural-network <b>100</b> can expand values for a one-dimensional time signal provided in a data matrix into an eight-dimensional time-and-frequency representation at the output of the signal-analyzing neural-network <b>100</b>, which will provide a wider amount of values and features that can be associated with each different potential type of object being classified into a specific category.</p><p id="p-0041" num="0040">Progressing Down the Example Machine Learning Architecture</p><p id="p-0042" num="0041">A multiple value data structure can be used to supply different values of the sampled data of the time-varying signals under analysis to the signal-analyzing neural-network <b>100</b>. The multiple value data structure to supply the different values of the sampled data of the time-varying signals can be a mathematical matrix supplying the different values as a one-dimensional time signal that is expanded into an eight-dimensional time-and-frequency representation via operations performed by and within the signal-analyzing neural-network <b>100</b>. For an example matrix:</p><p id="p-0043" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="5"><colspec colname="offset" colwidth="28pt" align="left"/><colspec colname="1" colwidth="21pt" align="center"/><colspec colname="2" colwidth="70pt" align="char"/><colspec colname="3" colwidth="21pt" align="center"/><colspec colname="4" colwidth="77pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>1</entry><entry>2</entry><entry>. . .</entry><entry>N</entry></row><row><entry/><entry>2</entry><entry>1</entry><entry>9</entry><entry>&#x2212;13</entry></row><row><entry/><entry>&#x2193;</entry><entry>&#x2212;3</entry><entry>7</entry><entry>4</entry></row><row><entry/><entry>N</entry><entry>&#x2212;6</entry><entry>5</entry><entry>8</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0044" num="0042">The neural network has an input block to input/consume values for the time-varying signals in a time domain, where the input block is configured to apply a Fast Fourier Transform on the input values for the time-varying signals under analysis from the time domain in order to produce values for the time-varying signals under analysis in the frequency domain. The Fast Fourier Transform is the very first thing that happens. The Fast Fourier Transform is what turns the values for the time parameters for the time-varying signal submitted as an input to the time branch of the neural network into the input to the frequency branch of the neural network.</p><p id="p-0045" num="0043">In this example, input values for 512 samples of the time-varying signals, via the Fast Fourier Transform, becomes 512 time domain samples and 512 frequency domain samples, which are represented as i) real, ii) imaginary, iii) magnitude, and iv) phase values for the time-varying signals.</p><p id="p-0046" num="0044">One or more portions, such as layers, of the signal-analyzing neural-network <b>100</b> are constructed to include:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0045">i) a left branch (e.g. a hierarchical layer and/or a parallel branch on a same layer) where input values of the time-varying signals in a time domain are supplied into and operated upon in the left branch of the signal-analyzing neural-network <b>100</b>. Multiple one-dimensional-convolution layers in the left branch are configured to apply the one-dimensional convolutional-based operation on the input values of the time-varying signals in a time domain, under analysis, and</li>        <li id="ul0002-0002" num="0046">ii) a second branch (e.g. a second layer and/or a parallel branch) where input values of the time-varying signals are supplied into and operated upon in the right branch of the signal-analyzing neural-network <b>100</b>. Multiple one-dimensional-convolution layers in the right branch are configured to apply the one-dimensional convolutional-based operation on the input values of the time-varying signals in a frequency domain, under analysis, at a same time and/or in parallel with the operations in the left branch.</li>    </ul>    </li></ul></p><p id="p-0047" num="0047">In this example, two (or more) branches of the signal-analyzing neural-network <b>100</b> are constructed to apply two or more successive layers of the one-dimensional-convolution layer to apply the one-dimensional-convolution based operation followed by the non-linear activation function layer to values of time and frequency features of the time-varying signals.</p><p id="p-0048" num="0048">The neural network can be a sequence of multiple iterations of the one-dimensional convolutional-based operation followed by a non-linear activation function in order to change each output of the one-dimensional convolutional-based operation from linear to non-linear.</p><p id="p-0049" num="0049">Note, conceptually, this architecture starts with a small number of short filters (e.g. one-dimensional conventional filters) early in the branches, and progressively increases the number of filters later in the network. This captures an increasingly non-linear set of feature relations as signals propagate through the machine learning architecture. The example machine learning architecture doubles the number of filters in subsequent layers of the network, from 32 up to 256.</p><p id="p-0050" num="0050">Overall, an output result (e.g. representation) in the time domain is generated by the left branch of the signal-analyzing neural-network <b>100</b>. An output result (e.g. representation) in the frequency domain is generated by the right branch of the signal-analyzing neural-network <b>100</b>. The output results from the left branch on the time domain and the right branch on the frequency domain of the signal-analyzing neural-network <b>100</b> are combined (e.g. concatenated) in a later portion (e.g. 2nd layer or a 3rd layer) of the signal-analyzing neural network.</p><p id="p-0051" num="0051">In an embodiment, the signal-analyzing neural-network <b>100</b> can be a convolutional neural network architecture built with a parallel set of one-dimensional-convolution layers, used as filter banks, and non-linear activation function layers after each filter bank, and then concatenation layer in a final portion of the signal-analyzing neural-network <b>100</b> to merge the output results from the parallel set of filter banks and non-linear activation function layers.</p><p id="p-0052" num="0052">Again, input values for 512 samples of the time-varying signals, via the FFT becomes 512 time domain samples and 512 frequency domain samples, which are represented as real and imaginary i) magnitude values and ii) phase values. The example machine learning architecture doubles the number of filters in subsequent layers of the network, from 32 up to 256. In the layer below the input layer, the first filter block has 32 convolutional filters of length 256 to operate on the frequency domain [and the same occurs in the time domain in the other branch of the network] to change the shape of the 512 frequency samples from the input block into 256 frequency samples at the output of the filter block.</p><p id="p-0053" num="0053">Generally, each filter layer/block frequency data samples are being operated upon to change the shape of the data by ratio that the designer choses, such as a multiplier of four. The second filter block has 64 convolutional filters of length 64 to operate on the frequency domain [and the same occurs in the time domain in the other branch of the network] to change the shape of the 256 frequency samples from the output of a first non-linearity function block into 64 frequency samples at the output of the filter block.</p><p id="p-0054" num="0054">Next, all of the output numbers from each convolution operation/filter such as 256 outputs from the upstream convolution block are supplied to the input of each of the neuron blocks of the non-linearity function, then they're all added up and pass through a non-linearity in that non-linearity function block and then the output of that is passed to the next layer block. Each neuron block can have varying weights so that the inputs come from the outputs of the previous layer/block but due to the varying weights the output values will vary. The same process occurs at the next layer of neurons. All of the inputs from all the previous layers are added up and then pass through a non-linearity and that gives some value. The neural network contains a sequence of multiple iterations of one-dimensional convolutional layers. Each one-dimensional convolutional layer is configured to apply the one-dimensional convolutional-based operation followed by the non-linear activation function layer in order to change each output of each one-dimensional convolutional layer from a linear feature of the time-varying signal to a non-linear feature. Thus, in each branch, after each convolution operation/filter, there is a non-linear activation function block that transforms the output from being a linear representation of its inputs to a non-linear one. Also, the more layers of non-linearity in the constructed network, the more non-linear phenomenology the architecture can approximate. The amount of successive filters/convolution operations and non-linearity functions created in the network is set to be enough to statistically close to 100% of the time, based on the type of time-varying signal being analyzed and an amount of categories to classify, transform a linear value over to the non-linear function to produce a classification of an entity into a specific category of a source, such as an object, corresponding to identifying features of the time-varying signals with a reliability of, for example, 95%. In another embodiment, the amount of successive filters/convolution operations and non-linearity functions created in the network is set to be enough to produce a classification of an entity into a specific category of an object corresponding to identifying features of the time-varying signals with a reliability of, for example, 99%. In another embodiment, the amount of successive filters/convolution operations and non-linearity functions created in the network is set to be enough to produce a classification of an entity into a specific category of an object corresponding to identifying features of the time-varying signals with a reliability of, for example, 99.9%.</p><p id="p-0055" num="0055">In the example architecture, there are two additional filter blocks in a series of filter blocks feeding non-non-linearity function blocks and then a data reshaping block is next in the network for each branch. Again an example number of 4 filter blocks are shown but far more filters could be in this series before the reshaping block.</p><p id="p-0056" num="0056">Thus, the third filter block has 128 convolutional filters of length 16 to operate on the frequency domain [and similarly time domain in the other branch of the network] to change the shape of the 64 frequency samples from the output of a second non-linearity function block into 16 frequency samples at the output of the filter block.</p><p id="p-0057" num="0057">Note, a rectified linear unit (ReLU) is an example of a non-linear activation function, meaning that is output is not a scalar (single number) multiple of its input, which can be observed on a typical X-Y graph as a line which is not straight. In the ReLU, for example, the function's graph has a bend at X=0, showing different behavior for positive and negative values of X. The rectified linear unit bends the output at the x-axis to cause a non-linearity.</p><p id="p-0058" num="0058">A dropout layer exists in the branch but its purpose will be discussed later as its function aids most during the training of this signal-analyzing neural-network <b>100</b>.</p><p id="p-0059" num="0059">Finally, batch normalization layers are inserted in, for example, the first and fourth layers of the network, for the purpose of improving stability, by centering and scaling activations to achieve a fixed mean and variance, thereby reducing internal covariate shift within the network.</p><p id="p-0060" num="0060">Next, the branch in the example network has a reshaping block. The reshaping block is configured to reshape the current data for the samples roughly back to the dimensionality of the initial input samples. In this example machine learning architecture, the data is reshaped into 1024 frequency samples.</p><p id="p-0061" num="0061">The first dense layer in the signal-analyzing neural-network <b>100</b> is where everything is connected to everything else, except that the data goes from 1024 into every point in every point in the 64 in that dense connection.</p><p id="p-0062" num="0062">In the dense layers, all outputs of the neurons' are connected to the inputs of subsequent neurons and thus connected everywhere in a multilayer perceptron. The successive dense layers in the branches change the one-dimensional signal from the convolutional filters that is expanded into an 8-dimensional signal coming out of the successive dense layers.</p><p id="p-0063" num="0063">The neural network uses two parallel branches of one-dimensional convolution neural networks, one for time, one for frequency, and a final portion of the signal-analyzing neural-network <b>100</b> combining the two parallel branches via a concatenation, and then two or more densely connected layers in the final portion of the network to decrease the dimensions to an amount of possible categories that the entity can be classified into.</p><p id="p-0064" num="0064">The batch normalization block in the left branch supplies an output of a length 64 vector. The batch normalization block supplies in the right branch an output of a length 64 vector. The concatenation operation takes those two vectors and reshapes the data and combines them into a size 128.</p><p id="p-0065" num="0065">The concatenation block merges the eight-dimensional time signal coming out of the time branch of the network and the eight-dimensional frequency signal coming out of the frequency branch of the network into an eight-dimensional time and frequency signal and changes the size of the data from an input 64 from the outputs of each branch to 128 at the output of the concatenation block.</p><p id="p-0066" num="0066">The final fully-connected layers in the final portion of the network operate to condense the amount of samples. The fully-connected layers can perform nonlinearly bounding the input feature space.</p><p id="p-0067" num="0067">Eventually, the output of the final stage of the fully-connected layer is equal to or approximately equal to an amount of different categories that the entity classified from the sampled data of time-varying signals, under analysis, can belong to.</p><p id="p-0068" num="0068">For example, when the machine learning architecture with the signal-analyzing neural-network <b>100</b> is used for Automatic Target Recognition of electromagnetic signals. The Automatic Target Recognition of electromagnetic signals can use the signal-analyzing neural-network <b>100</b> to correspond and identify at least unmanned aircraft (UAS) systems in radar data without any prior knowledge of the UAS actually being present in the scene or the type of UAS present in the scene, in the sampled electromagnetic signals, in real time.</p><p id="p-0069" num="0069">The signal-analyzing neural-network <b>100</b> is configured to apply successive one-dimensional-convolution layers followed by non-linear activation function layer on the sampled data of time-varying signals, such as sampled complex-valued radio frequency data, under analysis in order to produce a classification of an entity into a specific category of object into its specific category of drone, rocket, bird, etc. corresponding to identifying features of the time-varying signals. Classification occurs based on different reflections of electromagnetic signals. The machine learning is used to improve the classification of radar returns.</p><p id="p-0070" num="0070">In summary, one representation of a one-dimensional complex-valued signal as a real value eight-dimensional matrix with real and imaginary magnitude values and phase values is supplied as input data for both the time domain representation of the time-varying signal and its corresponding frequency domain representation, via a fast Fourier transform, two domains for values each eight total dimensions.</p><p id="p-0071" num="0071">Thus, the network combining a series of one-dimensional linear filters in the time domain branch and the frequency domain branch and eventually combines the sampled data under analysis together, then applies a dense layer.</p><p id="p-0072" num="0072">In this example, the time domain and the frequency domain were processed and analyzed separately and then they are being combined together to produce a final output.</p><p id="p-0073" num="0073">Note, that the both the functions of the batch normalization layer and the dropout layer can be used as conveniences during the pre-deployment training of the machine learning architecture. The dropout layer serves to prevent the network from fixating on any consistent features across samples, balancing learning across filters. The dropout layer at random zeroes out some of the vectors/samples being examined before they go to subsequent layers, so that the neural network can't get fixated on a particular feature/preventing over-emphasis of common features. The dropout functions can counter the computer dropping off minute differences mathematically due to typical computer rounding functions. In this case, the dropout function can be back-propagated through, so that error gets fed back through previous layers to update the numbers/values of the weights.</p><p id="p-0074" num="0074"><figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> illustrate block diagrams of an embodiment of an example machine learning architecture that uses a signal-analyzing neural-network to contain a one-dimensional-convolution layer to apply a one-dimensional convolutional-based operation on the sampled data of the time-varying signals followed by a non-linear activation function layer on the sampled data of the time-varying signals under analysis.</p><p id="p-0075" num="0075">The layers and blocks of the signal-analyzing neural-network <b>100</b> function similarly as described in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The signal-analyzing neural-network <b>100</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref> has two branches. Each branch of the signal-analyzing neural-network <b>100</b> has four one-dimensional-convolution layers in series to act as filters, each one-dimensional-convolution layer followed by a non-linear activation function layer on the sampled data of the time-varying signals, a reshaping block, two dense layers, and multiple drop out layers. The final portion contains a concatenation layer and two dense layers.</p><p id="p-0076" num="0076">The device with one or more processors, and a memory, can also have a radio frequency transmitter, and one or more antennas. The device is implemented in a Linear Frequency Modulated (LFM) Pulse-Doppler radar cooperating with a computing device with the machine learning architecture that uses the signal-analyzing neural-network <b>100</b> to analyze the sampled complex values of the radio frequency signals under analysis.</p><p id="p-0077" num="0077">The machine learning architecture that uses the signal-analyzing neural-network <b>100</b> uses one-dimensional convolution operations on the time and frequency domains separately and combines these representations at a later layer of the neural network. This helps identify and classify different reflections of electromagnetic signals without prior knowledge of their presence or type in the scene. In this example, the signal-analyzing neural-network <b>100</b> operates on complex-valued radio frequency data in order to produce a classification of an entity. The complex values of the radio frequency signals under analysis are supplied to the signal-analyzing neural-network <b>100</b> as a data structure of a matrix with real and imaginary values and then the signal-analyzing neural-network <b>100</b> expands a 1-dimensional time signal into an 8-dimensional time-and-frequency representation. The data structure is a multiple dimension matrix that can contain values of both real-valued and an imaginary valued data. Again, the one-dimensional complex-valued signal can be represented as a real-valued 8-dimensional matrix with real-imaginary, magnitude, and phase values for both the time-domain representation of the signal, and its corresponding frequency-domain.</p><p id="p-0078" num="0078">More Training Information</p><p id="p-0079" num="0079">Prior to deployment of the neural network composed of two or more branches, the neural network composed of two or more branches is trained with one or more supervised machine learning algorithms on sampled data to take in an input of the sampled data X, and produce a correct truth value, y, where truth-value y of the entity is assigned a class label/is classified into a specific category of object such as &#x201c;UAS,&#x201d; &#x201c;bird,&#x201d; &#x201c;rocket,&#x201d; etc. This is true for the example where the source is an object that has specific categories of objects such as &#x201c;UAS,&#x201d; &#x201c;bird,&#x201d; &#x201c;rocket,&#x201d; etc. When other sources are used then the labels will correspond to the specific categories for that source.</p><p id="p-0080" num="0080">The machine learning trains to automatically identify any signal types from the complex-valued RF data on which it is trained.</p><p id="p-0081" num="0081">In one embodiment, the network was trained on randomly-parameterized signals that were generated, which were combined at random amplitudes, and to which were applied additive white gaussian noise at a random SNR ranging from &#x2212;15 dB to 10 dB, as well as a random frequency shift to simulate imperfect basebanding prior to sampling. Each training example consisted of a 512&#xd7;8 array generated from a single length-512 complex time-domain sample, as described above. 30,000 training examples were generated, and 5,000 validation examples were used to generate the accuracy metrics.</p><p id="p-0082" num="0082">The machine learning architecture discussed herein may use many types of machine learning to train the signal-analyzing neural-network <b>100</b> capable of identifying objects from time varying signals into specific categories without needing a priori knowledge that a candidate is in the scene being analyzed. For example, the machine learning system <b>124</b> may apply one or more of nearest neighbor, naive Bayes, decision trees, linear regression, support vector machines, neural networks, k-Means clustering, Q-learning, temporal difference, deep adversarial networks, or other supervised, unsupervised, semi-supervised, or reinforcement learning algorithms to train one or more models for a suitable training set of previous time-varying signals such as electromagnetic signals flying through the air.</p><p id="p-0083" num="0083">The machine learning architecture cooperating with the output module to automatically identify any signal types from the complex-valued RF data on which it is trained.</p><p id="p-0084" num="0084">In one embodiment, this architecture was trained to classify Phase-Modulated Radar, Frequency Modulated Radar, and OFDM communications signals. Initial results show 97% accuracy for this reduced set of signal classes, which were combined and overlaid to test the robustness of the architecture. This can be shown in a confusion matrix. Note that accuracy starts to suffer significantly&#x2014;requiring more training data and time&#x2014;on signals below &#x2212;5 dB SNR.</p><p id="p-0085" num="0085">Modern ML architectures are often parameterized according to heuristics, and a more artistic than scientific approach. This can lead to a lack of model interpretability, and sub-optimal performance: a network can succeed according to accuracy metrics, while simultaneously performing far more computation than necessary. To address this efficiently, a flexible library known as Ray has been developed to work with TensorFlow. Ray allows a rapid, systematic sweeping of the hyperparameters which define a neural network, with a convenient integration into the TensorBoard platform commonly used with TensorFlow-based architectures. The Ray dashboard will permit an embodiment of the disclosed apparatuses and methods to minimize the network's size while improving training efficiency.</p><p id="p-0086" num="0086">While the focus has primarily been on the automotive radar, frequency-hopping spread spectrum, and OFDM QAM signals, the disclosed system and methods could also include a wider variety of signals such as those specific to Bluetooth, WiFi, and common cellular modulations such as Cyclic-Prefix OFDM. In one embodiment, the signal generation suite is built upon Matlab libraries, which will allow these extensions flexibly. In addition, more realistic channel models&#x2014;including background signals taken from existing data captures&#x2014;will enhance the robustness of the network, and reduce misclassification. Finally, the addition of multipath propagation will further buttress the fast classification of the one-dimensional-convolution neural-network (CNN) architecture.</p><p id="p-0087" num="0087">The techniques described in this disclosure may be implemented, at least in part, in hardware, software, firmware, or any combination thereof. For example, various aspects of the described techniques may be implemented within one or more processors, including one or more microprocessors, digital signal processors (DSPs), application specific integrated circuits (ASICs), field programmable gate arrays (FPGAs), or any other equivalent integrated or discrete logic circuitry, as well as any combinations of such components. The term &#x201c;processor&#x201d; or &#x201c;processing circuitry&#x201d; may generally refer to any of the foregoing logic circuitry, alone or in combination with other logic circuitry, or any other equivalent circuitry. A control unit comprising hardware may also perform one or more of the techniques of this disclosure. A module may be implemented in software stored in a memory and executed by one or more processors, in electronic hardware such as logic circuits, and any combination of both.</p><p id="p-0088" num="0088">Network</p><p id="p-0089" num="0089"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a diagram of a number of electronic systems and devices communicating with each other in a network environment in accordance with an embodiment of the device with the machine learning architecture that use a signal-analyzing neural-network. The network environment <b>800</b> has a communications network <b>820</b>. The network <b>820</b> can include one or more networks selected from an optical network, a cellular network, the Internet, a Local Area Network (&#x201c;LAN&#x201d;), a Wide Area Network (&#x201c;WAN&#x201d;), a satellite network, a fiber network, a cable network, and combinations thereof. In an embodiment, the communications network <b>820</b> is the Internet. As shown, there may be many server computing systems and many client computing systems connected to each other via the communications network <b>820</b>. However, it should be appreciated that, for example, a single client computing system can also be connected to a single server computing system. Thus, any combination of server computing systems and client computing systems may connect to each other via the communications network <b>820</b>.</p><p id="p-0090" num="0090">The communications network <b>820</b> can connect one or more server computing systems selected from at least a first server computing system <b>804</b>A and a second server computing system <b>804</b>B to each other and to at least one or more client computing systems as well. The server computing system <b>804</b>A can be, for example, the one or more server systems. The server computing systems <b>804</b>A and <b>804</b>B can each optionally include organized data structures such as databases <b>806</b>A and <b>806</b>B. Each of the one or more server computing systems can have one or more virtual server computing systems, and multiple virtual server computing systems can be implemented by design. Each of the one or more server computing systems can have one or more firewalls to protect data integrity.</p><p id="p-0091" num="0091">The at least one or more client computing systems can be selected from a first mobile computing device <b>802</b>A (e.g., smartphone with an Android-based operating system), a second mobile computing device <b>802</b>E (e.g., smartphone with an iOS-based operating system), a first wearable electronic device <b>802</b>C (e.g., a smartwatch), a first portable computer <b>802</b>B (e.g., laptop computer), a third mobile computing device or second portable computer <b>802</b>F (e.g., tablet with an Android- or iOS-based operating system), a smart device or system incorporated into a first smart vehicle with the radar system using the machine learning architecture <b>802</b>D, a smart device or system incorporating the radar system using the machine learning architecture <b>802</b>G, and the like. The device using the machine learning architecture can communicate and cooperate with other devices in the communications network <b>820</b>. The client computing system <b>802</b>B can be, for example, one of the one or more client systems <b>210</b>, and any one or more of the other client computing systems (e.g., <b>802</b>A, <b>802</b>C, <b>802</b>D, <b>802</b>E, <b>802</b>F, <b>802</b>G, <b>802</b>H, and/or <b>804</b>C) can include, for example, the software application or the hardware-based system in which the trained machine learning architecture can be deployed. Each of the one or more client computing systems can have one or more firewalls to protect data integrity.</p><p id="p-0092" num="0092">It should be appreciated that the use of the terms &#x201c;client computing system&#x201d; and &#x201c;server computing system&#x201d; is intended to indicate the system that generally initiates a communication and the system that generally responds to the communication. For example, a client computing system can generally initiate a communication and a server computing system generally responds to the communication. No hierarchy is implied unless explicitly stated. Thus, if the first portable computer <b>802</b>B (e.g., the client computing system) and the server computing system <b>804</b>A can both initiate and respond to communications, their communications can be viewed as peer-to-peer. Additionally, the server computing systems <b>804</b>A and <b>804</b>B include circuitry and software enabling communication with each other across the network <b>820</b>. Server <b>804</b>B may send, for example, simulator data to server <b>804</b>A.</p><p id="p-0093" num="0093">Any one or more of the server computing systems can be a cloud provider. A cloud provider can install and operate application software in a cloud (e.g., the network <b>820</b> such as the Internet) and cloud users can access the application software from one or more of the client computing systems. Generally, cloud users that have a cloud-based site in the cloud cannot solely manage a cloud infrastructure or platform where the application software runs. Thus, the server computing systems and organized data structures thereof can be shared resources, where each cloud user is given a certain amount of dedicated use of the shared resources. Each cloud user's cloud-based site can be given a virtual amount of dedicated space and bandwidth in the cloud. Cloud applications can be different from other applications in their scalability, which can be achieved by cloning tasks onto multiple virtual machines at run-time to meet changing work demand. Load balancers distribute the work over the set of virtual machines. This process is transparent to the cloud user, who sees only a single access point.</p><p id="p-0094" num="0094">Cloud-based remote access can be coded to utilize a protocol, such as Hypertext Transfer Protocol (&#x201c;HTTP&#x201d;), to engage in a request and response cycle with an application on a client computing system such as a web-browser application resident on the client computing system. The cloud-based remote access can be accessed by a smartphone, a desktop computer, a tablet, or any other client computing systems, anytime and/or anywhere. The cloud-based remote access is coded to engage in 1) the request and response cycle from all web browser based applications, 3) the request and response cycle from a dedicated on-line server, 4) the request and response cycle directly between a native application resident on a client device and the cloud-based remote access to another client computing system, and 5) combinations of these.</p><p id="p-0095" num="0095">In an embodiment, the server computing system <b>804</b>A can include a server engine, a web page management component or direct application component, a content management component, and a database management component. The server engine can perform basic processing and operating-system level tasks. The web page management component can handle creation and display or routing of web pages or screens associated with receiving and providing digital content and digital advertisements, through a browser. Likewise, the direct application component may work with a client app resident on a user's device. Users (e.g., cloud users) can access one or more of the server computing systems by means of a Uniform Resource Locator (&#x201c;URL&#x201d;) associated therewith. The content management component can handle most of the functions in the embodiments described herein. The database management component can include storage and retrieval tasks with respect to the database, queries to the database, and storage of data.</p><p id="p-0096" num="0096">In an embodiment, a server computing system can be configured to display information in a window, a web page, or the like. An application including any program modules, applications, services, processes, and other similar software executable when executed on, for example, the server computing system <b>804</b>A, can cause the server computing system <b>804</b>A to display windows and user interface screens in a portion of a display screen space.</p><p id="p-0097" num="0097">Each application has a code scripted to perform the functions that the software component is coded to carry out such as presenting fields to take details of desired information. Algorithms, routines, and engines within, for example, the server computing system <b>804</b>A can take the information from the presenting fields and put that information into an appropriate storage medium such as a database (e.g., database <b>806</b>A). A comparison wizard can be scripted to refer to a database and make use of such data. The applications may be hosted on, for example, the server computing system <b>804</b>A and served to the specific application or browser of, for example, the client computing system <b>802</b>B. The applications then serve windows or pages that allow entry of details.</p><p id="p-0098" num="0098">Computing Systems</p><p id="p-0099" num="0099"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a diagram of an embodiment of a computing device that can be a part of the systems associated with the device and its associated modules and the machine learning architecture discussed herein. The computing device <b>600</b> may include one or more processors or processing units <b>620</b> to execute instructions, one or more memories <b>630</b>-<b>632</b> to store information, one or more data input components <b>660</b>-<b>663</b> to receive data input from a user of the computing device <b>600</b>, one or more modules that include the management module, a network interface communication circuit <b>670</b> to establish a communication link to communicate with other computing devices external to the computing device, one or more sensors where an output from the sensors is used for sensing a specific triggering condition and then correspondingly generating one or more preprogrammed actions, a display screen <b>691</b> to display at least some of the information stored in the one or more memories <b>630</b>-<b>632</b> and other components. Note, portions of this system that are implemented in software <b>644</b>, <b>645</b>, <b>646</b> may be stored in the one or more memories <b>630</b>-<b>632</b> and are executed by the one or more processors <b>620</b>.</p><p id="p-0100" num="0100">As discussed, the device and its associated modules and the machine learning architecture can be implemented with aspects of the computing device. The output module can work with one or more processors to execute instructions and a memory to store data and instructions. The output module cooperates with a machine learning architecture to analyze parameter-varying signals. The machine learning architecture uses a signal-analyzing neural-network. The signal-analyzing neural-network is trained with one or more machine learning algorithms on sampled data of the parameter-varying signals. The signal-analyzing neural-network contains at least a one-dimensional-convolution layer to apply a series of i) a one-dimensional convolutional-based operation on the data of the time-varying signals ii) followed by a non-linear activation function on the data of the parameter-varying signals, under analysis, with multiple representations of the parameter-varying signals. Each representation of the parameter-varying signal is analyzed in a different domain. Each representation of the parameter-varying signal is analyzed in a different domain with the series of i) the one-dimensional convolutional-based operation on the data of the time-varying signals ii) followed by the non-linear activation function on the data of the parameter-varying signals, under analysis, in order to produce a classification of an entity into a specific category of an object corresponding to identifying features of the parameter-varying signals.</p><p id="p-0101" num="0101">The system memory <b>630</b> includes computer storage media in the form of volatile and/or nonvolatile memory such as read-only memory (ROM) <b>631</b> and random access memory (RAM) <b>632</b>. These computing machine-readable media can be any available media that can be accessed by computing system <b>600</b>. By way of example, and not limitation, computing machine-readable media use includes storage of information, such as computer-readable instructions, data structures, other executable software, or other data. Computer-storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other tangible medium which can be used to store the desired information and which can be accessed by the computing device <b>600</b>. Transitory media such as wireless channels are not included in the machine-readable media. Communication media typically embody computer readable instructions, data structures, other executable software, or other transport mechanism and includes any information delivery media.</p><p id="p-0102" num="0102">The system further includes a basic input/output system <b>633</b> (BIOS) containing the basic routines that help to transfer information between elements within the computing system <b>600</b>, such as during start-up, is typically stored in ROM <b>631</b>. RAM <b>632</b> typically contains data and/or software that are immediately accessible to and/or presently being operated on by the processing unit <b>620</b>. By way of example, and not limitation, the RAM <b>632</b> can include a portion of the operating system <b>634</b>, application programs <b>635</b>, other executable software <b>636</b>, and program data <b>637</b>.</p><p id="p-0103" num="0103">The computing system <b>600</b> can also include other removable/non-removable volatile/nonvolatile computer storage media. By way of example only, the system has a solid-state memory <b>641</b>. The solid-state memory <b>641</b> is typically connected to the system bus <b>621</b> through a non-removable memory interface such as interface <b>640</b>, and USB drive <b>651</b> is typically connected to the system bus <b>621</b> by a removable memory interface, such as interface <b>650</b>.</p><p id="p-0104" num="0104">A user may enter commands and information into the computing system <b>600</b> through input devices such as a keyboard, touchscreen, or software or hardware input buttons <b>662</b>, a microphone <b>663</b>, a pointing device and/or scrolling input component, such as a mouse, trackball or touch pad. These and other input devices are often connected to the processing unit <b>620</b> through a user input interface <b>660</b> that is coupled to the system bus <b>621</b>, but can be connected by other interface and bus structures, such as a parallel port, game port, or a universal serial bus (USB). A display monitor <b>691</b> or other type of display screen device is also connected to the system bus <b>621</b> via an interface, such as a display interface <b>690</b>. In addition to the monitor <b>691</b>, computing devices may also include other peripheral output devices such as speakers <b>697</b>, a vibrator <b>699</b>, and other output devices, which may be connected through an output peripheral interface <b>695</b>.</p><p id="p-0105" num="0105">The computing system <b>600</b> can operate in a networked environment using logical connections to one or more remote computers/client devices, such as a remote computing system <b>680</b>. The remote computing system <b>680</b> can a personal computer, a mobile computing device, a server, a router, a network PC, a peer device or other common network node, and typically includes many or all of the elements described above relative to the computing system <b>600</b>. The logical connections can include a personal area network (PAN) <b>672</b> (e.g., Bluetooth&#xae;), a local area network (LAN) <b>671</b> (e.g., Wi-Fi), and a wide area network (WAN) <b>673</b> (e.g., cellular network), but may also include other networks such as a personal area network (e.g., Bluetooth&#xae;). Such networking environments are commonplace in offices, enterprise-wide computer networks, intranets and the Internet. A browser application may be resonant on the computing device and stored in the memory.</p><p id="p-0106" num="0106">When used in a LAN networking environment, the computing system <b>600</b> is connected to the LAN <b>671</b> through a network interface <b>670</b>, which can be, for example, a Bluetooth&#xae; or Wi-Fi adapter. When used in a WAN networking environment (e.g., Internet), the computing system <b>600</b> typically includes some means for establishing communications over the WAN <b>673</b>. With respect to mobile telecommunication technologies, for example, a radio interface, which can be internal or external, can be connected to the system bus <b>621</b> via the network interface <b>670</b>, or other appropriate mechanism. In a networked environment, other software depicted relative to the computing system <b>600</b>, or portions thereof, may be stored in the remote memory storage device. By way of example, and not limitation, the system has remote application programs <b>685</b> as residing on remote computing device <b>680</b>. It will be appreciated that the network connections shown are examples and other means of establishing a communications link between the computing devices that may be used.</p><p id="p-0107" num="0107">As discussed, the computing system <b>600</b> can include mobile devices with a processing unit <b>620</b>, a memory (e.g., ROM <b>631</b>, RAM <b>632</b>, etc.), a built in battery to power the computing device, an AC power input to charge the battery, a display screen, a built-in Wi-Fi circuitry to wirelessly communicate with a remote computing device connected to network.</p><p id="p-0108" num="0108">It should be noted that the present design can be carried out on a computing system such as that described with respect to shown herein. However, the present design can be carried out on a server, a computing device devoted to message handling, or on a distributed system in which different portions of the present design are carried out on different parts of the distributed computing system.</p><p id="p-0109" num="0109">In some embodiments, software used to facilitate algorithms discussed herein can be embedded onto a non-transitory machine-readable medium. A machine-readable medium includes any mechanism that stores information in a form readable by a machine (e.g., a computer). For example, a non-transitory machine-readable medium can include read-only memory (ROM); random access memory (RAM); magnetic disk storage media; optical storage media; flash memory devices; Digital Versatile Disc (DVD's), EPROMs, EEPROMs, FLASH memory, magnetic or optical cards, or any type of media suitable for storing electronic instructions.</p><p id="p-0110" num="0110">Note, an application described herein includes but is not limited to software applications, mobile applications, and programs that are part of an operating system application. Some portions of this description are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here, and generally, conceived to be a self-consistent sequence of steps leading to a desired result. The steps are those requiring physical manipulations of physical quantities. Usually, though not necessarily, these quantities take the form of electrical or magnetic signals capable of being stored, transferred, combined, compared, and otherwise manipulated. It has proven convenient at times, principally for reasons of common usage, to refer to these signals as bits, values, elements, symbols, characters, terms, numbers, or the like. These algorithms can be written in a number of different software programming languages such as C, C++, HTTP, Java, Python, or other similar languages. Also, an algorithm can be implemented with lines of code in software, configured logic gates in software, or a combination of both. In an embodiment, the logic consists of electronic circuits that follow the rules of Boolean Logic, software that contain patterns of instructions, or any combination of both. Any portions of an algorithm implemented in software can be stored in an executable format in portion of a memory and is executed by one or more processors. In an embodiment, a module can be implemented with electronic circuits, software being stored in a memory and executed by one or more processors, and any combination of both.</p><p id="p-0111" num="0111">It should be borne in mind, however, that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the above discussions, it is appreciated that throughout the description, discussions utilizing terms such as &#x201c;processing&#x201d; or &#x201c;computing&#x201d; or &#x201c;calculating&#x201d; or &#x201c;determining&#x201d; or &#x201c;displaying&#x201d; or the like, refer to the action and processes of a computer system, or similar electronic computing device, that manipulates and transforms data represented as physical (electronic) quantities within the computer system's registers and memories into other data similarly represented as physical quantities within the computer system memories or registers, or other such information storage, transmission or display devices.</p><p id="p-0112" num="0112">Many functions performed by electronic hardware components can be duplicated by software emulation. Thus, a software program written to accomplish those same functions can emulate the functionality of the hardware components in input-output circuitry. Thus, provided herein are one or more non-transitory machine-readable medium configured to store instructions and data that when executed by one or more processors on the computing device of the foregoing system, causes the computing device to perform the operations outlined as described herein.</p><p id="p-0113" num="0113">References in the specification to &#x201c;an embodiment,&#x201d; &#x201c;an example&#x201d;, etc., indicate that the embodiment or example described may include a particular feature, structure, or characteristic, but every embodiment may not necessarily include the particular feature, structure, or characteristic. Such phrases can be not necessarily referring to the same embodiment. Further, when a particular feature, structure, or characteristic is described in connection with an embodiment, it is believed to be within the knowledge of one skilled in the art to affect such feature, structure, or characteristic in connection with other embodiments whether or not explicitly indicated.</p><p id="p-0114" num="0114">While the foregoing design and embodiments thereof have been provided in considerable detail, it is not the intention of the applicant(s) for the design and embodiments provided herein to be limiting. Additional adaptations and/or modifications are possible, and, in broader aspects, these adaptations and/or modifications are also encompassed. Accordingly, departures may be made from the foregoing design and embodiments without departing from the scope afforded by the following claims, which scope is only limited by the claims when appropriately construed.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An apparatus, comprising:<claim-text>an output module configured to work with one or more processors to execute instructions and a memory to store data and instructions, where the output module is configured to cooperate with a machine learning architecture to analyze parameter-varying signals, and</claim-text><claim-text>where the machine learning architecture is configured to use a signal-analyzing neural-network, where the signal-analyzing neural-network is trained with one or more machine learning algorithms on sampled data of the parameter-varying signals, where the signal-analyzing neural-network is configured to contain a one-dimensional-convolution layer to apply a series of i) a one-dimensional convolutional-based operation on the data of the parameter-varying signals ii) followed by a non-linear activation function on the data of the parameter-varying signals with multiple representations of the parameter-varying signals, where each representation of the parameter-varying signal is analyzed in a different domain, in order to produce a classification of an entity into a specific category of an object corresponding to identifying features of the parameter-varying signals.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, where a multiple value data structure is utilized to supply different values of the parameter-varying signals to the signal-analyzing neural-network, where the parameter-varying signals under analysis are time-varying signals.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, where one or more branches of the signal-analyzing neural-network are constructed to apply at least two or more successive layers of the one-dimensional-convolution layer to apply the one-dimensional-convolution based operation followed by a non-linear activation function layer to apply the non-linear activation function to data values of time and frequency in the time-varying signals.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The apparatus of <claim-ref idref="CLM-00003">claim 3</claim-ref>, where the signal-analyzing neural-network is a convolutional neural network architecture.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, where one or more portions of the signal-analyzing neural-network are constructed to include<claim-text>a first branch where input values of the time-varying signals in a first domain are supplied into the first branch of the signal-analyzing neural-network, where a first one-dimensional-convolution layer in the first branch is configured to apply the one-dimensional convolutional-based operation on the input values of the time-varying signals in the first domain, and</claim-text><claim-text>a second branch where input values of the time-varying signals in a second domain are supplied into the second branch of the signal-analyzing neural-network, where a second one-dimensional-convolution layer in the second branch is configured to apply the one-dimensional convolutional-based operation on the input values of the time-varying signals in a second domain at a same time with operations in the first branch.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, where a first output result in a first domain is generated by a first branch of the signal-analyzing neural-network, and where a second output result in a second domain is generated by a second branch of the signal-analyzing neural-network, where the first and second output results from the first branch on the first domain and the second branch on the second domain of the signal-analyzing neural-network are combined in a concatenation layer of a later portion of the signal-analyzing neural network.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, where the signal-analyzing neural-network has a final portion of the signal-analyzing neural-network containing a concatenation layer to combine the multiple representations from the different domains and one or more fully connected layers that are configured to determine the specific category of the object from a group of two or more possible categories of objects.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, where the signal-analyzing neural-network contains a sequence of multiple iterations of one-dimensional convolutional layers, where each one-dimensional convolutional layer is configured to apply the one-dimensional convolutional-based operation followed by the non-linear activation function layer in order to change each output of each one-dimensional convolutional layer from a linear feature of the time-varying signal into a non-linear feature.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a user interface configured to convey the produced classification of the entity into the specific category of the object corresponding to identifying features of the parameter-varying signals from a group of two or more possible categories of the object.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, where a multiple value data structure is configured to supply different values of sampled data of the time-varying signals as a matrix to supply the different values as a one-dimensional time signal that is expanded into a multiple dimensional time-and-frequency representation via operations performed by a preprocessing portion of the signal-analyzing neural-network.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, where the time-varying signals are radio frequency signals.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A method for a machine learning architecture, comprising:<claim-text>using the machine learning architecture with a signal-analyzing neural-network to analyze data of parameter-varying signals, where the signal-analyzing neural-network is trained with one or more machine learning algorithms on data of the parameter-varying signals, and</claim-text><claim-text>using a one-dimensional-convolution layer in the signal-analyzing neural-network to apply a series of i) a one-dimensional convolutional-based operation on the data of the parameter-varying signals ii) followed by a non-linear activation function on the data of the parameter-varying signals with multiple representations of the parameter-varying signals, where each representation of the parameter-varying signal is analyzed in a different domain, in order to produce a classification of an entity into a specific category of an object corresponding to identifying features of the parameter-varying signals.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A non-transitory machine-readable medium, which stores further instructions in the executable format by the one or more processors to cause operations as follows, comprising:<claim-text>using a machine learning architecture with a signal-analyzing neural-network to analyze data of parameter-varying signals, where the signal-analyzing neural-network is trained with one or more machine learning algorithms on data of the parameter-varying signals, and</claim-text><claim-text>using a one-dimensional-convolution layer in the signal-analyzing neural-network to apply a series of i) a one-dimensional convolutional-based operation on the data of the parameter-varying signals ii) followed by a non-linear activation function on the data of the parameter-varying signals with multiple representations of the parameter-varying signals, where each representation of the parameter-varying signal is analyzed in a different domain, in order to produce a classification of an entity into a specific category of an object corresponding to identifying features of the parameter-varying signals.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The non-transitory machine-readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, which stores further instructions in the executable format by the one or more processors to cause further operations as follows, comprising:<claim-text>where the parameter-varying signals under analysis are time-varying signals,</claim-text><claim-text>supplying input values of the time-varying signals in a time domain into a first branch of the signal-analyzing neural-network and then operating upon the time-varying signals in the time domain in the first branch of the signal-analyzing neural-network, where a first one-dimensional-convolution layer in the first branch is configured to apply the one-dimensional convolutional-based operation on the input values of the time-varying signals in the time domain, under analysis, and</claim-text><claim-text>supplying input values of the time-varying signals in a frequency domain into a second branch of the signal-analyzing neural-network and then operating upon the time-varying signals in the frequency domain in the second branch of the signal-analyzing neural-network, where a second one-dimensional-convolution layer in the second branch is configured to apply the one-dimensional convolutional-based operation on the input values of the time-varying signals in the frequency domain, under analysis.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The non-transitory machine-readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, which stores further instructions in the executable format by the one or more processors to cause further operations as follows, comprising:<claim-text>generating a first output result in a time domain in a first branch of the signal-analyzing neural-network,</claim-text><claim-text>generating a second output result in a frequency domain in a second branch of the signal-analyzing neural-network, and</claim-text><claim-text>combining the first and second output results from the first branch on the time domain and the second branch on the frequency domain of the signal-analyzing neural-network in a concatenation layer of a later portion of the signal-analyzing neural network.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory machine-readable medium of <claim-ref idref="CLM-00014">claim 14</claim-ref>, which stores further instructions in the executable format by the one or more processors to cause further operations as follows, comprising:<claim-text>using a matrix to supply different values of the data of the time-varying signals under analysis as a one-dimensional time signal that is expanded into a multiple-dimensional time-and-frequency representation via operations performed by and within the signal-analyzing neural-network.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory machine-readable medium of <claim-ref idref="CLM-00014">claim 14</claim-ref>, which stores further instructions in the executable format by the one or more processors to cause further operations as follows, comprising:<claim-text>using two or more successive layers of the one-dimensional-convolution layer to apply the one-dimensional-convolution based operation followed by a non-linear activation function layer to apply the non-linear activation function to the data of values of time and frequency features of the time-varying signals in order to change an output of the one-dimensional convolutional layer from a linear feature of the time-varying signal into a non-linear feature.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. An apparatus, comprising:<claim-text>a machine learning architecture configured to use a signal-analyzing neural-network, using the machine learning architecture with a signal-analyzing neural-network to analyze data of parameter-varying signals, where the signal-analyzing neural-network is trained with one or more machine learning algorithms on data of the parameter-varying signals, and</claim-text><claim-text>a one-dimensional-convolution layer in the signal-analyzing neural-network is configured to apply a series of i) a one-dimensional convolutional-based operation on the data of the parameter-varying signals ii) followed by a non-linear activation function on the data of the parameter-varying signals with multiple representations of the parameter-varying signals, where each representation of the parameter-varying signal is analyzed in a different domain in order to produce a classification of an entity into a specific category of an object corresponding to identifying features of the parameter-varying signals.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The apparatus of <claim-ref idref="CLM-00018">claim 18</claim-ref>, where the parameter-varying signals under analysis are time-varying signals, and<claim-text>where the signal-analyzing neural-network is constructed to include</claim-text><claim-text>i) a first branch where input values of the time-varying signals in a first domain are supplied into and operated upon in the first branch of the signal-analyzing neural-network, where a first one-dimensional-convolution layer in the first branch is configured to apply the one-dimensional convolutional-based operation on the input values of the time-varying signals in the first domain, under analysis, and</claim-text><claim-text>ii) a second branch where input values of the time-varying signals in a second domain are supplied into and operated upon in the second branch of the signal-analyzing neural-network, where a second one-dimensional-convolution layer in the second branch is configured to apply the one-dimensional convolutional-based operation on the input values of the time-varying signals in a second domain, under analysis, at a same time with operations in the first branch, where a first output result in the first domain is generated by the first branch of the signal-analyzing neural-network, and where a second output result in the second domain is generated by the second branch of the signal-analyzing neural-network, where the first and second output results from the first branch and the second branch of the signal-analyzing neural-network are combined in a later portion of the signal-analyzing neural network.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The apparatus of <claim-ref idref="CLM-00018">claim 18</claim-ref>, where the neural network contains a sequence of multiple iterations of one-dimensional convolutional layers, where each one-dimensional convolutional layer is configured to apply the one-dimensional convolutional-based operation followed by the non-linear activation function in order to change each output of each one-dimensional convolutional layer from a linear feature of the parameter-varying signal into a non-linear feature.</claim-text></claim></claims></us-patent-application>