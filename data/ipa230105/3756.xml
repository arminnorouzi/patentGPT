<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230003757A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230003757</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17942697</doc-number><date>20220912</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>IL</country><doc-number>273288</doc-number><date>20200312</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>P</subclass><main-group>3</main-group><subgroup>68</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>11</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>P</subclass><main-group>3</main-group><subgroup>68</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>11</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">SYSTEM AND METHOD FOR DETERMINING A RELATIVE MOTION BETWEEN TWO OR MORE OBJECTS</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/IL2021/050243</doc-number><date>20210304</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17942697</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>ELBIT SYSTEMS LTD.</orgname><address><city>Haifa</city><country>IL</country></address></addressbook><residence><country>IL</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>GUROVICH</last-name><first-name>Evgeni</first-name><address><city>Haifa</city><country>IL</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>OPHIR</last-name><first-name>Yoav</first-name><address><city>Haifa</city><country>IL</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>ELBIT SYSTEMS LTD.</orgname><role>03</role><address><city>Haifa</city><country>IL</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems and methods for determining a relative motion between two or more objects are disclosed. The system may include an excitation unit adapted to be disposed on a first object and configured to excite and to induce at least one change in at least a portion of a second object. The system may include a sensing unit adapted to be disposed on the first object, the sensing unit may include at least one sensor configured to detect the at least one change in the second object at two or more different time instances and to generate corresponding two or more sensor output datasets. The system may include a processing unit configured to determine a relative motion between the first object and the second object based on the two or more sensor output datasets thereof.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="234.95mm" wi="86.70mm" file="US20230003757A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="207.69mm" wi="134.54mm" file="US20230003757A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="207.69mm" wi="134.54mm" file="US20230003757A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="271.44mm" wi="173.91mm" file="US20230003757A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="222.67mm" wi="175.85mm" file="US20230003757A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="223.44mm" wi="176.19mm" file="US20230003757A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="263.57mm" wi="139.95mm" file="US20230003757A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="240.20mm" wi="78.82mm" file="US20230003757A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="157.14mm" wi="161.63mm" file="US20230003757A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="211.41mm" wi="126.66mm" file="US20230003757A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="157.14mm" wi="161.63mm" file="US20230003757A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This Application is a continuation of PCT Application No. PCT/IL2021/050243 filed on Mar. 4, 2021, claiming priority from Israeli Patent Application No. 273288 filed on Mar. 12, 2020, all of which are incorporated herein by reference in their entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD OF THE INVENTION</heading><p id="p-0003" num="0002">The present invention relates to the field of determining a relative motion between objects, and more particularly, to remote determining thereof.</p><heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading><p id="p-0004" num="0003">Current systems and methods for determining a relative motion between objects typically track visual markers on the objects (e.g., such as shape and/or color of the object) or utilize Doppler effect. Such systems and methods may fail to determine a relative motion between objects when the objects lack the visual markers and/or when the objects have uniform structure. Furthermore, some of such systems and methods (e.g., radar-like detectors of relative motion) may be sensitive to, for example, geometry of objects, physical properties of object's surface, etc.</p><heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0005" num="0004">Some embodiments of the present invention may provide a system for determining a relative motion between two or more objects, the system may include: an excitation unit adapted to be disposed on a first object, the excitation unit is configured to excite and to induce at least one change in at least a portion of a second object; a sensing unit adapted to be disposed on the first object, the sensing unit may include at least one sensor configured to detect the at least one change in the second object at two or more different time instances and to generate corresponding two or more sensor output datasets; and a processing unit in communication with the sensing unit, the processing unit is configured to determine a relative motion between the first object and the second object based on the two or more sensor output datasets.</p><p id="p-0006" num="0005">In some embodiments, the processing unit is configured to quantify the relative motion between the first object and the second object based on the two or more sensor output datasets thereof.</p><p id="p-0007" num="0006">In some embodiments, the at least one change is at least one of: a physical change, a chemical change or a combination thereof.</p><p id="p-0008" num="0007">In some embodiments, the at least one change is temporal and wherein the second object reverts to its initial state prior to excitation after a period of time.</p><p id="p-0009" num="0008">In some embodiments, the processing unit is configured to determine a period of time after which a magnitude of the at least one temporal change reduces below a predefined threshold based on at least one of: a type of the at least one temporal change, one or more surface parameters of the second object and environmental conditions.</p><p id="p-0010" num="0009">In some embodiments, the excitation unit is configured to set a magnitude of the excitation to adjust the period of time to a desired value.</p><p id="p-0011" num="0010">In some embodiments, the excitation unit is configured to induce at least one pattern of changes, wherein the at least one pattern of changes may include multiple spots, each of the multiple spots may include the at least one change.</p><p id="p-0012" num="0011">In some embodiments, the processing unit is configured to detect, based on at least one of the two or more sensor output datasets, an upcoming disappearance of the at least one change from a field-of-view (FOV) of the at least one sensor.</p><p id="p-0013" num="0012">In some embodiments, the disappearance of the at least one change from the FOV of the at least one sensor is due to at least one of: temporarily nature of the at least one change, relative motion between the first object and the second object and at least partial eclipse of the FOV by a third object.</p><p id="p-0014" num="0013">In some embodiments, upon the detection of the upcoming disappearance of the at least one change from the FOV of the at least one sensor, the processing unit is configured to control the excitation unit to excite at least a portion of the second object before the at least one change disappears from the FOV of the at least one sensor.</p><p id="p-0015" num="0014">In some embodiments, the processing unit is configured to control the excitation unit to excite at least one of: the same portion of the second object that has been excited at a preceding excitation cycle; and a second portion of the second object such that previously excited portion of the second object and the second portion thereof are both in the FOV of the at least one sensor during at least one time point.</p><p id="p-0016" num="0015">In some embodiments, the excitation unit is configured to induce, during at least one of excitation cycles, the at least one change that is different from the at least one change induced during other excitation cycles.</p><p id="p-0017" num="0016">In some embodiments, the excitation unit is configured to encode a supplementary information in the at least one change.</p><p id="p-0018" num="0017">In some embodiments, the system may include at least one distance sensor adapted to be disposed on at least one of the first object and the second object, the at least one distance sensor is configured to measure a distance between the first object and the second object.</p><p id="p-0019" num="0018">In some embodiments, the processing unit is configured to determine at least one of: determine a real-world geographical location of the first object based on a real-world geographical location of the second object, the determined relative motion between the first object and the second object, the measured distance between the first object and the second object and a known real-world geographical location of the first object at some time instance during the tracking process; and determine a real-world geographical location of the second object based on a real-world geographical location of the first object, the determined relative motion between the first object, the second object and the measured distance between the first object and the second object and a known real-world geographical location of the second object at some time instance during the tracking process.</p><p id="p-0020" num="0019">Some embodiments of the present invention may provide a method of determining a relative motion between two or more objects, the method may include: exciting, by an excitation unit disposed on a first object, at least a portion of a second object to induce at least one change therein; detecting, by at least one sensor of a sensing unit disposed on the first object, the at least one change in the second object at two or more different time instances; generating, by the at least one sensor, corresponding two or more sensor output datasets; and determining, by a processing unit, a relative motion between the first object and the second object based on the two or more sensor output datasets.</p><p id="p-0021" num="0020">Some embodiments may include quantifying, by the processing unit, the relative motion between the first object and the second object based on the two or more sensor output datasets thereof.</p><p id="p-0022" num="0021">Some embodiments may include inducing, by the excitation unit, at least one of: a physical change, a chemical change or a combination thereof, in the second object.</p><p id="p-0023" num="0022">Some embodiments may include inducing, by the excitation unit, at least one temporal change in the second object such that the second body reverts to its initial state prior to excitation after a period of time.</p><p id="p-0024" num="0023">Some embodiments may include determining, by the processing unit, a period of time after which a magnitude of the at least one temporal change reduces below a predefined threshold based on at least one of: a type of the at least one temporal change, one or more surface parameters of the second object and environmental conditions.</p><p id="p-0025" num="0024">Some embodiments may include setting, by the excitation unit, a magnitude of the excitation to adjust the period of time to a desired value.</p><p id="p-0026" num="0025">Some embodiments may include inducing, by the excitation unit, at least one pattern of changes, wherein the at least one pattern of changes may include multiple spots, each of the multiple spots may include the at least one change.</p><p id="p-0027" num="0026">Some embodiments may include detecting, by the processing unit, based on at least one of the two or more sensor output datasets, an upcoming disappearance of the at least one change from a field-of-view (FOV) of the at least one sensor of the sensing unit.</p><p id="p-0028" num="0027">In some embodiments, the disappearance of the at least one change from the FOV of the at least one sensor is due to at least one of: temporarily nature of the at least one change, relative motion between the first object and the second object and at least partial eclipse of the FOV by a third object.</p><p id="p-0029" num="0028">Some embodiments may include, upon the detection of the upcoming disappearance of the at least one change from the FOV of the at least one sensor, exciting, by the excitation unit, at least a portion of the second object before the at least one change disappears from the FOV of the at least one sensor.</p><p id="p-0030" num="0029">Some embodiments may include exciting, by the excitation unit, at least one of: the same portion of the second object that has been excited at a preceding excitation cycle; and a second portion of the second object such that the previously excited portion of the second object and the second portion of the second object are both in the FOV of the at least one sensor during at least one time point.</p><p id="p-0031" num="0030">Some embodiments may include inducing, by the excitation unit, during at least one of excitation cycles, the at least one change that is different from the at least one change induced during other excitation cycles.</p><p id="p-0032" num="0031">Some embodiments may include encoding, by the excitation unit, a supplementary information in the at least one change.</p><p id="p-0033" num="0032">Some embodiments may include measuring a distance between the first object and the second object by at least one distance sensor disposed on at least one of the first object and the second object.</p><p id="p-0034" num="0033">Some embodiments may include determining at least one of: determining, by the processing unit, a real-world geographical location of the first object based on a real-world geographical location of the second object, the determined relative motion between the first object and the second object, the measured distance between the first object and the second object and a known real-world geographical location of the first object at some time instance during the tracking process; and determining, by the processing unit, a real-world geographical location of the second object based on a real-world geographical location of the first object, the determined relative motion between the first object and the second object, the measured distance between the first object and the second object and a known real-world geographical location of the second object at some time instance during the tracking process.</p><p id="p-0035" num="0034">These, additional, and/or other aspects and/or advantages of the present invention are set forth in the detailed description which follows; possibly inferable from the detailed description; and/or learnable by practice of the present invention.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0036" num="0035">For a better understanding of embodiments of the invention and to show how the same can be carried into effect, reference will now be made, purely by way of example, to the accompanying drawings in which like numerals designate corresponding elements or sections throughout.</p><p id="p-0037" num="0036">In the accompanying drawings:</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> are schematic block diagrams of a system for determining a relative motion between two or more objects, according to some embodiments of the invention;</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>1</b>C</figref> is a schematic illustration of an example of determining a relative motion between at least a first object and at least a second object by a system for determining a relative motion between two or more object, according to some embodiments of the invention;</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIGS. <b>1</b>D and <b>1</b>E</figref> are schematic illustrations of a system for determining a relative motion between two or more object and capable of detecting an upcoming disappearance of at least one change or at least one pattern of changes from a field-of-view of a sensor of a sensing unit of the system, according to some embodiments of the invention;</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>1</b>F</figref> depicts an example of determining a relative motion between two or more objects based on image frames obtained by sensor(s) of a sensing unit of a system <b>100</b> for determining a relative motion between two or more object, according to some embodiments of the invention;</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIGS. <b>2</b>A and <b>2</b>B</figref> are schematic illustrations of an excitation unit for a system for determining a relative motion between two or more objects, according to some embodiments of the invention;</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of a method of determining a relative motion between two or more objects, according to some embodiments of the invention;</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic block diagram of a system for determining a relative motion of at least one object with respect to a specified location, according to some embodiments of the invention; and</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of a method of determining a relative motion of at least one object with respect to a specified location, according to some embodiments of the invention.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0046" num="0045">It will be appreciated that, for simplicity and clarity of illustration, elements shown in the figures have not necessarily been drawn to scale. For example, the dimensions of some of the elements may be exaggerated relative to other elements for clarity. Further, where considered appropriate, reference numerals may be repeated among the figures to indicate corresponding or analogous elements.</p><heading id="h-0006" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading><p id="p-0047" num="0046">In the following description, various aspects of the present invention are described. For purposes of explanation, specific configurations and details are set forth in order to provide a thorough understanding of the present invention. However, it will also be apparent to one skilled in the art that the present invention can be practiced without the specific details presented herein. Furthermore, well known features can have been omitted or simplified in order not to obscure the present invention. With specific reference to the drawings, it is stressed that the particulars shown are by way of example and for purposes of illustrative discussion of the present invention only and are presented in the cause of providing what is believed to be the most useful and readily understood description of the principles and conceptual aspects of the invention. In this regard, no attempt is made to show structural details of the invention in more detail than is necessary for a fundamental understanding of the invention, the description taken with the drawings making apparent to those skilled in the art how the several forms of the invention can be embodied in practice.</p><p id="p-0048" num="0047">Before at least one embodiment of the invention is explained in detail, it is to be understood that the invention is not limited in its application to the details of construction and the arrangement of the components set forth in the following description or illustrated in the drawings. The invention is applicable to other embodiments that can be practiced or carried out in various ways as well as to combinations of the disclosed embodiments. Also, it is to be understood that the phraseology and terminology employed herein is for the purpose of description and should not be regarded as limiting.</p><p id="p-0049" num="0048">Unless specifically stated otherwise, as apparent from the following discussions, it is appreciated that throughout the specification discussions utilizing terms such as &#x201c;processing&#x201d;, &#x201c;computing&#x201d;, &#x201c;calculating&#x201d;, &#x201c;determining&#x201d;, &#x201c;enhancing&#x201d; or the like, refer to the action and/or processes of a computer or computing system, or similar electronic computing device, that manipulates and/or transforms data represented as physical, such as electronic, quantities within the computing system's registers and/or memories into other data similarly represented as physical quantities within the computing system's memories, registers or other such information storage, transmission or display devices. Any of the disclosed modules or units can be at least partially implemented by a computer processor.</p><p id="p-0050" num="0049">Generally, systems and methods for determining and/or quantifying a relative motion between two or more objects (e.g., between at least a first object and at least a second object) or between one or more objects and a specified location, are disclosed.</p><p id="p-0051" num="0050">According to some embodiments, the system may include an excitation unit, a sensing unit and a processing unit. The excitation unit and/or the sensing unit may be adapted to be disposed on, for example, a first object. The excitation unit may excite at least a portion of a second object and to induce at least one change therein. In some embodiments, the at least one change may be one of: a physical change, a chemical change or a combination thereof. In some embodiments, the at least one change is temporal, and the second body reverts to its initial state after a predetermined period of time.</p><p id="p-0052" num="0051">The sensing unit may detect the at least one change thereof at two or more different time instances (e.g., time points) and to generate corresponding two or more sensor output datasets.</p><p id="p-0053" num="0052">The processing unit may determine, and in some embodiments to quantify, a relative motion between, for example, the first object and the second object based on the two or more sensor output datasets thereof.</p><p id="p-0054" num="0053">According to some embodiments, the excitation unit and the sensing unit may be adapted to be disposed at, for example, a specified location in a terrain and the processing unit is configured to determine the relative motion of one or more objects with respect to the specified location thereof.</p><p id="p-0055" num="0054">Advantageously, the disclosed systems and methods may enable determination and/or quantification of the relative motion between two or more objects, and/or the relative motion between one or more objects and the specified location, for example in the case when the objects lack any (or sufficient number of) visual markers and/or have uniform structure. Furthermore, the determination and/or quantification of the relative motion thereof may be performed, in some embodiments, without inducing long-term changes (such as, for example, permanent visual markers or continuous electromagnetic radiation) in the objects.</p><p id="p-0056" num="0055">Reference is now made to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>, which are schematic block diagrams of a system <b>100</b> for determining a relative motion between two or more objects, according to some embodiments of the invention.</p><p id="p-0057" num="0056">According to some embodiments, system <b>100</b> may include an excitation unit <b>110</b>, a sensing unit <b>120</b> and a processing unit <b>130</b> (e.g., as shown in <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>). System <b>100</b> may determine and/or quantify a relative motion between, for example, two or more objects. For example, system <b>100</b> may determine and/or quantify a relative motion between a first object <b>92</b> and a second object <b>94</b>, as described below with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>.</p><p id="p-0058" num="0057">The two or more objects (e.g., first object <b>92</b> and/or second object <b>94</b>) may, for example, be vehicles, planes, humans, terrains, etc. For example, first object <b>92</b> may be a human or a vehicle and second object <b>94</b> may be a homogeneous (or substantially homogeneous) part of a terrain (e.g., a road or a wall) that has no distinguishing visible features. In this example, first object <b>92</b> (e.g., human/vehicle) may move through/along second object <b>94</b> (e.g., part of the terrain) and the relative motion therebetween may be determined and/or quantified by system <b>100</b>.</p><p id="p-0059" num="0058">According to some embodiments, excitation unit <b>110</b> may be adapted to be disposed on, for example, first object <b>92</b>. Excitation unit <b>110</b> may excite at least a portion <b>94</b><i>a </i>of second object <b>94</b> (e.g., as indicated by arrow <b>111</b> in <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>). Excitation <b>111</b> by excitation unit <b>110</b> may induce at least one change <b>112</b> at least in portion <b>94</b><i>a </i>of second object <b>94</b>.</p><p id="p-0060" num="0059">Change(s) <b>112</b> induced by excitation unit <b>110</b> in second object <b>94</b> may be physical change(s), chemical change(s) or a combination thereof.</p><p id="p-0061" num="0060">The physical change(s) (e.g., change(s) <b>112</b>) may, for example, include a change of a temperature of second body <b>94</b> at least within portion <b>94</b><i>b </i>thereof as compared to a temperature prior to excitation <b>111</b>.</p><p id="p-0062" num="0061">In another example, the physical change(s) may include a change of a density of second body <b>94</b> at least within portion <b>94</b><i>b </i>thereof as compared to a density prior to excitation <b>111</b>.</p><p id="p-0063" num="0062">In another example, the physical change(s) may include a change of a permeability of second body <b>94</b> at least within portion <b>94</b><i>b </i>thereof as compared to a permeability prior to excitation <b>111</b>.</p><p id="p-0064" num="0063">In another example, the physical change(s) may include an ionization of second body <b>94</b> at least in portion <b>94</b><i>a </i>thereof.</p><p id="p-0065" num="0064">In another example, the physical change(s) may include an excitation of electrons' energy levels of second body <b>94</b> at least within portion <b>94</b><i>a </i>thereof.</p><p id="p-0066" num="0065">In another example, the physical change(s) may include a change of a color of second body <b>94</b> at least within portion <b>94</b><i>b </i>thereof as compared to a color prior to excitation <b>111</b>.</p><p id="p-0067" num="0066">In another example, the physical change(s) may include a radiation of light at least from portion <b>94</b><i>b </i>of second object <b>94</b>.</p><p id="p-0068" num="0067">The chemical change(s) (e.g., change(s) <b>112</b>) may, for example, include a change of a chemical composition of second body <b>94</b> at least within portion <b>94</b> thereof.</p><p id="p-0069" num="0068">In various embodiments, excitation unit <b>110</b> may be an optical excitation unit. In this case, excitation unit <b>110</b> may illuminate second object <b>94</b> with at least one excitation light beam that may be a collimated light beam or a coherent laser beam. In some other embodiments, excitation unit <b>110</b> may be an acoustic excitation unit or a radiofrequency excitation unit.</p><p id="p-0070" num="0069">In some embodiments, excitation unit <b>110</b> may excite <b>111</b> two or more different portions <b>94</b><i>a</i>on second object <b>94</b> to thereby induce corresponding two or more changes <b>112</b> within two or more portions <b>94</b><i>a </i>thereof. Excitations <b>111</b> of two or more different portions <b>94</b><i>a </i>of second object <b>94</b> may be performed simultaneously or subsequently. In some embodiments, system <b>100</b> may include two or more excitation units <b>110</b> each configured to excite different portion <b>94</b><i>a </i>of second object <b>94</b>.</p><p id="p-0071" num="0070">In some embodiments, change(s) <b>112</b> induced by excitation unit <b>110</b> may be temporal, such that second body <b>94</b> reverts to its initial state (e.g., state prior to excitation <b>111</b>) after a period of time. The period of time may, for example, depend on the type of change(s) <b>112</b>, surface parameters of second object <b>94</b> (e.g., type of surface, whether the surface produces/removes heat, tec.) and environmental conditions (e.g., ambient temperature, etc.). The predetermined period of time may, for example, range between few seconds to few minutes.</p><p id="p-0072" num="0071">In some embodiments, processing unit <b>130</b> may determine a period of time after which a magnitude of temporal change(s) <b>112</b> reduces below a predefined threshold based on at least one of: a type of change(s) <b>112</b>, surface parameter(s) of second object <b>94</b> and environmental conditions. In some embodiments, excitation unit <b>110</b> may set a magnitude of excitation <b>111</b> to adjust the period of time to a desired value.</p><p id="p-0073" num="0072">According to some embodiments, sensing unit <b>120</b> may be adapted to be disposed on, for example, first object <b>92</b>. Sensing unit <b>120</b> may include at least one sensor <b>122</b>. Sensor(s) <b>122</b> may detect change(s) <b>112</b> in second object <b>94</b> at two or more different time instances (e.g., as indicated by doubled arrow <b>123</b> in <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>). Sensor(s) <b>122</b> may generate corresponding two or more sensor output datasets <b>124</b> based on the detected change(s) <b>112</b>. For example, each of two or more sensor output datasets <b>124</b> may correspond to one of the two or more different time instances. Sensor output datasets <b>124</b> may, for example, include sensor output data values representing change(s) <b>112</b> in second object <b>94</b>.</p><p id="p-0074" num="0073">In some embodiments, sensor(s) <b>122</b> may be optical sensor(s). The optical sensor(s) may, for example, be radioactive sensor(s), thermal sensor(s), etc. For example, when sensor(s) <b>122</b> is/are optical sensor(s), sensor output datasets <b>124</b> may include images of second object <b>94</b> (or of at least portion <b>94</b><i>a </i>thereof).</p><p id="p-0075" num="0074">In various embodiments, sensing unit <b>120</b> and/or sensor(s) <b>122</b> may be stationary or gimbaled. Processing unit <b>130</b> may track a relative position and orientation of sensing unit <b>120</b> and/or sensors <b>122</b> with respect to excitation unit <b>110</b> such that the relative position therebetween may be known at all times. In various embodiments, processing unit <b>130</b> may account for the tracked relative position and orientation of sensor(s) sensing unit <b>120</b> and/or sensors <b>122</b> and excitation unit <b>110</b> when determining and/or quantifying the relative motion between first object <b>92</b> and second object <b>94</b>.</p><p id="p-0076" num="0075">According to some embodiments, processing unit <b>130</b> may receive sensor output datasets <b>124</b> from sensing unit <b>120</b> (e.g., as shown in <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>). Processing unit <b>130</b> may determine the relative motion between first object <b>92</b> and second object <b>94</b> based on sensor output datasets <b>124</b>.</p><p id="p-0077" num="0076">In some embodiments, processing unit <b>130</b> may quantify the relative motion between first object <b>92</b> and second object <b>94</b> based on sensor output datasets <b>124</b>. The quantification of the relative motion between first object <b>92</b> and second object <b>94</b> may, for example, include calculation of a relative velocity, a relative acceleration and/or a relative motion vector (e.g., position and direction of motion) between first object <b>92</b> and second object <b>94</b>.</p><p id="p-0078" num="0077">One example of determining and/or quantifying the relative motion between two or more objects is described below with respect to <figref idref="DRAWINGS">FIG. <b>1</b>F</figref>.</p><p id="p-0079" num="0078">According to some embodiments, system <b>100</b> may include at least one distance sensor <b>140</b>. Distance sensor(s) <b>140</b> may be disposed on, for example, first object <b>92</b> and/or on second object <b>94</b> (e.g., as shown in <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>). Distance sensor(s) <b>94</b> may measure a distance between first object <b>92</b> and second object <b>94</b> (e.g., as indicated by doubled arrow <b>141</b> in <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>) and transmit data concerning the distance thereof to processing unit <b>130</b> (e.g., as indicated by arrow <b>142</b> in <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>).</p><p id="p-0080" num="0079">In some embodiments, processing unit <b>130</b> may determine a real-world geographical location of first object <b>92</b> based on a real-world geographical location of second object <b>94</b>, the determined relative motion between first object <b>92</b> and second object <b>94</b>, the measured distance between first object <b>92</b> and second object <b>94</b> and a known real-world geographical location of first object <b>92</b> at some time instance during the tracking process. In these embodiments, the real-world geographical location of second object <b>94</b> may be known or determined (e.g., using a geolocation sensor, such as a GPS sensor, disposed thereon).</p><p id="p-0081" num="0080">In some other embodiments, processing unit <b>130</b> may determine a real-world geographical location of second object <b>94</b> based on a real-world geographical location of first object <b>92</b>, the determined relative motion between first object <b>92</b> and second object <b>94</b>, the measured distance between first object <b>92</b> and second object <b>94</b> and a known real-world geographical location of second object <b>94</b> at some time instance during the tracking process. In these embodiments, the real-world geographical location of first object <b>92</b> may be known or determined (e.g., using a geolocation sensor, such as a GPS sensor, disposed thereon).</p><p id="p-0082" num="0081">According to some embodiments, excitation unit <b>110</b> may induce one or more patterns <b>114</b> of changes <b>112</b> in second object <b>94</b> (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>). Each of patterns <b>114</b> of changes <b>112</b> may include multiple spots of changes <b>112</b>. The spots may have any shape (e.g., points, lines, markers, figures, etc.) and dimensions. Sensing unit <b>120</b> may detect pattern(s) <b>114</b> of changes <b>112</b> at two or more different time instances to thereby generate corresponding two or more sensor output datasets <b>124</b> based on the detected pattern(s) <b>114</b> of changes <b>112</b>. Processing unit <b>130</b> determine and/or quantify relative motion between first object <b>92</b> and second object <b>94</b> based on the sensor output datasets.</p><p id="p-0083" num="0082">For example, excitation unit <b>110</b> may induce a pattern of thermal changes in second object <b>94</b>. In this example, sensing unit <b>120</b> may generate two or more thermal images of the pattern of changes (e.g., two or more sensor output datasets <b>124</b>) based on the detected pattern of thermal changes. Yet in this example, processing unit <b>130</b> may compare the two or more thermal images of the detected patterns of changes and determine the relative motion between first object <b>92</b> and second object <b>94</b> based on the comparison thereof. In some embodiments, processing unit <b>130</b> may quantify the relative motion between first object <b>92</b> and second object <b>94</b> based on the spots of the detected patterns (e.g., using triangulation techniques). In some embodiments, sensing unit <b>120</b> may include two or more sensors <b>122</b> and processing unit <b>130</b> may be configured to quantify the relative motion between first object <b>92</b> and second object <b>94</b> based on outputs from the two or more sensors <b>122</b> (e.g., using triangulation techniques).</p><p id="p-0084" num="0083">Reference is now made to <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>, which is a schematic illustration of an example of determining a relative motion between at least a first object <b>92</b> and at least a second object <b>94</b> by a system <b>100</b> for determining a relative motion between two or more object, according to some embodiments of the invention.</p><p id="p-0085" num="0084">In example schematically illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>, second object <b>94</b> is disposed at a first position (x<sub>1</sub>, y<sub>1</sub>, z<sub>1</sub>) relative to first object <b>92</b> at a first time point t<sub>1 </sub>(e.g., as shown in illustration <b>100</b><i>a </i>in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>). Yet in these examples, during a time interval &#x394;t<sub>12</sub>, second object <b>94</b> may move (e.g., as indicated by arrow <b>93</b> in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>) with respect to first object <b>92</b> by (&#x394;x<sub>12</sub>, &#x394;y<sub>12</sub>, &#x394;z<sub>12</sub>) such that at a second time point t<sub>2 </sub>(e.g., where t<sub>2</sub>=t<sub>1</sub>+&#x394;t<sub>12</sub>) second object <b>94</b> may be disposed at a second position (x<sub>2</sub>, y<sub>2</sub>, z<sub>2</sub>) relative to first object <b>92</b> (e.g., where x<sub>2</sub>=x<sub>1</sub>+&#x394;x<sub>12</sub>, y<sub>2</sub>=y<sub>1</sub>+&#x394;y<sub>12 </sub>and z<sub>2</sub>=z<sub>1</sub>+&#x394;z<sub>12</sub>) (e.g., as shown in illustration <b>100</b><i>b </i>in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>).</p><p id="p-0086" num="0085">Excitation unit <b>110</b> may excite at least portion <b>94</b><i>a </i>of second object <b>94</b> at a zero-time point t<sub>0 </sub>(e.g., wherein t<sub>0</sub>&#x3c;t<sub>1</sub>) such that at first time point t<sub>1 </sub>change(s) <b>112</b> or pattern(s) <b>114</b> of changes <b>112</b> (e.g., as shown in illustration <b>100</b><i>a </i>in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>) may be detectable by sensor(s) <b>122</b>. Sensor(s) <b>122</b> may detect change(s) <b>112</b> or pattern(s) <b>114</b> of changes <b>112</b> at first time point t<sub>1 </sub>(e.g., when second object <b>94</b> is at first position (x<sub>1</sub>, y<sub>1</sub>, z<sub>1</sub>)) and generate a first sensor output dataset <b>124</b><i>a </i>(e.g., as shown in illustration <b>100</b><i>a </i>in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>). Sensor(s) <b>122</b> may detect change(s) <b>112</b> or pattern(s) <b>114</b> of changes <b>112</b> at second time point t<sub>2 </sub>(e.g., when second object <b>94</b> is at second position (x<sub>2</sub>, y<sub>2</sub>, z<sub>2</sub>)) and generate a second sensor output dataset <b>124</b><i>b </i>(e.g., as shown in illustration <b>100</b><i>b </i>in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>).</p><p id="p-0087" num="0086">Processing unit <b>130</b> may determine relative motion <b>93</b> between first object <b>92</b> and second object <b>94</b>, based on first sensor output dataset <b>124</b><i>a </i>and second sensor output dataset <b>124</b><i>b </i>(e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>). In some embodiments, processing unit <b>130</b> may quantify relative motion <b>93</b> between first object <b>92</b> and second object <b>94</b>, based on first sensor output dataset <b>124</b><i>a </i>and second sensor output dataset <b>124</b><i>b </i>(e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>).</p><p id="p-0088" num="0087">In some embodiments, processing unit <b>130</b> may control sensor(s) <b>122</b> to follow second object <b>94</b> (e.g., according to the determined and/or quantified relative motion between first object <b>92</b> and second object <b>94</b>) such that a field-of-view (FOV) <b>122</b><i>a </i>of sensor(s) <b>122</b> follows excited portion <b>94</b><i>a </i>of second object <b>94</b> with change(s) <b>112</b> or pattern(s) <b>114</b> of changes <b>112</b> induced therein (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>).</p><p id="p-0089" num="0088">Reference is now made to <figref idref="DRAWINGS">FIGS. <b>1</b>D and <b>1</b>E</figref>, which are schematic illustrations of a system <b>100</b> for determining a relative motion between two or more object and capable of detecting an upcoming disappearance of at least one change <b>112</b> or at least one pattern <b>114</b> of changes <b>112</b> from a field-of-view <b>122</b><i>a </i>of a sensor <b>122</b> of a sensing unit <b>120</b> of system <b>100</b>, according to some embodiments of the invention.</p><p id="p-0090" num="0089">Illustrations <b>100</b><i>c</i>, <b>100</b><i>d </i>and illustrations <b>100</b><i>e</i>, <b>100</b><i>f </i>in <figref idref="DRAWINGS">FIGS. <b>1</b>D, <b>1</b>E</figref>, respectively, depict system <b>100</b> at second time point t<sub>2 </sub>(e.g., as described above with respect to <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>) and a third time point t<sub>3 </sub>(e.g., wherein t<sub>3</sub>&#x3e;t<sub>2</sub>), respectively. At third time point t<sub>3</sub>, second object <b>94</b> may be in second position (x<sub>2</sub>, y<sub>2</sub>, z<sub>2</sub>) (e.g., as described above with respect to <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>) or at a position that is different from the second position (x<sub>2</sub>, y<sub>2</sub>, z<sub>2</sub>).</p><p id="p-0091" num="0090">According to some embodiments, processing unit <b>130</b> may detect an upcoming disappearance of change(s) <b>112</b> or pattern(s) <b>114</b> of changes <b>112</b> from FOV <b>122</b><i>a </i>of sensor <b>122</b> of sensing unit <b>120</b>. The upcoming disappearance of change(s) <b>112</b> or pattern(s) <b>114</b> of changes <b>112</b> from FOV <b>122</b><i>a </i>may be detected based on at least one of sensor output datasets <b>124</b>.</p><p id="p-0092" num="0091">For example, the disappearance of change(s) <b>112</b> or pattern(s) <b>114</b> of changes <b>112</b> from FOV <b>122</b><i>a </i>of sensor <b>122</b> may be due to a temporarily nature of change(s) <b>112</b> (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>1</b>D</figref> and as discussed above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>). In another example, the disappearance of change(s) <b>112</b> or pattern(s) <b>114</b> of changes <b>112</b> from FOV <b>122</b><i>a </i>of sensor <b>122</b> may be due to the relative motion between first object <b>92</b> and second object <b>94</b> (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>1</b>E</figref>). In another example, the disappearance of change(s) <b>112</b> or pattern(s) <b>114</b> of changes <b>112</b> from FOV <b>122</b><i>a </i>of sensor <b>122</b> may be due to at least a partial eclipse of FOV <b>122</b><i>a </i>by a third object.</p><p id="p-0093" num="0092">Upon detection of the upcoming disappearance of change(s) <b>112</b> or pattern(s) <b>114</b> of changes <b>112</b> from FOV <b>122</b><i>a </i>of sensor <b>122</b>, processing unit <b>130</b> may control excitation unit <b>110</b> to excite <b>111</b> at least a portion of second object <b>94</b> before change(s) <b>112</b> or pattern(s) <b>114</b> of changes <b>112</b> disappear from FOV <b>122</b><i>a </i>of sensor(s) <b>122</b> (e.g., as shown in <figref idref="DRAWINGS">FIGS. <b>1</b>D and <b>1</b>E</figref>). In this manner, continuous tracking of the relative motion between first object <b>92</b> and second object <b>94</b> may be performed.</p><p id="p-0094" num="0093">In some embodiments, upon the detection thereof, processing unit <b>130</b> may control excitation unit <b>110</b> to excite <b>111</b> the same portion <b>94</b><i>a </i>of second object <b>94</b> that has been excited at a preceding excitation cycle (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>).</p><p id="p-0095" num="0094">In some embodiments, upon the detection thereof, processing unit <b>130</b> may control excitation unit <b>110</b> to excite <b>111</b> a second portion <b>94</b><i>a</i>&#x2032; on second object <b>94</b> (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>1</b>E</figref>). In some embodiments, previously excited portion <b>94</b><i>a </i>and excited second portion <b>94</b><i>a</i>&#x2032; may be both in FOV <b>122</b><i>a </i>of sensor <b>122</b> during at least one time point (e.g., third time point t<sub>3 </sub>as shown in <figref idref="DRAWINGS">FIG. <b>1</b>E</figref>).</p><p id="p-0096" num="0095">In various embodiments, processing unit <b>130</b> may control excitation unit <b>110</b> to induce different change(s) <b>112</b> or different patterns <b>114</b> of changes <b>112</b> during at least some of excitation cycles. For example, upon the detection of the upcoming disappearance of pattern <b>114</b> of changes <b>112</b> from FOV <b>122</b><i>a </i>of sensor <b>122</b>, processing unit <b>130</b> may control excitation unit <b>110</b> to induce a second pattern <b>114</b>&#x2032; of second changes <b>112</b>&#x2032; that is different from pattern <b>114</b> of changes <b>112</b> induced during a preceding excitation cycle (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>1</b>E</figref>). In this manner, interference between different changes <b>112</b>, <b>112</b>&#x2032; or different patterns <b>114</b>, <b>114</b>&#x2032; of changes <b>112</b>, <b>112</b>&#x2032; may be avoided.</p><p id="p-0097" num="0096">In various embodiments, change(s) <b>112</b> or pattern(s) <b>114</b> of change(s) <b>112</b> may encode a supplementary information. For example, different change(s) <b>112</b> and/or different patterns <b>114</b> of changes <b>112</b> may encode different supplementary information. The supplementary information may, for example, include a time stamp representing a time at which the excitation that caused this particular change(s) <b>112</b> or pattern(s) <b>114</b> of changes <b>112</b> has occurred. In another example, the supplementary information may include a system identification. In this manner, multiple systems (like system <b>100</b>) may operate simultaneously without interfering with each other.</p><p id="p-0098" num="0097">In various embodiments, processing unit <b>130</b> may control excitation unit <b>110</b> to apply excitation cycles based on the determined and/or quantified relative motion between first object <b>92</b> and second object <b>94</b>, such that one or more of changes <b>112</b> in second object <b>94</b> may be detectable by sensing unit <b>120</b> at all times. For example, a frequency of the excitation cycles may be based on the determined relative velocity between first object <b>92</b> and second object <b>94</b>.</p><p id="p-0099" num="0098">Reference is now made to <figref idref="DRAWINGS">FIG. <b>1</b>F</figref>, which depicts an example of determining a relative motion between two or more objects based on image frames obtained by sensor(s) <b>122</b> of a sensing unit <b>120</b> of a system <b>100</b> for determining a relative motion between two or more object, according to some embodiments of the invention.</p><p id="p-0100" num="0099">It is noted that first object <b>92</b>, excitation unit <b>110</b>, sensing unit <b>120</b> and processing unit <b>130</b> are not shown in <figref idref="DRAWINGS">FIG. <b>1</b>F</figref> for sake of clarity only.</p><p id="p-0101" num="0100">Illustrations <b>100</b><i>g </i>and <b>100</b><i>h </i>in <figref idref="DRAWINGS">FIG. <b>1</b>F</figref> depict second object <b>94</b> at a fourth time point t<sub>4 </sub>and a fifth time point t<sub>5 </sub>(e.g., wherein t<sub>5</sub>&#x3e;t<sub>4</sub>), respectively. At fourth time point t<sub>4</sub>, second object is at a fourth position (x<sub>4</sub>, y<sub>4</sub>, z<sub>4</sub>). During a time interval &#x394;t<sub>45</sub>, second object <b>94</b> may move by a distance &#x394;x<sub>45 </sub>along an arbitrary X axis such that at fifth time point t<sub>5 </sub>second object <b>94</b> is at a fifth position (x<sub>5</sub>=x<sub>4</sub>+&#x394;x<sub>45</sub>, y<sub>5</sub>, z<sub>5</sub>).</p><p id="p-0102" num="0101">Sensor(s) <b>122</b> may capture two subsequent image frames <b>124</b><i>d</i>, <b>124</b><i>e </i>(e.g., sensor datasets) of FOV <b>122</b><i>a </i>thereof when second object <b>94</b> at fourth time point t<sub>4 </sub>and fifth time point t<sub>5</sub>, respectively (corresponding to fourth position and fifth position, respectively). Pixels <b>124</b><i>f </i>in image frames <b>124</b><i>d</i>, <b>124</b><i>e </i>may represent changes <b>112</b> in portion <b>94</b><i>a </i>on second object <b>94</b>. A shift of pixels <b>124</b><i>f </i>in image frames <b>124</b><i>d</i>, <b>124</b><i>e </i>may correspond to actual motion of second object <b>94</b> with respect to sensor(s) <b>122</b> (e.g., disposed on first object <b>92</b>).</p><p id="p-0103" num="0102">Processing unit <b>130</b> may receive image frames <b>124</b><i>d</i>, <b>124</b><i>e </i>and detect the shift of pixels <b>124</b><i>f </i>in image frames <b>124</b><i>d</i>, <b>124</b><i>e</i>. Processing unit <b>130</b> may further determine based on, for example, the shift of pixels <b>124</b><i>f </i>in image frames <b>124</b><i>d</i>, <b>124</b><i>e </i>the relative motion of second object <b>94</b> with respect to sensor(s) <b>122</b> (and thus with respect to first object <b>92</b>). For example, processing unit <b>130</b> may determine the relative motion further based on, for example, know position of sensor(s) <b>122</b>, known position of FOV <b>122</b><i>a </i>thereof, relative position of sensor(s) <b>122</b> and excitation unit <b>110</b>, a resolution of sensor(s) <b>122</b>, etc. Processing unit <b>130</b> may further quantify the relative motion of the second object <b>94</b> with respect to sensor(s) <b>122</b> (and thus with respect to first object <b>92</b>) based on specified parameters of sensor(s) <b>122</b>.</p><p id="p-0104" num="0103">Reference is now made to <figref idref="DRAWINGS">FIGS. <b>2</b>A and <b>2</b>B</figref>, which are schematic illustrations of an excitation unit <b>200</b> for a system for determining a relative motion between two or more objects (such as system <b>100</b>), according to some embodiments of the invention.</p><p id="p-0105" num="0104">Excitation unit <b>200</b> may be, for example, excitation unit <b>110</b> in system <b>100</b> for determining a relative motion between two or more objects (e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A, <b>1</b>B, <b>1</b>C, <b>1</b>D and <b>1</b>E</figref>).</p><p id="p-0106" num="0105">According to some embodiments, excitation unit <b>200</b> may illuminate at one portion <b>82</b> on an object <b>80</b> with at least one excitation beam <b>210</b> to thereby excite and induce at least one change at least in at least one portion <b>82</b> of object <b>80</b>. For example, object <b>80</b>, at least one portion <b>82</b> and the at least one change may be similar to second object <b>94</b>, at least one portion <b>94</b><i>a </i>and at least one change <b>112</b>, respectively, as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A, <b>1</b>B, <b>1</b>C, <b>1</b>D and <b>1</b>E</figref>.</p><p id="p-0107" num="0106">In some embodiments, excitation unit <b>200</b> may illuminate portion <b>82</b> on object <b>80</b> with single excitation beam <b>210</b> to generate a single spot <b>220</b> on object <b>80</b> (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>). Single spot <b>220</b> may, for example, induce change <b>112</b> within portion <b>82</b> on object <b>80</b> (e.g., as described above with respect to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>).</p><p id="p-0108" num="0107">In some embodiments, excitation unit <b>200</b> may illuminate multiple locations <b>84</b> in portion <b>82</b> on object <b>80</b> with corresponding multiple excitation beams <b>230</b> to generate a pattern <b>240</b> of multiple spots <b>242</b> within portion <b>82</b> on object <b>80</b> (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>). In some embodiments, at least some of multiple spots <b>220</b> are at different depths below a surface of object <b>80</b>. Pattern <b>240</b> of multiple spots <b>242</b> may, for example, induce pattern <b>114</b> of changes <b>112</b> within portion <b>82</b> on object <b>80</b> (e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>B, <b>1</b>C, <b>1</b>D and <b>1</b>E</figref>).</p><p id="p-0109" num="0108">Reference is now made to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, which is a flowchart of a method <b>300</b> of determining a relative motion between two or more objects, according to some embodiments of the invention.</p><p id="p-0110" num="0109">Method <b>300</b> may be implemented by, for example, system <b>100</b>, which may be configured to implement method <b>300</b>. It is noted that method <b>300</b> is not limited to the flowcharts illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> and to the corresponding description. For example, in various embodiments, method <b>300</b> needs not move through each illustrated box or stage, or in exactly the same order as illustrated and described.</p><p id="p-0111" num="0110">According to some embodiments, method <b>300</b> may include exciting, by an excitation unit disposed on a first object, at least a portion of a second object to induce at least one change therein (stage <b>310</b>). For example, excitation unit <b>110</b>, sensing unit <b>120</b>, first object <b>92</b>, second object <b>94</b> and a portion <b>94</b><i>a </i>thereof, and at least one change <b>112</b> as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A, <b>1</b>B, <b>1</b>C, <b>1</b>D and <b>1</b>E</figref>.</p><p id="p-0112" num="0111">In some embodiments, method <b>300</b> may include inducing, by the excitation unit, at least one of: a physical change, a chemical change or a combination thereof, in the second object (e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>).</p><p id="p-0113" num="0112">In some embodiments, method <b>300</b> may include inducing, by the excitation unit, at least one temporal change in the second object such that the second body reverts to its initial state prior to the excitation after a period of time (e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>).</p><p id="p-0114" num="0113">In some embodiments, method <b>300</b> may include determining, by the processing unit, a period of time after which a magnitude of the at least one temporal change reduces below a predefined threshold based on at least one of: a type of the at least one temporal change, one or more surface parameters of the second object and environmental conditions.</p><p id="p-0115" num="0114">In some embodiments, method <b>300</b> may include setting, by the excitation unit, a magnitude of the excitation to adjust the period of time to a desired value.</p><p id="p-0116" num="0115">In some embodiments, method <b>300</b> may include inducing, by the excitation unit, at least one pattern of changes, wherein the at least one pattern of changes comprises multiple spots, each of the multiple spots comprises the at least one change (e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>B, <b>1</b>C, <b>1</b>D and <b>1</b>E</figref> and <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>).</p><p id="p-0117" num="0116">In some embodiments, method <b>300</b> may include encoding, by the excitation unit, a supplementary information into the at least one change (e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>D and <b>1</b>E</figref>).</p><p id="p-0118" num="0117">In various embodiments, method <b>300</b> may include inducing, by the excitation unit, during at least one of excitation cycles, the at least one change that is different from the at least one change induced during other excitation cycles (e.g., as described above with respect to <figref idref="DRAWINGS">FIG. <b>1</b>E</figref>).</p><p id="p-0119" num="0118">In some embodiments, method <b>300</b> may include exciting, by the excitation unit, two or more different portions on the second object to induce corresponding two or more changes within the two or more portions thereof (e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>).</p><p id="p-0120" num="0119">In various embodiments, method <b>300</b> may include simultaneously or subsequently exciting the two or more different portions thereof (e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>).</p><p id="p-0121" num="0120">In some embodiments, method <b>300</b> may include exciting, by each of two or more excitation units, a different portion of corresponding two or more portions of the second object (e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>).</p><p id="p-0122" num="0121">According to some embodiments, method <b>300</b> may include detecting, by at least one sensor of a sensing unit disposed on the first object, the at least one change in the second object at two or more different time instances (stage <b>320</b>). For example, sensing unit <b>120</b> and sensor(s) <b>122</b> described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A, <b>1</b>B, <b>1</b>C, <b>1</b>D and <b>1</b>E</figref>.</p><p id="p-0123" num="0122">According to some embodiments, method <b>300</b> may include generating, by the at least one sensor, corresponding two or more sensor output datasets (stage <b>330</b>). For example, sensor output datasets <b>124</b> described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A, <b>1</b>B, <b>1</b>C, <b>1</b>D and <b>1</b>E</figref>.</p><p id="p-0124" num="0123">According to some embodiments, method <b>300</b> may include determining, by a processing unit, a relative motion between the first object and the second object based on the two or more sensor output datasets (stage <b>340</b>) (e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>). For example, processing unit <b>130</b> described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A, <b>1</b>B, <b>1</b>C, <b>1</b>D and <b>1</b>E</figref>.</p><p id="p-0125" num="0124">According to some embodiments, method <b>300</b> may include quantifying, by the processing unit, the relative motion between the first object and the second object based on the two or more sensor output datasets (e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>).</p><p id="p-0126" num="0125">According to various embodiments, method <b>300</b> may include controlling, by the processing unit, the at least one sensor to follow the second object according to the determined and/or quantified relative motion between the first object and the second object such that a field-of-view (FOV) of the at least one sensor follows the excited portion with the induced at least one change.</p><p id="p-0127" num="0126">According to some embodiments, method <b>300</b> may include measuring, by at least one distance sensor disposed on at least one of the first body and the second body, a distance between the first object and the second object.</p><p id="p-0128" num="0127">In some embodiments, method <b>300</b> may include determining a real-world geographical location of the first object based on a real-world geographical location of the second object, the determined relative motion between the first object and the second object, the measured distance between the first object and the second object and a known real-world geographical location of the first object at some time instance during the tracking process (e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>).</p><p id="p-0129" num="0128">In some other embodiments, method <b>300</b> may include determining a real-world geographical location of the second object based on a real-world geographical location of the first object, the determined relative motion between the first object, the second object and the measured distance between the first object and the second object and a known real-world geographical location of the second object at some time instance during the tracking process (e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>).</p><p id="p-0130" num="0129">According to some embodiments, method <b>300</b> may include detecting, by the processing unit, based on at least one of the two or more sensor output datasets, an upcoming disappearance of the at least one change from a field-of-view (FOV) of the at least one sensor of the sensing unit (e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>D and <b>1</b>E</figref>).</p><p id="p-0131" num="0130">In some embodiments, upon the detection of the upcoming disappearance of the at least one change from the FOV of the at least one sensor, method <b>300</b> may include exciting, by the excitation unit, at least a portion of the second object, before the at least one change disappears from the FOV of the at least one sensor (e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>D and <b>1</b>E</figref>).</p><p id="p-0132" num="0131">In some embodiments, method <b>300</b> may include, upon the detection of the upcoming disappearance of the at least one change from the FOV of the at least one sensor, exciting, by the excitation unit, the same portion of second object that has been excited at a preceding excitation cycle (e.g., described above with respect to <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>).</p><p id="p-0133" num="0132">In some embodiments, upon the detection of the upcoming disappearance of the at least one change from the FOV of the at least one sensor, method <b>300</b> may include exciting, by the excitation unit, a second portion on the second object such that the previously excited portion on the second object and the second portion thereof are both in the FOV of the at least one sensor during at least one time point (e.g., as described above with respect to <figref idref="DRAWINGS">FIG. <b>1</b>E</figref>).</p><p id="p-0134" num="0133">According to various embodiments, method <b>300</b> may include controlling the excitation unit by the processing unit to apply excitation cycles according to the determined and/or quantified relative motion between the first object and the second object, such that one or more of changes in the second object may be detectable by at least one sensor (e.g., may be in the FOV of the at least one sensor, etc.) at all times (e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>D and <b>1</b>E</figref>).</p><p id="p-0135" num="0134">Reference is now made to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, which is a schematic block diagram of a system <b>400</b> for determining a relative motion of at least one object <b>60</b> with respect to a specified location <b>70</b>, according to some embodiments of the invention.</p><p id="p-0136" num="0135">According to some embodiments, system <b>400</b> may include an excitation unit <b>410</b>, a sensing unit <b>420</b> and a processing <b>430</b>. For example, excitation unit <b>410</b> may be excitation unit <b>110</b> (e.g., described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A, <b>1</b>B, <b>1</b>C, <b>1</b>D and <b>1</b>E</figref>) or excitation unit <b>200</b> (e.g., described above with respect to <figref idref="DRAWINGS">FIGS. <b>2</b>A and <b>2</b>B</figref>), sensing unit <b>420</b> may be sensing unit <b>120</b> (e.g., described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A, <b>1</b>B, <b>1</b>C, <b>1</b>D and <b>1</b>E</figref>) and processing unit <b>430</b> may be processing unit <b>130</b> (e.g., described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A, <b>1</b>B, <b>1</b>C, <b>1</b>D and <b>1</b>E</figref>).</p><p id="p-0137" num="0136">System <b>400</b> may be disposed at a specified location <b>70</b>, for example within a terrain, and may be configured to determine and/or quantify a relative motion of one or more objects <b>60</b> with respect to specified location <b>70</b>.</p><p id="p-0138" num="0137">According to some embodiments, excitation unit <b>410</b> may excite (e.g., as indicated by arrow <b>411</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref>) at least a portion <b>62</b> of object <b>60</b> to thereby induce at least one change <b>412</b> therein (e.g., described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A, <b>1</b>B, <b>1</b>C, <b>1</b>D and <b>1</b>E</figref>). For example, change(s) <b>412</b> may be any of the physical change(s) or the chemical change(s) as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A, <b>1</b>B, <b>1</b>C, <b>1</b>D and <b>1</b>E</figref>.</p><p id="p-0139" num="0138">According to some embodiments, sensing unit <b>420</b> may include at least one sensor <b>422</b> that may measure at least one change <b>412</b> in object <b>60</b> (e.g., as indicated by doubled arrow <b>423</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref> and as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>). For example, sensor(s) <b>422</b> may be sensor(s) <b>122</b> described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A, <b>1</b>B, <b>1</b>C, <b>1</b>D and <b>1</b>E</figref>.</p><p id="p-0140" num="0139">According to some embodiments, sensor(s) <b>422</b> of sensing unit <b>420</b> may measure change(s) <b>412</b> in object <b>60</b> at two or more different time instances and generate corresponding two or more sensor output datasets <b>424</b> representing change(s) <b>412</b> thereof (e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A, <b>1</b>B, <b>1</b>C, <b>1</b>D and <b>1</b>E</figref>).</p><p id="p-0141" num="0140">According to various embodiments, processing unit <b>430</b> may receive two or more sensor output dataset(s) <b>424</b> from sensing unit <b>420</b> and determine and/or quantify the relative motion of object <b>60</b> with respect to specified location <b>70</b> based on two or more sensor output datasets <b>424</b> (e.g., as described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A, <b>1</b>B, <b>1</b>C, <b>1</b>D and <b>1</b>E</figref>).</p><p id="p-0142" num="0141">Reference is now made to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, which is a flowchart of a method <b>500</b> of determining a relative motion of at least one object with respect to a specified location, according to some embodiments of the invention.</p><p id="p-0143" num="0142">Method <b>500</b> may be implemented by, for example, system <b>400</b>, which may be configured to implement method <b>500</b>. It is noted that method <b>500</b> is not limited to the flowcharts illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref> and to the corresponding description. For example, in various embodiments, method <b>500</b> needs not move through each illustrated box or stage, or in exactly the same order as illustrated and described.</p><p id="p-0144" num="0143">According to some embodiments, method <b>500</b> may include exciting, by an excitation unit disposed at a specified location, at least a portion of an object to induce at least one change therein (stage <b>510</b>) (e.g., as described above with respect to <figref idref="DRAWINGS">FIG. <b>4</b></figref>).</p><p id="p-0145" num="0144">According to some embodiments, method <b>500</b> may include detecting, by at least one sensor of a sensing unit disposed at the specified location, the at least one change in the object at two or more different time instances (stage <b>520</b>) (e.g., as described above with respect to <figref idref="DRAWINGS">FIG. <b>4</b></figref>).</p><p id="p-0146" num="0145">According to some embodiments, method <b>500</b> may include generating, by the at least one sensor, corresponding two or more sensor output datasets representing the at least one change thereof (stage <b>530</b>) (e.g., as described above with respect to <figref idref="DRAWINGS">FIG. <b>4</b></figref>).</p><p id="p-0147" num="0146">According to some embodiments, method <b>500</b> may include determining, by a processing unit, a relative motion of the object with respect to the specified location based on the two or more sensor output datasets (stage <b>540</b>) (e.g., as described above with respect to <figref idref="DRAWINGS">FIG. <b>4</b></figref>).</p><p id="p-0148" num="0147">According to some embodiments, method <b>500</b> may include quantifying, by the processing unit, the relative motion of the object with respect to the specified location based on the two or more sensor output datasets (e.g., as described above with respect to <figref idref="DRAWINGS">FIG. <b>4</b></figref>).</p><p id="p-0149" num="0148">Advantageously, the disclosed systems and methods may enable determination/measurement of the relative motion between two or more objects and/or between one or more objects and the specified location in the case when the objects lack any (or sufficient number of) visual markers and/or have uniform structure, without. Furthermore, the determination/measurement of the relative motion thereof may be performed without inducing long-term changes (such as, for example, permanent visual markers or continuous electromagnetic radiation).</p><p id="p-0150" num="0149">Advantageously, the disclosed systems and methods may enable determination and/or quantification of the relative motion between two or more objects and/or between one or more objects and the specified location in the case when the objects lack any (or sufficient number of) visual markers and/or have uniform structure. Furthermore, the determination and/or quantification of the relative motion thereof may be performed, in some embodiments, without inducing long-term changes (such as, for example, permanent visual markers or continuous electromagnetic radiation) in the objects.</p><p id="p-0151" num="0150">Aspects of the present invention are described above with reference to flowchart illustrations and/or portion diagrams of methods, apparatus (systems) and computer program products according to embodiments of the invention. It will be understood that each portion of the flowchart illustrations and/or portion diagrams, and combinations of portions in the flowchart illustrations and/or portion diagrams, can be implemented by computer program instructions. These computer program instructions can be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or portion diagram or portions thereof.</p><p id="p-0152" num="0151">These computer program instructions can also be stored in a computer readable medium that can direct a computer, other programmable data processing apparatus, or other devices to function in a particular manner, such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function/act specified in the flowchart and/or portion diagram portion or portions thereof. The computer program instructions can also be loaded onto a computer, other programmable data processing apparatus, or other devices to cause a series of operational steps to be performed on the computer, other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions/acts specified in the flowchart and/or portion diagram portion or portions thereof.</p><p id="p-0153" num="0152">The aforementioned flowchart and diagrams illustrate the architecture, functionality, and operation of possible implementations of systems, methods and computer program products according to various embodiments of the present invention. In this regard, each portion in the flowchart or portion diagrams can represent a module, segment, or portion of code, which includes one or more executable instructions for implementing the specified logical function(s). It should also be noted that, in some alternative implementations, the functions noted in the portion can occur out of the order noted in the figures. For example, two portions shown in succession can, in fact, be executed substantially concurrently, or the portions can sometimes be executed in the reverse order, depending upon the functionality involved. It will also be noted that each portion of the portion diagrams and/or flowchart illustration, and combinations of portions in the portion diagrams and/or flowchart illustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts, or combinations of special purpose hardware and computer instructions.</p><p id="p-0154" num="0153">In the above description, an embodiment is an example or implementation of the invention. The various appearances of &#x201c;one embodiment&#x201d;, &#x201c;an embodiment&#x201d;, &#x201c;certain embodiments&#x201d; or &#x201c;some embodiments&#x201d; do not necessarily all refer to the same embodiments. Although various features of the invention can be described in the context of a single embodiment, the features can also be provided separately or in any suitable combination. Conversely, although the invention can be described herein in the context of separate embodiments for clarity, the invention can also be implemented in a single embodiment. Certain embodiments of the invention can include features from different embodiments disclosed above, and certain embodiments can incorporate elements from other embodiments disclosed above. The disclosure of elements of the invention in the context of a specific embodiment is not to be taken as limiting their use in the specific embodiment alone. Furthermore, it is to be understood that the invention can be carried out or practiced in various ways and that the invention can be implemented in certain embodiments other than the ones outlined in the description above.</p><p id="p-0155" num="0154">The invention is not limited to those diagrams or to the corresponding descriptions. For example, flow need not move through each illustrated box or state, or in exactly the same order as illustrated and described. Meanings of technical and scientific terms used herein are to be commonly understood as by one of ordinary skill in the art to which the invention belongs, unless otherwise defined. While the invention has been described with respect to a limited number of embodiments, these should not be construed as limitations on the scope of the invention, but rather as exemplifications of some of the preferred embodiments. Other possible variations, modifications, and applications are also within the scope of the invention. Accordingly, the scope of the invention should not be limited by what has thus far been described, but by the appended claims and their legal equivalents.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system for determining a relative motion between two or more objects, the system comprising:<claim-text>an excitation unit adapted to be disposed on a first object, the excitation unit is configured to excite and to induce at least one change in at least a portion of a second object;</claim-text><claim-text>a sensing unit adapted to be disposed on the first object, the sensing unit comprises at least one sensor configured to detect the at least one change in the second object at two or more different time instances and to generate corresponding two or more sensor output datasets; and</claim-text><claim-text>a processing unit in communication with the sensing unit, the processing unit is configured to determine a relative motion between the first object and the second object based on the two or more sensor output datasets.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processing unit is configured to quantify the relative motion between the first object and the second object based on the two or more sensor output datasets thereof.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one change is at least one of: a physical change, a chemical change or a combination thereof.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one change is temporal and wherein the second object reverts to its initial state prior to excitation after a period of time.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the processing unit is configured to determine a period of time after which a magnitude of the at least one temporal change reduces below a predefined threshold based on at least one of: a type of the at least one temporal change, one or more surface parameters of the second object and environmental conditions.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the excitation unit is configured to set a magnitude of the excitation to adjust the period of time to a desired value.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the excitation unit is configured to induce at least one pattern of changes, wherein the at least one pattern of changes comprises multiple spots, each of the multiple spots comprises the at least one change.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processing unit is configured to detect, based on at least one of the two or more sensor output datasets, an upcoming disappearance of the at least one change from a field-of-view (FOV) of the at least one sensor.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the disappearance of the at least one change from the FOV of the at least one sensor is due to at least one of: temporarily nature of the at least one change, relative motion between the first object and the second object and at least partial eclipse of the FOV by a third object.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein upon the detection of the upcoming disappearance of the at least one change from the FOV of the at least one sensor, the processing unit is configured to control the excitation unit to excite at least a portion of the second object before the at least one change disappears from the FOV of the at least one sensor.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the processing unit is configured to control the excitation unit to excite at least one of:<claim-text>the same portion of the second object that has been excited at a preceding excitation cycle; and</claim-text><claim-text>a second portion of the second object such that previously excited portion of the second object and the second portion thereof are both in the FOV of the at least one sensor during at least one time point.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the excitation unit is configured to induce, during at least one of excitation cycles, the at least one change that is different from the at least one change induced during other excitation cycles.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the excitation unit is configured to encode a supplementary information in the at least one change.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising at least one distance sensor adapted to be disposed on at least one of the first object and the second object, the at least one distance sensor is configured to measure a distance between the first object and the second object.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the processing unit is configured to determine at least one of:<claim-text>a real-world geographical location of the first object based on a real-world geographical location of the second object, the determined relative motion between the first object and the second object, the measured distance between the first object and the second object and a known real-world geographical location of the first object at some time instance during the tracking process; and</claim-text><claim-text>a real-world geographical location of the second object based on a real-world geographical location of the first object, the determined relative motion between the first object and the second object, the measured distance between the first object and the second object and a known real-world geographical location of the second object at some time instance during the tracking process.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A method of determining a relative motion between two or more objects, the method comprising:<claim-text>exciting, by an excitation unit disposed on a first object, at least a portion of a second object to induce at least one change therein;</claim-text><claim-text>detecting, by at least one sensor of a sensing unit disposed on the first object, the at least one change in the second object at two or more different time instances;</claim-text><claim-text>generating, by the at least one sensor, corresponding two or more sensor output datasets; and</claim-text><claim-text>determining, by a processing unit, a relative motion between the first object and the second object based on the two or more sensor output datasets.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising quantifying, by the processing unit, the relative motion between the first object and the second object based on the two or more sensor output datasets thereof.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising inducing, by the excitation unit, at least one of: a physical change, a chemical change or a combination thereof, in the second object.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising inducing, by the excitation unit, at least one temporal change in the second object such that the second body reverts to its initial state prior to excitation after a period of time.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00019">claim 19</claim-ref>, further comprising determining, by the processing unit, a period of time after which a magnitude of the at least one temporal change reduces below a predefined threshold based on at least one of: a type of the at least one temporal change, one or more surface parameters of the second object and environmental conditions.</claim-text></claim></claims></us-patent-application>