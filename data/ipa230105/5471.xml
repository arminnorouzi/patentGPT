<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005472A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005472</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17366169</doc-number><date>20210702</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>183</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>183</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>2015</main-group><subgroup>223</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">AUTOMATED CONTEXT-SPECIFIC SPEECH-TO-TEXT TRANSCRIPTIONS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>VMware, Inc.</orgname><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Shetty</last-name><first-name>Rohit Pradeep</first-name><address><city>Bangalore</city><country>IN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Chawla</last-name><first-name>Ravish</first-name><address><city>Chamblee</city><state>GA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Chow</last-name><first-name>Adam</first-name><address><city>Atlanta</city><state>GA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Disclosed are various approaches for generating a text transcript of a soundtrack. The soundtrack can correspond to an event in a conferencing service. Language models can be trained on data that is specific to organizations, users within the organization, and metadata associated with an agenda for the event. The metadata can include texts, attachments, and other data associated with the event. The language models can be arranged into a convolutional neural network and output a text transcript. The text transcript can be used to retrain the language models for subsequent use.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="183.56mm" wi="145.46mm" file="US20230005472A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="220.13mm" wi="176.78mm" file="US20230005472A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="168.91mm" wi="170.35mm" file="US20230005472A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="202.01mm" wi="147.49mm" file="US20230005472A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="202.01mm" wi="147.57mm" file="US20230005472A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">In an enterprise setting, individuals can collaborate using conferencing applications and services. In some conferencing services, a speech-to-text feature can be made available that attempts to generate a text transcript of discussions held during a conference for the benefit of the viewer. The text transcript can be generated for the benefit of hearing impaired individuals or for later review by attendees or other users. However, in some scenarios, a text transcript that it generated by a conferencing service or a viewer client associated with the conferencing service often lacks certain context that information to generate a fully accurate transcript.</p><p id="p-0003" num="0002">Additionally, some users may have varying levels of domain expertise and knowledge, which can result in varying needs for a text transcript. For example, some users may require an acronym to be fully expanded while other users may intuitively understand the acronym and may not desire the acronym to be fully expanded in a transcript for efficiency. Accordingly, existing systems and methods that generate text transcripts from a soundtrack can yield inconsistent results depending upon the user.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0002" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0004" num="0003">Many aspects of the present disclosure can be better understood with reference to the following drawings. The components in the drawings are not necessarily to scale, with emphasis instead being placed upon clearly illustrating the principles of the disclosure. Moreover, in the drawings, like reference numerals designate corresponding parts throughout the several views.</p><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a drawing of an example of a networked environment, including client devices, network services, an identity manager service, a workflow service, and other components in communication through a network.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a drawing illustrating a neural network utilizing language models according to examples of the disclosure.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart outlining functionalities implemented by the workflow service and other components of the networked environment.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is another flowchart outlining functionalities implemented by the workflow service and other components of the networked environment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0003" level="1">DETAILED DESCRIPTION</heading><p id="p-0009" num="0008">The present disclosure relates to generating text transcripts from a soundtrack that can correspond to video in a conferencing service. a text transcript can be generated using a convolutional neural network that includes an organizational specific language model, a user specific language model, and in some examples, an agenda specific language model. the organizational specific language model can be trained to generate a text transcript from a soundtrack and generate the transcript such that organization all specific words, phrases, and acronyms can be easily identified, which can result in a final text transcripts that are highly accurate.</p><p id="p-0010" num="0009">The user specific language model can be tailored for or customized by the user within an enterprise environment so that users can receive a personalized text transcript that takes into account user preferences, a user's role within an organization, and a user's level of expertise or knowledge on a given topic or domain. Additionally, an agenda specific language model can be generated from or trained by metadata associated with an event in a calendar or within the conferencing service. The metadata can be extracted from one or more files attached to an event in a user's calendar, or documents that are shared using the conferencing service. By utilizing a multi layered approach , examples of the disclosure can generate a text transcript from an audio soundtrack that can include potentially highly technical or domain specific words and phrases, that a more generalized language model would be unable to transcribe.</p><p id="p-0011" num="0010">With reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, shown is an example of a networked environment <b>100</b>. The networked environment <b>100</b> can include a computing environment <b>103</b> executing a transcription service <b>119</b>, workflow service <b>120</b>, network services <b>106</b>, conferencing service <b>107</b>, client devices <b>109</b>, an enterprise-hosted data store <b>113</b>, and external connectors <b>189</b> in communication through a network <b>112</b>.</p><p id="p-0012" num="0011">The network <b>112</b> can include the Internet, intranets, extranets, wide area networks (WANs), local area networks (LANs), wired networks, wireless networks, other suitable networks, or any combination of two or more such networks. The networks can include satellite networks, cable networks, Ethernet networks, telephony networks, and other types of networks. The network <b>112</b> includes wide area networks (WANs) and local area networks (LANs). These networks can include wired or wireless components or a combination thereof. Wired networks can include Ethernet networks, cable networks, fiber optic networks, and telephone networks, such as dial-up, digital subscriber line (DSL), and integrated services digital network (ISDN) networks. Wireless networks can include cellular networks, satellite networks, Institute of Electrical and Electronic Engineers (IEEE) 802.11 wireless networks (e.g., WI-FI&#xae;), BLUETOOTH&#xae; networks, microwave transmission networks, as well as other networks relying on radio broadcasts. The network <b>112</b> can also include a combination of two or more networks <b>112</b>. Examples of networks <b>112</b> can include the Internet, intranets, extranets, virtual private networks (VPNs), and similar networks.</p><p id="p-0013" num="0012">The computing environment <b>103</b> executing the workflow service <b>120</b> can include a server computer or any other system providing computing capability. While referred to in the singular, the computing environment <b>103</b> can include a plurality of computing devices that are arranged in one or more server banks, computer banks, or other arrangements. The computing devices of the computing environment <b>103</b> can be located in a single installation or can be distributed among many different geographical locations local and/or remote from the other components. The computing environment <b>103</b> can include a grid computing resource or any other distributed computing arrangement. The computing environment <b>103</b> can also include or be operated as one or more virtualized computer instances. For purposes of convenience, the computing environment <b>103</b> is referred to herein in the singular. The components executed on the computing environment <b>103</b> can include a workflow service <b>120</b> as well as other applications, services, processes, systems, engines, or functionality not discussed in detail herein.</p><p id="p-0014" num="0013">The transcription service <b>119</b> can generate a text transcript <b>181</b> from a soundtrack <b>176</b> that is provided as an input. The transcription service <b>119</b> can also train language models <b>131</b> that are utilized to generate the text transcript <b>181</b>. The transcription service <b>119</b> can utilize a series of language models <b>131</b>, each of which serves a different purpose. As described herein, a first language model <b>131</b> can be organization specific and trained based on terms, acronyms, phrases, etc., that are specific to an organization. A second language model <b>131</b> can be user-specific, or at least specific to a group of users as defined by the organization. A third language model <b>131</b> can be one that is trained on an agenda that is shared for a particular meeting or event within a conferencing service <b>107</b>. A text transcript <b>181</b> that is output can be provided to a user in real time or after an event concludes.</p><p id="p-0015" num="0014">In some examples, the transcription service <b>119</b> can provide the language models <b>131</b> to the client device <b>109</b>, which can generate a text transcript <b>181</b> from a soundtrack that is extracted from content received from the conferencing service <b>107</b>. In this scenario, the language models <b>131</b> utilized by the client device <b>109</b> to generate the text transcript <b>181</b> can be trained by the transcription service <b>119</b>.</p><p id="p-0016" num="0015">The workflow service <b>120</b> can manage enterprise workflows performed by users of the client devices <b>109</b>. The workflow service <b>120</b> can also manage authentication with network services <b>106</b> that are utilized as backend services for the workflows. The workflow service <b>120</b> can monitor one or more network services <b>106</b> that a user is authorized to access. For example, the workflow service <b>120</b> can periodically query a network service <b>106</b> for notifications, updates, messages, changes, actions to be performed, or other events. If a notification, update, message, change, action to be performed, or other event is identified, the workflow service <b>120</b> can then notify the user. For example, the workflow service <b>120</b> can send a message to the client device <b>109</b> to notify the user. The operating system <b>155</b> of the client device <b>109</b> can include a notification framework that provides a notification for the message using the client device <b>109</b>. The notification can include a sound generated using an audio device <b>165</b>, a visual notification using a display device <b>167</b>, and haptic feedback using a haptic device <b>169</b>.</p><p id="p-0017" num="0016">The workflow service <b>120</b> can also obtain data from network service <b>106</b> and provide the data to the transcription service <b>119</b> to train a language model <b>131</b>, such as an agenda-specific language model <b>142</b>, an organization-specific language model <b>134</b> or a user-specific language model <b>137</b>. For example, if a user is invited to a meeting hosted using the conferencing service <b>107</b>, the workflow service <b>120</b> can provide metadata from a calendar event, such as text or attachments provided to the invitees, to the transcription service <b>119</b>.</p><p id="p-0018" num="0017">The workflow service <b>120</b> can include authentication functionality, which can include retrieving, caching or storing service authentication data provided by various network services <b>106</b>. The authentication data can be used to query the network services <b>106</b> for information. In some embodiments, use of the workflow service <b>120</b> can be invisible to the user.</p><p id="p-0019" num="0018">The data store <b>121</b> can include any storage device or medium that can contain, store, or maintain the instructions, logic, or applications described herein for use by or in connection with the computing environment <b>103</b>. The data store <b>121</b> can be a hard drive or disk of a host, server computer, or any other system providing storage capability. While referred to in the singular, the data store <b>121</b> can include a plurality of storage devices that are arranged in one or more hosts, server banks, computer banks, or other arrangements. The data store <b>121</b> can include any one of many physical media, such as magnetic, optical, or semiconductor media. More specific examples include solid-state drives or flash memory.</p><p id="p-0020" num="0019">The data store <b>121</b> can include memory of the computing environment <b>103</b>, mass storage resources of the computing environment <b>103</b>, or any other storage resources on which data can be stored by the computing environment <b>103</b>. The data stored in the data store <b>121</b> can include, for example, a number of user accounts <b>129</b>, language models <b>131</b>, and other data that is not depicted and not necessary for a full understanding of examples of the disclosure.</p><p id="p-0021" num="0020">The data store <b>121</b> can also store a language model <b>131</b>. A language model <b>131</b> represents a portion of a natural language processing model that can perform speech-to-text conversion to generate a text transcript of an audio input. A language model <b>131</b> is a software model that can distinguish between words and phrases in an audio input. The language model <b>131</b> can also output a text transcription of the detected words and phrases in the audio input. A language model <b>131</b> can be a machine-learning model that is trained on a corpus of user interactions within the enterprise. For example, a language model <b>131</b> can represent a node of a neural network that is trained on one or more user interactions, such as user specific data that is available through enterprise messaging services, emails, and other data sources.</p><p id="p-0022" num="0021">Examples of this disclosure can utilize multiple language models in a convolutional neural network to generate accurate text transcript. Accordingly, the data store <b>121</b> can include an organization-specific language model <b>134</b> and a user-specific language model <b>137</b>. An organization-specific language model <b>134</b> can comprise a language model <b>131</b> that is trained using terms, acronyms, phrases, names, and other content that is specific to an organization. For example, in a particular enterprise, there can be terms and phrases that are specialized to or that have special meaning within the enterprise. For example, there can be internal or external product names, proper names, acronyms, and other content that is specific to an organization. Accordingly, an organization-specific language model <b>134</b> can be trained to generate a transcription of these words and phrases within an audio input that are inserted into a text transcript. The organization-specific language model <b>134</b> can be trained using a corpus of data, such as product pages, documentation, presentations, and other enterprise content that is specific or internal to the organization.</p><p id="p-0023" num="0022">A user-specific language model <b>137</b> represents a language model that is trained on data that is specific to a particular user within the enterprise. A user-specific language model <b>137</b> can be trained on one or more user interactions, such as user specific data that is available through enterprise messaging services, emails, and other data sources. An agenda-specific language model <b>142</b> can be generated for a particular event on a user's calendar or within a conferencing service <b>107</b>. Accordingly, when a particular user is detected as speaking during an event for which a soundtrack is provided, the user-specific language model <b>137</b> can transcribe the words or phrases spoken by the user. The agenda-specific language model <b>142</b> can be trained using attachments, text, or other content that is shared by users attending an event or viewing an event in a conferencing service <b>107</b>. For example, a meeting organizer might include attachments or text in a meeting invitation that is sent to users. The agenda-specific language model <b>142</b> can be generated using the attachments or text in the meeting invitation as training material to generate the agenda-specific language model <b>142</b>.</p><p id="p-0024" num="0023">In some examples of the disclosure, an audio input from which a text transcript is generated can be a soundtrack obtained from a conferencing service <b>107</b>. A language model <b>131</b> can represent a natural language processing model that can comprise portions of a multi-layered neural network, such as a convolutional neural network. The neural network can receive audio as an input and generate a text transcript as an output.</p><p id="p-0025" num="0024">The user accounts <b>129</b> can be associated with users of an enterprise. The user accounts <b>129</b> can be associated with a directory service that can facilitate user management. The user accounts <b>129</b> be associated with authentication data, single sign-on tokens, service records, local connectors, and external connectors. Other information about the user can also be stored as part of the user account <b>129</b>, such as the user's name, email address, contact information, enterprise responsibilities, and client devices <b>109</b>. User accounts <b>129</b> can also be associated with a user-specific classifier <b>150</b>. A user-specific classifier <b>150</b> can comprise a user customization or correction to a user-specific language model <b>137</b>.</p><p id="p-0026" num="0025">The user account <b>129</b> can identify device data for associated client devices <b>109</b>. Device data can include one or more of a device identifier, a unique device identifier (UDID), a media access control (MAC) address, an internet protocol (IP) address, or another identifier that uniquely identifies a device with respect to other devices. The device data can include device specifications, and a type of each client device <b>109</b>. Specifications for the client device <b>109</b> can include a hardware configuration that specifies a chipset identifier, a video card identifier, a memory identifier, a monitor or screen identifier, an input device identifier, and a peripheral identifier for each peripheral utilized by the client device <b>109</b>. Specifications for the client device <b>109</b> can include a software configuration that specifies an application identifier for each application installed on the client device <b>109</b>, a driver or driver version for hardware device and peripheral of the client device <b>109</b>, an operating system or operating system version installed on the client device <b>109</b>, and other information. The user account <b>129</b> can also include an organizational group of the user account <b>129</b> or the client device <b>109</b>.</p><p id="p-0027" num="0026">An organization-specific classifier <b>151</b> can include a rule that modifies the output of the organization-specific language model <b>134</b>. There can be multiple organization-specific classifiers <b>151</b> associated with an organization-specific language model <b>134</b>. For example, an organization-specific classifier <b>151</b> can specify how a particular acronym can be expanded or definitions for words or phrases that might not be apparent or known to some users. A given word or phrase can be associated with more than one organization-specific classifier <b>151</b> or user-specific classifier <b>150</b>. For example, a user in a first group within the organization might not have the same domain expertise as a user in a second group. Accordingly, these users can be associated with different organization-specific classifier <b>151</b> that specify that particular words or phrases should be defined or expanded differently depending upon the user who is currently speaking.</p><p id="p-0028" num="0027">A network service <b>106</b> can be a web application, web service, or other network facing application. The network service <b>106</b> can be federated or associated with a shared identity manager so each can be accessed using the identity manager. One or more network services <b>106</b> can be provided by the same provider or by different providers. The network service <b>106</b> can receive a service request <b>175</b>, and provide a service return <b>177</b> based on the service request <b>175</b>. The workflow service <b>120</b> can generate a service request <b>175</b>, or receive a service request <b>175</b> from a client device <b>109</b>. The workflow service <b>120</b> can use a local connector or an external connector <b>189</b> to provide the service request <b>175</b> to the network service <b>106</b>.</p><p id="p-0029" num="0028">Data obtained from a network service <b>106</b> can be utilized to train an organization-specific language model <b>134</b> or user-specific language model <b>137</b>. For example, data obtained from a third party email or message service can be used to train a user-specific language model <b>137</b>. Data obtained from documentation repository can be used to train an organization-specific language model <b>134</b> for a particular group within the enterprise.</p><p id="p-0030" num="0029">The conferencing service <b>107</b> can be a third party service or a service that is hosted within the computing environment <b>103</b>. The conferencing service <b>107</b> represents a service in which users can conduct video or audio conferences. Users can join conferences using a client application on a client device <b>109</b>, by dialing in from a phone, or using a browser-based client. The conferencing service <b>107</b> can provide a soundtrack <b>176</b> to the transcription service <b>119</b> or the workflow service <b>120</b>, which can in turn provide the soundtrack <b>176</b> to the transcription service <b>119</b>. The transcription service <b>119</b> can generate a text transcript <b>181</b> of the soundtrack <b>176</b>, which can display the text transcript <b>181</b> in real time in a viewer client associated with the conferencing service <b>107</b>. Additionally or alternatively, the text transcript <b>181</b> can be provided to the user after an event has concluded. In some cases, the text transcript <b>181</b> can be saved with a recording of the audio and/or video of the event by the conferencing service <b>107</b>.</p><p id="p-0031" num="0030">The network service <b>106</b> conferencing service <b>107</b> can be accessed by the workflow service <b>120</b> or the transcription service <b>119</b> using a connector, which can be built into the workflow service <b>120</b> or transcription service <b>119</b>. In some cases, external connectors <b>189</b> can be utilized. Connectors can refer to services or components that can be configure to fetch information that can be retrieved by the workflow service <b>120</b> and/or transcription service <b>119</b>. In some cases the connectors can be provided using a virtual machine or another virtual resource. Connectors can include local connectors hosted in the computing environment <b>103</b>, or external connectors <b>189</b> hosted external to the computing environment <b>103</b>. Connectors can handle communications with the network services <b>106</b>. For example, a connector can fetch user-specific and action-specific information from network services <b>106</b> or the conferencing service <b>107</b>.</p><p id="p-0032" num="0031">The client device <b>109</b> can be representative of one or more client devices <b>109</b>. The client device <b>109</b> can include a processor-based system, such as a computer system, that can include a desktop computer, a laptop computer, a personal digital assistant, a cellular telephone, a smartphone, a set-top step, a music player, a tablet computer system, a game console, an electronic book reader, a smartwatch, a voice activated smart device, or any other device with like capability. The client device <b>109</b> can have an operating system that can perform functionalities and execute applications. The operating system <b>155</b> can be stored in a data store <b>153</b> that also includes applications <b>157</b>, a management application <b>159</b>, and other data. The client device <b>109</b> can execute the client management application <b>159</b> to perform or access the functionality described for the workflow service <b>120</b>. The client device <b>109</b> can be equipped with networking capability or networking interfaces, including a localized networking or communication capability, such as a near-field communication (NFC) capability, radio-frequency identification (RFID) read or write capability, or other localized communication capability. In some embodiments, the client device <b>109</b> is mobile where the client device <b>109</b> is easily portable from one location to another, such as a smart phone, tablet, or laptop computer. In other situations, the client device <b>109</b> can be a desktop machine, a voice activated smart device, or a kiosk that is in a particular location or is not easily portable. The client device <b>109</b> can include user interface devices <b>163</b> that can be utilized to interact with users. The user interface devices <b>163</b> can include audio devices <b>165</b>, display devices <b>167</b>, and haptic devices <b>169</b>.</p><p id="p-0033" num="0032">The operating system <b>155</b> of the client device <b>109</b> can execute various client functionalities or client applications <b>157</b>, such as a management application <b>159</b>, a browser application, a voice interaction functionality, or another application. The operating system <b>155</b> and some applications <b>157</b> can access network content served up by the computing environment <b>103</b>, or other servers and can present this information to a user through one or more of the user interface devices <b>163</b>. For example, the client device <b>109</b> can render a user interface on a display, such as a liquid crystal display (LCD), organic light emitting diode (OLED) display, touch-screen display, or other type of display device. The client device <b>109</b> can also present audio information using the audio device <b>165</b> and can provide haptic or physical feedback using the haptic device <b>169</b>.</p><p id="p-0034" num="0033">Some applications <b>157</b> can include a browser or a dedicated application, and a user interface can include a network page, an application screen, or other interface. The client device <b>109</b> can also access web applications using the browser application. Further, other applications <b>157</b> can include device management applications, enterprise applications, social networking applications, word processors, spreadsheet applications, media player applications, or other applications. In some cases, an application <b>157</b> can comprise a client for the conferencing service <b>107</b> through which users can host or attend events that are hosted using the conferencing service <b>107</b>.</p><p id="p-0035" num="0034">In this scenario, a meeting organizer can create an event on a calendar and invite users to the event. The event can be associated with metadata, such as text or attachments that are distributed to the attendees. When the meeting or event commences in the conferencing service <b>107</b>, a soundtrack <b>176</b> can be streamed or provided to the transcription service <b>119</b>, which can generate a text transcript <b>181</b> using one or more language model <b>131</b> and/or classifiers that can facilitate the creation of accurate text transcript <b>181</b>. In one implementation, the transcription service <b>119</b> can implement a convolutional neural network (CNN) that can utilize various language models <b>131</b> to generate the text transcript <b>181</b>. Other neural network or machine learning models can also be utilized that take advantage of the language model <b>131</b> and other classifiers for further customization.</p><p id="p-0036" num="0035">A viewer application <b>158</b> represents an application that is utilized to access events or content from the conferencing service <b>107</b>. The viewer application <b>158</b> can be a client application through which the user can join events, view video from other users who have joined the event, listen to audio shared during the event, and consume or share other content, such as documents, screen-sharing and other features that are utilized in a conferencing service <b>107</b>.</p><p id="p-0037" num="0036">In some examples, the viewer application <b>158</b> can receive video and/or audio from the conferencing service <b>107</b> and render the content on the client device <b>109</b>. Additionally, the viewer application <b>158</b> can generate a text transcript <b>181</b> using a CNN that employs language models <b>131</b> provided by the transcription service <b>119</b> for the user. The text transcript <b>181</b> can be displayed by the viewer application <b>158</b>. The text transcript <b>181</b> can also be sent back to the transcription service <b>119</b> to further train the language models <b>131</b> that are utilized in the CNN. The viewer application <b>158</b> can periodically obtain updated language models <b>131</b> from the transcription service <b>119</b> or for each event viewed using the viewer application <b>158</b>. The updated language models <b>131</b> can be trained and retrained using various text transcripts <b>181</b> that are provided to the transcription service <b>119</b>.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows an example CNN <b>300</b> that can be assembled using language models <b>131</b> that are trained by the transcription service <b>119</b>. The CNN <b>300</b> can utilize a series of language models <b>131</b> that transcribe a soundtrack <b>176</b> provided as an input to the CNN <b>300</b>. A first layer of the CNN <b>300</b> can be implemented by the organization-specific language model <b>134</b> associated with a user. In some examples, the organization-specific language model <b>134</b> can be selected based upon a group within the organization the user is in. For example, the transcription service <b>119</b> or viewer application <b>158</b> can determine a group within a directory service of the enterprise in which the group is assigned, which can in turn be associated with a particular organization-specific language model <b>134</b>. Additionally, organization-specific classifiers <b>151</b> can also be selected by the transcription service <b>119</b> or the viewer application <b>158</b> depending upon the group within an organization in which the user is a member.</p><p id="p-0039" num="0038">For example, different people belonging to different groups within an organization can attend the same event. The meeting content involves slides and language specific to each person and includes organization-specific terminology (e.g., acronyms, product names, etc.) with which some attendees in certain groups within an organization may not be familiar. The CNN <b>300</b> utilizes the depicted multi-tier pipeline and provides an accurate transcription of the vocabulary used. The organization-specific classifier <b>151</b> for a particular user in a given group within the organization can expand upon or define certain terminology that a user in a different group may not require. Additional contextual information provided can be used by other members of the call in real-time so they can follow along without having to research anything that is mentioned during the meeting.</p><p id="p-0040" num="0039">In some examples, a language model <b>131</b> can be selected based upon the identity of the user speaking. The viewer application <b>158</b> or the conferencing service <b>107</b> can determine based upon the identity of the event attendees who is speaking a any given time. Accordingly, based on the identity of the user speaking, the corresponding models are loaded and used for inferencing. When a first user is speaking, the organization-specific language model <b>134</b> corresponding to that user can be loaded into the CNN <b>300</b>. When a second user is speaking, the organization-specific language model <b>134</b> for that other user can be loaded into the CNN <b>300</b> and utilized for inferencing.</p><p id="p-0041" num="0040">Similarly, a user-specific language model <b>137</b> that corresponds to the currently speaking user can be loaded into the CNN <b>300</b>. When a first user is speaking, the user-specific language model <b>137</b> corresponding to that user can be loaded into the CNN <b>300</b>. When a second user is speaking, the user-specific language model <b>137</b> for that other user can be loaded into the CNN <b>300</b> and utilized for inferencing. Additionally, user-specific classifiers <b>150</b> can also be selected by the transcription service <b>119</b> or the viewer application <b>158</b> depending upon the user who is currently speaking.</p><p id="p-0042" num="0041">These models can be deployed in series such that the transcript hypotheses are ranked accordingly at each level allowing the text transcript <b>181</b> to be the most accurate transcript. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the organization-specific language model <b>134</b> and user-specific language model <b>137</b> can include a main neural network head that can the feature from speech. The organization-specific language model <b>134</b> and user-specific language model <b>137</b> can have respective organization-specific classifiers <b>151</b> and user-specific classifiers <b>150</b> that can refine the generated transcript according to preferences or selections of the viewing user. In this way, each user does not require his or her own fully trained neural network, and we can feed the output from the model heads to the final layer classifier, which can generate the transcript for the specific organization-specific language model <b>134</b> or user-specific language model <b>137</b> before the transcript is passed to the next level in the CNN <b>300</b>. The organization-specific language model <b>134</b> can be assigned as the head of the CNN <b>300</b>.</p><p id="p-0043" num="0042">The user-specific language model <b>137</b> can feed the output of its processing to the agenda-specific language model <b>142</b> that can be generated based upon the metadata associated with the event. As noted above, the metadata can comprise text associated with an invitation to an event, text, documents or images associated with the event in the conferencing service <b>107</b>, or attachments to the invitation to the event. The metadata can be provided to the transcription service <b>119</b> when the event is created within a user's calendar or within the conferencing service <b>107</b>. The transcription service <b>119</b> can then generate or train an agenda-specific language model <b>142</b>, which can be provided to the viewer application <b>158</b>. Using the agenda-specific language model <b>142</b>, the viewer application <b>158</b> can generate a text transcript <b>181</b>, which can be displayed to the user through the viewer application <b>158</b> in real time or after the event.</p><p id="p-0044" num="0043">Additionally, the text transcript <b>181</b> can be provided back to the transcription service <b>119</b> which can retrain and refine an organization-specific language model <b>134</b> corresponding to an organization as well as retrain user-specific language models <b>137</b> that correspond to attendees of the meeting that were speaking. In some examples, the transcription service <b>119</b> can generate a text transcript <b>181</b> using a subtitling application programming interface (API) that can generate subtitles based upon a text input provided by the CNN <b>300</b>.</p><p id="p-0045" num="0044">The subtitling API can also allow a user or the transcription service <b>119</b> to select a verbosity mode. For example, a given word can be associated with a first definition in a verbose mode and a shorter definition in a less verbose mode. The user or the transcription service <b>119</b> can select a verbose or non-verbose mode when requesting a text transcript of a soundtrack provided to the transcription service <b>119</b>.</p><p id="p-0046" num="0045">In some examples, a verbosity mode can be selected by the transcription service <b>119</b> based upon a respective user's group, team, or role within an enterprise. The verbosity mode can also be selected based upon previous acronyms that have been provided to the respective user in the current or a previous transcript.</p><p id="p-0047" num="0046">The transcription service <b>119</b> can also incorporate data from other sources to augment a text transcript <b>181</b> generated by the transcription service <b>119</b>. For example, transcription service <b>119</b> can generate actionable user interface elements that can be overlaid into a user interface through which a user is viewing content from the conferencing service <b>107</b>. For example, if during a call, the host talks about revenue generated during a previous quarter and mentions a specific customer, the transcription service <b>119</b> can request additional content from a network service <b>106</b> on behalf of the user and make the subtitles interactive by allowing the guests to interact with content from the network service <b>106</b>. Similarly, if a data item in a network service <b>106</b>, such as a bug tracking service, is mentioned by name, the transcription service <b>119</b> can execute a connector to retrieve and show the data item to show the ticket to the user.</p><p id="p-0048" num="0047">As another example, the transcription service <b>119</b> can perform optical character recognition (OCR) on content shown in the conferencing service <b>107</b> and make the content interactive. For example, embedded content on PowerPoints can be made clickable where possible. If an embedded video is streamed by the host, a popup can be displayed on top to allow each guest to open up the video on their side to view it instead of viewing the video through the conferencing service <b>107</b>.</p><p id="p-0049" num="0048">Referring next to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, shown is a flowchart <b>302</b> describing steps that can be performed by the components of the networked environment <b>100</b>. Generally, the flowchart <b>302</b> describes how the viewer application <b>158</b> or the transcription service <b>119</b> can generate a text transcript <b>181</b> using a CNN <b>300</b> or language models <b>131</b> according to examples of the disclosure.</p><p id="p-0050" num="0049">At step <b>303</b>, the viewer application <b>158</b> can identify an event in a calendar of a user or group. The even can also be identified in a conferencing service <b>107</b>. The event can be one to which various users within an organization are invited. In one example, the transcription service <b>119</b> can also obtain an indication of the event on a user's calendar and determine whether a user-specific language model <b>137</b> exists for the user. If no user-specific language model <b>137</b> exists for the attendees of the event, the transcription service <b>119</b> can generate the user-specific language model <b>137</b>.</p><p id="p-0051" num="0050">At step <b>305</b>, the viewer application <b>158</b> can obtain a soundtrack <b>176</b> corresponding to the event. The soundtrack <b>176</b> can correspond to video content that is rendered by the viewer application <b>158</b> during the event. The video content can be associated with a videoconference hosted by the conferencing service <b>107</b>. The soundtrack <b>176</b> can be an input into a CNN <b>300</b> that is implemented by the viewer application <b>158</b> to generate a text transcript <b>181</b> associated with the soundtrack <b>176</b>.</p><p id="p-0052" num="0051">At step <b>307</b>, the viewer application <b>158</b> can identify a user associated with the viewer application <b>158</b>. The viewer application <b>158</b> can identify the currently logged in user.</p><p id="p-0053" num="0052">At step <b>309</b>, the viewer application <b>158</b> can identify an organization-specific language model <b>134</b> associated with the currently logged in user. The organization-specific language model <b>134</b> can specify how to generate a text transcript <b>181</b> from a soundtrack <b>176</b> provided as an input from the conferencing service <b>107</b>. The organization-specific language model <b>134</b> can be trained on a data set that includes organization-specific terminology as well as based on other speech recognition data so that an accurate text transcript <b>181</b> can be generated for the user. The organization-specific language model <b>134</b> can transcribe words or phrases that might be specific or unique to an organization. Because the organization-specific language model <b>134</b> can be trained on domain-specific content, the organization-specific language model <b>134</b> can generate a highly accurate text transcript <b>181</b> from a soundtrack <b>176</b> associated with an event.</p><p id="p-0054" num="0053">At step <b>311</b>, the viewer application <b>158</b> can identify one or more user-specific language models <b>137</b> associated with users attending the event in the conferencing service <b>107</b>. The attending users can be identified by querying the conferencing service <b>107</b> using a network call provided by the conferencing service <b>107</b>. Upon identifying the attending users of the event, the viewer application <b>158</b> can request the respective user-specific language model <b>137</b> for the attending users so that the viewer application <b>158</b> can transcribe words or phrases spoken by those users. As noted above, a user-specific language model <b>137</b> can be generated for each user in the organization. In some examples, a user-specific language model <b>137</b> might not be created for each user in the organization. In this scenario, various user-specific classifiers <b>150</b> can be utilized for each user to refine the output of a generalized user-specific language model <b>137</b> that is utilized.</p><p id="p-0055" num="0054">At step <b>313</b>, the viewer application <b>158</b> can obtain an agenda-specific language model <b>142</b> for the event in the conferencing service <b>107</b>. The agenda-specific language model <b>142</b> can be generated by the transcription service <b>119</b> based upon metadata associated with the event. As noted above, the metadata can include documents or other attachments included in a meeting invitation. The metadata can include documents or other attachments associated with the event that are stored in the conferencing service <b>107</b>. The agenda-specific language model <b>142</b> can be generated by the transcription service <b>119</b> and provided to the viewer application <b>158</b> when the event commences.</p><p id="p-0056" num="0055">At step <b>315</b>, the viewer application <b>158</b> can generate the text transcript <b>181</b>. The text transcript <b>181</b> can be displayed within the viewer application <b>158</b> during the event or provided to the user after the event. The viewer application <b>158</b> can utilize the language models <b>131</b> provided to the viewer application <b>158</b> to implement a neural network, such as the CNN <b>300</b>. In some examples, the CNN <b>300</b> can also obtain user-specific classifiers <b>150</b> and/or organization-specific classifiers <b>151</b> associated with the language models <b>131</b> provided to generate the text transcript <b>181</b>.</p><p id="p-0057" num="0056">To generate the text transcript <b>181</b>, the viewer application <b>158</b> can identify the currently speaking user and identify an organization-specific language model <b>134</b> and/or organization-specific classifiers <b>151</b> corresponding to the user. The viewer application <b>158</b> can also identify the user-specific language model <b>137</b> and/or user-specific classifier <b>150</b> corresponding to the currently speaking user. The viewer application <b>158</b> can also identify the agenda-specific language model <b>142</b> corresponding to the event. The viewer application <b>158</b> can utilize the identified language models <b>131</b> and classifiers, such as in a CNN <b>300</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> and generate a text transcript <b>181</b> corresponding to the soundtrack <b>176</b> obtained at step <b>305</b>.</p><p id="p-0058" num="0057">At step <b>317</b>, the viewer application <b>158</b> can upload the text transcript <b>181</b> to the transcription service <b>119</b>. In some implementations, the transcription service <b>119</b> can update or retrain the language models <b>131</b> that were utilized to generate the text transcript <b>181</b> from the soundtrack <b>176</b>. Additionally, the text transcript <b>181</b> can be archived in the data store <b>121</b> by the transcription service <b>119</b> for later retrieval by a user. Thereafter, the process proceeds to completion. In some implementations of the disclosure, the process of generating a text transcript <b>181</b> can be performed remotely from the viewer application <b>158</b>, such as by the transcription service <b>119</b> or the conferencing service <b>107</b>. In this scenario, the CNN <b>300</b> can be implemented in the transcription service <b>119</b> or the conferencing service <b>107</b> and output a text transcript <b>181</b> that can be displayed by the viewer application <b>158</b>.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an example flowchart <b>400</b> describing steps that can be performed by the transcription service <b>119</b>. Generally, the flowchart <b>400</b> describes how the transcription service <b>119</b> can provide language models <b>131</b> to a viewer application <b>158</b> for the purpose of generating a text transcript <b>181</b>.</p><p id="p-0060" num="0059">At step <b>403</b>, the transcription service <b>119</b> can train one or more language models <b>131</b> using various data sources. As noted above, an organization-specific language model <b>134</b> can be trained using organization-specific data that may include organization-specific terminology, acronyms, and the like. The organization-specific language model <b>134</b> corresponding to the organization can also be trained using previous text transcripts <b>181</b> generated using the organization-specific language model <b>134</b>. In this scenario, the text transcript <b>181</b> and other data sources can be provided as an input to a language model <b>131</b> training process.</p><p id="p-0061" num="0060">A user-specific language model <b>137</b> can be trained using data sources specific to a user, such as publications of the user, email messages, or other writings. The user-specific language model <b>137</b> corresponding to the user can also be trained using previous text transcripts <b>181</b> generated using the user-specific language model <b>137</b>. The data sources specific to the user can be provided as an input to a language model <b>131</b> training process.</p><p id="p-0062" num="0061">The transcription service <b>119</b> can also train an agenda-specific language model <b>142</b> corresponding to an event in the conferencing service <b>107</b>. The agenda-specific language model <b>142</b> can be trained using metadata associated with the event in the conferencing service <b>107</b> or in a calendar invitation referencing the event. The metadata can be provided as an input to a language model <b>131</b> training process.</p><p id="p-0063" num="0062">At step <b>405</b>, the transcription service <b>119</b> can receive a request for one or more language models <b>131</b> from a viewer application <b>158</b>. The viewer application <b>158</b> can be running on a client device <b>109</b>. The viewer application <b>158</b> can request language models <b>131</b> corresponding to users who are attending an event hosted using the conferencing service <b>107</b>.</p><p id="p-0064" num="0063">At step <b>407</b>, the transcription service <b>119</b> can identify the users associated with the event. The users associated with the event are users who have been invited to the event. The users can be identified by the viewer application <b>158</b>, and an identifier associated with the users provided to the transcription service <b>119</b> by the viewer application <b>158</b>.</p><p id="p-0065" num="0064">At step <b>409</b>, the transcription service <b>119</b> can identify an organization-specific language model <b>134</b> for one or more users invited to or attending the event. The organization-specific language models <b>134</b> can be identified in the data store <b>121</b>. The transcription service <b>119</b> can provide the organization-specific language models <b>134</b> to the viewer application <b>158</b>. In some examples, the transcription service <b>119</b> can also identify one or more organization-specific classifier <b>151</b> to the viewer application <b>158</b>.</p><p id="p-0066" num="0065">At step <b>411</b>, the transcription service <b>119</b> can identify a user-specific language model <b>137</b> for one or more users invited to or attending the event. The user-specific language models <b>137</b> can be identified in the data store <b>121</b>. The transcription service <b>119</b> can provide the user-specific language models <b>137</b> to the viewer application <b>158</b>. In some examples, the transcription service <b>119</b> can also identify one or more user-specific classifier <b>150</b> to the viewer application <b>158</b>.</p><p id="p-0067" num="0066">At step <b>413</b>, the transcription service <b>119</b> can identify the agenda-specific language model <b>142</b> generated for the event to the viewer application <b>158</b>. The viewer application <b>158</b> can then generate a text transcript <b>181</b> corresponding to an event using the provided language models <b>131</b> and classifiers.</p><p id="p-0068" num="0067">At step <b>415</b>, the viewer application <b>158</b> can obtain a text transcript <b>181</b> from the viewer application <b>158</b>. The text transcript <b>181</b> can be generated by a CNN <b>300</b> that utilizes the language models <b>131</b> provided to the viewer application <b>158</b> by the transcription service <b>119</b>.</p><p id="p-0069" num="0068">At step <b>417</b>, the transcription service <b>119</b> can retrain the language models <b>131</b> using the text transcript <b>181</b>. The language models <b>131</b> can be provided as an input for a language model <b>131</b> training process. Thereafter, the process proceeds to completion.</p><p id="p-0070" num="0069">A number of software components are stored in the memory and executable by a processor. In this respect, the term &#x201c;executable&#x201d; means a program file that is in a form that can ultimately be run by the processor. Examples of executable programs can be, for example, a compiled program that can be translated into machine code in a format that can be loaded into a random access portion of one or more of the memory devices and run by the processor, code that can be expressed in a format such as object code that is capable of being loaded into a random access portion of the one or more memory devices and executed by the processor, or code that can be interpreted by another executable program to generate instructions in a random access portion of the memory devices to be executed by the processor. An executable program can be stored in any portion or component of the memory devices including, for example, random access memory (RAM), read-only memory (ROM), hard drive, solid-state drive, USB flash drive, memory card, optical disc such as compact disc (CD) or digital versatile disc (DVD), floppy disk, magnetic tape, or other memory components.</p><p id="p-0071" num="0070">Memory can include both volatile and nonvolatile memory and data storage components. Also, a processor can represent multiple processors and/or multiple processor cores, and the one or more memory devices can represent multiple memories that operate in parallel processing circuits, respectively. Memory devices can also represent a combination of various types of storage devices, such as RAM, mass storage devices, flash memory, or hard disk storage. In such a case, a local interface can be an appropriate network that facilitates communication between any two of the multiple processors or between any processor and any of the memory devices. The local interface can include additional systems designed to coordinate this communication, including, for example, performing load balancing. The processor can be of electrical or of some other available construction.</p><p id="p-0072" num="0071">The client devices <b>109</b> can include a display upon which a user interface generated by an application <b>157</b>, workflow service <b>120</b>, or another application can be rendered. In some examples, the user interface can be generated with user interface data provided by the computing environment <b>103</b>. The client devices <b>109</b> can also include one or more input/output devices that can include, for example, a capacitive touchscreen or other type of touch input device, fingerprint reader, or keyboard.</p><p id="p-0073" num="0072">Although the workflow service <b>120</b>, client applications <b>157</b>, and other various services and functions described can be embodied in software or code executed by general purpose hardware as discussed above, as an alternative the same can also be embodied in dedicated hardware or a combination of software/general purpose hardware and dedicated hardware. If embodied in dedicated hardware, each can be implemented as a circuit or state machine that employs any one of or a combination of technologies. These technologies can include discrete logic circuits having logic gates for implementing various logic functions upon an application of one or more data signals, application specific integrated circuits (ASICs) having appropriate logic gates, field-programmable gate arrays (FPGAs), or other components.</p><p id="p-0074" num="0073">The flowcharts show an example of the functionality and operation of an implementation of portions of components described. If embodied in software, each block can represent a module, segment, or portion of code that can include program instructions to implement the specified logical function(s). The program instructions can be embodied in the form of source code that can include human-readable statements written in a programming language or machine code that can include numerical instructions recognizable by a suitable execution system such as a processor in a computer system or other system. The machine code can be converted from the source code. If embodied in hardware, each block can represent a circuit or a number of interconnected circuits to implement the specified logical function(s).</p><p id="p-0075" num="0074">Although the flowcharts show a specific order of execution, it is understood that the order of execution can differ from that which is depicted. For example, the order of execution of two or more blocks can be scrambled relative to the order shown. Also, two or more blocks shown in succession can be executed concurrently or with partial concurrence. Further, in some embodiments, one or more of the blocks shown in the drawings can be skipped or omitted.</p><p id="p-0076" num="0075">Also, any logic or application described that includes software or code can be embodied in any non-transitory computer-readable medium for use by or in connection with an instruction execution system such as a processor in a computer system or other system. In this sense, the logic can include, for example, statements including instructions and declarations that can be fetched from the computer-readable medium and executed by the instruction execution system. In the context of the present disclosure, a &#x201c;computer-readable medium&#x201d; can be any medium that can contain, store, or maintain the logic or application described for use by or in connection with the instruction execution system. The computer-readable medium can include any one of many physical media, such as magnetic, optical, or semiconductor media. Examples of a suitable computer-readable medium include solid-state drives or flash memory. Further, any logic or application described can be implemented and structured in a variety of ways. For example, one or more applications can be implemented as modules or components of a single application. Further, one or more applications described can be executed in shared or separate computing devices or a combination thereof. For example, a plurality of the applications described can execute in the same computing device, or in multiple computing devices.</p><p id="p-0077" num="0076">It is emphasized that the above-described embodiments of the present disclosure are merely possible examples of implementations described for a clear understanding of the principles of the disclosure. Many variations and modifications can be made to the above-described embodiments without departing substantially from the spirit and principles of the disclosure. All such modifications and variations are intended to be included within the scope of this disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>Therefore, the following is claimed:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system, comprising:<claim-text>a computing device comprising at least one processor and at least one memory; and</claim-text><claim-text>machine-readable instructions stored in the at least one memory, wherein the instructions, when executed by the at least one processor, cause the computing device to at least:<claim-text>identify an event in at least one of a user calendar or a conferencing service;</claim-text><claim-text>obtain a soundtrack corresponding to a video input, the video input obtained from the conferencing service, the video input further being associated with a viewer displaying the video input to a user;</claim-text><claim-text>identify an organization-specific language model associated with at least one user invited to the event;</claim-text><claim-text>identify a user-specific language model associated with the at least one user invited to the event;</claim-text><claim-text>identify an agenda-specific language model associated with the event, the agenda-specific language model generated using metadata associated with the event and wherein the organization-specific language model, the user-specific language model, and the agenda-specific language model comprise a convolutional neural network; and</claim-text><claim-text>generate a text transcript based upon the soundtrack from the video input using the convolutional neural network.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the organization-specific language model further comprises organizational-specific classifiers that specify how to generate the text transcript based upon organization wide rules.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the user-specific language model further comprises user-specific classifiers that specify how to generate the text transcript based upon user-specified rules.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the metadata associated with the event comprises an event description or at least one document associated with the event.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the organization-specific language model comprises a head of the convolutional neural network.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the organization-specific language model is trained to expand on a plurality of organization-specific acronyms that are inserted into the text transcript.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the convolutional neural network is configurable to generate an abbreviated text transcript or a verbose text transcript.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A non-transitory computer-readable medium comprising machine-readable instructions, wherein the instructions, when executed by at least one processor, cause a computing device to at least:<claim-text>identify an event in at least one of a user calendar or a conferencing service;</claim-text><claim-text>obtain a soundtrack corresponding to a video input, the video input obtained from the conferencing service, the video input further being associated with a viewer displaying the video input to a user;</claim-text><claim-text>identify an organization-specific language model associated with at least one user invited to the event;</claim-text><claim-text>identify a user-specific language model associated with the at least one user invited to the event;</claim-text><claim-text>identify an agenda-specific language model associated with the event, the agenda-specific language model generated using metadata associated with the event and wherein the organization-specific language model, the user-specific language model, and the agenda-specific language model comprise a convolutional neural network; and</claim-text><claim-text>generate a text transcript based upon the soundtrack from the video input using the convolutional neural network.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the organization-specific language model further comprises organizational-specific classifiers that specify how to generate the text transcript based upon organization wide rules.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the user-specific language model further comprises user-specific classifiers that specify how to generate the text transcript based upon user-specified rules.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the metadata associated with the event comprises an event description or at least one document associated with the event.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the organization-specific language model comprises a head of the convolutional neural network.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the organization-specific language model is trained to expand on a plurality of organization-specific acronyms that are inserted into the text transcript.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the convolutional neural network is configurable to generate an abbreviated text transcript or a verbose text transcript.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A method comprising:<claim-text>identifying an event in at least one of a user calendar or a conferencing service;</claim-text><claim-text>obtaining a soundtrack corresponding to a video input, the video input obtained from the conferencing service, the video input further being associated with a viewer displaying the video input to a user;</claim-text><claim-text>identifying an organization-specific language model associated with at least one user invited to the event;</claim-text><claim-text>identifying a user-specific language model associated with the at least one user invited to the event;</claim-text><claim-text>identifying an agenda-specific language model associated with the event, the agenda-specific language model generated using metadata associated with the event and wherein the organization-specific language model, the user-specific language model, and the agenda-specific language model comprise a convolutional neural network; and</claim-text><claim-text>generating a text transcript based upon the soundtrack from the video input using the convolutional neural network.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the organization-specific language model further comprises organizational-specific classifiers that specify how to generate the text transcript based upon organization wide rules.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the user-specific language model further comprises user-specific classifiers that specify how to generate the text transcript based upon user-specified rules.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the metadata associated with the event comprises an event description or at least one document associated with the event.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the organization-specific language model comprises a head of the convolutional neural network.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the organization-specific language model is trained to expand on a plurality of organization-specific acronyms that are inserted into the text transcript.</claim-text></claim></claims></us-patent-application>