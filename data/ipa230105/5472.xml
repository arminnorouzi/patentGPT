<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005473A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005473</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17943778</doc-number><date>20220913</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2020-052780</doc-number><date>20200324</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>18</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>63</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>1815</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>63</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>016</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>2015</main-group><subgroup>088</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">INFORMATION OUTPUT APPARATUS, INFORMATION OUTPUT METHOD, AND NON-TRANSITORY RECORDING MEDIUM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/JP2021/005022</doc-number><date>20210210</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17943778</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>JVCKENWOOD Corporation</orgname><address><city>Yokohama-shi</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>NAKAHARA</last-name><first-name>Yoshikazu</first-name><address><city>Yokohama-shi</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>TANAKA</last-name><first-name>Kodal</first-name><address><city>Yokohama-shi</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>MATSUMOTO</last-name><first-name>Yuto</first-name><address><city>Yokohama-shi</city><country>JP</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>WANG</last-name><first-name>Bo</first-name><address><city>Yokohama-shi</city><country>JP</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An information output apparatus is realized that is capable of notifying information based on an input voice content. An information output apparatus according to the present embodiment includes an input unit to which data indicating a sound are input from an outside, a voice extraction unit that analyzes data which are input from the input unit and indicate a sound and that extracts data which indicate a voice emitted by a person, a vibration generation unit that generates vibration data, which are associated with data indicating a sound in advance set, based on a result of a comparison between the data which indicate the voice and are extracted by the voice extraction unit and the data indicating the sound in advance set, and a vibrator that vibrates based on the vibration data generated by the vibration generation unit.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="161.54mm" wi="135.47mm" file="US20230005473A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="225.21mm" wi="155.96mm" orientation="landscape" file="US20230005473A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="230.46mm" wi="139.78mm" file="US20230005473A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="189.91mm" wi="137.50mm" file="US20230005473A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="187.54mm" wi="142.49mm" file="US20230005473A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="194.31mm" wi="151.72mm" file="US20230005473A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is based upon and claims the benefit of priority from Japanese patent application No. 2020-52780, filed on Mar. 24, 2020, the disclosure of which is incorporated herein in its entirety by reference.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">The present disclosure relates to an information output apparatus, an information output method, and a non-transitory recording medium.</p><p id="p-0004" num="0003">Games such as an FPS (first-person shooter) and a TPS (third-person shooter) may be played while a group is made. In this case, for example, the game is often advanced while environmental sounds in the game are heard and communication is made with another player by voice chat.</p><p id="p-0005" num="0004">Incidentally, a portable terminal in Japanese Unexamined Patent Application Publication No. 2009-300915 is configured such that when a voice which is in advance set is input, a vibrator is actuated and notifies a user of the input.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0006" num="0005">Because in a portable terminal in Japanese Unexamined Patent Application Publication No. 2009-300915 has a problem that because a vibrator is equivalently actuated when a voice which is in advance set is input, information based on an input voice content cannot be notified.</p><p id="p-0007" num="0006">The present disclosure has been made to solve the above problem, and an object of the present disclosure is to realize an information output apparatus, an information output method, and a non-transitory recording medium that are capable of notifying information based on an input voice content.</p><p id="p-0008" num="0007">An information output apparatus according to an embodiment of the present invention includes:</p><p id="p-0009" num="0008">an input unit to which data indicating a sound are input from an outside;</p><p id="p-0010" num="0009">a voice extraction unit that analyzes data which are input from the input unit and indicate a sound and that extracts data which indicate a voice emitted by a person;</p><p id="p-0011" num="0010">a vibration generation unit that generates vibration data, which are associated with data indicating a sound in advance set, based on a result of a comparison between the data which indicate the voice and are extracted by the voice extraction unit and the data indicating the sound in advance set; and</p><p id="p-0012" num="0011">a vibrator that vibrates based on the vibration data generated by the vibration generation unit.</p><p id="p-0013" num="0012">An information output method according to an embodiment of the present invention includes:</p><p id="p-0014" num="0013">a step of analyzing data which are input from an outside and indicate a sound and of extracting data which indicate a voice emitted by a person;</p><p id="p-0015" num="0014">a step of generating vibration data, which are associated with data indicating a sound in advance set, based on a result of a comparison between the data which indicate the voice and are extracted and the data indicating the sound in advance set; and</p><p id="p-0016" num="0015">a step of causing a vibrator to vibrate based on the generated vibration data.</p><p id="p-0017" num="0016">An information output program according to an embodiment of the present invention causes a computer to execute:</p><p id="p-0018" num="0017">a process of analyzing data which are input from an outside and indicate a sound and of extracting data which indicate a voice emitted by a person;</p><p id="p-0019" num="0018">a process of generating vibration data, which are associated with data indicating a sound in advance set, based on a result of a comparison between the data which indicate the voice and are extracted and the data indicating the sound in advance set; and</p><p id="p-0020" num="0019">a process of causing a vibrator to vibrate based on the generated vibration data.</p><p id="p-0021" num="0020">In the present embodiments, an information output apparatus, an information output method, and a non-transitory recording medium can be realized that are capable of notifying information based on an input voice content.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0022" num="0021">The above and other aspects, advantages and features will be more apparent from the following description of certain embodiments taken in conjunction with the accompanying drawings, in which:</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating a configuration of an information output apparatus of a first embodiment.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart illustrating a flow of an information output method by using the information output apparatus of the first embodiment.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for explaining an action in a case where a player gives an instruction to other players in a game by using the information output apparatus of the first embodiment.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram for explaining an action in a case where a player tells another player a situation in a game by using an information output apparatus of a second embodiment.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram for explaining an action in a case where a player tells other players a situation in a game by using an information output apparatus of a third embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0028" num="0027">Hereinafter, specific embodiments to which the present invention is applied will be described in detail with reference to drawings. However, the present invention is not limited to the following embodiments. Further, the following descriptions and drawings are appropriately simplified for the purpose of clarifying the descriptions.</p><heading id="h-0006" level="1">First Embodiment</heading><p id="p-0029" num="0028">First, a description will be made about a configuration of an information output apparatus of the present embodiment. <figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating a configuration of the information output apparatus of the present embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, for example, the information output apparatus <b>1</b> includes headphones <b>2</b>.</p><p id="p-0030" num="0029">The headphones <b>2</b> are in a shape generally equivalent to common headphones and are worn on a head of a user by being put on left and right ears of the user across his/her head, for example. The headphones <b>2</b> include an input unit <b>3</b>, a reception unit <b>4</b>, a voice extraction unit <b>5</b>, an output unit <b>6</b>, speakers <b>7</b>, a database <b>8</b>, a vibration generation unit <b>9</b>, a vibrator control unit <b>10</b>, and a vibrator <b>11</b>, for example.</p><p id="p-0031" num="0030">To the input unit <b>3</b>, data which indicate a sound including at least one of a voice or an environmental sound are input from a microphone, a PC (personal computer), or the like, which is not illustrated. The reception unit <b>4</b> outputs the data which are input from the input unit <b>3</b> and indicate the sound including at least one of a voice or an environmental sound to the voice extraction unit <b>5</b> and the output unit <b>6</b>. The voice extraction unit <b>5</b> extracts data indicating the voice from the data which indicate the sound including at least one of a voice or an environmental sound.</p><p id="p-0032" num="0031">The voice extraction unit <b>5</b> is caused to learn environmental sounds, for example, and can thereby remove data indicating the environmental sound from the data which indicate the sound including at least one of a voice or an environmental sound and extract data indicating the voice. However, the voice extraction unit <b>5</b> may, by using a commonly used technique, extract the data indicating the voice from the data which indicate the sound including at least one of a voice or an environmental sound. The voice extraction unit <b>5</b> outputs the data which are extracted and indicate a voice to the vibration generation unit <b>9</b>.</p><p id="p-0033" num="0032">The output unit <b>6</b> outputs the data which are input from the reception unit <b>4</b> and indicate the sound including at least one of a voice or an environmental sound to the speakers <b>7</b>. The speakers <b>7</b> output the sound which is indicated by the data input from the output unit <b>6</b> and includes at least one of a voice or an environmental sound.</p><p id="p-0034" num="0033">The database <b>8</b> stores data which indicate character strings of words included in voices in advance set, vibration data which are associated with character strings of words included in voices in advance set, data which indicate voices in nervous states in advance set (for example, data which indicate a maximum width of dispersion of pitches or levels of voices in nervous states), and vibration data which are associated with cases where voices are in nervous states, for example.</p><p id="p-0035" num="0034">The vibration generation unit <b>9</b> converts the data, which are input from the voice extraction unit <b>5</b> and indicate the voice, into a text, thereafter executes a morphological analysis, thereby extracts a character string of a word included in the voice, compares the character string of the extracted word with character strings of words which are indicated by data read out from the database <b>8</b> and are in advance set, and reads out, from the database <b>8</b>, (that is, generates) vibration data which are associated with the character string of the word in advance set in a case where a comparison result indicates equality. However, the vibration generation unit <b>9</b> is not limited to the morphological analysis but can use a common technique as long as words can be recognized from data indicating a voice.</p><p id="p-0036" num="0035">Further, the vibration generation unit <b>9</b> compares the dispersion of the pitch or the level of the voice with the maximum width of the dispersion of the pitch or the level of the voice in the nervous state, the maximum width being read out from the database <b>8</b>, based on the data which are input from the voice extraction unit <b>5</b> and indicate the voice, determines that the user is in the nervous state in a case where the dispersion of the pitch or the level of the voice is small compared to the maximum width of the dispersion of the pitch or the level of the voice in the nervous state, and reads out, from the database <b>8</b>, (that is, generates) the vibration data associated with the data indicating the voice in the nervous state. However, the vibration generation unit <b>9</b> can use a common technique as long as it can be determined whether or not the nervous state is exhibited based on the data indicating the voice.</p><p id="p-0037" num="0036">The vibration generation unit <b>9</b> outputs the read-out vibration data to the vibrator control unit <b>10</b>. The vibrator control unit <b>10</b> generates control data to cause the vibrator <b>11</b> to vibrate based on the vibration data input from the vibration generation unit <b>9</b> and outputs the control data to the vibrator <b>11</b>. The vibrator <b>11</b> vibrates based on the control data input from the vibrator control unit <b>10</b>.</p><p id="p-0038" num="0037">The voice extraction unit <b>5</b>, the vibration generation unit <b>9</b>, and the vibrator control unit <b>10</b> are formed with a microprocessor, an MPU (microprocessing unit), or a CPU (central processing unit), for example. Further, the reception unit <b>4</b> and the output unit <b>6</b> serve as interfaces which receive and output data, for example. The database <b>8</b> is formed with an HDD (hard disk drive) as a large capacity recording medium, a ROM as a semiconductor memory such as a mask ROM (read-only memory) or a PROM, and a RAM (random access memory) such as a DRAM or an SRAM, for example.</p><p id="p-0039" num="0038">Next, a description will be made about a flow of an information output method by using the information output apparatus <b>1</b> of the present embodiment. <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart illustrating the flow of the information output method by using the information output apparatus of the present embodiment. <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for explaining an action in a case where a player (that is, the user) gives an instruction to other players in a game by using the information output apparatus of the present embodiment.</p><p id="p-0040" num="0039">Here, in the following description, as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a case is described, as an example, where players A, B, C, and D are executing a game on PCs and when each of the players is called, the vibrator <b>11</b> of the headphones <b>2</b> of the called player vibrates. In this case, it is assumed that the PCs of the players A, B, C, and D are connected together via a network and the PCs execute the information output method illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> by the respective information output apparatuses <b>1</b>.</p><p id="p-0041" num="0040">First, when the player A says &#x201c;player C, move forward by 20 m&#x201d;, data which indicate a sound including a voice of &#x201c;player C, move forward by 20 m&#x201d; and an environmental sound around the player A are input to each of the input units <b>3</b> of the headphones <b>2</b> (S<b>1</b>).</p><p id="p-0042" num="0041">Next, each of the input units <b>3</b> of the headphones <b>2</b> outputs the data which indicate the sound including the voice and the environmental sound to the reception unit <b>4</b>. Then, the reception unit <b>4</b> outputs the data which indicate the sound including the voice and the environmental sound to the voice extraction unit <b>5</b> and the output unit <b>6</b> (S<b>2</b>).</p><p id="p-0043" num="0042">Next, each of the output units <b>6</b> of the headphones <b>2</b> outputs the data which indicate the sound including the voice and the environmental sound to the speakers <b>7</b>. Then, the speakers <b>7</b> output the sound which is indicated by the data and includes the voice and the environmental sound (S<b>3</b>). Accordingly, each pair of speakers <b>7</b> of the headphones <b>2</b> outputs the sound including the voice of &#x201c;player C, move forward by 20 m&#x201d; and the environmental sound around the player A.</p><p id="p-0044" num="0043">At the same time, each of the voice extraction units <b>5</b> of the headphones <b>2</b> determines whether or not data indicating a voice are included in the data which indicate the sound including at least one of a voice or an environmental sound (S<b>4</b>). In the present embodiment, because the data include the data indicating the voice of &#x201c;player C, move forward by 20 m&#x201d;, the voice extraction unit <b>5</b> determines that the data indicating a voice are included in the data which indicate the sound including at least one of a voice or an environmental sound (YES in S<b>4</b>), extracts the data indicating a voice from the data which indicate the sound including at least one of a voice or an environmental sound, and outputs the data indicating the voice to the vibration generation unit <b>9</b>. On the other hand, in a case where the data indicating a voice are not included in the data which indicate the sound including at least one of a voice or an environmental sound (NO in S<b>4</b>), the voice extraction unit <b>5</b> finishes a process.</p><p id="p-0045" num="0044">Each of the vibration generation units <b>9</b> of the headphones <b>2</b> in parallel starts a generation process of vibration data based on a word included in a voice and a generation process of vibration data based on a nervous state of the voice in the following, for the data indicating the voice (S<b>5</b>). Specifically, the vibration generation unit <b>9</b> extracts a character string of a word included in the voice from the data indicating the voice (S<b>6</b>).</p><p id="p-0046" num="0045">Next, each of the vibration generation units <b>9</b> of the headphones <b>2</b> compares the character string of the extracted word with a character string of the word which is indicated by data read out from the database <b>8</b> and is in advance set and determines whether or not a comparison result indicates equality (S<b>7</b>).</p><p id="p-0047" num="0046">Specifically, the character string of each extracted word is compared with the character string of each of words which are indicated by the data read out from the database <b>8</b> and are in advance set, and it is determined whether or not the sets of character strings of the words, which agree with each other, are present.</p><p id="p-0048" num="0047">Here, it is assumed that each of the databases <b>8</b> of the headphones <b>2</b> stores data which indicate the character string of a designation for specifying a player himself/herself who wears the headphones <b>2</b> and vibration data which indicate that the vibrator <b>11</b> provided in the headphones <b>2</b> is caused to vibrate in a case where the character string of the extracted word is equivalent to the character string of the designation for specifying the player himself/herself who wears the headphones <b>2</b>.</p><p id="p-0049" num="0048">In other words, the database <b>8</b> of the headphones <b>2</b> worn by the player A stores the data which indicate the character string of the designation for specifying the player A and the vibration data which indicate that the vibrator <b>11</b> provided in the headphones <b>2</b> worn by the player A is caused to vibrate in a case where the character string of the extracted word is equivalent to the character string of the designation for specifying the player A. Similarly, the databases <b>8</b> of the headphones <b>2</b> of the player B, the player C, and the player D store similar data.</p><p id="p-0050" num="0049">In the present embodiment, because the voice does not include the character string of the designation for specifying each of the players A, B, and D, each of the vibration generation units <b>9</b> of the headphones <b>2</b> of the players A, B, and D determines that the comparison result indicates disagreement (NO in S<b>7</b>), does not read out the vibration data from the database <b>8</b>, and finishes the generation process of the vibration data based on the word included in the voice.</p><p id="p-0051" num="0050">On the other hand, because the voice includes the character string of the designation for specifying &#x201c;player C&#x201d;, the vibration generation unit <b>9</b> of the headphones <b>2</b> of the player C determines that the comparison result indicates equality (YES in S<b>7</b>), reads out the vibration data from the database <b>8</b>, and outputs the vibration data to the vibrator control unit <b>10</b> (S<b>8</b>).</p><p id="p-0052" num="0051">Next, the vibrator control unit <b>10</b> of the headphones <b>2</b> of the player C generates the control data based on the vibration data and outputs the control data to the vibrator <b>11</b>. The vibrator <b>11</b> vibrates based on the control data input from the vibrator control unit <b>10</b> (S<b>9</b>).</p><p id="p-0053" num="0052">At the same time, each of the vibration generation units <b>9</b> of the headphones <b>2</b> calculates the dispersion of the pitch or the level of the voice based on the data indicating the voice (S<b>10</b>). Then, the vibration generation unit <b>9</b> compares the dispersion of the pitch or the level of the voice with the maximum width of the dispersion of the pitch or the level of the voice in the nervous state, the maximum width being read out from the database <b>8</b>, and determines whether or not the dispersion of the pitch or the level of the voice is small compared to the maximum width of the dispersion of the pitch or the level of the voice in the nervous state (S<b>11</b>).</p><p id="p-0054" num="0053">For example, in a case where the dispersion of the pitch or the level of the voice of &#x201c;player C, move forward by 20 m&#x201d; is equivalent to or more than a maximum value of the dispersion of the pitch or the level of the voice in the nervous state (NO in S<b>11</b>), each of the vibration generation units <b>9</b> of the headphones <b>2</b> determines that the voice is not in the nervous state, does not read out the vibration data from the database <b>8</b>, and finishes the generation process of the vibration data based on the nervous state of the voice.</p><p id="p-0055" num="0054">On the other hand, in a case where the dispersion of the pitch or the level of the voice of &#x201c;player C, move forward by 20 m&#x201d; is small compared to the maximum value of the dispersion of the pitch or the level of the voice in the nervous state (YES in S<b>11</b>), each of the vibration generation units <b>9</b> of the headphones <b>2</b> determines that the voice is in the nervous state, reads out, from the database <b>8</b>, the vibration data associated with a case where the voice is in the nervous state, and outputs the vibration data to the vibrator control unit <b>10</b> (S<b>12</b>). Here, for example, in a case where the voice is in the nervous state, the vibration data associated with the case where the voice is in the nervous state are instruction data which cause the vibrator <b>11</b> to vibrate.</p><p id="p-0056" num="0055">Next, each of the vibrator control units <b>10</b> of the headphones <b>2</b> generates the control data which cause the vibrator <b>11</b> to vibrate based on the vibration data and outputs the control data to the vibrator <b>11</b>. The vibrator <b>11</b> vibrates based on the control data input from the vibrator control unit <b>10</b> (S<b>13</b>).</p><p id="p-0057" num="0056">Such information output apparatus <b>1</b> and information output method generate vibration data, which are associated with data indicating a sound in advance set, based on a result of a comparison between data indicating a voice and the data indicating the sound in advance set. Thus, information based on a voice content to be input is capable of being notified.</p><p id="p-0058" num="0057">In other words, in the above-described example, the fact that an instruction is for the player C can be notified to the player C by vibration. Thus, the player C can easily recognize that the instruction is for the player C himself/herself and can carefully listen to the instruction by the player A without failing to hear that. Meanwhile, the other players B and D can recognize that the instruction is for the player C and concentrate on the game.</p><p id="p-0059" num="0058">Note that the designation specifying the player himself/herself who wears the headphones <b>2</b> may be a name, a nickname, or the like as long as the name, nickname, or the like can specify the player himself/herself who wears the headphones <b>2</b>.</p><heading id="h-0007" level="1">Second Embodiment</heading><p id="p-0060" num="0059">In an information output apparatus and an information output method of the present embodiment, when a voice for specifying a direction is input, a vibrator <b>11</b> corresponding to the direction vibrates. However, the information output apparatus and the information output method of the present embodiment are generally equivalent to the information output apparatus <b>1</b> and the information output method of the first embodiment, and the concerned descriptions will not be repeated, and descriptions will be made by using equivalent reference characters for equivalent elements.</p><p id="p-0061" num="0060">The information output apparatus of the present embodiment includes left and right vibrators <b>11</b>, for example. Furthermore, the database <b>8</b> of the information output apparatus stores data which indicate character strings of designations for specifying directions (such as left and right, for example) and vibration data in which the character strings of the designations for specifying the directions are associated with the vibrators caused to vibrate.</p><p id="p-0062" num="0061">For example, in the vibration data in which the character strings of the designations for specifying the directions are associated with the vibrators caused to vibrate, &#x201c;left&#x201d; as the character string of the designation is associated with the left vibrator as the vibrator caused to vibrate, and &#x201c;right&#x201d; as the character string of the designation is associated with the right vibrator as the vibrator caused to vibrate.</p><p id="p-0063" num="0062">The vibration generation unit <b>9</b> of the information output apparatus compares the character string of an extracted word with the character string of the designation for specifying the direction indicated by the data read out from the database <b>8</b>. In a case where the character string of the extracted word includes the character string of the designation for specifying the direction, the vibration generation unit <b>9</b> reads out, from the database <b>8</b>, the vibration data corresponding to the character string of the designation and outputs the vibration data to the vibrator control unit <b>10</b>. In other words, the vibration generation unit <b>9</b> selects the vibrator <b>11</b> caused to vibrate in accordance with the designation which is included in the voice and is for specifying the direction.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram for explaining an action in a case where a player tells another player a situation in a game by using the information output apparatus of the present embodiment. Here, in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, it is assumed that players who are executing the game are players A and B.</p><p id="p-0065" num="0064">When the player A says &#x201c;an enemy is on the left&#x201d;, because the character string of the extracted words includes &#x201c;left&#x201d;, the vibration generation units <b>9</b> of the headphones <b>2</b> of the players A and B output the vibration data which cause the left vibrators <b>11</b> to vibrate to the vibrator control units <b>10</b>.</p><p id="p-0066" num="0065">Accordingly, the player B can instinctively recognize that the enemy is present on the left side. However, in the present embodiment, the headphones <b>2</b> are configured to include the left and right vibrators <b>11</b> but may include plural, more than two vibrators <b>11</b>. In this case also, the database <b>8</b> preferably stores vibration data in which the character strings of designations for specifying directions are associated with the vibrators <b>11</b> caused to vibrate.</p><heading id="h-0008" level="1">Third Embodiment</heading><p id="p-0067" num="0066">In the present embodiment, a case where a voice is in a nervous state will be described as an example. Note that a flow for processing the input voice is equivalent to the flow of the information output method of the first embodiment. <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram for explaining an action in a case where a player tells other players a situation in a game by using the information output apparatus of the present embodiment. Here, in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, it is assumed that players who are executing the game are players A, B, C, and D.</p><p id="p-0068" num="0067">When the player A says &#x201c;we are surrounded by enemies&#x201d;, each of the vibration generation units <b>9</b> of the headphones of the players B, C, and D determines whether or not the dispersion of the pitch or the level of a voice of &#x201c;we are surrounded by enemies&#x201d; is small compared to the maximum value of the dispersion of the pitch or the level of the voice in the nervous state (S<b>11</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0069" num="0068">For example, in a case where &#x201c;we are surrounded by enemies&#x201d; is said in the nervous state, the dispersion of the pitch or the level of the voice of &#x201c;we are surrounded by enemies&#x201d; becomes small compared to the maximum value of the dispersion of the pitch or the level of the voice in the nervous state, each of the vibration generation units <b>9</b> of the headphones <b>2</b> determines that the voice is in the nervous state, reads out the vibration data from the database <b>8</b>, and outputs the vibration data to the vibrator control unit <b>10</b> (S<b>12</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0070" num="0069">Accordingly, the players B, C, and D can carefully listen to nervous saying by the player A without failing to hear that.</p><p id="p-0071" num="0070">The present invention is not limited to the above embodiments but may appropriately be modified without departing from the scope of the gist.</p><p id="p-0072" num="0071">For example, in the information output apparatus in the above embodiments, the headphones include the voice extraction unit <b>5</b>, the database <b>8</b>, and the vibration generation unit <b>9</b>; however, a server separate from the headphones may include the voice extraction unit <b>5</b>, the database <b>8</b>, and the vibration generation unit <b>9</b>. In this case, pairs of headphones can share the voice extraction unit <b>5</b>, the database <b>8</b>, and the vibration generation unit <b>9</b>. In this case, when the databases <b>8</b> stores data in which each of the pairs of headphones is associated with a designation for specifying the user who wears the pair of headphones, in a case where a voice including the designation for specifying the user who wears the pair of headphones is input, the vibrator provided in the pair of headphones worn by the user can be caused to vibrate.</p><p id="p-0073" num="0072">For example, the information output apparatus <b>1</b> includes the headphones <b>2</b> but is not limited to this and may be a device which includes the input unit <b>3</b>, the reception unit <b>4</b>, the voice extraction unit <b>5</b>, the output unit <b>6</b>, the speaker <b>7</b>, the database <b>8</b>, the vibration generation unit <b>9</b>, the vibrator control unit <b>10</b>, and the vibrator <b>11</b>, which are illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Further, a device does not have to include all of those configurations.</p><p id="p-0074" num="0073">For example, a sound which is in advance set may be a sound of footsteps of an approaching enemy or the like and can appropriately be set in accordance with contents of games and so forth. Further, the vibration data may be generated such that vibration patterns are changed in accordance with character strings of extracted words or nervous states.</p><p id="p-0075" num="0074">For example, in the above embodiments, the generation process of the vibration data based on the word included in a voice and the generation process of the vibration data based on the nervous state of the voice are in parallel executed; however, only either one may be executed, or the generation process of the vibration data based on the word included in a voice and the generation process of the vibration data based on the nervous state of the voice may be executed at different timings from each other.</p><p id="p-0076" num="0075">For example, in the above embodiments, the present invention is described as configurations of hardware, but the present invention is not limited to such configurations. It is possible to realize the present invention by causing an arbitrary process to be executed by causing a CPU (central processing unit) to execute a computer program.</p><p id="p-0077" num="0076">A program can be stored and provided to a computer using any type of non-transitory computer readable media. Non-transitory computer readable media include any type of tangible storage media. Examples of non-transitory computer readable media include magnetic storage media (such as floppy disks, magnetic tapes, hard disk drives, etc.), optical magnetic storage media (e.g. magneto-optical disks), CD-ROM (compact disc read only memory), CD-R (compact disc recordable), CD-R/W (compact disc rewritable), and semiconductor memories (such as mask ROM, PROM (programmable ROM), EPROM (erasable PROM), flash ROM, RAM (random access memory), etc.). The program may be provided to a computer using any type of transitory computer readable media. Examples of transitory computer readable media include electric signals, optical signals, and electromagnetic waves. Transitory computer readable media can provide the program to a computer via a wired communication line (e.g. electric wires, and optical fibers) or a wireless communication line.</p><p id="p-0078" num="0077">While the invention has been described in terms of several embodiments, those skilled in the art will recognize that the invention can be practiced with various modifications within the spirit and scope of the appended claims and the invention is not limited to the examples described above.</p><p id="p-0079" num="0078">Further, the scope of the claims is not limited by the embodiments described above.</p><p id="p-0080" num="0079">Furthermore, it is noted that, Applicant's intent is to encompass equivalents of all claim elements, even if amended later during prosecution.</p><p id="p-0081" num="0080">The first, second and third embodiments can be combined as desirable by one of ordinary skill in the art.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An information output apparatus comprising:<claim-text>an input unit to which data indicating a sound are input from an outside;</claim-text><claim-text>a voice extraction unit that analyzes data which are input from the input unit and indicate a sound and that extracts data which indicate a voice emitted by a person;</claim-text><claim-text>a vibration generation unit that generates vibration data, which are associated with data indicating a sound in advance set, based on a result of a comparison between the data which indicate the voice and are extracted by the voice extraction unit and the data indicating the sound in advance set; and</claim-text><claim-text>plural vibrators that vibrate based on the vibration data generated by the vibration generation unit.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The information output apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein in a case where the data which indicate a voice and are extracted by the voice extraction unit include data which indicate a designation for specifying a user, the vibration generation unit generates the vibration data such that the vibrator provided in the information output device worn by the user vibrates.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The information output apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein in a case where the data which indicate a voice and are extracted by the voice extraction unit include data which indicate a designation for specifying a direction, the vibration generation unit generates the vibration data which select the vibrator caused to vibrate from the plural vibrators based on the data which indicate the designation for specifying the direction.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The information output apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>,<claim-text>wherein in a case where the data which indicate a voice and are extracted by the voice extraction unit include data which indicate a designation for specifying a direction, the vibration generation unit generates the vibration data which select the vibrator caused to vibrate from the plural vibrators based on the data which indicate the designation for specifying the direction.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The information output apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the vibration generation unit compares the data which indicate a voice and are extracted by the voice extraction unit with data which indicate a voice in a nervous state and are in advance set and generates the vibration data, which are associated with the data indicating the voice in the nervous state, in a case where the voice extracted by the voice extraction unit is in the nervous state.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The information output apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>,<claim-text>wherein the vibration generation unit compares the data which indicate a voice and are extracted by the voice extraction unit with data which indicate a voice in a nervous state and are in advance set and generates the vibration data, which are associated with the data indicating the voice in the nervous state, in a case where the voice extracted by the voice extraction unit is in the nervous state.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. An information output method comprising:<claim-text>a step of analyzing data which are input from an outside and indicate a sound and of extracting data which indicate a voice emitted by a person;</claim-text><claim-text>a step of generating vibration data, which are associated with data indicating a sound in advance set, based on a result of a comparison between the data which indicate the voice and are extracted and the data indicating the sound in advance set; and</claim-text><claim-text>a step of causing plural vibrators to vibrate based on the generated vibration data.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A non-transitory computer-readable recording medium recording an information output program causing a computer to execute:<claim-text>a process of analyzing data which are input from an outside and indicate a sound and of extracting data which indicate a voice emitted by a person;</claim-text><claim-text>a process of generating vibration data, which are associated with data indicating a sound in advance set, based on a result of a comparison between the data which indicate the voice and are extracted and the data indicating the sound in advance set; and</claim-text><claim-text>a process of causing plural vibrators to vibrate based on the generated vibration data.</claim-text></claim-text></claim></claims></us-patent-application>