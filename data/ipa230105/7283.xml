<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007284A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007284</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17779380</doc-number><date>20191223</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>436</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>176</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>593</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>159</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>11</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>124</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>149</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>436</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>176</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>593</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>159</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>11</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>124</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>149</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Ultra Light Models and Decision Fusion for Fast Video Coding</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>62940272</doc-number><date>20191126</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Google LLC</orgname><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Li</last-name><first-name>Shan</first-name><address><city>Fremont</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Coelho</last-name><first-name>Claudionor</first-name><address><city>Redwood City</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Chong</last-name><first-name>In Suk</first-name><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Kuusela</last-name><first-name>Aki</first-name><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/US2019/068279</doc-number><date>20191223</date></document-id><us-371c12-date><date>20220524</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Ultra light models and decision fusion for increasing the speed of intra-prediction are described. Using a machine-learning (ML) model, an ML intra-prediction mode is obtained. A most-probable intra-prediction mode is obtained from amongst available intra-prediction modes for encoding the current block. As an encoding intra-prediction mode, one of the ML intra-prediction mode or the most-probable intra-prediction mode is selected, and the encoding intra-prediction mode is encoded in a compressed bitstream. A current block is encoded using the encoding intra-prediction mode. Selection of the encoding intra-prediction mode is based on relative reliabilities of the ML intra-prediction mode and the most-probable intra-prediction mode.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="207.09mm" wi="158.75mm" file="US20230007284A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="201.85mm" wi="82.38mm" file="US20230007284A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="194.48mm" wi="172.21mm" orientation="landscape" file="US20230007284A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="241.98mm" wi="119.46mm" orientation="landscape" file="US20230007284A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="194.90mm" wi="125.98mm" orientation="landscape" file="US20230007284A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="252.65mm" wi="160.36mm" file="US20230007284A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="247.90mm" wi="173.99mm" orientation="landscape" file="US20230007284A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="248.84mm" wi="167.81mm" file="US20230007284A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="249.09mm" wi="159.68mm" orientation="landscape" file="US20230007284A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="242.23mm" wi="171.03mm" orientation="landscape" file="US20230007284A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="236.73mm" wi="162.98mm" file="US20230007284A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="237.32mm" wi="166.45mm" orientation="landscape" file="US20230007284A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="239.01mm" wi="164.42mm" orientation="landscape" file="US20230007284A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="227.25mm" wi="164.85mm" orientation="landscape" file="US20230007284A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="178.31mm" wi="124.29mm" file="US20230007284A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="256.54mm" wi="169.76mm" orientation="landscape" file="US20230007284A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading><p id="p-0002" num="0001">This application claims priority to and the benefit of U.S. Provisional Patent Application Ser. No. 62/940,272, filed Nov. 26, 2019, the entire disclosure of which is hereby incorporated by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Digital video streams may represent video using a sequence of frames or still images. Digital video can be used for various applications, including, for example, video conferencing, high-definition video entertainment, video advertisements, or sharing of user-generated videos. A digital video stream can contain a large amount of data and consume a significant amount of computing or communication resources of a computing device for processing, transmission, or storage of the video data. Various approaches have been proposed to reduce the amount of data in video streams, including compression and other encoding techniques.</p><p id="p-0004" num="0003">Over the years, the coding efficiency of video encoders has improved. Coding efficiency can mean encoding a video at the lowest possible bit rate while minimizing distortion (i.e., while maintaining a certain level of video quality). However, the improved coding efficiency has resulted in increased computational complexity. That is, more computation time is required by an encoder to achieve the improved coding efficiency. As such, it is desirable to obtain improved coding efficiencies with less computation time (i.e., reduced computational complexity).</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">One aspect of the disclosed implementations is a method for encoding a current block of video using intra-prediction. The method includes obtaining, using a machine-learning (ML) model, an ML intra-prediction mode; obtaining a most-probable intra-prediction mode from amongst available intra-prediction modes for encoding the current block; selecting, as an encoding intra-prediction mode, one of the ML intra-prediction mode or the most-probable intra-prediction mode; encoding, in a compressed bitstream, the encoding intra-prediction mode; and encoding the current block using the encoding intra-prediction mode. The selecting is based on relative reliabilities of the ML intra-prediction mode and the most-probable intra-prediction mode.</p><p id="p-0006" num="0005">Another aspect is a method for encoding a current block of video using intra-prediction. The method includes obtaining pre-calculated features; obtaining, using a machine-learning (ML) model, an ML intra-prediction mode, where the ML model receives the pre-calculated features as inputs; selecting an encoding intra-prediction mode using at least the ML intra-prediction mode; encoding, in a compressed bitstream, the encoding intra-prediction mode; and encoding the current block using the encoding intra-prediction mode. The pre-calculated features can include at least two of a first feature, the first feature being a non-linear function of a quantization parameter; second features, the second features being respective errors between the current block and respective prediction blocks, wherein each prediction block corresponds to an available intra-prediction mode; a mean and a variance of the current block; and a sum-of-absolute values of a convolution block, the convolution block obtained from the current block.</p><p id="p-0007" num="0006">Another aspect is an apparatus for encoding a current block of video using intra-prediction. The apparatus obtains, using a machine-learning (ML) model, ML intra-prediction modes; obtains most-probable intra-prediction modes from amongst available intra-prediction modes for encoding the current block; selects, as an encoding intra-prediction mode, one of the ML intra-prediction mode or one of the most-probable intra-prediction modes; encodes, in a compressed bitstream, the encoding intra-prediction mode; and encodes the current block using the encoding intra-prediction mode.</p><p id="p-0008" num="0007">Another aspect is an apparatus for encoding a current block of video using intra-prediction. The apparatus obtains pre-calculated features; obtains, using a machine-learning (ML) model, an ML intra-prediction mode, where the ML model receives the pre-calculated features as inputs; selects an encoding intra-prediction mode using at least the ML intra-prediction mode; encodes, in a compressed bitstream, the encoding intra-prediction mode; and encodes the current block using the encoding intra-prediction mode. The pre-calculated features can include at least two of a first feature, which is a non-linear function of a quantization parameter; second features, which are respective errors between the current block and respective prediction blocks, where each prediction block corresponds to an available intra-prediction mode; a mean and a variance of the current block; and a sum-of-absolute values of a convolution block, the convolution block obtained from the current block.</p><p id="p-0009" num="0008">These and other aspects of the present disclosure are disclosed in the following detailed description of the embodiments, the appended claims, and the accompanying figures.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0010" num="0009">The description herein makes reference to the accompanying drawings, wherein like reference numerals refer to like parts throughout the several views.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic of a video encoding and decoding system.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of an example of a computing device that can implement a transmitting station or a receiving station.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram of a video stream to be encoded and subsequently decoded.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of an encoder according to implementations of this disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram of a decoder according to implementations of this disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram of a representation of a portion of a frame according to implementations of this disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram of examples of intra prediction modes according to implementations of this disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart of a process for searching for a best mode to code a block.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram of an example of estimating the rate and distortion costs of coding an image block by using a prediction mode.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIGS. <b>10</b>A and <b>10</b>B</figref> are block diagrams of examples of convolutional neural networks (CNNs) for mode decisions.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart of a process for encoding, by an encoder, an image block using intra-prediction according to implementations of this disclosure.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>12</b>A</figref> is a diagram of an example of an all-dense network structure according to implementations of this disclosure.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>12</b>B</figref> is a diagram of an example of a network structure according to implementations of this disclosure.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is an example of obtaining a sum-of-absolute values of a convolution block according to implementations of this disclosure.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates examples of decision matrices according to implementations of this disclosure.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flowchart of a process for encoding a current block using intra-prediction according to implementations of this disclosure.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a flowchart of a process for encoding a current block using intra-prediction according to implementations of this disclosure.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is an example <b>1700</b> of a rate-distortion performance comparison.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0029" num="0028">Modern video codecs (e.g., H.264, which is also known as MPEG-4 Part 10; VP9; H.265, which is also known as High Efficiency Video Coding (HEVC); the Audio Video Coding Standard; and AOMedia Video 1 (AV1)) define and use many tools and configurations to improve coding efficiency. Coding efficiency is typically measured in terms of both rate and distortion. Rate refers to the number of bits required for encoding (such as encoding a block, a frame, etc.). Distortion measures the quality loss between, for example, a source video block and a reconstructed version of source video block. By performing a rate-distortion optimization (RDO) process, a video codec optimizes the amount of distortion against the rate required to encode the video.</p><p id="p-0030" num="0029">To determine an optimal combination of tools and configurations (e.g., parameters) to be used, a video encoder can use a mode decision process. The mode decision process can examine (e.g., test, evaluate, etc.) at least some of the valid combinations of tools. In an example, all possible combinations are examined. Examining all possible combinations, or a subset thereof, is referred to herein as a brute-force approach.</p><p id="p-0031" num="0030">The brute-force approach for mode decision can compute the rate-distortion cost of all possible combinations (or a subset thereof) and can then choose the one with the best cost for each basic processing block. Different codecs use different nomenclature to refer to the basic processing block. The basic processing block may be referred to as a superblock (i.e., in AV1), as a macroblock (i.e., in HEVC), and so on. The brute-force approach can be, especially if all combinations are tested, extremely computationally intensive. As the number of possible tools and parameters increases, the number of combinations also increases, which, in turn, increases the time required to determine the best mode. For example, the AV1 codec includes roughly 160 additional tools over the VP9 codec, thereby resulting in a significant increase in search time for the best mode.</p><p id="p-0032" num="0031">Assume that a first combination of parameters results in a first rate (e.g., rate=100) and a first distortion (e.g., distortion=90) and that a second combination of parameters results in a second rate (e.g., rate=120) and a second distortion (e.g., distortion=80). A procedure (e.g., a technique, etc.) is required to evaluate which of the first combination and the second combination is the better combination of parameters. To evaluate whether one combination is better than another, a metric can be computed for each of the examined combinations and the respective metrics compared. In an example, the metric can combine the rate and distortion to produce one single scalar value, as described below. In this disclosure, the rate-distortion cost is used as such a scalar value.</p><p id="p-0033" num="0032">An example of a mode decision process is an intra-prediction mode decision process, which determines the best intra-prediction mode for coding a coding block.</p><p id="p-0034" num="0033">To encode a basic processing block (which may be of size 64&#xd7;64, 128&#xd7;128, or some other size), an encoder may attempt to determine (e.g., extract, select, etc.) a hierarchical representation (i.e., a partitioning) of the basic processing block all the way to 4&#xd7;4 blocks, with the options of not splitting the block. The possible partitions (of the basic processing block or any square sub-block) may include not partitioning the block, using vertical or horizontal splits, using square or rectangular splits, or a combination thereof.</p><p id="p-0035" num="0034">Encoding a video stream, or a portion thereof, such as a frame or a block, can include using temporal or spatial similarities in the video stream to improve coding efficiency. For example, a current block of a video stream may be encoded based on identifying a difference (residual) between the previously coded pixel values, or between a combination of previously coded pixel values, and those in the current block.</p><p id="p-0036" num="0035">Encoding using temporal similarities can be known as inter prediction. Inter prediction attempts to predict the pixel values of a block using a possibly displaced block or blocks from a temporally nearby frame (i.e., reference frame) or frames.</p><p id="p-0037" num="0036">Encoding using spatial similarities can be known as intra prediction. Intra prediction attempts to predict the pixel values of a block of a frame of video using pixels peripheral to the block. The pixels peripheral to the block can include top pixels, which are pixels above the block; left pixels, which are pixels to the left of the block; and/or one or more top-left pixels. That is, the peripheral pixels are pixels that are in the same frame as the block but that are outside of the block. The mechanism of how the edge (i.e., the peripheral) pixels are used to generate the prediction is referred to as the intra prediction mode.</p><p id="p-0038" num="0037">In the HEVC encoder, for example, 35 intra-prediction modes are possible for luminance blocks that are larger than or equal to 4&#xd7;4. The VP9 encoder uses ten intra-prediction modes. The AV1 encoder can be said to include 58 intra-prediction modes (two non-directional modes plus eight main directional angles with three, positive and negative, offset delta angles from each of the main angles). Each of the intra-prediction modes dictates how a respective prediction block is determined. The mode decision process, in this context, may determine a respective prediction block for each of the intra-prediction modes and select the intra-prediction mode corresponding to the smallest rate-distortion cost. Said another way, the mode decision process selects the intra-prediction mode that provides the best rate-distortion performance. Examples of intra-prediction modes are provided with respect to <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0039" num="0038">As mentioned above, the best intra-prediction mode (or simply, a best mode) can be selected from many possible combinations.</p><p id="p-0040" num="0039">Accordingly, techniques, such as machine learning, may be exploited to reduce the time required to determine the best intra-prediction mode. Machine learning can be well suited to address the computational complexity problem in video coding. The brute-force, on-the-fly mode decision process that is typically performed by an encoder can be replaced with a trained machine-learning (ML) model, which can infer an intra-prediction mode decision for encoding a current block. A well-trained ML model can be expected to closely match the brute-force approach in coding efficiency but at a significantly lower computational cost or with a regular or dataflow-oriented computational cost.</p><p id="p-0041" num="0040">A vast amount of training data can be generated, for example, by using the brute-force approaches to mode decision (e.g., intra-prediction mode decision). That is, the training data can be obtained by an encoder performing standard encoding techniques, such as those described with respect to <figref idref="DRAWINGS">FIGS. <b>4</b>, <b>8</b>, and <b>9</b></figref>.</p><p id="p-0042" num="0041">The training data can be used, during the learning phase of machine learning, to derive (e.g., learn, infer, etc.) a machine-learning (ML) model that is (e.g., defines, constitutes, etc.) a mapping from the input data to an output that constitutes a mode decision, such as the intra-prediction mode to be used for encoding a current block. Accordingly, the ML model can be used to replace the brute-force, computation heavy encoding processes (such as those described with respect to <figref idref="DRAWINGS">FIGS. <b>4</b>, <b>8</b>, and <b>9</b></figref>), thereby reducing the computation complexity in mode decision.</p><p id="p-0043" num="0042">The predictive capabilities (i.e., accuracy) of an ML model are as good as the inputs used to train the machine-learning model and the inputs presented to the machine-learning model to predict a result (e.g., the best mode). As such, when machine learning is used for video encoding, it is critical that the correct set of inputs and the correct (e.g., appropriate, optimal, etc.) forms of such inputs are used. Once an ML model is trained, the model computes the output as a deterministic function of its input(s). As such, it can be critical to have the right architecture for an ML model.</p><p id="p-0044" num="0043">An efficient ML model architecture designed for video coding is described herein. In an example, the machine-learning model can be a neural-network model. In an example, the neural-network model can be ultra-light (i.e., contains a relatively small number of parameters and/or operations) and includes only dense operations (i.e., fully connected layers). In another example, the neural-network model can include convolutional layers, yet it is still relatively ultra-light.</p><p id="p-0045" num="0044">The well-known universal approximation theorem of information theory states that a feed-forward neural network can be used to approximate any continuous function on a compact subset of the n-dimensional real coordinate space R<sup>n</sup>. It is noted that the intrinsic linear nature of existing neural networks implies that a smaller network or shorter learning time may be achieved if a neural network is tasked (i.e., trained) to approximate (e.g., map, solve, infer) a linear function (e.g., mapping) than a non-linear function. It is also noted that the mapping of video blocks to mode decisions can be characterized as a continuous function.</p><p id="p-0046" num="0045">The universal approximation theorem does not characterize feasibility or time and space complexity of the learning phase. That is, while a neural network may be theoretically capable of approximating the non-linear function, an unreasonably large (e.g., in terms of the number of nodes and/or layers) network and/or an unreasonably long training time may be required for the neural network to learn to approximate, using linear functions, the non-linear function. For practical purposes, the unreasonable size and time required may render the learning, the inference, or both infeasible.</p><p id="p-0047" num="0046">As mentioned, a well-trained neural network model can be expected to closely match the brute-force approach in coding efficiency. However, for many resource-constrained encoding platforms (e.g., mobile devices such as cell phones, online video streaming, etc.), codec hardware area size and encoding speed can be of great concern.</p><p id="p-0048" num="0047">As such, a small model (i.e., one with a small set of parameters and few inference operations) may be preferred over large models. However, a small model may not have the same accuracy as a deep (or large) model.</p><p id="p-0049" num="0048">To boost, or supplement, the potential lower inference accuracy of small ML models, implementations according to this disclosure can combine (i.e., fuse) the ML model with heuristics that may be used in video codecs in best mode selections (e.g., intra-prediction mode selection) to maintain the encoding efficiency and significantly lower computational cost. Using the right features (as input to the ML model) and choosing the right fusion strategy of the ML model output with heuristics (e.g., codec heuristics for choosing an intra-prediction mode) can be of utmost importance to achieve a compact deep learning/machine learning model that can attempt to analyze intra prediction modes during the encoding process.</p><p id="p-0050" num="0049">A small ML model size can make it possible to perform inferring on a power/capacity-constrained mobile platform. A small ML model size according to implementations of this disclosure can be obtained by 1) pre-calculating a set of directional features as input and including existing feature(s) from the codec, such as mean and variance of the block, quantization parameter (QP) and sum of absolute differences SAD-based mode cost as part of the model input; and 2) merging the ML prediction with SAD-based mode decision to optimize mode accuracy.</p><p id="p-0051" num="0050">Ultra-light models and decision fusion for fast video coding is described herein first with reference to a system in which the teachings may be incorporated.</p><p id="p-0052" num="0051">It is noted that details of machine learning, neural networks, and/or details that are known to a person skilled in the art are omitted herein. For example, a skilled person in the art recognizes that the values of the weights of connections between nodes (i.e., neurons) in a neural network are determined during the training phase. Accordingly, such are not discussed in detail herein.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic of a video encoding and decoding system <b>100</b>. A transmitting station <b>102</b> can be, for example, a computer having an internal configuration of hardware, such as that described with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>. However, other suitable implementations of the transmitting station <b>102</b> are possible. For example, the processing of the transmitting station <b>102</b> can be distributed among multiple devices.</p><p id="p-0054" num="0053">A network <b>104</b> can connect the transmitting station <b>102</b> and a receiving station <b>106</b> for encoding and decoding of the video stream. Specifically, the video stream can be encoded in the transmitting station <b>102</b>, and the encoded video stream can be decoded in the receiving station <b>106</b>. The network <b>104</b> can be, for example, the Internet. The network <b>104</b> can also be a local area network (LAN), wide area network (WAN), virtual private network (VPN), cellular telephone network, or any other means of transferring the video stream from the transmitting station <b>102</b> to, in this example, the receiving station <b>106</b>.</p><p id="p-0055" num="0054">In one example, the receiving station <b>106</b> can be a computer having an internal configuration of hardware, such as that described with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>. However, other suitable implementations of the receiving station <b>106</b> are possible. For example, the processing of the receiving station <b>106</b> can be distributed among multiple devices.</p><p id="p-0056" num="0055">Other implementations of the video encoding and decoding system <b>100</b> are possible. For example, an implementation can omit the network <b>104</b>. In another implementation, a video stream can be encoded and then stored for transmission at a later time to the receiving station <b>106</b> or any other device having memory. In one implementation, the receiving station <b>106</b> receives (e.g., via the network <b>104</b>, a computer bus, and/or some communication pathway) the encoded video stream and stores the video stream for later decoding. In an example implementation, a real-time transport protocol (RTP) is used for transmission of the encoded video over the network <b>104</b>. In another implementation, a transport protocol other than RTP may be used e.g., Hypertext Transfer Protocol-based (HTTP-based video) streaming protocol.</p><p id="p-0057" num="0056">When used in a video conferencing system, for example, the transmitting station <b>102</b> and/or the receiving station <b>106</b> may include the ability to both encode and decode a video stream as described below. For example, the receiving station <b>106</b> could be a video conference participant who receives an encoded video bitstream from a video conference server (e.g., the transmitting station <b>102</b>) to decode and view and further encodes and transmits its own video bitstream to the video conference server for decoding and viewing by other participants.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of an example of a computing device <b>200</b> that can implement a transmitting station or a receiving station. For example, the computing device <b>200</b> can implement one or both of the transmitting station <b>102</b> and the receiving station <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The computing device <b>200</b> can be in the form of a computing system including multiple computing devices, or in the form of a single computing device, for example, a mobile phone, a tablet computer, a laptop computer, a notebook computer, a desktop computer, and the like.</p><p id="p-0059" num="0058">A CPU <b>202</b> in the computing device <b>200</b> can be a central processing unit. Alternatively, the CPU <b>202</b> can be any other type of device, or multiple devices, now existing or hereafter developed, capable of manipulating or processing information. Although the disclosed implementations can be practiced with a single processor as shown (e.g., the CPU <b>202</b>), advantages in speed and efficiency can be achieved by using more than one processor.</p><p id="p-0060" num="0059">In an implementation, a memory <b>204</b> in the computing device <b>200</b> can be a read-only memory (ROM) device or a random-access memory (RAM) device. Any other suitable type of storage device can be used as the memory <b>204</b>. The memory <b>204</b> can include code and data <b>206</b> that is accessed by the CPU <b>202</b> using a bus <b>212</b>. The memory <b>204</b> can further include an operating system <b>208</b> and application programs <b>210</b>, the application programs <b>210</b> including at least one program that permits the CPU <b>202</b> to perform the methods described herein. For example, the application programs <b>210</b> can include applications <b>1</b> through N, which further include a video coding application that performs the methods described herein. The computing device <b>200</b> can also include a secondary storage <b>214</b>, which can, for example, be a memory card used with a computing device <b>200</b> that is mobile. Because the video communication sessions may contain a significant amount of information, they can be stored in whole or in part in the secondary storage <b>214</b> and loaded into the memory <b>204</b> as needed for processing.</p><p id="p-0061" num="0060">The computing device <b>200</b> can also include one or more output devices, such as a display <b>218</b>. The display <b>218</b> may be, in one example, a touch-sensitive display that combines a display with a touch-sensitive element that is operable to sense touch inputs. The display <b>218</b> can be coupled to the CPU <b>202</b> via the bus <b>212</b>. Other output devices that permit a user to program or otherwise use the computing device <b>200</b> can be provided in addition to or as an alternative to the display <b>218</b>. When the output device is or includes a display, the display can be implemented in various ways, including as a liquid crystal display (LCD); a cathode-ray tube (CRT) display; or a light-emitting diode (LED) display, such as an organic LED (OLED) display.</p><p id="p-0062" num="0061">The computing device <b>200</b> can also include or be in communication with an image-sensing device <b>220</b>, for example, a camera, or any other image-sensing device, now existing or hereafter developed, that can sense an image, such as the image of a user operating the computing device <b>200</b>. The image-sensing device <b>220</b> can be positioned such that it is directed toward the user operating the computing device <b>200</b>. In an example, the position and optical axis of the image-sensing device <b>220</b> can be configured such that the field of vision includes an area that is directly adjacent to the display <b>218</b> and from which the display <b>218</b> is visible.</p><p id="p-0063" num="0062">The computing device <b>200</b> can also include or be in communication with a sound-sensing device <b>222</b>, for example, a microphone, or any other sound-sensing device, now existing or hereafter developed, that can sense sounds near the computing device <b>200</b>. The sound-sensing device <b>222</b> can be positioned such that it is directed toward the user operating the computing device <b>200</b> and can be configured to receive sounds, for example, speech or other utterances, made by the user while the user operates the computing device <b>200</b>.</p><p id="p-0064" num="0063">Although <figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts the CPU <b>202</b> and the memory <b>204</b> of the computing device <b>200</b> as being integrated into a single unit, other configurations can be utilized. The operations of the CPU <b>202</b> can be distributed across multiple machines (each machine having one or more processors) that can be coupled directly or across a local area or other network. The memory <b>204</b> can be distributed across multiple machines, such as a network-based memory or memory in multiple machines performing the operations of the computing device <b>200</b>. Although depicted here as a single bus, the bus <b>212</b> of the computing device <b>200</b> can be composed of multiple buses. Further, the secondary storage <b>214</b> can be directly coupled to the other components of the computing device <b>200</b> or can be accessed via a network and can comprise a single integrated unit, such as a memory card, or multiple units, such as multiple memory cards. The computing device <b>200</b> can thus be implemented in a wide variety of configurations.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram of an example of a video stream <b>300</b> to be encoded and subsequently decoded. The video stream <b>300</b> includes a video sequence <b>302</b>. At the next level, the video sequence <b>302</b> includes a number of adjacent frames <b>304</b>. While three frames are depicted as the adjacent frames <b>304</b>, the video sequence <b>302</b> can include any number of adjacent frames <b>304</b>. The adjacent frames <b>304</b> can then be further subdivided into individual frames, for example, a frame <b>306</b>. At the next level, the frame <b>306</b> can be divided into a series of segments <b>308</b> or planes. The segments <b>308</b> can be subsets of frames that permit parallel processing, for example. The segments <b>308</b> can also be subsets of frames that can separate the video data into separate colors. For example, the frame <b>306</b> of color video data can include a luminance plane and two chrominance planes. The segments <b>308</b> may be sampled at different resolutions.</p><p id="p-0066" num="0065">Whether or not the frame <b>306</b> is divided into the segments <b>308</b>, the frame <b>306</b> may be further subdivided into blocks <b>310</b>, which can contain data corresponding to, for example, 16&#xd7;16 pixels in the frame <b>306</b>. The blocks <b>310</b> can also be arranged to include data from one or more segments <b>308</b> of pixel data. The blocks <b>310</b> can also be of any other suitable size, such as 4&#xd7;4 pixels, 8&#xd7;8 pixels, 16&#xd7;8 pixels, 8&#xd7;16 pixels, 16&#xd7;16 pixels, or larger.</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of an encoder <b>400</b> in accordance with implementations of this disclosure. The encoder <b>400</b> can be implemented, as described above, in the transmitting station <b>102</b>, such as by providing a computer software program stored in memory, for example, the memory <b>204</b>. The computer software program can include machine instructions that, when executed by a processor, such as the CPU <b>202</b>, cause the transmitting station <b>102</b> to encode video data in manners described herein. The encoder <b>400</b> can also be implemented as specialized hardware included in, for example, the transmitting station <b>102</b>. The encoder <b>400</b> has the following stages to perform the various functions in a forward path (shown by the solid connection lines) to produce an encoded or compressed bitstream <b>420</b> using the video stream <b>300</b> as input: an intra/inter-prediction stage <b>402</b>, a transform stage <b>404</b>, a quantization stage <b>406</b>, and an entropy encoding stage <b>408</b>. The encoder <b>400</b> may also include a reconstruction path (shown by the dotted connection lines) to reconstruct a frame for encoding of future blocks. In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the encoder <b>400</b> has the following stages to perform the various functions in the reconstruction path: a dequantization stage <b>410</b>, an inverse transform stage <b>412</b>, a reconstruction stage <b>414</b>, and a loop filtering stage <b>416</b>. Other structural variations of the encoder <b>400</b> can be used to encode the video stream <b>300</b>.</p><p id="p-0068" num="0067">When the video stream <b>300</b> is presented for encoding, the frame <b>306</b> can be processed in units of blocks. At the intra/inter-prediction stage <b>402</b>, a block can be encoded using intra-frame prediction (also called intra-prediction) or inter-frame prediction (also called inter-prediction), or a combination of both. In any case, a prediction block can be formed. In the case of intra-prediction, all or part of a prediction block may be formed from samples in the current frame that have been previously encoded and reconstructed. In the case of inter-prediction, all or part of a prediction block may be formed from samples in one or more previously constructed reference frames determined using motion vectors.</p><p id="p-0069" num="0068">Next, still referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the prediction block can be subtracted from the current block at the intra/inter-prediction stage <b>402</b> to produce a residual block (also called a residual). The transform stage <b>404</b> transforms the residual into transform coefficients in, for example, the frequency domain using block-based transforms. Such block-based transforms (i.e., transform types) include, for example, the Discrete Cosine Transform (DCT) and the Asymmetric Discrete Sine Transform (ADST). Other block-based transforms are possible. Further, combinations of different transforms may be applied to a single residual. In one example of application of a transform, the DCT transforms the residual block into the frequency domain where the transform coefficient values are based on spatial frequency. The lowest frequency (DC) coefficient is at the top-left of the matrix, and the highest frequency coefficient is at the bottom-right of the matrix. It is worth noting that the size of a prediction block, and hence the resulting residual block, may be different from the size of the transform block. For example, the prediction block may be split into smaller blocks to which separate transforms are applied.</p><p id="p-0070" num="0069">The quantization stage <b>406</b> converts the transform coefficients into discrete quantum values, which are referred to as quantized transform coefficients, using a quantizer value or a quantization level. For example, the transform coefficients may be divided by the quantizer value and truncated. The quantized transform coefficients are then entropy encoded by the entropy encoding stage <b>408</b>. Entropy coding may be performed using any number of techniques, including token and binary trees. The entropy-encoded coefficients, together with other information used to decode the block (which may include, for example, the type of prediction used, transform type, motion vectors, and quantizer value), are then output to the compressed bitstream <b>420</b>. The information to decode the block may be entropy coded into block, frame, slice, and/or section headers within the compressed bitstream <b>420</b>. The compressed bitstream <b>420</b> can also be referred to as an encoded video stream or encoded video bitstream; these terms will be used interchangeably herein.</p><p id="p-0071" num="0070">The reconstruction path in <figref idref="DRAWINGS">FIG. <b>4</b></figref> (shown by the dotted connection lines) can be used to ensure that both the encoder <b>400</b> and a decoder <b>500</b> (described below) use the same reference frames and blocks to decode the compressed bitstream <b>420</b>. The reconstruction path performs functions that are similar to functions that take place during the decoding process and that are discussed in more detail below, including dequantizing the quantized transform coefficients at the dequantization stage <b>410</b> and inverse transforming the dequantized transform coefficients at the inverse transform stage <b>412</b> to produce a derivative residual block (also called a derivative residual). At the reconstruction stage <b>414</b>, the prediction block that was predicted at the intra/inter-prediction stage <b>402</b> can be added to the derivative residual to create a reconstructed block. The loop filtering stage <b>416</b> can be applied to the reconstructed block to reduce distortion, such as blocking artifacts.</p><p id="p-0072" num="0071">Other variations of the encoder <b>400</b> can be used to encode the compressed bitstream <b>420</b>. For example, a non-transform-based encoder <b>400</b> can quantize the residual signal directly without the transform stage <b>404</b> for certain blocks or frames. In another implementation, an encoder <b>400</b> can have the quantization stage <b>406</b> and the dequantization stage <b>410</b> combined into a single stage.</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram of a decoder <b>500</b> in accordance with implementations of this disclosure. The decoder <b>500</b> can be implemented in the receiving station <b>106</b>, for example, by providing a computer software program stored in the memory <b>204</b>. The computer software program can include machine instructions that, when executed by a processor, such as the CPU <b>202</b>, cause the receiving station <b>106</b> to decode video data in the manners described below. The decoder <b>500</b> can also be implemented in hardware included in, for example, the transmitting station <b>102</b> or the receiving station <b>106</b>.</p><p id="p-0074" num="0073">The decoder <b>500</b>, similar to the reconstruction path of the encoder <b>400</b> discussed above, includes in one example the following stages to perform various functions to produce an output video stream <b>516</b> from the compressed bitstream <b>420</b>: an entropy decoding stage <b>502</b>, a dequantization stage <b>504</b>, an inverse transform stage <b>506</b>, an intra/inter-prediction stage <b>508</b>, a reconstruction stage <b>510</b>, a loop filtering stage <b>512</b>, and a post filtering stage <b>514</b>. Other structural variations of the decoder <b>500</b> can be used to decode the compressed bitstream <b>420</b>.</p><p id="p-0075" num="0074">When the compressed bitstream <b>420</b> is presented for decoding, the data elements within the compressed bitstream <b>420</b> can be decoded by the entropy decoding stage <b>502</b> to produce a set of quantized transform coefficients. The dequantization stage <b>504</b> dequantizes the quantized transform coefficients (e.g., by multiplying the quantized transform coefficients by the quantizer value), and the inverse transform stage <b>506</b> inverse transforms the dequantized transform coefficients using the selected transform type to produce a derivative residual that can be identical to that created by the inverse transform stage <b>412</b> in the encoder <b>400</b>. Using header information decoded from the compressed bitstream <b>420</b>, the decoder <b>500</b> can use the intra/inter-prediction stage <b>508</b> to create the same prediction block as was created in the encoder <b>400</b>, for example, at the intra/inter-prediction stage <b>402</b>. At the reconstruction stage <b>510</b>, the prediction block can be added to the derivative residual to create a reconstructed block. The loop filtering stage <b>512</b> can be applied to the reconstructed block to reduce blocking artifacts. Other filtering can be applied to the reconstructed block. In an example, the post filtering stage <b>514</b> is applied to the reconstructed block to reduce blocking distortion, and the result is output as an output video stream <b>516</b>. The output video stream <b>516</b> can also be referred to as a decoded video stream; these terms will be used interchangeably herein.</p><p id="p-0076" num="0075">Other variations of the decoder <b>500</b> can be used to decode the compressed bitstream <b>420</b>. For example, the decoder <b>500</b> can produce the output video stream <b>516</b> without the post filtering stage <b>514</b>. In some implementations of the decoder <b>500</b>, the post filtering stage <b>514</b> is applied after the loop filtering stage <b>512</b>. The loop filtering stage <b>512</b> can include an optional deblocking filtering stage. Additionally, or alternatively, the encoder <b>400</b> includes an optional deblocking filtering stage in the loop filtering stage <b>416</b>.</p><p id="p-0077" num="0076">A codec can use multiple transform types. For example, a transform type can be the transform type used by the transform stage <b>404</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> to generate the transform block. For example, the transform type (i.e., an inverse transform type) can be the transform type to be used by the dequantization stage <b>504</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>. Available transform types can include a one-dimensional Discrete Cosine Transform (1D DCT) or its approximation, a one-dimensional Discrete Sine Transform (1D DST) or its approximation, a two-dimensional DCT (2D DCT) or its approximation, a two-dimensional DST (2D DST) or its approximation, and an identity transform. Other transform types can be available. In an example, a one-dimensional transform (1D DCT or 1D DST) can be applied in one dimension (e.g., row or column), and the identity transform can be applied in the other dimension.</p><p id="p-0078" num="0077">In the cases where a 1D transform (e.g., 1D DCT, 1D DST) is used (e.g., 1D DCT is applied to columns (or rows, respectively) of a transform block), the quantized coefficients can be coded by using a row-by-row (i.e., raster) scanning order or a column-by-column scanning order. In the cases where 2D transforms (e.g., 2D DCT) are used, a different scanning order may be used to code the quantized coefficients. As indicated above, different templates can be used to derive contexts for coding the non-zero flags of the non-zero map based on the types of transforms used. As such, in an implementation, the template can be selected based on the transform type used to generate the transform block. As indicated above, examples of a transform type include: 1D DCT applied to rows (or columns) and an identity transform applied to columns (or rows); 1D DST applied to rows (or columns) and an identity transform applied to columns (or rows); 1D DCT applied to rows (or columns) and 1D DST applied to columns (or rows); a 2D DCT; and a 2D DST. Other combinations of transforms can comprise a transform type.</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram of a representation of a portion <b>600</b> of a frame, such as the frame <b>306</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, according to implementations of this disclosure. As shown, the portion <b>600</b> of the frame includes four 64&#xd7;64 blocks <b>610</b>, which may be referred to as superblocks, in two rows and two columns in a matrix or Cartesian plane. A superblock can have a larger or a smaller size. While <figref idref="DRAWINGS">FIG. <b>6</b></figref> is explained with respect to a superblock of size 64&#xd7;64, the description is easily extendable to larger (e.g., 128&#xd7;128) or smaller superblock sizes.</p><p id="p-0080" num="0079">In an example, and without loss of generality, a superblock can be a basic or maximum coding unit (CU). Each superblock can include four 32&#xd7;32 blocks <b>620</b>. Each 32&#xd7;32 block <b>620</b> can include four 16&#xd7;16 blocks <b>630</b>. Each 16&#xd7;16 block <b>630</b> can include four 8&#xd7;8 blocks <b>640</b>. Each 8&#xd7;8 block <b>640</b> can include four 4&#xd7;4 blocks <b>650</b>. Each 4&#xd7;4 block <b>650</b> can include 16 pixels, which can be represented in four rows and four columns in each respective block in the Cartesian plane or matrix. The pixels can include information representing an image captured in the frame, such as luminance information, color information, and location information. In an example, a block, such as a 16&#xd7;16-pixel block as shown, can include a luminance block <b>660</b>, which can include luminance pixels <b>662</b>; and two chrominance blocks <b>670</b>/<b>680</b>, such as a U or Cb chrominance block <b>670</b>, and a V or Cr chrominance block <b>680</b>. The chrominance blocks <b>670</b>/<b>680</b> can include chrominance pixels <b>690</b>. For example, the luminance block <b>660</b> can include 16&#xd7;16 luminance pixels <b>662</b>, and each chrominance block <b>670</b>/<b>680</b> can include 8&#xd7;8 chrominance pixels <b>690</b>, as shown. Although one arrangement of blocks is shown, any arrangement can be used. Although <figref idref="DRAWINGS">FIG. <b>6</b></figref> shows N&#xd7;N blocks, in some implementations, N&#xd7;M, where N&#x2260;M, blocks can be used. For example, 32&#xd7;64 blocks, 64&#xd7;32 blocks, 16&#xd7;32 blocks, 32&#xd7;16 blocks, or any other size blocks can be used. In some implementations, N&#xd7;2N blocks, 2N&#xd7;N blocks, or a combination thereof can be used.</p><p id="p-0081" num="0080">In some implementations, video coding can include ordered block-level coding. Ordered block-level coding can include coding blocks of a frame in an order, such as raster-scan order, wherein blocks can be identified and processed starting with a block in the upper left corner of the frame, or a portion of the frame, and proceeding along rows from left to right and from the top row to the bottom row, identifying each block in turn for processing. For example, the superblock in the top row and left column of a frame can be the first block coded, and the superblock immediately to the right of the first block can be the second block coded. The second row from the top can be the second row coded, such that the superblock in the left column of the second row can be coded after the superblock in the rightmost column of the first row.</p><p id="p-0082" num="0081">In an example, coding a block can include using quad-tree coding, which can include coding smaller block units with a block in raster-scan order. The 64&#xd7;64 superblock shown in the bottom-left corner of the portion of the frame shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, for example, can be coded using quad-tree coding in which the top-left 32&#xd7;32 block can be coded, then the top-right 32&#xd7;32 block can be coded, then the bottom-left 32&#xd7;32 block can be coded, and then the bottom-right 32&#xd7;32 block can be coded. Each 32&#xd7;32 block can be coded using quad-tree coding in which the top-left 16&#xd7;16 block can be coded, then the top-right 16&#xd7;16 block can be coded, then the bottom-left 16&#xd7;16 block can be coded, and then the bottom-right 16&#xd7;16 block can be coded. Each 16&#xd7;16 block can be coded using quad-tree coding in which the top-left 8&#xd7;8 block can be coded, then the top-right 8&#xd7;8 block can be coded, then the bottom-left 8&#xd7;8 block can be coded, and then the bottom-right 8&#xd7;8 block can be coded. Each 8&#xd7;8 block can be coded using quad-tree coding in which the top-left 4&#xd7;4 block can be coded, then the top-right 4&#xd7;4 block can be coded, then the bottom-left 4&#xd7;4 block can be coded, and then the bottom-right 4&#xd7;4 block can be coded. In some implementations, 8&#xd7;8 blocks can be omitted for a 16&#xd7;16 block, and the 16&#xd7;16 block can be coded using quad-tree coding in which the top-left 4&#xd7;4 block can be coded, and then the other 4&#xd7;4 blocks in the 16&#xd7;16 block can be coded in raster-scan order.</p><p id="p-0083" num="0082">In an example, video coding can include compressing the information included in an original, or input, frame by omitting some of the information in the original frame from a corresponding encoded frame. For example, coding can include reducing spectral redundancy, reducing spatial redundancy, reducing temporal redundancy, or a combination thereof.</p><p id="p-0084" num="0083">In an example, reducing spectral redundancy can include using a color model based on a luminance component (Y) and two chrominance components (U and V or Cb and Cr), which can be referred to as the YUV or YCbCr color model or color space. Using the YUV color model can include using a relatively large amount of information to represent the luminance component of a portion of a frame and using a relatively small amount of information to represent each corresponding chrominance component for the portion of the frame. For example, a portion of a frame can be represented by a high-resolution luminance component, which can include a 16&#xd7;16 block of pixels, and by two lower resolution chrominance components, each of which representing the portion of the frame as an 8&#xd7;8 block of pixels. A pixel can indicate a value (e.g., a value in the range from 0 to 255) and can be stored or transmitted using, for example, eight bits. Although this disclosure is described with reference to the YUV color model, any color model can be used.</p><p id="p-0085" num="0084">Reducing spatial redundancy can include transforming a block into the frequency domain as described above. For example, a unit of an encoder, such as the entropy encoding stage <b>408</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, can perform a DCT using transform coefficient values based on spatial frequency.</p><p id="p-0086" num="0085">Reducing temporal redundancy can include using similarities between frames to encode a frame using a relatively small amount of data based on one or more reference frames, which can be previously encoded, decoded, and reconstructed frames of the video stream. For example, a block or a pixel of a current frame can be similar to a spatially corresponding block or pixel of a reference frame. A block or a pixel of a current frame can be similar to a block or a pixel of a reference frame at a different spatial location. As such, reducing temporal redundancy can include generating motion information indicating the spatial difference (e.g., a translation between the location of the block or the pixel in the current frame and the corresponding location of the block or the pixel in the reference frame).</p><p id="p-0087" num="0086">Reducing temporal redundancy can include identifying a block or a pixel in a reference frame, or a portion of the reference frame, that corresponds with a current block or pixel of a current frame. For example, a reference frame, or a portion of a reference frame, which can be stored in memory, can be searched for the best block or pixel to use for encoding a current block or pixel of the current frame. For example, the search may identify the block of the reference frame for which the difference in pixel values between the reference block and the current block is minimized and can be referred to as motion searching. The portion of the reference frame searched can be limited. For example, the portion of the reference frame searched, which can be referred to as the search area, can include a limited number of rows of the reference frame. In an example, identifying the reference block can include calculating a cost function, such as a sum of absolute differences (SAD), between the pixels of the blocks in the search area and the pixels of the current block.</p><p id="p-0088" num="0087">The spatial difference between the location of the reference block in the reference frame and the current block in the current frame can be represented as a motion vector. The difference in pixel values between the reference block and the current block can be referred to as differential data, residual data, or as a residual block. In some implementations, generating motion vectors can be referred to as motion estimation, and a pixel of a current block can be indicated based on location using Cartesian coordinates such as &#x192;<sub>x,y</sub>. Similarly, a pixel of the search area of the reference frame can be indicated based on a location using Cartesian coordinates such as r<sub>x,y</sub>. A motion vector (MV) for the current block can be determined based on, for example, a SAD between the pixels of the current frame and the corresponding pixels of the reference frame.</p><p id="p-0089" num="0088">As mentioned above, a current block can be predicted using intra prediction. An intra prediction mode uses pixels peripheral to the current block being predicted. Pixels peripheral to the current block are pixels outside the current block. Many different intra prediction modes can be available.</p><p id="p-0090" num="0089">Some intra prediction modes use a single value for all pixels within the prediction block generated using at least one of the peripheral pixels. As an example, the VP9 codec includes an intra-prediction mode, referred to as true-motion (TM_PRED) mode in which all values of a prediction block have the value predicted pixel(x,y)=(top neighbor+left neighbor&#x2212;topleft neighbor)&#x192; or all x and y. As another example, a DC intra-prediction mode (DC_PRED) is such that each pixel of the prediction block is set to the value predicted pixel(x,y)=average value of entire top row and left column.</p><p id="p-0091" num="0090">Other intra prediction modes, which may be referred to as directional intra prediction modes, are such that each can have a corresponding prediction angle. Other types of intra prediction modes can also be available.</p><p id="p-0092" num="0091">An intra prediction mode may be selected by the encoder as part of a rate distortion loop. In brief, various intra prediction modes may be tested to determine which type of prediction will have the lowest distortion for a given rate, or number of bits to be transmitted in an encoded video bitstream, including overhead bits included in the bitstream to indicate the type of prediction used.</p><p id="p-0093" num="0092">In an example codec, the following 13 intra prediction modes can be available: DC_PRED, V_PRED, H_PRED, D45_PRED, D135_PRED, D117_PRED, D153_PRED, D207_PRED, D63_PRED, SMOOTH_PRED, SMOOTH_V_PRED, and SMOOTH_H_PRED, and PAETH_PRED. One of the 13 intra prediction modes can be used to predict a luminance block.</p><p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram of examples of intra prediction modes according to implementations of this disclosure. Intra prediction mode <b>710</b> illustrates the V_PRED intra prediction mode, which is referred to generally as a vertical intra prediction mode. In this mode, prediction block pixels in the first column are set to the value of peripheral pixel A; prediction block pixels in the second column are set to the value of pixel B; prediction block pixels in the third column are set to the value of pixel C; and prediction block pixels in the fourth column are set to the value of pixel D.</p><p id="p-0095" num="0094">Intra prediction mode <b>720</b> illustrates the H_PRED intra prediction mode, which is referred to generally as a horizontal intra prediction mode. In this mode, prediction block pixels in the first row are set to the value of peripheral pixel I; prediction block pixels in the second row are set to the value of pixel J; prediction block pixels in the third row are set to the value of pixel K; and prediction block pixels in the fourth row are set to the value of pixel L.</p><p id="p-0096" num="0095">Intra prediction mode <b>730</b> illustrates the D117_PRED intra prediction mode, so-called because the direction of the arrows, along which the peripheral pixels will be propagated to generate the prediction block form a diagonal, is at an angle of about 1170 from the horizontal. That is, in the D117_PRED, the prediction angle is 117&#xb0;. Intra prediction mode <b>740</b> illustrates the D63_PRED intra prediction mode, which corresponds to a prediction angle of 63&#xb0;. Intra prediction mode <b>750</b> illustrates the D153_PRED intra prediction mode, which corresponds to a prediction angle of 153&#xb0;. Intra prediction mode <b>760</b> illustrates the D135_PRED intra prediction mode, which corresponds to a prediction angle of 135&#xb0;.</p><p id="p-0097" num="0096">The prediction modes D45_PRED and D207_PRED (not shown) correspond, respectively, to the prediction angles 45&#xb0; and 207&#xb0;. DC_PRED corresponds to a prediction mode where all prediction block pixels are set to a single value that is a combination of the peripheral pixels A through M.</p><p id="p-0098" num="0097">In the PAETH_PRED intra prediction mode, the prediction value of a pixel is determined as follows: 1) calculate a base value as a combination of some peripheral pixels, and 2) use, as the prediction pixel, the one peripheral pixel of the some peripheral pixels that is closest to the base value. The PAETH_PRED intra prediction mode is illustrated using, as an example, a pixel <b>712</b> (at location x=1, y=2). In an example of a combination of some peripheral pixels, the base value can be calculated as base=B+K&#x2212;M. That is, the base value is equal to: the value of the left peripheral pixel that is in the same row as the pixel to be predicted+the value of the above peripheral pixel that is in the same column as the pixel&#x2212;the value of the pixel in the top-left corner.</p><p id="p-0099" num="0098">In the SMOOTH_V intra prediction mode, the prediction pixels of the bottom-most row of the prediction block are estimated with the value of the last pixel in the left column (i.e., the value of pixel at location L). The remaining pixels of the prediction block are calculated by quadratic interpolation in the vertical direction.</p><p id="p-0100" num="0099">In the SMOOTH_H intra prediction mode, the prediction pixels of the right-most column of the prediction block are estimated with the value of the last pixel in the top row (i.e., the value of pixel at location D). The remaining pixels of the prediction block are calculated by quadratic interpolation in the horizontal direction.</p><p id="p-0101" num="0100">In the SMOOTH_PRED intra prediction mode, the prediction pixels of the bottom-most row of the prediction block are estimated with the value of the last pixel in the left column (i.e., the value of pixel at location L) and the prediction pixels of the right-most column of the prediction block are estimated with the value of the last pixel in the top row (i.e., the value of pixel at location D). The remaining pixels of the prediction block are calculated as scaled weighted sums. For example, the value of a prediction pixel at location (i,j) of the prediction block can be calculated as the scaled weighted sum of the values of pixels L<sub>j</sub>, R, T<sub>i</sub>, and B. The pixel L<sub>j </sub>is a pixel in the left column and on the same row as the prediction pixel. The pixel R is the pixel as provided by SMOOTH_H. The pixel T<sub>i </sub>is a pixel in the above row and on the same column as the prediction pixel. The pixel B is the pixel as provided by SMOOTH_V. The weights can be equivalent to a quadratic interpolation in the horizontal and vertical directions.</p><p id="p-0102" num="0101">The intra prediction mode selected by the encoder can be transmitted to a decoder in the bitstream. The intra prediction mode can be entropy coded (encoded by the encoder and/or decoded by a decoder) using a context model.</p><p id="p-0103" num="0102">Some codecs use the intra prediction modes of the left and above neighbor blocks as the context for coding the intra prediction mode of a current block. Using <figref idref="DRAWINGS">FIG. <b>7</b></figref> as an example, the left neighbor block can be the block containing the pixels I-L, and the above neighbor block can be the block containing the pixels A-D.</p><p id="p-0104" num="0103">A diagram <b>770</b> illustrates the intra-prediction modes available in the VP9 codec. The VP9 coded supports a set of 10 intra-prediction modes for block sizes ranging from 4&#xd7;4 up to 32&#xd7;32. These intra-prediction modes are DC_PRED, TM_PRED, H_PRED, V_PRED, and 6 oblique directional prediction modes: D45_PRED, D63_PRED, D117_PRED, D135_PRED, D153_PRED, D207_PRED, corresponding approximately to angles 45, 63, 117, 135, 153, and 207 degrees (counterclockwise measured against the horizontal axis).</p><p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart of a process <b>800</b> for searching for a best mode to code a block. The process <b>800</b> is an illustrative, high-level process of a mode decision process that determines a best mode using the brute-force approach. For ease of description, the process <b>800</b> is described with respect to selecting an intra-prediction mode for encoding a prediction block. Other examples of best modes that can be determined by processes similar to the process <b>800</b> include determining a transform type and determining a transform size. The process <b>800</b> can be implemented by an encoder, such as the encoder <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, using a brute-force approach to mode decision.</p><p id="p-0106" num="0105">At <b>802</b>, the process <b>800</b> receives an image block. As the process <b>800</b> is described with respect to determining an intra-prediction mode, the image block can be a prediction unit. As described with respect to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, each of the leaf node coding blocks (e.g., a block <b>702</b>-<b>1</b>, <b>702</b>-<b>5</b>, <b>702</b>-<b>6</b>, <b>702</b>-<b>7</b>, <b>702</b>-<b>8</b>, <b>702</b>-<b>3</b>, or <b>702</b>-<b>4</b>) can be partitioned into one or more prediction units. As such, the image block can be one such prediction unit.</p><p id="p-0107" num="0106">At <b>804</b>, the process <b>800</b> determines (e.g., selects, calculates, choses, etc.) a list of modes. The list of modes can include K modes, where K is an integer number. The list of modes can be denoted {m<sub>1</sub>, m<sub>2</sub>, . . . , m<sub>k</sub>}. The encoder can have available a list of intra-prediction modes. For example, the list of available intra-prediction modes can be {DC_PRED, TM_PRED, V_PRED, H_PRED, D45_PRED, D135_PRED, D117_PRED, D153_PRED, D207_PRED, D63_PRED}. In another example, the list of available intra-prediction modes can be {DC_PRED, V_PRED, H_PRED, D45_PRED, D135_PRED, D117_PRED, D153_PRED, D207_PRED, D63_PRED, SMOOTH_PRED, SMOOTH_V_PRED, and SMOOTH_H_PRED, PAETH_PRED}. A description of these intra-prediction modes is omitted as the description is impertinent to the understanding of this disclosure. The list of modes determined at <b>804</b> can be any subset of the list of available intra-prediction modes.</p><p id="p-0108" num="0107">At <b>806</b>, the process <b>800</b> initializes a BEST_COST variable to a high value (e.g., INT_MAX, which may be equal to 2,147,483,647) and initializes a loop variable i to 1, which corresponds to the first mode to be examined.</p><p id="p-0109" num="0108">At <b>808</b>, the process <b>800</b> computes (e.g., calculates) an RD_COST<sub>i </sub>for the mode<sub>i</sub>. At <b>810</b>, the process <b>800</b> tests whether the rate-distortion (RD) cost, RD_COST<sub>i</sub>, of the current mode under examination, mode<sub>i</sub>, is less than the current best cost, BEST_COST. If the test is positive, then at <b>812</b>, the process <b>800</b> updates the best cost to be the cost of the current mode (i.e., BEST_COST=RD_COST<sub>i</sub>) and sets the current best mode index (BEST_MODE) to the loop variable i (BEST_MODE=i). The process <b>800</b> then proceeds to <b>814</b> to increment the loop variable i (i.e., i=i+1) to prepare for examining the next mode (if any). If the test is negative, then the process <b>800</b> proceeds to <b>814</b>.</p><p id="p-0110" num="0109">At <b>816</b>, if there are more modes to examine, the process <b>800</b> proceeds back to <b>808</b>; otherwise the process <b>800</b> proceeds to <b>816</b>. At <b>818</b>, the process <b>800</b> outputs the index of the best mode, BEST_MODE. Outputting the best mode can mean returning the best mode to a caller of the process <b>800</b>. Outputting the best mode can mean encoding the image using the best mode. Outputting the best mode can have other semantics. The process <b>800</b> terminates after outputting the best mode.</p><p id="p-0111" num="0110"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram of an example <b>900</b> of estimating the rate and distortion costs of coding an image block X by using a prediction mode m<sub>i</sub>. The process <b>900</b> can be performed by an encoder, such as the encoder <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The process <b>900</b> includes performing a hypothetical encoding of the image block X using the prediction mode m<sub>i </sub>to determine the RD cost of encoding the block. The process <b>900</b> can be used by the process <b>800</b> at <b>808</b>.</p><p id="p-0112" num="0111">A hypothetical encoding process is a process that carries out the coding steps but does not output bits into a compressed bitstream, such as the compressed bitstream <b>420</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Because the purpose is to estimate a rate (also referred as bit rate), a hypothetical encoding process may be regarded or called a rate estimation process. The hypothetical encoding process computes the number of bits (RATE) required to encode the image block X. The example <b>900</b> also calculates a distortion (DISTORTION) based on a difference between the image block X and a reconstructed version of the image block X.</p><p id="p-0113" num="0112">At <b>904</b>, a prediction, using the mode m<sub>i</sub>, is determined. The prediction can be determined as described with respect to intra/inter-prediction stage <b>402</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. At <b>906</b>, a residual is determined as a difference between the image block <b>902</b> and the prediction. At <b>908</b> and <b>910</b>, the residual is transformed and quantized, such as described, respectively, with respect to the transform stage <b>404</b> and the quantization stage <b>406</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The rate (RATE) is calculated by a rate estimator <b>912</b>, which performs the hypothetical encoding. In an example, the rate estimator <b>912</b> can perform entropy encoding, such as described with respect to the entropy encoding stage <b>408</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0114" num="0113">The quantized residual is dequantized at <b>914</b> (such as described, for example, with respect to the dequantization stage <b>410</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>), inverse transformed at <b>916</b> (such as described, for example, with respect to the inverse transform stage <b>412</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>), and reconstructed at <b>918</b> (such as described, for example, with respect to the reconstruction stage <b>414</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) to generate a reconstructed block. A distortion estimator <b>920</b> calculates the distortion (i.e., the loss in video quality) between the image block X and the reconstructed block. In an example, the distortion can be a mean square error between pixel values of the image block X and the reconstructed block. The distortion can be a sum of absolute differences error between pixel values of the image block X and the reconstructed block. Any other suitable distortion measure can be used.</p><p id="p-0115" num="0114">The rate, RATE, and distortion, DISTORTION, are then combined into a scalar value (i.e., the RD cost) by using the Lagrange multiplier as shown in formula (5)</p><p id="p-0116" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>DISTORTION+&#x3bb;<sub>mode</sub>&#xd7;RATE,&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0117" num="0115">The Lagrange multiplier &#x3bb;<sub>mode </sub>of the formula 5 can be calculated as described above, depending on the encoder performing the operations of the example <b>900</b>.</p><p id="p-0118" num="0116"><figref idref="DRAWINGS">FIGS. <b>8</b> and <b>9</b></figref> illustrate that the traditional (i.e., brute-force) approach to mode decision is largely a serial process that essentially codes an image block X by using candidate modes to determine the mode with the best cost. Techniques have been used to reduce the complexity in mode decision. For example, early termination techniques have been used to terminate the loop of the process <b>800</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref> as soon as certain conditions are met, such as, for example, that the rate distortion cost is lower than a threshold.</p><p id="p-0119" num="0117">Other techniques include selecting, for example based on heuristics, a subset of the available candidate modes or using multi-passes over the candidate modes. As such, in most codec, heuristics are used to pick the top N candidates (i.e., N candidate intra-prediction modes) among all possible modes, and rate-distortion optimization is usually used only on these top N candidates to pick the best candidate. In a typical case, the N candidates can include a most-probable candidate. The heuristics can include metrics such as the simple sum of absolute differences (SAD), sum of absolute transformed differences (SATD), mean square error (MSE), or some other heuristic metric. That is, for example, when encoding a current block and given a candidate intra-prediction mode m<sub>i</sub>, a prediction block is determined and one or more of the heuristic metrics (e.g., SAD, SATD, MSE, etc.) is calculated between the current block and the prediction block.</p><p id="p-0120" num="0118">Such readily available (e.g., calculable) heuristic metrics (i.e., heuristic features) can be included in an ML model for intra-prediction mode selection as input features and be expected to improve the prediction accuracy of the ML model.</p><p id="p-0121" num="0119"><figref idref="DRAWINGS">FIGS. <b>10</b>A and <b>10</b>B</figref> are block diagrams of examples <b>1000</b> and <b>1050</b> of convolutional neural networks (CNNs) for mode decisions.</p><p id="p-0122" num="0120"><figref idref="DRAWINGS">FIG. <b>10</b>A</figref> illustrates a high-level block diagram of an example <b>1000</b> of a typical CNN network, or simply a CNN. As mentioned above, a CNN is an example of a machine-learning model. In a CNN, a feature extraction portion typically includes a set of convolutional operations, which is typically a series of filters that are used to filter an input image based on a filter (typically a square of size k, without loss of generality). For example, and in the context of machine vision, these filters can be used to find features in an input image. The features can include, for example, edges, corners, endpoints, and so on. As the number of stacked convolutional operations increases, later convolutional operations can find higher-level features.</p><p id="p-0123" num="0121">In a CNN, a classification portion is typically a set of fully connected (FC) layers, which may also be referred to as dense operations. The fully connected layers can be thought of as looking at all the input features of an image to generate a high-level classifier. Several stages (e.g., a series) of high-level classifiers eventually generate the desired classification output.</p><p id="p-0124" num="0122">As mentioned, a typical CNN network is composed of several convolutional operations (e.g., the feature-extraction portion) followed by several fully connected layers. The number of operations of each type and their respective sizes is typically determined during the training phase of the machine learning. As a person skilled in the art recognizes, additional layers and/or operations can be included in each portion. For example, combinations of Pooling, MaxPooling, Dropout, Activation, Normalization, BatchNormalization, and other operations can be grouped with convolution operations (i.e., in the features-extraction portion) and/or the fully connected operation (i.e., in the classification portion). The fully connected layers may be referred to as Dense operations. As a person skilled in the art recognizes, a convolution operation can use a SeparableConvolution2D or Convolution2D operation.</p><p id="p-0125" num="0123">As used in this disclosure, a convolution layer can be a group of operations starting with a Convolution2D or SeparableConvolution2D operation followed by zero or more operations (e.g., Pooling, Dropout, Activation, Normalization, BatchNormalization, other operations, or a combination thereof), until another convolutional layer, a Dense operation, or the output of the CNN is reached. Similarly, a Dense layer can be a group of operations or layers starting with a Dense operation (i.e., a fully connected layer) followed by zero or more operations (e.g., Pooling, Dropout, Activation, Normalization, BatchNormalization, other operations, or a combination thereof) until another convolution layer, another Dense layer, or the output of the network is reached. The boundary between feature extraction based on convolutional networks and a feature classification using Dense operations can be marked by a Flatten operation, which flattens the multidimensional matrix from the feature extraction into a vector.</p><p id="p-0126" num="0124">In a typical CNN, each of the convolution layers may consist of a set of filters. While a filter is applied to a subset of the input data at a time, the filter is applied across the full input, such as by sweeping over the input. The operations performed by this layer are typically linear/matrix multiplications. The output of the convolution filter may be further filtered using an activation function. The activation function may be a linear function or non-linear function (e.g., a sigmoid function, an arcTan function, a tan H function, a ReLu function, or the like).</p><p id="p-0127" num="0125">Each of the fully connected operations is a linear operation in which every input is connected to every output by a weight. As such, a fully connected layer with N number of inputs and M outputs can have a total of N&#xd7;M weights. As mentioned above, a Dense operation may be generally followed by a non-linear activation function to generate an output of that layer.</p><p id="p-0128" num="0126">Some CNN network architectures used to perform analysis of frames and superblocks (such as to infer a partition as described herein), may include several feature extraction portions that extract features at different granularities (e.g., at different sub-block sizes of a superblock) and a flattening layer (which may be referred to as a concatenation layer) that receives the output(s) of the last convolution layer of each of the extraction portions. The flattening layer aggregates all the features extracted by the different feature extraction portions into one input set. The output of the flattening layer may be fed into (i.e., used as input to) the fully connected layers of the classification portion. As such, the number of parameters of the entire network may be dominated (e.g., defined, set, etc.) by the number of parameters at the interface between the feature extraction portion (i.e., the convolution layers) and the classification portion (i.e., the fully connected layers). That is, the number of parameters of the network is dominated by the parameters of the flattening layer.</p><p id="p-0129" num="0127">The machine-learning model of such architectures tend to have a large number of parameters and operations. In some situations, the machine-learning model may include millions of parameters. Such large models may not be effectively or efficiently used, if at all, to infer classifications on devices (e.g., mobile devices) that may be constrained (e.g., computationally constrained, energy constrained, and/or memory constrained). That is, some devices may not have sufficient computational capabilities (for example, in terms of speed) or memory storage (e.g., RAM) to handle (e.g., execute) such large models.</p><p id="p-0130" num="0128"><figref idref="DRAWINGS">FIG. <b>10</b>B</figref> illustrates a high-level block diagram of an example <b>1050</b> of a prior art CNN for predicting one of the 35 intra prediction modes in HEVC. In CNNs such as the example <b>1050</b>, convolutional layers are used for extracting features and fully connected layers are used as the classification layers.</p><p id="p-0131" num="0129">In the example <b>1050</b>, a current block (e.g., a current block <b>1054</b>) of a frame (e.g., a frame <b>1052</b>) to be encoded can be fed through one or more convolutional layers (e.g., convolutional layers <b>1056</b> and <b>1058</b>), one or more max pooling layers (e.g., a pooling layer <b>1060</b>), and one or more fully connected layers (e.g., fully connected layers <b>1062</b> and <b>1064</b>).</p><p id="p-0132" num="0130">The capacity of the model (i.e., the example <b>1050</b>) is chosen such that a good trade-off between a low training error and a good generalization can be achieved. In the final layer (i.e., the fully connected layer <b>1064</b>), a classification into 35 classes (i.e. intra prediction modes) is carried out.</p><p id="p-0133" num="0131">In analyzing the performance of architectures such as the example <b>1050</b>, several observations can be made.</p><p id="p-0134" num="0132">A first observation is that such models tend to be large; that is, such models have a large number of parameters and operations (e.g., over 67 million parameters for a 32&#xd7;32 block). The large number of parameters and operations can make such models impractical to operate (e.g., use for inference) on computation- or size-constrained platforms such as mobile platforms.</p><p id="p-0135" num="0133">A second observation is that important features, such as QP and/or codec-generated heuristics, may not exploited (i.e., are not used as inputs to the model). Such features can have significant impact on the decision of the optimal intra-prediction mode.</p><p id="p-0136" num="0134">A third observation is that the output of CNN is used to completely replace the current codec decision. The codec decision is disregarded (e.g., replaced) regardless of the confidence level of the ML in its prediction.</p><p id="p-0137" num="0135">With respect to the first observation, it is desirable that ML models be as small (e.g., ultra light) as possible. With respect to the second observation, the inventors have discovered that inserting non-linear combinations of QP greatly improves the accuracy of the ML model. This is especially the case on deeply scaled down network models (e.g., ultra-light models). With respect to the third observation, a better approach would be, depending on the confidence level of the ML prediction, adaptively combining the ML prediction and existing heuristics-based codec decisions to optimize the mode selection accuracy. In an example, the existing heuristics-based codec decisions can include using ML model inputs that are derived from one or more candidate modes, such as a most-probable intra-prediction mode (MPM).</p><p id="p-0138" num="0136"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart of a process <b>1100</b> for encoding, by an encoder (i.e., a first encoder), an image block using intra-prediction according to implementations of this disclosure. The process <b>1100</b> trains, using input data, a machine-learning model to infer a mode decision, such as an intra-prediction mode decision (i.e., an ML intra-prediction mode). The process <b>1100</b> then uses the trained machine-learning model to infer a mode decision (e.g., an ML intra-prediction mode) for encoding an image block (i.e., a current block). The current block can be a luminance (Y) block, a chrominance (U or V) block, or any other type of image block that is to be encoded using intra-prediction. The image block can be of M&#xd7;N size, where M can be equal to N. That is, the image block can be any type and size of a block that is capable of being encoded using intra-prediction.</p><p id="p-0139" num="0137">At <b>1102</b>, the process <b>1100</b> trains the machine-learning (ML) model. The ML model can be trained using training data <b>1112</b>. Each training datum of the training data <b>1112</b> can include a video block that was encoded by traditional encoding methods (e.g., by a second encoder), such as described with respect to <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>6</b>-<b>9</b></figref>; a ground truth intra-prediction mode decision; pre-calculated features; and zero or more additional inputs. The ground truth intra-prediction mode decision is the intra-prediction mode that the second encoder determined for the block using, for example, a brute-force approach.</p><p id="p-0140" num="0138">In the training phase, parameters of the ML model are generated such that, for at least some of the training data, the ML model can infer, for a training datum, the resulting intra-prediction mode decision (i.e., the ML intra-prediction mode) of the training datum. Desirably, for every input block, the ML intra-prediction mode (i.e., the mode inferred by the ML model) is the same as the ground truth intra-prediction mode decision.</p><p id="p-0141" num="0139">The pre-calculated features are so called (i.e., &#x201c;pre-calculated&#x201d;) to distinguish them from features that may be extracted (e.g., derived, inferred, etc.) by a deep learning neural network (such as a convolutional neural network).</p><p id="p-0142" num="0140">In an example, the machine-learning (ML) model can be a small, all-dense network structure. That is, the ML model only includes fully connected layers and does not include convolutional and/or feature-extraction layers.</p><p id="p-0143" num="0141"><figref idref="DRAWINGS">FIG. <b>12</b>A</figref> is a diagram of an example <b>1200</b> of an all-dense network structure according to implementations of this disclosure. An input layer <b>1202</b> includes N number of pre-calculated features which are input to hidden layers <b>1204</b>. M outputs are output from an output layer <b>1206</b>. In an example, the hidden layers <b>1204</b> includes only one hidden layer with 50 nodes. The M outputs of the output layer <b>1206</b> correspond to the number of available intra-prediction modes in the encoder in/with which the example <b>1200</b> is used. For example, if the example <b>1200</b> is to be used with VP9, which includes 10 available intra-prediction modes, then M=10.</p><p id="p-0144" num="0142">In an example, N pre-calculated features can be 17 pre-calculated features. However, additional or fewer features can be used. In an example, one or more of the pre-calculated features can be scalar values. The pre-calculated features are now described with respect to a current block. During the training phase, the current block is a training block. During the inferencing phase (described below), the current block is a block for which an intra-prediction mode is to be inferred.</p><p id="p-0145" num="0143">The example <b>1200</b>, as described herein, includes roughly 1600 parameters, which is significantly smaller than the CNN models described above.</p><p id="p-0146" num="0144">A first pre-calculated feature can be a non-linear function of a quantization parameter.</p><p id="p-0147" num="0145">As is known, quantization parameters in video codecs can be used to control the tradeoff between rate and distortion. Usually, a larger quantization parameter means higher quantization (such as of transform coefficients) resulting in a lower rate but higher distortion; and a smaller quantization parameter means lower quantization resulting in a higher rate but a lower distortion. The variables QP, q, and Q may be used interchangeably in this disclosure to refer to a quantization parameter.</p><p id="p-0148" num="0146">As the quantization parameter can be used to control the tradeoff between rate and distortion, the quantization parameter can be used to calculate the metrics associated with each combination of parameters. The metric can combine the rate and the distortion values of a combination of encoding parameters. In an example, the QP can be used to derive a multiplier that is used to combine the rate and distortion values into one metric. Some codecs may refer to the multiplier as the Lagrange multiplier (denoted &#x3bb;<sub>mode</sub>); other codecs may use a similar multiplier that is referred as rdmult. Each codec may have a different method of calculating the multiplier. For example, HEVC uses the formula &#x3bb;<sub>mode</sub>=0.85&#xd7;2<sup>(QP-12)/3</sup>. For example, H.263 uses the formula &#x3bb;<sub>mode</sub>=0.85&#xb7;Q<sub>H263</sub><sup>2</sup>. For example, VP9 uses the formula rdmult=88&#xb7;q<sup>2</sup>/24. For example, AV1 uses the formula &#x3bb;<sub>mode</sub>=0.12 Q<sub>AV1</sub><sup>2</sup>/256.</p><p id="p-0149" num="0147">The quantization parameter can be an important feature in determining the best mode (i.e., the encoding intra-prediction mode). As such, in an example, QP should be used as an input feature to the ML model. As can be seen in the above formulas, the multiplier has a non-linear relationship to the quantization parameter. As such, in another example, a non-linear function (e.g., value) of the QP can be used as an input to the ML model. In an example, log(qp) can be used. A non-linear QP can greatly improve the accuracy of the ML model. Furthermore, log(qp), when multiplied by any resulting training weight, can nicely imitate a QP polynomial with high orders (e.g., QP, QP<sup>2</sup>, QP<sup>3</sup>, etc.). In terms of the log function, the ML model can correspondingly learn n*log(qp), where n corresponds to a polynomial exponent. Said another way, the ML model can learn a multinomial expression of the quantization parameter, such as log(QP)+log(QP)<sup>2</sup>+log(QP)<sup>3</sup>. In yet another example, the non-linear function &#x192;(QP)=QP<sup>&#x3b1;</sup>, where is a integer that is not equal to 0 or 1 (i.e., a&#x2260;0 and a&#x2260;1), can be used. In an example, &#x3b1;=2. In the general case, the non-linear function is of a same type as a function used by the second encoder for determining a multiplier used in a rate-distortion calculation, as described above.</p><p id="p-0150" num="0148">Second pre-calculated features can be respective errors between the current block (I(x,y)) and respective prediction blocks. That is, for each available intra-prediction mode (model), a respective prediction block (i.e., I&#x2032;(x,y)) is calculated (e.g., determined, derived, obtained, etc.), and an error (e.g., cost<sub>i</sub>) is calculated between the current block and the respective prediction block. In an example, the cost can be the sum of absolute differences (SAD). As such, for each mode<sub>i</sub>, a cost<sub>i </sub>is calculated as cost<sub>i</sub>=&#x3a3;<sub>for all x and y</sub>|I&#x2032;(x,y)&#x2212;I(x,y)|. Other examples of errors/costs (such as sum of square differences or some other error) can be used. As such, in the case of VP9, where there are 10 available intra-prediction modes, 10 costs can be calculated. These block-based SAD costs can carry significant information regarding what the optimal prediction mode might be.</p><p id="p-0151" num="0149">In an example, the second pre-calculated costs can be costs associated with a subset of the available intra-prediction modes. For example, costs may be calculated for directional intra-prediction modes. As such, with respect to VP9, costs may not be calculated, as pre-calculated features for the TM_PRED and/or the DC_PRED intra-prediction modes.</p><p id="p-0152" num="0150">A third pre-calculated feature can be the mean of the current block. More specifically, the mean of the pixel values of the current block. A fourth pre-calculated feature can be the variance of current block. That is, the variance can be the variance of the pixel values of the current block.</p><p id="p-0153" num="0151">Additional pre-calculated features can be obtained by convolving the current block with one or more filters. For each filter, a convolution block is obtained. An additional pre-calculated feature can be scalar value that is derived from the convolution block. In an example, the scalar value can be a sum-of-absolute values of the values of the convolution block. An example of obtaining a sum-of-absolute values of the values of a convolution block is described with respect to <figref idref="DRAWINGS">FIG. <b>13</b></figref>.</p><p id="p-0154" num="0152"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is an example of obtaining a sum-of-absolute values of a convolution block according to implementations of this disclosure. The example <b>1300</b> includes a region <b>1302</b> of an image block (i.e., a current block). The region <b>1302</b> is shown as a 4&#xd7;4 region for the purposes of this example. However, it is to be understood that convolution filters can be applied to any size block, superblock, region of image, or an image.</p><p id="p-0155" num="0153">A filter <b>1304</b> of size 3&#xd7;3 is used in this example. However, filters can have different sizes. The example <b>1300</b> uses an overlapping convolution operation with a stride of one. However, the stride can be greater than one. For example, non-overlapping convolution operations with a stride that is equal to the filter size can be used. As such, the stride size, in each of the horizontal and vertical directions can be 3.</p><p id="p-0156" num="0154">Convolution block <b>1314</b> is the output of convolving the filter <b>1304</b> with the region <b>1302</b>.</p><p id="p-0157" num="0155">The filter <b>1304</b> is first convolved (e.g., using a matrix multiplication operation) with a sub-region <b>1306</b>. As such, a value <b>1316</b> of the convolution block <b>1314</b> can be calculated as (230&#xd7;0.25+226&#xd7;0.5+10&#xd7;0.25+163&#xd7;0+124&#xd7;0+173&#xd7;0+201&#xd7;(&#x2212;0.25)+104&#xd7;(&#x2212;0.5)+150&#xd7;(&#x2212;0.25))=33.25. The filter <b>1304</b> is then convolved with a sub-region <b>1308</b>. As such, a value <b>1318</b> can be calculated as (226&#xd7;0.25+10&#xd7;0.5+232&#xd7;0.25+124&#xd7;0+173&#xd7;0+110&#xd7;0+104&#xd7;(&#x2212;0.25)+150&#xd7;(&#x2212;0.5)+100&#xd7;(&#x2212;0.25))=&#x2212;6.5. The filter <b>1304</b> is then convolved with a sub-region <b>1310</b>. As such, a value <b>1320</b> can be calculated as (163&#xd7;0.25+124&#xd7;0.5+173&#xd7;0.25+201&#xd7;0+104&#xd7;0+150&#xd7;0+100&#xd7;(&#x2212;0.25)+81&#xd7;(&#x2212;0.5)+91&#xd7;(&#x2212;0.25))=57.75. The filter <b>1304</b> is then convolved with a sub-region <b>1312</b>. As such, a value <b>1322</b> can be calculated as (124&#xd7;0.25+173&#xd7;0.5+110&#xd7;0.25+104&#xd7;0+150&#xd7;0+100&#xd7;0+81&#xd7;(&#x2212;0.25)+91&#xd7;(&#x2212;0.5)+17&#xd7;(&#x2212;0.25))=75.</p><p id="p-0158" num="0156">The sum-of-absolute values is then calculated as |33.25|+|&#x2212;6.5|+|57.75|+|75|=172.5.</p><p id="p-0159" num="0157">Returning to <figref idref="DRAWINGS">FIG. <b>12</b></figref>. The filters (which may also be referred to as kernels) can be selected so as to measure overall edge directions in the current block. In an example, filters can be selected so as to measure overall edge directions along the main directions of the available intra-prediction modes.</p><p id="p-0160" num="0158">The main directions can correspond to the most common modes selected by the brute-force approach. For example, in VP9, the most common modes are those associated with the horizontal direction (i.e., the H_PRED intra-prediction mode), the vertical direction (i.e., the V_PRED intra-prediction mode), the 45-degree direction (i.e., the D45_PRED intra-prediction mode), and the 135-degree direction (i.e., the D135_PRED intra-prediction mode).</p><p id="p-0161" num="0159">In an example, Sobel operators can be used. The Sobel operator performs a 2-D spatial gradient measurement on an image block. Thus, the Sobel operator can emphasize regions of high spatial frequency corresponding to edges with the corresponding directions in the image block.</p><p id="p-0162" num="0160">Any number of filters can be used. In an example, four filters can be used. As such, four convolution blocks can be obtained and four corresponding scalar values. In an example, each scalar value can be the sum of absolute values of all values in the corresponding convolution block. The four filters used for detecting strong edge responses in the horizontal, vertical, 135-degree, and 45-degree directions are given, respectively, by filters <b>1350</b> (which is the same as the filter <b>1304</b>), <b>1352</b>, <b>1354</b>, and <b>1356</b> of <figref idref="DRAWINGS">FIG. <b>13</b></figref>.</p><p id="p-0163" num="0161">To reiterate, for each filter (e.g., Sobel filter), the filter is convolved with a current block (e.g., a N&#xd7;N block) to generate an M&#xd7;M convolution block. The value of M can be different with different value of the stride and convolution mode. Each direction can be calculated as the sum of the absolute value of all the pixels in the convolution image.</p><p id="p-0164" num="0162">The use of pre-calculated features such as those described above (e.g., mean and variance of the pixel values of the current block, filter- (e.g., Sobel-filter-) based directional features, and SAD-based heuristics costs can provide effective description of the current block and contain vital information that can lead to the right choice of an intra-prediction mode for the current block. Additionally, such pre-calculated features can effectively replace the need for large feature extraction layers in conventional models, thus significantly reducing ML model complexity and size.</p><p id="p-0165" num="0163">In another example, instead of using pre-selected kernel weights (e.g., the filters <b>1350</b>-<b>1356</b>), kernel (filter) weights can be learned using a set of convolutional operations. In an example, the kernel weights can be learned during an off-line training using a convolutional network. The learned kernels can be used in place of the filters <b>1350</b>-<b>1356</b> to obtain respective convolutional blocks. In another example, the convolutional operations can be part of the ML model, as shown in <figref idref="DRAWINGS">FIG. <b>12</b>B</figref>.</p><p id="p-0166" num="0164"><figref idref="DRAWINGS">FIG. <b>12</b>B</figref> is a diagram of an example <b>1250</b> of a network structure according to implementations of this disclosure. The example <b>1250</b> is similar the example <b>1200</b> with the exception that the example <b>1250</b> includes a set of convolutional layers <b>1252</b> for generating the filter weights. The filter weights that are output from the convolutional layers <b>1252</b> can be used to obtain respective convolutional blocks. For each of respective convolutional blocks, a sum-of-absolute values is calculated. The sums-of-absolute values of the respective convolutional blocks are then used as part of the inputs of the input layer <b>1202</b>. As such, the values (i.e., weights) of a filter can be determined, by the machine-learning model, during the training phase of the machine-learning model, such as at <b>1102</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>. The input to the set of convolutional layers <b>1252</b> can be the raw pixel values of the image block.</p><p id="p-0167" num="0165">To reiterate, during the training phase (i.e., at <b>1102</b>), the ML model learns (e.g., trains, builds, derives, etc.) a mapping (i.e., a function) that accepts, as input, a block and pre-calculated features and outputs an ML intra-prediction mode.</p><p id="p-0168" num="0166">During the training phase, and so that the learned function can be as useful as possible, it is preferable that the ML model be trained using a large range of input blocks and a large range of possible QP values, such as QP values that are used in representative of real-world applications.</p><p id="p-0169" num="0167">As mentioned above, the ML model that is trained is a small model, which can accommodate (e.g., to be used on/by) resource-constrained devices. With a trade-off between model complexity and prediction accuracy, a much smaller ML model usually means accuracy degradation.</p><p id="p-0170" num="0168">To compensate for the degraded accuracy, some implementations according to this disclosure can merge (e.g., fuse, supplement, etc.) the ML decision (i.e., the ML intra-prediction mode) with the encoder's heuristic-based selection (i.e., a most-probable intra-prediction mode) adaptively according to the prediction quality of the ML model.</p><p id="p-0171" num="0169">In an example, the most-probable intra-prediction mode can be the SAD-based mode decision. The SAD-based decision can be the intra-prediction mode that has the minimum SAD cost, as described above. In another example, the encoder can select a most-probable intra-prediction mode in some other way. For example, the most-probable intra-prediction mode for a current block can be selected based on the intra-prediction modes of one or more of the current block's neighboring blocks (e.g., a left neighboring block and an above neighboring block).</p><p id="p-0172" num="0170">The merging (i.e., fusing) strategy is now described.</p><p id="p-0173" num="0171">Let x denote the most-probable intra-prediction mode. Let m denote the ML intra-prediction mode. That is, m is the intra-prediction mode that is predicted by the ML model. Let y be the ground truth intra-prediction mode decision. That is, y denotes the intra-prediction mode that is selected by the encoder using the brute-force approach for encoding the current block.</p><p id="p-0174" num="0172">During the training phase, two probabilities are calculated according to equations (1) and (2).</p><p id="p-0175" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Px</i>[<i>i</i>]=<i>p</i>(<i>y=i|x=i,m&#x2260;i</i>)&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0176" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Pm</i>[<i>i</i>]=<i>p</i>(<i>y=i|x&#x2260;i,m=i</i>)&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0177" num="0173">In equation (1), the ground truth is mode i, x and m are different predicted modes, and Px[i] is the probability of x being the correct mode. That is, the ML intra-prediction mode (i.e., the intra-prediction mode that is inferred by the ML mode) is not equal to the brute-force selected intra-prediction mode; however, the heuristic (i.e., the most-probable) mode is equal to the brute-force selected intra-prediction mode.</p><p id="p-0178" num="0174">In equation (2), the ground truth is mode i, x and m are different predicted modes, and Pm[i] is the probability of m being the correct mode. That is, the ML intra-prediction mode (i.e., the intra-prediction mode that is inferred by the ML mode) is equal to the brute-force selected intra-prediction mode; however, the heuristic (i.e., the most-probable) mode is not equal to the brute-force selected intra-prediction mode.</p><p id="p-0179" num="0175">Based on the Px[i] and Pm[i] probabilities that are accumulated during the training over all of the test data, a decision matrix can also be generated. If an encoder includes N available intra-prediction modes, then the decision matrix can be an N&#xd7;N matrix. For example, if there are 10 available intra prediction modes (such as in VP9), then the decision matrix can be a 10&#xd7;10 matrix.</p><p id="p-0180" num="0176">In an example, the entries of the decision matrix, A, can be set as shown in equation (3).</p><p id="p-0181" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mrow>       <mi>A</mi>       <mo>[</mo>       <mi>i</mi>       <mo>]</mo>      </mrow>      <mo>[</mo>      <mi>j</mi>      <mo>]</mo>     </mrow>     <mo>=</mo>     <mrow>      <mo>{</mo>      <mtable>       <mtr>        <mtd>         <mrow>          <mrow>           <mn>1</mn>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <mi>if</mi>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <mrow>            <mi>Px</mi>            <mo>[</mo>            <mi>i</mi>            <mo>]</mo>           </mrow>          </mrow>          <mo>&#x3c;</mo>          <mrow>           <mi>pm</mi>           <mo>[</mo>           <mi>j</mi>           <mo>]</mo>          </mrow>         </mrow>        </mtd>       </mtr>       <mtr>        <mtd>         <mrow>          <mn>0</mn>          <mo>&#x2062;</mo>          <mtext>   </mtext>          <mi>otherwise</mi>         </mrow>        </mtd>       </mtr>      </mtable>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0182" num="0177">During inference, and in an example, the decision matrix can be used as follows: If the most-probable intra-prediction mode (e.g., the SAD-based mode) is predicted to be the mode i, and the ML model predicts the mode to be j, the relative reliabilities of the ML model prediction and model-probable mode prediction are evaluated. The most reliable prediction is selected. Again, the relative reliabilities are the values of the decision matrix, which is generated during the training. The power of the adaptive decision fusion strategy described herein can significantly boost the performance of a small model.</p><p id="p-0183" num="0178">In an example, during inference, if the ML-predicted mode (i.e., the ML intra-prediction mode) is chosen (i.e., is selected because it is more reliable than the most-probable mode), and the ML intra-prediction mode is different from the most-probable intra-prediction mode (e.g., the SAD-based mode), the cost associated with the ML intra-prediction mode can be set to a value that is smaller than the cost associated with the most-probable intra-prediction mode. As is known, the mode cost can be used in the encoder for deciding, for example, the optimal partition, prediction mode, and/or transform sizes. In an example, the cost associated with the ML intra-prediction mode can be reduced by a certain percentage (e.g., 10%, 25%, or some other percentage).</p><p id="p-0184" num="0179"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates examples of decision matrices according to implementations of this disclosure. A first decision matrix <b>1402</b> is generated during training of a first ML model that is as described with respect to <figref idref="DRAWINGS">FIG. <b>12</b>A</figref>. The first ML model includes one hidden layer containing 10 nodes. The first ML model size is 300 parameters. A second decision matrix <b>1404</b> is also generated during training of a second ML model that is also as described with respect to <figref idref="DRAWINGS">FIG. <b>12</b>A</figref>. The second ML model includes one hidden layer containing 50 nodes. The second ML model size is 1600 parameters.</p><p id="p-0185" num="0180">The first ML model and the second ML model are trained using the same input data. The first ML model and the second ML model are trained for use with the VP9 decoder. As such, i and j of the first decision matrix <b>1402</b> and the second decision matrix <b>1404</b> have values from 0 to 9 corresponding, respectively to the available intra-prediction modes DC_PRED, V_PRED, H_PRED, D45_PRED, D135_PRED, D117_PRED, D153_PRED, D207_PRED, D63_PRED, and TM_PRED.</p><p id="p-0186" num="0181">In comparing the first decision matrix <b>1402</b> and the second decision matrix <b>1404</b>, it can be observed that for the larger model (i.e., the second ML model), because its prediction accuracy is high, the decision matrix (i.e., the second decision matrix <b>1404</b>) almost always chooses the ML decision instead of the most-probable intra-prediction decision; and less so for the smaller model (i.e., the first ML model). This comparison illustrates the importance of using the fusion strategy for a small model. When the ML model is big enough, such fusion is not as important and may not be necessary because the ML model decision is always preferred to the most-probable mode decision. However, for a smaller ML model, the lower-quality ML decisions can be modified with (e.g., supplemented by, fused with, etc.) most-probable intra-prediction mode decisions.</p><p id="p-0187" num="0182">The ML model can then be used by the process <b>1100</b> during an inference phase. The inference phase includes the operations <b>1104</b> and <b>1106</b>. A separation <b>1110</b> indicates that the training phase and the inference phase can be separated in time. As such, the inferencing phase can be performed by a first encoder and the training data <b>1112</b> can be generated by a second encoder. In an example, the first encoder and the second encoder are the same encoder. That is, the training data <b>1112</b> can be generated by the same encoder that performs the inference phase. In either case, the inference phase uses a machine-learning model that is trained as described with respect to <b>1102</b>. As mentioned above, in some examples, the decision matrix can also be used during the inference phase.</p><p id="p-0188" num="0183">At <b>1104</b>, inputs are presented to the ML module. That is, the inputs are presented to a module that incorporates, includes, executes, implements, and the like the ML model. The ML module can be a hardware-implemented module. The ML module can be stored in a memory as executable instructions, which can be executed by a processor.</p><p id="p-0189" num="0184">The inputs can include the image block and pre-calculated features, as described above with respect to the training phase of the ML model.</p><p id="p-0190" num="0185">At <b>1106</b>, the process <b>1100</b> obtains an encoding intra-prediction mode. The encoding intra-prediction mode is to be used for encoding the image block (i.e., the current block).</p><p id="p-0191" num="0186">In an example, an encoding intra-prediction mode can be obtained from the ML model. That is, the ML intra-prediction mode (i.e., the output of the ML model) can be the encoding intra-prediction mode. In another example, and as described above, the process <b>1100</b> can also use a decision matrix at <b>1104</b>. That is, the process <b>1100</b> selects between a most-probable intra-prediction mode and the ML intra-prediction mode based on the decision matrix.</p><p id="p-0192" num="0187">At <b>1108</b>, the process <b>1100</b> encodes the image block using the encoding intra-prediction mode. That is, for example, and described with respect to the intra/inter-prediction stage <b>402</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a prediction block can be obtained using the encoding intra-prediction mode, a residual block can then be obtained and, consistent with the description of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, ultimately entropy encode, as described with respect to the entropy encoding stage <b>408</b>, the residual block in a compressed bitstream, such as the bitstream <b>420</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0193" num="0188"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flowchart of a process <b>1500</b> for encoding a current block using intra-prediction according to implementations of this disclosure and consistent with the above description. The process <b>1500</b> can be executed (i.e., performed) by an encoder after a determination by the encoder that an intra-prediction mode is to be selected for the current block. The process <b>1500</b> uses a machine learning (ML) model and a decision matrix to select (e.g., obtain, determine, infer, etc.) an intra-prediction mode for encoding the current block. The current block can be a block of an image (e.g., frame) of a video stream. The current block can be a luminance block, such as the luminance block <b>660</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The current block can be a chrominance block, such as the U or Cb chrominance block <b>670</b> or the V or Cr chrominance block <b>680</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The current block can be a superblock, a macroblock, any largest coding unit sized block, or a sub-block thereof. For example, the image can be a 128&#xd7;128 luminance block, a 64&#xd7;64 chrominance block, a 4&#xd7;4 luminance block, or some other sized block. The current block can be any block that can be prediction using intra-prediction by the encoder performing the process <b>1500</b>.</p><p id="p-0194" num="0189">The process <b>1500</b> can be implemented, for example, as a software program that may be executed by computing devices such as the transmitting station <b>102</b>. For example, the software program can include machine-readable instructions that may be stored in a memory such as the memory <b>204</b> or the secondary storage <b>214</b>, and that, when executed by a processor, such as the CPU <b>202</b>, may cause the computing device to perform the process <b>1500</b>. The process <b>1500</b> can be implemented using specialized hardware or firmware. For example, a hardware component can be configured to perform the process <b>1500</b>. As explained above, some computing devices may have multiple memories or processors, and the operations described in the process <b>1500</b> can be distributed using multiple processors, memories, or both. In an example, the process <b>1500</b> can be executed as part of an intra/inter-prediction stage, such as the intra/inter-prediction stage <b>402</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0195" num="0190">At <b>1502</b>, the process <b>1500</b> obtains, using a machine-learning (ML) model, an ML intra-prediction mode. The ML model can be a model that is trained as explained above with respect to <figref idref="DRAWINGS">FIG. <b>11</b></figref>. As such, in an example, the ML model can be trained using input data to output the ML intra-prediction mode for the current block. As also described above, an input datum of the input data can include an input block of video and a corresponding ground-truth intra-prediction mode for encoding the input block of video. The ground-truth intra-prediction mode being the intra-prediction mode selected by an encoder using a brute-force approach to mode prediction.</p><p id="p-0196" num="0191">As explained above, pre-calculated features can be input to the ML model. As such, in an example, the process <b>1500</b> can calculate at least one of a mean and a variance of the pixel values of the current block. The at least one of the mean or the variance can be used as input to the ML model.</p><p id="p-0197" num="0192">In an example, the pre-calculated features can include one or more directional-based features. Each of the directional-based features can be obtained applying a respective kernel to the current block to obtain a convolution block, obtaining a scalar from the convolution block, and using the scalar as an input to the ML model. In an example, the scalar can be the sum-of-absolute values of the convolution block. As also mentioned above, in an example, the filter can be a Sobel filter corresponding to a direction associated with an available intra-prediction mode. The available intra-prediction mode is one of the possible intra-prediction modes that the encoder can use to intra-predict a block. As also, mentioned above, in an example, the filter weights are not fixed. Rather, in an example, the filter weights can be learned during the training phase of the ML model. As such, the convolution block can be obtained by using the current block as an input to one or more convolution layers.</p><p id="p-0198" num="0193">As also mentioned above, in an example, a non-linear function of a quantization parameter can be used as a pre-calculated feature input to the ML model. The quantization parameter (i.e., the value of the quantization parameter) can be selected by the encoder.</p><p id="p-0199" num="0194">As also mentioned above, in an example, pre-calculated costs associated with at least a subset of the available intra-prediction modes can be used as inputs to the ML model. In a typical video encoder, such pre-calculated costs are usually already available in the encoder, therefore such pre-calculated costs can be directly used as the ML input. That is, the process <b>1500</b> can calculate the costs associated with the at least a subset of the available intra-prediction modes and can feed the costs, as inputs, to the ML model. As such, the process <b>1500</b> can calculate, for at some modes of the available intra-prediction modes, respective prediction blocks; calculate respective errors between the respective prediction blocks and the current block; and use the respective errors as inputs to the ML model.</p><p id="p-0200" num="0195">At <b>1504</b>, the process <b>1500</b> obtains a most-probable intra-prediction mode from amongst available intra-prediction modes for encoding the current block. In an example, the most-probable intra-prediction mode can be selected by the encoder based on the intra-prediction modes of neighboring blocks. In an example, the most-probable intra-prediction mode can be the intra-prediction mode corresponding to the smallest cost of the pre-calculated costs described above. That is, the most-probable intra-prediction mode can be the SAD-based intra-prediction mode. As such, whether or not the pre-calculated costs are used as inputs to the ML mode, the process <b>1500</b> can use the pre-calculated costs to select the most-probable intra-prediction mode.</p><p id="p-0201" num="0196">At <b>1506</b>, the process <b>1500</b> selects, as an encoding intra-prediction mode, one of the ML intra-prediction mode or the most-probable intra-prediction mode. The process <b>1500</b> selects the encoding intra-prediction mode based on relative reliabilities of the ML intra-prediction mode and the most-probable intra-prediction mode.</p><p id="p-0202" num="0197">The one of the ML intra-prediction mode or the most-probable intra-prediction mode is more reliable than the other of the one of the ML intra-prediction mode or the most-probable intra-prediction mode in case where the one of the ML intra-prediction mode or the most-probable intra-prediction mode is a better predictor of an optimal intra-prediction mode. The optimal intra-prediction mode is mode that would be selected by encoder that performs a best mode search. In an example, the best model can be as described with respect to <figref idref="DRAWINGS">FIG. <b>8</b></figref> above.</p><p id="p-0203" num="0198">In an example, the process <b>1500</b> uses a decision matrix to select the encoding intra-prediction mode. In an example, and as described above, the decision matrix can be generated during the training phase of the ML model using statistics that compare respective performances of the ML model and an encoder selecting the most-probable intra-prediction modes as compared to ground-truth intra prediction modes. The decision matrix indicates whether the ML intra-prediction mode or the most-probable intra-prediction mode is to be selected as the encoding intra-prediction mode.</p><p id="p-0204" num="0199">In an example, and in the case that the ML intra-prediction mode is selected, and as described above, the process <b>1500</b> can reduce the cost associated with the ML intra-prediction mode. As such, the selecting the ML intra-prediction mode can include calculating a first cost associated with the ML intra-prediction mode and reducing the first cost to a second cost. In an example, calculating the first cost may already have been done as part of the pre-calculated costs.</p><p id="p-0205" num="0200">At <b>1508</b>, the process <b>1500</b> encodes the encoding intra-prediction mode in a compressed bitstream. The compressed bitstream can be the compressed bitstream <b>420</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0206" num="0201">At <b>1510</b>, the process <b>1500</b> encodes the current block using the encoding intra-prediction mode. For example, the process <b>1500</b> can perform one or more of the following: generate a prediction block, generate a prediction block, generate a residual block, transform the residual block to obtain a transform block, quantize and encode the transform block in the compressed bitstream, as described above with respect to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. When decoding the current block, a decoder can use (i.e., decode) the encoding intra-prediction mode, which the decoder can use to reconstruct the current block, as described above with respect to <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0207" num="0202">In an example, an encoder can use the top N (&#x3e;1) prediction modes to estimate and compare their final bit rates. As such, in an example, the ML model described herein can be used to derive (e.g., output, infer, etc.) the N-most-probable modes according to their respective prediction values. These N top modes can then be combined (e.g., fused, as described herein) with the SAD-derived top N modes to obtain a final list of top N modes. One of the N modes can then be selected, based, for example, on their respective costs for encoding a current block.</p><p id="p-0208" num="0203">As such, in an implementation, N ML intra-prediction modes can be obtained using an ML model; N most probable intra-prediction modes (e.g., SAD-based most probable intra-prediction modes) can be obtained from among the available intra-prediction modes; one of the N ML intra-prediction modes or one of the N most-probable intra-prediction modes can be selected as the encoding intra-prediction mode; the encoding intra-prediction mode can be encoded in a compressed bitstream; and the current block can then be encoded using the encoding intra-prediction mode.</p><p id="p-0209" num="0204"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a flowchart of a process <b>1600</b> for encoding a current block using intra-prediction according to implementations of this disclosure and consistent with the above description. The process <b>1600</b> can be executed (i.e., performed) by an encoder subsequent to a determination by the encoder that an intra-prediction mode is to be selected for the current block. The process <b>1600</b> uses a machine learning (ML) model and a decision matrix to select (e.g., obtain, determine, infer, etc.) an intra-prediction mode for encoding the current block. The current block can be a block of an image (e.g., frame) of a video stream. The current block can be a luminance block, such as the luminance block <b>660</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The current block can be a chrominance block, such as the U or Cb chrominance block <b>670</b> or the V or Cr chrominance block <b>680</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The current block can be a superblock, a macroblock, any largest coding unit sized block, or a sub-block thereof. For example, the image can be a 128&#xd7;128 luminance block, a 64&#xd7;64 chrominance block, a 4&#xd7;4 luminance block, or some other sized block. The current block can be any block that is capable of being intra-predicted by the encoder performing the process <b>1600</b>.</p><p id="p-0210" num="0205">The process <b>1600</b> can be implemented, for example, as a software program that may be executed by computing devices such as the transmitting station <b>102</b>. For example, the software program can include machine-readable instructions that may be stored in a memory such as the memory <b>204</b> or the secondary storage <b>214</b>, and that, when executed by a processor, such as the CPU <b>202</b>, may cause the computing device to perform the process <b>1600</b>. The process <b>1600</b> can be implemented using specialized hardware or firmware. For example, a hardware component can be configured to perform the process <b>1600</b>. As explained above, some computing devices may have multiple memories or processors, and the operations described in the process <b>1600</b> can be distributed using multiple processors, memories, or both. In an example, the process <b>1600</b> can be executed as part of an intra/inter-prediction stage, such as the intra/inter-prediction stage <b>402</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0211" num="0206">At <b>1602</b>, the process <b>1600</b> obtains pre-calculated features. Obtaining the pre-calculated features can mean calculating, selecting, determining, or in any way obtaining. The pre-calculated features can include two or more of: a first feature that is a non-linear function of a quantization parameter; second features that are respective errors between the current block and respective prediction blocks, wherein each prediction block corresponds to an available intra-prediction mode; a mean and a variance of the current block; and a sum-of-absolute values of a convolution block that is obtained from the current block. Each of the pre-calculated features can be as described above.</p><p id="p-0212" num="0207">In an example, the sum-of-absolute values of the convolution image are obtained from the current block by applying a Sobel filter to the current block to obtain the convolution block. In an example, the sum-of-absolute values of the convolution image are obtained from the current block by steps including obtaining the convolution block using convolutional operations.</p><p id="p-0213" num="0208">In an example, the pre-calculated features can include all of the first feature, the second features, the mean and the variance, and the sum-of-absolute values of the convolution block.</p><p id="p-0214" num="0209">At <b>1604</b>, the process <b>1600</b> obtains, using a machine-learning (ML) model, an ML intra-prediction mode. The ML model can receive the pre-calculated features as inputs. The ML model can be trained, in a training phase, using input data to output the ML intra-prediction mode for the current block. Each input datum of the input data (i.e., the training data) can include an input block of video and a corresponding ground-truth intra-prediction mode for encoding the input block of video. The input datum can also include a corresponding QP value that is used for encoding the input block of video using the ground-truth intra-prediction mode. As described above, a non-linear function of the QP value can be used in the input datum.</p><p id="p-0215" num="0210">At <b>1606</b>, the process <b>1600</b> selects an encoding intra-prediction mode using at least the ML intra-prediction mode. In an example, the ML intra-prediction mode that is output by the ML model can be the encoding intra-prediction mode, which is used to encode the current block.</p><p id="p-0216" num="0211">In an example, selecting the encoding intra-prediction mode using at least the ML intra-prediction mode can include obtaining a most-probable intra-prediction mode from amongst available intra-prediction modes for encoding the current block; and using a decision matrix to select the encoding intra-prediction mode. The encoding intra-prediction mode can be one of the ML intra-prediction mode or the most-probable intra-prediction mode. The most-probable intra-prediction mode can be as described above. In an example, the decision matrix can be generated during the training phase using statistics that compare respective performances of the ML model and an encoder selecting most-probable intra-prediction modes as compared to ground-truth intra prediction modes. The decision matrix indicates whether the ML intra-prediction mode or the most-probable intra-prediction mode is to be selected as the encoding intra-prediction mode.</p><p id="p-0217" num="0212">At <b>1608</b>, the process <b>1600</b> encodes the encoding intra-prediction mode in a compressed bitstream. The compressed bitstream can be the compressed bitstream <b>420</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. At <b>1610</b>, the process <b>1600</b> encodes the current block using the encoding intra-prediction mode. For example, the process <b>1600</b> can perform one or more of the following: generate a prediction block, generate a prediction block, generate a residual block, transform the residual block to obtain a transform block, quantize and encode the transform block in the compressed bitstream, as described above with respect to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. When decoding the current block, a decoder can use (i.e., decode) the encoding intra-prediction mode, which the decoder can use to reconstruct the current block, as described above with respect to <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0218" num="0213"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is an example <b>1700</b> of a rate-distortion performance comparison. The example <b>1700</b> illustrates the results of an experiment with diversified genres to verify the effectiveness of the implementations according to this disclosure. The rate-distortion performance resulting from using an ML model to infer prediction modes to be used in a VP9 hardware encoder for keyframe-only encoding (denoted as valid_mode <b>1712</b>) is compared to that resulting from using the most-probable intra-prediction mode (denoted as valid_vanil <b>1714</b>). The ML model includes only 1600 parameters. The small model size makes it possible to perform inferring on a power- and/or capacity constrained platform (such as a mobile device).</p><p id="p-0219" num="0214">In the experiment, the peak signal-to-noise ratio (PSNR) is used as the distortion metric. PSNR is used as the distortion metric. That is, in graphs <b>1710</b> and <b>1720</b>, the x-axis indicates the data rate for encoding the sample video sequence measured in kbps, while the y-axis indicates PSNR quality measured in decibels (dB).</p><p id="p-0220" num="0215">The graph <b>1710</b> illustrates that a consistent performance enhancement is achieved. On average, a higher rate-distortion performance can be achieved when using ML predictions to completely replace SAD-based approach (e.g., the most-probable intra-prediction mode). The performance in the Bj&#xf8;ntegaard rate difference (BD-rate) is about 0.9% better than the SAD-based approach.</p><p id="p-0221" num="0216">The graph <b>1720</b> shows the rate-distortion performance when merging ML decisions and SAD-based decisions according to the adaptive fusion strategy described above. When combining the two decisions, on average, the performance in BD-rate (i.e., a curve <b>1724</b>) is about 1.2% better than SAD-based approach (e.g., a curve <b>1712</b>), and 0.3% better than ML-only approach. This shows the effectiveness of adaptively combining ML decisions and SAD-based decisions according to the ML prediction quality.</p><p id="p-0222" num="0217">The ML model used has only 1600 parameters. The small model size makes it possible to perform inferring on a power/capacity-constrained mobile platform.</p><p id="p-0223" num="0218">As described above with respect to <b>1108</b> of <figref idref="DRAWINGS">FIG. <b>11</b>, <b>1508</b></figref> of <figref idref="DRAWINGS">FIG. <b>15</b></figref>, or <b>1608</b> of <figref idref="DRAWINGS">FIG. <b>16</b></figref>, an encoder that uses a machine-learning model, such as the ML model described with respect to <figref idref="DRAWINGS">FIG. <b>12</b>A</figref> or the ML model described with respect to <figref idref="DRAWINGS">FIG. <b>12</b>B</figref>, to infer mode decision parameters for image block, can encode the mode decision parameters (i.e., an encoding intra-prediction mode), in a compressed bitstream, such as the bitstream <b>420</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0224" num="0219">As such, a decoder, such as the decoder <b>500</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, can decode the image block using the mode decisions parameters received in the compressed bitstream.</p><p id="p-0225" num="0220">As such, a process of decoding an image block can include receiving, in a compressed bitstream, such as the compressed bitstream <b>420</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, an encoding intra-prediction mode (e.g., an encoded index or symbol indicative of the encoding intra-prediction mode); and decoding the image block using the encoding intra-prediction mode.</p><p id="p-0226" num="0221">As mentioned above, a ML model that is configured as described above can be used by an encoder, such as the encoder <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, to infer an intra-prediction mode. As such, the intra-prediction mode is not derived by brute force methods as are known in the art. In an example, the ML model can be used by the intra/inter-prediction stage <b>402</b>.</p><p id="p-0227" num="0222">For simplicity of explanation, the processes <b>800</b>, <b>1100</b>, <b>1500</b>, and <b>1600</b> are each depicted and described as a series of blocks, steps, or operations. However, the blocks, steps, or operations in accordance with this disclosure can occur in various orders and/or concurrently. Additionally, other steps or operations not presented and described herein may be used. Furthermore, not all illustrated steps or operations may be required to implement a technique in accordance with the disclosed subject matter.</p><p id="p-0228" num="0223">The aspects of encoding and decoding described above illustrate some encoding and decoding techniques. However, it is to be understood that &#x201c;encoding&#x201d; and &#x201c;decoding,&#x201d; as those terms are used in the claims, could mean compression, decompression, transformation, or any other processing or change of data.</p><p id="p-0229" num="0224">The words &#x201c;example&#x201d; or &#x201c;implementation&#x201d; are used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as &#x201c;example&#x201d; or &#x201c;implementation&#x201d; is not necessarily to be construed as being preferred or advantageous over other aspects or designs. Rather, use of the words &#x201c;example&#x201d; or &#x201c;implementation&#x201d; is intended to present concepts in a concrete fashion. As used in this application, the term &#x201c;or&#x201d; is intended to mean an inclusive &#x201c;or&#x201d; rather than an exclusive &#x201c;or.&#x201d; That is, unless specified otherwise or clearly indicated otherwise by the context, &#x201c;X includes A or B&#x201d; is intended to mean any of the natural inclusive permutations thereof. That is, if X includes A; X includes B; or X includes both A and B, then &#x201c;X includes A or B&#x201d; is satisfied under any of the foregoing instances. In addition, the articles &#x201c;a&#x201d; and &#x201c;an&#x201d; as used in this application and the appended claims should generally be construed to mean &#x201c;one or more&#x201d; unless specified otherwise or clear from the context to be directed to a singular form. Moreover, use of the term &#x201c;an implementation&#x201d; or &#x201c;one implementation&#x201d; throughout is not intended to mean the same embodiment or implementation unless described as such.</p><p id="p-0230" num="0225">Implementations of the transmitting station <b>102</b> and/or the receiving station <b>106</b> (and the algorithms, methods, instructions, etc., stored thereon and/or executed thereby, including by the encoder <b>400</b> and the decoder <b>500</b>) can be realized in hardware, software, or any combination thereof. The hardware can include, for example, computers, intellectual property (IP) cores, application-specific integrated circuits (ASICs), programmable logic arrays, optical processors, programmable logic controllers, microcode, microcontrollers, servers, microprocessors, digital signal processors, or any other suitable circuit. In the claims, the term &#x201c;processor&#x201d; should be understood as encompassing any of the foregoing hardware, either singly or in combination. The terms &#x201c;signal&#x201d; and &#x201c;data&#x201d; are used interchangeably. Further, portions of the transmitting station <b>102</b> and the receiving station <b>106</b> do not necessarily have to be implemented in the same manner.</p><p id="p-0231" num="0226">Further, in one aspect, for example, the transmitting station <b>102</b> or the receiving station <b>106</b> can be implemented using a general-purpose computer or general-purpose processor with a computer program that, when executed, carries out any of the respective methods, algorithms, and/or instructions described herein. In addition, or alternatively, for example, a special-purpose computer/processor, which can contain other hardware for carrying out any of the methods, algorithms, or instructions described herein, can be utilized.</p><p id="p-0232" num="0227">The transmitting station <b>102</b> and the receiving station <b>106</b> can, for example, be implemented on computers in a video conferencing system. Alternatively, the transmitting station <b>102</b> can be implemented on a server, and the receiving station <b>106</b> can be implemented on a device separate from the server, such as a handheld communications device. In this instance, the transmitting station <b>102</b>, using an encoder <b>400</b>, can encode content into an encoded video signal and transmit the encoded video signal to the communications device. In turn, the communications device can then decode the encoded video signal using a decoder <b>500</b>. Alternatively, the communications device can decode content stored locally on the communications device, for example, content that was not transmitted by the transmitting station <b>102</b>. Other transmitting station <b>102</b> and receiving station <b>106</b> implementation schemes are available. For example, the receiving station <b>106</b> can be a generally stationary personal computer rather than a portable communications device, and/or a device including an encoder <b>400</b> may also include a decoder <b>500</b>.</p><p id="p-0233" num="0228">Further, all or a portion of implementations of the present disclosure can take the form of a computer program product accessible from, for example, a tangible computer-usable or computer-readable medium. A computer-usable or computer-readable medium can be any device that can, for example, tangibly contain, store, communicate, or transport the program for use by or in connection with any processor. The medium can be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable mediums are also available.</p><p id="p-0234" num="0229">The above-described embodiments, implementations, and aspects have been described to allow easy understanding of the present disclosure and do not limit the present disclosure. On the contrary, the disclosure is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims, which scope is to be accorded the broadest interpretation as is permitted under the law to encompass all such modifications and equivalent arrangements.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230007284A1-20230105-M00001.NB"><img id="EMI-M00001" he="5.67mm" wi="76.20mm" file="US20230007284A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for encoding a current block of video using intra-prediction, comprising:<claim-text>obtaining, using a machine-learning (ML) model, an ML intra-prediction mode;</claim-text><claim-text>obtaining a most-probable intra-prediction mode from amongst available intra-prediction modes for encoding the current block;</claim-text><claim-text>selecting, as an encoding intra-prediction mode, one of the ML intra-prediction mode or the most-probable intra-prediction mode, wherein the selecting is based on relative reliabilities of the ML intra-prediction mode and the most-probable intra-prediction mode;</claim-text><claim-text>encoding, in a compressed bitstream, the encoding intra-prediction mode; and</claim-text><claim-text>encoding the current block using the encoding intra-prediction mode.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one of the ML intra-prediction mode or the most-probable intra-prediction mode is more reliable than the other of the one of the ML intra-prediction mode or the most-probable intra-prediction mode in case where the one of the ML intra-prediction mode or the most-probable intra-prediction mode is a better predictor of an optimal intra-prediction mode.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, in a training phase, the ML model is trained using input data to output the ML intra-prediction mode for the current block, and an input datum of the input data comprises an input block of video and a corresponding ground-truth intra-prediction mode for encoding the input block of video.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein selecting, as the encoding intra-prediction mode, one of the ML intra-prediction mode or the most-probable intra-prediction mode comprises:<claim-text>using a decision matrix to select the encoding intra-prediction mode, wherein:<claim-text>the decision matrix is generated during the training phase using statistics that compare respective performances of the ML model and an encoder selecting most-probable intra-prediction modes as compared to ground-truth intra prediction modes, and</claim-text><claim-text>the decision matrix indicating whether the ML intra-prediction mode or the most-probable intra-prediction mode is to be selected as the encoding intra-prediction mode.</claim-text></claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining, using the ML model, the ML intra-prediction mode comprises:<claim-text>using at least one of a mean of the current block or a variance of the current block as input to the ML model.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining, using the ML model, the ML intra-prediction mode comprises:<claim-text>applying a kernel to the current block to obtain a convolution block;</claim-text><claim-text>obtaining a scalar from the convolution block; and</claim-text><claim-text>using the scalar as an input to the ML model.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the kernel is a Sobel filter corresponding to a direction associated with an available intra-prediction mode of the available intra-prediction modes, and the scalar is a sum of absolute values of the convolution block resulting from applying the Sobel filter.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the kernel is applied to the current block using one or more convolution layers that are trained during a training phase.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining, using the ML model, the ML intra-prediction mode comprises using a non-linear function of a quantization parameter as an input to the ML model.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining, using the ML model, the ML intra-prediction mode comprises:<claim-text>calculating, for at some modes of the available intra-prediction modes, respective prediction blocks;</claim-text><claim-text>calculating respective errors between the respective prediction blocks and the current block; and</claim-text><claim-text>using the respective errors as inputs to the ML model.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein selecting, as the encoding intra-prediction mode, one of the ML intra-prediction mode or the most-probable intra-prediction mode comprises:<claim-text>calculating a first cost associated with the ML intra-prediction mode;</claim-text><claim-text>selecting the ML intra-prediction mode as the encoding intra-prediction mode; and</claim-text><claim-text>reducing the first cost to a second cost.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A method for encoding a current block of video using intra-prediction, comprising:<claim-text>obtaining pre-calculated features, wherein the pre-calculated features comprising at least two of:<claim-text>a first feature, the first feature being a non-linear function of a quantization parameter;</claim-text><claim-text>second features, the second features being respective errors between the current block and respective prediction blocks, wherein each prediction block corresponds to an available intra-prediction mode;</claim-text><claim-text>a mean and a variance of the current block; and</claim-text><claim-text>a sum-of-absolute values of a convolution block, the convolution block obtained from the current block;</claim-text></claim-text><claim-text>obtaining, using a machine-learning (ML) model, an ML intra-prediction mode, wherein the ML model receives the pre-calculated features as inputs;</claim-text><claim-text>selecting an encoding intra-prediction mode using at least the ML intra-prediction mode;</claim-text><claim-text>encoding, in a compressed bitstream, the encoding intra-prediction mode; and</claim-text><claim-text>encoding the current block using the encoding intra-prediction mode.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the pre-calculated features comprise the first feature, the second features, the mean and the variance, and the sum-of-absolute values of the convolution block.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein selecting the encoding intra-prediction mode using at least the ML intra-prediction mode comprises:<claim-text>obtaining a most-probable intra-prediction mode from amongst available intra-prediction modes for encoding the current block; and</claim-text><claim-text>using a decision matrix to select the encoding intra-prediction mode, wherein the encoding intra-prediction mode is one of the ML intra-prediction mode or the most-probable intra-prediction mode.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein:<claim-text>the decision matrix is generated during a training phase using statistics that compare respective performances of the ML model and an encoder selecting most-probable intra-prediction modes as compared to ground-truth intra prediction modes, and</claim-text><claim-text>the decision matrix indicating whether the ML intra-prediction mode or the most-probable intra-prediction mode is to be selected as the encoding intra-prediction mode.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein, in a training phase, the ML model is trained using input data to output the ML intra-prediction mode for the current block, wherein an input datum of the input data comprises an input block of video and a best intra-prediction mode for encoding the input block of video.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein calculating the sum-of-absolute values of the convolution block obtained from the current block comprises applying a Sobel filter to the current block to obtain the convolution block.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein calculating the sum-of-absolute values of the convolution block obtained from the current block comprises obtaining the convolution block using convolutional operations.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. An apparatus for encoding a current block of video using intra-prediction, wherein the apparatus:<claim-text>obtains, using a machine-learning (ML) model, ML intra-prediction modes;</claim-text><claim-text>obtains most-probable intra-prediction modes from amongst available intra-prediction modes for encoding the current block;</claim-text><claim-text>selects, as an encoding intra-prediction mode, one of the ML intra-prediction modes or one of the most-probable intra-prediction modes;</claim-text><claim-text>encodes, in a compressed bitstream, the encoding intra-prediction mode; and</claim-text><claim-text>encodes the current block using the encoding intra-prediction mode.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The apparatus of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein:<claim-text>the apparatus selects, as the encoding intra-prediction mode, one of the ML intra-prediction modes or one of the most-probable intra-prediction mode using a decision matrix,</claim-text><claim-text>the decision matrix is generated during a training phase of the ML model using statistics that compare respective performances of the ML model and an encoder selecting most-probable intra-prediction modes as compared to ground-truth intra prediction modes, and</claim-text><claim-text>the decision matrix indicates whether an ML intra-prediction mode or a most-probable intra-prediction mode is to be selected as the encoding intra-prediction mode.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. (canceled)</claim-text></claim></claims></us-patent-application>