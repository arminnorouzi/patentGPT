<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230001951A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230001951</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17822890</doc-number><date>20220829</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>60</class><subclass>W</subclass><main-group>60</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>07</class><subclass>C</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>07</class><subclass>C</subclass><main-group>5</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20200201</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>60</main-group><subgroup>007</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0061</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0214</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0276</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>07</class><subclass>C</subclass><main-group>5</main-group><subgroup>008</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>07</class><subclass>C</subclass><main-group>5</main-group><subgroup>0841</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200201</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>2556</main-group><subgroup>45</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>2510</main-group><subgroup>0676</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>2530</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">RESPONDER OVERSIGHT SYSTEM FOR AN AUTONOMOUS VEHICLE</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17150581</doc-number><date>20210115</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11447156</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17822890</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>TuSimple, Inc.</orgname><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Tam</last-name><first-name>Joyce</first-name><address><city>Pleasanton</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A system includes an autonomous vehicle (AV) comprising a sensor, a control subsystem, and an operation server. The control subsystem receives sensor data comprising location coordinates of the AV from the sensor. The operation server detects an unexpected event from the sensor data, comprising at least one of an accident, an inspection, and a report request. The operation server receives a message from a user comprising a request to access particular information regarding the AV and location data. The operation server associates the AV with the user if the location coordinates of the AV match location data of the user. The operation server establishes a communication path between the user and a remote operator for further communications.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="100.75mm" wi="158.75mm" file="US20230001951A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="243.84mm" wi="161.80mm" orientation="landscape" file="US20230001951A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="241.05mm" wi="153.84mm" orientation="landscape" file="US20230001951A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="111.34mm" wi="123.70mm" file="US20230001951A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="241.38mm" wi="150.71mm" orientation="landscape" file="US20230001951A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="249.68mm" wi="120.06mm" file="US20230001951A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="247.82mm" wi="161.97mm" orientation="landscape" file="US20230001951A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="249.85mm" wi="162.56mm" orientation="landscape" file="US20230001951A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="113.88mm" wi="127.85mm" file="US20230001951A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">The application is a continuation of U.S. patent application Ser. No. 17/150,581, filed Jan. 15, 2021, by Joyce Tam, and entitled &#x201c;RESPONDER OVERSIGHT SYSTEM FOR AN AUTONOMOUS VEHICLE,&#x201d; which is incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates generally to autonomous vehicles. More particularly, the present disclosure is related to a responder oversight system for an autonomous vehicle.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">One aim of autonomous vehicle technologies is to provide vehicles that can safely navigate towards a destination. In some cases, an autonomous vehicle may encounter an unexpected situation on its way to a destination. For example, an autonomous vehicle may be involved in an accident, an inspection, or stopped by a law enforcement officer. Current autonomous vehicle technologies may not be configured to account for encountering specific unexpected situations.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">This disclosure recognizes various problems and previously unmet needs related to autonomous vehicle (AV) involvement in an unexpected event. For example, current autonomous vehicle technologies may not be configured to account for situations where an AV encounters an unexpected event. Certain embodiments of this disclosure provide unique technical solutions to technical problems of the autonomous vehicle technologies, including those problems described above by facilitating coordination with a user (e.g., a first responder, law enforcement office, emergency personnel, etc.) who is at the AV to resolve an unexpected event involving the AV. Some examples of unexpected events include but are not limited to when the AV is involved in an accident, an inspection, pulled over to provide a report to a user, e.g., a law enforcement officer (i.e., encountered an unplanned pullover), and encountered an unplanned re-route situation (e.g., encountered a road closure, a road re-route sign, etc.).</p><p id="p-0006" num="0005">In one embodiment, a system comprises an autonomous vehicle (AV), a control subsystem, and an operation server. The AV comprises at least one vehicle sensor located on the AV. The control subsystem is associated with the AV and comprises a first processor. The first processor is configured to receive sensor data from at least one vehicle sensor of the AV, where the sensor data comprises location coordinates of the AV. The first processor communicates the sensor data to the operation server.</p><p id="p-0007" num="0006">The operation server is communicatively coupled with the control subsystem. The operation server comprises a second processor that is operably coupled with a memory. The memory of the operation server is operable to store login credentials of a user to an application by which the user is authorized to access information related to the AV.</p><p id="p-0008" num="0007">The second processor is configured to detect an unexpected event related to the AV from the sensor data. The unexpected event comprises at least one of an accident, an inspection, and a report request related to the AV. The second processor receives a message from an electronic device associated with the user, where the message comprises a request to access particular information regarding the AV and location data of the user. The second processor determines whether the location data of the user matches the location coordinates of the AV. The second processor associates the AV with the user, in response to determining that the location data of the user matches the location coordinates of the AV. The second processor generates a ticket for the unexpected event to record events that will be carried out to address the unexpected event. The second processor establishes a communication path between the user and a remote operator using the electronic device. The second processor communicates the particular information related to the AV to the electronic device via the communication path. The second processor receives a request from the user to provide assistance to address the unexpected event. The second processor provides instructions to the remote operator to forward to the user to address the unexpected event. The second processor determines whether the unexpected event is addressed. The second processor closes the ticket, in response to determining that the unexpected event is addressed.</p><p id="p-0009" num="0008">The disclosed systems provide several practical applications and technical advantages which include: 1) technology that automatically associates a user at the AV with the AV based on determining that location data of the user matches or corresponds to location coordinates of the AV; 2) technology establishes a communication path with the user at the AV using an electronic device of the user, by which the user is enabled to request particular information regarding the AV to resolve the unexpected event; 3) technology that determines and communicates particular information related to the AV, such as sensor data in a particular time duration, to the user to resolve the unexpected event; 4) technology that remotely grants entry to the user to enter the cab of the AV; 5) technology that establishes a second communication path with the user from an in-cab communication module to communicate the particular information to the user; 6) technology that remotely disengages autonomous functions of the AV; and 7) technology utilizes an emergency stop button for local disengagement of autonomous functions of the AV.</p><p id="p-0010" num="0009">As such, the systems described in this disclosure may be integrated into a practical application of determining a more efficient, safe, and reliable solution to address and resolve an unexpected event involving the AV. For example, in a case where the AV is involved in an accident, the disclosed system provides particular information and instructions to be forwarded to the user to provide an emergency assist to resolve the accident. The particular information may include sensor data in a particular time duration in which the accident has occurred. The particular instructions may include instructions for entering a cab of the AV (upon remotely unlocking the entry door of the cab), locally disengaging autonomous functions of the AV, manually operating the AV to pull the AV over to a side of a road. In a particular example, where the AV is involved in an accident with a vehicle, the particular information may include an image/video feed from the sensor data recorded during the accident. As such, by accessing and reviewing the image/video feed, the user may be able to determine a safer way to assist passengers in the other vehicle that are stuck in their vehicle. As such, the disclosed system may provide an additional practical application of improving safety of passengers of the other vehicle that is involved in the accident with the AV.</p><p id="p-0011" num="0010">In another example, in a case where the AV is requested to be inspected, the disclosed system provides particular information to be forwarded to the user to provide an inspection assist to conduct and conclude the inspection. The particular information may include one or more items on an inspection checklist, such as the health status report, physical characteristics status report (e.g., the weight, cargo, tire inflations, system's temperature, etc.), and autonomous faculties status report (e.g., autonomous algorithms/functions, sensor functions, map data, routing plan, etc.) associated with the AV.</p><p id="p-0012" num="0011">In another example, in a case where the AV is requested to provide a report to a user, the disclosed system provides particular information to be forwarded to the user to satisfy the request of the user. The particular information may include one or more of a driving history of the AV in particular time duration, such as steering information, speed information (i.e., accelerometer readings), brake information, and health status report associated with the AV. As such, the disclosed systems may improve the autonomous vehicle technologies by accounting for unexpected situations/events and coordinating with a user at the AV to resolve the unexpected situations involving the AV.</p><p id="p-0013" num="0012">Certain embodiments of this disclosure may include some, all, or none of these advantages. These advantages and other features will be more clearly understood from the following detailed description taken in conjunction with the accompanying drawings and claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0014" num="0013">For a more complete understanding of this disclosure, reference is now made to the following brief description, taken in connection with the accompanying drawings and detailed description, wherein like reference numerals represent like parts.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIGS. <b>1</b>A to <b>1</b>C</figref> illustrate simplified schematic diagrams of example unexpected events involving an AV according to certain embodiments of this disclosure;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example system for coordinating with a user at the AV to resolve an unexpected event involving the AV;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a block diagram of a control subsystem of the AV illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a block diagram of the operation server illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example flowchart of a method for coordinating with a user at the AV to resolve an unexpected event involving the AV;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a block diagram of an example autonomous vehicle configured to implement autonomous driving operations;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example system for providing autonomous driving operations used by the AV of <figref idref="DRAWINGS">FIG. <b>6</b></figref>; and</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a block diagram of an in-vehicle control computer included in the AV of <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0023" num="0022">As described above, previous technologies may fail to provide efficient, reliable, and safe solutions for addressing and resolving an unexpected event involving an AV. This disclosure provides various systems and methods for improving the autonomous vehicle technologies by providing provisions to address and resolve various unexpected events where the AV is involved.</p><heading id="h-0007" level="2">Examples of Unexpected Events</heading><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIGS. <b>1</b>A to <b>1</b>C</figref> illustrate simplified schematic diagrams of example unexpected events <b>100</b> involving an AV <b>602</b>. In one embodiment, the AV <b>602</b> may include a semi-truck tractor unit attached to a trailer to transport cargo or freight from one location to another location (see <figref idref="DRAWINGS">FIG. <b>6</b></figref>). In this disclosure, the semi-truck tractor may be referred to as a cab of the AV <b>602</b>. The AV <b>602</b> is navigated by a plurality of components described in detail in <figref idref="DRAWINGS">FIGS. <b>6</b>-<b>8</b></figref>. The operation of the AV <b>602</b> is described in greater detail below in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The corresponding description below includes brief descriptions of certain components of the AV <b>602</b>.</p><p id="p-0025" num="0024">In brief, the AV <b>602</b> includes a control subsystem <b>300</b> which is operated to facilitate autonomous driving of the AV <b>602</b>. The control subsystem <b>300</b> of the AV <b>602</b> generally includes one or more computing devices in signal communication with other components of the AV <b>602</b> (see <figref idref="DRAWINGS">FIG. <b>6</b></figref>). The control subsystem <b>300</b> is generally configured to control the operation of the AV <b>602</b> and its components. The control subsystem <b>300</b> is further configured to determine a pathway in front of the AV <b>602</b> that is safe to travel and free of objects/obstacles, and navigate the AV <b>602</b> to travel in that pathway. This process is described in more detail in <figref idref="DRAWINGS">FIGS. <b>3</b>, <b>4</b>, and <b>6</b>-<b>8</b></figref>.</p><p id="p-0026" num="0025">In brief, the control subsystem <b>300</b> receives sensor data <b>310</b> from one or more sensors <b>646</b> positioned on the AV <b>602</b> to determine a safe pathway to travel. The sensor data <b>310</b> includes data captured by the sensors <b>646</b>. The sensors <b>646</b> are configured to capture any object within their detection zones or fields of view, such as landmarks, lane markings, lane boundaries, road boundaries, vehicles, pedestrians, road/traffic signs, among other objects. Sensors <b>646</b> may include cameras, LiDAR sensors, motion sensors, infrared sensors, and the like. In one embodiment, the sensors <b>646</b> may be positioned around the AV <b>602</b> to capture the environment surrounding the AV <b>602</b>. In some cases, the sensor data <b>310</b> may include one or more indications indicating an unexpected event <b>100</b>. In some examples, the unexpected events <b>100</b> involving the AV <b>602</b> may generally include at least one of an accident <b>110</b> (described in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>), an inspection <b>120</b> (described in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>), and a report request <b>130</b> (described in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>).</p><heading id="h-0008" level="2">Example Accident</heading><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> illustrates a simplified schematic diagram of an unexpected event <b>100</b>, where the unexpected event <b>100</b> comprises an accident <b>110</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, the AV <b>602</b> is stalled due to an accident on a road <b>112</b>. For example, assume that while the AV <b>602</b> was driving on the road <b>112</b>, sensors <b>646</b> of the AV <b>602</b> capture the presence of an object <b>116</b> on the path of the AV <b>602</b> whose distance to the AV <b>602</b> was decreasing. As such, sensor data <b>310</b> captured by the sensors <b>646</b> may indicate a decreasing distance of the object <b>116</b> to the AV <b>602</b>. Also, assume that despite the AV <b>602</b> attempting to maneuver to avoid the object <b>116</b>, there is an impact with the object <b>116</b>. As such, the sensor data <b>310</b> may further indicate the impact or collision with the object <b>116</b>. Once the sensors <b>646</b> detect the impact, the AV <b>602</b> may stop in place, i.e., brakes <b>648</b><i>b </i>are applied (see <figref idref="DRAWINGS">FIG. <b>6</b></figref>). As such, the sensor data <b>310</b> related to the accident <b>110</b> may also indicate an unexpected stoppage of the AV <b>602</b>. In response to detecting one or more indications that the AV <b>602</b> was involved in the accident <b>110</b>, the autonomous faculties of the AV <b>602</b> may be stopped and the AV <b>602</b> stalled.</p><p id="p-0028" num="0027">In such cases, a first user <b>140</b><i>a </i>may arrive at the scene of the accident <b>110</b> to provide an emergency assist <b>114</b> to address (and perhaps resolve) the accident <b>110</b>. The first user <b>140</b><i>a </i>may be a first responder at the scene of the accident <b>110</b>, such as a law enforcement officer, an emergency personnel, etc. The first user <b>140</b><i>a </i>may initiate a communication with an operation server <b>400</b> (see <figref idref="DRAWINGS">FIG. <b>2</b></figref>) using their electronic device <b>150</b> to request particular information regarding the AV <b>602</b> so that they can provide the emergency assist <b>114</b> upon being authenticated. This scenario is explained in more detail in conjunction with <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><heading id="h-0009" level="2">Example Inspection</heading><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> illustrates a simplified schematic diagram of an unexpected event <b>100</b>, where the unexpected event <b>100</b> comprises an inspection <b>120</b>. In one embodiment, inspection <b>120</b> may include a spot inspection or a roadside inspection. This scenario is explained further below in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>. In one embodiment, inspection <b>120</b> may be carried out at an inspection station <b>122</b>. In one embodiment, the inspection station <b>122</b> is a weigh station. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, the AV <b>602</b> is pulled into an inspection station <b>122</b> to be inspected.</p><p id="p-0030" num="0029">Generally, the inspection station <b>122</b> is a place where vehicles including the AV <b>602</b> may be asked to pull into to be inspected by a second user <b>140</b><i>b</i>. For example, assume that while the AV <b>602</b> was driving on a road, sensors <b>646</b> detect one or more indications indicating that the AV <b>602</b> is asked to pull into the inspection station <b>122</b>. For example, the sensors <b>646</b> may capture the presence of an inspection road sign ahead of the AV <b>602</b>. As such, the sensor data <b>310</b> may include one or more of an image feed, video feed, LiDAR feed, etc. indicating the inspection road sign.</p><p id="p-0031" num="0030">In another example, the sensor data <b>310</b> related to inspection <b>120</b> may include a signal that is received by an application (and/or a transponder) that is installed in the AV <b>602</b> indicating to pull into the inspection station <b>122</b>. The application (and/or transponder) is in signal communication with the systems of the AV <b>602</b> and the inspection station <b>122</b>, and can receive signals indicating that the AV <b>602</b> is requested to pull into the inspection station <b>122</b> or that an inspection of the AV <b>602</b> is not requested and the AV <b>602</b> may bypass the inspection <b>120</b> at the inspection station <b>122</b>. When the sensors <b>646</b> receives one or more indications to pull into the inspection station <b>122</b>, the AV <b>602</b> may follow those indications and pull into the inspection station <b>122</b>. In such cases, the second user <b>140</b><i>b </i>may provide an inspection assist <b>124</b> to address (and resolve) the inspection <b>120</b>.</p><p id="p-0032" num="0031">For initiating the inspection <b>120</b>, the second user <b>140</b><i>b </i>may initiate a communication with the operation server <b>400</b> (see <figref idref="DRAWINGS">FIG. <b>2</b></figref>) from their electronic device <b>150</b> and provide the inspection assist <b>124</b> upon being authenticated. During the inspection <b>120</b>, the second user <b>140</b><i>b </i>may request particular information regarding the AV <b>602</b>. In a particular example, the particular information may be according to road safety regulation rules. This scenario is explained in detail in conjunction with <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0033" num="0032">In one embodiment, the inspection <b>120</b> may include a spot inspection or a roadside inspection. For example, assume that while the AV <b>602</b> is driving on a road, the AV <b>602</b> reaches a spot/roadside inspection point. As such, the sensors <b>646</b> detect one or more indications that the AV <b>602</b> is reaching the spot inspection point, for example, by detecting the presence of an inspection road sign, a user <b>140</b><i>b </i>(ahead of the AV <b>602</b>) holding an inspection stop sign, etc. As such, the control subsystem <b>300</b> pulls the AV <b>602</b> over at the inspection stop point. Indications that the AV <b>602</b> is requested to pull into the inspection station <b>122</b> may include a fixed road sign, a lighted marquee, a changeable sign, a radio signal, an audio signal, and the like.</p><heading id="h-0010" level="2">Example Report Request</heading><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>1</b>C</figref> illustrates a simplified schematic diagram of an unexpected event <b>100</b>, where the unexpected event <b>100</b> comprises a report request <b>130</b> from a third user <b>140</b><i>c </i>(e.g., a law enforcement officer). For example, assume that the AV <b>602</b> was driving on a road <b>132</b>, and the third user <b>140</b><i>c </i>flagged the AV <b>602</b> to provide a status report <b>134</b> of one or more aspects of the AV <b>602</b>. In some examples, the status report <b>134</b> may be related to a driving history of the AV <b>602</b>, health diagnostics of the AV <b>602</b>, among others.</p><p id="p-0035" num="0034">In a first example, assume that the status report <b>134</b> is related to the driving history of the AV <b>602</b>. In this particular example, assume that while the AV <b>602</b> was driving on the road <b>132</b>, the third user <b>140</b><i>c </i>may have assumed that the AV <b>602</b> is speeding over a speed limit of the road <b>132</b> and flagged the AV <b>602</b> to pull over. The third user <b>140</b><i>c </i>may flag the AV <b>602</b> to pull over by one or more methods including turning on the sirens of their vehicle, instructing to pull over from a speaker, turning on the beacons and flashing lights of their vehicle, etc. The sensors <b>646</b> can detect these indications and generate sensor data <b>310</b> corresponding to those indications.</p><p id="p-0036" num="0035">For example, in such cases, the sensor data <b>310</b> may include one or more indications that the AV <b>602</b> is being flagged by the third user <b>140</b><i>c</i>, such as audio feeds of the sirens, audio feeds of the instructions to pull over from the speaker, image feeds of the beacons and/or flashing lights, etc. In response to detecting one or more of these indications, the AV <b>602</b> is pulled over by the control subsystem <b>300</b>. Once the AV <b>602</b> is pulled over, the third user <b>140</b><i>c </i>may initiate a communication with the operation server <b>400</b> from their electronic device <b>150</b> and request for the driving history of the AV <b>602</b>.</p><p id="p-0037" num="0036">In a second example, assume that the report request <b>130</b> is related to the health diagnostics of the AV <b>602</b>. In this particular example, assume that while the AV <b>602</b> was driving on the road <b>132</b>, the third user <b>140</b><i>c </i>may have observed a malfunction of a component of the AV <b>602</b>, such as smoke coming out of a brake of the AV <b>602</b>, etc. The third user <b>140</b><i>c </i>may initiate a communication with the operation server <b>400</b> from their electronic device <b>150</b> and request to receive the health diagnostics report of the AV <b>602</b>. This scenario is explained in detail further below in conjunction with <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0038" num="0037">Each of the users <b>140</b><i>a</i>-<i>c </i>may generally be any person who is pre-registered with the operation server <b>400</b> (see <figref idref="DRAWINGS">FIG. <b>4</b></figref>) to obtain credentials to access particular information regarding the AV <b>602</b>. For example, users <b>140</b><i>a</i>-<i>c </i>may include law enforcement officers, emergency personnel, mechanics, technicians, roadside assistance personnel, tow-truck operators, or other appropriate individuals who are pre-registered with the operation server <b>400</b>.</p><p id="p-0039" num="0038">Electronic device <b>150</b> may be any computing device that is configured to receive data from and transmit data to other computing devices, such as the operation server <b>400</b>, the control subsystem <b>300</b>, etc. (see <figref idref="DRAWINGS">FIG. <b>2</b></figref>). The electronic device <b>150</b> is capable of communicating with users <b>140</b>, for example, via user interfaces. Examples of the electronic device <b>150</b> include but are not limited to a mobile phone, a laptop, a tablet, etc. The electronic device <b>150</b> is associated with the user <b>140</b>, meaning that the user <b>140</b> uses the electronic device <b>150</b> to communicate with other devices, such as the operation server <b>400</b> as described above.</p><p id="p-0040" num="0000">Example System for Coordinating with a User at the AV to Resolve an Unexpected Event Involving the AV</p><p id="p-0041" num="0039"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an exemplary embodiment of a system <b>200</b> for coordinating with a user <b>140</b> at the AV <b>602</b> to resolve an unexpected event <b>100</b> involving the AV <b>602</b>. System <b>200</b> comprises the AV <b>602</b> and its components (such as the control subsystem <b>300</b>) and the operation server <b>400</b>. It further includes the electronic device <b>150</b>, a remote operator <b>216</b>, and a network <b>232</b> that provides a communication path for all of the illustrated components of system <b>200</b> to communicate with each other. The system <b>200</b> may be configured as shown or any other suitable configurations.</p><p id="p-0042" num="0040">In general, the system <b>200</b> detects an unexpected event <b>100</b> involving the AV <b>602</b> at the operation server <b>400</b>, receives a request from a user <b>140</b> at the AV <b>602</b> to address the unexpected event <b>100</b>, and provides particular information and/or instructions to be forwarded to the user <b>140</b> to address (and perhaps resolve) the unexpected event <b>100</b>.</p><p id="p-0043" num="0041">As described above in <figref idref="DRAWINGS">FIGS. <b>1</b>A to <b>1</b>C</figref>, the control subsystem <b>300</b> is configured to navigate the AV <b>602</b> to travel in a safe pathway that is free of objects/obstacles. To this end, the control subsystem <b>300</b> receives sensor data <b>310</b> captured by sensors <b>646</b> of the AV <b>602</b>. The control subsystem <b>300</b> is in signal communication with the operation server <b>400</b>. The control subsystem <b>300</b> is configured to communicate the sensor data <b>310</b> to the operation server <b>400</b>, for example, via network <b>232</b>. The control subsystem <b>300</b> may communicate the sensor data <b>310</b> to the operation server <b>400</b> periodically (e.g., every minute, every few minutes, or any other suitable interval), continuously, and/or upon receiving a request from the operation server <b>400</b> to send sensor data <b>310</b>. See the corresponding description of <figref idref="DRAWINGS">FIG. <b>3</b></figref> for further description of the control sub system <b>300</b>.</p><p id="p-0044" num="0042">The sensor data <b>310</b> may include location coordinates <b>202</b> of the AV <b>602</b>. The sensor data <b>310</b> may further include data describing the environment surrounding the AV <b>602</b>, such as image feed, video feed, LiDAR data feed, and other data captured from the fields of view of the sensors <b>646</b>.</p><p id="p-0045" num="0043">Operation server <b>400</b> is generally configured to oversee the operations of the AV <b>602</b>. The operation server <b>400</b> is in signal communication with the AV <b>602</b> and its components. See the corresponding description of <figref idref="DRAWINGS">FIG. <b>4</b></figref> for a further description of the operation server <b>400</b>. In brief, the operation server <b>400</b> comprises a processor <b>402</b> that is operably coupled with a memory <b>404</b>. The processor <b>402</b> may include one or more processing units that perform various functions as described herein. The memory <b>404</b> stores any data and/or instructions used by the processor <b>402</b> to perform its functions. For example, the memory <b>404</b> stores software instructions <b>410</b> that when executed by the processor <b>402</b> causes the operation server <b>400</b> to perform one or more functions described herein. The operation server <b>400</b> may include one or more computing devices, such as servers that serve to perform functions of the operation server <b>400</b>. In one embodiment, the operation server <b>400</b> may be implemented in a cluster of servers where distributed computing or cloud computing is applied. The memory <b>404</b> may include one or more of a local database, cloud database, Network-attached storage (NAS), etc.</p><p id="p-0046" num="0044">The operation server <b>400</b> is configured to receive the sensor data <b>310</b> from the control subsystem <b>300</b>, analyze the sensor data <b>310</b> and, in response, detect any of the unexpected events <b>100</b>, such as accident <b>110</b>, inspection <b>120</b>, and report request <b>130</b> described in <figref idref="DRAWINGS">FIGS. <b>1</b>A to <b>1</b>C</figref>.</p><p id="p-0047" num="0045">In one embodiment, the operation server <b>400</b> may analyze the sensor data <b>310</b> by implementing object detection machine learning modules <b>420</b>. See <figref idref="DRAWINGS">FIG. <b>4</b></figref> for a further description of the object detection machine learning modules <b>420</b>. In brief, the object detection machine learning modules <b>420</b> may be implemented using neural networks and/or machine learning algorithms for detecting objects from images, videos, infrared images, point clouds, radar data, etc.</p><p id="p-0048" num="0000">Initiating Communication with the Operation Server</p><p id="p-0049" num="0046">In one embodiment, assume that the AV <b>602</b> has been involved in an unexpected event <b>100</b> and the user <b>140</b> has arrived at the scene. Also, assume that the user <b>140</b> wishes to assist the AV <b>602</b> to address the unexpected event <b>100</b>. In order for the user <b>140</b> to address the unexpected event <b>100</b>, the user <b>140</b> may require particular information regarding the AV <b>602</b>. To obtain the particular information, the user <b>140</b> may initiate a communication with the operation server <b>400</b> from their electronic device <b>150</b>. In one embodiment, the user <b>140</b> may call a phone number previously provided to the user <b>140</b> to initiate the communication with the operation server <b>400</b>. In a particular embodiment, the user <b>140</b> may use an application <b>204</b> that is installed on the electronic device <b>150</b> to initiate the communication with the operation server <b>400</b>.</p><p id="p-0050" num="0047">Application <b>204</b> is a software/mobile/web application that is generally configured to establish a communication with the operation server <b>400</b>, and receive data from and transfer data to the operation server <b>400</b>. The application <b>204</b> may include user interfaces to interact with the user <b>140</b>. The user <b>140</b> may login to their account on the application <b>204</b> using their login credentials <b>206</b>.</p><p id="p-0051" num="0048">As discussed in <figref idref="DRAWINGS">FIG. <b>1</b>A-<b>1</b>C</figref>, the user <b>140</b> is an individual who is pre-registered with the operation server <b>400</b> in order to be authorized to access particular information regarding the AV <b>602</b>. For example, the user <b>140</b> may previously have gone through a registration process with the operation server <b>400</b> and received login credentials <b>206</b> to access particular information regarding the AV <b>602</b> from the application <b>204</b>. As such, the operation server <b>400</b> may keep a record of the login credentials <b>206</b> associated with the user <b>140</b> (in memory <b>404</b>) in order to authenticate the user <b>140</b> in their logins to the application <b>204</b>.</p><p id="p-0052" num="0049">In one embodiment, the application <b>204</b> may be associated with the operation server <b>400</b>. In another embodiment, the application <b>204</b> may be a third-party application <b>204</b> that is associated with a third-party organization. In a particular example, the third-party application <b>204</b> may be associated and in signal communication with the inspection stations <b>122</b> (see <figref idref="DRAWINGS">FIG. <b>1</b></figref>) as well as the operation server <b>400</b>.</p><p id="p-0053" num="0050">When the user <b>140</b> logs into the application <b>204</b> (using their login credentials <b>206</b>) and initiates the communication with the operation server <b>400</b>, the operation server <b>400</b> authenticates the user <b>140</b>, for example, by comparing the login credentials <b>206</b> associated with the user <b>140</b> with login credentials (of a plurality of users <b>140</b>) that are stored in the memory <b>404</b>. If a match is found, the user <b>140</b> is authenticated and the communication with the operation server <b>400</b> is established.</p><p id="p-0054" num="0051">When the communication with the operation server <b>400</b> is established, the user <b>140</b> may send a message <b>208</b> to the operation server <b>400</b> from the application <b>204</b>. For example, the message <b>208</b> may include a request <b>210</b> to access particular information <b>220</b> regarding the AV <b>602</b> and location data <b>212</b> of the user <b>140</b>. The location data <b>212</b> of the user <b>140</b> may correspond to Global Positioning System (GPS) coordinates of the electronic device <b>150</b>.</p><p id="p-0055" num="0052">Upon receiving the message <b>208</b>, the operation server <b>400</b> may determine whether the location coordinates <b>202</b> of the AV <b>602</b> match the location data <b>212</b> of the user <b>140</b>. In response to determining that the location coordinates <b>202</b> of the AV <b>602</b> match the location data <b>212</b> of the user <b>140</b>, the operation server <b>400</b> associates the AV <b>602</b> to the user <b>140</b>. As such, the operation server <b>400</b> determines that the user <b>140</b> is at the scene of the unexpected event <b>100</b> involving the AV <b>602</b> by determining that the location coordinates <b>202</b> correspond to or match and location data <b>212</b>.</p><p id="p-0056" num="0053">The operation server <b>400</b> generates a ticket <b>214</b> for the unexpected event <b>100</b> to record events that will be carried out to address (and perhaps resolve) the unexpected event <b>100</b>.</p><p id="p-0057" num="0054">The operation server <b>400</b> establishes a communication path <b>218</b> between the user <b>140</b> and a remote operator <b>216</b>. The remote operator <b>216</b> may be an individual who is associated with and has access to the operation server <b>400</b>. For example, the remote operator <b>216</b> may be an administrator that can access and view the information regarding the AV <b>602</b>, such as sensor data <b>310</b> and other information that is available on the memory <b>404</b>. In one example, the remote operator <b>216</b> may access the operation server <b>400</b> from an application server <b>430</b> that is acting as a presentation layer via the network <b>232</b>. See <figref idref="DRAWINGS">FIG. <b>4</b></figref> for a further description of the application server <b>430</b>. In another example, the remote operator <b>216</b> may directly access the operation server <b>400</b> via user interfaces associated with the operation server <b>400</b>.</p><p id="p-0058" num="0055">In one embodiment, the communication path <b>218</b> may follow a two-way communication protocol, where data can be transmitted and received from both sides. The communication path <b>218</b> is configured to support voice-based communication, message-based communication, and/or any other appropriate communication. Using voice-based communication, the user <b>140</b> and the remote operator <b>216</b> can converse with one another. Using message-based communication, each of the user <b>140</b> and the remote operator <b>216</b> can send and receive messages, such as text, images, videos, or any other type of data.</p><p id="p-0059" num="0056">As discussed above, the particular information <b>220</b> comprises information that the operation server <b>400</b> communicates to the user <b>140</b> so that the user <b>140</b> is able to assist the AV <b>602</b> and address the unexpected event <b>100</b>. The corresponding description below describes various particular information <b>220</b> for each unexpected event <b>100</b>. In one embodiment, the operation server <b>400</b> may communicate the particular information <b>220</b> to the user <b>140</b>. In one embodiment, the operation server <b>400</b> may provide the particular information <b>220</b> to the remote operator <b>216</b> to forward to the electronic device <b>150</b> (to the user <b>140</b>). The remote operator <b>216</b> may confirm (or update) the particular information <b>220</b> before communicating the particular information <b>220</b> to the electronic device <b>150</b>. The user <b>140</b> can view the particular information <b>220</b> (e.g., on the application <b>204</b>), and send a request to provide assistance <b>222</b> to address (and perhaps resolve) the unexpected event <b>100</b>. Similar to the particular information <b>220</b>, assistance <b>222</b> may also be different for each case of an unexpected event <b>100</b>. For example, the assistance <b>222</b> may include an emergency assist <b>114</b> (for a case of accident <b>110</b> described in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>), an inspection assist <b>124</b> (for a case of inspection <b>120</b> described in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>), and reporting assist/report <b>134</b> (for a case of report request <b>130</b> described in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>). These scenarios are described below.</p><heading id="h-0011" level="2">Example Particular Information to Address an Accident</heading><p id="p-0060" num="0057">Referring back to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, in a case where the unexpected event <b>100</b> comprises the accident <b>110</b>, the particular information <b>220</b> may include data related to the accident <b>110</b>, such as sensor data <b>310</b> captured by the sensors <b>646</b> in a particular time range in which the accident <b>110</b> has occurred. The particular information <b>220</b> (see <figref idref="DRAWINGS">FIG. <b>2</b></figref>) may include one or more particular data formats of the sensor data <b>310</b> in the particular range, such as image feeds, video feeds, LiDAR feeds, among others.</p><p id="p-0061" num="0058">The user <b>140</b> may specify the one or more particular data formats of the sensor data <b>310</b> to be included in the particular information <b>220</b>. For example, the user <b>140</b> may specify image feeds and/or video feeds captured by the sensors <b>646</b> from two minutes before the occurrence of the accident <b>110</b> till three minutes after the occurrence of the accident <b>110</b>. As such, the operation server <b>400</b> determines the timestamp of the occurrence of the accident <b>110</b> from the sensor data <b>310</b>, pulls sensor data <b>310</b> in the particular time range from memory <b>404</b>, and provides that to the remote operator <b>216</b> to forward to the electronic device <b>150</b>, e.g., at the application <b>204</b> (see <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0062" num="0059">In another example, in a case where the AV <b>602</b> is involved in accident <b>110</b>, the particular information <b>220</b> may include one or more of speed information (e.g., accelerometer readings), steering information, brake information, engine speed (Revolutions Per Minute (RPM)) associated with the AV <b>602</b> in a particular range in which the accident <b>110</b> has occurred.</p><p id="p-0063" num="0060">In another example, in a case where the AV <b>602</b> is involved in the accident <b>110</b>, the particular information <b>220</b> may include the health diagnostics report of the AV <b>602</b>, e.g., in a particular range in which the accident <b>110</b> has occurred. Alternatively or in addition, user <b>140</b> may specify any other information associated with the AV <b>602</b> to be included in the particular information <b>220</b>.</p><heading id="h-0012" level="2">Example Assistance and Instructions to Address an Accident</heading><p id="p-0064" num="0061">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, in the case where the AV <b>602</b> is involved in the accident <b>110</b> described in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, the assistance <b>222</b> to address (and perhaps resolve) the accident <b>110</b> includes an emergency assist <b>114</b>. As such, the operation server <b>400</b> may provide instructions for accident <b>224</b> to the remote operator <b>216</b> to forward to the user <b>140</b> to enable them to provide the emergency assist <b>114</b>.</p><p id="p-0065" num="0062">In one embodiment, the instructions for accident <b>224</b> may include instructions for remotely granting entry to the user <b>140</b> to enter the cab of the AV <b>602</b> (upon remotely unlocking the entry door of the cab) and disengaging the autonomous operation of the AV <b>602</b>. In one embodiment, disengaging the autonomous operation of the AV <b>602</b> may be performed remotely from the operation server <b>400</b>. In one embodiment, disengaging the autonomous operation of the AV <b>602</b> may be performed locally by the user <b>140</b> following the instructions for accident <b>224</b>.</p><p id="p-0066" num="0063">In a particular example, the user <b>140</b> can disengage the autonomous operation of the AV <b>602</b> using an emergency stop button <b>604</b>. See <figref idref="DRAWINGS">FIG. <b>6</b></figref> for a further description of the emergency stop button <b>604</b>. In brief, the emergency stop button <b>604</b> may include a physical button that is in signal communication with the components of the AV <b>602</b>, and is configured to disconnect or disengage the autonomous functions of the AV <b>602</b> upon being activated. Once the emergency stop button <b>604</b> is activated, the AV <b>602</b> can be operated manually similar to a non-autonomous vehicle. In one embodiment, the local disengagement of the autonomous functions of the AV <b>602</b> may be carried out for an additional confirmation in addition to the remote disengagement of the autonomous function of the AV <b>602</b>.</p><p id="p-0067" num="0064">In a particular example, the user <b>140</b> may request to enter the cab of the AV <b>602</b> and locally disengage the autonomous function of the AV <b>602</b>. As such, the emergency assist <b>114</b> may include the local disengagement of the autonomous function of the AV <b>602</b>.</p><p id="p-0068" num="0065">In one embodiment, the instructions for accident <b>224</b> may include instructions to manually operate the AV <b>602</b> without an ignition key. For example, when the user <b>140</b> has entered the cab of the AV <b>602</b> and disengaged the autonomous function of the AV <b>602</b>, they may receive the instructions to manually operate the AV <b>602</b> without the ignition key. As such, the user <b>140</b> can drive and pull over the AV <b>602</b> to a side of the road <b>112</b>.</p><p id="p-0069" num="0066">In some cases, it may not be safe to manually operate the AV <b>602</b>, for example, due to damages from the accident <b>110</b>. As such, the operation server <b>400</b> may determine whether it is safe to manually operate the AV <b>602</b> based on the health diagnostics report of the AV <b>602</b>. For example, upon detecting accident <b>110</b>, the operation server <b>400</b> may analyze the health diagnostics of the AV <b>602</b> and determine whether it is safe to manually operate the AV <b>602</b>. For example, the operation server <b>400</b> may determine that it is safe to manually operate the AV <b>602</b> if the health diagnostics report of the AV <b>602</b> indicates that the overall health of the components of the AV <b>602</b> is above a threshold percentage (e.g., above 60%, above 70%, above 80%, or any other appropriate percentage range). In response to determining that it is safe to manually operate the AV <b>602</b>, the operation server <b>400</b> may provide instructions to the remote operator <b>216</b> to inform the user <b>140</b> that it is safe to manually operate the AV <b>602</b>.</p><p id="p-0070" num="0067">In response to determining that it is not safe to manually operate the AV <b>602</b>, the operation server <b>400</b> may dispatch a tow truck to move the AV <b>602</b>. In a particular example, the operation server <b>400</b> may also provide an estimated arrival time of the towing truck to the remote operator <b>216</b> to forward to the user <b>140</b>.</p><p id="p-0071" num="0068">In one embodiment, the operation server <b>400</b> may establish a second communication path <b>230</b> between the user <b>140</b> and the remote operator <b>216</b>, where the second communication path <b>230</b> is established by a communication module associated with the AV <b>602</b>. The communication module may be installed inside the cab of the AV <b>602</b>. As such, the communication module may be referred to as an in-cab communication module. In other examples, the communication module may be installed at any location on the AV <b>602</b>, such as on an outer body of the AV <b>602</b>. The communication module may include one or more user interfaces. For example, the user interfaces may include a speaker, a microphone, and a display screen.</p><p id="p-0072" num="0069">For example, assume that the user <b>140</b> has entered the cab of the AV <b>602</b> upon remotely being granted entry to the cab of the AV <b>602</b> as described above. Upon detecting that the user <b>140</b> has entered the cab of the AV <b>602</b>, the operation server <b>400</b> may boot up the in-cab communication module, initiate, and establish the second communication path <b>230</b>. The second communication path <b>230</b> may be similar to communication path <b>218</b>. For example, similar to communication path <b>218</b>, the second communication path <b>230</b> may follow a two-way communication protocol. The user <b>140</b> and the remote operator <b>216</b> may communicate with each other using the user interfaces of the in-cab communication module. For example, the user <b>140</b> may access and view the particular information <b>220</b> on the display screen.</p><p id="p-0073" num="0070">In one embodiment, for the case of the AV <b>602</b> involving in accident <b>110</b> (see <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>), the emergency assist <b>114</b>, the instructions for accident <b>224</b>, particular information <b>220</b> that was sent to the user <b>140</b>, their corresponding timestamps of communication, and the outcome of the accident <b>110</b> may be stored in the ticket <b>214</b>.</p><heading id="h-0013" level="2">Example Particular Information to Address Inspection</heading><p id="p-0074" num="0071">Referring back to <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, in a case where the unexpected event <b>100</b> comprises the inspection <b>120</b>, the particular information <b>220</b> may include an inspection checklist that may be based on road safety regulation rules. For example, the particular information <b>220</b> may include one or more of a status report of autonomous faculties of the AV <b>602</b> and a status report of the physical characteristics of the AV <b>602</b>.</p><p id="p-0075" num="0072">The status report of the autonomous faculties of the AV <b>602</b> may include the autonomous functions/algorithms stored in components of the AV <b>602</b>, including those stored in the control subsystem <b>300</b>, such as software instructions <b>308</b>, sensor data <b>310</b>, object detection instructions <b>312</b>, map data <b>412</b>, routing plans <b>416</b>, and sensor functions (see <figref idref="DRAWINGS">FIG. <b>3</b></figref>).</p><p id="p-0076" num="0073">The status report of the physical characteristics of the AV <b>602</b> may include one or more of the weight, cargo, temperature of the interior components, temperature of the cab, tire inflations, among other information associated with the AV <b>602</b>.</p><p id="p-0077" num="0074">The user <b>140</b><i>b </i>may specify one or more items listed above to be included in the particular information <b>220</b> (see <figref idref="DRAWINGS">FIG. <b>2</b></figref>). Alternatively or in addition, the user <b>140</b><i>b </i>may specify any other appropriate information associated with the AV <b>602</b> to be included in the particular information <b>220</b> (see <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0078" num="0075">In the case where the user <b>140</b><i>b </i>wishes to inspect the AV <b>602</b> (i.e., the AV <b>602</b> is involved in the inspection <b>120</b>), the assistance <b>222</b> (see <figref idref="DRAWINGS">FIG. <b>2</b></figref>) to address (and perhaps resolve) the inspection <b>120</b> includes an inspection assist <b>124</b>. Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, as such, in one embodiment, the operation server <b>400</b> may provide instructions for inspection <b>226</b> to the remote operator <b>216</b> to forward to the user <b>140</b><i>b </i>(see <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, an instance of user <b>140</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>) to enable them to provide the inspection assist <b>124</b>. In one embodiment, the instruction for inspection <b>226</b> may include providing the inspection checklist or the particular information <b>220</b> to the user <b>140</b><i>b </i>(see <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>) as described above.</p><p id="p-0079" num="0076">In one embodiment, for the case of the AV <b>602</b> involving in inspection <b>120</b> (see <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>), the inspection assist <b>124</b>, instructions for inspection <b>226</b>, particular information <b>220</b> that was sent to the user <b>140</b>, their corresponding timestamps, and the outcome of the inspection <b>120</b> may be stored in the ticket <b>214</b>.</p><heading id="h-0014" level="2">Example Particular Information to Address Report Request</heading><p id="p-0080" num="0077">Referring back to <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>, in a case where the unexpected event <b>100</b> comprises the report request <b>130</b>, the particular information <b>220</b> may include the report <b>134</b> that is requested by the user <b>140</b><i>c</i>. For example, the report <b>134</b> may include one or more of the driving history of the AV <b>602</b> and health diagnostics of the AV <b>602</b>. For example, the user <b>140</b><i>c </i>may request to include a driving history of the AV <b>602</b> in a particular time range in the report <b>134</b>. The driving history of the AV <b>602</b> may include one or more of accelerometer records, steering records, brake records, etc. for example, in a table format with timestamps or any other appropriate data format.</p><p id="p-0081" num="0078">In the case of the first example described above in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref> where the user <b>140</b><i>c </i>has flagged the AV <b>602</b> to pull over because they assumed the AV <b>602</b> was speeding, the user <b>140</b><i>c </i>may request to receive the driving history of the AV <b>602</b>, e.g., from two minutes before the AV <b>602</b> was pulled over till the AV <b>602</b> was pulled over. As such, the report <b>134</b> may include the driving history of the AV <b>602</b> in the requested time range.</p><p id="p-0082" num="0079">In the case of the second example described in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref> where the user <b>140</b><i>c </i>observed a malfunction of a component of the AV <b>602</b> (e.g., smoke coming out of a brake of the AV <b>602</b>), the user <b>140</b><i>c </i>may request to receive the health diagnostics of the AV <b>602</b> of that component or the general health diagnostics of the AV <b>602</b>. As such, the report <b>134</b> may include the health diagnostics of the AV <b>602</b>.</p><p id="p-0083" num="0080">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, alternatively or in addition, the user <b>140</b> may specify any other appropriate information associated with the AV <b>602</b> to be included in the report <b>134</b>.</p><p id="p-0084" num="0081">In the case where the user <b>140</b> wishes to receive the report <b>134</b> from the AV <b>602</b> (i.e., the AV <b>602</b> is involved in the report request <b>130</b>), the assistance <b>222</b> to address (and perhaps resolve) the report request <b>130</b> includes reporting assist or report <b>134</b> to the user <b>140</b>. In one embodiment, the operation server <b>400</b> may provide instructions for report request <b>228</b> to the remote operator <b>216</b> to forward to the user <b>140</b> to satisfy the request of the user <b>140</b> to receive the report <b>134</b>.</p><p id="p-0085" num="0082">In one embodiment, for the case of the AV <b>602</b> involving a report request <b>130</b> (see <figref idref="DRAWINGS">FIG. <b>1</b></figref>), the instructions for report request <b>228</b>, particular information <b>220</b> that was sent to the user <b>140</b>, the report <b>134</b>, their corresponding timestamps of communication, and the outcome of the report request <b>130</b> may be stored in the ticket <b>214</b>.</p><p id="p-0086" num="0083">The operation server <b>400</b> determines whether the unexpected event <b>100</b> is addressed (and perhaps resolved) by determining whether the assistance <b>222</b> of the user <b>140</b> to address (and perhaps resolve) the unexpected event <b>100</b> has been accomplished and successful. In response to determining that the unexpected event <b>100</b> is addressed (and perhaps resolved), the operation server <b>400</b> closes the ticket <b>214</b>.</p><p id="p-0087" num="0084">In some embodiments, the user <b>140</b> may also transfer data to the remote operator <b>216</b> and/or the operation server <b>400</b> in providing assistance <b>222</b> to address (and perhaps resolve) the unexpected event <b>100</b>. For example, in the case of the AV <b>602</b> involving an accident <b>110</b>, the user <b>140</b> may capture one or more images and/or videos from the scene of the accident <b>110</b>, and send those to the remote operator <b>216</b> and/or the operation server <b>400</b>. The remote operator <b>216</b> may review the received images and/or videos, determine the extent of damage to the AV <b>602</b>, and confirm (or update) health diagnostics of the AV <b>602</b>. The remote operator <b>216</b> may also use the received images and/or videos to confirm (update) the instructions for accidents <b>224</b> for the user <b>140</b>.</p><p id="p-0088" num="0085">In some embodiments, the operation server <b>400</b> may communicate the particular information <b>220</b> to the electronic device <b>150</b> even while the AV <b>602</b> is driving. For example, while the AV <b>602</b> is driving on a road, the AV <b>602</b> may encounter a situation where it may need to account for an unplanned pullover or an unplanned re-route. These scenarios are described below.</p><p id="p-0089" num="0086">In the case of the second example described in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>, when the user observes a malfunction of a component of the AV <b>602</b>, while the AV <b>602</b> is driving, the user <b>140</b> may initiate the communication with the operation server <b>400</b>. Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, similar to as described above, once the communication path <b>218</b> is established between the user <b>140</b> and the remote operator <b>216</b>, the user <b>140</b> may inform the remote operator <b>216</b> and/or the operation server <b>400</b> about the malfunction. The remote operator <b>216</b> and/or the operation server <b>400</b> may review the status of that component of the AV <b>602</b> (by accessing the operation server <b>400</b>). In one example, the remote operator <b>216</b> and/or the operation server <b>400</b> may reply to the user <b>140</b> that the status of that component indicates a warning but it is fully operational, and forward a status report of that component to the user <b>140</b> (in the report <b>134</b>). In another example, the remote operator <b>216</b> and/or the operation server <b>400</b> may reply to the user <b>140</b> that the status of that component indicates damage and the control subsystem <b>300</b> was already looking for a safe region to pull the AV <b>602</b> over. In this example, the user <b>140</b> may provide assistance <b>222</b> by suggesting particular location coordinates of a safe region to the remote operator <b>216</b> and/or the operation server <b>400</b> to pull the AV <b>602</b> over. The remote operator <b>216</b> and/or the operation server <b>400</b> may evaluate the suggestion of the user <b>140</b> and set (or update) the region for the control subsystem <b>300</b> to pull the AV <b>602</b> over. As such, the assistance <b>222</b> provided by the user <b>140</b> may include suggesting the safe region to pull the AV <b>602</b> over into.</p><p id="p-0090" num="0087">In some cases, while the AV <b>602</b> is driving on a road, the AV <b>602</b> may encounter a situation where it may need to account for an unplanned re-route. For example, the AV <b>602</b> may encounter a re-route road sign, a road closure, or any other situation that would prevent the AV <b>602</b> from continuing to follow its path. In such situations, the sensors <b>646</b> detect the presence of one or more of a re-route road sign, a road closure, etc. As such, the control subsystem <b>300</b> may pull the AV <b>602</b> over, if the control subsystem <b>300</b> and/or the operation server <b>400</b> determine that it is not safe to autonomously re-route the AV <b>602</b>. For example, the control subsystem <b>300</b> and/or the operation server <b>400</b> may determine that it is not safe to autonomously re-route the AV <b>602</b> if the traffic on the road is congested.</p><p id="p-0091" num="0088">In such situations, the user <b>140</b> may approach the AV <b>602</b> and initiate a communication with the operation server <b>400</b> to provide roadside assistance <b>222</b> to the AV <b>602</b>. The roadside assistance <b>222</b> may include re-routing the AV <b>602</b> to an appropriate road to reach its destination. For example, after receiving a confirmation that the AV <b>602</b> is fully operational (e.g., from the remote operator <b>216</b> and/or the operation server <b>400</b>), the user <b>140</b> may be granted entry to enter the cab of the AV <b>602</b>, disengage the autonomous functions of the AV <b>602</b>, and manually drive the AV <b>602</b> to the appropriate road to reach its destination.</p><p id="p-0092" num="0089">In some cases, while the AV <b>602</b> is driving on a road, the sensors <b>646</b> on the AV <b>602</b> may detect a blown tire. As such, the control subsystem <b>300</b> may pull the AV <b>602</b> over to a side of a road. A user <b>140</b> may arrive at the AV <b>602</b> and initiate a communication with the operation server <b>400</b> to provide roadside assistance <b>222</b> to the AV <b>602</b>. In this case, the roadside assistance <b>222</b> may include changing the blown tire of the AV <b>602</b>.</p><p id="p-0093" num="0090">Network <b>232</b> may be any suitable type of wireless and/or wired network including, but not limited to, all or a portion of the Internet, an Intranet, a private network, a public network, a peer-to-peer network, the public switched telephone network, a cellular network, a local area network (LAN), a metropolitan area network (MAN), a wide area network (WAN), and a satellite network. The network <b>232</b> may be configured to support any suitable type of communication protocol as would be appreciated by one of ordinary skill in the art.</p><heading id="h-0015" level="2">Example Control Subsystem</heading><p id="p-0094" num="0091"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an embodiment of the control subsystem <b>300</b>. Aspects of one embodiment of the control subsystem <b>300</b> have been covered in descriptions of <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>C and <b>2</b></figref>, and additional aspects are provided below. The control subsystem <b>300</b> includes at least one processor <b>302</b>, at least one memory <b>304</b>, and at least one network interface <b>306</b>. The control subsystem <b>300</b> may be configured as shown or in any other suitable configuration.</p><p id="p-0095" num="0092">Processor <b>302</b> comprises one or more processors operably coupled to the memory <b>304</b>. The processor <b>302</b> is any electronic circuitry including, but not limited to, state machines, one or more central processing unit (CPU) chips, logic units, cores (e.g. a multi-core processor), field-programmable gate array (FPGAs), application-specific integrated circuits (ASICs), or digital signal processors (DSPs). The processor <b>302</b> may be a programmable logic device, a microcontroller, a microprocessor, or any suitable combination of the preceding. The processor <b>302</b> is communicatively coupled to and in signal communication with the memory <b>304</b> and the network interface <b>306</b>. The one or more processors are configured to process data and may be implemented in hardware or software. For example, the processor <b>302</b> may be 8-bit, 16-bit, 32-bit, 64-bit or of any other suitable architecture. The processor <b>302</b> may include an arithmetic logic unit (ALU) for performing arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that fetches instructions from memory and executes them by directing the coordinated operations of the ALU, registers and other components. The one or more processors are configured to implement various instructions. For example, the one or more processors are configured to execute software instructions <b>308</b> to implement the functions disclosed herein, such as some or all of those described with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b>, and <b>5</b></figref>. In some embodiments, the function described herein is implemented using logic units, FPGAs, ASICs, DSPs, or any other suitable hardware or electronic circuitry.</p><p id="p-0096" num="0093">Memory <b>304</b> stores any of the information described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> and below with respect to <figref idref="DRAWINGS">FIG. <b>5</b></figref> along with any other data, instructions, logic, rules, or code operable to implement the function(s) described herein when executed by processor <b>302</b>. For example, the memory <b>304</b> may store software instructions <b>308</b>, sensor data <b>310</b> received from the sensors <b>646</b> of the AV <b>602</b>, obstruction detection instructions <b>312</b>, map data <b>412</b>, routing plan <b>416</b>, driving instructions <b>418</b>, and/or any other data/instructions described herein. The memory <b>304</b> comprises one or more disks, tape drives, or solid-state drives, and may be used as an over-flow data storage device, to store programs when such programs are selected for execution, and to store instructions and data that are read during program execution. The memory <b>304</b> may be volatile or non-volatile and may comprise read-only memory (ROM), random-access memory (RAM), ternary content-addressable memory (TCAM), dynamic random-access memory (DRAM), and static random-access memory (SRAM).</p><p id="p-0097" num="0094">Network interface <b>306</b> is configured to enable wired and/or wireless communications. The network interface <b>306</b> is configured to communicate data between the control subsystem <b>300</b> and other network devices, systems, or domain(s). For example, the network interface <b>306</b> may comprise a WIFI interface, a local area network (LAN) interface, a wide area network (WAN) interface, a modem, a switch, or a router. The processor <b>302</b> is configured to send and receive data using the network interface <b>306</b>. The network interface <b>306</b> may be configured to use any suitable type of communication protocol.</p><p id="p-0098" num="0095">In one embodiment, the control subsystem <b>300</b> may be a subsystem of the in-vehicle control computer system <b>650</b> (see <figref idref="DRAWINGS">FIG. <b>6</b></figref>). The control subsystem <b>300</b> may be implemented by one or more computing devices that may serve to determine a traveling pathway free of objects/obstacles for the AV <b>602</b>. For example, the one or more computing devices may implement computing systems to facilitate detecting objects surrounding the AV <b>602</b>. The control subsystem <b>300</b> is in signal communication with the in-vehicle control computer system <b>650</b> (and its components) and the operation server <b>400</b>.</p><p id="p-0099" num="0096">To determine a traveling pathway for the AV <b>602</b>, the control subsystem <b>300</b> receives the sensor data <b>310</b> from the sensors <b>646</b> of the AV <b>602</b>. The control subsystem <b>300</b> then compares the received sensor data <b>310</b> with a portion of the map data <b>412</b> that covers the detection zones of the sensors <b>646</b>. Some examples of sensors <b>646</b> are described in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The control subsystem <b>300</b> may use the obstruction detection instructions <b>312</b> to detect those objects and their characteristics.</p><p id="p-0100" num="0097">The obstruction detection instructions <b>312</b> may be implemented by the processor <b>302</b> executing software instructions <b>308</b>, and is generally configured to detect objects and their characteristics, such as their identification (e.g., a vehicle, an animal, a person, a tree, a traffic light, etc.), and speed, among other characteristics. The obstruction detection instructions <b>312</b> may be implemented using neural networks and/or machine learning algorithms for detecting objects from images, videos, infrared images, point clouds, Radar data, etc. For example, if the sensors <b>646</b> include cameras, the sensor data <b>310</b> may include images and/or videos of the environment surrounding the AV <b>602</b> (i.e., the detection zones of the sensors <b>646</b>). In such cases, the control subsystem <b>300</b> may employ obstruction detection instructions <b>312</b> which include functions for detecting objects in the images and/or videos and determining whether there is any object within the detection zones of the sensors <b>646</b>.</p><p id="p-0101" num="0098">The obstruction detection instructions <b>312</b> may include code to employ object detection techniques to identify objects from the images and/or videos, such as an object that is a vehicle, a road sign, a lane marking, a pedestrian, a construction vehicle, a delineator, etc. If the control subsystem <b>300</b> detects an object within the detection zones of the cameras, the control subsystem <b>300</b> (in signal communication with the in-vehicle control computer system <b>650</b>) may employ image based object detection module <b>718</b> to determine characteristics of that object, such as its location coordinates, speed, trajectory, among other characteristics (See the description of the image based object detection module <b>718</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). For example, if the sensors <b>646</b> include LiDAR sensors, the sensor data <b>310</b> may include distance measurements. For example, the distance measurements may include a distance traveled by an object (i.e., a displacement of an object), distances of an object from a LiDAR sensor at different times (t), etc.</p><p id="p-0102" num="0099">In another example, the sensor data <b>310</b> from LiDAR sensors may include a cloud of point data representing obstacles or objects, which have been hit by the laser (e.g., radio wave), within the environment surrounding the AV <b>602</b> (i.e., the detection zones of the sensors <b>646</b>). The cloud of point data may include points corresponding to light emitted from the LiDAR sensors and reflected from objects within the environment surrounding the AV <b>602</b>. The time delay between the transmitted light and the reflected light bounced off an object corresponds to the distance between the LiDAR sensor and that object. The intensity of the reflected light bounced off an object may be indicative of a surface type of that object, e.g., a metal, skin, plastic, fabric, concrete, etc. As such, the control subsystem <b>300</b> (via obstruction detection instructions <b>312</b>) may identify that object.</p><p id="p-0103" num="0100">In one embodiment, the control subsystem <b>300</b> may use obstruction detection instructions <b>312</b> for detecting objects within the detections zones of the sensors <b>646</b>. The control subsystem <b>300</b> may use the obstruction detection instructions <b>312</b> to determine if an object is detected in the sensor data <b>310</b> (e.g., in image/video feeds, LiDAR data feeds, motion sensor data feeds, infrared data feeds, etc.) received from the sensors <b>646</b>. For example, the obstruction detection instructions <b>312</b> may include code for implementing object detection methods from the feed of images corresponding to frames of videos (e.g., detecting objects within detections zones of the sensors <b>646</b> from videos). Similarly, the obstruction detection instructions <b>312</b> may include code for detecting objects from LiDAR data, motion sensor data (e.g., detecting motions of the objects within the detections zones of the sensors <b>646</b>), sounds (e.g., detecting sounds near the AV <b>602</b>), and infrared data (e.g., detecting objects within the detections zones of the sensors <b>646</b> in infrared images). The obstruction detection instructions <b>312</b> may include code for detecting objects using other data types as well.</p><p id="p-0104" num="0101">In one embodiment, the obstruction detection instructions <b>312</b> may include object classification techniques to determine to which class each detected object belongs. For example, the object classification techniques may be trained to classify objects based on their features, such as their geometries, sizes, speeds, among features other. As such, the object classification techniques may classify objects with common features in one class. In one embodiment, the object classification techniques may be trained by a training dataset of data types representing objects, such as in images, videos, LiDAR data, radar, motion data, etc.</p><p id="p-0105" num="0102">For example, the obstruction detection instructions <b>312</b> may include code to employ object detection techniques to identify objects from the distance measurements and/or a cloud of point data. The control subsystem <b>300</b> may employ obstruction detection instructions <b>312</b> which include functions for detecting objects based on characteristics and/or changes in the distance measurements and/or a cloud of point data, and determining whether there is any object within the detection zones of the sensors <b>646</b>. If the control subsystem <b>300</b> detects an object within the detection zones of the LiDAR sensors, the control subsystem <b>300</b> (in signal communication with the in-vehicle control computer system <b>650</b>) may employ LiDAR based object detection module <b>712</b> to determine location coordinates, speed, trajectory, among other characteristics of that object (see the description of the LiDAR based object detection module <b>712</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0106" num="0103">In another example, if the sensors <b>646</b> include motion sensors, the sensor data <b>310</b> may include motion measurements. For example, the motion measurements may include the motion of an object from a first location to a second location. In such cases, the control subsystem <b>300</b> may employ obstruction detection instructions <b>312</b> which include functions for detecting objects based on characteristics and/or changes in the motion measurements and determining whether there is any object within the detection zones of the sensors <b>646</b>. For example, changes in motions measured by a motion sensor may indicate the presence of an object within the detection zone of the sensors <b>646</b>, and the direction of movement of that object. As such, the control subsystem <b>300</b> may determine whether an object is moving towards the AV <b>602</b> (e.g., a vehicle on the opposite side of a road), away from the AV <b>602</b> (e.g., a vehicle speeding ahead of the AV <b>602</b>), across the AV <b>602</b> (e.g., a pedestrian crossing the road), etc.</p><p id="p-0107" num="0104">While certain examples of the detection of objects are described above, it should be understood that any other appropriate method of object detection may be used by the control subsystem <b>300</b>. In some embodiments, the control subsystem <b>300</b> may use two or more types of sensor data to determine whether an object is detected (e.g., by combining camera images, LiDAR data, and Radar data as described with respect to the sensor fusion module <b>702</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>). In other words, the obstruction detection instructions <b>312</b> may include instructions, rules, and/or code for implementing any of the modules as described with respect to <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0108" num="0105">If the control subsystem <b>300</b> detects a distance of an object from the AV <b>602</b> is getting close to a configurable threshold distance (e.g., 200 feet, 250 feet, or any other appropriate distance), the control subsystem <b>300</b> sends signals to the in-vehicle control computer system <b>650</b> (see <figref idref="DRAWINGS">FIG. <b>6</b></figref>) to navigate the AV <b>602</b> according to each case and to keep a safe distance from that object.</p><p id="p-0109" num="0106">In one case, the control subsystem <b>300</b> may detect a distance of a stationary object from the AV <b>602</b> is getting close to a configurable threshold distance, such as a building, a stopped vehicle, a pedestrian, among other objects. In another case, the control subsystem <b>300</b> may detect that a distance of a moving object from the AV <b>602</b> is getting close to a configurable threshold distance, such as a moving vehicle, object <b>116</b> discussed in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, among other objects.</p><p id="p-0110" num="0107">The control subsystem <b>300</b> may determine the configurable threshold distance based at least in part upon the allowed speed range of a road traveled by the AV <b>602</b>, speed of the AV <b>602</b>, and speeds of surrounding vehicles. For example, in a highway where the allowed speed range is 65-75 mph, when the AV <b>602</b> is moving with a speed of 65 mph, and the average speed of surrounding vehicles is 70 mph, the configurable threshold distance may be 300 feet.</p><heading id="h-0016" level="2">Example Operation Server</heading><p id="p-0111" num="0108"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an embodiment of the operation server <b>400</b>. Aspects of one embodiment of the operation server <b>400</b> have been covered in descriptions of <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>C and <b>2</b></figref>, and additional aspects are provided below. The operation server <b>400</b> includes at least one processor <b>402</b>, at least one memory <b>404</b>, and at least one network interface <b>406</b>. The operation server <b>400</b> may be configured as shown or in any other suitable configuration.</p><p id="p-0112" num="0109">Processor <b>402</b> comprises one or more processors operably coupled to the memory <b>404</b>. The processor <b>402</b> is any electronic circuitry including, but not limited to, state machines, one or more central processing unit (CPU) chips, logic units, cores (e.g. a multi-core processor), field-programmable gate array (FPGAs), application specific integrated circuits (ASICs), or digital signal processors (DSPs). The processor <b>402</b> may be a programmable logic device, a microcontroller, a microprocessor, or any suitable combination of the preceding. The processor <b>402</b> is communicatively coupled to and in signal communication with the memory <b>404</b>, network interface <b>406</b>, and user interface <b>408</b>. The one or more processors are configured to process data and may be implemented in hardware or software. For example, the processor <b>402</b> may be 8-bit, 16-bit, 32-bit, 64-bit or of any other suitable architecture. The processor <b>402</b> may include an arithmetic logic unit (ALU) for performing arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that fetches instructions from memory and executes them by directing the coordinated operations of the ALU, registers and other components. The one or more processors are configured to implement various instructions. For example, the one or more processors are configured to execute software instructions <b>410</b> to implement the function disclosed herein, such as some or all of those described with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>C, <b>2</b>, and <b>5</b></figref>. In some embodiments, the function described herein is implemented using logic units, FPGAs, ASICs, DSPs, or any other suitable hardware or electronic circuitry.</p><p id="p-0113" num="0110">Memory <b>404</b> stores any of the information described above with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>C and <b>2</b></figref> and below with respect to <figref idref="DRAWINGS">FIG. <b>5</b></figref> along with any other data, instructions, logic, rules, or code operable to implement the function(s) described herein when executed by processor <b>402</b>. For example, the memory <b>404</b> may store software instructions <b>410</b>, map data <b>412</b>, map building module <b>414</b>, routing plan <b>416</b>, driving instructions <b>418</b>, object detection machine learning modules <b>420</b>, training dataset <b>422</b>, traffic data <b>424</b>, emergency assist <b>114</b>, inspection assist <b>124</b>, report <b>134</b>, ticket <b>214</b>, particular information <b>220</b>, sensor data <b>310</b> received from the AV <b>602</b>, and/or any other data/instructions. The software instructions <b>410</b> include code that when executed by the processor <b>402</b> causes the operation server <b>400</b> to perform the functions described herein, such as some or all of those described in <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b>, and <b>5</b></figref>. The memory <b>404</b> further stores instructions <b>426</b> that are forwarded to the user <b>140</b> to address different cases of unexpected events <b>110</b> as described in <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>C, <b>2</b>, and <b>5</b></figref>. Instructions <b>426</b> may include instructions for accident <b>224</b>, instructions for inspection <b>226</b>, instructions for report request <b>228</b>, and/or any other data/instructions. Instructions <b>426</b> are laid out by the remote operator <b>216</b> to coordinate with a user <b>140</b> to address each unexpected event <b>100</b>. The memory <b>404</b> comprises one or more disks, tape drives, or solid-state drives, and may be used as an over-flow data storage device, to store programs when such programs are selected for execution, and to store instructions and data that are read during program execution. The memory <b>404</b> may be volatile or non-volatile and may comprise read-only memory (ROM), random-access memory (RAM), ternary content-addressable memory (TCAM), dynamic random-access memory (DRAM), and static random-access memory (SRAM).</p><p id="p-0114" num="0111">Network interface <b>406</b> is configured to enable wired and/or wireless communications. The network interface <b>406</b> is configured to communicate data between the control subsystem <b>300</b> and other network devices, systems, or domain(s). For example, the network interface <b>406</b> may comprise a WIFI interface, a local area network (LAN) interface, a wide area network (WAN) interface, a modem, a switch, or a router. The processor <b>402</b> is configured to send and receive data using the network interface <b>406</b>. The network interface <b>406</b> may be configured to use any suitable type of communication protocol.</p><p id="p-0115" num="0112">User interfaces <b>408</b> may include one or more user interfaces that are configured to interact with users, such as the remote operator <b>216</b>. For example, the user interface <b>408</b> may include peripherals of the operation server <b>400</b>, such as monitors, keyboards, one or more computer mice, trackpads, touchpads, etc. The remote operator <b>216</b> may use the user interfaces <b>408</b> to access the memory <b>404</b> to forward data/instruction to the user <b>140</b> to address an unexpected event <b>100</b>, such as any of the instructions <b>426</b>, particular information <b>220</b>, sensor data <b>310</b> (e.g., in a particular time range), among others, similar to that described in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In one embodiment, the remote operator <b>216</b> may confirm (or update) any of the instructions <b>426</b> and particular information <b>220</b> before communicating those to the user <b>140</b>, similar to that described in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In one embodiment, the process of communicating instructions <b>426</b> to the user <b>140</b> may be computerized and performed by the operation server <b>400</b>.</p><p id="p-0116" num="0113">In one embodiment, the operation server <b>400</b> may be implemented by a cluster of computing devices that may serve to oversee the operations of the AV <b>602</b>. For example, the operation server <b>400</b> may be implemented by a plurality of computing devices using distributed computing and/or cloud computing systems. In another example, the operation server <b>400</b> may be implemented by a plurality of computing devices in one or more data centers. As such, in one embodiment, the operation server <b>400</b> may include more processing power than the control subsystem <b>300</b>. The operation server <b>400</b> is in signal communication with one or more AVs <b>602</b> and their components (e.g., the in-vehicle control computer <b>650</b>). In one embodiment, the operation server <b>400</b> is configured to determine a particular routing plan <b>416</b> for the AV <b>602</b>. For example, the operation server <b>400</b> may determine a particular routing plan <b>416</b> for an AV <b>602</b> that leads to reduced driving time and a safer driving experience for reaching the destination of that AV <b>602</b>.</p><p id="p-0117" num="0114">In one embodiment, in cases where an AV <b>602</b> encounters a re-route road sign or a road closure, the operation server <b>400</b> may evaluate sensor data <b>310</b> and map data <b>412</b>, and determine that it is not safe to autonomously conduct re-routing the AV <b>602</b>. In one example, the operation server <b>400</b> may determine that it is not sate to autonomously conduct re-routing the AV <b>602</b> when the sensor data indicates a congested traffic. As such, the operation server <b>400</b> may send instructions to the AV <b>602</b> to pull over to a side of the road. In one embodiment, the remote operator <b>216</b> may confirm, modify, and/or override the suggested navigation plan of the operation server <b>400</b> as described below.</p><p id="p-0118" num="0115">In one embodiment, the navigating solutions or routing plans <b>416</b> for the AV <b>602</b> may be determined from Vehicle-to-Vehicle (V2V) communications, such as one AV <b>602</b> with another. In one embodiment, the navigating solutions or routing plans <b>416</b> for the AV <b>602</b> may be determined from Vehicle-to-Cloud (V2C) communications, such as the AV <b>602</b> with the operation server <b>400</b>.</p><p id="p-0119" num="0116">In one embodiment, the navigating solutions or routing plans <b>416</b> for the AV <b>602</b> may be determined by Vehicle-to-Cloud-to-Human (V2C2H) and/or Vehicle-to-Human (V2H) communications, where human intervention is incorporated in determining navigating solutions for the AV <b>602</b>. For example, the remote operator <b>216</b> may review the sensor data <b>310</b> from the user interface <b>408</b> and confirm, modify, and/or override navigating solutions for the AV <b>602</b> determined by the control subsystem <b>300</b> and/or the operation server <b>400</b>. The remote operator <b>216</b> may add a human perspective in determining the navigation plan of the AV <b>602</b> that the control subsystem <b>300</b> and/or the operation server <b>400</b> otherwise do not have the human perspective which is more preferable compared to machine's perspectives in terms of safety, fuel-saving, etc.</p><p id="p-0120" num="0117">In one embodiment, the navigating solutions for the AV <b>602</b> may be determined by any combination of V2V, V2C, V2C2H, V2H communications, among other types of communications.</p><p id="p-0121" num="0118">As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the remote operator <b>216</b> can access the application server <b>430</b> via communication path <b>434</b>, and similarly, access the operation server <b>400</b> via communication path <b>436</b>. In one embodiment, the operation server <b>400</b> may send the sensor data <b>310</b>, instructions <b>426</b>, particular information <b>220</b>, and/or any other data/instructions to an application server <b>430</b> to be reviewed by the remote operator <b>216</b>. As such, in one embodiment, the remote operator <b>216</b> can remotely access the operation server <b>400</b> via the application server <b>430</b>. The application server <b>430</b> is generally any computing device configured to communicate with other devices, such as other servers (e.g., operation server <b>400</b>), AV <b>602</b>, databases, etc., via a network interface. The application server <b>430</b> is configured to perform specific functions described herein and interact with remote operator <b>216</b>, e.g., via its user interfaces <b>432</b> and communication path <b>434</b>. Examples of the application server <b>430</b> include but are not limited to desktop computers, laptop computers, servers, etc. In one example, the application server <b>430</b> may act as a presentation layer where remote operator <b>216</b> access the operation server <b>400</b>. As such, the operation server <b>400</b> may send sensor data <b>310</b>, instructions <b>426</b>, particular information <b>220</b>, and/or any other data/instructions to the application server <b>430</b>. The remote operator <b>216</b>, after establishing the communication path <b>434</b> with the application server <b>430</b>, may review the received data from the user interface <b>432</b> and confirm, modify, and/or override any of the instructions <b>426</b> and particular information <b>220</b> before communicating to the user <b>140</b>, similar to that described in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In another embodiment, the remote operator <b>216</b> can directly access the operation server <b>400</b>, and after establishing the communication path <b>436</b> with the operation server <b>400</b>, may confirm, modify, and/or override any of the instructions <b>426</b> and particular information <b>220</b> before communicating to the user <b>140</b>. The remote operator <b>216</b> may also confirm, modify, and/or override navigating solutions for the AV <b>602</b> determined by the control subsystem <b>300</b> and/or the operation server <b>400</b>, such as described above.</p><p id="p-0122" num="0119">Map data <b>412</b> may include a virtual map of a city which includes roads <b>112</b>, <b>132</b>, and inspection station <b>122</b> (see <figref idref="DRAWINGS">FIG. <b>1</b></figref>). In some examples, the map data <b>412</b> may include the map database <b>758</b> and map database <b>736</b> (see <figref idref="DRAWINGS">FIG. <b>7</b></figref> for descriptions of the map database <b>758</b> and map database <b>736</b>). The map data <b>412</b> may include drivable areas, such as roads, paths, highways, and undrivable areas, such as terrain (determined by the occupancy grid module <b>760</b>, see <figref idref="DRAWINGS">FIG. <b>7</b></figref> for descriptions of the occupancy grid module <b>760</b>). The map data <b>412</b> may specify location coordinates of road signs, lanes, lane markings, lane boundaries, road boundaries, traffic lights, etc.</p><p id="p-0123" num="0120">The map data <b>412</b> may also specify connections between lanes (e.g., which lanes can feed into other adjacent lanes). The map data <b>412</b> may specify information indicating types of lanes of a road (e.g., traffic lane, passing lane, emergency lane, turning lane, bus lane, etc.), types of lane boundaries (e.g., white lines, yellow lines, other road surface markings and/or mechanical markings, etc.), types of road boundaries (e.g., regular curbs, red curbs, sidewalks, guard rails, other barriers, etc.) road intersections, one or more obstacles ahead of the autonomous vehicle, and other information about the road or areas adjacent to the road.</p><p id="p-0124" num="0121">The map data <b>412</b> may include information about elevations of the roads and grade of the roads (i.e., its incline, decline, slop). For example, the map data <b>412</b> may specify elevations of roads, such as curves, hills, valleys; road hazards, such as speed bumps, potholes; road sections, such as road school zones, railroad crossings, etc. For example, if a section of a road is perfectly flat and level, then the map data <b>412</b> would specify that the grade along that section is zero. The map data <b>412</b> may include elevation of different segments of the roads, such as hills, valleys, curves, etc., among other information. This information may be used to determine shifting to proper gear, such as shifting to high gear in an uphill road and shifting to low gear in a downhill road. As such, by detecting the elevation change in a road, proper gear shifting may be applied, thereby saving fuel of the AV <b>602</b>, increasing longevity of the vehicle drive subsystems <b>642</b> of the AVs <b>602</b>, i.e., engines <b>642</b><i>a</i>, tires <b>642</b><i>b</i>, transmission components <b>642</b><i>c</i>, electrical components <b>642</b><i>d</i>, and power sources <b>642</b><i>e</i>. In one embodiment, if the control subsystem <b>300</b> of an AV <b>602</b> determines that a grade of a road is more than a threshold elevation change (e.g., 7%), it may determine that it is not safe for the AV <b>602</b> to drive in that road.</p><p id="p-0125" num="0122">Map building module <b>414</b> may be implemented by the processor <b>402</b> executing software instructions <b>410</b>, is configured to build the map data <b>412</b>. In one embodiment, the map building module <b>414</b> may build the map data <b>412</b> from sensor data received from one or more mapping vehicles. In one example, a mapping vehicle may be an AV <b>602</b>. In another example, a mapping vehicle may be an AV <b>602</b> or un-autonomous vehicle connected or integrated with sensors <b>646</b> operated by a driver.</p><p id="p-0126" num="0123">The map building module <b>414</b> is configured to use the sensor data to determine which portion of the map data <b>412</b> they are associated with. The map building module <b>414</b> may dynamically build each section of the map data <b>412</b> by merging different sensor data associated with each section of the map data <b>412</b>. The map building module <b>414</b> also uses the sensor data to discover overlapping portions of the map data <b>412</b> (e.g., by matching corresponding images, videos, LiDAR data, Radar data, etc. observing the same portion of the map data <b>412</b>). The map building module <b>414</b> then connects portions of the map data <b>412</b> with their corresponding adjacent portions. In other words, the map building module <b>414</b> discovers adjacent portions of the map data <b>412</b>, stitches them together, and builds the map data <b>412</b>. The map building module <b>414</b> is also configured to update a portion of the map data <b>412</b> that based on the received sensor data needs to be updated.</p><p id="p-0127" num="0124">Routing plan <b>416</b> is a plan for traveling from a start location (e.g., a first AV launchpad/landing pad) to a destination (e.g., a second AV launchpad/landing pad). For example, the routing plan <b>416</b> of the AV <b>602</b> may specify a combination of one or more streets/roads/highways in a specific order from the start location to the destination. The routing plan <b>416</b> of the AV <b>602</b> may specify stages including the first stage (e.g., moving out from the start location), a plurality of intermediate stages (e.g., traveling along particular lanes of one or more particular street/road/highway), and the last stage (e.g., entering the destination). The routing plan <b>416</b> may include other information about the route from the start position to the destination, such as road/traffic signs in that routing plan <b>416</b>, etc.</p><p id="p-0128" num="0125">Driving instructions <b>418</b> may be implemented by the planning module <b>762</b> (See descriptions of the planning module <b>762</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). The driving instructions <b>418</b> may include instructions and rules to adapt the autonomous driving of the AV <b>602</b> according to the driving rules of each stage of the routing plan <b>416</b>. For example, the driving instructions <b>418</b> may include instructions to stay within the speed range of a road traveled by the AV <b>602</b>, adapt the speed of the AV <b>602</b> with respect to observed changes by the sensors <b>646</b>, such as speeds of surrounding vehicles, objects within the detection zones of the sensors, etc.</p><p id="p-0129" num="0126">Object detection machine learning modules <b>420</b> may be implemented by the processor <b>402</b> executing software instructions <b>410</b>, and is generally configured to detect objects from the sensor data <b>310</b>. The object detection machine learning modules <b>420</b> may be implemented using neural networks and/or machine learning algorithms for detecting objects from any data type, such as images, videos, infrared images, point clouds, Radar data, etc.</p><p id="p-0130" num="0127">In one embodiment, the object detection machine learning modules <b>420</b> may be implemented using machine learning algorithms, such as Support Vector Machine (SVM), Naive Bayes, Logistic Regression, k-Nearest Neighbors, Decision trees, or the like. In one embodiment, the object detection machine learning modules <b>420</b> may utilize a plurality of neural network layers, convolutional neural network layers, and/or the like, in which weights and biases of perceptrons of these layers are optimized in the training process of the object detection machine learning modules <b>420</b>. The object detection machine learning modules <b>420</b> may be trained by the training dataset <b>422</b> which includes samples of data types labeled with one or more objects in each sample. For example, the training dataset <b>422</b> may include sample images of objects (e.g., vehicles, lane markings, pedestrian, road signs, etc.) labeled with object(s) in each sample image. Similarly, the training dataset <b>422</b> may include samples of other data types, such as videos, infrared images, point clouds, Radar data, etc. labeled with object(s) in each sample data. The object detection machine learning modules <b>420</b> may be trained, testes, and refined by the training dataset <b>422</b> and the sensor data <b>310</b>. The object detection machine learning modules <b>420</b> uses the sensor data <b>310</b> (which are not labeled with objects) to increase their accuracy of predictions in detecting objects. For example, supervised and/or unsupervised machine learning algorithms may be used to validate the predictions of the object detection machine learning modules <b>420</b> in detecting objects in the sensor data <b>310</b>.</p><p id="p-0131" num="0128">Traffic data <b>424</b> may include traffic data of roads/streets/highways in the map data <b>412</b>. The operation server <b>400</b> may use traffic data <b>424</b> gathered by one or more mapping vehicles. The operation server <b>400</b> may use traffic data <b>424</b> that is captured from any source, such as crowd-sourced traffic data <b>424</b> captured from external sources, e.g., Waze&#xae; and Google Map&#xae;, live traffic reporting, etc.</p><p id="p-0132" num="0000">Example Method for Coordinating with a User at the AV to Resolve an Unexpected Event Involving the AV</p><p id="p-0133" num="0129"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example flowchart of a method <b>500</b> for coordinating with a user <b>140</b> at the AV <b>602</b> to resolve an unexpected event <b>100</b> involving the AV <b>602</b>. Modifications, additions, or omissions may be made to method <b>500</b>. Method <b>500</b> may include more, fewer, or other steps. For example, steps may be performed in parallel or in any suitable order. While at times discussed as the AV <b>602</b>, operation server <b>400</b>, control subsystem <b>300</b>, or components of any of thereof performing steps, any suitable system or components of the system may perform one or more steps of the method <b>500</b>. For example, one or more steps of method <b>500</b> may be implemented, at least in part, in the form of software instructions <b>308</b>, <b>410</b>, and <b>680</b>, respectively from <figref idref="DRAWINGS">FIGS. <b>3</b>, <b>4</b>, and <b>6</b></figref>, stored on non-transitory, tangible, machine-readable media (e.g., memories <b>304</b>, <b>404</b>, <b>690</b>, and <b>802</b>, respectively from <figref idref="DRAWINGS">FIGS. <b>3</b>, <b>4</b>, <b>6</b>, and <b>8</b></figref>) that when run by one or more processors (e.g., processors <b>302</b>, <b>402</b>, <b>670</b>, <b>804</b>, respectively from <figref idref="DRAWINGS">FIGS. <b>3</b>, <b>4</b>, <b>6</b>, and <b>8</b></figref>) may cause the one or more processors to perform steps <b>502</b>-<b>522</b>.</p><p id="p-0134" num="0130">Method <b>500</b> begins at step <b>502</b> where the operation server <b>400</b> detects an unexpected event <b>100</b> related to the AV <b>602</b> from sensor data <b>310</b>. In some examples, the unexpected event <b>100</b> related to the AV <b>602</b> may include an accident <b>110</b>, an inspection <b>120</b>, or a report request <b>130</b> described in <figref idref="DRAWINGS">FIGS. <b>1</b>A to <b>1</b>C</figref>.</p><p id="p-0135" num="0131">In this process, the operation server <b>400</b> first receives the sensor data <b>310</b> from the control subsystem <b>300</b>, where the sensor data <b>310</b> is captured by the one or more sensors <b>646</b> associated with the AV <b>602</b> and processed by the control subsystem <b>300</b>, similar to that described above in <figref idref="DRAWINGS">FIGS. <b>1</b>A to <b>3</b></figref>. In one example, the sensor data <b>310</b> may include location coordinates <b>202</b> of the AV <b>602</b>. In another example, the sensor data <b>310</b> may include information about the environment within detection zones of the sensors <b>646</b> covering a region in front of the AV <b>602</b>, such as detected objects, among others. In another example, the sensor data <b>310</b> may include information about the health status of the operation of the AV <b>602</b> and its components, such as the health status of the sensors <b>646</b>, control subsystem <b>300</b>, autonomous components, among other components of the AV <b>602</b>.</p><p id="p-0136" num="0132">By processing the sensor data <b>310</b>, the operation server <b>400</b> may determine one or more indications indicating the unexpected event <b>100</b>. In some cases, the lack of communication of sensor data <b>310</b> may be an indication that the AV <b>602</b> has been involved in the unexpected event <b>100</b>. For example, due to an accident <b>110</b>, communicating components of the AV <b>602</b> may be damaged, thus, an unexpected lack of communication of the sensor data <b>310</b> may be an indication that the AV <b>602</b> has been involved in accident <b>110</b>.</p><p id="p-0137" num="0133">In the case where the unexpected event <b>100</b> includes an accident <b>110</b>, the sensor data <b>310</b> may indicate a decreasing distance from an object <b>116</b> in the path of the AV <b>602</b> leading to a collision with the object <b>116</b>, similar to that described in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>.</p><p id="p-0138" num="0134">In the case where the unexpected event <b>100</b> includes an inspection <b>120</b>, the sensor data <b>310</b> may indicate the presence of an inspection road sign ahead of the AV <b>602</b>. In another example, in the case where the unexpected event <b>100</b> includes an inspection <b>120</b>, the sensor data <b>310</b> may indicate a signal that is received by an application (and/or a transponder) that is installed in or integrated with the AV <b>602</b> indicating to pull into an inspection station <b>122</b>, similar to that described in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>.</p><p id="p-0139" num="0135">In the cases where unexpected event <b>100</b> includes a report request <b>130</b> from a user <b>140</b>, the sensor data <b>310</b> may indicate that the AV <b>602</b> is flagged by the user <b>140</b>, for example, by the user <b>140</b> signaling the AV <b>602</b> using sirens among other methods, similar to that described in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>.</p><p id="p-0140" num="0136">At step <b>504</b>, the operation server <b>400</b> receives a message <b>208</b> from an electronic device <b>150</b> associated with the user <b>140</b> comprising a request <b>210</b> to access particular information <b>220</b> regarding the AV <b>602</b> and location data <b>212</b> of the user <b>140</b>. In some embodiments, the operation server <b>400</b> may authenticate the user <b>140</b> prior to receiving the message <b>208</b>. For example, the operation server <b>400</b> may authenticate the user <b>140</b> by verifying their login credentials <b>206</b> when the user <b>140</b> logs in to their account on the application <b>204</b>, similar to that described above in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0141" num="0137">The particular information <b>220</b> regarding the AV <b>602</b> may be different for each case of unexpected events <b>100</b>. For each case of unexpected event <b>100</b>, particular information <b>220</b> is described in detail in <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>C and <b>2</b></figref>. In the case where the AV <b>602</b> is involved in accident <b>110</b>, the particular information <b>220</b> may include sensor data <b>310</b> in a particular time range in which the accident <b>110</b> has occurred. In the case where the AV <b>602</b> is requested to be inspected (i.e., the case of inspection <b>120</b>), the particular information <b>220</b> may include an inspection checklist including one or more of a status report of physical characteristics of the AV <b>602</b> and a status report of autonomous faculties of the AV <b>602</b>. In the case where the AV <b>602</b> is requested to provide the report <b>134</b> (i.e., the case of report request <b>130</b>), the particular information <b>220</b> may include one or more of the driving history of the AV <b>602</b> and health diagnostics information of the AV <b>602</b> in a particular time range.</p><p id="p-0142" num="0138">At step <b>506</b>, the operation server <b>400</b> determines whether the location data <b>212</b> of the user <b>140</b> matches the location coordinates <b>202</b> of the AV <b>602</b>. If it is determined that the location data <b>212</b> does not match the location coordinates <b>202</b>, the operation server <b>400</b> determines that the user <b>140</b> is not at the sight of the AV <b>602</b>, for example, is not within a threshold vicinity of the AV <b>602</b> (e.g., ten feet from the AV <b>602</b>), and the method <b>500</b> is terminated. If it is determined that the location data <b>212</b> matches the location coordinates <b>202</b>, the method <b>500</b> proceeds to step <b>508</b>.</p><p id="p-0143" num="0139">At step <b>508</b>, the operation server <b>400</b> associates the AV <b>602</b> with the user <b>140</b>. This may be an indication that the user <b>140</b> is at the sight of the AV <b>602</b>, i.e., within the threshold vicinity from the AV <b>602</b>.</p><p id="p-0144" num="0140">At step <b>510</b>, the operation server <b>400</b> generates a ticket <b>214</b> to record events that will be carried out to address (and perhaps resolve) the unexpected event <b>100</b>.</p><p id="p-0145" num="0141">At step <b>512</b>, the operation server establishes a communication path <b>218</b> between the user <b>140</b> and the remote operator <b>216</b> using the electronic device <b>150</b>. Using the communication path <b>218</b>, the user <b>140</b> and the remote operator <b>216</b> may transfer any appropriate data to one another. For example, the remote operator <b>216</b> may transfer particular sensor data <b>310</b> requested by the user <b>140</b> to the user <b>140</b> (at the electronic device <b>150</b>); and the user <b>140</b> may transfer one or more images and/or videos, for example of the AV <b>602</b> to the remote operator <b>216</b> (at the operation server <b>400</b>).</p><p id="p-0146" num="0142">At step <b>514</b>, the operation server <b>400</b> communicates the particular information <b>220</b> regarding the AV <b>602</b> to the electronic device <b>150</b>. In some embodiments, the remote operator <b>216</b> may confirm (or update) the particular information <b>220</b> before communicating the particular information <b>220</b> to the electronic device <b>150</b>.</p><p id="p-0147" num="0143">At step <b>516</b>, the operation server <b>400</b> receives a request from the user <b>140</b> to provide assistance <b>222</b> to address the unexpected event <b>100</b>. The request of the user <b>140</b> to provide assistance <b>222</b> for each case of unexpected events <b>100</b> may be different. Different examples of the assistance <b>222</b> for each case of unexpected events <b>100</b> are described in detail in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0148" num="0144">In the case where the AV <b>602</b> is involved in the accident <b>110</b>, the assistance <b>222</b> provided by the user <b>140</b> (i.e., emergency assist <b>114</b>) may include local disengagement of autonomous functions of the AV <b>602</b>, manually operating the AV <b>602</b> to a side of a road, among any other appropriate assistance that would lead to addressing (and resolving) the accident <b>110</b>.</p><p id="p-0149" num="0145">In the case where the AV <b>602</b> is requested to be inspected (i.e., the case of inspection <b>120</b>), the assistance <b>222</b> provided by the user <b>140</b> (i.e., inspection assist <b>124</b>) may include inspecting the AV <b>602</b>, and confirming (or updating) the health diagnostics and status report of the physical characteristics of the AV <b>602</b>, among any other appropriate assistance that would lead to addressing (and resolving) the inspection <b>120</b>.</p><p id="p-0150" num="0146">In the case where the AV <b>602</b> is requested to provide a report <b>134</b> (i.e., the case of report request <b>130</b>), the assistance <b>222</b> provided by the user <b>140</b> may include confirming that the AV <b>602</b> is eligible to resume autonomous driving. In another example for this case, where user <b>140</b> observed a malfunction of a component of the AV <b>602</b>, the assistance <b>222</b> may include suggesting a safe region to pull the AV <b>602</b> over. In other examples, the assistance <b>222</b> may include any other appropriate assistance that would lead to addressing (and resolving) the report request <b>130</b>.</p><p id="p-0151" num="0147">In the case where the AV <b>602</b> encountered an unplanned pullover, e.g., due to a blown tire, roadside assistance <b>222</b> may include changing the blown tire and confirming that the AV <b>602</b> is eligible to resume autonomous driving.</p><p id="p-0152" num="0148">In the case where the AV <b>602</b> encountered an unplanned re-route, e.g., due to a road closure, a re-route road sign, roadside assistance <b>222</b> may include manually driving the AV <b>602</b> to re-route the AV <b>602</b> to a road on which the AV <b>602</b> can resume autonomous driving or driving the AV <b>602</b> to its destination.</p><p id="p-0153" num="0149">At step <b>518</b>, the operation server <b>400</b> provides instructions <b>426</b> to the remote operator <b>216</b> to forward to the user <b>140</b> to address the unexpected event <b>100</b>. In some examples, instructions <b>426</b> may include instructions for accident <b>224</b>, instructions for inspection <b>226</b>, or instructions for report request <b>228</b> depending on different cases of the unexpected event <b>100</b>, similar to that described in <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>4</b></figref>.</p><p id="p-0154" num="0150">At step <b>520</b>, the operation server <b>400</b> determines whether the unexpected event <b>100</b> is addressed. In one example, in the case where the AV <b>602</b> is involved in accident <b>110</b>, the operation server <b>400</b> may determine that the accident <b>110</b> is addressed based on updated sensor data <b>310</b> indicating that the AV <b>602</b> has been pulled over to a side of the road.</p><p id="p-0155" num="0151">In another example, in the case where the AV <b>602</b> is requested to be inspected, the operation server <b>400</b> may determine that the inspection <b>120</b> is concluded by receiving a confirmation signal from the user <b>140</b> that the inspection <b>120</b> is concluded.</p><p id="p-0156" num="0152">In another example, in the case where the AV <b>602</b> is requested to provide the report <b>134</b>, the operation server <b>400</b> may determine that the report request <b>130</b> is concluded by receiving a confirmation signal from the user <b>140</b> that the report <b>134</b> is satisfactory.</p><p id="p-0157" num="0153">If it is determined that the unexpected event <b>100</b> is not addressed, the method <b>500</b> repeats the step <b>520</b> and determine whether the unexpected event <b>100</b> is still addressed. The operation server <b>400</b> determines that the unexpected event <b>100</b> is addressed if it receives a confirmation signal from the user <b>140</b> that the unexpected event <b>100</b> is addressed. In other words, at step <b>520</b>, the operation server <b>400</b> waits until it receives the confirmation signal from the user <b>140</b> indicating that the unexpected event <b>100</b> is addressed. If it is determined that the unexpected event <b>100</b> is addressed, method <b>500</b> proceeds to step <b>522</b> where the operation server <b>400</b> closes the ticket <b>214</b>.</p><heading id="h-0017" level="2">Example AV and its Operation</heading><p id="p-0158" num="0154"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a block diagram of an example vehicle ecosystem <b>600</b> in which autonomous driving operations can be determined. As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the AV <b>602</b> may be a semi-trailer truck. The vehicle ecosystem <b>600</b> includes several systems and components that can generate and/or deliver one or more sources of information/data and related services to the in-vehicle control computer <b>650</b> that may be located in an AV <b>602</b>. The in-vehicle control computer <b>650</b> can be in data communication with a plurality of vehicle subsystems <b>640</b>, all of which can be resident in the AV <b>602</b>. A vehicle subsystem interface <b>660</b> is provided to facilitate data communication between the in-vehicle control computer <b>650</b> and the plurality of vehicle subsystems <b>640</b>. In some embodiments, the vehicle subsystem interface <b>660</b> can include a controller area network (CAN) controller to communicate with devices in the vehicle subsystems <b>640</b>.</p><p id="p-0159" num="0155">The AV <b>602</b> may include various vehicle subsystems that support of the operation of AV <b>602</b>. The vehicle subsystems may include the control subsystem <b>300</b>, emergency stop button <b>604</b>, a vehicle drive subsystem <b>642</b>, a vehicle sensor subsystem <b>644</b>, and/or a vehicle control subsystem <b>648</b>. The components or devices of the vehicle drive subsystem <b>642</b>, the vehicle sensor subsystem <b>644</b>, and the vehicle control subsystem <b>648</b> shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> are examples. The AV <b>602</b> may be configured as shown or any other configurations.</p><p id="p-0160" num="0156">The emergency stop button <b>604</b> may include a physical button that is configured to disconnect or disengage the autonomous functions of the AV <b>602</b> upon being activated. The emergency stop button <b>604</b> is in signal communication with the plurality of vehicle subsystems <b>640</b> and in-vehicle control computer <b>650</b>. The emergency stop button <b>604</b> may be activated by any appropriate method, such as, by pressing down, pulling out, sliding, switching, using a key, etc. When activated, the emergency stop button <b>604</b> may start the fail-safe sequence to disengage the autonomous functions of the AV <b>602</b>. In this process, when the emergency stop button <b>604</b> is activated, it disconnects control subsystem <b>300</b>, vehicle drive subsystems <b>642</b>, vehicle sensor subsystems <b>644</b>, and vehicle control subsystem <b>648</b> from in-vehicle control computer <b>650</b>. In other words, when the emergency stop button <b>604</b> is activated, it cuts the power from the autonomous systems of the AV <b>602</b>. In one embodiment, when the emergency stop button <b>604</b> is activated, the engine <b>642</b><i>a </i>may be turned off, brake units <b>648</b><i>b </i>may be applied, and hazard lights may be turned on. Upon activation, the emergency stop button <b>604</b> may override all related start sequence functions of the AV <b>602</b>.</p><p id="p-0161" num="0157">The vehicle drive subsystem <b>642</b> may include components operable to provide powered motion for the AV <b>602</b>. In an example embodiment, the vehicle drive subsystem <b>642</b> may include an engine/motor <b>642</b><i>a</i>, wheels/tires <b>642</b><i>b</i>, a transmission <b>642</b><i>c</i>, an electrical subsystem <b>642</b><i>d</i>, and a power source <b>642</b><i>e. </i></p><p id="p-0162" num="0158">The vehicle sensor subsystem <b>644</b> may include a number of sensors <b>646</b> configured to sense information about an environment or condition of the AV <b>602</b>. The vehicle sensor subsystem <b>644</b> may include one or more cameras <b>646</b><i>a </i>or image capture devices, a Radar unit <b>646</b><i>b</i>, one or more temperature sensors <b>646</b><i>c</i>, a wireless communication unit <b>646</b><i>d </i>(e.g., a cellular communication transceiver), an inertial measurement unit (IMU) <b>646</b><i>e</i>, a laser range finder/LiDAR unit <b>646</b><i>f</i>, a Global Positioning System (GPS) transceiver <b>646</b><i>g</i>, and/or a wiper control system <b>646</b><i>h</i>. The vehicle sensor subsystem <b>644</b> may also include sensors configured to monitor internal systems of the AV <b>602</b> (e.g., an <b>02</b> monitor, a fuel gauge, an engine oil temperature, etc.).</p><p id="p-0163" num="0159">The IMU <b>646</b><i>e </i>may include any combination of sensors (e.g., accelerometers and gyroscopes) configured to sense position and orientation changes of the AV <b>602</b> based on inertial acceleration. The GPS transceiver <b>646</b><i>g </i>may be any sensor configured to estimate a geographic location of the AV <b>602</b>. For this purpose, the GPS transceiver <b>646</b><i>g </i>may include a receiver/transmitter operable to provide information regarding the position of the AV <b>602</b> with respect to the Earth. The Radar unit <b>646</b><i>b </i>may represent a system that utilizes radio signals to sense objects within the local environment of the AV <b>602</b>. In some embodiments, in addition to sensing the objects, the Radar unit <b>646</b><i>b </i>may additionally be configured to sense the speed and the heading of the objects proximate to the AV <b>602</b>. The laser range finder or LiDAR unit <b>646</b><i>f </i>may be any sensor configured to sense objects in the environment in which the AV <b>602</b> is located using lasers. The cameras <b>646</b><i>a </i>may include one or more devices configured to capture a plurality of images of the environment of the AV <b>602</b>. The cameras <b>646</b><i>a </i>may be still image cameras or motion video cameras.</p><p id="p-0164" num="0160">The vehicle control subsystem <b>648</b> may be configured to control the operation of the AV <b>602</b> and its components. Accordingly, the vehicle control subsystem <b>648</b> may include various elements such as a throttle and gear <b>648</b><i>a</i>, a brake unit <b>648</b><i>b</i>, a navigation unit <b>648</b><i>c</i>, a steering system <b>648</b><i>d</i>, and/or an autonomous control unit <b>648</b><i>e</i>. The throttle <b>648</b><i>a </i>may be configured to control, for instance, the operating speed of the engine and, in turn, control the speed of the AV <b>602</b>. The gear <b>648</b><i>a </i>may be configured to control the gear selection of the transmission. The brake unit <b>648</b><i>b </i>can include any combination of mechanisms configured to decelerate the AV <b>602</b>. The brake unit <b>648</b><i>b </i>can use friction to slow the wheels in a standard manner. The brake unit <b>648</b><i>b </i>may include an Anti-lock brake system (ABS) that can prevent the brakes from locking up when the brakes are applied. The navigation unit <b>648</b><i>c </i>may be any system configured to determine a driving path or route for the AV <b>602</b>. The navigation <b>648</b><i>c </i>unit may additionally be configured to update the driving path dynamically while the AV <b>602</b> is in operation. In some embodiments, the navigation unit <b>648</b><i>c </i>may be configured to incorporate data from the GPS transceiver <b>646</b><i>q </i>and one or more predetermined maps so as to determine the driving path (e.g., along the roads <b>112</b>, <b>132</b> of <figref idref="DRAWINGS">FIG. <b>1</b>A-<b>1</b>C</figref>) for the AV <b>602</b>. The steering system <b>648</b><i>d </i>may represent any combination of mechanisms that may be operable to adjust the heading of AV <b>602</b> in an autonomous mode or in a driver-controlled mode.</p><p id="p-0165" num="0161">The autonomous control unit <b>648</b><i>e </i>may represent a control system configured to identify, evaluate, and avoid or otherwise negotiate potential obstacles or obstructions in the environment of the AV <b>602</b>. In general, the autonomous control unit <b>648</b><i>e </i>may be configured to control the AV <b>602</b> for operation without a driver or to provide driver assistance in controlling the AV <b>602</b>. In some embodiments, the autonomous control unit <b>648</b><i>e </i>may be configured to incorporate data from the GPS transceiver <b>646</b><i>g</i>, the Radar <b>646</b><i>b</i>, the LiDAR unit <b>646</b><i>f</i>, the cameras <b>646</b><i>a</i>, and/or other vehicle subsystems to determine the driving path or trajectory for the AV <b>602</b>.</p><p id="p-0166" num="0162">Many or all of the functions of the AV <b>602</b> can be controlled by the in-vehicle control computer <b>650</b>. The in-vehicle control computer <b>650</b> may include at least one data processor <b>670</b> (which can include at least one microprocessor) that executes processing instructions <b>680</b> stored in a non-transitory computer readable medium, such as the data storage device <b>690</b> or memory. The in-vehicle control computer <b>650</b> may also represent a plurality of computing devices that may serve to control individual components or subsystems of the AV <b>602</b> in a distributed fashion. In some embodiments, the data storage device <b>690</b> may contain processing instructions <b>680</b> (e.g., program logic) executable by the data processor <b>670</b> to perform various methods and/or functions of the AV <b>602</b>, including those described with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A-C</figref>, <b>2</b>, and <b>5</b>.</p><p id="p-0167" num="0163">The data storage device <b>690</b> may contain additional instructions as well, including instructions to transmit data to, receive data from, interact with, or control one or more of the vehicle drive subsystem <b>642</b>, the vehicle sensor subsystem <b>644</b>, and the vehicle control subsystem <b>648</b>. The in-vehicle control computer <b>650</b> can be configured to include a data processor <b>670</b> and a data storage device <b>690</b>. The in-vehicle control computer <b>650</b> may control the function of the AV <b>602</b> based on inputs received from various vehicle subsystems (e.g., the vehicle drive subsystem <b>642</b>, the vehicle sensor subsystem <b>644</b>, and the vehicle control subsystem <b>648</b>).</p><p id="p-0168" num="0164"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an exemplary system <b>700</b> for providing precise autonomous driving operations. The system <b>700</b> includes several modules that can operate in the in-vehicle control computer <b>650</b>, as described in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The in-vehicle control computer <b>650</b> includes a sensor fusion module <b>702</b> shown in the top left corner of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, where the sensor fusion module <b>702</b> may perform at least four image or signal processing operations. The sensor fusion module <b>702</b> can obtain images from cameras located on an autonomous vehicle to perform image segmentation <b>704</b> to detect the presence of moving objects (e.g., other vehicles, pedestrians, etc.) and/or static obstacles (e.g., stop sign, speed bump, terrain, etc.) located around the autonomous vehicle. The sensor fusion module <b>702</b> can obtain LiDAR point cloud data item from LiDAR sensors located on the autonomous vehicle to perform LiDAR segmentation <b>706</b> to detect the presence of objects and/or obstacles located around the autonomous vehicle.</p><p id="p-0169" num="0165">The sensor fusion module <b>702</b> can perform instance segmentation <b>708</b> on image and/or point cloud data item to identify an outline (e.g., boxes) around the objects and/or obstacles located around the autonomous vehicle. The sensor fusion module <b>702</b> can perform temporal fusion <b>710</b> where objects and/or obstacles from one image and/or one frame of point cloud data item are correlated with or associated with objects and/or obstacles from one or more images or frames subsequently received in time.</p><p id="p-0170" num="0166">The sensor fusion module <b>702</b> can fuse the objects and/or obstacles from the images obtained from the camera and/or point cloud data item obtained from the LiDAR sensors. For example, the sensor fusion module <b>702</b> may determine based on a location of two cameras that an image from one of the cameras comprising one half of a vehicle located in front of the autonomous vehicle is the same as the vehicle located captured by another camera. The sensor fusion module <b>702</b> sends the fused object information to the interference module <b>746</b> and the fused obstacle information to the occupancy grid module <b>760</b>. The in-vehicle control computer includes the occupancy grid module <b>760</b> which can retrieve landmarks from a map database <b>758</b> stored in the in-vehicle control computer. The occupancy grid module <b>760</b> can determine drivable areas and/or obstacles from the fused obstacles obtained from the sensor fusion module <b>702</b> and the landmarks stored in the map database <b>758</b>. For example, the occupancy grid module <b>760</b> can determine that a drivable area may include a speed bump obstacle.</p><p id="p-0171" num="0167">Below the sensor fusion module <b>702</b>, the in-vehicle control computer <b>650</b> includes a LiDAR based object detection module <b>712</b> that can perform object detection <b>716</b> based on point cloud data item obtained from the LiDAR sensors <b>714</b> located on the autonomous vehicle. The object detection <b>716</b> technique can provide a location (e.g., in <b>3</b>D world coordinates) of objects from the point cloud data item. Below the LiDAR based object detection module <b>712</b>, the in-vehicle control computer includes an image-based object detection module <b>718</b> that can perform object detection <b>724</b> based on images obtained from cameras <b>720</b> located on the autonomous vehicle. The object detection <b>724</b> technique can employ a deep machine learning technique <b>724</b> to provide a location (e.g., in <b>3</b>D world coordinates) of objects from the image provided by the camera <b>720</b>.</p><p id="p-0172" num="0168">The Radar <b>756</b> on the autonomous vehicle can scan an area in front of the autonomous vehicle or an area towards which the autonomous vehicle is driven. The Radar data is sent to the sensor fusion module <b>702</b> that can use the Radar data to correlate the objects and/or obstacles detected by the Radar <b>756</b> with the objects and/or obstacles detected from both the LiDAR point cloud data item and the camera image. The Radar data is also sent to the interference module <b>746</b> that can perform data processing on the Radar data to track objects by object tracking module <b>748</b> as further described below.</p><p id="p-0173" num="0169">The in-vehicle control computer includes an interference module <b>746</b> that receives the locations of the objects from the point cloud and the objects from the image, and the fused objects from the sensor fusion module <b>702</b>. The interference module <b>746</b> also receives the Radar data with which the interference module <b>746</b> can track objects by object tracking module <b>748</b> from one point cloud data item and one image obtained at one time instance to another (or the next) point cloud data item and another image obtained at another subsequent time instance.</p><p id="p-0174" num="0170">The interference module <b>746</b> may perform object attribute estimation <b>750</b> to estimate one or more attributes of an object detected in an image or point cloud data item. The one or more attributes of the object may include a type of object (e.g., pedestrian, car, or truck, etc.). The interference module <b>746</b> may perform behavior prediction <b>752</b> to estimate or predict motion pattern of an object detected in an image and/or a point cloud. The behavior prediction <b>752</b> can be performed to detect a location of an object in a set of images received at different points in time (e.g., sequential images) or in a set of point cloud data item received at different points in time (e.g., sequential point cloud data items). In some embodiments the behavior prediction <b>752</b> can be performed for each image received from a camera and/or each point cloud data item received from the LiDAR sensor. In some embodiments, the interference module <b>746</b> can be performed (e.g., run or executed) to reduce computational load by performing behavior prediction <b>752</b> on every other or after every pre-determined number of images received from a camera or point cloud data item received from the LiDAR sensor (e.g., after every two images or after every three point cloud data items).</p><p id="p-0175" num="0171">The behavior prediction <b>752</b> feature may determine the speed and direction of the objects that surround the autonomous vehicle from the Radar data, where the speed and direction information can be used to predict or determine motion patterns of objects. A motion pattern may comprise a predicted trajectory information of an object over a pre-determined length of time in the future after an image is received from a camera. Based on the motion pattern predicted, the interference module <b>746</b> may assign motion pattern situational tags to the objects (e.g., &#x201c;located at coordinates (x,y),&#x201d; &#x201c;stopped,&#x201d; &#x201c;driving at 50 mph,&#x201d; &#x201c;speeding up&#x201d; or &#x201c;slowing down&#x201d;). The situation tags can describe the motion pattern of the object. The interference module <b>746</b> sends the one or more object attributes (e.g., types of the objects) and motion pattern situational tags to the planning module <b>762</b>. The interference module <b>746</b> may perform an environment analysis <b>754</b> using any information acquired by system <b>700</b> and any number and combination of its components.</p><p id="p-0176" num="0172">The in-vehicle control computer includes the planning module <b>762</b> that receives the object attributes and motion pattern situational tags from the interference module <b>746</b>, the drivable area and/or obstacles, and the vehicle location and pose information from the fused localization module <b>726</b> (further described below).</p><p id="p-0177" num="0173">The planning module <b>762</b> can perform navigation planning <b>764</b> to determine a set of trajectories on which the autonomous vehicle can be driven. The set of trajectories can be determined based on the drivable area information, the one or more object attributes of objects, the motion pattern situational tags of the objects, location of the obstacles, and the drivable area information. In some embodiments, the navigation planning <b>764</b> may include determining an area next to the road where the autonomous vehicle can be safely parked in case of emergencies. The planning module <b>762</b> may include behavioral decision making <b>766</b> to determine driving actions (e.g., steering, braking, throttle) in response to determining changing conditions on the road (e.g., traffic light turned yellow, or the autonomous vehicle is in an unsafe driving condition because another vehicle drove in front of the autonomous vehicle and in a region within a pre-determined safe distance of the location of the autonomous vehicle). The planning module <b>762</b> performs trajectory generation <b>768</b> and selects a trajectory from the set of trajectories determined by the navigation planning operation <b>764</b>. The selected trajectory information is sent by the planning module <b>762</b> to the control module <b>770</b>.</p><p id="p-0178" num="0174">The in-vehicle control computer includes a control module <b>770</b> that receives the proposed trajectory from the planning module <b>762</b> and the autonomous vehicle location and pose from the fused localization module <b>726</b>. The control module <b>770</b> includes a system identifier <b>772</b>. The control module <b>770</b> can perform a model based trajectory refinement <b>774</b> to refine the proposed trajectory. For example, the control module <b>770</b> can applying a filtering (e.g., Kalman filter) to make the proposed trajectory data smooth and/or to minimize noise. The control module <b>770</b> may perform the robust control <b>776</b> by determining, based on the refined proposed trajectory information and current location and/or pose of the autonomous vehicle, an amount of brake pressure to apply, a steering angle, a throttle amount to control the speed of the vehicle, and/or a transmission gear. The control module <b>770</b> can send the determined brake pressure, steering angle, throttle amount, and/or transmission gear to one or more devices in the autonomous vehicle to control and facilitate precise driving operations of the autonomous vehicle.</p><p id="p-0179" num="0175">The deep image-based object detection <b>724</b> performed by the image based object detection module <b>718</b> can also be used detect landmarks (e.g., stop signs, speed bumps, etc.) on the road. The in-vehicle control computer includes a fused localization module <b>726</b> that obtains landmarks detected from images, the landmarks obtained from a map database <b>736</b> stored on the in-vehicle control computer, the landmarks detected from the point cloud data item by the LiDAR based object detection module <b>712</b>, the speed and displacement from the odometer sensor <b>744</b> and the estimated location of the autonomous vehicle from the GPS/IMU sensor <b>738</b> (i.e., GPS sensor <b>740</b> and IMU sensor <b>742</b>) located on or in the autonomous vehicle. Based on this information, the fused localization module <b>726</b> can perform a localization operation <b>728</b> to determine a location of the autonomous vehicle, which can be sent to the planning module <b>762</b> and the control module <b>770</b>.</p><p id="p-0180" num="0176">The fused localization module <b>726</b> can estimate pose <b>730</b> of the autonomous vehicle based on the GPS and/or IMU sensors <b>738</b>. The pose of the autonomous vehicle can be sent to the planning module <b>762</b> and the control module <b>770</b>. The fused localization module <b>726</b> can also estimate status (e.g., location, possible angle of movement) of the trailer unit based on (e.g., trailer status estimation <b>734</b>), for example, the information provided by the IMU sensor <b>742</b> (e.g., angular rate and/or linear velocity). The fused localization module <b>726</b> may also check the map content <b>732</b>.</p><p id="p-0181" num="0177"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows an exemplary block diagram of an in-vehicle control computer <b>650</b> included in an autonomous AV <b>602</b>. The in-vehicle control computer <b>650</b> includes at least one processor <b>804</b> and a memory <b>802</b> having instructions stored thereupon (e.g., software instructions <b>308</b>, <b>410</b>, and processing instructions <b>680</b> in <figref idref="DRAWINGS">FIGS. <b>4</b>, <b>5</b>, and <b>6</b></figref>, respectively). The instructions, upon execution by the processor <b>804</b>, configure the in-vehicle control computer <b>650</b> and/or the various modules of the in-vehicle control computer <b>650</b> to perform the operations described in <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>7</b></figref>. The transmitter <b>806</b> transmits or sends information or data to one or more devices in the autonomous vehicle. For example, the transmitter <b>806</b> can send an instruction to one or more motors of the steering wheel to steer the autonomous vehicle. The receiver <b>808</b> receives information or data transmitted or sent by one or more devices. For example, the receiver <b>808</b> receives a status of the current speed from the odometer sensor or the current transmission gear from the transmission. The transmitter <b>806</b> and receiver <b>808</b> are also configured to communicate with plurality of vehicle subsystems <b>640</b> and the in-vehicle control computer <b>650</b> described above in <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref>.</p><p id="p-0182" num="0178">While several embodiments have been provided in this disclosure, it should be understood that the disclosed systems and methods might be embodied in many other specific forms without departing from the spirit or scope of this disclosure. The present examples are to be considered as illustrative and not restrictive, and the intention is not to be limited to the details given herein. For example, the various elements or components may be combined or integrated in another system or certain features may be omitted, or not implemented.</p><p id="p-0183" num="0179">In addition, techniques, systems, subsystems, and methods described and illustrated in the various embodiments as discrete or separate may be combined or integrated with other systems, modules, techniques, or methods without departing from the scope of this disclosure. Other items shown or discussed as coupled or directly coupled or communicating with each other may be indirectly coupled or communicating through some interface, device, or intermediate component whether electrically, mechanically, or otherwise. Other examples of changes, substitutions, and alterations are ascertainable by one skilled in the art and could be made without departing from the spirit and scope disclosed herein.</p><p id="p-0184" num="0180">To aid the Patent Office, and any readers of any patent issued on this application in interpreting the claims appended hereto, applicants note that they do not intend any of the appended claims to invoke 35 U.S.C. &#xa7; 112(f) as it exists on the date of filing hereof unless the words &#x201c;means for&#x201d; or &#x201c;step for&#x201d; are explicitly used in the particular claim.</p><p id="p-0185" num="0181">Implementations of the disclosure can be described in view of the following clauses, the features of which can be combined in any reasonable manner.</p><p id="p-0186" num="0182">Clause 1. A system, comprising:</p><p id="p-0187" num="0183">an autonomous vehicle (AV) comprising at least one vehicle sensor located on the AV, wherein the AV is on a road;</p><p id="p-0188" num="0184">a control subsystem associated with the AV and comprising a first processor configured to:</p><p id="p-0189" num="0185">receive sensor data from the at least one vehicle sensor of the AV, wherein the sensor data comprises location coordinates of the AV;</p><p id="p-0190" num="0186">communicate the sensor data to an operation server;</p><p id="p-0191" num="0187">the operation server, communicatively coupled with the control subsystem, comprising:</p><p id="p-0192" num="0188">a memory operable to store login credentials of a user to an application by which the user is authorized to access information related to the AV; and</p><p id="p-0193" num="0189">a second processor, operably coupled with the memory, configured to:</p><p id="p-0194" num="0190">detect an unexpected event related to the AV from the sensor data, wherein the unexpected event comprises at least one of an accident, an inspection, and a report request related to the AV;</p><p id="p-0195" num="0191">receive a message from an electronic device associated with the user, wherein:</p><p id="p-0196" num="0192">the message comprises a request to access particular information regarding the AV; and</p><p id="p-0197" num="0193">the message further comprises location data of the user;</p><p id="p-0198" num="0194">determine whether the location data of the user matches the location coordinates of the AV;</p><p id="p-0199" num="0195">in response to determining that the location data of the user matches the location coordinates of the AV, associate the AV with the user;</p><p id="p-0200" num="0196">generate a ticket for the unexpected event to record events that will be carried out to address the unexpected event;</p><p id="p-0201" num="0197">establish a communication path between the user and a remote operator using the electronic device;</p><p id="p-0202" num="0198">communicate the particular information related to the AV to the electronic device via the communication path;</p><p id="p-0203" num="0199">receive a request from the user to provide assistance to address the unexpected event;</p><p id="p-0204" num="0200">provide instructions to the remote operator to forward to the user to address the unexpected event;</p><p id="p-0205" num="0201">determine whether the unexpected event is addressed; and</p><p id="p-0206" num="0202">in response to determining that the unexpected event is addressed, close the ticket.</p><p id="p-0207" num="0203">Clause 2. The system of Clause 1, wherein when the unexpected event comprises the accident involving the AV, providing instructions to the remote operator to forward to the user to address the unexpected event comprises:</p><p id="p-0208" num="0204">remotely granting the user entry to enter a cab of the AV;</p><p id="p-0209" num="0205">disengaging autonomous operations of the AV; and</p><p id="p-0210" num="0206">providing instructions to the remote operator to forward to the user to manually operate the AV without an ignition key.</p><p id="p-0211" num="0207">Clause 3. The system of Clause 2, wherein disengaging the autonomous operations of the AV comprises at least one of:</p><p id="p-0212" num="0208">remotely disengaging the autonomous operations of the AV by the second processor; and</p><p id="p-0213" num="0209">providing instructions to the remote operator to forward to the user to locally disengage the</p><p id="p-0214" num="0210">autonomous operations of the AV by an emergency stop button.</p><p id="p-0215" num="0211">Clause 4. The system of Clause 1, wherein the second processor is further configured to:</p><p id="p-0216" num="0212">determine whether it is safe to manually operate the AV based at least in part upon health diagnostics of the AV;</p><p id="p-0217" num="0213">in response to determining that it is safe to manually operate the AV, provide instruction to the remote operator to inform the user that it is safe to operate the AV; and</p><p id="p-0218" num="0214">in response to determining that it is not safe to manually operate the AV, dispatch a towing truck to move the AV.</p><p id="p-0219" num="0215">Clause 5. The system of Clause 1, wherein:</p><p id="p-0220" num="0216">the AV further comprises one or more user interfaces inside a cab of the AV comprising a speaker, a microphone, and a display screen; and</p><p id="p-0221" num="0217">the second processor is further configured to:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0218">establish a second communication path between the user and the remote operator using the one or more user interfaces; and</li>        <li id="ul0002-0002" num="0219">communicate the particular information related to the AV to the user via the second communication path.</li>    </ul>    </li></ul></p><p id="p-0222" num="0220">Clause 6. The system of Clause 1, wherein the assistance comprises at least one of:</p><p id="p-0223" num="0221">an emergency assist when the unexpected event comprises the accident involving the AV;</p><p id="p-0224" num="0222">an inspection assist when the unexpected event comprises the inspection of the AV, and</p><p id="p-0225" num="0223">a reporting assist when the unexpected event comprises the report request.</p><p id="p-0226" num="0224">Clause 7. The system of Clause 1, wherein when the unexpected event comprises the inspection of the AV, providing instructions to the remote operator to forward to the user to address the unexpected event comprises communicating information related to the AV requested by the user to the electronic device comprising at least one of:</p><p id="p-0227" num="0225">a first status report of autonomous faculties of the AV comprising one or more of autonomous functions, map data, routing plans, and sensor functions; and</p><p id="p-0228" num="0226">a second status report of physical characteristics of the AV comprising one or more of a digital weight of the AV, a cargo of the AV, and a digital temperature of an engine of the AV.</p><p id="p-0229" num="0227">Clause 8. The system of Clause 1, wherein when the unexpected event comprises the report request, providing instructions to the remote operator to forward to the user to address the unexpected event comprises communicating a driving history of the AV in a particular time range requested by the user to the electronic device, wherein the driving history comprises a speed of the AV.</p><p id="p-0230" num="0228">Clause 9. A method, comprising:</p><p id="p-0231" num="0229">detecting an unexpected event related to an autonomous vehicle (AV) from sensor data received from at least one vehicle sensor of the AV, wherein the unexpected event comprises at least one of an accident, an inspection, and a report request related to the AV;</p><p id="p-0232" num="0230">receiving a message from an electronic device associated with a user, wherein:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0231">the user is authenticated by verifying login credentials associated with the user on an application on the electronic device;</li>        <li id="ul0004-0002" num="0232">the message comprises a request to access particular information regarding the AV; and</li>        <li id="ul0004-0003" num="0233">the message further comprises location data of the user;</li>    </ul>    </li></ul></p><p id="p-0233" num="0234">determining whether the location data of the user matches the location coordinates of the AV;</p><p id="p-0234" num="0235">in response to determining that the location data of the user matches the location coordinates of the AV, associating the AV with the user;</p><p id="p-0235" num="0236">generating a ticket for the unexpected event to record events that will be carried out to address the unexpected event;</p><p id="p-0236" num="0237">establishing a communication path between the user and a remote operator using the electronic device;</p><p id="p-0237" num="0238">communicating the particular information related to the AV to the electronic device via the communication path;</p><p id="p-0238" num="0239">receiving a request from the user to provide assistance to address the unexpected event;</p><p id="p-0239" num="0240">providing instructions to the remote operator to forward to the user to address the unexpected event;</p><p id="p-0240" num="0241">determining whether the unexpected event is addressed; and</p><p id="p-0241" num="0242">in response to determining that the unexpected event is addressed, closing the ticket.</p><p id="p-0242" num="0243">Clause 10. The method of Clause 9, determining whether it is safe to manually operate the AV based at least in part upon health diagnostics of the AV;</p><p id="p-0243" num="0244">in response to determining that it is safe to manually operate the AV, providing instruction to the remote operator to inform the user that it is safe to operate the AV; and</p><p id="p-0244" num="0245">in response to determining that it is not safe to manually operate the AV, dispatching a towing truck to move the AV.</p><p id="p-0245" num="0246">Clause 11. The method of Clause 9, further comprising:</p><p id="p-0246" num="0247">establishing a second communication path between the user and the remote operator using one or more user interfaces inside a cab of the AV comprising a speaker, a microphone, and a display screen; and</p><p id="p-0247" num="0248">communicating the particular information related to the AV to the user via the second communication path.</p><p id="p-0248" num="0249">Clause 12. The method of Clause 9, further comprising:</p><p id="p-0249" num="0250">receiving one or more images from the environment surrounding the AV involved in the unexpected event from the user via the communication path;</p><p id="p-0250" num="0251">based at least in part upon the one or more images:<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0252">updating sensor data associated with the unexpected event; and</li>        <li id="ul0006-0002" num="0253">updating the instructions to forward to the user for addressing the unexpected event.</li>    </ul>    </li></ul></p><p id="p-0251" num="0254">Clause 13. The method of Clause 9, wherein when the unexpected event comprises the inspection of the AV, providing instructions to the remote operator to forward to the user to address the unexpected event comprises communicating information related to the AV requested by the user to the electronic device comprising at least one of:</p><p id="p-0252" num="0255">a first status report of autonomous faculties of the AV comprising one or more of autonomous functions, map data, routing plans, and sensor functions; and</p><p id="p-0253" num="0256">a second status report of physical characteristics of the AV comprising one or more of a digital weight of the AV, a cargo of the AV, and a digital temperature of an engine of the AV.</p><p id="p-0254" num="0257">Clause 14. The method of Clause 9, further comprising:</p><p id="p-0255" num="0258">receiving a confirmation signal from the user indicating that the unexpected event is addressed; and</p><p id="p-0256" num="0259">in response to receiving the confirmation signal, determining that the unexpected event is addressed.</p><p id="p-0257" num="0260">Clause 15. A computer program comprising executable instructions stored in a non-transitory computer-readable medium that when executed by one or more processors causes the one or more processors to:</p><p id="p-0258" num="0261">detect an unexpected event related to an autonomous vehicle (AV) from sensor data received from at least one vehicle sensor of the AV, wherein the unexpected event comprises at least one of an accident, an inspection, and a report request related to the AV;</p><p id="p-0259" num="0262">receive a message from an electronic device associated with a user, wherein:<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0263">the user is authenticated by verifying login credentials associated with the user on an application on the electronic device;</li>        <li id="ul0008-0002" num="0264">the message comprises a request to access particular information regarding the AV; and</li>        <li id="ul0008-0003" num="0265">the message further comprises location data of the user;</li>    </ul>    </li></ul></p><p id="p-0260" num="0266">determine whether the location data of the user matches the location coordinates of the AV;</p><p id="p-0261" num="0267">in response to determining that the location data of the user matches the location coordinates of the AV, associate the AV with the user;</p><p id="p-0262" num="0268">generate a ticket for the unexpected event to record events that will be carried out to address the unexpected event;</p><p id="p-0263" num="0269">establish a communication path between the user and a remote operator using the electronic device;</p><p id="p-0264" num="0270">communicate the particular information related to the AV to the electronic device via the communication path;</p><p id="p-0265" num="0271">receive a request from the user to provide assistance to address the unexpected event;</p><p id="p-0266" num="0272">provide instructions to the remote operator to forward to the user to address the unexpected event;</p><p id="p-0267" num="0273">determine whether the unexpected event is addressed; and</p><p id="p-0268" num="0274">in response to determining that the unexpected event is addressed, close the ticket.</p><p id="p-0269" num="0275">Clause 16. The computer program of Clause 15, wherein receiving the message from the electronic device associated with the user comprises at least one of:</p><p id="p-0270" num="0276">receive the message from an application installed on the electronic device, wherein the application is associated with an operation server that is in signal communication with the AV; and</p><p id="p-0271" num="0277">receive the message from a third-party application installed on the electronic device.</p><p id="p-0272" num="0278">Clause 17. The computer program of Clause 15, wherein when the unexpected event comprises the accident involving the AV, providing instructions to the remote operator to forward to the user to address the unexpected event comprises:</p><p id="p-0273" num="0279">remotely granting the user entry to enter a cab of the AV;</p><p id="p-0274" num="0280">disengaging autonomous operations of the AV; and</p><p id="p-0275" num="0281">providing instructions to the remote operator to forward to the user to manually operate the AV without an ignition key.</p><p id="p-0276" num="0282">Clause 18. The computer program of Clause 15, wherein the one or more processors are further configured to:</p><p id="p-0277" num="0283">determine whether it is safe to manually operate the AV based at least in part upon health diagnostics of the AV;</p><p id="p-0278" num="0284">in response to determining that it is safe to manually operate the AV, provide instruction to the remote operator to inform the user that it is safe to operate the AV; and</p><p id="p-0279" num="0285">in response to determining that it is not safe to manually operate the AV, dispatch a towing truck to move the AV.</p><p id="p-0280" num="0286">Clause 19. The computer program of Clause 15, wherein the one or more processors are further configured to:</p><p id="p-0281" num="0287">establish a second communication path between the user and the remote operator using one or more user interfaces inside a cab of the AV comprising a speaker, a microphone, and a display screen; and</p><p id="p-0282" num="0288">communicate the particular information related to the AV to the user via the second communication path.</p><p id="p-0283" num="0289">Clause 20. The computer program of Clause 15, wherein when the unexpected event comprises the inspection of the AV, providing instructions to the remote operator to forward to the user to address the unexpected event comprises communicating information related to the AV requested by the user to the electronic device comprising at least one of:</p><p id="p-0284" num="0290">a first status report of autonomous faculties of the AV comprising one or more of autonomous functions, map data, routing plans, and sensor functions; and</p><p id="p-0285" num="0291">a second status report of physical characteristics of the AV comprising one or more of a digital weight of the AV, a cargo of the AV, and a digital temperature of an engine of the AV.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system, comprising:<claim-text>an operation server comprising:<claim-text>a memory; and</claim-text><claim-text>a processor, operably coupled with the memory, configured to:<claim-text>detect an unexpected event related to the autonomous vehicle from the sensor data, wherein the unexpected event comprises at least one of an accident, an inspection, and a report request related to the autonomous vehicle; receive a message from an electronic device associated with the user, wherein:<claim-text>the message comprises a request to access particular information regarding the autonomous vehicle; and</claim-text><claim-text>the message further comprises location data of the user;</claim-text></claim-text><claim-text>receive location coordinates of the autonomous vehicle;</claim-text><claim-text>determine whether the location data of the user matches the location coordinates of the autonomous vehicle;</claim-text><claim-text>in response to determining that the location data of the user matches the location coordinates of the autonomous vehicle, associate the autonomous vehicle with the user;</claim-text><claim-text>establish a communication path between the user and a remote operator using the electronic device;</claim-text><claim-text>communicate the particular information related to the autonomous vehicle corresponding to the unexpected event to the electronic device;</claim-text><claim-text>receive a request from the user to provide assistance to address the unexpected event; and</claim-text><claim-text>provide instructions corresponding to the assistance to the remote operator to forward to the user via the communication path.</claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>when the unexpected event comprises the accident involving the autonomous vehicle, providing instructions to the remote operator to forward to the user to address the unexpected event comprises:</claim-text><claim-text>remotely granting the user entry to enter a cab of the autonomous vehicle;</claim-text><claim-text>disengaging autonomous operations of the autonomous vehicle; and</claim-text><claim-text>providing instructions to the remote operator to forward to the user to manually operate the autonomous vehicle without an ignition key.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein disengaging the autonomous operations of the autonomous vehicle comprises at least one of:<claim-text>remotely disengaging the autonomous operations of the autonomous vehicle by the second processor; or</claim-text><claim-text>providing instructions to the remote operator to forward to the user to locally disengage the autonomous operations of the autonomous vehicle by an emergency stop button.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second processor is further configured to:<claim-text>determine whether it is safe to manually operate the autonomous vehicle based at least in part upon health diagnostics of the autonomous vehicle;</claim-text><claim-text>in response to determining that it is safe to manually operate the autonomous vehicle, provide instruction to the remote operator to inform the user that it is safe to operate the autonomous vehicle; or</claim-text><claim-text>in response to determining that it is not safe to manually operate the autonomous vehicle, dispatch a towing truck to move the autonomous vehicle.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the autonomous vehicle further comprises one or more user interfaces inside a cab of the autonomous vehicle comprising a speaker, a microphone, and a display screen; and</claim-text><claim-text>the second processor is further configured to:<claim-text>establish a second communication path between the user and the remote operator using the one or more user interfaces; and</claim-text><claim-text>communicate the particular information related to the autonomous vehicle[to the user via the second communication path.</claim-text></claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the assistance comprises at least one of:<claim-text>an emergency assist when the unexpected event comprises the accident involving the autonomous vehicle,</claim-text><claim-text>an inspection assist when the unexpected event comprises the inspection of the autonomous vehicle, or</claim-text><claim-text>a reporting assist when the unexpected event comprises the report request.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein when the unexpected event comprises the inspection of the autonomous vehicle, providing instructions to the remote operator to forward to the user to address the unexpected event comprises communicating information related to the autonomous vehicle requested by the user to the electronic device comprising at least one of:<claim-text>a first status report of autonomous faculties of the autonomous vehicle comprising one or more of autonomous functions, map data, routing plans, and sensor functions; or</claim-text><claim-text>a second status report of physical characteristics of the autonomous vehicle comprising one or more of a digital weight of the autonomous vehicle, a cargo of the autonomous vehicle, and a digital temperature of an engine of the autonomous vehicle.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein when the unexpected event comprises the report request, providing instructions to the remote operator to forward to the user to address the unexpected event comprises communicating a driving history of the autonomous vehicle in a particular time range requested by the user to the electronic device, wherein the driving history comprises a speed of the autonomous vehicle.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A method, comprising:<claim-text>detecting an unexpected event related to an autonomous vehicle from sensor data received from at least one vehicle sensor of the autonomous vehicle, wherein the unexpected event comprises at least one of an accident, an inspection, and a report request related to the autonomous vehicle; receiving a message from an electronic device associated with a user, wherein:<claim-text>the message comprises a request to access particular information regarding an autonomous vehicle; and</claim-text><claim-text>the message further comprises location data of the user;</claim-text></claim-text><claim-text>receiving location coordinates of the autonomous vehicle;</claim-text><claim-text>determining whether the location data of the user matches the location coordinates of the autonomous vehicle;</claim-text><claim-text>in response to determining that the location data of the user matches the location coordinates of the autonomous vehicle, associating the autonomous vehicle with the user;</claim-text><claim-text>establishing a communication path between the user and a remote operator using the electronic device;</claim-text><claim-text>communicating the particular information related to the autonomous vehicle corresponding to the unexpected event to the electronic device;</claim-text><claim-text>receiving a request from the user to provide assistance to address the unexpected event; and</claim-text><claim-text>providing instructions corresponding to the assistance to the remote operator to forward to the user via the communication path.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>determining whether it is safe to manually operate the autonomous vehicle based at least in part upon health diagnostics of the autonomous vehicle;</claim-text><claim-text>in response to determining that it is safe to manually operate the autonomous vehicle, providing instruction to the remote operator to inform the user that it is safe to operate the autonomous vehicle; and</claim-text><claim-text>in response to determining that it is not safe to manually operate the autonomous vehicle, dispatching a towing truck to move the autonomous vehicle.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>establishing a second communication path between the user and the remote operator using one or more user interfaces inside a cab of the autonomous vehicle comprising a speaker, a microphone, and a display screen; and</claim-text><claim-text>communicating the particular information related to the autonomous vehicle to the user via the second communication path.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>receiving one or more images from the environment surrounding the autonomous vehicle involved in the unexpected event from the user via the communication path;</claim-text><claim-text>based at least in part upon the one or more images:<claim-text>updating sensor data associated with the unexpected event; and</claim-text><claim-text>updating the instructions to forward to the user for addressing the unexpected event.</claim-text></claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein when the unexpected event comprises the inspection of the autonomous vehicle, providing instructions to the remote operator to forward to the user to address the unexpected event comprises communicating information related to the autonomous vehicle requested by the user to the electronic device comprising at least one of:<claim-text>a first status report of autonomous faculties of the autonomous vehicle comprising one or more of autonomous functions, map data, routing plans, and sensor functions; or</claim-text><claim-text>a second status report of physical characteristics of the autonomous vehicle comprising one or more of a digital weight of the autonomous vehicle, a cargo of the autonomous vehicle, and a digital temperature of an engine of the autonomous vehicle.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>receiving a confirmation signal from the user indicating that the unexpected event is addressed; and</claim-text><claim-text>in response to receiving the confirmation signal, determining that the unexpected event is addressed.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory computer readable medium storing executable instructions that, when executed by one or more processors, causes the one or more processors to:<claim-text>detect an unexpected event related to an autonomous vehicle from sensor data received from at least one vehicle sensor of the autonomous vehicle, wherein the unexpected event comprises at least one of an accident, an inspection, and a report request related to the autonomous vehicle;</claim-text><claim-text>receive a message from an electronic device associated with a user, wherein:<claim-text>the message comprises a request to access particular information regarding the autonomous vehicle; and</claim-text><claim-text>the message further comprises location data of the user;</claim-text></claim-text><claim-text>receive location coordinates of the autonomous vehicle;</claim-text><claim-text>determine whether the location data of the user matches the location coordinates of the autonomous vehicle;</claim-text><claim-text>in response to determining that the location data of the user matches location coordinates of the autonomous vehicle, associate the autonomous vehicle with the user;</claim-text><claim-text>establish a communication path between the user and a remote operator using the electronic device;</claim-text><claim-text>communicate the particular information related to the autonomous vehicle corresponding to the unexpected event to the electronic device;</claim-text><claim-text>receive a request from the user to provide assistance to address the unexpected event; and</claim-text><claim-text>provide instructions corresponding to the assistance to the remote operator to forward to the user to address the unexpected event via the communication path.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein receiving the message from the electronic device associated with the user comprises at least one of:<claim-text>receive the message from an application installed on the electronic device, wherein the application is associated with an operation server that is in signal communication with the autonomous vehicle; or</claim-text><claim-text>receive the message from a third-party application installed on the electronic device.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein when the unexpected event comprises the accident involving the autonomous vehicle, providing instructions to the remote operator to forward to the user to address the unexpected event comprises:<claim-text>remotely granting the user entry to enter a cab of the autonomous vehicle;</claim-text><claim-text>disengaging autonomous operations of the autonomous vehicle; and</claim-text><claim-text>providing instructions to the remote operator to forward to the user to manually operate the autonomous vehicle without an ignition key.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the one or more processors further:<claim-text>determine whether it is safe to manually operate the autonomous vehicle based at least in part upon health diagnostics of the autonomous vehicle;</claim-text><claim-text>in response to determining that it is safe to manually operate the autonomous vehicle, provide instruction to the remote operator to inform the user that it is safe to operate the autonomous vehicle; and</claim-text><claim-text>in response to determining that it is not safe to manually operate the autonomous vehicle, dispatch a towing truck to move the autonomous vehicle.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the one or more processors further:<claim-text>establish a second communication path between the user and the remote operator using one or more user interfaces inside a cab of the autonomous vehicle comprising a speaker, a microphone, and a display screen; and</claim-text><claim-text>communicate the particular information related to the autonomous vehicle to the user via the second communication path.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein when the unexpected event comprises the inspection of the autonomous vehicle, providing instructions to the remote operator to forward to the user to address the unexpected event comprises communicating information related to the autonomous vehicle requested by the user to the electronic device comprising at least one of:<claim-text>a first status report of autonomous faculties of the autonomous vehicle comprising one or more of autonomous functions, map data, routing plans, and sensor functions; or</claim-text><claim-text>a second status report of physical characteristics of the autonomous vehicle comprising one or more of a digital weight of the autonomous vehicle, a cargo of the autonomous vehicle, and a digital temperature of an engine of the autonomous vehicle.</claim-text></claim-text></claim></claims></us-patent-application>