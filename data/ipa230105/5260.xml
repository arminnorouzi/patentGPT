<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005261A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005261</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17727265</doc-number><date>20220422</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>10</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>52</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>764</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>194</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>52</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>188</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>764</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">OBJECT DETECTION OVER WATER USING NORMALIZED DIFFERENCE VEGETATION INDEX SYSTEM AND METHOD</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63217435</doc-number><date>20210701</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>The Johns Hopkins University</orgname><address><city>Baltimore</city><state>MD</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Kolodner</last-name><first-name>Marc A.</first-name><address><city>Gaithersburg</city><state>MD</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Hagstrom</last-name><first-name>Shea T.</first-name><address><city>Columbia</city><state>MD</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Morris</last-name><first-name>Leo A.</first-name><address><city>Severna Park</city><state>MD</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method and a system for object detection over water are provided. The method includes acquiring, from a storage system, image data associated with a target area. The image data includes radiance of a plurality of pixels in a first spectral band and in a second spectral band. The method also includes determining a metric corresponding to a pixel of the plurality of pixels as a function of the radiance in the first spectral band and the radiance in the second spectral band and detecting an object in the target area in response to a determination that the metric satisfies a criteria.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="133.94mm" wi="158.75mm" file="US20230005261A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="161.29mm" wi="163.66mm" file="US20230005261A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="139.36mm" wi="161.63mm" file="US20230005261A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="192.96mm" wi="128.95mm" file="US20230005261A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="215.73mm" wi="124.88mm" file="US20230005261A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="109.90mm" wi="131.15mm" file="US20230005261A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="214.88mm" wi="158.58mm" orientation="landscape" file="US20230005261A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="194.90mm" wi="165.86mm" file="US20230005261A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="141.31mm" wi="109.64mm" file="US20230005261A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="222.50mm" wi="147.07mm" file="US20230005261A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Application No. 63/217,435 filed on Jul. 1, 2021, the entire contents of which are hereby incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?federal-research-statement description="Federal Research Statement" end="lead"?><heading id="h-0002" level="1">STATEMENT OF GOVERNMENTAL INTEREST</heading><p id="p-0003" num="0002">This invention was made with Government support under contract number 70B02C19C00000093 awarded by the Department of Homeland Security. The Government has certain rights in the invention.</p><?federal-research-statement description="Federal Research Statement" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0003" level="1">FIELD</heading><p id="p-0004" num="0003">The present disclosure relates to object detection over water, for example, a system and a method for object detection using normalized difference vegetation index.</p><heading id="h-0004" level="1">BACKGROUND</heading><p id="p-0005" num="0004">Detection of objects over water has many applications including search and rescue, monitoring and surveillance, and security surveillance. For example, vessel detection is used in maritime security and surveillance applications (e.g., fishing vessels). Various images are used to survey and monitor scenes (e.g., oceans, seas, lakes). Such detection may be time consuming as it may involve intervention of a user or extensive model training. In addition, current techniques suffer from high rate of false detections due to atmospheric conditions while acquiring the images. These delays in processing can be significant enough such that a search or rescue mission or a surveillance mission may fail. What is needed are systems and methods to address the aforementioned problems, and to provide improved techniques for accurate and rapid object detection and localization.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0006" num="0005">Accordingly, it is desirable to improve detection of objects over water to deliver enhanced capabilities while mitigating adverse impact of processing time and the like.</p><p id="p-0007" num="0006">In some embodiments, a method includes acquiring, from a storage system, image data associated with a target area. The image data includes radiance of a plurality of pixels in a first spectral band and in a second spectral band. The method also includes determining a metric corresponding to a pixel of the plurality of pixels as a function of the radiance in the first spectral band and the radiance in the second spectral band and detecting an object in the target area in response to a determination that the metric satisfies a criteria.</p><p id="p-0008" num="0007">In some embodiments, a system includes a memory and a processor. The memory is configured to store image data associated with a target area. The image data includes radiance of a plurality of pixels in a first spectral band and in a second spectral band. The processor is configured to determine a metric corresponding to a pixel as a function of the radiance in the first spectral band and the radiance in the second spectral band, and detect an object in the target area in response to a determination that the metric satisfies a criteria.</p><p id="p-0009" num="0008">Further features of the present disclosure, as well as the structure and operation of various embodiments, are described in detail below with reference to the accompanying drawings. It is noted that the present disclosure is not limited to the specific embodiments described herein. Such embodiments are presented herein for illustrative purposes only. Additional embodiments will be apparent to persons skilled in the relevant art(s) based on the teachings contained herein.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS/FIGURES</heading><p id="p-0010" num="0009">The accompanying drawings, which are incorporated herein and form part of the specification, illustrate the present disclosure and, together with the description, further serve to explain the principles of the present disclosure and to enable a person skilled in the relevant art(s) to make and use embodiments described herein.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic that illustrates a system for detecting an object, according to some embodiments.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a graph that illustrates spectra at multiple bands for a vessel and a body of water, according to some embodiments.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart for a method for detecting an object, according to some embodiments.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart for a method for detecting an object, according to some embodiments.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic for a graphical user interface for the system for detecting an object, according to some embodiments.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic for a graphical user interface for the system for detecting an object, according to some embodiments.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic that shows an output of the system for detecting an object, according to some embodiments.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a normalized difference vegetation index (NDVI) image, according to some embodiments.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows a computer system for implementing various embodiments of this disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0020" num="0019">The features of the present disclosure will become more apparent from the detailed description set forth below when taken in conjunction with the drawings, in which like reference characters identify corresponding elements throughout. In the drawings, like reference numbers generally indicate identical, functionally similar, and/or structurally similar elements. Additionally, generally, the left-most digit(s) of a reference number identifies the drawing in which the reference number first appears. Unless otherwise indicated, the drawings provided throughout the disclosure should not be interpreted as to-scale drawings.</p><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0021" num="0020">This specification discloses one or more embodiments that incorporate the features of the present disclosure. The disclosed embodiment(s) are provided as examples. The scope of the present disclosure is not limited to the disclosed embodiment(s). Claimed features are defined by the claims appended hereto.</p><p id="p-0022" num="0021">The embodiment(s) described, and references in the specification to &#x201c;one embodiment,&#x201d; &#x201c;an embodiment,&#x201d; &#x201c;an example embodiment,&#x201d; etc., indicate that the embodiment(s) described may include a particular feature, structure, or characteristic, but every embodiment may not necessarily include the particular feature, structure, or characteristic. Moreover, such phrases are not necessarily referring to the same embodiment. Further, when a particular feature, structure, or characteristic is described in connection with an embodiment, it is understood that it is within the knowledge of one skilled in the art to effect such feature, structure, or characteristic in connection with other embodiments whether or not explicitly described.</p><p id="p-0023" num="0022">Spatially relative terms, such as &#x201c;beneath,&#x201d; &#x201c;below,&#x201d; &#x201c;lower,&#x201d; &#x201c;above,&#x201d; &#x201c;on,&#x201d; &#x201c;upper&#x201d; and the like, may be used herein for ease of description to describe one element or feature's relationship to another element(s) or feature(s) as illustrated in the figures. The spatially relative terms are intended to encompass different orientations of the device in use or operation in addition to the orientation depicted in the figures. The apparatus may be otherwise oriented (rotated 90 degrees or at other orientations) and the spatially relative descriptors used herein may likewise be interpreted accordingly.</p><p id="p-0024" num="0023">The term &#x201c;about,&#x201d; &#x201c;approximately,&#x201d; or the like may be used herein to indicate a value of a quantity that may vary or be found to be within a range of values, based on a particular technology. Based on the particular technology, the terms may indicate a value of a given quantity that is within, for example, 1-20% of the value (e.g., &#xb1;1%, &#xb1;5%&#xb1;10%, &#xb1;15%, or &#xb1;20% of the value).</p><p id="p-0025" num="0024">Embodiments of the disclosure may be implemented in hardware, firmware, software, or any combination thereof. Embodiments of the disclosure may also be implemented as instructions stored on a machine-readable medium, which may be read and executed by one or more processors. A machine-readable medium may include any mechanism for storing or transmitting information in a form readable by a machine (e.g., a computing device). For example, a machine-readable medium may include read only memory (ROM); random access memory (RAM); magnetic disk storage media; optical storage media; flash memory devices; electrical, optical, acoustical or other forms of propagated signals (e.g., carrier waves, infrared signals, digital signals, etc.), and others. Further, firmware, software, routines, and/or instructions may be described herein as performing certain actions. However, it should be appreciated that such descriptions are merely for convenience and that such actions in fact result from computing devices, processors, controllers, or other devices executing the firmware, software, routines, instructions, etc. In the context of computer storage media, the term &#x201c;non-transitory&#x201d; may be used herein to describe all forms of computer readable media, with the sole exception being a transitory, propagating signal.</p><p id="p-0026" num="0025">Embodiments of the present disclosure are directed to object detection using normalized difference vegetation index (NDVI). Embodiments described herein may be used to, for example, detect an object and determine a location of the object using imagery data while reducing processing time. In some aspects, approaches described herein may be used to detect one or more objects in a body of water such as an ocean or a lake.</p><p id="p-0027" num="0026">In some embodiments, the object may comprise a vessel. The systems and methods described herein may be used to locate the vessel (i.e., determine a location of the vessel) during a search and rescue operation. For example, when the vessel is unable to communicate with a third party (e.g., lost communication), the imagery data may be used to locate the vessel. In some embodiments, a size of the vessel may be in the range from about 6 meters to about 10 meters. The resolution of the imagery data may affect a minimum detectable length. That is, a size of a pixel of the image may be equal to the minimum detectable length of the object.</p><p id="p-0028" num="0027">In some aspects, extensive testing can be performed to determine a robust capability with a high probability of detection and low false alarm rate to spectrally detect vessels in the open ocean among varying surface and atmospheric conditions including water currents, white caps, thin clouds, and cloud shadows.</p><p id="p-0029" num="0028">Different materials have unique spectral reflectance. For example, healthy vegetation has a strong NDVI due to a red-edge increase in a near infrared (NIR) intensity. Thus, the NDVI may be used to calculate vegetation coverage. In some aspects, the detection of the object is based on a NDVI. While healthy vegetation has a strong NDVI due to a &#x201c;red-edge&#x201d; increase in the NIR intensity, man-made surfaces/materials found on some objects may have a measurable effect on the NIR band intensity relative to the red band intensity. For example, hydrocarbon materials found on vessels such as paints, polymers, and canopies have a measurable effect on the NIR band intensity relative to the red band intensity.</p><p id="p-0030" num="0029">The imagery data may be used determine the NDVI. The imagery data may be captured using a plurality of techniques. For example, the imagery data may be captured using satellite imagery techniques. The satellite imagery techniques may comprise multispectral imagery (MSI), panchromatic imagery (PAN), and the like. Exemplary commercial space imaging satellites may include Worldview 2, GeoEye1, Worldview 3, and the SkySat constellation. In some aspects, the imagery data may be, for example, an electronic file in a geo tagged image file format (GeoTIFF) or in a National Imagery Transmission Format (NITF) (or other electronic format that may be downloaded from a data server).</p><p id="p-0031" num="0030">The MSI may include several bands (e.g., blue band, green band, red band, NIR band). The red band may range from about 0.63 to about 0.69 microns and the NIR band may range from about 0.77 to 0.89 about microns.</p><p id="p-0032" num="0031">The workflow described herein may also include a sequence of steps including a sun glint filter (to help remove images with large specular reflections that prevent robust vessel detection), a minimum NIR amplitude requirement (to help remove potential false alarms due to shadowing from white caps), and/or a contiguous pixel detection requirement (to help remove remaining single pixel false alarms from other effects such as sensor artifacts).</p><p id="p-0033" num="0032">In some embodiments, the systems and methods described herein may perform batch processing on sequences of large satellite imagery collections in minutes, generating output that is seamlessly and clearly displayed in geographic information system (GIS) platforms.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic that illustrates a system <b>100</b> for detecting an object <b>108</b>, according to some embodiments. The system <b>100</b> may comprise a sensor <b>102</b>, a computing device <b>104</b>, and a database <b>106</b> (e.g., a storage system, data server, imagery server).</p><p id="p-0035" num="0034">A sensor <b>102</b> may capture an image of a target area <b>110</b>. The sensor <b>102</b> may be flown or positioned over the target area <b>110</b> on a satellite, an aircraft, a drone, or the like. The sensor <b>102</b> can comprise a plurality of image capture devices and/or a single device configured to capture images of the target area <b>110</b> in two or more spectral bands. The sensor <b>102</b> may be a multispectral imaging camera configured to capture one or more images in two or more spectral bands (i.e., multiband image). The image may be composed of a plurality of pixels. Each pixel of the plurality of pixels may have a radiance in two or more spectral bands (e.g., in the red band and in the NIR band). The sensor <b>102</b> can record the imagery data. The sensor <b>102</b> may transmit the imagery data to the database <b>106</b> via a network <b>112</b>.</p><p id="p-0036" num="0035">In some embodiments, the database <b>106</b> may be one or more databases or data sources such as a data lake. Although, only a single database <b>106</b> is shown, system <b>100</b> may include any number of databases or data sources. The database <b>106</b> may receive imagery data from the sensor <b>106</b> and store the imagery data. In some aspects, the database <b>106</b> can download/fetch the imagery data from the sensor <b>102</b> at preset intervals. The database <b>106</b> may store multiple imagery data associated with the target area. For example, the imagery data may include imagery data collected at different times/date and/or from different types of sensors. For example, the imagery data may include panchromatic imagery (PAN) along with MSI imagery data or hyperspectral imagery (HSI) data. The database <b>106</b> may associate the MSI imagery data for the target area <b>110</b> with the imagery data. In some aspects, the MSI imagery data and the PAN may have the same file name with a different extension.</p><p id="p-0037" num="0036">The database <b>106</b> may be communicatively coupled to the computing device <b>104</b>. The computing device <b>104</b> can retrieve the imagery data from the database <b>106</b> and process the imagery data associated with the target area <b>110</b>. In one aspect, the computing device <b>104</b> may download the imagery data via file transfer protocol (FTP). In some aspects, the imagery data may be processed within a predetermined time period, e.g., less than four hours after the imagery data is captured. The computing device <b>104</b> may detect the object <b>108</b> in the target area <b>110</b> based on the NDVI. The NDVI may be determined from the MSI imagery data as discussed further below.</p><p id="p-0038" num="0037">In some aspects, functions of the computing device <b>104</b> may be implemented on one or more servers. The servers may be a variety of centralized servers and/or may be a variety of centralized or decentralized computing devices. For example, a server may be a mobile device, a laptop computer, a desktop computer, grid-computing resources, a virtualized computing resource, cloud-computing resources, peer-to-peer distributed computing devices, a server farm, or a combination thereof. The servers may be centralized in a single room, distributed across different rooms, distributed across different geographic locations, or embedded within the network <b>112</b>.</p><p id="p-0039" num="0038">In some embodiment, the network <b>112</b> refers to a telecommunications network, such as a wired or wireless network. The network <b>112</b> can span and represent a variety of networks and network topologies. For example, the network <b>112</b> can include wireless communication, wired communication, optical communication, ultrasonic communication, or a combination thereof. For example, satellite communication, cellular communication, Bluetooth, Infrared Data Association standard (IrDA), wireless fidelity (WiFi), and worldwide interoperability for microwave access (WiMAX) are examples of wireless communication that may be included in the network <b>112</b>. Cable, Ethernet, digital subscriber line (DSL), fiber optic lines, fiber to the home (FTTH), and plain old telephone service (POTS) are examples of wired communication that may be included in the network <b>112</b>. Further, the network <b>112</b> can traverse a number of topologies and distances. For example, the network <b>112</b> can include a direct connection, personal area network, local area network (LAN), metropolitan area network (MAN), wide area network (WAN), or a combination thereof.</p><p id="p-0040" num="0039">After the computing device <b>104</b> detects one or more objects, the computing device <b>104</b> may output a representation of the object along with the imagery data. In some embodiments, the detected object may be output with one or more images collected at different times or from different kinds of sensors (e.g., hyperspectral, long wave infrared (LWIR)). In some embodiments, the computing device <b>104</b> may retrieve from the database <b>106</b> an MSI image and a PAN image of the target area <b>110</b>. The computing device <b>104</b> may output the PAN image with the MSI image when the object <b>108</b> is detected in the target area <b>110</b>.</p><p id="p-0041" num="0040">Panchromatic imagery may have a higher spatial resolution compared to MSI imagery (e.g., from about 30 cm to several meters), but MSI may have a higher spectral resolution compared to PAN. In some aspects, PAN imagery may be a grayscale. In some aspects, MSI images may be combined with the PAN image using a PAN sharpening technique. PAN sharpening techniques may combine two or more bands from the multispectral image with the high spatial resolution PAN image to produce an image that has spatial and spectral properties of both image types. The PAN sharpened image may be output along with an indication of a location of the detected object.</p><p id="p-0042" num="0041">In some embodiments, the computing device <b>104</b> may download multiple images covering the target area <b>110</b>. For example, twenty images covering the target area <b>110</b> may be acquired. Imagery data is downloaded from the database <b>106</b> and analyzed in parallel or sequentially by the computing device <b>104</b>. The imagery data may be downloaded with a corresponding metadata file. In some aspects, a user may use a grid map to select the imagery data to be downloaded in response to a search or rescue mission. The computing device <b>104</b> may also be configured to receive an input from the user. The input may include a set of threshold values used to detect the object and minimize false positive detection as described further below.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a graph <b>200</b> that illustrates spectra at multiple bands for a vessel and a body of water (e.g., ocean), according to some embodiments. For example, curve <b>202</b> shows the spectra associated with the vessel and curve <b>204</b> shows the spectra associated with the body of water. In one aspect, the spectra is shown from a blue band to the NIR band (i.e., from about 500 nm to about 900 nm). For example, curve <b>202</b> shows there is a flattening effect in the NIR for the vessel compared to the body of water (curve <b>204</b>). That is, the radiance in the NIR band may remain substantially flat for the vessel. However, the radiance in the NIR band for the body of water may have a non-flat response. This difference in the spectral response between the body of water and the vessel may provide a difference in the NDVI, which may be used to help detect the presence of an object (e.g., a boat, vessel, etc.) in a body of water.</p><p id="p-0044" num="0043">The flattening effect in the NIR for a vessel may primarily be due to dielectric properties and solar reflectivity of the hydrocarbon materials (e.g. polymer plastic) that reside on a surface of the vessel. This may result in a detectable difference in the NDVI for a vessel when compared to the body of water. <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a flowchart for a method <b>300</b> for detecting an object, according to some embodiments.</p><p id="p-0045" num="0044">In one aspect, in operation <b>302</b>, imagery data associated with a target area is acquired. The imagery data may comprise radiance of a plurality of pixels in a first spectral band and in a second spectral band. In some embodiments, the first spectral band and the second spectral band may be selected such as the spectra of a potential object and the spectra of the open area where a potential object may be located diverge. That is, the spectral response of an object and the spectral response of the open area in one or more bands are not identical. In some embodiments, the first spectral band can comprise a near infrared (NIR) band, and the second spectral band can comprise a red band.</p><p id="p-0046" num="0045">In some embodiments, one or more images (e.g., two-dimensional images) may be acquired. The images may be obtained externally via a network, such as network <b>926</b> with respect to <figref idref="DRAWINGS">FIG. <b>9</b></figref> or network <b>112</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Additionally or alternatively, the images may be obtained from a library or a database, such as the memory <b>910</b> as described with respect to <figref idref="DRAWINGS">FIG. <b>9</b></figref> or database <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0047" num="0046">In one aspect, in operation <b>304</b>, a metric corresponding to a pixel of the plurality of pixels is determined based on the imagery data. The metric may be based on the radiance in the first spectral band and the radiance in the second spectral band. The metric may correspond to the NDVI. The NDVI may be expressed as:</p><p id="p-0048" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>NDVI=(NIR&#x2212;<i>R</i>)/(NIR+<i>R</i>)&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0049" num="0000">where NIR corresponds to the radiance in the NIR band and R corresponds to the radiance in the red band. An example NDVI image is shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The NDVI is determined for each pixel of the plurality of pixels using equation (1).</p><p id="p-0050" num="0047">In one aspect, in operation <b>306</b>, an object is detected in the target area when the metric satisfies a criterion. For example, the metric may be compared with a threshold. In some aspects, the criterion may be satisfied when the metric exceeds the threshold. In some embodiments, a threshold value may be determined based on processing of a plurality of imagery collections at a plurality of locations (i.e., empirically derived). For example, the threshold value may be varied and the threshold value that leads to an accurate detection level and low risk of false detection may be selected. In some aspects, the threshold value may be input by the user as described further below.</p><p id="p-0051" num="0048">In some embodiments, additional criteria may be used to reduce the risk of false detections. For example, a minimum contiguous detection and a minimum NIR may be used to reduce the risk of false detection.</p><p id="p-0052" num="0049">A set of contiguous pixels satisfying the criterion may be used in order to validate the detection (i.e., a pixel having the metric exceeding the threshold). That is, each metric corresponding to each pixel of the set of contiguous pixels exceeds the threshold. If, for example, the number of contiguous pixels is less than a contiguous pixels threshold than the detected object is classified as a false detection.</p><p id="p-0053" num="0050">In some embodiments, a total number of detected objects in a target area is determined. In some aspects, the detected objects are classified as false detection when the total number of the detected object exceeds a glint threshold (i.e., a maximum number of objects).</p><p id="p-0054" num="0051">In some embodiments, a radiance of the pixel in the first band is compared to a radiance threshold. For example, the radiance of the pixel associated with the detected object in the NIR band is compared to a NIR threshold. If the radiance of the pixel does not exceed the NIR threshold, the detected object may be classified as a false detection.</p><p id="p-0055" num="0052">In one aspect, in operation <b>308</b>, a determination is made whether additional imagery data are available. Imagery data may correspond to the previous target area or an additional target area. In some aspects, multiple images may cover the target area.</p><p id="p-0056" num="0053">In response to determining that there is additional imagery data available, i.e., YES at <b>308</b>, the process proceeds to step <b>304</b>.</p><p id="p-0057" num="0054">In response to determining that there is no more image data available, i.e., NO at <b>308</b>, the process proceeds to step <b>310</b>.</p><p id="p-0058" num="0055">In one aspect, in operation <b>310</b>, a detection image is output to the user. The detection image may be representative of metrics of the plurality of pixels. For example, the metric may correspond to the NDVI and the detection image may correspond to the NDVI image (e.g., NDVI image shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>). The detection image may correspond to one or more detected objects. In some aspects, the detected objects classified as a false detection are not output in the detection image. The detection image may change one or more attributes of the pixels corresponding to the detected object. In some aspects, the detected object may be represented by a pin.</p><p id="p-0059" num="0056">It should be understood that some of the above steps of the flow diagram of <figref idref="DRAWINGS">FIG. <b>3</b></figref> can be executed or performed in an order or sequence other than the order shown and described in the figure. Further, some of the above steps may be performed well in advance of other steps, or may be executed or performed substantially simultaneously or in parallel.</p><p id="p-0060" num="0057">The operations of method <b>300</b> are performed, for example, by system <b>100</b>, in accordance with aspects described above.</p><p id="p-0061" num="0058"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a flowchart for a method <b>400</b> for detecting an object, according to some embodiments.</p><p id="p-0062" num="0059">In one aspect, in operation <b>402</b>, imagery data (e.g., radiance or reflectance image) is acquired and are calibrated. Radiance image may refer to an intensity image. In some aspects, zero values along the edges of the radiance images are cropped. In some aspects, the imagery data may comprise spectral radiance for two or more bands. In some aspects, the imagery data may comprise an MSI data set. In some aspects, the MSI data set acquired from database <b>106</b> may be previously calibrated (i.e., calibrated before downloading the set from the database).</p><p id="p-0063" num="0060">When the MSI data set is not previously calibrated, the information used to calibrate the images is provided and acquired with the imagery data. For example, a set of coefficients used to calibrate the images is retrieved with the imagery data. A different coefficient may be used to calibrate raw data in each band of the two or more bands. Calibrating the raw data ensures that the bands are being compared correctly in radiance engineering units, for example, microWatts/(square cm-steradian-nm).</p><p id="p-0064" num="0061">In some embodiments, the radiance image may be provided as an electronic file in a geo tagged image file format (GeoTIFF) or in a national imagery transmission format (NITF). The electronic file may include the image data in a binary file format and at least one other file with information about the image file (e.g., geolocation information, satellite source, time of collection, and sun/satellite collection geometry).</p><p id="p-0065" num="0062">In some embodiments, the imagery data may include an image pair including panchromatic and multispectral images and associated metadata. The metadata may include a date and time associated with when the imagery data were acquired, a position of the sensor acquiring the imagery data, coordinates of the location being imaged, and other identifying information. The other identifying information may include one or more values used by the sensor for acquiring the imagery data (i.e., settings of the sensor).</p><p id="p-0066" num="0063">In one aspect, in operation <b>404</b>, an NDVI image is generated. The NDVI image may be generated by determining the NDVI for each pixel of the image. As described previously herein, the NDVI is determined as a function of the radiance in the red band and the radiance in the NIR band. For example, the NDVI for each pixel may be determined using equation (1). In some aspects, the NDVI may range from &#x2212;1 to +1. Negative values of NDVI may indicate water. Traditionally, an NDVI value close to +1, for example, may be associated with dense green leaves. A low reflectance in the red channel and a high reflectance in the NIR channel yield a high NDVI value and vice versa. Healthy vegetation (chlorophyll) reflects more NIR and green light compared to other wavelengths but it absorbs more red and blue light.</p><p id="p-0067" num="0064">In one aspect, in operation <b>406</b>, primary pixel detection is performed by applying a NDVI threshold to the NDVI image. For example, exceeding the NDVI threshold may indicate that the pixel corresponds to an object. The NDVI threshold may be set by the user or pre-stored in the system. In some aspects, the NDVI threshold may be a function of the slope of curve <b>202</b> in the NIR region. For example, a NDVI threshold of zero may indicate a flat change in the response of the object from the red band to the NIR band. A negative index may indicate a downward slope. A positive index may indicate an upward slope. The NDVI threshold may be negative, positive, or equal to zero and may be adjusted by the user.</p><p id="p-0068" num="0065">In some aspects, a range of NDVI threshold may be from about &#x2212;0.075 to about &#x2212;0.050. A higher number may reduce detections and false alarms.</p><p id="p-0069" num="0066">In one aspect, in operation <b>408</b>, a check may be performed to ensure that the images have no major glint issues due to specular reflections. For example, the number of pixels in the NDVI image having the NDVI exceeding the threshold is determined. In some aspects, if the number of pixels exceeds a glint threshold, then the image is not processed (i.e., is not used to detect objects). That is, none of the pixels that exceed the NDVI threshold is classified as an object. The high number of pixels exceeding the NDVI threshold may indicate that the imagery data have a glint issue and may not be suitable for object detection. In some embodiments, the glint threshold may range from about 500 pixels to about 1000 pixels. A lower number may reduce detections and false alarms. In some aspects, the glint threshold may be selected by the user based on an expected number of objects in the target area. For example, when the expected number of objects is small (e.g., less than 100) the user may select a glint threshold of 500 pixels.</p><p id="p-0070" num="0067">In some embodiments, additional images of the same area captured at a different time of the day may be processed (i.e., relative position of the sun and satellite is less susceptible for glint). In some aspects, the additional images may be captured before or after few hours from the imagery data. For example, when the image is not processed due to the glint issue, additional imagery data associated with the same target area are retrieved from the database <b>106</b> and processed.</p><p id="p-0071" num="0068">In addition to the glint filter, a filter may be used to filter out areas of land captured in the image. For example, when the number of pixels exceeds the glint threshold, the image may not be processed or the image may be cropped to remove a sub-area where a high number of pixels exceeds the glint threshold. The concentration of pixels exceeding the threshold may be indicative of an area of land.</p><p id="p-0072" num="0069">In one aspect, in operation <b>410</b>, a subset of primary pixels that are accompanied by contiguous detections is identified. Primary pixels may refer to each pixel having a metric that exceeds a primary NDVI threshold. For each primary pixel, the processor may check whether a contiguous pixel also exceeds a contiguous NDVI threshold. In some aspects, contiguous pixels may be defined as two or more pixels having at least a border and/or a corner in common. In some aspects, the pixels may be contiguous upward, downward, right and/or left. For each primary pixel, the number of contiguous pixels that exceeds the threshold is determined. In some aspects, only contiguous pixels are identified as objects. For example, two or more contiguous pixels may exceed the threshold (NDVI index threshold) for the object to be detected. The number of contiguous pixels may be compared to a contiguous pixels threshold (minimum contiguous pixels). When the number of contiguous pixels is less than the contiguous pixels threshold the primary pixel is classified as a false detection. In some aspects, this condition is removed and a single pixel exceeding the index may correspond to an object.</p><p id="p-0073" num="0070">The contiguous pixels threshold may be set by the user. The contiguous pixels threshold may be adjusted based on a type of the object to be detected. For example, an attribute (e.g., size) of the object may be input and the contiguous pixels threshold may be determined based on the input. In some aspects, an approximate length of the object may be input to computing device <b>104</b> and the computing device <b>104</b> may adjust the number of contiguous pixels based on the length. For example, if the size of the expected object (e.g., debris from an airplane) is relatively small (i.e., less than two pixels), the user may set the contiguous pixels threshold to one pixel. If the size of the expected object exceeds certain dimensions (i.e., more than two pixels) then the contiguous pixels threshold may be increased to minimize the risk of false detection. In some embodiments, contiguous pixels threshold may range from 2 to 3 contiguous pixels.</p><p id="p-0074" num="0071">In some embodiments, a minimum contiguous NDVI threshold may be equal to or different from the NDVI threshold used to detect the primary pixel (i.e., in <b>406</b>). In some aspects, a range of the minimum contiguous NDVI threshold may be from about &#x2212;0.1 to &#x2212;0.075. A higher number may reduce detections and false alarms.</p><p id="p-0075" num="0072">In one aspect, in operation <b>412</b>, a radiance of the primary pixel may be compared with a radiance threshold. For example, the radiance of the primary pixel in the NIR band may be compared with a minimum NIR amplitude. Any detection that does not meet the minimum NIR amplitude is removed. Due to conditions (e.g., atmospheric conditions) when the imagery data was captured, some pixels in the image may have a low NIR which may lead to a flat slope, that is an index (i.e., NDVI) exceeding the threshold value that in turn leads to a false positive detection. In some aspects, a range of the minimum NIR amplitude may be from about 2 to 3 microflicks. In some embodiments, the minimum NIR may be equal to 1 microflick. A higher number may reduce detections and false alarms.</p><p id="p-0076" num="0073">In one aspect, in operation <b>414</b> detections are verified using high resolution images (e.g., a PAN image). For each detection, a high resolution image comprising the detected object is retrieved. As described previously, each MSI image may have a corresponding high resolution image. The high resolution image may be presented to the user on a graphical user interface next to the MSI image. The user may compare the image and confirm the detection.</p><p id="p-0077" num="0074">The operations of method <b>400</b> are performed, for example, by system <b>100</b>, in accordance with aspects described above.</p><p id="p-0078" num="0075">It should be understood that some of the above steps of the flow diagram of <figref idref="DRAWINGS">FIG. <b>4</b></figref> can be executed or performed in an order or sequence other than the order shown and described in the figure. Further, some of the above steps may be performed well in advance of other steps, or may be executed or performed substantially simultaneously or in parallel.</p><p id="p-0079" num="0076"><figref idref="DRAWINGS">FIG. <b>5</b></figref> and <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrate exemplary user interfaces for system <b>100</b>, in accordance with some embodiments of the present disclosure. For example, user interfaces shown in <figref idref="DRAWINGS">FIGS. <b>5</b>-<b>6</b></figref> can be part of system <b>100</b> and configured to allow a user to interact with system <b>100</b> and the computer device <b>104</b>.</p><p id="p-0080" num="0077">Starting with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, upon login to the computer device <b>104</b>, the user may be presented with GUI <b>500</b>. The user may be presented with an input field <b>512</b>. Upon selecting the input field <b>512</b>, the user may be presented with a folder directory to select the imagery data.</p><p id="p-0081" num="0078">The user may be presented with a settings pane that includes one or more fields to enter the one or more thresholds described previously herein. A first field <b>502</b> may be used to input the NDVI threshold. A second field <b>504</b> may be used to input the minimum contiguous NDVI threshold. A third field <b>506</b> may be used to input a minimum NIR amplitude. A fourth field <b>508</b> may be used to input the minimum contiguous pixels. A fifth field <b>510</b> may be used to input the glint overload threshold. In some aspects, the one or more fields may be prepopulated with preset values.</p><p id="p-0082" num="0079">A control field <b>514</b> may be used to start the processing of the imagery data. Upon activation of the control field <b>514</b>, the user may be presented with user interface, e.g., graphical user interface (GUI) <b>600</b>, shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. GUI <b>600</b> may include a progress pane <b>618</b>. The progress pane <b>618</b> may output the number of files located in the folder selected. The number of objects detected in each file and the total number of detections.</p><p id="p-0083" num="0080">The progress pane <b>618</b> may show the file (i.e., MSI) being analyzed. If a file is skipped due to glint as discussed previously herein, a display attribute of the file name may be changed. For example, the name of the file may be displayed in color (e.g., in red). The attribute of the file may also be changed if a PAN file corresponding to the MSI file is not located. For example, the file name may be displayed in orange. The progress pane <b>618</b> may also display the total time of the simulation.</p><p id="p-0084" num="0081">In addition to the progress pane <b>618</b>, the GUI <b>600</b> may include a status pane <b>616</b>. The status pane <b>616</b> may show the status of the simulation (all files). For example, &#x201c;Done&#x201d; may be displayed when all imagery data has been processed. Another status pane (not shown) may show the completion status for the MSI file being analyzed.</p><p id="p-0085" num="0082">The example shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> corresponds to the simulation of two separate folders. The first folder includes one MSI file and seven objects were detected. The second folder includes five MSI files and seven objects were detected among the five MSI files. In addition, the progress pane <b>618</b> shows that detections from a file from the second folder were skipped due to the high number of detections.</p><p id="p-0086" num="0083"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic <b>700</b> that shows an output of the system <b>100</b> for detecting an object, in accordance with an embodiment of the present disclosure. The output may be in, for example, the form of a keyhole markup language (KML) or a compressed version of the KML (KMZ) output file.</p><p id="p-0087" num="0084">For each target area, a PAN image, a MSI image, and a detection image may be displayed. A detected object may be represented by a pin <b>704</b> on the map. The number of pins may match the total number of detections for each input folder. Associated with each pin <b>704</b> is a pane <b>702</b>. The pane <b>702</b> may include the file name for the detection, the latitude and longitude coordinates in decimal degrees, the number of contiguous pixels, and the approximate extent of the displayed map. In some embodiments, the latitude and longitude coordinates can be copied to another application. For example, the latitude and longitude coordinates may be copied by highlighting the coordinates with the left mouse button and copying with the right mouse button.</p><p id="p-0088" num="0085">In the detection map, the display attribute of the pixel may be indicative of the primary detection threshold associated with the pin, pixels meeting the contiguous detection threshold for the detection associated with the pin <b>704</b>, other pixels in the image display meeting the primary detection threshold, and other pixels in the image display meeting the contiguous detection threshold. The pane <b>702</b> may also include a legend for the detection map.</p><p id="p-0089" num="0086">Each image can be copied by right mouse clicking over the image of interest and pasting into any editor (e.g. MS Word or PowerPoint). The images are provided at a full pixel resolution and can be zoomed in to as needed. The PAN image may be centered and zoomed to the object.</p><p id="p-0090" num="0087">In some embodiments, a pan-sharpened image of the detection may be output. A user may input the latitude and longitude coordinates of the detection in a tool that has pan-sharpening capabilities such as global enhanced geospatial intelligence delivery (G-EGD), environmental systems research institute (ESRI), or RemoteView. Computing device <b>104</b> may also connect to the database <b>106</b> to download the pan-sharpened image corresponding to the latitude and longitude coordinates of the detection.</p><p id="p-0091" num="0088"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a NDVI image <b>800</b> of a target area, in accordance with an embodiment of the present disclosure. The target area may be a hazy area over a body of water having a size of &#x2154; of km by &#x2154; of km. For example, the area shows two fishing boats (i.e., about 27 feet in length) shown as white dots at the end of the black arrows. In this example, the detected objects, the white dots, have a higher NDVI than the background.</p><p id="p-0092" num="0089"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows a computer system <b>900</b>, according to some embodiments. Various embodiments and components therein can be implemented, for example, using computer system <b>900</b> or any other well-known computer systems. For example, the method steps of <figref idref="DRAWINGS">FIG. <b>3</b></figref> and <figref idref="DRAWINGS">FIG. <b>4</b></figref> may be implemented via computer system <b>900</b>.</p><p id="p-0093" num="0090">In some embodiments, computer system <b>900</b> may comprise one or more processors (also called central processing units, or CPUs), such as a processor <b>904</b>. Processor <b>904</b> may be connected to a communication infrastructure or bus <b>906</b>.</p><p id="p-0094" num="0091">In some embodiments, one or more processors <b>904</b> may each be a graphics processing unit (GPU). In an embodiment, a GPU is a processor that is a specialized electronic circuit designed to process mathematically intensive applications. The GPU may have a parallel structure that is efficient for parallel processing of large blocks of data, such as mathematically intensive data common to computer graphics applications, images, videos, etc.</p><p id="p-0095" num="0092">In some embodiments, computer system <b>900</b> may further comprise user input/output device(s) <b>903</b>, such as monitors, keyboards, pointing devices, etc., that communicate with communication infrastructure <b>906</b> through user input/output interface(s) <b>902</b>. Computer system <b>900</b> may further comprise a main or primary memory <b>908</b>, such as random access memory (RAM). Main memory <b>908</b> may comprise one or more levels of cache. Main memory <b>908</b> has stored therein control logic (i.e., computer software) and/or data.</p><p id="p-0096" num="0093">In some embodiments, computer system <b>900</b> may further comprise one or more secondary storage devices or memory <b>910</b>. Secondary memory <b>910</b> may comprise, for example, a hard disk drive <b>912</b> and/or a removable storage device or drive <b>914</b>. Removable storage drive <b>914</b> may be a floppy disk drive, a magnetic tape drive, a compact disk drive, an optical storage device, tape backup device, and/or any other storage device/drive. removable storage drive <b>914</b> may interact with a removable storage unit <b>918</b>. Removable storage unit <b>918</b> may comprise a computer usable or readable storage device having stored thereon computer software (control logic) and/or data. Removable storage unit <b>918</b> may be a floppy disk, magnetic tape, compact disk, DVD, optical storage disk, and/any other computer data storage device. Removable storage drive <b>914</b> reads from and/or writes to removable storage unit <b>918</b> in a well-known manner.</p><p id="p-0097" num="0094">In some embodiments, secondary memory <b>910</b> may comprise other means, instrumentalities or other approaches for allowing computer programs and/or other instructions and/or data to be accessed by computer system <b>900</b>. Such means, instrumentalities or other approaches may comprise, for example, a removable storage unit <b>922</b> and an interface <b>920</b>. Examples of the removable storage unit <b>922</b> and the interface <b>920</b> may comprise a program cartridge and cartridge interface (such as that found in video game devices), a removable memory chip (such as an EPROM or PROM) and associated socket, a memory stick and USB port, a memory card and associated memory card slot, and/or any other removable storage unit and associated interface.</p><p id="p-0098" num="0095">In some embodiments, computer system <b>900</b> may further comprise a communication or network interface <b>924</b>. Communication interface <b>924</b> enables computer system <b>900</b> to communicate and interact with any combination of remote devices, remote networks, remote entities, etc. (individually and collectively referenced by reference number <b>928</b>). For example, communication interface <b>924</b> may allow computer system <b>900</b> to communicate with remote devices <b>928</b> over communications path <b>926</b>, which may be wired and/or wireless, and which may comprise any combination of LANs, WANs, the Internet, etc. Control logic and/or data may be transmitted to and from computer system <b>900</b> via communications path <b>926</b>.</p><p id="p-0099" num="0096">In some embodiments, a non-transitory, tangible apparatus or article of manufacture comprising a non-transitory, tangible computer useable or readable medium having control logic (software) stored thereon is also referred to herein as a computer program product or program storage device. This includes, but is not limited to, computer system <b>900</b>, main memory <b>908</b>, secondary memory <b>910</b>, and removable storage units <b>918</b> and <b>922</b>, as well as tangible articles of manufacture embodying any combination of the foregoing. Such control logic, when executed by one or more data processing devices (such as computer system <b>900</b>), causes such data processing devices to operate as described herein.</p><p id="p-0100" num="0097">Based on the teachings contained in this disclosure, it will be apparent to those skilled in the relevant art(s) how to make and use embodiments of this disclosure using data processing devices, computer systems and/or computer architectures other than that shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. In particular, embodiments may operate with software, hardware, and/or operating system implementations other than those described herein.</p><p id="p-0101" num="0098">It is to be understood that the phraseology or terminology herein is for the purpose of description and not of limitation, such that the terminology or phraseology of the present disclosure is to be interpreted by those skilled in relevant art(s) in light of the teachings herein.</p><p id="p-0102" num="0099">It is to be appreciated that the Detailed Description section, and not the Summary and Abstract sections, is intended to be used to interpret the claims. The Summary and Abstract sections may set forth one or more but not all exemplary embodiments of the present disclosure as contemplated by the inventor(s), and thus, are not intended to limit the present disclosure and the appended claims in any way.</p><p id="p-0103" num="0100">The present disclosure has been described above with the aid of functional building blocks illustrating the implementation of specified functions and relationships thereof. The boundaries of these functional building blocks have been arbitrarily defined herein for the convenience of the description. Alternate boundaries can be defined so long as the specified functions and relationships thereof are appropriately performed.</p><p id="p-0104" num="0101">While specific embodiments of the disclosure have been described above, it will be appreciated that embodiments of the present disclosure may be practiced otherwise than as described. The descriptions are intended to be illustrative, not limiting. Thus it will be apparent to one skilled in the art that modifications may be made to the disclosure as described without departing from the scope of the claims set out below.</p><p id="p-0105" num="0102">The foregoing description of the specific embodiments will so fully reveal the general nature of the present disclosure that others can, by applying knowledge within the skill of the art, readily modify and/or adapt for various applications such specific embodiments, without undue experimentation, without departing from the general concept of the present disclosure. Therefore, such adaptations and modifications are intended to be within the meaning and range of equivalents of the disclosed embodiments, based on the teaching and guidance presented herein.</p><p id="p-0106" num="0103">The breadth and scope of the protected subject matter should not be limited by any of the above-described exemplary embodiments, but should be defined only in accordance with the following claims and their equivalents.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method, comprising:<claim-text>acquiring, from a storage system, image data associated with a target area over water, wherein the image data comprises radiance of a plurality of pixels in a first spectral band and in a second spectral band;</claim-text><claim-text>determining, using a processor, a metric corresponding to a pixel of the plurality of pixels as a function of the radiance in the first spectral band and the radiance in the second spectral band; and</claim-text><claim-text>detecting, using the processor, an object in the target area in response to a determination that the metric satisfies a criteria.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the first spectral band comprises a near infrared (NIR) band;</claim-text><claim-text>the second spectral band comprises a red band; and</claim-text><claim-text>the metric is a normalized difference vegetation index (NDVI).</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein:<claim-text>the NDVI is expressed as NDVI=(NIR&#x2212;R)/(NIR+R),</claim-text><claim-text>NIR corresponds to the radiance in the NIR band and R corresponds to the radiance in the red band.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>comparing the metric with a threshold; and</claim-text><claim-text>wherein the criteria is satisfied in response to the metric exceeding the threshold.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein:<claim-text>the detecting the object comprises identifying a set of contiguous pixels representing the object, and</claim-text><claim-text>each metric corresponding to each pixel of the set of contiguous pixels exceeds the threshold.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising:<claim-text>in response to a number of pixels in the set of the contiguous pixels being less than a contiguous pixels threshold, classifying the detected object as a false detection and removing the false detection.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>determining a total number of detected objects in the target area; and</claim-text><claim-text>in response to the total number of the detected objects exceeding a glint threshold, classifying the detected objects as false detections and removing the false detections.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>comparing the radiance of the pixel in the first spectral band with a radiance threshold; and</claim-text><claim-text>validating the detection when the radiance exceeds the radiance threshold.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>retrieving a high resolution image of an area comprising the detected object; and</claim-text><claim-text>displaying on a user interface the high resolution image along with a detection image, wherein the detection image is representative of metrics of the plurality of pixels.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the image data comprises multispectral (MSI) data from at least four bands and the high resolution image is a panchromatic image.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>retrieving the image data from the storage system; and</claim-text><claim-text>calibrating the image data based on at least information retrieved with the image data.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the image data corresponds to a plurality of target areas; and</claim-text><claim-text>the method further comprising repeating the determining and the detecting for the plurality of target areas.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>providing a user interface to a user; and</claim-text><claim-text>receiving via the user interface one or more thresholds, wherein the one or more thresholds are used to validate the detected object.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A system, comprising:<claim-text>a memory configured to store image data associated with a target area over water, wherein the image data comprises radiance of a plurality of pixels in a first spectral band and in a second spectral band; and</claim-text><claim-text>a processor configured to:</claim-text><claim-text>determine a metric corresponding to a pixel as a function of the radiance in the first spectral band and the radiance in the second spectral band, and</claim-text><claim-text>detect an object in the target area in response to a determination that the metric satisfies a criteria.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein:<claim-text>the first spectral band comprises a near infrared (NIR) band;</claim-text><claim-text>the second spectral band comprises a red band; and</claim-text><claim-text>the metric is a normalized difference vegetation index (NDVI).</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein:<claim-text>the processor is further configured to compare the metric with a threshold; and</claim-text><claim-text>the criteria is satisfied in response the metric exceeding the threshold.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein:<claim-text>the processor is further configured to:</claim-text><claim-text>detect a set of contiguous pixels representing the object, wherein each metric corresponding to each pixel of the set of contiguous pixels exceeds the threshold; and</claim-text><claim-text>in response to a number of pixels in the set of the contiguous pixels being less than a contiguous pixels threshold, classify the detected object as a false detection and remove the false detection.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the processor is further configured to:<claim-text>determine a total number of detected objects in the target area; and</claim-text><claim-text>in response to the total number of the detected objects exceeding a glint threshold, classify the detected objects as false detections and remove the false detections.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the processor is further configured to:<claim-text>compare the radiance of the pixel in the first band with a radiance threshold; and</claim-text><claim-text>validate the detection in response to the radiance exceeding the radiance threshold.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A non-transitory computer readable medium comprising stored instructions, which when executed by a processor, cause the processor to:<claim-text>acquire image data associated with a target area over water, wherein the image data comprises radiance of a plurality of pixels in a first spectral band and in a second spectral band;</claim-text><claim-text>determine a metric corresponding to a pixel as a function of the radiance in the first spectral band and the radiance in the second spectral band; and</claim-text><claim-text>detect an object in the target area in response to a determination that the metric satisfies a criteria.</claim-text></claim-text></claim></claims></us-patent-application>