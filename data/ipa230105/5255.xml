<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005256A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005256</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17940145</doc-number><date>20220908</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>13</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>64</class><subclass>C</subclass><main-group>39</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>08</class><subclass>G</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>08</class><subclass>B</subclass><main-group>25</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>08</class><subclass>B</subclass><main-group>13</main-group><subgroup>196</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>147</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>52</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>13</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>64</class><subclass>C</subclass><main-group>39</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>08</class><subclass>G</subclass><main-group>5</main-group><subgroup>0013</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>08</class><subclass>B</subclass><main-group>25</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>08</class><subclass>B</subclass><main-group>13</main-group><subgroup>1965</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>08</class><subclass>G</subclass><main-group>5</main-group><subgroup>0034</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>08</class><subclass>G</subclass><main-group>5</main-group><subgroup>0069</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>147</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>52</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>64</class><subclass>C</subclass><main-group>2201</main-group><subgroup>141</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>64</class><subclass>C</subclass><main-group>2201</main-group><subgroup>127</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">DRONE PRE-SURVEILLANCE</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17195297</doc-number><date>20210308</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11468668</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17940145</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16901255</doc-number><date>20200615</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10943113</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17195297</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15839710</doc-number><date>20171212</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10685227</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16901255</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62433019</doc-number><date>20161212</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Alarm.com Incorporated</orgname><address><city>Tysons</city><state>VA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Kerzner</last-name><first-name>Daniel Todd</first-name><address><city>McLean</city><state>VA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Trundle</last-name><first-name>Stephen Scott</first-name><address><city>Falls Church</city><state>VA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Methods, systems, and apparatus, including computer programs encoded on a storage device, for using a drone to pre-surveil a portion of a property. In one aspect, a system may include a monitoring unit. The monitoring unit may include a network interface, a processor, and a storage device that includes instructions to cause the processor to perform operations. The operations may include obtaining data that is indicative of one or more acts of an occupant of the property, applying the obtained data that is indicative of one or more acts of the occupant of the property to a pre-surveillance rule, determining that the pre-surveillance rule is satisfied, determining a drone navigation path that is associated with the pre-surveillance rule, transmitting, to a drone, an instruction to perform pre-surveillance of the portion of the one or more properties using the drone navigation path.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="116.59mm" wi="158.75mm" file="US20230005256A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="202.69mm" wi="154.86mm" orientation="landscape" file="US20230005256A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="242.49mm" wi="132.16mm" orientation="landscape" file="US20230005256A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="170.26mm" wi="154.69mm" file="US20230005256A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="208.79mm" wi="205.91mm" orientation="landscape" file="US20230005256A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="223.10mm" wi="165.78mm" file="US20230005256A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="146.56mm" wi="145.97mm" file="US20230005256A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="146.56mm" wi="145.97mm" file="US20230005256A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="168.49mm" wi="152.32mm" file="US20230005256A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of U.S. application Ser. No. 17/195,297, filed Mar. 8, 2021, which is a continuation of U.S. application Ser. No. 16/901,255, filed Jun. 15, 2020, now U.S. Pat. No. 10,943,113, issued Mar. 9, 2021, which is a continuation of U.S. application Ser. No. 15/839,710, filed Dec. 12, 2017, now U.S. Pat. No. 10,685,227, issued Jun. 16, 2020, which claims the benefit of the U.S. Provisional Patent Application No. 62/433,019 filed Dec. 12, 2016 and entitled &#x201c;Drone Pre-Surveillance.&#x201d; The complete disclosures of all of the above patent applications are hereby incorporated by reference in their entirety for all purposes.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">The use of robotic devices is starting to become more prevalent in a variety of different applications. By way of example, a robotic device may be used by a property monitoring system. In such systems, the robotic device may be configured to navigate to the location of a monitoring system sensor in response to the detection, by the monitoring system, of sensor data generated by the monitoring system sensor.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0004" num="0003">Techniques are described for a monitoring system that can use one or more robotic devices to pre-surveil one or more portions of a property based on learned user behavior patterns. The portions of the property pre-surveilled may include portions of the property that are inside the property or outside the property. In some implementations, after the monitoring system learns a particular pattern of user behavior, the monitoring system may use a robotic device such as a drone to pre-surveil the area the user is expected to travel to based on the particular pattern.</p><p id="p-0005" num="0004">The robotic device may include any type of robotic device such as robotic device that navigates on land, a robotic device that navigate on water, and a robotic device that navigates through the air. The property may include any type of property including a residential property, a commercial property, or an industrial property. For purposes of this disclosure, the term &#x201c;property&#x201d; may include any portion of a residential property, a commercial property, or an industrial property. A portion of a residential property, a commercial property, or an industrial property may be indoors, outdoors, and any surrounding area that is associated with the property such as yards, streets, sidewalks, parking lots, and the like.</p><p id="p-0006" num="0005">According to one innovative aspect of the present disclosure monitoring system is disclosed for using a drone to pre-surveil at least a portion of one or more properties. The monitoring system may include a plurality of sensors and a monitoring unit that is configured to monitor sensor data generated by one or more of the plurality of sensors. The monitoring unit may include a network interface, one or more processors, and one or more storage devices that include instructions that are operable, when executed by the one or more processors, to cause the one or more processors to perform operations. The operations may include obtaining data that is indicative of one or more acts of an occupant of the property, applying the obtained data that is indicative of one or more acts of the occupant of the property to a pre-surveillance rule that identifies a portion of one or more properties for pre-surveillance based on one or more actions of the occupant of the property indicating that the occupant of the property is expected to travel to the portion of the one or more properties, based on application of the obtained data to the pre-surveillance rule, determining that the pre-surveillance rule is satisfied, based on the determination that the pre-surveillance rule is satisfied, determining a drone navigation path that is associated with the pre-surveillance rule, and transmitting, to a drone, an instruction to perform pre-surveillance of the portion of the one or more properties using the drone navigation path.</p><p id="p-0007" num="0006">Other aspects include corresponding methods, apparatus, and computer programs to perform actions of methods defined by instructions encoded on computer storage devices.</p><p id="p-0008" num="0007">These and other versions may optionally include one or more of the following features. For example, in some implementations, obtaining data that is indicative of one or more acts of an occupant of the property may include obtaining data from one or more user devices that describes a location of the occupant of the property. In such implementations, applying the obtained data that is indicative of one or more acts of the occupant of the property to a pre-surveillance rule includes applying the obtained data from one or more user devices that describes the location of the occupant of the property to the pre-surveillance rule.</p><p id="p-0009" num="0008">In some implementations, obtaining data that is indicative of one or more acts of an occupant of the property may include obtaining data from one or more user devices that is indicative of a first location of the occupant of the property, obtaining data from the one or more user devices that is indicative of a second location of the occupant of the property, and determining, based on the first location and the second location, a change in the location of the occupant of the property. In such implementations, applying the obtained data that is indicative of one or more acts of the occupant of the property to a pre-surveillance rule includes applying data describing the determined change in the location of the occupant of the property to the pre-surveillance rule that identifies.</p><p id="p-0010" num="0009">In some implementations, obtaining data that is indicative of one or more acts of an occupant of the property may include obtaining sensor data generated by one or more of the plurality of sensors. In such implementations, applying the obtained data that is indicative of one or more acts of the occupant of the property to a pre-surveillance rule includes applying the obtained sensor data generated by one or more of the plurality of sensors to the pre-surveillance rule that identifies.</p><p id="p-0011" num="0010">In some implementations, obtaining data that is indicative of one or more acts of an occupant of the property may include obtaining sensor data generated by a first sensor of the plurality of sensors that is indicative of movement in a first portion of the property, obtaining sensor data generated by a second sensor of the plurality of sensors that is indicative of movement in a second, different portion of the property, and determining, based on the sensor data generated by the first sensor and the sensor data generated by the second sensor, a change in a location of the occupant of the property. In such implementations, applying the obtained data that is indicative of one or more acts of the occupant of the property to a pre-surveillance rule includes applying data describing the determined change in the location of the occupant of the property to the pre-surveillance rule.</p><p id="p-0012" num="0011">In some implementations, obtaining data that is indicative of one or more acts of an occupant of the property may include obtaining sensor data generated by one or more of the plurality of sensors that is indicative of movement in a first portion of the property, obtaining data from one or more user devices that is indicative of a location of the occupant of the property, and determining, based on the sensor data and the data from the one or more user devices, a change in the location of the occupant of the property. In such implementations, applying the obtained data that is indicative of one or more acts of the occupant of the property to a pre-surveillance rule includes applying data describing the determined change in the location of the occupant of the property to the pre-surveillance rule.</p><p id="p-0013" num="0012">In some implementations, determining that the pre-surveillance rule is satisfied may include determining, based on the obtained data, that the one or more acts of the occupant of the property occurred at a current time, and determining that the current time satisfies a trigger time defined by the pre-surveillance rule.</p><p id="p-0014" num="0013">In some implementations, determining that the pre-surveillance rule is satisfied may include determining, based on the obtained data, that the one or more acts of the occupant of the property occurred at a current time and determining that the occurrence of the one or more acts at the current time falls within a predetermined time range defined by the pre-surveillance rule.</p><p id="p-0015" num="0014">In some implementations, determining the drone navigation path that is associated with the pre-surveillance rule may include selecting a predetermined drone navigation path that is associated with the pre-surveillance rule.</p><p id="p-0016" num="0015">In some implementations, the portion of the one or more properties includes an indoor portion of the property, an outdoor portion of the property, or both.</p><p id="p-0017" num="0016">In some implementations, the portion of the one or more properties includes (i) an indoor portion of the property, (ii) an outdoor portion of the property, (iii) an outdoor portion of a different property other than the property, or (iv) any combination thereof.</p><p id="p-0018" num="0017">In some implementations, the portion of the one or more properties comprises a portion of a parking lot for the property.</p><p id="p-0019" num="0018">In some implementations, the plurality of sensors includes at least one of a motion sensor, a contact sensor, or a temperature sensor.</p><p id="p-0020" num="0019">In some implementations, the monitoring unit resides at a location that is remote from the property.</p><p id="p-0021" num="0020">In some implementations, the may further include a drone, wherein the drone comprises one or more second storage devices that include second instructions that are operable, when executed by the one or more second processors, to cause the one or more second processors to perform second operations. The second operations may include receiving, by the drone, the instruction transmitted by the monitoring unit to perform pre-surveillance of an area associated with the drone navigation path, navigating along the drone navigation path, obtaining video data of the area that is associated with the drone navigation path, and analyzing the obtained video data to determine a level of safeness of the area associated with the drone navigation path based on a one or more factors that comprise whether the drone identified a loitering person, a person with a weapon, a person wearing a mask, a person who has been issued an outstanding warrant for arrest, or a combination thereof.</p><p id="p-0022" num="0021">In some implementations, the operations may also include receiving, from the drone, data that provides an indication of a level of safeness of the area that is associated with the drone navigation path, wherein the level of safeness is based on one or more factors that include whether the drone identified a loitering person, a person with a weapon, a person wearing a mask, a person who has been issued an outstanding warrant for arrest, or a combination thereof.</p><p id="p-0023" num="0022">In some implementations, the data that provides an indication of the level of safeness that is associated with the drone navigation path includes a probability that the area is safe.</p><p id="p-0024" num="0023">In some implementations, the drone includes a flying quadcopter drone.</p><p id="p-0025" num="0024">The monitoring system described by the present disclosure provides multiple advantages over existing systems. For example, the monitoring system provides on-demand monitoring and pre-surveillance of portions of one or more properties without detection of an alarm event and within instructing the drone to navigate to a particular sensor location. Such a system can reduce crimes by stopping a person from traveling along a path whether a potential trespasser, hazard, or other threat may have been detected by the drone. In some implementations, the monitoring system can also help to alleviate the strain on local law enforcement by reducing their need to regularly surveil certain neighborhoods because the pre-surveillance drones are performing on-demand pre-surveillance of one or more paths to be traveled by a person. In one or more of these ways, the system can increase safety and security of a one or more persons because of the pre-surveillance operations performed by the one or more drones.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a contextual diagram of an example of a monitoring system that uses a drone to perform pre-surveillance inside a property</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is a contextual diagram of an example of a monitoring system that tracks user behavior to detect user behavior patterns.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is a contextual diagram of an example of a monitoring system that uses a drone to perform pre-surveillance outside a property based on a detected user behavior pattern.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>2</b>C</figref> is another contextual diagram of an example of a monitoring system that uses a drone to perform pre-surveillance outside a property based on a detected user behavior pattern.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> is a contextual diagram of an example of a monitoring system that tracks user behavior to detect long-range user behavior patterns.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> is a contextual diagram of an example of a monitoring system that uses a drone to perform long-range pre-surveillance outside a property based on a detected user behavior pattern.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of an example of a monitoring system that can use one or more drones to perform pre-surveillance.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of an example of a process for tracking user behavior pattern.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart of a process for performing drone pre-surveillance based on a detected user behavior pattern.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of a process for performing drone pre-surveillance.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a contextual diagram of an example of a monitoring system <b>100</b> that uses a drone <b>120</b> to perform pre-surveillance inside a property <b>101</b>.</p><p id="p-0037" num="0036">The monitoring system <b>100</b> includes one or more sensors <b>105</b><i>a</i>, <b>105</b><i>b</i>, <b>105</b><i>c</i>, <b>105</b><i>d</i>, <b>105</b><i>e</i>, a monitoring system control unit <b>110</b>, one or more wireless charging stations <b>115</b><i>a</i>, <b>115</b><i>b</i>, <b>115</b><i>c</i>, and at least one drone <b>120</b>. In some implementations, the monitoring system <b>100</b> may also include one or more cameras <b>108</b><i>a</i>, <b>108</b><i>b</i>, <b>108</b><i>c</i>, <b>108</b><i>d</i>, <b>108</b><i>e</i>. In some implementations, the monitoring system <b>100</b> may also include a user device <b>103</b>. In such instances, the monitoring system control unit <b>110</b>, drone <b>120</b>, or other components of monitoring system <b>100</b> may be able to communicate with the user device <b>103</b> using the network <b>150</b>. In some implementations, the monitoring system <b>100</b> may also include a monitoring application server <b>190</b>. In such instances, the monitoring system control unit <b>110</b>, drone <b>120</b>, or other components of monitoring system <b>100</b> may communicate with the monitoring application server <b>190</b> via the network <b>180</b> using one or more communications links <b>182</b>. The network <b>180</b> may include one or more networks such as a LAN, a WAN, a cellular network, the Internet, or the like.</p><p id="p-0038" num="0037">The monitoring system <b>100</b> may facilitate networked communication between each component of monitoring system <b>100</b> such as one or more sensors <b>105</b><i>a</i>, <b>105</b><i>b</i>, <b>105</b><i>c</i>, <b>105</b><i>d</i>, <b>105</b><i>e</i>, one or more cameras <b>108</b><i>a</i>, <b>108</b><i>b</i>, <b>108</b><i>c</i>, <b>108</b><i>d</i>, <b>108</b><i>e</i>, a monitoring system control unit <b>110</b>, one or more wireless charging stations <b>115</b><i>a</i>, <b>115</b><i>b</i>, <b>115</b><i>c</i>, and at least one drone <b>120</b> via a network <b>150</b>. The network <b>150</b> may include, for example, any type of wired network, wireless network, or a combination thereof, that facilitates communication between the components of monitoring system <b>100</b> including a LAN, a WAN, a cellular network, the Internet, or a combination thereof. One or more devices connected to network <b>150</b> may also be able to communicate with one or more remote devices such as the monitoring application server <b>190</b> via the network <b>180</b> using one or more communication links <b>182</b>. Though the user device <b>103</b> is depicted as a device that can communicate with one or more components of monitoring system <b>100</b> connected to network <b>150</b>, the present disclosure need not be so limited. For instance, there may be instances where the user device <b>103</b> is a located outside the range of network <b>150</b>. However, when located outside the network <b>150</b>, the user device may still communicate with one or more components of monitoring system <b>100</b> via network <b>180</b>. Then, the user device <b>103</b> may communicate with the components of monitoring system <b>100</b> using a combination of network <b>180</b>, network <b>150</b>, and one or more communications links <b>182</b>.</p><p id="p-0039" num="0038">With reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the monitoring system <b>100</b> is configured to use a drone <b>120</b> to perform pre-surveillance. To facilitate pre-surveillance as described with respect to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the monitoring system control unit <b>110</b> can obtain, aggregate, and analyze data related to the behavior of a user <b>102</b> to identify one or more patterns. The data obtained, aggregated, and analyzed by the monitoring system control unit <b>110</b> may include, for example, data that is output by one or more sensors <b>105</b><i>a</i>, <b>105</b><i>b</i>, <b>105</b><i>c</i>, <b>105</b><i>d</i>, <b>105</b><i>e </i>over a predetermined period of time. Alternatively, or in addition, the data obtained, aggregated, and analyzed by the monitoring system control unit <b>110</b> may include data that is detected by one or more sensors onboard the drone <b>120</b>. Alternatively, or in addition, the data obtained, aggregated, and analyzed by the monitoring system control unit <b>110</b> may include data obtained from one or more user devices <b>103</b>. The user device <b>103</b> may include, for example, a smartphone (or other handheld device), a smartwatch (or other wearable device), or the like that is capable of tracking the user's <b>102</b> movements. Such data may be stored locally and analyzed by the monitoring system control unit <b>110</b> in order to identify patterns that can be used to create pre-surveillance rules. Alternatively, such data may be forwarded for storage and analysis by the monitoring application server <b>190</b> via network <b>180</b> using one or more communications links. In such instances, the monitoring application server <b>190</b> may analyze the data to identify patterns that can be used to create pre-surveillance rules. The pre-surveillance rules, once created, may be stored and executed by the monitoring system control unit <b>110</b> using a drone <b>120</b>.</p><p id="p-0040" num="0039">By way of example, the monitoring system control unit <b>110</b> may determine that every morning, for at least a predetermined period of time, the user <b>102</b> wakes up in Room B, walks into Room C, and walks down the stairs from Room C to Room A. The monitoring system control unit <b>110</b> may make this determination based on sensor data that it obtained from sensors <b>105</b><i>b</i>, <b>105</b><i>c</i>, and <b>105</b><i>a </i>indicating that the sensors <b>105</b><i>b</i>, <b>105</b><i>c</i>, and <b>105</b><i>a </i>detect movement from Room B to Room A. In addition, the sensor data may include a timestamp. Alternatively, the monitoring system control unit may associate a timestamp with sensor data upon its receipt.</p><p id="p-0041" num="0040">Using timestamp data associated with obtained data that is indicative of the user's behavior (e.g., sensor data, sensor data collected by a drone <b>120</b>, video data collected by a drone <b>120</b>, image data collected by a drone <b>120</b>, data from a user device <b>103</b>, or the like), the monitoring system control unit <b>110</b> may determine that movement between sensor <b>105</b><i>c </i>and <b>105</b><i>a </i>consistently occurs at 6:30 am. Based on this analysis of data that is indicative of the user's behavior over a predetermined period of time, the monitoring system control unit <b>110</b> may generate a pre-surveillance rule to pre-surveil the downstairs of property <b>101</b> (e.g., Room A and Room D) at least a predetermined amount of time prior to 6:30 am. For example, such a pre-surveillance rule may trigger pre-surveillance of the Room A and Room D 5 minutes prior to 6:30 am at 6:25 am. The pre-surveillance rule may be stored, and enforced, by the monitoring system control unit <b>110</b>. The pre-surveillance rule may include (i) a user's behavioral pattern, (ii) a trigger time (e.g., 6:25 am) that is a predetermined amount of time before the user's <b>102</b> behavioral pattern begins (e.g., 6:30 am), (iii) a location (e.g., downstairs, upstairs, parking lot, driveway, neighborhood, or the like), (iv) an initial predetermined navigational path (e.g., navigation path <b>117</b>), or a combination thereof.</p><p id="p-0042" num="0041">Assume that the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts a morning after user <b>102</b> wakes up and before the user <b>102</b> walks downstairs. At 6:25 am, the stored pre-surveillance rule may trigger the monitoring system control unit <b>110</b> to transmit an instruction to the drone <b>120</b> charging on the wireless charging station <b>115</b><i>c</i>. The drone <b>120</b> may receive the instruction, navigate on a predetermined navigation path <b>117</b>, and pre-surveil the downstairs of property <b>101</b> prior to the user <b>102</b> walking downstairs at 6:30 am. The drone <b>120</b> may scan the entire downstairs of property <b>101</b> using a camera <b>120</b><i>a </i>to capture video and images of the downstairs of property <b>101</b>. In some implementations, the video or images <b>120</b><i>b </i>may be streamed to the user's <b>102</b> user device <b>103</b>. Once the drone's pre-surveillance is complete, the drone <b>120</b> may generate a status report that is transmitted to the user's device <b>103</b>.</p><p id="p-0043" num="0042">The status report may provide an indication as to the level of safety associated with the portion of the property <b>101</b> that was pre-surveilled. For example, the status report may indicate whether the portion of the property <b>101</b> that was pre-surveilled is safe, unsafe, or unknown (e.g., due to surveillance difficulties). The status report may provide the user with an estimated level of safeness that is associated with the pre-surveilled portion of the property. For instance, the property may be determined to be relatively safe. In some instances, the level of safeness may be provided in the form of a probability. In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the drone <b>120</b> does not find any intruders, abnormalities, or the like based on the drone's <b>120</b> scan of the downstairs of property <b>101</b>. Accordingly, the drone <b>120</b> may transmit a status report to the user's <b>102</b> user device <b>103</b> indicating that it is safe for the user <b>102</b> to walk down the stairs from Room B to Room A.</p><p id="p-0044" num="0043">The monitoring system control unit <b>110</b> is described as being local monitoring unit that is located at the property <b>101</b>. However, the present disclosure need not be so limited. For example, the functionality described with reference to the monitoring system control unit <b>110</b> may also be embodied in a monitoring unit that is remote from property. Such a monitoring unit may include, for example, the monitoring application server <b>190</b> that can obtain data (e.g., location data from user devices, sensor data from sensors, drone data from one or more drones, or the like) via one or more networks <b>150</b>, <b>180</b> and analyze the obtained data in the same way described with respect to the monitoring system control unit <b>110</b>. Upon determining that a pre-surveillance rule should be executed, the remote monitoring unit such as the monitoring application server <b>190</b> may transmit one or more instructions to the drone <b>120</b> that instruct the drone <b>120</b> to (i) navigate a flight path associated with a pre-surveillance rule and (ii) perform pre-surveillance of an area associated with the flight path. Alternatively, the remote monitoring unit can transmit one or more instructions to the local monitoring unit, and then the local monitoring unit can transmit one or more instructions to the drone <b>120</b> to (i) navigate a flight path associated with a pre-surveillance rule and (ii) perform pre-surveillance of an area associated with the flight path.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is a contextual diagram of an example of a monitoring system <b>100</b> that tracks user behavior to detect user behavior patterns.</p><p id="p-0046" num="0045">The example of <figref idref="DRAWINGS">FIG. <b>2</b>A</figref> shows the outside of the property <b>101</b> that employs a monitoring system <b>100</b> shown and described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The monitoring system <b>100</b> includes a sensor <b>105</b><i>e </i>that can transmit sensor data via the network <b>150</b> to a monitoring system control unit <b>110</b>.</p><p id="p-0047" num="0046">Over a predetermined period of time, the monitoring system control unit <b>110</b> of monitoring system <b>100</b> may receive sensor data from sensor <b>105</b><i>e </i>via the network <b>150</b>. In some implementations, the sensor <b>105</b><i>e </i>may include a motion sensor that detects movement of a user <b>102</b> as the user <b>102</b> exits <b>210</b> the property <b>101</b>. In some instances, the sensor <b>105</b><i>e </i>may not be used alone in order to determine when the user leaves the property. For instance, in some implementations, the monitoring system control unit <b>110</b> may analyze sensor data from the sensor <b>105</b><i>d </i>(see <figref idref="DRAWINGS">FIG. <b>1</b></figref>) and the sensor <b>105</b><i>e </i>in combination. If motion is detected by both sensors <b>105</b><i>d</i>, <b>105</b><i>e</i>, then the motion is more likely to be a user exiting <b>210</b> the property <b>101</b> than if only the outdoor sensor <b>105</b><i>e </i>detects the movement. If only the outside sensor <b>105</b><i>e </i>detects movement that is not associated with corresponding movement detected by sensor <b>105</b><i>d</i>, then the movement detected by <b>105</b><i>e </i>may be the result of something other than the user <b>102</b> exiting the property <b>101</b>.</p><p id="p-0048" num="0047">After analyzing the sensor data that was obtained from one or more sensors associated with property <b>101</b> over a predetermined period of time, the system may determine <b>230</b> that the user <b>102</b> regularly exits <b>210</b> the property <b>101</b> on weekdays at 7:00 am. Based on this determination, the monitoring system control unit <b>110</b> may create a pre-surveillance rule that, when triggered, results in the monitoring system control unit <b>110</b> instructing a drone <b>120</b> to pre-surveil the exterior of the property <b>101</b> prior to the user <b>102</b> leaving the property <b>120</b>. The pre-surveillance rule may include (i) a trigger time (e.g., 6:55 am) that is a predetermined amount of time before the user's <b>102</b> behavioral pattern begins (e.g., 7:00 am), (ii) a location (e.g., downstairs, upstairs, parking lot, driveway, neighborhood, or the like), (iii) an initial predetermined navigational path (e.g., navigation path <b>250</b> in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>), or a combination thereof.</p><p id="p-0049" num="0048">In the example of <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, the monitoring system control unit <b>110</b> analyzes data obtained by one or more sensors such as sensors <b>105</b><i>d</i>, <b>105</b><i>e </i>to determine when the user <b>102</b> exits the property <b>101</b>. However, the present disclosure need not be so limited. Instead, the monitoring system control unit <b>110</b> may also obtain other type of data that can be analyzed to determine a pattern of user <b>102</b> behavior. For instance, the user device <b>103</b> may keep a log of times the user exists the property that can be accessed by, or provided to, the monitoring system control unit <b>110</b>. Alternatively, or in addition, the user device <b>103</b> may transmit a notification to the monitoring system control unit <b>110</b> each time the user travels a predetermined distance from the property <b>101</b> such as, e.g., 5 feet, 10 feet, 15 feet, or the like. The notification may include a time stamp. Such data may be analyzed to identify user <b>102</b> patterns in addition to, or independent of, sensor data obtained by the monitoring system control unit <b>110</b>. The user device <b>103</b> may include, e.g., a smartphone (or other handheld device), a smartwatch (or other wearable device), or the like that can store obtained location data, transmit location data, or the like.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is a contextual diagram of an example of a monitoring system <b>100</b> that uses a drone to perform pre-surveillance outside a property based on a detected user behavior pattern.</p><p id="p-0051" num="0050">With reference to <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>, assume that the example of <figref idref="DRAWINGS">FIG. <b>2</b>B</figref> depicts a weekday morning prior to when the user <b>102</b> exits the property to go to work. The time at issue in the example of <figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is 6:55 am. At 6:55 am, the stored pre-surveillance rule <b>260</b> may trigger the monitoring system control unit <b>110</b> to transmit an instruction to the drone <b>120</b> charging on a wireless charging station. The drone <b>120</b> may receive the instruction, navigate on a predetermined navigation path <b>250</b>, and pre-surveil an outside portion of the property <b>101</b> that exists within a predetermined distance of the predetermined navigation path. The drone <b>120</b> may scan an outside portion of the property <b>101</b> using a camera <b>120</b><i>a </i>to capture video and images <b>220</b><i>b </i>of the environment that exists around all sides of the user's car <b>220</b>. The predetermined flight path <b>250</b> may include navigating out the front door (or another opening such as an open window, open garage door, or the like) navigating a path towards the user's <b>102</b> car <b>220</b>, around the car <b>220</b>, and then back to the property <b>101</b>. In some implementations, the video or images <b>220</b><i>b </i>may be streamed to the user's <b>102</b> user device <b>103</b> using network <b>150</b>. Once the drone's pre-surveillance is complete, the drone <b>120</b> may generate a status report that is transmitted to the user's device <b>103</b>.</p><p id="p-0052" num="0051">In the example of <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, the drone <b>120</b> did not detect any potential safety threats. Accordingly, the drone <b>120</b> may transmit a message back to the user device <b>103</b> indicating that the outside of the property <b>101</b> within a predetermined distance of the predetermined navigation path is safe. As a result, the user <b>102</b> knows that it is safe to walk to the user's car <b>220</b>.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>2</b>C</figref> is another contextual diagram of an example of a monitoring system <b>100</b> that uses a drone to perform pre-surveillance outside a property based on a detected user behavior pattern.</p><p id="p-0054" num="0053">With reference to <figref idref="DRAWINGS">FIG. <b>2</b>C</figref>, assume that the example of <figref idref="DRAWINGS">FIG. <b>2</b>C</figref> depicts a weekday morning prior to when the user <b>102</b> exits the property to go to work. The time at issue in the example of <figref idref="DRAWINGS">FIG. <b>2</b>C</figref> is 6:55 am. At 6:55 am, the stored pre-surveillance rule <b>260</b> may trigger the monitoring system control unit <b>110</b> to transmit an instruction to the drone <b>120</b> charging on a wireless charging station. The drone <b>120</b> may receive the instruction, navigate on a predetermined navigation path <b>250</b>, and pre-surveil an outside portion of the property <b>101</b> that exists within a predetermined distance of the predetermined navigation path.</p><p id="p-0055" num="0054">However, in <figref idref="DRAWINGS">FIG. <b>2</b>C</figref>, the drone <b>120</b> captures video and images <b>220</b><i>c </i>that show a person <b>270</b> loitering in the vicinity of the car <b>220</b>. In addition, the drone <b>120</b> may snap a picture of the loitering person's <b>270</b> hands, search an image database, and determine that the loitering person <b>270</b> is holding a knife. Based on the identification of a loitering person <b>270</b> that is holding a knife, the drone <b>120</b> may transmit a notification to the user device <b>103</b> that indicates it is not safe for the user to exit the property <b>101</b>.</p><p id="p-0056" num="0055">In some instances, the drone <b>120</b> may engage the loitering person <b>270</b> with varying levels of aggressiveness based on the potential threat posed by the loitering person in an attempt to make the area safe for the user to travel to. The level of aggressiveness used by the drone <b>120</b> may be based on the level of safeness determination made by the drone <b>120</b>. For example, if the potential threat is merely that the loitering person <b>270</b> is loitering, then the drone <b>120</b> may try to scare the loitering person off by flashing lights or playing loud audio signals. However, by way of another example, if the loitering person <b>270</b> is determined to be wearing a mask and a facial recognition scan and search determines that the loitering person has been issued an outstanding warrant out for the person's arrest, the drone <b>120</b> may take more aggressive action by swarming the loitering person <b>270</b> by flying around the loitering person at high speeds in random patterns. Moreover, as another example, if the drone <b>120</b> determines, based on facial recognition analysis, that the loitering person is a person who is a known terrorist wanted by the FBI, the drone <b>120</b> may crash into the loitering person <b>170</b>, shock the loitering person <b>170</b>, deploy a net to capture the loitering person <b>270</b>, or the like.</p><p id="p-0057" num="0056">Though the example of <figref idref="DRAWINGS">FIG. <b>2</b>C</figref> is shown in the context of a drone <b>120</b> determining whether it is safe for a user to leave the user's house and walk to the user's vehicle <b>220</b>, the present disclosure need not be so limited. For example, such a system may also be implemented by employers, hospitals, business owners or the like. In such instances, a drone such as drone <b>120</b> may be dispatched ahead of the time a user is about to leave (e.g., when a user's shift is nearing an end). Then, the drone may pre-surveil a parking garage associated with the user's office building, prior to the user walking to the user's car. The drone can report back to the user's device <b>103</b> letting the user <b>102</b> know whether the safeness of the parking garage is determined to be safe, unsafe, or unknown.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> is a contextual diagram of an example of a monitoring system <b>100</b> that tracks user <b>102</b> behavior to detect long-range user <b>102</b> behavior patterns.</p><p id="p-0059" num="0058">In some implementations, the monitoring system <b>100</b> may provide drone pre-surveillance at longer ranges. With reference to <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, for example, the user <b>102</b> leaves the property <b>101</b> to take his dog <b>320</b> for a walk. The monitoring system control unit <b>110</b> may determine, based on sensor data output by one or more sensors of monitoring system <b>100</b>, that the user <b>102</b> left the property <b>101</b>. Once the user leaves the property <b>101</b>, the user's <b>102</b> user device <b>103</b> may connect to a network <b>180</b> such as a LAN, a WAN, a cellular network, the Internet, or the like. While outside of the reach of network <b>150</b> of property <b>101</b>, the user's device <b>102</b> may include one or more applications that report information back to the monitoring system control unit <b>110</b>. Accordingly, while the user <b>102</b> walks the user's dog along a path <b>330</b> through the neighborhood shown in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, the user's <b>102</b> user device <b>103</b> periodically reports the user's <b>102</b> location. The user's device <b>103</b> may include a smartphone (or other handheld device), a smartwatch (or other wearable device), or the like.</p><p id="p-0060" num="0059">Based on an analysis of historical sensor data output by one or more sensors of monitoring system <b>100</b> such as sensor <b>105</b><i>e </i>(<figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b>A, <b>2</b>B, <b>2</b>C</figref>) and historical data received from the user's device <b>103</b>, the monitoring system control unit <b>110</b> may determine <b>340</b> that the user <b>102</b> takes the dog for a nightly walk through the neighborhood on path <b>330</b> every evening at 8:45 pm. Accordingly, the monitoring system control unit may generate a pre-surveillance rule that triggers pre-surveillance of the user's <b>102</b> route <b>330</b> through the neighborhood of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>. The pre-surveillance rule may include (i) a trigger time (e.g., 8:35 PM) that is a predetermined amount of time before the user's <b>102</b> behavioral pattern begins (e.g., 8:45 PM), (ii) a location (e.g., downstairs, upstairs, parking lot, driveway, neighborhood, or the like), (iii) an initial predetermined navigational path (e.g., dog walk path <b>330</b>), or a combination thereof.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> is a contextual diagram of an example of a monitoring system <b>100</b> that uses a drone <b>120</b> to perform long-range pre-surveillance outside a property <b>101</b> based on a detected user <b>102</b> behavior pattern.</p><p id="p-0062" num="0061">With reference to <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, assume that the example of <figref idref="DRAWINGS">FIG. <b>3</b>B</figref> depicts a day on any night of the week just prior to when the user <b>102</b> exits the property to walk his dog. The time at issue in the example of <figref idref="DRAWINGS">FIG. <b>3</b>B</figref> is 8:35 PM. At 8:35 PM, the stored pre-surveillance rule <b>360</b> may trigger the monitoring system control unit <b>110</b> to transmit an instruction to the drone <b>120</b> charging on a wireless charging station. The drone <b>120</b> may receive the instruction, navigate on a predetermined navigation path <b>350</b>, and perform long-range pre-surveillance of the neighborhood of <figref idref="DRAWINGS">FIG. <b>3</b>B</figref> prior to the user <b>102</b> taking the user's dog <b>320</b> for a walk. The long-range pre-surveillance of the neighborhood of <figref idref="DRAWINGS">FIG. <b>3</b>B</figref> may include pre-surveillance of the vicinity of the user's dog walk path. For example, the drone <b>120</b> may pre-surveil the area that is within a predetermined distance of navigational flight path <b>350</b>. The drone <b>120</b> may scan the portion of the neighborhood that is within a predetermined distance of the navigational flight path <b>350</b> using a camera <b>120</b><i>a </i>to capture video and images <b>320</b><i>b</i>. The predetermined flight path <b>350</b> may include navigating out the front door (or another opening such as an open window, open garage door, or the like) of the property <b>101</b>, navigating a path down the user's street towards property <b>303</b>, around the cul-de-sac towards property <b>308</b> and then towards property <b>306</b>, and then back to property <b>101</b>. In some implementations, the video or images <b>320</b><i>b </i>may be streamed to the user's <b>102</b> user device <b>103</b> using networks <b>180</b>, <b>150</b>, or a combination thereof. Once the drone's pre-surveillance is complete, the drone <b>120</b> may generate a status report that is transmitted to the user's device <b>103</b>. The status report may provide an indication as the level of safeness of the dog walk path <b>330</b>. In the example of <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, since the drone <b>120</b> did not encounter any safety issues, the drone <b>120</b> may report that the dog walk path <b>330</b> is safe.</p><p id="p-0063" num="0062">Though the example of long-range pre-surveillance described herein is in the context of a dog walk, the present discloses should not be so limited. For example, long-range surveillance may also extend to activities such as walks to a convenience store, jogging paths, marathon training paths, or the like.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of an example of a monitoring system <b>400</b> that can use one or more drones to perform pre-surveillance.</p><p id="p-0065" num="0064">The electronic system <b>400</b> includes a network <b>405</b>, a monitoring system control unit <b>410</b>, one or more user devices <b>440</b>, <b>450</b>, a monitoring application server <b>460</b>, and a central alarm station server <b>470</b>. In some examples, the network <b>405</b> facilitates communications between the monitoring system control unit <b>410</b>, the one or more user devices <b>440</b>, <b>450</b>, the monitoring application server <b>460</b>, and the central alarm station server <b>470</b>.</p><p id="p-0066" num="0065">The network <b>405</b> is configured to enable exchange of electronic communications between devices connected to the network <b>405</b>. For example, the network <b>405</b> may be configured to enable exchange of electronic communications between the monitoring system control unit <b>410</b>, the one or more user devices <b>440</b>, <b>450</b>, the monitoring application server <b>460</b>, and the central alarm station server <b>470</b>. The network <b>405</b> may include, for example, one or more of the Internet, Wide Area Networks (WANs), Local Area Networks (LANs), analog or digital wired and wireless telephone networks (e.g., a public switched telephone network (PSTN), Integrated Services Digital Network (ISDN), a cellular network, and Digital Subscriber Line (DSL)), radio, television, cable, satellite, or any other delivery or tunneling mechanism for carrying data. Network <b>405</b> may include multiple networks or subnetworks, each of which may include, for example, a wired or wireless data pathway. The network <b>405</b> may include a circuit-switched network, a packet-switched data network, or any other network able to carry electronic communications (e.g., data or voice communications). For example, the network <b>405</b> may include networks based on the Internet protocol (IP), asynchronous transfer mode (ATM), the PSTN, packet-switched networks based on IP, X.25, or Frame Relay, or other comparable technologies and may support voice using, for example, VoIP, or other comparable protocols used for voice communications. The network <b>405</b> may include one or more networks that include wireless data channels and wireless voice channels. The network <b>405</b> may be a wireless network, a broadband network, or a combination of networks including a wireless network and a broadband network.</p><p id="p-0067" num="0066">The monitoring system control unit <b>410</b> includes a controller <b>412</b>, a network module <b>414</b>, and storage unit <b>416</b>. The controller <b>412</b> is configured to control a monitoring system (e.g., a home alarm or security system) that includes the monitoring system control unit <b>410</b>. In some examples, the controller <b>412</b> may include a processor or other control circuitry configured to execute instructions of a program that controls operation of an alarm system. In these examples, the controller <b>412</b> may be configured to receive input from sensors, detectors, or other devices included in the alarm system and control operations of devices included in the alarm system or other household devices (e.g., a thermostat, an appliance, lights, etc.). For example, the controller <b>412</b> may be configured to control operation of the network module <b>414</b> included in the monitoring system control unit <b>410</b>.</p><p id="p-0068" num="0067">In some implementations, the monitoring system control unit <b>410</b> may store received input from sensors, detectors, user devices <b>440</b> and <b>450</b>, or other devices included in system <b>400</b> may be stored in the storage unit <b>416</b>. The monitoring system control unit <b>410</b> may analyze the stored input to detect one or more user behavioral patterns. Once a user behavioral pattern is identified, the monitoring system control unit <b>410</b> may generate a pre-surveillance rule that, when triggered, instructs one or more robotic devices <b>480</b> and <b>482</b> to perform pre-surveillance of a location associated with the user's behavioral patterns.</p><p id="p-0069" num="0068">The network module <b>414</b> is a communication device configured to exchange communications over the network <b>405</b>. The network module <b>414</b> may be a wireless communication module configured to exchange wireless communications over the network <b>405</b>. For example, the network module <b>414</b> may be a wireless communication device configured to exchange communications over a wireless data channel and a wireless voice channel. In this example, the network module <b>414</b> may transmit alarm data over a wireless data channel and establish a two-way voice communication session over a wireless voice channel. The wireless communication device may include one or more of a LTE module, a GSM module, a radio modem, cellular transmission module, or any type of module configured to exchange communications in one of the following formats: LTE, GSM or GPRS, CDMA, EDGE or EGPRS, EV-DO or EVDO, UMTS, or IP.</p><p id="p-0070" num="0069">The network module <b>414</b> also may be a wired communication module configured to exchange communications over the network <b>405</b> using a wired connection. For instance, the network module <b>414</b> may be a modem, a network interface card, or another type of network interface device. The network module <b>414</b> may be an Ethernet network card configured to enable the monitoring system control unit <b>410</b> to communicate over a local area network and/or the Internet. The network module <b>414</b> also may be a voiceband modem configured to enable the alarm panel to communicate over the telephone lines of Plain Old Telephone Systems (POTS).</p><p id="p-0071" num="0070">The monitoring system that includes the monitoring system control unit <b>410</b> includes one or more sensors or detectors. For example, the monitoring system may include multiple sensors <b>420</b>. The sensors <b>420</b> may include a contact sensor, a motion sensor, a glass break sensor, or any other type of sensor included in an alarm system or security system. The sensors <b>420</b> also may include an environmental sensor, such as a temperature sensor, a water sensor, a rain sensor, a wind sensor, a light sensor, a smoke detector, a carbon monoxide detector, an air quality sensor, etc. The sensors <b>420</b> further may include a health monitoring sensor, such as a prescription bottle sensor that monitors taking of prescriptions, a blood pressure sensor, a blood sugar sensor, a bed mat configured to sense presence of liquid (e.g., bodily fluids) on the bed mat, etc. In some examples, the sensors <b>420</b> may include a radio-frequency identification (RFID) sensor that identifies a particular article that includes a pre-assigned RFID tag.</p><p id="p-0072" num="0071">The monitoring system control unit <b>410</b> communicates with the module <b>422</b> and the camera <b>430</b> to perform surveillance or monitoring. The module <b>422</b> is connected to one or more devices that enable home automation control. For instance, the module <b>422</b> may be connected to one or more lighting systems and may be configured to control operation of the one or more lighting systems. Also, the module <b>422</b> may be connected to one or more electronic locks at the property and may be configured to control operation of the one or more electronic locks (e.g., control Z-Wave locks using wireless communications in the Z-Wave protocol. Further, the module <b>422</b> may be connected to one or more appliances at the property and may be configured to control operation of the one or more appliances. The module <b>422</b> may include multiple modules that are each specific to the type of device being controlled in an automated manner. The module <b>422</b> may control the one or more devices based on commands received from the monitoring system control unit <b>410</b>. For instance, the module <b>422</b> may cause a lighting system to illuminate an area to provide a better image of the area when captured by a camera <b>430</b>.</p><p id="p-0073" num="0072">The camera <b>430</b> may be a video/photographic camera or other type of optical sensing device configured to capture images. For instance, the camera <b>430</b> may be configured to capture images of an area within a building monitored by the monitoring system control unit <b>410</b>. The camera <b>430</b> may be configured to capture single, static images of the area and also video images of the area in which multiple images of the area are captured at a relatively high frequency (e.g., thirty images per second). The camera <b>430</b> may be controlled based on commands received from the monitoring system control unit <b>410</b>.</p><p id="p-0074" num="0073">The camera <b>430</b> may be triggered by several different types of techniques. For instance, a Passive Infra Red (PIR) motion sensor may be built into the camera <b>430</b> and used to trigger the camera <b>430</b> to capture one or more images when motion is detected. The camera <b>430</b> also may include a microwave motion sensor built into the camera and used to trigger the camera <b>430</b> to capture one or more images when motion is detected. The camera <b>430</b> may have a &#x201c;normally open&#x201d; or &#x201c;normally closed&#x201d; digital input that can trigger capture of one or more images when external sensors (e.g., the sensors <b>420</b>, PIR, door/window, etc.) detect motion or other events. In some implementations, the camera <b>430</b> receives a command to capture an image when external devices detect motion or another potential alarm event. The camera <b>430</b> may receive the command from the controller <b>412</b> or directly from one of the sensors <b>420</b>.</p><p id="p-0075" num="0074">In some examples, the camera <b>430</b> triggers integrated or external illuminators (e.g., Infra Red, Z-wave controlled &#x201c;white&#x201d; lights, lights controlled by the module <b>422</b>, etc.) to improve image quality when the scene is dark. An integrated or separate light sensor may be used to determine if illumination is desired and may result in increased image quality.</p><p id="p-0076" num="0075">The camera <b>430</b> may be programmed with any combination of time/day schedules, system &#x201c;arming state&#x201d;, or other variables to determine whether images should be captured or not when triggers occur. The camera <b>430</b> may enter a low-power mode when not capturing images. In this case, the camera <b>430</b> may wake periodically to check for inbound messages from the controller <b>412</b>. The camera <b>430</b> may be powered by internal, replaceable batteries if located remotely from the monitoring control unit <b>410</b>. The camera <b>430</b> may employ a small solar cell to recharge the battery when light is available. Alternatively, the camera <b>430</b> may be powered by the controller's <b>412</b> power supply if the camera <b>430</b> is co-located with the controller <b>412</b>.</p><p id="p-0077" num="0076">In some implementations, the camera <b>430</b> communicates directly with the monitoring application server <b>460</b> over the Internet. In these implementations, image data captured by the camera <b>430</b> does not pass through the monitoring system control unit <b>410</b> and the camera <b>430</b> receives commands related to operation from the monitoring application server <b>460</b>.</p><p id="p-0078" num="0077">The system <b>400</b> further includes one or more robotic devices <b>480</b> and <b>482</b>. The robotic devices <b>480</b> and <b>482</b> may be any type of robots that are capable of moving and taking actions that assist monitoring user behavior patterns. For example, the robotic devices <b>480</b> and <b>482</b> may include drones that are capable of moving throughout a property based on automated control technology and/or user input control provided by a user. In this example, the drones may be able to fly, roll, walk, or otherwise move about the property. The drones may include helicopter type devices (e.g., quad copters), rolling helicopter type devices (e.g., roller copter devices that can fly and also roll along the ground, walls, or ceiling) and land vehicle type devices (e.g., automated cars that drive around a property). In some cases, the robotic devices <b>480</b> and <b>482</b> may be robotic devices that are intended for other purposes and merely associated with the monitoring system <b>400</b> for use in appropriate circumstances. For instance, a robotic vacuum cleaner device may be associated with the monitoring system <b>400</b> as one of the robotic devices <b>480</b> and <b>482</b> and may be controlled to take action responsive to monitoring system events.</p><p id="p-0079" num="0078">In some examples, the robotic devices <b>480</b> and <b>482</b> automatically navigate within a property. In these examples, the robotic devices <b>480</b> and <b>482</b> include sensors and control processors that guide movement of the robotic devices <b>480</b> and <b>482</b> within the property. For instance, the robotic devices <b>480</b> and <b>482</b> may navigate within the property using one or more cameras, one or more proximity sensors, one or more gyroscopes, one or more accelerometers, one or more magnetometers, a global positioning system (GPS) unit, an altimeter, one or more sonar or laser sensors, and/or any other types of sensors that aid in navigation about a space. The robotic devices <b>480</b> and <b>482</b> may include control processors that process output from the various sensors and control the robotic devices <b>480</b> and <b>482</b> to move along a path that reaches the desired destination and avoids obstacles. In this regard, the control processors detect walls or other obstacles in the property and guide movement of the robotic devices <b>480</b> and <b>482</b> in a manner that avoids the walls and other obstacles.</p><p id="p-0080" num="0079">In addition, the robotic devices <b>480</b> and <b>482</b> may store data that describes attributes of the property. For instance, the robotic devices <b>480</b> and <b>482</b> may store a floorplan and/or a three-dimensional model of the property that enables the robotic devices <b>480</b> and <b>482</b> to navigate the property. During initial configuration, the robotic devices <b>480</b> and <b>482</b> may receive the data describing attributes of the property, determine a frame of reference to the data (e.g., a home or reference location in the property), and navigate the property based on the frame of reference and the data describing attributes of the property. Further, initial configuration of the robotic devices <b>480</b> and <b>482</b> also may include learning of one or more navigation patterns in which a user provides input to control the robotic devices <b>480</b> and <b>482</b> to perform a specific navigation action (e.g., fly to an upstairs bedroom and spin around while capturing video and then return to a home charging base). In this regard, the robotic devices <b>480</b> and <b>482</b> may learn and store the navigation patterns such that the robotic devices <b>480</b> and <b>482</b> may automatically repeat the specific navigation actions upon a later request.</p><p id="p-0081" num="0080">In addition to navigation patterns that are learned during initial configuration, the robotic devices <b>480</b> and <b>482</b> may also be configured to learn additional navigational patterns. For instance, a robotic devices <b>480</b> and <b>482</b> can be programmed to travel along particular navigational paths in response to an instruction to perform pre-surveillance of a particular location associated with a user behavioral pattern. In some implementations, for example, the particular pre-surveillance navigational pattern may be based on, for example, a navigational pattern that the user follows when engaged in the behavioral pattern associated with a triggered pre-surveillance rule.</p><p id="p-0082" num="0081">In some examples, the robotic devices <b>480</b> and <b>482</b> may include data capture and recording devices. In these examples, the robotic devices <b>480</b> and <b>482</b> may include one or more cameras, one or more motion sensors, one or more microphones, one or more biometric data collection tools, one or more temperature sensors, one or more humidity sensors, one or more air flow sensors, and/or any other types of sensors that may be useful in capturing monitoring data related to the property and users in the property. The one or more biometric data collection tools may be configured to collect biometric samples of a person in the home with or without contact of the person. For instance, the biometric data collection tools may include a fingerprint scanner, a hair sample collection tool, a skin cell collection tool, and/or any other tool that allows the robotic devices <b>480</b> and <b>482</b> to take and store a biometric sample that can be used to identify the person (e.g., a biometric sample with DNA that can be used for DNA testing).</p><p id="p-0083" num="0082">In some implementations, the robotic devices <b>480</b> and <b>482</b> may include output devices. In these implementations, the robotic devices <b>480</b> and <b>482</b> may include one or more displays, one or more speakers, one or more projectors, and/or any type of output devices that allow the robotic devices <b>480</b> and <b>482</b> to communicate information to a nearby user. The one or more projectors may include projectors that project a two-dimensional image onto a surface (e.g., wall, floor, or ceiling) and/or holographic projectors that project three-dimensional holograms into a nearby space.</p><p id="p-0084" num="0083">The robotic devices <b>480</b> and <b>482</b> also may include a communication module that enables the robotic devices <b>480</b> and <b>482</b> to communicate with the monitoring system control unit <b>410</b>, each other, and/or other devices. The communication module may be a wireless communication module that allows the robotic devices <b>480</b> and <b>482</b> to communicate wirelessly. For instance, the communication module may be a Wi-Fi module that enables the robotic devices <b>480</b> and <b>482</b> to communicate over a local wireless network at the property. The communication module further may be a 900 MHz wireless communication module that enables the robotic devices <b>480</b> and <b>482</b> to communicate directly with the monitoring system control unit <b>410</b>. Other types of short-range wireless communication protocols, such as Bluetooth, Bluetooth LE, Zwave, Zigbee, etc., may be used to allow the robotic devices <b>480</b> and <b>482</b> to communicate with other devices in the property.</p><p id="p-0085" num="0084">The robotic devices <b>480</b> and <b>482</b> further may include processor and storage capabilities. The robotic devices <b>480</b> and <b>482</b> may include any suitable processing devices that enable the robotic devices <b>480</b> and <b>482</b> to operate applications and perform the actions described throughout this disclosure. In addition, the robotic devices <b>480</b> and <b>482</b> may include solid state electronic storage that enables the robotic devices <b>480</b> and <b>482</b> to store applications, configuration data, collected sensor data, and/or any other type of information available to the robotic devices <b>480</b> and <b>482</b>.</p><p id="p-0086" num="0085">The robotic devices <b>480</b> and <b>482</b> are associated with one or more charging stations <b>490</b> and <b>492</b>. The charging stations <b>490</b> and <b>492</b> may be located at predefined home base or reference locations in the property. The robotic devices <b>480</b> and <b>482</b> may be configured to navigate to the charging stations <b>490</b> and <b>492</b> after completion of tasks needed to be performed for the monitoring system <b>400</b>. For instance, after completion of a monitoring operation or upon instruction by the monitoring system control unit <b>410</b>, the robotic devices <b>480</b> and <b>482</b> may be configured to automatically fly to and land on one of the charging stations <b>490</b> and <b>492</b>. In this regard, the robotic devices <b>480</b> and <b>482</b> may automatically maintain a fully charged battery in a state in which the robotic devices <b>480</b> and <b>482</b> are ready for use by the monitoring system <b>400</b>.</p><p id="p-0087" num="0086">The charging stations <b>490</b> and <b>492</b> may be contact based charging stations and/or wireless charging stations. For contact based charging stations, the robotic devices <b>480</b> and <b>482</b> may have readily accessible points of contact that the robotic devices <b>480</b> and <b>482</b> are capable of positioning and mating with a corresponding contact on the charging station. For instance, a helicopter type robotic device may have an electronic contact on a portion of its landing gear that rests on and mates with an electronic pad of a charging station when the helicopter type robotic device lands on the charging station. The electronic contact on the robotic device may include a cover that opens to expose the electronic contact when the robotic device is charging and closes to cover and insulate the electronic contact when the robotic device is in operation.</p><p id="p-0088" num="0087">For wireless charging stations, the robotic devices <b>480</b> and <b>482</b> may charge through a wireless exchange of power. In these cases, the robotic devices <b>480</b> and <b>482</b> need only locate themselves closely enough to the wireless charging stations for the wireless exchange of power to occur. In this regard, the positioning needed to land at a predefined home base or reference location in the property may be less precise than with a contact based charging station. Based on the robotic devices <b>480</b> and <b>482</b> landing at a wireless charging station, the wireless charging station outputs a wireless signal that the robotic devices <b>480</b> and <b>482</b> receive and convert to a power signal that charges a battery maintained on the robotic devices <b>480</b> and <b>482</b>.</p><p id="p-0089" num="0088">In some implementations, each of the robotic devices <b>480</b> and <b>482</b> has a corresponding and assigned charging station <b>490</b> and <b>492</b> such that the number of robotic devices <b>480</b> and <b>482</b> equals the number of charging stations <b>490</b> and <b>492</b>. In these implementations, the robotic devices <b>480</b> and <b>482</b> always navigate to the specific charging station assigned to that robotic device. For instance, the robotic device <b>480</b> may always use changing station <b>490</b> and the robotic device <b>482</b> may always use changing station <b>492</b>.</p><p id="p-0090" num="0089">In some examples, the robotic devices <b>480</b> and <b>482</b> may share charging stations. For instance, the robotic devices <b>480</b> and <b>482</b> may use one or more community charging stations that are capable of charging multiple robotic devices <b>480</b> and <b>482</b>. The community charging station may be configured to charge multiple robotic devices <b>480</b> and <b>482</b> in parallel. The community charging station may be configured to charge multiple robotic devices <b>480</b> and <b>482</b> in serial such that the multiple robotic devices <b>480</b> and <b>482</b> take turns charging and, when fully charged, return to a predefined home base or reference location in the property that is not associated with a charger. The number of community charging stations may be less than the number of robotic devices <b>480</b> and <b>482</b>.</p><p id="p-0091" num="0090">Also, the charging stations <b>490</b> and <b>492</b> may not be assigned to specific robotic devices <b>480</b> and <b>482</b> and may be capable of charging any of the robotic devices <b>480</b> and <b>482</b>. In this regard, the robotic devices <b>480</b> and <b>482</b> may use any suitable, unoccupied charging station when not in use. For instance, when one of the robotic devices <b>480</b> and <b>482</b> has completed an operation or is in need of battery charge, the monitoring system control unit <b>410</b> references a stored table of the occupancy status of each charging station and instructs the robotic device to navigate to the nearest charging station that is unoccupied.</p><p id="p-0092" num="0091">The sensors <b>420</b>, the module <b>422</b>, the camera <b>430</b>, and the robotic devices <b>480</b> and <b>482</b> communicate with the controller <b>412</b> over communication links <b>424</b>, <b>426</b>, <b>428</b>, <b>432</b>, <b>484</b>, and <b>486</b>. The communication links <b>424</b>, <b>426</b>, <b>428</b>, <b>432</b>, <b>484</b>, and <b>486</b> may be a wired or wireless data pathway configured to transmit signals from the sensors <b>420</b>, the module <b>422</b>, the camera <b>430</b>, and the robotic devices <b>480</b> and <b>482</b> to the controller <b>412</b>. The sensors <b>420</b>, the module <b>422</b>, the camera <b>430</b>, and the robotic devices <b>480</b> and <b>482</b> may continuously transmit sensed values to the controller <b>412</b>, periodically transmit sensed values to the controller <b>412</b>, or transmit sensed values to the controller <b>412</b> in response to a change in a sensed value.</p><p id="p-0093" num="0092">The communication links <b>424</b>, <b>426</b>, <b>428</b>, <b>432</b>, <b>484</b>, and <b>486</b> may include a local network. The sensors <b>420</b>, the module <b>422</b>, the camera <b>430</b>, and the robotic devices <b>480</b> and <b>482</b> and the controller <b>412</b> may exchange data and commands over the local network. The local network may include 802.11 &#x201c;WiFi&#x201d; wireless Ethernet (e.g., using low-power WiFi chipsets), Z-Wave, Zigbee, Bluetooth, &#x201c;Homeplug&#x201d; or other &#x201c;Powerline&#x201d; networks that operate over AC wiring, and a Category 5 (CATS) or Category 6 (CAT6) wired Ethernet network. The local network may be a mesh network constructed based on the devices connected to the mesh network.</p><p id="p-0094" num="0093">The monitoring application server <b>460</b> is an electronic device configured to provide monitoring services by exchanging electronic communications with the monitoring system control unit <b>410</b>, the one or more user devices <b>440</b>, <b>450</b>, and the central alarm station server <b>470</b> over the network <b>405</b>. For example, the monitoring application server <b>460</b> may be configured to monitor events (e.g., alarm events) generated by the monitoring system control unit <b>410</b>. In this example, the monitoring application server <b>460</b> may exchange electronic communications with the network module <b>414</b> included in the monitoring system control unit <b>410</b> to receive information regarding events (e.g., alarm events) detected by the monitoring system control unit <b>410</b>. The monitoring application server <b>460</b> also may receive information regarding events (e.g., alarm events) from the one or more user devices <b>440</b>, <b>450</b>.</p><p id="p-0095" num="0094">In some examples, the monitoring application server <b>460</b> may route alarm data received from the network module <b>414</b> or the one or more user devices <b>440</b>, <b>450</b> to the central alarm station server <b>470</b>. For example, the monitoring application server <b>260</b> may transmit the alarm data to the central alarm station server <b>470</b> over the network <b>405</b>.</p><p id="p-0096" num="0095">The monitoring application server <b>460</b> may store sensor and image data received from the monitoring system and perform analysis of sensor and image data received from the monitoring system. Based on the analysis, the monitoring application server <b>460</b> may communicate with and control aspects of the monitoring system control unit <b>410</b> or the one or more user devices <b>440</b>, <b>450</b>.</p><p id="p-0097" num="0096">The central alarm station server <b>470</b> is an electronic device configured to provide alarm monitoring service by exchanging communications with the monitoring system control unit <b>410</b>, the one or more mobile devices <b>440</b>, <b>450</b>, and the monitoring application server <b>460</b> over the network <b>405</b>. For example, the central alarm station server <b>470</b> may be configured to monitor alarm events generated by the monitoring system control unit <b>410</b>. In this example, the central alarm station server <b>470</b> may exchange communications with the network module <b>414</b> included in the monitoring system control unit <b>410</b> to receive information regarding alarm events detected by the monitoring system control unit <b>410</b>. The central alarm station server <b>470</b> also may receive information regarding alarm events from the one or more mobile devices <b>440</b>, <b>450</b> and/or the monitoring application server <b>460</b>.</p><p id="p-0098" num="0097">The central alarm station server <b>470</b> is connected to multiple terminals <b>472</b> and <b>474</b>. The terminals <b>472</b> and <b>474</b> may be used by operators to process alarm events. For example, the central alarm station server <b>470</b> may route alarm data to the terminals <b>472</b> and <b>474</b> to enable an operator to process the alarm data. The terminals <b>472</b> and <b>474</b> may include general-purpose computers (e.g., desktop personal computers, workstations, or laptop computers) that are configured to receive alarm data from a server in the central alarm station server <b>470</b> and render a display of information based on the alarm data. For instance, the controller <b>412</b> may control the network module <b>414</b> to transmit, to the central alarm station server <b>470</b>, alarm data indicating that a sensor <b>420</b> detected a door opening when the monitoring system was armed. The central alarm station server <b>470</b> may receive the alarm data and route the alarm data to the terminal <b>472</b> for processing by an operator associated with the terminal <b>472</b>. The terminal <b>472</b> may render a display to the operator that includes information associated with the alarm event (e.g., the name of the user of the alarm system, the address of the building the alarm system is monitoring, the type of alarm event, etc.) and the operator may handle the alarm event based on the displayed information.</p><p id="p-0099" num="0098">In some implementations, the terminals <b>472</b> and <b>474</b> may be mobile devices or devices designed for a specific function. Although <figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates two terminals for brevity, actual implementations may include more (and, perhaps, many more) terminals.</p><p id="p-0100" num="0099">The one or more user devices <b>440</b>, <b>450</b> are devices that host and display user interfaces. For instance, the user device <b>440</b> is a mobile device that hosts one or more native applications (e.g., the native surveillance application <b>442</b>). The user device <b>440</b> may be a cellular phone or a non-cellular locally networked device with a display. The user device <b>440</b> may include a cell phone, a smart phone, a tablet PC, a personal digital assistant (&#x201c;PDA&#x201d;), or any other portable device configured to communicate over a network and display information. For example, implementations may also include Blackberry-type devices (e.g., as provided by Research in Motion), electronic organizers, iPhone-type devices (e.g., as provided by Apple), iPod devices (e.g., as provided by Apple) or other portable music players, other communication devices, and handheld or portable electronic devices for gaming, communications, and/or data organization. The user device <b>440</b> may perform functions unrelated to the monitoring system, such as placing personal telephone calls, playing music, playing video, displaying pictures, browsing the Internet, maintaining an electronic calendar, etc.</p><p id="p-0101" num="0100">The user device <b>440</b> includes a native surveillance application <b>442</b>. The native surveillance application <b>442</b> refers to a software/firmware program running on the corresponding mobile device that enables the user interface and features described throughout. The user device <b>440</b> may load or install the native surveillance application <b>442</b> based on data received over a network or data received from local media. The native surveillance application <b>442</b> runs on mobile devices platforms, such as iPhone, iPod touch, Blackberry, Google Android, Windows Mobile, etc. The native surveillance application <b>442</b> enables the user device <b>440</b> to receive and process image and sensor data from the monitoring system.</p><p id="p-0102" num="0101">The user device <b>450</b> may be a general-purpose computer (e.g., a desktop personal computer, a workstation, or a laptop computer) that is configured to communicate with the monitoring application server <b>460</b> and/or the monitoring system control unit <b>410</b> over the network <b>405</b>. The user device <b>450</b> may be configured to display a surveillance monitoring user interface <b>452</b> that is generated by the user device <b>450</b> or generated by the monitoring application server <b>460</b>. For example, the user device <b>450</b> may be configured to display a user interface (e.g., a web page) provided by the monitoring application server <b>460</b> that enables a user to perceive images captured by the camera <b>430</b> and/or reports related to the monitoring system. Although <figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates two user devices for brevity, actual implementations may include more (and, perhaps, many more) or fewer user devices.</p><p id="p-0103" num="0102">In some implementations, the one or more user devices <b>440</b>, <b>450</b> communicate with and receive monitoring system data from the monitoring system control unit <b>410</b> using the communication link <b>438</b>. For instance, the one or more user devices <b>440</b>, <b>450</b> may communicate with the monitoring system control unit <b>410</b> using various local wireless protocols such as wifi, Bluetooth, zwave, zigbee, HomePlug (ethernet over powerline), or wired protocols such as Ethernet and USB, to connect the one or more user devices <b>440</b>, <b>450</b> to local security and automation equipment. The one or more user devices <b>440</b>, <b>450</b> may connect locally to the monitoring system and its sensors and other devices. The local connection may improve the speed of status and control communications because communicating through the network <b>405</b> with a remote server (e.g., the monitoring application server <b>460</b>) may be significantly slower.</p><p id="p-0104" num="0103">Although the one or more user devices <b>440</b>, <b>450</b> are shown as communicating with the monitoring system control unit <b>410</b>, the one or more user devices <b>440</b>, <b>450</b> may communicate directly with the sensors and other devices controlled by the monitoring system control unit <b>410</b>. In some implementations, the one or more user devices <b>440</b>, <b>450</b> replace the monitoring system control unit <b>410</b> and perform the functions of the monitoring system control unit <b>410</b> for local monitoring and long range/offsite communication.</p><p id="p-0105" num="0104">In other implementations, the one or more user devices <b>440</b>, <b>450</b> receive monitoring system data captured by the monitoring system control unit <b>410</b> through the network <b>405</b>. The one or more user devices <b>440</b>, <b>450</b> may receive the data from the monitoring system control unit <b>410</b> through the network <b>405</b> or the monitoring application server <b>460</b> may relay data received from the monitoring system control unit <b>410</b> to the one or more user devices <b>440</b>, <b>450</b> through the network <b>405</b>. In this regard, the monitoring application server <b>460</b> may facilitate communication between the one or more user devices <b>440</b>, <b>450</b> and the monitoring system.</p><p id="p-0106" num="0105">In some implementations, the one or more user devices <b>440</b>, <b>450</b> may be configured to switch whether the one or more user devices <b>440</b>, <b>450</b> communicate with the monitoring system control unit <b>410</b> directly (e.g., through link <b>438</b>) or through the monitoring application server <b>460</b> (e.g., through network <b>405</b>) based on a location of the one or more user devices <b>440</b>, <b>450</b>. For instance, when the one or more user devices <b>440</b>, <b>450</b> are located close to the monitoring system control unit <b>410</b> and in range to communicate directly with the monitoring system control unit <b>410</b>, the one or more user devices <b>440</b>, <b>450</b> use direct communication. When the one or more user devices <b>440</b>, <b>450</b> are located far from the monitoring system control unit <b>410</b> and not in range to communicate directly with the monitoring system control unit <b>410</b>, the one or more user devices <b>440</b>, <b>450</b> use communication through the monitoring application server <b>460</b>.</p><p id="p-0107" num="0106">Although the one or more user devices <b>440</b>, <b>450</b> are shown as being connected to the network <b>405</b>, in some implementations, the one or more user devices <b>440</b>, <b>450</b> are not connected to the network <b>405</b>. In these implementations, the one or more user devices <b>440</b>, <b>450</b> communicate directly with one or more of the monitoring system components and no network (e.g., Internet) connection or reliance on remote servers is needed.</p><p id="p-0108" num="0107">In some implementations, the one or more user devices <b>440</b>, <b>450</b> are used in conjunction with only local sensors and/or local devices in a house. In these implementations, the system <b>400</b> only includes the one or more user devices <b>440</b>, <b>450</b>, the sensors <b>420</b>, the module <b>422</b>, the camera <b>430</b>, and the robotic devices <b>480</b> and <b>482</b>. The one or more user devices <b>440</b>, <b>450</b> receive data directly from the sensors <b>420</b>, the module <b>422</b>, the camera <b>430</b>, and the robotic devices <b>480</b> and <b>482</b> and sends data directly to the sensors <b>420</b>, the module <b>422</b>, the camera <b>430</b>, and the robotic devices <b>480</b> and <b>482</b>. The one or more user devices <b>440</b>, <b>450</b> provide the appropriate interfaces/processing to provide visual surveillance and reporting.</p><p id="p-0109" num="0108">In other implementations, the system <b>400</b> further includes network <b>405</b> and the sensors <b>420</b>, the module <b>422</b>, the camera <b>430</b>, and the robotic devices <b>480</b> and <b>482</b> are configured to communicate sensor and image data to the one or more user devices <b>440</b>, <b>450</b> over network <b>405</b> (e.g., the Internet, cellular network, etc.). In yet another implementation, the sensors <b>420</b>, the module <b>422</b>, the camera <b>430</b>, and the robotic devices <b>480</b> and <b>482</b> (or a component, such as a bridge/router) are intelligent enough to change the communication pathway from a direct local pathway when the one or more user devices <b>440</b>, <b>450</b> are in close physical proximity to the sensors <b>420</b>, the module <b>422</b>, the camera <b>430</b>, and the robotic devices <b>480</b> and <b>482</b> to a pathway over network <b>405</b> when the one or more user devices <b>440</b>, <b>450</b> are farther from the sensors <b>420</b>, the module <b>422</b>, the camera <b>430</b>, and the robotic devices <b>480</b> and <b>482</b>. In some examples, the system leverages GPS information from the one or more user devices <b>440</b>, <b>450</b> to determine whether the one or more user devices <b>440</b>, <b>450</b> are close enough to the sensors <b>420</b>, the module <b>422</b>, the camera <b>430</b>, and the robotic devices <b>480</b> and <b>482</b> to use the direct local pathway or whether the one or more user devices <b>440</b>, <b>450</b> are far enough from the sensors <b>420</b>, the module <b>422</b>, the camera <b>430</b>, and the robotic devices <b>480</b> and <b>482</b> that the pathway over network <b>405</b> is required. In other examples, the system leverages status communications (e.g., pinging) between the one or more user devices <b>440</b>, <b>450</b> and the sensors <b>420</b>, the module <b>422</b>, the camera <b>430</b>, and the robotic devices <b>480</b> and <b>482</b> to determine whether communication using the direct local pathway is possible. If communication using the direct local pathway is possible, the one or more user devices <b>440</b>, <b>450</b> communicate with the sensors <b>420</b>, the module <b>422</b>, the camera <b>430</b>, and the robotic devices <b>480</b> and <b>482</b> using the direct local pathway. If communication using the direct local pathway is not possible, the one or more user devices <b>440</b>, <b>450</b> communicate with the sensors <b>420</b>, the module <b>422</b>, the camera <b>430</b>, and the robotic devices <b>480</b> and <b>482</b> using the pathway over network <b>405</b>.</p><p id="p-0110" num="0109">In some implementations, the system <b>400</b> provides end users with access to images captured by the camera <b>430</b> to aid in decision making. The system <b>400</b> may transmit the images captured by the camera <b>430</b> over a wireless WAN network to the user devices <b>440</b>, <b>450</b>. Because transmission over a wireless WAN network may be relatively expensive, the system <b>400</b> uses several techniques to reduce costs while providing access to significant levels of useful visual information.</p><p id="p-0111" num="0110">In some implementations, a state of the monitoring system and other events sensed by the monitoring system may be used to enable/disable video/image recording devices (e.g., the camera <b>430</b>). In these implementations, the camera <b>430</b> may be set to capture images on a periodic basis when the alarm system is armed in an &#x201c;Away&#x201d; state, but set not to capture images when the alarm system is armed in a &#x201c;Stay&#x201d; state or disarmed. In addition, the camera <b>430</b> may be triggered to begin capturing images when the alarm system detects an event, such as an alarm event, a door opening event for a door that leads to an area within a field of view of the camera <b>430</b>, or motion in the area within the field of view of the camera <b>430</b>. In other implementations, the camera <b>430</b> may capture images continuously, but the captured images may be stored or transmitted over a network when needed.</p><p id="p-0112" num="0111">Further, in some implementations, the system <b>400</b> intelligently leverages the robotic devices <b>480</b> and <b>482</b> to aid in security monitoring, property automation, and property management. For example, the robotic devices <b>480</b> and <b>482</b> may aid in investigating alarm events detected at the property by the monitoring system control unit <b>410</b>. In this example, the monitoring system control unit <b>410</b> may detect an alarm event (e.g., a fire alarm, an entry into the property when the system is armed &#x201c;Stay,&#x201d; etc.) and, based on the detected alarm event, control the robotic devices <b>480</b> and <b>482</b> to attempt to identify persons in the property at the time of the alarm event. Specifically, the monitoring system control unit <b>410</b> may send a control command to each of the robotic devices <b>480</b> and <b>482</b> that causes the robotic devices <b>480</b> and <b>482</b> to perform a coordinated and automated search for persons in the property. Based on the control command received, each of the robotic devices <b>480</b> and <b>482</b> begins navigating the property and captures images of the property while navigating. Each of the robotic devices <b>480</b> and <b>482</b> may execute a predefined navigation pattern within the property or the robotic devices <b>480</b> and <b>482</b> may execute a coordinated scan of the property in which the robotic devices <b>480</b> and <b>482</b> exchange location information and navigate to areas that have not been explored by one of the other devices.</p><p id="p-0113" num="0112">In some examples, the robotic devices <b>480</b> and <b>482</b> may analyze the images captured during the scan of the property for the presence of persons in the captured images. For instance, the robotic devices <b>480</b> and <b>482</b> may use image processing techniques in an attempt to identify shapes in the captured images that resemble a human body. The robotic devices <b>480</b> and <b>482</b> also may analyze the images for moving objects (or use other techniques to identify moving objects) and target imaging on capture of moving objects.</p><p id="p-0114" num="0113">Based on detection of a human or a moving object, the robotic devices <b>480</b> and <b>482</b> may lock onto the human or moving object and follow the human or moving object throughout the property. In this regard, the robotic devices <b>480</b> and <b>482</b> may follow the human or moving object throughout the property and capture images of the movement. In addition, once one of the robotic devices <b>480</b> and <b>482</b> locks onto a human or moving object, the robotic devices <b>480</b> and <b>482</b> coordinate to ensure that multiple of the robotic devices <b>480</b> and <b>482</b> do not lock onto the same human or moving object. The coordination may be direct amongst the robotic devices <b>480</b> and <b>482</b> and/or through the monitoring system control unit <b>410</b>. The coordination may involve sharing the location of the human or moving object and/or attributes of the human or moving object being imaged. Based on the shared location and attributes, the robotic devices <b>480</b> and <b>482</b> may determine whether multiple robotic devices <b>480</b> and <b>482</b> have locked onto the same object and take action accordingly. If the robotic devices <b>480</b> and <b>482</b> determine that the robotic devices <b>480</b> and <b>482</b> have not locked onto the same object, the appropriate one of the robotic devices <b>480</b> and <b>482</b> continues to lock onto the object while the other robotic devices scan other areas of the property for other objects. If the robotic devices <b>480</b> and <b>482</b> determine that the robotic devices <b>480</b> and <b>482</b> have locked onto the same object, the robotic devices <b>480</b> and <b>482</b> negotiate to determine which of the robotic devices <b>480</b> and <b>482</b> will continue to lock onto the object while the other robotic devices stop locking onto the object and scan other areas of the property for other objects. The negotiation may select the robotic device that continues tracking the object based on one or more factors including the timing of when the devices locked onto the object (e.g., which device locked onto the object first), the positioning of the devices relative to the object (e.g., which is best positioned to image the object), the amount of battery power remaining (e.g., the device with the most battery power remaining), or any other factor that indicates the device most suited to track the object. To the extent the device tracking an object becomes less suitable for tracking the object (e.g., the battery power is running low), the robotic devices <b>480</b> and <b>482</b> may coordinate to hand off tracking of the object to another one of the robotic devices <b>480</b> and <b>482</b>.</p><p id="p-0115" num="0114">In some examples, the robotic devices <b>480</b> and <b>482</b> perform image recognition processing on the one or more images in an attempt to detect whether any identified humans are legitimate users of the property or intruders. In these examples, the robotic devices <b>480</b> and <b>482</b> may have access to images of legitimate users of the property and may compare images being captured to the accessed images of legitimate users. Based on the comparison, the robotic devices <b>480</b> and <b>482</b> use facial recognition techniques to determine whether the imaged user matches a legitimate user of the property or an intruder. The robotic devices <b>480</b> and <b>482</b> then use the determination of whether the imaged user matches a legitimate user of the property or an intruder to control further tracking operation.</p><p id="p-0116" num="0115">For example, based on a determination that the imaged user is an intruder, the robotic devices <b>480</b> and <b>482</b> may continue tracking the intruder and ensure that images sufficient to identify the intruder have been captured. In this example, the robotic devices <b>480</b> and <b>482</b> may attempt to capture biometric data from the intruder, such as voiceprint data, fingerprint data, and/or biological samples with DNA of the intruder. In addition, the robotic devices <b>480</b> and <b>482</b> may take action to thwart the purpose of the intruder. For example, the robotic devices <b>480</b> and <b>482</b> may fly in random patterns around the intruder, may play loud sounds near the intruder, may shine lights near the intruder, may output identifying information collected about the intruder (e.g., male, around six feet tall and one hundred eighty pounds), may enable a central station operator or first responder to talk to the intruder through a two-way voice communication session established through the monitoring system control unit <b>410</b> and the robotic device, and may take other actions directed to disrupting the intruder.</p><p id="p-0117" num="0116">Alternatively, based on a determination that the imaged user is a legitimate user, the robotic devices <b>480</b> and <b>482</b> may discontinue tracking the legitimate user and scan for intruders. The robotic devices <b>280</b> and <b>482</b> also may report the location of the legitimate user. The robotic devices <b>480</b> and <b>482</b> further may continue tracking the legitimate user and attempt to provide assistance to the user. For instance, if the alarm is a fire alarm event, the robotic devices <b>480</b> and <b>482</b> may stay near the legitimate user, continuously or periodically update the location of the legitimate user to assist another user or first responder in helping the legitimate user, provide audible reminders of what types of actions should be taken in a fire, enable a central station operator or first responder to talk to the legitimate user through a two-way voice communication session established through the monitoring system control unit <b>410</b> and the robotic device, and may take other actions directed to assisting the legitimate user.</p><p id="p-0118" num="0117">In some examples, the robotic devices <b>480</b> and <b>482</b> may be assigned to different areas of the property where the robotic devices <b>480</b> and <b>482</b> can move in an unobstructed manner. In these examples, the robotic devices <b>480</b> and <b>482</b> may be assigned to different levels in a property (e.g., an upstairs robotic device and a downstairs robotic device) and even different rooms or sections that are potentially blocked by doors. The monitoring system control unit <b>410</b> coordinate tracking movement based on the assigned areas. For instance, the monitoring system control unit <b>410</b> determines areas in a property where an event has been detected (e.g., where motion is sensed, where a door or window is opened, etc.) and only controls the robotic devices assigned to the determined areas to operate. In this regard, the monitoring system control unit <b>410</b> may use location of users determined using sensors to control operation of the robotic devices <b>480</b> and <b>482</b>.</p><p id="p-0119" num="0118">In addition, the robotic devices <b>480</b> and <b>482</b> may be assigned as interior and exterior devices. The interior devices may navigate throughout an interior of the property. The exterior devices may navigate about an exterior periphery of the property. The exterior devices may be weather conditioned to remain outdoors (e.g., in an outdoor enclosure) at all times such that the exterior devices can explore an exterior of the property at any suitable time. In addition, the exterior devices may remain inside the property and the monitoring system control unit <b>410</b> may open a door to enable an exterior robotic device to leave and return to the property. For instance, an exterior device may have a base or reference location in a garage of the property and the monitoring system control unit <b>410</b> may automatically open a garage door to allow the exterior device to leave the garage and explore the exterior of the property.</p><p id="p-0120" num="0119">In some implementations, the monitoring system control unit <b>410</b> may monitor operational status of the robotic devices <b>480</b> and <b>482</b> and coordinate further operation based on the operational status. In these implementations, the monitoring system control unit <b>410</b> may detect that a particular robotic device is no longer operational and control one or more other robotic devices to perform operations originally assigned to the non-operational robotic device. In addition, the monitoring system control unit <b>410</b> may determine that the non-operational robotic device was navigating close to an intruder and received an impact based on accelerometer data prior to becoming non-operational. In this case, the monitoring system control unit <b>410</b> may infer that the robotic device was smashed by the intruder and control other robotic devices based on the inference. For instance, after inferring a smash event, the monitoring system control unit <b>410</b> may control operation of other robotic devices to maintain distance from the intruder by only flying high overhead.</p><p id="p-0121" num="0120">In some implementations, the monitoring system control unit <b>410</b> may determine battery power available for each of the robotic devices <b>480</b> and <b>482</b> and coordinate operation of the robotic devices <b>480</b> and <b>482</b> based on available battery power. In these implementations, the robotic devices <b>480</b> and <b>482</b> may report battery power remaining to the monitoring system control unit <b>410</b> and the monitoring system control unit <b>410</b> may determine a subset of the robotic devices <b>480</b> and <b>482</b> to deploy based on the battery power information. For instance, the monitoring system control unit <b>410</b> may select to initially deploy the robotic device with the most available battery power to allow the other robotic devices to charge while the selected device assists with monitoring. Once the battery power for the selected device falls below a threshold, the monitoring system control unit <b>410</b> may return the selected device to a charging station and select the robotic device with the presently highest available battery power to resume the monitoring options being performed. The monitoring system control unit <b>410</b> may cycle through all of the robotic devices <b>480</b> and <b>482</b> in an intelligent manner that best leverages the battery power available. If the battery power of a device becomes too low to effectively operate as a navigating device, the monitoring system control unit <b>410</b> may control the robotic device to remain stationary and act as a stationary camera or other sensor to still assist with monitoring, although the added benefit of navigation no longer exists.</p><p id="p-0122" num="0121">In addition to battery, the monitoring system control unit <b>410</b> may select the robotic device to deploy and what action to take based on the sensor that triggered the event, a time of day, and a state of the system. For instance, if the monitoring system control unit <b>410</b> detects an unusual motion sensor event, the monitoring system control unit <b>410</b> may select the nearest robotic device to navigate to an area of the property where motion was detected and investigate. Alternatively, if the monitoring system control unit <b>410</b> detects a critical alarm event (e.g., a security breach of a system armed stay, a fire alarm, a carbon monoxide alarm, etc.), the monitoring system control unit <b>410</b> may deploy all robotic devices <b>480</b> and <b>482</b> at any time of the day. If the monitoring system control unit <b>410</b> detects an intrusion breach, the monitoring system control unit <b>410</b> may assign some devices to &#x201c;attack&#x201d; the intruder by disrupting the purpose of the intruder and collecting identifying information for the intruder and assign some devices to search for other users in the property. The selected devices and actions taken may vary based on sensor data, time of day, and the state of the monitoring system.</p><p id="p-0123" num="0122">In some implementations, the system <b>400</b> allows central station operators, first responders, and/or users of the property to interact with and control the robotic devices <b>480</b> and <b>482</b>. In these implementations, a central station operator, first responder, or user of the property may provide input to control the robotic devices <b>480</b> and <b>482</b> in a manner that best assists with monitoring and investigation of detected events. For instance, the central station operator, first responder, or user of the property may remotely control navigation of the robotic devices <b>480</b> and <b>482</b>. The central station operator, first responder, or user of the property also may provide general commands related to actions the robotic devices <b>480</b> and <b>482</b> are designed to take. In response to these general commands, the robotic devices <b>480</b> and <b>482</b> may automatically perform the desired actions, such as following an instruction to explore the property or following an instruction to navigate to an upstairs bedroom.</p><p id="p-0124" num="0123">In some examples, the robotic devices <b>480</b> and <b>482</b> may periodically perform test sequences to ensure the robotic devices <b>480</b> and <b>482</b> will operate correctly if needed. In these examples, the robotic devices <b>480</b> and <b>482</b> may periodically navigate predefined navigation patterns used to investigate the property and/or may navigate around the property in a scanning sequence. The robotic devices <b>480</b> and <b>482</b> may determine whether the test sequences perform correctly or whether an error occurs that prevents full investigation of the property. To the extent an error occurs, the robotic devices <b>480</b> and <b>482</b> report the error and enable a user of the property or a technician to correct the error prior to a time when the robotic devices <b>480</b> and <b>482</b> would be needed for safety monitoring.</p><p id="p-0125" num="0124">The monitoring system control unit <b>410</b> also may arrange the test sequences to occur during periods of time that are convenient for users of the property. For example, the monitoring system control unit <b>410</b> may assess sensor data at the property and determine a time period in which the property is unoccupied and unlikely to be occupied until the test sequences complete. In this example, the monitoring system control unit <b>410</b> waits until the preferred time period to initiate test sequences for one or more of the robotic devices <b>480</b> and <b>482</b>.</p><p id="p-0126" num="0125">In some examples, the robotic devices <b>480</b> and <b>482</b> may be used to provide a critical alert to a user in the property or attempt to wake a sleeping person as appropriate. In these examples, none of the users may be responding to a critical alert and, in response, the monitoring system control unit <b>410</b> may control the robotic devices <b>480</b> and <b>482</b> to search for a person in the property and provide the critical alert very close to an identified person in a manner that is highly likely to gain the person's attention to the critical alert. In the event that the person appears to be sleeping in the property, the robotic devices <b>480</b> and <b>482</b> may attempt to wake the person by providing loud input very near the person and/or by making contact with the person. In this regard, the robotic devices <b>480</b> and <b>482</b> may be useful in waking a sleeping person when a fire or carbon monoxide alarm has been detected and the person needs to leave the property. The robotic devices <b>480</b> and <b>482</b> also may determine when a person is nonresponsive (e.g., unconscious) and may be need of immediate assistance. Also, the robotic devices <b>480</b> and <b>482</b> may serve as an alarm clock for critical meetings based on a person having trouble waking up using traditional alarm clocks.</p><p id="p-0127" num="0126">In some implementations, the robotic devices <b>480</b> and <b>482</b> may operate as mobile sensors that move throughout the property. In some examples, the robotic devices <b>480</b> and <b>482</b> may have humidity sensors, air flow sensors, temperature sensors, or the like. In these examples, the robotic devices <b>480</b> and <b>482</b> may periodically navigate throughout the property and take temperature, humidity, and air flow measurements at various locations throughout the property. The system <b>400</b> may use the temperature, humidity, and air flow measurements to detect inefficient areas of the property. The inefficiencies may be used to detect areas where insulation in the property in deficient (e.g., new siding, windows, and/or doors may be useful in certain areas) and/or where leaks exist in the property. The property efficiency information may be provided to a user of the property to enable the user to improve efficiency in the property.</p><p id="p-0128" num="0127"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of an example of a process <b>500</b> for tracking user behavior patterns. In general, the process <b>500</b> may include obtaining data related to a user's behavior (<b>510</b>), aggregating the obtained data based on a predetermined time period (<b>520</b>), detecting a user behavior pattern (<b>530</b>), and generating a pre-surveillance rule (<b>540</b>).</p><p id="p-0129" num="0128">In more detail, the process may begin with a monitoring system using a monitoring system control unit, or other computer, to obtain data related to user's behavior at stage <b>510</b>. Data related to a user's behavior may be obtained from one or more sensors. For instance, a property may include one or more sensors such as motion sensors, door open sensors, door close sensors, or a combination therefore. As a user moves through the property, the sensors may detect the user's movement, and transmit data related to the user's movement to a monitoring system control unit. Alternatively, or in addition, data related to a user's behavior may be obtained from one or more user devices used by the user. For instance, the monitoring system control unit may obtain data from the user's smartphone (or other handheld device), the user's smartwatch (or other wearable device), or the like. In some implementations, the user's user device may periodically transmit the user's location to the monitoring system control unit. Alternatively, or in addition, the data related to a user's behavior may be obtained by one or more sensors mounted to one or more drones that can move freely around the inside of a property, outside of a property, long range distances away from the property, or a combination thereof. For instance, a drone may be configured to follow within a predetermined distance of a user, and use one or more drone sensors to obtain data related to the user's behavior and transmit obtained data to the monitoring system control unit. Once received, the monitoring system control unit can use this data to identify one or more user behavior patterns, one or more drone navigational patters, or the like.</p><p id="p-0130" num="0129">At stage <b>520</b>, the monitoring system may use a monitoring system control unit, or other computer, to aggregate the obtained data. In some implementations, the obtained data may be aggregated over a predetermined period of time. For example, the monitoring system control unit may aggregate all data obtained for a particular user over the course of the last week, last two weeks, last month, or the like.</p><p id="p-0131" num="0130">At stage <b>530</b>, the monitoring system use a monitoring system control unit, or other computer, to detect a user behavior pattern. Detecting a user behavior pattern may include identifying similar actions performed by the user at the same time of particular days of a week. Such patterns may include, for example, a user leaving the house every weekday at 7:00 am. The system may make that determination because, for example, sensor data obtained over the last month indicates that the user consistently leaves the user's property every weekday at 7:00 am. Other examples of user behavior patterns include kids going outside to play in the yard at 12:00 pm on the weekends, a user leaving work at 5:00 pm on weekdays, a family leaving their house at 10:30 am on Sunday's to go to church, a user walking a dog every night at 8:45 pm, or the like.</p><p id="p-0132" num="0131">The stage of detecting a user's behavior also includes the monitoring system's ability to analyze data within a range of time periods, eliminate data that is a statistical outlier, and the like. For example, the monitoring system control unit may cluster data obtained from a sensor at the user's front door. The clustered data for a two week period of time may indicate that with the exception of one day in that two week span where the user left the house at 10:00 am, the user left the property within the time period of 6:53 am and 7:04 am. Accordingly, the monitoring system control unit may determine that user's behavioral pattern is leaving the house every day between 6:53 and 7:04 pm. The monitoring system control unit may disregard the statistical outlier time of 10:00 am because the user's behavioral history indicates that the user leaving late one day at 10:00 am is not related to the user's pattern of regularly leaving within the time range of 6:53 am to 7:04 pm.</p><p id="p-0133" num="0132">At stage <b>540</b>, the monitoring system control unit may generate a pre-surveillance rule. The pre-surveillance rule may include (i) a trigger time (e.g., 6:55 am) that is a predetermined amount of time before the user's <b>102</b> behavioral pattern begins (e.g., 7:00 am). In some implementations, a user's behavioral pattern may be associated with a range of times as opposed to a particular time. In such instances, the trigger time may be selected as a time that is a predetermined time before the earliest time in the range of times associated with the user's behavioral pattern. The pre-surveillance rule may also include a location that is to be pre-surveilled. The location to be pre-surveilled may be the location associated with the user's behavioral pattern. The location may include, for example, a downstairs portion of a property, an upstairs portion of a property, a parking lot, a driveway, a neighborhood, or the like. The pre-surveillance rule may also include an initial predetermined navigational path. The initial predetermined navigational path may include an initial navigational path that a drone is supposed to travel in response to receiving a pre-surveillance instruction. The initial navigational path is used to provide structured guidance to the drone during pre-surveillance activities.</p><p id="p-0134" num="0133">However, the drone is capable of veering from the initial navigational path if a potential threat to the safeness of the location being pre-surveilled is detected so that the drone can fully investigate the potential threat to the safeness of the location. In some implementations, a user of a user device may be provided the option to take control of the drone once a potential threat to safeness has been identified. In such instances, the user may manually fly the drone using, for example, an application on the user's device. Once the potential threat to safeness is investigated further, the user may transmit an instruction to the drone to return to autonomous navigation based on the initial navigational path.</p><p id="p-0135" num="0134"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart of a process <b>600</b> for performing drone pre-surveillance based on a detected user behavior pattern. In general, the process <b>600</b> may include monitoring a user's behavior using a monitoring system (<b>610</b>), determining whether to initiate execution of a pre-surveillance rule (<b>620</b>), determining a drone navigation path that is associated with a pre-surveillance rule (<b>630</b>), and transmitting an instruction to a drone to perform pre-surveillance along the navigation path (<b>640</b>).</p><p id="p-0136" num="0135">In more detail, the process <b>600</b> may begin at stage <b>610</b> by monitoring a user's behavior using a monitoring system. Monitoring the user's behavior may include obtaining data obtained related to the user's behavior from one or more sensors, one or more user devices, one or more drones, or the like. The obtained data may then be analyzed to determine if the data is consistent with user behavioral patterns associated with one or more pre-surveillance rules.</p><p id="p-0137" num="0136">The process <b>600</b> may continue at stage <b>620</b> by determining whether to initiate execution of a pre-surveillance rule. The determination of whether to initiate execution of a pre-surveillance rule may be based on the data collected by the monitoring system at stage <b>610</b>. For example, determining whether to initiate execution of a pre-surveillance rule may include applying the collected user data, sensor data, or a combination thereof, to a pre-surveillance rule that identifies an area of one or more properties for pre-surveillance based on a likely action of the occupant of the property. Execution of the pre-surveillance rule may be initiated and deployment of the drone triggered at the trigger time associated with the pre-surveillance rule if, for example, the applied user data, sensor data, or both, and a time associated with the applied user data, sensor data, or both, satisfies the constraints of the pre-surveillance rule.</p><p id="p-0138" num="0137">By way of example, if the data obtained by the monitoring system is consistent with a user's behavior pattern associated with a pre-surveillance rule, the monitoring system control unit may initiate execution of the pre-surveillance rule at the trigger time associated with the pre-surveillance rule. For example, a pre-surveillance rule may be (i) associated with the user behavioral pattern of leaving a property at 7:00 am on a weekday to go to work, and (ii) include a trigger time of 6:55 am. The monitoring system may determine that the user's behavioral pattern is consistent with the behavioral pattern associated with the pre-surveillance rule if one or more motion sensors throughout the house show the user moving from room to room getting ready for work between the times of, for example, 6:00 am to 6:54 am, or the like. Accordingly, in such instances, at 6:55 am the system may determine to initiate execution of the pre-surveillance rule.</p><p id="p-0139" num="0138">On the other hand, however, for example, a pre-surveillance rule may be (i) associated with the user behavioral pattern of leaving a property at 7:00 am on a weekday to go to work, and (ii) include a trigger time of 6:55 am. However, if the monitoring system determines that the user still has not gotten out of bed at 6:54 am (e.g., because one or more motion sensors in the user's bedroom, or any other portion of the house, are not detecting movement), then the monitoring system may determine to not initiate execution of the pre-surveillance rule at 6:55 am. Such an option may be employed to save wear and tear on the drone device by preventing the drone device from performing unnecessary pre-surveillance activities.</p><p id="p-0140" num="0139">In yet other implementations, however, the monitoring system may initiate execution of a pre-surveillance rule at the designated trigger time regardless of the current state of the user. However, over time, in some implementations, if the monitoring system fails to identify a user regularly performing a particular behavioral pattern associated with a particular pre-surveillance rule (e.g., the user stops performing the behavioral pattern, the user's habits change, or the like), the monitoring system may delete the generated pre-surveillance rule because it is no longer associated with a regular behavioral pattern of the user.</p><p id="p-0141" num="0140">The process <b>600</b> may continue at stage <b>630</b> by determining a drone navigation path. In some implementations, a drone navigation path may be selected based on the area of the property associated with the pre-surveillance rule. The drone navigation path may include a predetermined navigation path that was pre-programmed into the drone. In some implementations, the navigation path may be selected independent of user behavioral data collected by the monitoring system. For instance, a predefined navigational path may include a drone traveling out the door of a property, window of a property, garage door of the property or the like that was opened in response to an instruction from the monitoring system control unit. The door of a property, window of a property, garage door of a property may be connected to the monitoring system, be automated, and respond to open/close instructions from the monitoring control unit. Once it has exited the property, the drone may then fly in a circle around a car in the driveway of the property independent of a user's path to the car. Alternatively, the predefined navigational path may be based on user behavioral data collected by the monitoring system. For instance, the monitoring system may obtain information regarding a path traveled by a user such as a dog walk path. Then, for example, the information specifying the path of the user may be associated be associated with the dog walk pre-surveillance rule, and used as the initial predetermined navigational path for the drone.</p><p id="p-0142" num="0141">The process <b>600</b> may continue at stage <b>640</b> by transmitting data that includes an instruction to a drone to perform pre-surveillance along an initial navigation path. The instruction may include sufficient instructions to trigger the deployment of the drone that receives the transmitted data.</p><p id="p-0143" num="0142"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of a process <b>700</b> for performing drone pre-surveillance. The process <b>700</b> may include receiving an instruction to perform pre-surveillance (<b>710</b>), travelling along a predetermined navigation path (<b>720</b>), obtaining data associated with the environment within a predetermined distance of the navigation path (<b>730</b>), and generating a report that indicates a level of safeness of the environment within a predetermined distance of the navigation path (<b>740</b>).</p><p id="p-0144" num="0143">The process <b>700</b> may begin at stage <b>710</b> with a drone receiving an instruction to perform pre-surveillance. In response to the receipt of the instruction to perform pre-surveillance, the drone may begin travelling at stage <b>720</b> along an initial predetermined navigation path.</p><p id="p-0145" num="0144">While on traveling along the initial predetermined navigational path, the drone may being obtaining data associated with the environment within a predetermined distance of the navigation path at stage <b>730</b>. The obtained data may include one or more videos, still images, or the like that were taken of the environment during the pre-surveillance flight. The still images may include facial recognition scans of a person present in the environment. Alternatively, the obtained data may be indicative of sensor data that is collected by one or more drone sensors.</p><p id="p-0146" num="0145">The process may continue at stage <b>740</b> by determining a level of safeness associated with the environment based on the obtained data. The level of safeness may be determined based on a variety of factors. For example, a level of safeness may be determined based on a number of predefined safety risks. In one implementation, the level of safeness may be impacted based on whether one or more individuals are loitering in the property being pre-surveilled. Alternatively, or in addition, the level of safeness may be further impacted based on whether the loitering individuals are armed with one or more weapons. The drone may determine if the loitering individual is armed with one or more weapons by taking pictures of the loitering individual's hands, and searching one or more image databases to determine whether the captured image includes a weapon. Alternatively, or in addition, the drone may analyze the loitering individuals clothing to identify bulges that may be associated with a concealed weapon. Alternatively, or in addition, the level of safeness may be impacted based on the drone determining that the loitering individual is dressed suspiciously. A loitering individual may be dressed suspiciously, for example, if the loitering individual is wearing a mask. Alternatively, or in addition, the drone may perform facial recognition analysis to determine whether the loitering individual is associated with a record in one or more of a local law enforcement databases, a federal law enforcement databases, a sex offender database, or the like. For example, the drone may be able to determine if there is an issued warrant out for the loitering individual's arrest, whether the loitering individual has a criminal record, or the like. The existence of one or more of the aforementioned safety risk factors may result in an environment being determined to be less safe.</p><p id="p-0147" num="0146">If none of the aforementioned safety risks are identified, then a pre-surveilled region may be determined to be safe. On the other hand, if the drone identifies multiple safety risks, then a pre-surveilled region may be determined to be unsafe. In a similar manner, a pre-surveilled region may be determined to be moderately safe if the drone merely finds a loitering individual without identifying any other safety risks, or other minor safety issues.</p><p id="p-0148" num="0147">The process <b>700</b> may continue at stage <b>740</b> with the drone generating a report that indicates a level of safeness of the environment within a predetermined distance of the navigation path. In some implementations, the report includes data that provides a notification to the user's user device indicating whether the environment is safe, unsafe, unknown, or the like. Alternatively, or in addition, the level of safeness may be indicated as a probability that indicates the likelihood that the environment is safe.</p><p id="p-0149" num="0148">Embodiments of the subject matter, the functional operations and the processes described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible nonvolatile program carrier for execution by, or to control the operation of, data processing apparatus. Alternatively or in addition, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.</p><p id="p-0150" num="0149">The term &#x201c;data processing apparatus&#x201d; encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). The apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.</p><p id="p-0151" num="0150">A computer program (which may also be referred to or described as a program, software, a software application, a module, a software module, a script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a standalone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.</p><p id="p-0152" num="0151">The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).</p><p id="p-0153" num="0152">Computers suitable for the execution of a computer program include, by way of example, can be based on general or special purpose microprocessors or both, or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a universal serial bus (USB) flash drive), to name just a few.</p><p id="p-0154" num="0153">Computer readable media suitable for storing computer program instructions and data include all forms of nonvolatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.</p><p id="p-0155" num="0154">To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.</p><p id="p-0156" num="0155">Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (&#x201c;LAN&#x201d;) and a wide area network (&#x201c;WAN&#x201d;), e.g., the Internet.</p><p id="p-0157" num="0156">The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.</p><p id="p-0158" num="0157">While this specification contains many specific implementation details, these should not be construed as limitations on the scope of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.</p><p id="p-0159" num="0158">Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.</p><p id="p-0160" num="0159">Particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In certain implementations, multitasking and parallel processing may be advantageous. Other steps may be provided, or steps may be eliminated, from the described processes. Accordingly, other implementations are within the scope of the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. (canceled)</claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. A method comprising:<claim-text>receiving, by a robotic device, an instruction to perform pre-surveillance of a navigation path;</claim-text><claim-text>travelling, by the robotic device, along the navigation path;</claim-text><claim-text>obtaining, by the robotic device, data associated with an environment within a predetermined distance of the navigation path;</claim-text><claim-text>determining, by the robotic device, a safety level of the environment within the predetermined distance of the navigation path using the obtained data; and</claim-text><claim-text>performing, by the robotic device, one or more automated actions using the safety level of the environment within the predetermined distance of the navigation path.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the navigation path is associated with a likely location where a user is predicted to perform an action.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein:<claim-text>the action predicted to be performed by the user comprises the user to leaving a property through an exit; and</claim-text><claim-text>determining the safety level comprises determining a safety level associated with the user leaving the property through the exit.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein determining the safety level associated with the environment using the obtained data comprises:<claim-text>determining the safety level using a plurality of predefined safety risks.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, the plurality of predefined safety risks comprises one or more of:<claim-text>whether an individual is loitering in the environment being pre-surveilled;</claim-text><claim-text>whether the loitering individual is armed with one or more weapons;</claim-text><claim-text>whether the loitering individual is dressed suspiciously; or</claim-text><claim-text>whether the loitering individual is associated with a criminal record.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein determining the safety level using the plurality of predefined safety risks comprises:<claim-text>determining that none of the plurality of predefined safety risks are identified; and</claim-text><claim-text>determining that the environment within the predetermined distance of the navigation path is likely safe.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein determining the safety level using the plurality of predefined safety risks comprises:<claim-text>determining that a number of the plurality of predefined safety risks are identified; and</claim-text><claim-text>determining that the environment within the predetermined distance of the navigation path is likely unsafe.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the safety level comprises a probability that indicates a likelihood that the environment is likely safe.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein performing the one or more automated actions comprises:<claim-text>engaging, by the robotic device, with a loitering individual detected using the data associated with the environment in accordance with the safety level of the environment within the predetermined distance of the navigation path.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein performing the one or more automated actions comprises:<claim-text>sending, by the robotic device and to a user device, a notification comprising a report that indicates the safety level of the environment within the predetermined distance of the navigation path.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A system comprising:<claim-text>one or more computing devices; and</claim-text><claim-text>one or more storage devices storing instructions that, when executed by the one or more computing devices, cause the one or more computing devices to perform operations comprising:<claim-text>receiving an instruction to perform pre-surveillance of a navigation path;</claim-text><claim-text>travelling along the navigation path;</claim-text><claim-text>obtaining data associated with an environment within a predetermined distance of the navigation path;</claim-text><claim-text>determining a safety level of the environment within the predetermined distance of the navigation path using the obtained data; and</claim-text><claim-text>performing one or more automated actions using the safety level of the environment within the predetermined distance of the navigation path.</claim-text></claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the navigation path is associated with a likely location where a user is predicted to perform an action.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein:<claim-text>the action predicted to be performed by the user comprises the user to leaving a property through an exit; and</claim-text><claim-text>determining the safety level comprises determining a safety level associated with the user leaving the property through the exit.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein determining the safety level associated with the environment using the obtained data comprises:<claim-text>determining the safety level using a plurality of predefined safety risks.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, the plurality of predefined safety risks comprises one or more of:<claim-text>whether an individual is loitering in the environment being pre-surveilled;</claim-text><claim-text>whether the loitering individual is armed with one or more weapons;</claim-text><claim-text>whether the loitering individual is dressed suspiciously; or</claim-text><claim-text>whether the loitering individual is associated with a criminal record.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein determining the safety level using the plurality of predefined safety risks comprises:<claim-text>determining that none of the plurality of predefined safety risks are identified; and</claim-text><claim-text>determining that the environment within the predetermined distance of the navigation path is likely safe.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein determining the safety level using the plurality of predefined safety risks comprises:<claim-text>determining that a number of the plurality of predefined safety risks are identified; and</claim-text><claim-text>determining that the environment within the predetermined distance of the navigation path is likely unsafe.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the safety level comprises a probability that indicates a likelihood that the environment is likely safe.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein performing the one or more automated actions comprises:<claim-text>engaging with a loitering individual detected using the data associated with the environment in accordance with the safety level of the environment within the predetermined distance of the navigation path.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. At least one non-transitory computer-readable storage device storing instructions that, when executed by one or more computers, causes the one or more computers to perform operations comprising:<claim-text>receiving an instruction to perform pre-surveillance of a navigation path;</claim-text><claim-text>travelling along the navigation path;</claim-text><claim-text>obtaining data associated with an environment within a predetermined distance of the navigation path;</claim-text><claim-text>determining a safety level of the environment within the predetermined distance of the navigation path using the obtained data; and</claim-text><claim-text>performing one or more automated actions using the safety level of the environment within the predetermined distance of the navigation path.</claim-text></claim-text></claim></claims></us-patent-application>