<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005459A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005459</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17785051</doc-number><date>20201228</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2020-003795</doc-number><date>20200114</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>H</subclass><main-group>1</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>H</subclass><main-group>1</main-group><subgroup>0025</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc></classifications-cpc><invention-title id="d2e61">INFORMATION PROCESSING APPARATUS, INFORMATION PROCESSING METHOD, AND PROGRAM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Sony Group Corporation</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>AKAMA</last-name><first-name>Taketo</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Sony Group Corporation</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/049097</doc-number><date>20201228</date></document-id><us-371c12-date><date>20220614</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present disclosure relates to an information processing apparatus, an information processing method, and a program that make it possible to adjust commonness and eccentricity of automatically generated content by likelihood exploration while satisfying reality.</p><p id="p-0002" num="0000">Input content including a sequence of data is encoded to be converted into a latent variable, the latent variable is decoded to reconfigure output content, a loss function is calculated on the basis of a likelihood of the input content which is an input sequence, a gradient of the loss function is lowered to update the latent variable, and the updated latent variable is decoded to reconfigure output content. The present invention can be applied to an automatic content generation device.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="118.19mm" wi="158.75mm" file="US20230005459A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="55.03mm" wi="134.11mm" file="US20230005459A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="192.45mm" wi="152.40mm" orientation="landscape" file="US20230005459A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="162.64mm" wi="135.47mm" orientation="landscape" file="US20230005459A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="112.18mm" wi="114.22mm" file="US20230005459A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="195.58mm" wi="143.26mm" file="US20230005459A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="178.05mm" wi="139.87mm" file="US20230005459A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="204.39mm" wi="157.73mm" orientation="landscape" file="US20230005459A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="176.36mm" wi="152.74mm" orientation="landscape" file="US20230005459A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="220.90mm" wi="139.95mm" file="US20230005459A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="180.85mm" wi="91.61mm" orientation="landscape" file="US20230005459A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="96.18mm" wi="85.26mm" file="US20230005459A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="163.15mm" wi="127.68mm" orientation="landscape" file="US20230005459A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0001">The present disclosure relates to an information processing apparatus, an information processing method, and a program, and more particularly, to an information processing apparatus, an information processing method, and a program which make it possible to adjust commonness and eccentricity of automatically generated content while satisfying reality.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0004" num="0002">Information processing using machine learning is utilized in various technical fields. For example, a technique has been proposed in which new content is automatically generated by learning features of content (such as images and music) by using a neural network that simulates a mechanism of a cranial nervous system.</p><p id="p-0005" num="0003">For example, a technique has been proposed which enables automatic composition of an appropriate song suitable for lyrics by learning features of an existing song even when a user does not input parameters other than the lyrics (see Patent Document 1).</p><p id="p-0006" num="0004">With this technique, the language feature amount calculated from the lyrics data indicating the lyrics of each song and the attribute data indicating the attribute of the song are learned, so that when new lyrics data is given, a song matching the new lyrics data can be automatically generated.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Document</heading><p id="p-0007" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0005">Patent Document 1: Japanese Patent Application Laid-Open No. 2011-175006</li></ul></p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><heading id="h-0006" level="1">Problems to be Solved by the Invention</heading><p id="p-0008" num="0006">However, the technique described in Patent Document 1 only generates melodies and chords according to the lyrics, and there is a possibility that the generated music is excessively common or excessively eccentric.</p><p id="p-0009" num="0007">When the generated music is common, there is a possibility that the generated music lacks fun, and when the generated music is eccentric, there is a possibility that preferences are divided. Moreover, when the generated music is excessively eccentric, there is a possibility that the generated music cannot be recognized as music and lacks reality as music.</p><p id="p-0010" num="0008">In this regard, it is conceivable to adjust the generated music to be intermediate between commonness and eccentricity, but it is difficult to implement the adjustment for an intermediate between commonness and eccentricity.</p><p id="p-0011" num="0009">This is not limited to the automatically generated music, and the same applies to a case where various contents such as images and sentences are automatically generated.</p><p id="p-0012" num="0010">The present disclosure has been made in view of such a situation, and in particular, an objective of the present disclosure is to make it possible to adjust commonness and eccentricity of automatically generated content while satisfying reality.</p><heading id="h-0007" level="1">Solutions to Problems</heading><p id="p-0013" num="0011">An information processing apparatus and a program according to one aspect of the present disclosure are an information processing apparatus and a program including: an encoder configured to encode input content including a sequence of data to convert the input content into a latent variable; a decoder configured to decode the latent variable to reconfigure output content; a loss function calculation unit configured to calculate a loss function on the basis of a likelihood of the input content; and a control unit configured to lower a gradient of the loss function to update the latent variable, and control the decoder to decode the updated latent variable to reconfigure output content.</p><p id="p-0014" num="0012">An information processing method according to one aspect of the present disclosure is an information processing method of an information processing apparatus including an encoder, a decoder, a loss function calculation unit, and a control unit, the method including steps of: by the encoder, encoding input content including a sequence of data to convert the input content into a latent variable; by the decoder, decoding the latent variable to reconfigure output content; by the loss function calculation unit, calculating a loss function on the basis of a likelihood of the input content; and by the control unit, lowering a gradient of the loss function to update the latent variable and controlling the decoder to decode the updated latent variable to reconfigure output content.</p><p id="p-0015" num="0013">In one aspect of the present disclosure, input content including a sequence of data is encoded to be converted into a latent variable, the latent variable is decoded to reconfigure output content, a loss function is calculated on the basis of a likelihood of the input content, a gradient of the loss function is lowered to update the latent variable, and the updated latent variable is decoded by the decoder to reconfigure output content.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0008" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0016" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram for explaining an outline of the present disclosure.</p><p id="p-0017" num="0015"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram for explaining a configuration example of an information processing apparatus of the present disclosure.</p><p id="p-0018" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for explaining functions implemented by the information processing apparatus of <figref idref="DRAWINGS">FIG. <b>2</b></figref> in a first embodiment.</p><p id="p-0019" num="0017"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram for explaining a real label and a fake label used for learning of a reality evaluator according to the first embodiment.</p><p id="p-0020" num="0018"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram for explaining a change in a latent variable based on a likelihood and reality.</p><p id="p-0021" num="0019"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart for explaining a content generation process in the first embodiment.</p><p id="p-0022" num="0020"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram for explaining functions implemented by the information processing apparatus of <figref idref="DRAWINGS">FIG. <b>2</b></figref> in a second embodiment.</p><p id="p-0023" num="0021"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram for explaining a real label and a fake label used for learning of a reality evaluator according to the second embodiment.</p><p id="p-0024" num="0022"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating a content generation process in the second embodiment.</p><p id="p-0025" num="0023"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram for explaining a first modification of the present disclosure.</p><p id="p-0026" num="0024"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram for explaining a second modification of the present disclosure.</p><p id="p-0027" num="0025"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram for explaining a configuration example of a general-purpose personal computer.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0009" level="1">MODE FOR CARRYING OUT THE INVENTION</heading><p id="p-0028" num="0026">Preferred embodiments of the present disclosure will be described in detail below with reference to the accompanying drawings. Note that in this description and the drawings, components having substantially the same functional configuration are designated by the same reference numerals to omit duplicate description.</p><p id="p-0029" num="0027">Hereinafter, modes for carrying out the present technology will be described. The description will be given in the following order.<ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0000">    <ul id="ul0003" list-style="none">        <li id="ul0003-0001" num="0028">1. Outline of present disclosure</li>        <li id="ul0003-0002" num="0029">2. First Embodiment</li>        <li id="ul0003-0003" num="0030">3. Second Embodiment</li>        <li id="ul0003-0004" num="0031">4. First modification</li>        <li id="ul0003-0005" num="0032">5. Second modification</li>        <li id="ul0003-0006" num="0033">6. Example of execution by software</li>    </ul>    </li></ul></p><heading id="h-0010" level="1">1. Outline of Present Disclosure</heading><p id="p-0030" num="0034">In particular, the present disclosure is to make it possible to adjust commonness and eccentricity of automatically generated content while satisfying reality.</p><p id="p-0031" num="0035">When automatically generating various contents such as music, images, and sentences, the generated contents are not often generated in a satisfactory state as intended by a content creator, and there is some dissatisfaction.</p><p id="p-0032" num="0036">The dissatisfaction of the creator with the generated content is often such that a specific part of the content generated under a certain condition is satisfied, but a part different from the specific part is not satisfied.</p><p id="p-0033" num="0037">That is, the dissatisfaction of the creator is often such that the creator is satisfied with a specific part but remains dissatisfied with the generated content as a whole.</p><p id="p-0034" num="0038">In such a case, the creator causes content to be automatically generated repeatedly while changing conditions until content which is satisfactory as a whole is generated, but it is rare that the content which is satisfactory as a whole is automatically generated.</p><p id="p-0035" num="0039">Therefore, the generated content which is not satisfactory as a whole but is partially satisfactory, in other words, most of the content which causes the creator to feel regretful is discarded.</p><p id="p-0036" num="0040">Here, an example of the most basic case of the content which causes the creator to feel regretful is that the sequence configuring the content is excessively common or excessively eccentric.</p><p id="p-0037" num="0041">In generating content, the creator seeks content with high originality, and thus, in general, the creator does not feel originality when excessively common content which is commonplace is generated.</p><p id="p-0038" num="0042">Therefore, there is a high possibility that the excessively common content is boring for the creator and a target person who views or listens to the content.</p><p id="p-0039" num="0043">Conversely, when the generated content is excessively eccentric, preferences are likely to be divided although originality is felt.</p><p id="p-0040" num="0044">Therefore, even when the creator likes content which is excessively eccentric, the content may not be accepted by the target person.</p><p id="p-0041" num="0045">For this reason, it is required to adjust the generated content to be intermediate between commonness and eccentricity, but it is difficult to adjust the generated content to be intermediate between commonness and eccentricity.</p><p id="p-0042" num="0046">Even when content preferred by ordinary people is investigated, a preferred labeled data set is created, a model reflecting the data set is learned, and the content is automatically generated, the preference of the ordinary people can be reflected, but there is a high possibility that commonness is increased, and the preference of the target person cannot be reflected.</p><p id="p-0043" num="0047">In particular, in the case of being applied to an application program which automatically generates content, the preference of the generated content changes according to the purpose of use of the generated content or the target person who views or listens to the content, and thus, it is necessary to adjust the preference of the generated content.</p><p id="p-0044" num="0048">In this regard, in the present disclosure, an intermediate between commonness and eccentricity of the generated content is adjusted using a likelihood.</p><p id="p-0045" num="0049">Here, the likelihood is a probability that sample content can be obtained.</p><p id="p-0046" num="0050">For example, in a case where the content is music, a probability that automatically generated music is collected sample music is set as the likelihood.</p><p id="p-0047" num="0051">Therefore, in this case, the fact that the likelihood of the generated music is high indicates that the generated music is music close to the music collected as samples, and there is a high possibility that the generated music is commonplace music, that is, ordinary music (music with high commonness).</p><p id="p-0048" num="0052">On the other hand, the low likelihood of the generated music indicates that the generated music is far from the music collected as samples, and is highly likely to be eccentric music (music with high eccentricity).</p><p id="p-0049" num="0053">In the present disclosure, as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, when the likelihood of the automatically generated content is adjusted, the adjustment is made to generate content with high likelihood, thereby generating content with high commonness, and conversely, the adjustment is made to generate content with low likelihood, thereby generating content with high eccentricity.</p><p id="p-0050" num="0054">Here, in this description, naturalness is defined as an expression indicating an intermediate degree between commonness and eccentricity of the automatically generated content.</p><p id="p-0051" num="0055">The naturalness is used to express an intermediate degree (in likelihood) between commonness and eccentricity, and in other words, it can be said that the naturalness is an expression indicating a degree which is neither commonness nor eccentricity.</p><p id="p-0052" num="0056">That is, in the present disclosure, it can be said that the likelihood is adjusted to suit the preference of the target person of the automatically generated content, so that the naturalness of the content is adjusted to be the intermediate degree between commonness and eccentricity.</p><p id="p-0053" num="0057">However, when the naturalness is adjusted by adjusting the likelihood, reality decreases as the eccentricity of the generated content increases, that is, as the likelihood is adjusted to decrease.</p><p id="p-0054" num="0058">The reality as used herein is a likelihood expressing a possibility (probability) that the generated content is content generated by a human.</p><p id="p-0055" num="0059">For example, in a case where the content is music, a fact that the reality is reduced means that the generated music includes a discord which is not generated by a human, or a rhythm or a mode change that is difficult for a human to recognize as music.</p><p id="p-0056" num="0060">That is, as the reality decreases, the automatically generated content becomes closer to content which is not generated by a human, and the target person who views or listens to the content cannot recognize the content or feels uncomfortable in some cases.</p><p id="p-0057" num="0061">In this regard, in the present disclosure, when input content is changed stepwise while the naturalness which is intermediate between commonness and eccentricity is adjusted using likelihood exploration, the content is changed finally to the content desired by the user, thereby automatically generating the content.</p><p id="p-0058" num="0062">At this time, in the automatic generation of content, as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, when the reality is set to increase although the content is excessively eccentric, adjustment is performed such that the reality is maintained, and content (content which cannot be recognized as content by a human) which is not generated by a human is not generated.</p><p id="p-0059" num="0063">As a result, in the present disclosure, it is possible to automatically generate content while adjusting the naturalness which is intermediate between commonness and eccentricity by using likelihood exploration while keeping the reality satisfied.</p><heading id="h-0011" level="1">2. First Embodiment</heading><p id="p-0060" num="0064">Next, a configuration of an information processing apparatus <b>31</b> which is a hardware configuration example of an information processing apparatus of the present disclosure will be described with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0061" num="0065">Note that, in this description, a case where the information processing apparatus <b>31</b> automatically generates music as content will be described as an example. However, the same applies to a case where the information processing apparatus automatically generates various types of content, such as images and sentences, other than music.</p><p id="p-0062" num="0066">As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the information processing apparatus <b>31</b> includes a communication unit <b>51</b>, a control unit <b>52</b>, and a storage unit <b>53</b>. Note that the information processing apparatus <b>31</b> includes an input/output unit <b>32</b> which includes, for example, a keyboard, a mouse, or the like which receives various operations from an administrator or the like who manages the information processing apparatus <b>31</b> and, for example, a liquid crystal display or the like which presents various types of information.</p><p id="p-0063" num="0067">The communication unit <b>51</b> is implemented by, for example, a network interface card (NIC) or the like. The communication unit <b>51</b> is connected to a network including the Internet or the like in a wired or wireless manner, and transmits and receives information to and from another device or the like via the network.</p><p id="p-0064" num="0068">The control unit <b>52</b> includes a memory and a processor, and controls the entire operation of the information processing apparatus <b>31</b>.</p><p id="p-0065" num="0069">More specifically, the control unit <b>52</b> includes a learning unit <b>71</b>, an optimization unit <b>72</b>, and a generation unit <b>73</b>.</p><p id="p-0066" num="0070">The learning unit <b>71</b> causes an encoder <b>91</b> and a decoder <b>92</b> stored in a model storage unit <b>81</b> in the storage unit <b>53</b> (described later) to learn a variational auto encoder (VAE) by using music data stored as samples in a music DB <b>82</b> and to be configured as a learned model.</p><p id="p-0067" num="0071">The optimization unit <b>72</b> is controlled by the learning unit <b>71</b> to adjust and optimize the parameters of the encoder <b>91</b> and the decoder <b>92</b> such that a posterior distribution is regularized with a prior distribution (normal distribution) while the reconfiguration error is minimized when the encoder <b>91</b> and the decoder <b>92</b> repeat learning using the music data stored as samples in the music DB <b>82</b>.</p><p id="p-0068" num="0072">The generation unit <b>73</b> controls the encoder <b>91</b>, the decoder <b>92</b>, and a loss function calculation unit <b>93</b> stored in the model storage unit <b>81</b> in the storage unit <b>53</b> to adjust the naturalness of the content, which is intermediate between the commonness and the eccentricity of the input content (music), by the likelihood exploration and convert the content into the content (music) desired by the user, thereby generating (automatically generating) the content (music). Note that the automatic generation of the content by the generation unit <b>73</b> will be described later in detail with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0069" num="0073">The storage unit <b>53</b> is implemented by, for example, a semiconductor memory element such as a random access memory (RAM) and a flash memory, or a storage device such as a hard disk and an optical disk. The storage unit <b>53</b> includes the model storage unit <b>81</b> and the music database (DB) <b>82</b>.</p><p id="p-0070" num="0074">The model storage unit <b>81</b> stores a learned model learned in advance. Specifically, the model storage unit <b>81</b> includes the encoder <b>91</b> that extracts a latent variable which is a feature amount from the content, the decoder <b>92</b> that reconfigures the content on the basis of the latent variable, and the loss function calculation unit <b>93</b> that calculates a loss function which is a difference between the likelihood of the content as input data and the likelihood desired by the user.</p><p id="p-0071" num="0075">The music DB <b>82</b> stores data regarding content (music) as a sample input to the model. The music DB <b>82</b> also stores content which the generation unit <b>73</b> controls the encoder <b>91</b> and the decoder <b>92</b> to generate (automatically generate).</p><heading id="h-0012" level="1">Automatic Generation of Content by Generation Unit</heading><p id="p-0072" num="0076">Next, the automatic generation of content by the generation unit <b>73</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0073" num="0077">The generation unit <b>73</b> controls the encoder <b>91</b> and the decoder <b>92</b> learned in advance, and causes the encoder <b>91</b> to encode content (music) X(init) as input data to obtain a latent variable Zinit. Then, the generation unit <b>73</b> causes the decoder <b>92</b> to perform reconfiguration on the basis of the obtained latent variable Zinit, thereby generating content (music) X(init)&#x2032; as output data.</p><p id="p-0074" num="0078">More specifically, as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, for example, the encoder <b>91</b> encodes content (music) X(init) including a sequence such as partial data including a plurality of bars or the like, thereby converting the content into the latent variable Zinit as a feature amount including a vector or the like having a smaller number of dimensions than the content (music) X.</p><p id="p-0075" num="0079">Then, as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the decoder <b>92</b> decodes each piece of latent variable on the basis of the latent variable Zinit which is the feature amount of the content to return to the original dimension and restores the latent variable to be reconfigured as the content (music) X(init)&#x2032; including a sequence such as partial data including bars.</p><p id="p-0076" num="0080">Here, the encoder <b>91</b> and the decoder <b>92</b> are controlled by the learning unit <b>71</b> to be subject to unsupervised learning by the VAE in advance. The encoder <b>91</b> is configured to encode the content (music) Xinit which is input data to convert the content into the latent variable Zinit, and the decoder <b>92</b> is configured to be able to reconfigure the content as the content (music) X(init)&#x2032; on the basis of the latent variable Zinit. That is, since the encoder <b>91</b> and the decoder <b>92</b> are learned, the content X(init) and the content X(init)&#x2032; are substantially the same.</p><p id="p-0077" num="0081">When receiving the information of the likelihood indicating the degree of naturalness which is desired by the creator of the content (music) and intermediate between commonness and eccentricity, the generation unit <b>73</b> controls the loss function calculation unit <b>93</b> to calculate, as a loss function LLE, a difference between the likelihood of the content (music) X(init)&#x2032; reconfigured by the decoder <b>92</b> and the likelihood desired by the creator.</p><p id="p-0078" num="0082">Then, the generation unit <b>73</b> changes a latent variable Zi (i is the number of times of lowering the loss function LLE) to gradually lower the obtained loss function LLE by a predetermined value &#x394;, that is, to reduce the loss function LLE stepwise and causes the decoder <b>92</b> to perform decoding, thereby gradually generating the content (music) X(i)&#x2032; having the likelihood desired by the creator.</p><p id="p-0079" num="0083">For example, as indicated by following Formula (1), the loss function LLE is a function indicating a difference in likelihood between the content X(init)&#x2032; reconfigured from the latent variable Zinit obtained from the content X(init) which is the input data and the content of the desired likelihood, and includes a function F<b>1</b> configuring a term relating to the difference in likelihood of the content and a function F<b>2</b> configuring a term relating to the likelihood of the reality of the content.</p><p id="p-0080" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>LLE=F</i>1<i>&#x2212;&#x3b1;&#x3e;F</i>2&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0081" num="0084">Here, LLE is a loss function, F<b>1</b> is the function configuring a term relating to the likelihood of the content, F<b>2</b> is the function configuring a term relating to the likelihood of reality, and a is a predetermined coefficient and can be arbitrarily set.</p><p id="p-0082" num="0085">More specifically, the loss function calculation unit <b>93</b> includes a likelihood evaluator <b>101</b> and a reality evaluator <b>102</b>, calculates the function F<b>1</b> configuring a term relating to the likelihood of the content on the basis of the likelihood calculated by the likelihood evaluator <b>101</b>, and calculates the function F<b>2</b> configuring a term relating to the likelihood of the reality on the basis of the likelihood of the reality of the content calculated by the reality evaluator <b>102</b>.</p><p id="p-0083" num="0086">On the basis of a sequence generation model (language generation model), the likelihood evaluator <b>101</b> performs learning, for example, by RNN, Transformer, or the like as architecture to maximize a log likelihood, and obtains the likelihood of reconfigured content (music) X&#x2032; as the log likelihood.</p><p id="p-0084" num="0087">Here, the likelihood of the reconfigured content X&#x2032; is a probability that the reconfigured content X&#x2032; is music registered as a sample in the music DB <b>82</b>.</p><p id="p-0085" num="0088">More specifically, in a case where the reconfigured content (music) X&#x2032; is configured as sequence data X&#x2032;</p><p id="p-0086" num="0089">(X<b>1</b>&#x2032;, X<b>2</b>&#x2032;, . . . , Xn&#x2032;) such as partial data X<b>1</b>&#x2032;, X<b>2</b>&#x2032;, . . . , Xn&#x2032;, the likelihood of the content X&#x2032; is expressed as a product of respective probabilities of the partial data.</p><p id="p-0087" num="0090">For example, in a case where the likelihood (probability) of the music X&#x2032; is expressed by P(X&#x2032;), the likelihood (probability) P(X&#x2032;) of the music X&#x2032; is calculated as following Formula (2).</p><p id="p-0088" num="0091">Note that, in the case of the sequence data X&#x2032;</p><p id="p-0089" num="0092">(X<b>1</b>&#x2032;, X<b>2</b>&#x2032;, . . . , Xn&#x2032;), an initial value Start is input to generate the head partial data X<b>1</b>&#x2032;, the partial data X<b>1</b>&#x2032; is input to generate the adjacent partial data X<b>2</b>&#x2032;, the partial data X<b>2</b>&#x2032; is input to generate the adjacent partial data X<b>3</b>&#x2032;, . . . , the partial data X(n-<b>1</b>)&#x2032; is input to generate the adjacent partial data Xn&#x2032;.</p><p id="p-0090" num="0093">Therefore, the likelihood (probability) P(X&#x2032;) of the content X&#x2032; including this sequence data (X<b>1</b>&#x2032;, X<b>2</b>&#x2032;, . . . , Xn&#x2032;) is expressed as following Formula (2).</p><p id="p-0091" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>P</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <msup>       <mi>X</mi>       <mo>&#x2032;</mo>      </msup>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mrow>       <mi>P</mi>       <mo>(</mo>       <mrow>        <mrow>         <mi>X</mi>         <mo>&#x2062;</mo>         <msup>          <mn>1</mn>          <mo>&#x2032;</mo>         </msup>        </mrow>        <mo>&#x2758;</mo>        <mi>Start</mi>       </mrow>       <mtext> </mtext>       <mo>)</mo>      </mrow>      <mo>&#xd7;</mo>      <mrow>       <mi>P</mi>       <mo>(</mo>       <mrow>        <mrow>         <mrow>          <mi>X</mi>          <mo>&#x2062;</mo>          <msup>           <mn>2</mn>           <mo>&#x2032;</mo>          </msup>         </mrow>         <mo>&#x2758;</mo>         <mi>Start</mi>        </mrow>        <mo>,</mo>        <mrow>         <mi>X</mi>         <mo>&#x2062;</mo>         <msup>          <mn>1</mn>          <mo>&#x2032;</mo>         </msup>        </mrow>       </mrow>       <mtext> </mtext>       <mo>)</mo>      </mrow>      <mo>&#xd7;</mo>      <mrow>       <mi>P</mi>       <mo>(</mo>       <mrow>        <mrow>         <mrow>          <mi>X</mi>          <mo>&#x2062;</mo>          <msup>           <mn>3</mn>           <mo>&#x2032;</mo>          </msup>         </mrow>         <mo>&#x2758;</mo>         <mi>Start</mi>        </mrow>        <mo>,</mo>        <mrow>         <mi>X</mi>         <mo>&#x2062;</mo>         <msup>          <mn>1</mn>          <mo>&#x2032;</mo>         </msup>        </mrow>        <mtext> </mtext>        <mo>,</mo>        <mrow>         <mi>X</mi>         <mo>&#x2062;</mo>         <msup>          <mn>2</mn>          <mo>&#x2032;</mo>         </msup>        </mrow>       </mrow>       <mtext> </mtext>       <mo>)</mo>      </mrow>      <mo>&#xd7;</mo>      <mrow>       <mi>P</mi>       <mo>(</mo>       <mrow>        <mrow>         <mrow>          <mi>X</mi>          <mo>&#x2062;</mo>          <msup>           <mn>4</mn>           <mo>&#x2032;</mo>          </msup>         </mrow>         <mo>&#x2758;</mo>         <mi>Start</mi>        </mrow>        <mo>,</mo>        <mrow>         <mi>X</mi>         <mo>&#x2062;</mo>         <msup>          <mn>1</mn>          <mo>&#x2032;</mo>         </msup>        </mrow>        <mtext> </mtext>        <mo>,</mo>        <mrow>         <mi>X</mi>         <mo>&#x2062;</mo>         <msup>          <mn>2</mn>          <mo>&#x2032;</mo>         </msup>        </mrow>        <mo>,</mo>        <mrow>         <mi>X</mi>         <mo>&#x2062;</mo>         <msup>          <mn>3</mn>          <mo>&#x2032;</mo>         </msup>        </mrow>       </mrow>       <mtext>  </mtext>       <mo>)</mo>      </mrow>      <mo>&#xd7;</mo>      <mrow>       <mi>P</mi>       <mo>(</mo>       <mrow>        <mrow>         <mrow>          <mi>X</mi>          <mo>&#x2062;</mo>          <msup>           <mn>5</mn>           <mo>&#x2032;</mo>          </msup>         </mrow>         <mo>&#x2758;</mo>         <mi>Start</mi>        </mrow>        <mo>,</mo>        <mrow>         <mi>X</mi>         <mo>&#x2062;</mo>         <msup>          <mn>1</mn>          <mo>&#x2032;</mo>         </msup>        </mrow>        <mtext> </mtext>        <mo>,</mo>        <mrow>         <mi>X</mi>         <mo>&#x2062;</mo>         <msup>          <mn>2</mn>          <mo>&#x2032;</mo>         </msup>        </mrow>        <mo>,</mo>        <mrow>         <mi>X</mi>         <mo>&#x2062;</mo>         <msup>          <mn>3</mn>          <mo>&#x2032;</mo>         </msup>        </mrow>        <mtext> </mtext>        <mo>,</mo>        <mrow>         <mi>X</mi>         <mo>&#x2062;</mo>         <msup>          <mn>4</mn>          <mo>&#x2032;</mo>         </msup>        </mrow>       </mrow>       <mtext> </mtext>       <mo>)</mo>      </mrow>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mo>&#x2026;</mo>      <mo>&#xd7;</mo>      <mrow>       <mi>P</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mrow>         <mrow>          <mi>X</mi>          <mo>&#x2062;</mo>          <msup>           <mi>n</mi>           <mo>&#x2032;</mo>          </msup>         </mrow>         <mo>&#x2758;</mo>         <mi>Start</mi>        </mrow>        <mo>,</mo>        <mrow>         <mi>X</mi>         <mo>&#x2062;</mo>         <msup>          <mn>1</mn>          <mo>&#x2032;</mo>         </msup>        </mrow>        <mtext> </mtext>        <mo>,</mo>        <mrow>         <mi>X</mi>         <mo>&#x2062;</mo>         <msup>          <mn>2</mn>          <mo>&#x2032;</mo>         </msup>        </mrow>        <mo>,</mo>        <mo>&#x2026;</mo>        <mtext>    </mtext>        <mo>,</mo>        <mrow>         <mi>X</mi>         <mo>&#x2062;</mo>         <msup>          <mrow>           <mo>(</mo>           <mrow>            <mi>n</mi>            <mo>-</mo>            <mn>1</mn>           </mrow>           <mo>)</mo>          </mrow>          <mo>&#x2032;</mo>         </msup>        </mrow>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0092" num="0094">Here, P(X&#x2032;) is the likelihood (probability) of the content (music) X&#x2032;.</p><p id="p-0093" num="0095">Furthermore, P(Xn&#x2032;|Start, X<b>1</b>&#x2032;, X<b>2</b>&#x2032;, . . . , X(n-<b>1</b>)&#x2032;) is a conditional probability (likelihood) of the partial data Xn&#x2032; when the initial value is Start and the partial data are sequentially X<b>1</b>&#x2032;, X<b>2</b>&#x2032;, . . . , X(n-<b>1</b>)&#x2032;.</p><p id="p-0094" num="0096">The likelihood evaluator <b>101</b> logarithmizes the likelihood P(X&#x2032;) of the content (music) X&#x2032; obtained in this way, and outputs the result as a log likelihood EL(X&#x2032;).</p><p id="p-0095" num="0097">Furthermore, the reality evaluator <b>102</b> performs learning in advance, for example, by RNN, Transformer, or the like as architecture to maximize the log likelihood indicating reality on the basis of content including a sequence labeled with a real class including the content generated by a human as an input and content including a sequence labeled with a fake class which is not generated by a human.</p><p id="p-0096" num="0098">Then, the reality evaluator <b>102</b> obtains the likelihood of the reality of the content (music)&#xd7;(init)&#x2032; as a function of the term relating to the likelihood of the reality and logarithmically obtains the log likelihood of the reality as reality ER(X&#x2032;).</p><p id="p-0097" num="0099">Note that, hereinafter, the likelihood indicating the probability that the reconfigured content (music) X&#x2032; is music data registered as a sample in the music DB <b>82</b> is referred to as &#x201c;likelihood&#x201d;, and the likelihood of reality is simply referred to as &#x201c;reality&#x201d; for distinction although the likelihood of reality is still a likelihood as a concept, but there is no change in that both are likelihoods.</p><p id="p-0098" num="0100">Here, the content which is the sequence labeled with the real class is, for example, music data which is used for learning and is a sample registered in the music DB <b>82</b>.</p><p id="p-0099" num="0101">Furthermore, the content including the sequence labeled with the fake class is, for example, music data F reconfigured when the latent variable Z obtained from the prior distribution by the VAE relating to the learning of the encoder <b>91</b> and the decoder <b>92</b> is decoded by the decoder <b>92</b> as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0100" num="0102">That is, the reality evaluator <b>102</b> performs learning on the basis of a group of content &#x3b2; including the sequence labeled with the real class generated as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref> and a group of content F including the sequence labeled with the fake class, and obtains, as reality ER(X(init)&#x2032;), the log likelihood obtained by logarithmizing the likelihood which is the probability that the reconfigured content (music) X(init)&#x2032; is the content generated by a human.</p><p id="p-0101" num="0103">The loss function calculation unit <b>93</b> calculates a loss function LLEinit expressed by above-described Formula (1) from the likelihood (log likelihood) EL(X(init)&#x2032;) of the reconfigured content X&#x2032; calculated by the likelihood evaluator <b>101</b> and the reality (log likelihood) ER(X(init)&#x2032;) as the probability that the reconfigured content X&#x2032; calculated by the reality evaluator <b>102</b> is the content generated by a human.</p><p id="p-0102" num="0104">More specifically, the loss function calculation unit <b>93</b> calculates a term, which is the function F<b>1</b> of Formula (1), relating to the likelihood of the reconfigured content X&#x2032; as represented by following Formula (3).</p><p id="p-0103" num="0105">F1=(EL(X(init)&#x2032;) &#x2212;&#x3b2;&#xd7;ELinit)<sup>2</sup>&#x2014; (<b>3</b>)</p><p id="p-0104" num="0106">Here, F<b>1</b> is a function indicating a term relating to the likelihood of the reconfigured content X(init)&#x2032; in Formula (1), EL(X(init)&#x2032;) is the log likelihood of the reconfigured content X(init)&#x2032; obtained by the likelihood evaluator <b>101</b>, and &#x3b2;&#xd7;ELinit is a reference likelihood.</p><p id="p-0105" num="0107">The reference likelihood &#x3b2;&#xd7;ELinit is a value for setting the likelihood desired by the creator who tries to automatically generate the content, that is, a value which is the aim of the likelihood to be finally obtained, and is expressed as a product of a coefficient &#x3b2; and an initial value ELinit (predetermined fixed value) of the likelihood.</p><p id="p-0106" num="0108">For example, when it is desired to increase the likelihood of the content X(init) which is the input data and reconfigure (automatically generate) content having increased commonness, the reference likelihood is set to be larger than the initial value ELinit of the likelihood, and thus, the coefficient &#x3b2; is set to a value larger than 1. Furthermore, in a case where there is no specific desired likelihood and it is simply desired to increase the likelihood, the coefficient &#x3b2; may be set to a specific value larger than 1, for example, 1.2 or 1.5.</p><p id="p-0107" num="0109">On the other hand, when it is desired to lower the likelihood of the content to be generated automatically and to increase the eccentricity, the coefficient &#x3b2; is set to a value smaller than 1 in order to set the reference likelihood to be smaller than the initial value ELinit of the log likelihood EL. Furthermore, in a case where there is no specific desired likelihood and it is simply desired to reduce the likelihood, the coefficient &#x3b2; may be set to a specific value smaller than 1, for example, 0.8 or 0.5.</p><p id="p-0108" num="0110">Furthermore, the loss function calculation unit <b>93</b> substitutes the log likelihood ER(X(init)&#x2032;) of the reality of the reconfigured content X(init)&#x2032; into the function F<b>2</b> of above-described Formula (1) to perform calculation.</p><p id="p-0109" num="0111">From these, the loss function calculation unit <b>93</b> calculates the loss function LLE as expressed by following Formula (4).</p><p id="p-0110" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mtable>     <mtr>      <mtd>       <mrow>        <mi>LLE</mi>        <mo>=</mo>        <malignmark/>        <mrow>         <mrow>          <mi>F</mi>          <mo>&#x2062;</mo>          <mn>1</mn>         </mrow>         <mo>-</mo>         <mrow>          <mi>&#x3b1;</mi>          <mo>&#xd7;</mo>          <mi>F</mi>          <mo>&#x2062;</mo>          <mn>2</mn>         </mrow>        </mrow>       </mrow>      </mtd>     </mtr>     <mtr>      <mtd>       <mrow>        <mo>=</mo>        <malignmark/>        <mrow>         <msup>          <mrow>           <mo>(</mo>           <mrow>            <mrow>             <mi>EL</mi>             <mo>&#x2061;</mo>             <mo>(</mo>             <msup>              <mrow>               <mi>X</mi>               <mo>&#x2061;</mo>               <mo>(</mo>               <mi>init</mi>               <mo>)</mo>              </mrow>              <mo>&#x2032;</mo>             </msup>             <mo>)</mo>            </mrow>            <mo>-</mo>            <mrow>             <mi>&#x3b2;</mi>             <mo>&#xd7;</mo>             <mi>ELinit</mi>            </mrow>           </mrow>           <mo>)</mo>          </mrow>          <mn>2</mn>         </msup>         <mo>-</mo>         <mrow>          <mi>&#x3b1;</mi>          <mo>&#xd7;</mo>          <mrow>           <mi>ER</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <msup>            <mrow>             <mi>X</mi>             <mo>&#x2061;</mo>             <mo>(</mo>             <mi>init</mi>             <mo>)</mo>            </mrow>            <mo>&#x2032;</mo>           </msup>           <mo>)</mo>          </mrow>         </mrow>        </mrow>       </mrow>      </mtd>     </mtr>    </mtable>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>4</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0111" num="0112">Note that the loss function calculation unit <b>93</b> enables differentiation by using, for example, Gumbel Softmax so that the decoder <b>92</b> can receive evaluation signals by the likelihood evaluator <b>101</b> and the reality evaluator <b>102</b>.</p><heading id="h-0013" level="1">Change of Latent Variable Based on Loss Function</heading><p id="p-0112" num="0113">As described above, the loss function LLE includes the function F<b>1</b> depending on the difference between the likelihood (=the likelihood of the reconfigured content X(init)&#x2032;) of the content X(init) which is the input data and the likelihood desired by the creator, and the function F<b>2</b> depending on the reality.</p><p id="p-0113" num="0114">In this regard, the generation unit <b>73</b> reconfigures the content X(i) such that the loss function LLE becomes small, thereby generating the content having the likelihood desired by the creator.</p><p id="p-0114" num="0115">That is, since the loss function LLE has a configuration as expressed in Formula (4), the generation unit <b>73</b> increases the reality ER and causes the likelihood EL to approach the reference likelihood, thereby reconfiguring the content in which the loss function LEE becomes small.</p><p id="p-0115" num="0116">More specifically, on the basis of the loss function LLE described above, the generation unit <b>73</b> changes the latent variable Zi to stepwise reduce the loss function by the predetermined value &#x394;, and causes the decoder <b>92</b> to decode the changed latent variable Zi, thereby sequentially generating new content X&#x2032;(i).</p><p id="p-0116" num="0117">In this regard, next, a method of changing the latent variable Zi by reducing the loss function will be described.</p><p id="p-0117" num="0118">For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, it is considered that the latent variable Z obtained on the basis of the content which is various inputs is defined as a latent variable space expressed in a two-dimensional space, and the likelihood of the content at the time of reconfiguration using each latent variable Z is expressed by a contour line in the latent variable space.</p><p id="p-0118" num="0119">Note that, in the latent variable space of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, an example in which the latent variable Z is set to two dimensions for the sake of explanation will be described, but in reality, the latent variable Z is configured with more dimensions, and the latent variable space is similarly expressed with more dimensions.</p><p id="p-0119" num="0120">That is, in the latent variable space of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the distribution of the latent variable Z expressed two-dimensionally is indicated by a cross mark, and the likelihood when the content is reconfigured using each latent variable Z is indicated by solid lines L<b>1</b> to L<b>5</b> concentrically, and is set to have a distribution in which the likelihood decreases toward the outside of a paper surface. That is, the likelihoods indicated by the solid lines L<b>1</b> to L<b>5</b> are assumed to satisfy L<b>1</b>&#x3e;L<b>2</b>&#x3e;L<b>3</b>&#x3e;L<b>4</b>&#x3e;L<b>5</b> in the drawing. Note that the distribution of the likelihood in the latent variable space of <figref idref="DRAWINGS">FIG. <b>5</b></figref> is an example.</p><p id="p-0120" num="0121">Furthermore, in the latent variable space of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, when the reality when the content is reconfigured using the similar latent variable Z is obtained, the distribution of a predetermined threshold of the reality is indicated by a dotted line R<b>1</b>.</p><p id="p-0121" num="0122">Here, an upper portion from the dotted line R<b>1</b> in the drawing is a non-real region in which the reality is lower than the predetermined threshold, and the reconfigured content is regarded as &#x201c;non-real&#x201d; which is hardly recognized as the content generated by a human.</p><p id="p-0122" num="0123">Moreover, in the latent variable space of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, it is assumed that a lower portion from the dotted line R<b>1</b> is a real region which is a region where the reality is higher than the predetermined threshold, and the reconfigured content is regarded as &#x201c;real&#x201d; which is recognized as the content generated by a human.</p><p id="p-0123" num="0124">Note that the reality in the latent variable space of <figref idref="DRAWINGS">FIG. <b>5</b></figref> is also to be displayed with a plurality of contour lines similarly to the likelihood, but here, only the dotted line R<b>1</b>, which is the distribution of the predetermined threshold which is a boundary between the real region and the non-real region, is displayed.</p><p id="p-0124" num="0125">Here, for example, a case will be considered in which the latent variable generated when content Xa which is the input data is encoded by the encoder <b>91</b> is represented by a position Za in the latent variable space of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, and the creator desires to lower the likelihood from the current level of the solid line L<b>2</b> to the level of the solid line L<b>4</b>.</p><p id="p-0125" num="0126">When it is not necessary to be aware of reality, the generation unit <b>73</b> moves the position in the latent variable space toward the solid line L<b>4</b> in a vector VL direction perpendicular to the solid line L<b>2</b> which is the contour line representing the level of likelihood in the latent variable space of <figref idref="DRAWINGS">FIG. <b>5</b></figref> to obtain the latent variable, and causes the decoder <b>92</b> to decode the latent variable, whereby it is possible to reconfigure the content of the likelihood desired by the creator.</p><p id="p-0126" num="0127">That is, in the latent variable space, it can be said that latent variables existing at close positions are similar latent variables, and thus the latent variable obtained at a position Zx which is closest on the solid line L<b>4</b> as viewed from the position Za at which the likelihood level is on the solid line L<b>2</b> is considered to be the latent variable, which is most similar to the latent variable at the current position Za, of the likelihood represented by the solid line L<b>4</b>.</p><p id="p-0127" num="0128">However, in the case of considering reality, in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the region where the latent variable Za exists is a non-real region, and thus there is a possibility that the content reconfigured by decoding the latent variable, which is obtained by moving the position in the latent variable space in consideration of only the likelihood, by the decoder <b>92</b> has low reality although the likelihood is satisfied, and the content cannot be recognized as the content generated by a human when the target person views or listens to the content.</p><p id="p-0128" num="0129">In this regard, the generation unit <b>73</b> sets a vector VR of reality in a direction in which the position Za in the latent variable space is closest to the dotted line R<b>1</b>, moves the position Za to a position Zb, which is obtained by synthesizing the vector VL and the vector VR and obtained by lowering the gradient of the likelihood by the predetermined value &#x394;, to obtain the latent variable, and causes the decoder <b>92</b> to decode the latent variable, thereby reconfiguring new content.</p><p id="p-0129" num="0130">The generation unit <b>73</b> sequentially outputs latent variables obtained by repeating the similar operation thereafter, for example, by sequentially changing the position Za to the positions Zb, Zc, Zd, Ze, and Zf in the latent variable space to the decoder <b>92</b>, and causes the decoder to decode the latent variables to generate new content.</p><p id="p-0130" num="0131">That is, content Xb generated by decoding the latent variable at the position Zb in the latent variable space in <figref idref="DRAWINGS">FIG. <b>7</b></figref> by the decoder <b>92</b> has a reduced likelihood compared to the content Xa and is improved in reality to approach the dotted line R<b>1</b> which is the boundary with the real region.</p><p id="p-0131" num="0132">Furthermore, content Xc generated by decoding the latent variable of the position Zc by the decoder <b>92</b> has a further reduced likelihood compared to the content Xb and is further improved in reality to further approach the dotted line R<b>1</b>.</p><p id="p-0132" num="0133">Moreover, the content Xd generated by decoding the latent variable at the position Zd by the decoder <b>92</b> has a further reduced likelihood compared to the content Xc and is further improved in reality, so that the content Xd enters the real region cross the dotted line R<b>1</b>, and becomes a state sufficient in reality.</p><p id="p-0133" num="0134">Furthermore, content Xe generated by decoding the latent variable at the position Ze by the decoder <b>92</b> has a reduced likelihood compared to the content Xd, and since the content Xd is already sufficient in reality, the content Xe is moved in a direction close to the vertical direction with respect to the solid line L<b>3</b> which is a contour line indicating the likelihood.</p><p id="p-0134" num="0135">Moreover, the content Xf generated by decoding the latent variable at the position Zf by the decoder <b>92</b> has a reduced likelihood of compared to the content Xe, and since the content Xd is already sufficient in reality, the content Xf is moved in a substantially vertical direction with respect to the solid line L<b>3</b> which is the contour line indicating the likelihood.</p><p id="p-0135" num="0136">As described above, in the latent variable space as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a processing of changing the latent variable to reduce the likelihood in the loss function LLE stepwise while improving the reality and causing decoding is repeated, whereby the content as the input data can approach stepwise to the likelihood desired by the creator while the reality is improved.</p><p id="p-0136" num="0137">Note that, in the above description, an example has been described in which the naturalness of the content is made eccentric by stepwise reducing the likelihood of the content, which is the input data, to a desired likelihood. However, even in a case where the likelihood is made common, similar processing is performed only by setting the reference likelihood high.</p><p id="p-0137" num="0138">Furthermore, in the above description, a process of causing the likelihood to stepwise approach to the likelihood desired by the creator has been described, but instead of the stepwise process, the latent variable may be changed to reach the desired likelihood at one time.</p><heading id="h-0014" level="1">Content Generation Process in First Embodiment</heading><p id="p-0138" num="0139">Next, a content generation process in the first embodiment will be described with reference to a flowchart in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0139" num="0140">In step S<b>11</b>, the generation unit <b>73</b> initializes a counter i to <b>1</b>.</p><p id="p-0140" num="0141">In step S<b>12</b>, the generation unit <b>73</b> sets the reference likelihood. More specifically, the generation unit <b>73</b> sets the reference likelihood by, for example, receiving the value of the coefficient &#x3b2; in above-described Formula (4) and setting a specific value of the reference likelihood, or receiving the input of information indicating that the likelihood is desired to be increased or decreased and setting the coefficient &#x3b2; to a predetermined value.</p><p id="p-0141" num="0142">In step S<b>13</b>, the generation unit <b>73</b> receives the input of the content X(init) which is input data.</p><p id="p-0142" num="0143">In step S<b>14</b>, the generation unit <b>73</b> controls the encoder <b>91</b> to encode the partial data Xint and convert the partial data Xint into the latent variable Zinit.</p><p id="p-0143" num="0144">In step S<b>15</b>, the generation unit <b>73</b> controls the decoder <b>92</b> to decode the latent variable Zinit and reconfigure the content X(init)&#x2032;.</p><p id="p-0144" num="0145">In step S<b>16</b>, the generation unit <b>73</b> controls the loss function calculation unit <b>93</b> to calculate the loss function LLEinit based on the difference between the likelihood of the content X(init)&#x2032; and the likelihood desired by the creator by using above-described Formula (4).</p><p id="p-0145" num="0146">In step S<b>17</b>, the generation unit <b>73</b> obtains a loss function LLEi obtained by lowering the loss function LLEinit by the predetermined value &#x394;.</p><p id="p-0146" num="0147">In step S<b>18</b>, as described with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the generation unit <b>73</b> moves the position of the latent variable Zinit in the latent variable space while reducing the likelihood with the reality maintained, such that the loss function LLEinit is lowered by the predetermined value &#x394; to be changed to the loss function LLEi, and obtains and updates the latent variable Zi corresponding to the position in the new latent variable space.</p><p id="p-0147" num="0148">In step S<b>19</b>, the generation unit <b>73</b> controls the decoder <b>92</b> to decode the latent variable Zi and reconfigure the content X(i)&#x2032;.</p><p id="p-0148" num="0149">In step S<b>20</b>, the generation unit <b>73</b> stores the reconfigured content X(i)&#x2032; in the music DB <b>82</b> of the storage unit <b>53</b>.</p><p id="p-0149" num="0150">In step S<b>21</b>, the generation unit <b>73</b> increments the counter i by <b>1</b>.</p><p id="p-0150" num="0151">In step S<b>22</b>, the generation unit <b>73</b> determines whether or not the counter i is a maximum value imax, and in a case where the counter i is not the maximum value imax, the processing proceeds to step S<b>23</b>.</p><p id="p-0151" num="0152">In step S<b>23</b>, the generation unit <b>73</b> updates the loss function LLEi by lowering the loss function LLEi by the predetermined value &#x394;(LLEi=LLEi&#x2212;&#x394;), and the processing returns to step S<b>18</b>.</p><p id="p-0152" num="0153">At this time, in step S<b>18</b>, as described with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the generation unit <b>73</b> moves and updates the position of the latent variable Zi in the latent variable space to correspond to the change in the loss function LLEi which is lowered by the predetermined value &#x394; to be updated, and obtains and updates a new latent variable Zi.</p><p id="p-0153" num="0154">That is, the processes of steps S<b>18</b> to S<b>23</b> are repeated until the counter i reaches the maximum value imax, so that the latent variable Zi is updated while the loss function LLEi is sequentially lowered by the predetermined value &#x394;, and the updated latent variable Zi is sequentially decoded to generate new content Xi. As a result, the newly reconfigured content Xi is sequentially changed to gradually approach the likelihood desired by the creator while satisfying reality.</p><p id="p-0154" num="0155">Then, in a case where it is determined in step S<b>22</b> that the counter i reaches the maximum value imax, the processing proceeds to step S<b>24</b>.</p><p id="p-0155" num="0156">In step S<b>24</b>, the generation unit <b>73</b> outputs the content X(i) (i=1, 2, 3, . . . , imax) which is output data stored in the storage unit <b>53</b>.</p><p id="p-0156" num="0157">With the above processing, it is possible to generate the content X(i)&#x2032; by changing the content X(init), which is input data, stepwise to have the likelihood intended by the creator while satisfying reality.</p><p id="p-0157" num="0158">As a result, it is possible to automatically generate content while adjusting the naturalness which is intermediate between commonness and eccentricity of the content as intended by the creator.</p><p id="p-0158" num="0159">Note that, in the processing of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, in the processing until the loss function LLEinit is obtained which is the difference between the likelihood in the content X(init) which is the input data and the likelihood desired by the creator, the calculation is performed after the latent variable Zinit is once decoded by the decoder <b>92</b> to obtain the content X(init)&#x2032;. However, in the initial processing, no change is added to the latent variable Zinit, and thus the content Xinit and the content Xinit&#x2032; are substantially the same.</p><p id="p-0159" num="0160">Therefore, the initial loss function LLEinit may be directly obtained from the content Xinit which is the input data.</p><p id="p-0160" num="0161">Furthermore, in the above description, an example is described in which the process of sequentially changing the latent variable Zi while stepwise lowering the loss function LLEinit by the predetermined value &#x394; and repeatedly reconfiguring the content X(i)&#x2032; is repeated until the counter i reaches imax. However, the process may be ended when the loss function LLEi cannot be further reduced.</p><p id="p-0161" num="0162">Moreover, in the above description, an example is described in which all the content X(i)&#x2032; (i=1, 2,.. imax) repeatedly reconfigured by stepwise lowering the loss function LLEinit is finally output, but only the content X(imax)&#x2032; obtained last may be output.</p><p id="p-0162" num="0163">Furthermore, in the function F<b>1</b> including the term relating to the likelihood configuring the loss function LLE, it is only required to set the likelihood large in a case where it is only required to increase the likelihood and simply make the content common, and it is only required to set the likelihood small in a case where it is only required to reduce the likelihood and simply make the content eccentric.</p><p id="p-0163" num="0164">Therefore, in a case where there is no final aim or the like of the likelihood, the setting may be made as in following Formula (5).</p><p id="p-0164" num="0165">F1=EL(X(init))&#x2032; (when it is desired to reduce the likelihood and make the content eccentric)=-EL(X(init))&#x2032; (when it is desired to increase the likelihood and make the content common)&#x2014; (<b>5</b>)</p><p id="p-0165" num="0166">In other words, the function F<b>1</b> may be selectively used depending on whether it is desired to be eccentric or it is desired to be common as in Formula (5). Furthermore, Formula (5) may be used by multiplying the likelihood EL(X(init)) of the content X(init) by a positive coefficient when it is desired to reduce the likelihood and make the coefficient eccentric, and may be used by multiplying the likelihood EL(X(init)) by a negative coefficient when it is desired to increase the likelihood and make the content common.</p><heading id="h-0015" level="1">3. Second Embodiment</heading><p id="p-0166" num="0167">In the above description, an example is described in which the content X(init) which is input data is encoded to be converted into the latent variable Zinit, then the latent variable Zinit is decoded to obtain the content X(init)&#x2032;, the loss function LLEinit which is the difference between the likelihood of the content X(init)&#x2032; and the desired likelihood is obtained, the latent variable Zi is stepwise updated while the loss function LLEi is updated by lowering by the predetermined value &#x394;, and the updated latent variable Zi is decoded, thereby repeatedly reconfiguring the content X(i)&#x2032;.</p><p id="p-0167" num="0168">However, in the case of the above-described processing, there is a possibility that the content X(i)&#x2032; to be reconfigured is completely different from the content X(init) which is input data every time the loss function LLEi is updated.</p><p id="p-0168" num="0169">For this reason, even when there is a favored part in the content X(init) in which the naturalness which is intermediate between commonness and eccentricity is not changed, the entire content including the favored part is changed by changing the commonness and the eccentricity, and there is a possibility that it is difficult for the creator to like the entire content.</p><p id="p-0169" num="0170">In this regard, in the content Xinit which is input data, a part which the creator likes may be set not to be changed as context, and the naturalness which is intermediate between commonness and eccentricity may be adjusted by the likelihood exploration only for the other parts.</p><p id="p-0170" num="0171">Note that, hereinafter, the likelihood exploration performed in a state where the part which is not changed as the context is set is also referred to as contextual likelihood exploration.</p><p id="p-0171" num="0172">For example, in the content X(init) which is input data as illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the generation unit <b>73</b> receives the input regarding the information of a part which is desired not to be changed as intended by creator, and sets the part which is desired not to be changed as the context.</p><p id="p-0172" num="0173">In the example of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, in the content X(init), parts which the creator likes and which are desired not to be changed are set as contexts C<b>1</b> and C<b>2</b>, and a part which is desired to be changed other than the contexts C<b>1</b> and C<b>2</b> is set as partial data Y(init).</p><p id="p-0173" num="0174">Note that, although <figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example in which the contexts C<b>1</b> and C<b>2</b> are set before and after the partial data Y(init) which is desired to be changed, the position at which the contexts are set may be other than this or may be two or more positions.</p><p id="p-0174" num="0175">Then, the generation unit <b>73</b> controls the encoder <b>91</b> to encode only the partial data Y(init) which is the part which is desired to be changed and convert the partial data into the latent variable Zinit.</p><p id="p-0175" num="0176">The generation unit <b>73</b> controls the decoder <b>92</b> to reconfigure the partial data Y(init)&#x2032; on the basis of the latent variable Zinit.</p><p id="p-0176" num="0177">The generation unit <b>73</b> integrates the reconfigured partial data Y(init) and the contexts C<b>1</b> and C<b>2</b> to reconfigure the content X(init)&#x2032;.</p><p id="p-0177" num="0178">Thereafter, similarly to a case where there is no context, the generation unit <b>73</b> repeats a process of calculating a loss function LCLEinit which is the difference between the likelihood of the content X(init)&#x2032; and the likelihood desired by the creator, lowering by the predetermined value &#x394; to update a loss function LCLEi and update the corresponding latent variable Zi, controlling the decoder <b>92</b> to decode the latent variable Zi, reconfiguring partial data Y(i)&#x2032;, and further integrating the partial data Y(i)&#x2032; with the contexts C<b>1</b> and C<b>2</b> to reconfigure the context X(i)&#x2032;.</p><heading id="h-0016" level="1">Reality Evaluator in Case where Context is Used</heading><p id="p-0178" num="0179">In the case of using the context, as illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the reality evaluator <b>102</b> learns only partial data V other than the contexts C<b>11</b> and C<b>12</b> in the content R generated by a human by using the content F reconfigured by adding the contexts C<b>11</b> and C<b>12</b> to partial data V&#x2032; reconfigured when the decoder <b>92</b> decodes a latent variable Z&#x2032; in which a noise is added to the latent variable Z obtained from the prior distribution by the VAE relating to the learning of the encoder <b>91</b> and the decoder <b>92</b>.</p><p id="p-0179" num="0180">That is, in the case of using the context, the reality evaluator <b>102</b> sets the content R including the contexts C<b>11</b> and C<b>12</b> and the partial data V illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref> as the content including the sequence labeled with the real class, learns the content F reconfigured by adding the contexts C<b>11</b> and C<b>12</b> to the reconfigured partial data V&#x2032; as the content including the sequence labeled with the fake class, and obtains, as the reality ER(X&#x2032;), the log likelihood obtained by logarithmizing the likelihood which is the probability that the reconfigured content (music) X&#x2032; is the content generated by a human.</p><heading id="h-0017" level="1">Content Generation Processing in Second Embodiment</heading><p id="p-0180" num="0181">Next, a content generation process in the second embodiment will be described with reference to a flowchart in <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0181" num="0182">In step S<b>51</b>, the generation unit <b>73</b> initializes the counter i to <b>1</b>.</p><p id="p-0182" num="0183">In step S<b>52</b>, the generation unit <b>73</b> receives the setting of the reference likelihood.</p><p id="p-0183" num="0184">In step S<b>53</b>, the generation unit <b>73</b> receives the input of the content X(init) which is input data.</p><p id="p-0184" num="0185">In step S<b>54</b>, the generation unit <b>73</b> receives information of a part to be the context which is a part which the creator does not desire to change in the content X(init).</p><p id="p-0185" num="0186">In step S<b>55</b>, the generation unit <b>73</b> generates the partial data Y(init) obtained by removing the part to be the context from the content X(init).</p><p id="p-0186" num="0187">In step S<b>56</b>, the generation unit <b>73</b> controls the encoder <b>91</b> to encode the partial data Y(init) and convert the partial data Y(init) into the latent variable Zinit.</p><p id="p-0187" num="0188">In step S<b>57</b>, the generation unit <b>73</b> controls the decoder <b>92</b> to decode the latent variable Zinit and reconfigure the partial data Y(init)&#x2032;.</p><p id="p-0188" num="0189">In step S<b>58</b>, the generation unit <b>73</b> integrates the partial data Y(init)&#x2032; and the context to reconfigure the context X(init)&#x2032;.</p><p id="p-0189" num="0190">In step S<b>59</b>, the generation unit <b>73</b> controls the loss function calculation unit <b>93</b> to calculate the loss function LCLEinit of the content X(init)&#x2032; by using above-described Formula (4). Note that, here, the loss function LCLEinit in a case where the context is set and the loss function LLEinit in a case where the context is not set are denoted by different reference numerals, but the configurations of the formulas are the same as Formula (4).</p><p id="p-0190" num="0191">In step S<b>60</b>, the generation unit <b>73</b> obtains the loss function LCLEi obtained by lowering the loss function LCLEinit by the predetermined value &#x394;.</p><p id="p-0191" num="0192">Furthermore, the loss function LCLEi is basically similar to the loss function LLEi.</p><p id="p-0192" num="0193">In step S<b>61</b>, when the loss function LCLEinit is lowered by the predetermined value &#x394; to be changed into the loss function LCLEi, the generation unit <b>73</b> moves the position of the corresponding latent variable Zinit in the latent variable space to obtain and update the latent variable Zi.</p><p id="p-0193" num="0194">In step S<b>62</b>, the generation unit <b>73</b> controls the decoder <b>92</b> to decode the latent variable Zi and reconfigure the partial data Y(i)&#x2032;.</p><p id="p-0194" num="0195">In step S<b>63</b>, the generation unit <b>73</b> integrates the partial data Y(i)&#x2032; and the context to reconfigure the content X(i)&#x2032;.</p><p id="p-0195" num="0196">In step S<b>64</b>, the generation unit <b>73</b> stores the reconfigured content X(i)&#x2032; in the music DB <b>82</b> of the storage unit <b>53</b>.</p><p id="p-0196" num="0197">In step S<b>65</b>, the generation unit <b>73</b> increments the counter i by <b>1</b>.</p><p id="p-0197" num="0198">In step S<b>66</b>, the generation unit <b>73</b> determines whether or not the counter i is the maximum value imax, and in a case where the counter i is not the maximum value imax, the processing proceeds to step S<b>67</b>.</p><p id="p-0198" num="0199">In step S<b>67</b>, the generation unit <b>73</b> lowers the loss function LLEi by the predetermined value &#x394; and updates the loss function LLEi (LLEi=LLEi&#x2212;&#x394;), and the processing returns to step S<b>61</b>.</p><p id="p-0199" num="0200">That is, the processes of steps S<b>61</b> to S<b>67</b> are repeated until the counter i reaches the maximum value imax, so that the latent variable Zi is changed and decoded correspondingly while the loss function LLEi is sequentially lowered by the predetermined value &#x394;, new partial data Y(i)&#x2032; is generated, and the partial data</p><p id="p-0200" num="0201">Y(i)&#x2032; is integrated with the context to repeatedly reconfigure the content X(i)&#x2032;. As a result, the newly reconfigured content Xi is sequentially changed to gradually approach the likelihood desired by the creator while satisfying reality.</p><p id="p-0201" num="0202">At this time, since no change is added to the part set as the context, the newly reconfigured content Xi sequentially changes to gradually approach the likelihood desired by the creator while retaining the part which the creator likes and keeping the reality satisfied.</p><p id="p-0202" num="0203">Then, in a case where it is determined in step S<b>66</b> that the counter i reaches the maximum value imax, the processing proceeds to step S<b>68</b>.</p><p id="p-0203" num="0204">In step S<b>68</b>, the generation unit <b>73</b> outputs the contents X(i)&#x2032; (i=1, 2, 3, . . . , imax) which is output data stored in the music DB <b>82</b> of the storage unit <b>53</b>.</p><p id="p-0204" num="0205">With the above processing, it is possible to generate the content X(i)&#x2032; stepwise by the contextual likelihood exploration of changing the content X(i) which is input data to the likelihood intended by the creator while retaining the part set as the context and satisfying reality.</p><p id="p-0205" num="0206">As a result, as intended by the creator, it is possible to automatically generate the content while retaining the favored part of the content and adjusting the naturalness which is intermediate between commonness and eccentricity only for the other part.</p><heading id="h-0018" level="1">4. First Modification</heading><p id="p-0206" num="0207">In the above description, an example is described in which in the likelihood and the reality, one element is obtained for the entire content, and the loss function is calculated. However, since the content is a sequence including a plurality of elements, the likelihood can also be decomposed for each element, and a sequence including the likelihood for each element can be configured.</p><p id="p-0207" num="0208">A sequence including the likelihood of each element configuring the content is referred to as a likelihood sequence (information flow).</p><p id="p-0208" num="0209">In other words, for example, in a case where the content Xinit is music, as illustrated in the left part of <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the content Xinit includes elements X<b>1</b>, X<b>2</b>, . . . , Xn which are sequences in a time direction.</p><p id="p-0209" num="0210">Therefore, a likelihood EL(Xi) can be obtained for each element X<b>1</b>, X<b>2</b>, . . . , Xn, and this becomes the likelihood sequence (information flow).</p><p id="p-0210" num="0211">In a case where the likelihood sequence (information flow) is used, the reference likelihood is set for each element X<b>1</b>, X<b>2</b>, . . . , Xn, the sum of squares of the difference between the likelihood EL(Xi) of each element and the reference likelihood is used as the function F<b>1</b> in Formula (1) configuring the loss function described above, and, for example, following Formula (6) is obtained.</p><p id="p-0211" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>F</i>1=&#x3a3;(<i>EL</i>(<i>X</i>(<i>i</i>)&#x2032;)&#x2212;&#x3b2;<i>i</i>&#xd7;ELinit)<sup>2</sup>(<i>i=</i>1,2, . . . ,<i>n</i>)&#x2003;&#x2003;(6)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0212" num="0212">Here, i (i=1, 2, . . . , n) is an identifier of each element, EL(X(i)&#x2032;) is the likelihood of the element Xi, &#x3b2;i is the coefficient of each element, and ELinit is the initial value of the likelihood.</p><p id="p-0213" num="0213">Therefore, in Formula (6), the function F<b>1</b> is expressed as the sum of squares of the difference between the likelihood EL(Xi) of each element and a reference likelihood &#x3b2;i&#xd7;ELinit.</p><p id="p-0214" num="0214">As illustrated in the right part of <figref idref="DRAWINGS">FIG. <b>10</b></figref>, it can be said that the likelihood sequence for each element is, in other words, a change in the likelihood in the time direction, that is, a change in surprise level in the time direction.</p><p id="p-0215" num="0215">In <figref idref="DRAWINGS">FIG. <b>10</b></figref>, for example, the likelihood is a peak or a valley at the timing indicated by a dotted circle, and the surprising change is illustrated.</p><p id="p-0216" num="0216">In general, in the surprise level, it is known that a small surprise occurs after a large surprise, and a large surprise occurs after a small surprise.</p><p id="p-0217" num="0217">Therefore, a surprising change in the music can be reflected by using the likelihood sequence as illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0218" num="0218">Furthermore, here, for the elements X<b>1</b>, X<b>2</b>, . . . , Xn configuring the content Xinit, an element having a minimum configuration in the time direction is assumed, but a cluster including a plurality of elements may be formed, and the likelihood sequence may be set in units of clusters. Moreover, for the function F<b>1</b> in Formula (1) configuring the loss function, a correlation with the reference likelihood may be used instead of the square error described with reference to Formula (6), or a statistic such as a variance for the elements of the likelihood sequence (information flow) may be used without using the reference likelihood.</p><p id="p-0219" num="0219">Note that the content generation process is similar to the process of a case where the context described with reference to the flowchart of <figref idref="DRAWINGS">FIG. <b>9</b></figref> is set except that the method of calculating the loss function LLE is different, and thus the description thereof will be omitted.</p><heading id="h-0019" level="1">5. Second Modification</heading><p id="p-0220" num="0220">In the above description, in a case where the context is set, the likelihood for the entire reconfigured content X&#x2032; is used for the likelihood obtained by the likelihood evaluator <b>101</b>. However, as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a likelihood evaluator <b>101</b>&#x2032; may be provided instead of the likelihood evaluator <b>101</b>, and the likelihood evaluator <b>101</b>&#x2032; may obtain, as a conditional likelihood CEL, likelihood from only partial data Y&#x2032; which is other than the contexts C<b>1</b> and C<b>2</b> in the content X&#x2032; and is changed to the likelihood desired by the creator.</p><p id="p-0221" num="0221">Furthermore, in a case where the context is similarly set, as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, instead of the reality evaluator <b>102</b>, a reality evaluator <b>102</b>&#x2032; which is generated by the same sequence generation model as that of the likelihood evaluator <b>101</b> and is substantially the same as the likelihood evaluator <b>101</b> may be provided, and the likelihood EL itself may be used as the reality ER.</p><p id="p-0222" num="0222">As a result, the loss function is expressed as following Formula (7).</p><p id="p-0223" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mtable>     <mtr>      <mtd>       <mrow>        <mi>LLE</mi>        <mo>=</mo>        <malignmark/>        <mrow>         <mrow>          <mi>F</mi>          <mo>&#x2062;</mo>          <mn>1</mn>         </mrow>         <mo>-</mo>         <mrow>          <mi>&#x3b1;</mi>          <mo>&#xd7;</mo>          <mi>F</mi>          <mo>&#x2062;</mo>          <mn>2</mn>         </mrow>        </mrow>       </mrow>      </mtd>     </mtr>     <mtr>      <mtd>       <mrow>        <mo>=</mo>        <malignmark/>        <mrow>         <msup>          <mrow>           <mo>(</mo>           <mrow>            <mrow>             <mi>CEL</mi>             <mo>&#x2061;</mo>             <mo>(</mo>             <msup>              <mrow>               <mi>X</mi>               <mo>&#x2061;</mo>               <mo>(</mo>               <mi>init</mi>               <mo>)</mo>              </mrow>              <mo>&#x2032;</mo>             </msup>             <mo>)</mo>            </mrow>            <mo>-</mo>            <mrow>             <mi>&#x3b2;</mi>             <mo>&#xd7;</mo>             <mi>ELinit</mi>            </mrow>           </mrow>           <mo>)</mo>          </mrow>          <mn>2</mn>         </msup>         <mo>-</mo>         <mrow>          <mi>&#x3b1;</mi>          <mo>&#xd7;</mo>          <mrow>           <mi>EL</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <msup>            <mrow>             <mi>X</mi>             <mo>&#x2061;</mo>             <mo>(</mo>             <mi>init</mi>             <mo>)</mo>            </mrow>            <mo>&#x2032;</mo>           </msup>           <mo>)</mo>          </mrow>         </mrow>        </mrow>       </mrow>      </mtd>     </mtr>    </mtable>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>7</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0224" num="0223">Here, CEL(X(init)&#x2032;) is the conditional likelihood of the content X(init)&#x2032;,&#x3b2;&#xd7;ELinit is a reference likelihood, and EL(X(init)&#x2032;) is the likelihood of the content X(init)&#x2032;.</p><p id="p-0225" num="0224">Since the conditional likelihood is the likelihood of the partial data of the content X(init)&#x2032; which changes according to the likelihood and thus represents &#x201c;surprise&#x201d;, the reference likelihood is set with respect to the conditional likelihood, so that the content generated to make &#x201c;surprise&#x201d; larger or make &#x201c;surprise&#x201d; smaller can be adjusted.</p><p id="p-0226" num="0225">Furthermore, since the reality evaluator <b>102</b>&#x2032; substantially functions as the likelihood evaluator <b>101</b>, and thus the same sequence generation model is used for both the likelihood evaluator <b>101</b>&#x2032; and the reality evaluator <b>102</b>&#x2032;, both can be aggregated into only the configuration of any one to simplify the configuration.</p><p id="p-0227" num="0226">Moreover, in the second modification, an example is described in which the conditional likelihood CEL(X(init)&#x2032;) is used for the function F<b>1</b>, and the likelihood EL(X(init)&#x2032;) is used for the function F<b>2</b> as the reality ER(X(init)&#x2032;). However, the reality ER(X(init)&#x2032;) may be used for the function F<b>2</b> by using the conditional likelihood CEL(X(init)&#x2032;) for the function F<b>1</b>.</p><p id="p-0228" num="0227">Furthermore, similarly, the likelihood EL(X(init)&#x2032;) may be used instead of the reality ER(X(init)&#x2032;) of the function F<b>2</b> in Formulas (3), (5), and (6) described above.</p><p id="p-0229" num="0228">Note that the content generation process is similar to the process of a case where the context described with reference to the flowchart of <figref idref="DRAWINGS">FIG. <b>9</b></figref> is set except that the method of calculating the loss function LLE is different, and thus the description thereof will be omitted.</p><heading id="h-0020" level="1">6. Example of Execution by Software</heading><p id="p-0230" num="0229"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates a configuration example of a general-purpose computer. This personal computer has a built-in central processing unit (CPU) <b>1001</b>. An input/output interface <b>1005</b> is connected to the CPU <b>1001</b> via a bus <b>1004</b>. A read only memory (ROM) <b>1002</b> and a random access memory (RAM) <b>1003</b> are connected to the bus <b>1004</b>.</p><p id="p-0231" num="0230">The input/output interface <b>1005</b> is connected with an input unit <b>1006</b> configured by an input device such as a keyboard and a mouse for a user to input an operation command, an output unit <b>1007</b> which outputs a processing operation screen and an image of a processing result to a display device, a storage unit <b>1008</b> which includes a hard disk drive or the like for storing programs and various kinds of data, and a communication unit <b>1009</b> which includes a local area network (LAN) adapter or the like and executes communication processing via a network represented by the Internet. Furthermore, a drive <b>1010</b> which reads and writes data from and on a removable storage medium <b>1011</b> such as a magnetic disk (including a flexible disk), an optical disk (including a compact disc-read only memory (CD-ROM) and a digital versatile disc (DVD)), a magneto-optical disk (including a mini disc (MD)), and a semiconductor memory is connected.</p><p id="p-0232" num="0231">The CPU <b>1001</b> executes various processes according to a program stored in the ROM <b>1002</b> or a program which is read from the removable storage medium <b>1011</b> such as a magnetic disk, an optical disk, a magneto-optical disk, or a semiconductor memory, is installed in the storage unit <b>1008</b>, and is loaded from the storage unit <b>1008</b> into the RAM <b>1003</b>. The RAM <b>1003</b> also appropriately stores data or the like necessary for the CPU <b>1001</b> to execute various processes.</p><p id="p-0233" num="0232">In the computer configured as described above, the above-described series of processes are performed, for example, in such a manner that the CPU <b>1001</b> loads the program stored in the storage unit <b>1008</b> into the RAM <b>1003</b> via the input/output interface <b>1005</b> and the bus <b>1004</b> and executes the program.</p><p id="p-0234" num="0233">For example, the program executed by the computer (CPU <b>1001</b>) can be recorded and provided on the removable storage medium <b>1011</b> as a package medium and the like. Furthermore, the program can be provided via a wired or wireless transmission medium such as a local area network, the Internet, or digital satellite broadcasting.</p><p id="p-0235" num="0234">In the computer, the program can be installed in the storage unit <b>1008</b> via the input/output interface <b>1005</b> by mounting the removable storage medium <b>1011</b> in the drive <b>1010</b>. Furthermore, the program can be received by the communication unit <b>1009</b> and installed in the storage unit <b>1008</b> via a wired or wireless transmission medium. In addition, the program can be installed in advance in the ROM <b>1002</b> or the storage unit <b>1008</b>.</p><p id="p-0236" num="0235">Note that the program executed by the computer may be a program in which processing is performed in time series in the order described in this description or a program in which processing is performed in parallel or at a necessary timing such as when a call is made.</p><p id="p-0237" num="0236">Note that the CPU <b>1001</b> in <figref idref="DRAWINGS">FIG. <b>12</b></figref> implements the function of the control unit <b>52</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, and the storage unit <b>1008</b> implements the function of the storage unit <b>53</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0238" num="0237">Furthermore, in this description, the system means a set of a plurality of components (devices, modules (parts) and the like), and it does not matter whether or not all the components are in the same housing. Therefore, both a plurality of devices which is housed in separate housings and connected via a network and one device in which a plurality of modules is housed in one housing are the systems.</p><p id="p-0239" num="0238">Note that the embodiment of the present disclosure is not limited to the above-described embodiments, and various modifications can be made without departing from the scope of the present disclosure.</p><p id="p-0240" num="0239">For example, the present disclosure can be configured as cloud computing in which one function is shared by a plurality of devices via a network and jointly processed.</p><p id="p-0241" num="0240">Furthermore, each step described in the above-described flowcharts can be executed by one device or shared by a plurality of devices.</p><p id="p-0242" num="0241">Moreover, in a case where one step includes a plurality of processes, the plurality of processes included in the one step can be executed by one device or shared by a plurality of devices.</p><p id="p-0243" num="0242">Note that the present disclosure can also have the following configurations.</p><p id="p-0244" num="0243">&#x3c;1&#x3e;</p><p id="p-0245" num="0244">An information processing apparatus including: an encoder configured to encode input content including a sequence of data to convert the input content into a latent variable;<ul id="ul0004" list-style="none">    <li id="ul0004-0001" num="0000">    <ul id="ul0005" list-style="none">        <li id="ul0005-0001" num="0245">a decoder configured to decode the latent variable to reconfigure output content;</li>        <li id="ul0005-0002" num="0246">a loss function calculation unit configured to calculate a loss function on the basis of a likelihood of the input content; and</li>        <li id="ul0005-0003" num="0247">a control unit configured to lower a gradient of the loss function to update the latent variable, and control the decoder to decode the updated latent variable to reconfigure output content.</li>    </ul>    </li></ul></p><p id="p-0246" num="0248">&#x3c;2&#x3e;</p><p id="p-0247" num="0249">The information processing apparatus according to &#x3c;1&#x3e;, in which<ul id="ul0006" list-style="none">    <li id="ul0006-0001" num="0000">    <ul id="ul0007" list-style="none">        <li id="ul0007-0001" num="0250">the encoder and the decoder are variational auto encoder (VAE)-learned such that a posterior distribution is regularized by a prior distribution while keeping a reconfiguration error minimized.</li>    </ul>    </li></ul></p><p id="p-0248" num="0251">&#x3c;3&#x3e;</p><p id="p-0249" num="0252">The information processing apparatus according to &#x3c;1&#x3e;, in which<ul id="ul0008" list-style="none">    <li id="ul0008-0001" num="0000">    <ul id="ul0009" list-style="none">        <li id="ul0009-0001" num="0253">the loss function calculation unit includes        <ul id="ul0010" list-style="none">            <li id="ul0010-0001" num="0254">a likelihood evaluator that calculates a likelihood of the input content, and</li>            <li id="ul0010-0002" num="0255">a reality evaluator that calculates a reality likelihood which is a likelihood of reality of the input content, and</li>        </ul>        </li>        <li id="ul0009-0002" num="0256">the loss function is calculated on the basis of the likelihood which is an evaluation result of the likelihood evaluator and the reality likelihood which is an evaluation result of the reality evaluator.</li>    </ul>    </li></ul></p><p id="p-0250" num="0257">&#x3c;4&#x3e;</p><p id="p-0251" num="0258">The information processing apparatus according to &#x3c;3&#x3e;, in which<ul id="ul0011" list-style="none">    <li id="ul0011-0001" num="0000">    <ul id="ul0012" list-style="none">        <li id="ul0012-0001" num="0259">in a case where the control unit lowers the gradient of the loss function to update the latent variable, the control unit lowers the gradient of the loss function to update the latent variable such that the reality likelihood increases when the reality likelihood is smaller than a predetermined value, and controls the decoder to decode the updated latent variable to reconfigure the output content.</li>    </ul>    </li></ul></p><p id="p-0252" num="0260">&#x3c;5&#x3e;</p><p id="p-0253" num="0261">The information processing apparatus according to &#x3c;4&#x3e;, in which<ul id="ul0013" list-style="none">    <li id="ul0013-0001" num="0000">    <ul id="ul0014" list-style="none">        <li id="ul0014-0001" num="0262">the control unit lowers the gradient of the loss function and moves a position in the latent variable space to update the latent variable such that the reality increases when the reality likelihood is smaller than the predetermined value on the basis of a distribution of the likelihood for each latent variable in a latent variable space which is a space of the latent variable and a distribution of the reality likelihood, and controls the decoder to decode the updated latent variable to reconfigure the output content.</li>    </ul>    </li></ul></p><p id="p-0254" num="0263">&#x3c;6&#x3e;</p><p id="p-0255" num="0264">The information processing apparatus according to &#x3c;3&#x3e;, in which<ul id="ul0015" list-style="none">    <li id="ul0015-0001" num="0000">    <ul id="ul0016" list-style="none">        <li id="ul0016-0001" num="0265">the likelihood evaluator obtains, as a likelihood, a result obtained by logarithmizing a likelihood including a probability that the input content is content registered as a sample in advance.</li>    </ul>    </li></ul></p><p id="p-0256" num="0266">&#x3c;7&#x3e;</p><p id="p-0257" num="0267">The information processing apparatus according to &#x3c;6&#x3e;, in which<ul id="ul0017" list-style="none">    <li id="ul0017-0001" num="0000">    <ul id="ul0018" list-style="none">        <li id="ul0018-0001" num="0268">the likelihood evaluator obtains a likelihood which is a probability that a sequence of the data configuring the input content is content registered as a sample in advance, the likelihood being a result obtained by logarithmizing a likelihood obtained as a product of conditional probabilities for each sequence of the data.</li>    </ul>    </li></ul></p><p id="p-0258" num="0269">&#x3c;8&#x3e;</p><p id="p-0259" num="0270">The information processing apparatus according to &#x3c;3&#x3e;, in which<ul id="ul0019" list-style="none">    <li id="ul0019-0001" num="0000">    <ul id="ul0020" list-style="none">        <li id="ul0020-0001" num="0271">the reality evaluator obtains, as the reality likelihood, a result obtained by logarithmizing a likelihood of the reality including a probability that the input content is content generated by a human.</li>    </ul>    </li></ul></p><p id="p-0260" num="0272">&#x3c;9&#x3e;</p><p id="p-0261" num="0273">The information processing apparatus according to &#x3c;3&#x3e;, in which<ul id="ul0021" list-style="none">    <li id="ul0021-0001" num="0000">    <ul id="ul0022" list-style="none">        <li id="ul0022-0001" num="0274">the loss function includes a first term based on the likelihood of the input content and a second term based on the reality likelihood.</li>    </ul>    </li></ul></p><p id="p-0262" num="0275">&#x3c;10&#x3e;</p><p id="p-0263" num="0276">The information processing apparatus according to &#x3c;9&#x3e;, in which<ul id="ul0023" list-style="none">    <li id="ul0023-0001" num="0000">    <ul id="ul0024" list-style="none">        <li id="ul0024-0001" num="0277">the first term includes a square of a difference between the likelihood of the input content and a predetermined likelihood obtained by multiplying a predetermined constant by a predetermined coefficient.</li>    </ul>    </li></ul></p><p id="p-0264" num="0278">&#x3c;11&#x3e;</p><p id="p-0265" num="0279">The information processing apparatus according to&#x3c;10&#x3e;, in which<ul id="ul0025" list-style="none">    <li id="ul0025-0001" num="0000">    <ul id="ul0026" list-style="none">        <li id="ul0026-0001" num="0280">the predetermined coefficient is set to a value larger than 1 when the likelihood of the output content is increased to make the output content common and is set to a value smaller than 1 when the likelihood of the output content is decreased to make the output content eccentric.</li>    </ul>    </li></ul></p><p id="p-0266" num="0281">&#x3c;12&#x3e;</p><p id="p-0267" num="0282">The information processing apparatus according to &#x3c;10&#x3e;, in which<ul id="ul0027" list-style="none">    <li id="ul0027-0001" num="0000">    <ul id="ul0028" list-style="none">        <li id="ul0028-0001" num="0283">the predetermined constant is an initial value of the likelihood.</li>    </ul>    </li></ul></p><p id="p-0268" num="0284">&#x3c;13&#x3e;</p><p id="p-0269" num="0285">The information processing apparatus according to &#x3c;9&#x3e;, in which<ul id="ul0029" list-style="none">    <li id="ul0029-0001" num="0000">    <ul id="ul0030" list-style="none">        <li id="ul0030-0001" num="0286">in the first term, a positive coefficient is assigned to the likelihood of the input content when the likelihood is increased, and a negative coefficient is assigned to the likelihood of the input content when the likelihood is decreased.</li>    </ul>    </li></ul></p><p id="p-0270" num="0287">&#x3c;14&#x3e;<ul id="ul0031" list-style="none">    <li id="ul0031-0001" num="0000">    <ul id="ul0032" list-style="none">        <li id="ul0032-0001" num="0288">The information processing apparatus according to &#x3c;9&#x3e;, in which</li>        <li id="ul0032-0002" num="0289">the first term includes a sum of squares of differences between likelihoods of elements configuring the input content and a predetermined likelihood configured by a product of predetermined coefficient and constant of each element.</li>    </ul>    </li></ul></p><p id="p-0271" num="0290">&#x3c;15&#x3e;</p><p id="p-0272" num="0291">The information processing apparatus according to &#x3c;3&#x3e;, in which<ul id="ul0033" list-style="none">    <li id="ul0033-0001" num="0000">    <ul id="ul0034" list-style="none">        <li id="ul0034-0001" num="0292">in a case where a part not to be changed in the input content is designated as a context, the encoder encodes a part other than the context in the input content as input partial data and converts the input partial data into a latent variable,</li>        <li id="ul0034-0002" num="0293">the decoder decodes the latent variable to reconfigure output partial data, and</li>        <li id="ul0034-0003" num="0294">the control unit lowers a gradient of the loss function to update the latent variable, controls the decoder to decode the updated latent variable to reconfigure output partial data, and integrates the reconfigured output partial data and the context to reconfigure output content.</li>    </ul>    </li></ul></p><p id="p-0273" num="0295">&#x3c;16&#x3e;</p><p id="p-0274" num="0296">The information processing apparatus according to &#x3c;15&#x3e;, in which<ul id="ul0035" list-style="none">    <li id="ul0035-0001" num="0000">    <ul id="ul0036" list-style="none">        <li id="ul0036-0001" num="0297">the loss function calculation unit calculates the loss function on the basis of a conditional likelihood which is a likelihood of the input partial data other than the context in the input content.</li>    </ul>    </li></ul></p><p id="p-0275" num="0298">&#x3c;17&#x3e;</p><p id="p-0276" num="0299">The information processing apparatus according to &#x3c;15&#x3e;, in which<ul id="ul0037" list-style="none">    <li id="ul0037-0001" num="0000">    <ul id="ul0038" list-style="none">        <li id="ul0038-0001" num="0300">the loss function calculation unit uses the likelihood of the input content as the reality likelihood of the input content.</li>    </ul>    </li></ul></p><p id="p-0277" num="0301">&#x3c;18&#x3e;</p><p id="p-0278" num="0302">The information processing apparatus according to any one of &#x3c;1&#x3e; to &#x3c;17&#x3e;, in which<ul id="ul0039" list-style="none">    <li id="ul0039-0001" num="0000">    <ul id="ul0040" list-style="none">        <li id="ul0040-0001" num="0303">the control unit stepwise lowers a gradient of the loss function by a predetermined magnitude to stepwise update the latent variable and controls the decoder to repeatedly decode the updated latent variable to stepwise reconfigure a plurality of pieces of output content.</li>    </ul>    </li></ul></p><p id="p-0279" num="0304">&#x3c;19&#x3e;</p><p id="p-0280" num="0305">An information processing method of an information processing apparatus including<ul id="ul0041" list-style="none">    <li id="ul0041-0001" num="0000">    <ul id="ul0042" list-style="none">        <li id="ul0042-0001" num="0306">an encoder,</li>        <li id="ul0042-0002" num="0307">a decoder,</li>        <li id="ul0042-0003" num="0308">a loss function calculation unit, and</li>        <li id="ul0042-0004" num="0309">a control unit,</li>        <li id="ul0042-0005" num="0310">the method including steps of:</li>        <li id="ul0042-0006" num="0311">by the encoder, encoding input content including a sequence of data to convert the input content into a latent variable;</li>        <li id="ul0042-0007" num="0312">by the decoder, decoding the latent variable to reconfigure output content;</li>        <li id="ul0042-0008" num="0313">by the loss function calculation unit, calculating a loss function on the basis of a likelihood of the input content; and</li>        <li id="ul0042-0009" num="0314">by the control unit, lowering a gradient of the loss function to update the latent variable and controlling the decoder to decode the updated latent variable to reconfigure output content.</li>    </ul>    </li></ul></p><p id="p-0281" num="0315">&#x3c;20&#x3e;</p><p id="p-0282" num="0316">A program causing a computer to execute functions including:<ul id="ul0043" list-style="none">    <li id="ul0043-0001" num="0000">    <ul id="ul0044" list-style="none">        <li id="ul0044-0001" num="0317">an encoder configured to encode input content including a sequence of data to convert the input content into a latent variable;</li>        <li id="ul0044-0002" num="0318">a decoder configured to decode the latent variable to reconfigure output content;</li>        <li id="ul0044-0003" num="0319">a loss function calculation unit configured to calculate a loss function on the basis of a likelihood of the input content; and</li>        <li id="ul0044-0004" num="0320">a control unit configured to lower a gradient of the loss function to update the latent variable, and control the decoder to decode the updated latent variable to reconfigure output content.</li>    </ul>    </li></ul></p><heading id="h-0021" level="1">REFERENCE SIGNS LIST</heading><p id="p-0283" num="0000"><ul id="ul0045" list-style="none">    <li id="ul0045-0001" num="0321"><b>31</b> Information processing apparatus</li>    <li id="ul0045-0002" num="0322"><b>32</b> Input/output device</li>    <li id="ul0045-0003" num="0323"><b>51</b> Communication unit</li>    <li id="ul0045-0004" num="0324"><b>52</b> Control unit</li>    <li id="ul0045-0005" num="0325"><b>53</b> Storage unit</li>    <li id="ul0045-0006" num="0326"><b>71</b> Learning unit</li>    <li id="ul0045-0007" num="0327"><b>72</b> Optimization unit</li>    <li id="ul0045-0008" num="0328"><b>73</b> Generation unit</li>    <li id="ul0045-0009" num="0329"><b>81</b> Model storage unit</li>    <li id="ul0045-0010" num="0330"><b>82</b> Music DB</li>    <li id="ul0045-0011" num="0331"><b>91</b> Encoder</li>    <li id="ul0045-0012" num="0332"><b>92</b> Decoder</li>    <li id="ul0045-0013" num="0333"><b>93</b> Loss function calculation unit</li>    <li id="ul0045-0014" num="0334"><b>101</b>, <b>101</b>&#x2032; Likelihood evaluator</li>    <li id="ul0045-0015" num="0335"><b>102</b>, <b>102</b>&#x2032; Reality evaluator</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005459A1-20230105-M00001.NB"><img id="EMI-M00001" he="10.58mm" wi="76.20mm" file="US20230005459A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230005459A1-20230105-M00002.NB"><img id="EMI-M00002" he="6.35mm" wi="76.20mm" file="US20230005459A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230005459A1-20230105-M00003.NB"><img id="EMI-M00003" he="6.35mm" wi="76.20mm" file="US20230005459A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An information processing apparatus comprising:<claim-text>an encoder configured to encode input content including a sequence of data to convert the input content into a latent variable;</claim-text><claim-text>a decoder configured to decode the latent variable to reconfigure output content;</claim-text><claim-text>a loss function calculation unit configured to calculate a loss function on a basis of a likelihood of the input content; and</claim-text><claim-text>a control unit configured to lower a gradient of the loss function to update the latent variable, and control the decoder to decode the updated latent variable to reconfigure output content.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the encoder and the decoder are variational auto encoder (VAE)-learned such that a posterior distribution is regularized by a prior distribution while keeping a reconfiguration error minimized.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the loss function calculation unit includes<claim-text>a likelihood evaluator that calculates a likelihood of the input content, and</claim-text><claim-text>a reality evaluator that calculates a reality likelihood which is a likelihood of reality of the input content, and</claim-text></claim-text><claim-text>the loss function is calculated on a basis of the likelihood which is an evaluation result of the likelihood evaluator and the reality likelihood which is an evaluation result of the reality evaluator.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The information processing apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>in a case where the control unit lowers the gradient of the loss function to update the latent variable, the control unit lowers the gradient of the loss function to update the latent variable such that the reality likelihood increases when the reality likelihood is smaller than a predetermined value, and controls the decoder to decode the updated latent variable to reconfigure the output content.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The information processing apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein<claim-text>the control unit lowers the gradient of the loss function and moves a position in the latent variable space to update the latent variable such that the reality increases when the reality likelihood is smaller than the predetermined value on a basis of a distribution of the likelihood for each latent variable in a latent variable space which is a space of the latent variable and a distribution of the reality likelihood, and controls the decoder to decode the updated latent variable to reconfigure the output content.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The information processing apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the likelihood evaluator obtains, as a likelihood, a result obtained by logarithmizing a likelihood including a probability that the input content is content registered as a sample in advance.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The information processing apparatus according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein<claim-text>the likelihood evaluator obtains a likelihood which is a probability that a sequence of the data configuring the input content is content registered as a sample in advance, the likelihood being a result obtained by logarithmizing a likelihood obtained as a product of conditional probabilities for each sequence of the data.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The information processing apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the reality evaluator obtains, as the reality likelihood, a result obtained by logarithmizing a likelihood of the reality including a probability that the input content is content generated by a human.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The information processing apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the loss function includes a first term based on the likelihood of the input content and a second term based on the reality likelihood.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The information processing apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>the first term includes a square of a difference between the likelihood of the input content and a predetermined likelihood obtained by multiplying a predetermined constant by a predetermined coefficient.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The information processing apparatus according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein<claim-text>the predetermined coefficient is set to a value larger than 1 when the likelihood of the output content is increased to make the output content common and is set to a value smaller than 1 when the likelihood of the output content is decreased to make the output content eccentric.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The information processing apparatus according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein<claim-text>the predetermined constant is an initial value of the likelihood.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The information processing apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>in the first term, a positive coefficient is assigned to the likelihood of the input content when the likelihood is increased, and a negative coefficient is assigned to the likelihood of the input content when the likelihood is decreased.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The information processing apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>the first term includes a sum of squares of differences between likelihoods of elements configuring the input content and a predetermined likelihood configured by a product of predetermined coefficient and constant of each element.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The information processing apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>in a case where a part not to be changed in the input content is designated as a context, the encoder encodes a part other than the context in the input content as input partial data and converts the input partial data into a latent variable,</claim-text><claim-text>the decoder decodes the latent variable to reconfigure output partial data, and</claim-text><claim-text>the control unit lowers a gradient of the loss function to update the latent variable, controls the decoder to decode the updated latent variable to reconfigure output partial data, and integrates the reconfigured output partial data and the context to reconfigure output content.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The information processing apparatus according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein<claim-text>the loss function calculation unit calculates the loss function on a basis of a conditional likelihood which is a likelihood of the input partial data other than the context in the input content.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The information processing apparatus according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein<claim-text>the loss function calculation unit uses the likelihood of the input content as the reality likelihood of the input content.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the control unit stepwise lowers a gradient of the loss function by a predetermined magnitude to stepwise update the latent variable and controls the decoder to repeatedly decode the updated latent variable to stepwise reconfigure a plurality of pieces of output content.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. An information processing method of an information processing apparatus including an encoder,<claim-text>a decoder,</claim-text><claim-text>a loss function calculation unit, and</claim-text><claim-text>a control unit,</claim-text><claim-text>the method comprising steps of:</claim-text><claim-text>by the encoder, encoding input content including a sequence of data to convert the input content into a latent variable;</claim-text><claim-text>by the decoder, decoding the latent variable to reconfigure output content;</claim-text><claim-text>by the loss function calculation unit, calculating a loss function on a basis of a likelihood of the input content; and</claim-text><claim-text>by the control unit, lowering a gradient of the loss function to update the latent variable and controlling the decoder to decode the updated latent variable to reconfigure output content.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A program causing a computer to execute functions comprising:<claim-text>an encoder configured to encode input content including a sequence of data to convert the input content into a latent variable;</claim-text><claim-text>a decoder configured to decode the latent variable to reconfigure output content;</claim-text><claim-text>a loss function calculation unit configured to calculate a loss function on a basis of a likelihood of the input content; and</claim-text><claim-text>a control unit configured to lower a gradient of the loss function to update the latent variable, and control the decoder to decode the updated latent variable to reconfigure output content.</claim-text></claim-text></claim></claims></us-patent-application>