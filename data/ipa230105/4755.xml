<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004756A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004756</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17901494</doc-number><date>20220901</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>44</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6259</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6218</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6228</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>44</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6269</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">LEVERAGING SMART-PHONE CAMERAS AND IMAGE PROCESSING TECHNIQUES TO CLASSIFY MOSQUITO GENUS AND SPECIES</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17168941</doc-number><date>20210205</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11501113</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17901494</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16673641</doc-number><date>20191104</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10963742</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17168941</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62754971</doc-number><date>20181102</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>University of South Florida</orgname><address><city>Tampa</city><state>FL</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Chellappan</last-name><first-name>Sriram</first-name><address><city>Tampa</city><state>FL</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Bharti</last-name><first-name>Pratool</first-name><address><city>Tampa</city><state>FL</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Minakshi</last-name><first-name>Mona</first-name><address><city>Tampa</city><state>FL</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>McClinton</last-name><first-name>Willie</first-name><address><city>Tampa</city><state>FL</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Mirzakhalov</last-name><first-name>Jamshidbek</first-name><address><city>Tampa</city><state>FL</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Identifying insect species integrates image processing, feature selection, unsupervised clustering, and a support vector machine (SVM) learning algorithm for classification. Results with a total of 101 mosquito specimens spread across nine different vector carrying species demonstrate high accuracy in species identification. When implemented as a smart-phone application, the latency and energy consumption were minimal. The currently manual process of species identification and recording can be sped up, while also minimizing the ensuing cognitive workload of personnel. Citizens at large can use the system in their own homes for self-awareness and share insect identification data with public health agencies.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="47.67mm" wi="125.65mm" file="US20230004756A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="225.98mm" wi="171.20mm" orientation="landscape" file="US20230004756A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="212.51mm" wi="78.66mm" orientation="landscape" file="US20230004756A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="229.28mm" wi="156.04mm" orientation="landscape" file="US20230004756A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="210.65mm" wi="108.71mm" orientation="landscape" file="US20230004756A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="205.06mm" wi="146.13mm" orientation="landscape" file="US20230004756A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="210.99mm" wi="113.88mm" orientation="landscape" file="US20230004756A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="226.40mm" wi="123.53mm" orientation="landscape" file="US20230004756A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="194.06mm" wi="139.87mm" orientation="landscape" file="US20230004756A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims priority to and incorporates entirely by reference U.S. patent application Ser. No. 17/168,941 filed on Feb. 5, 2021, which is a continuation of U.S. patent application Ser. No. 16/673,641, filed on Nov. 4, 2019, now U.S. Pat. No. 10,963,742, which claims priority to Provisional Patent Application Ser. No. 62/754,971 filed on Nov. 2, 2018, all entitled Leveraging Smart-Phone Cameras and Image Processing Techniques to Classify Mosquito Genus and Species.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?federal-research-statement description="Federal Research Statement" end="lead"?><heading id="h-0002" level="1">ACKNOWLEDGMENT OF GOVERNMENT SUPPORT</heading><p id="p-0003" num="0002">This invention was made with government support under grant CBET 1743985 awarded by the National Science Foundation. The government has certain rights in the invention.</p><?federal-research-statement description="Federal Research Statement" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Mosquito borne diseases (e.g., Malaria, Dengue, West Nile Fever, and most recently Zika Fever) are among the biggest health care concerns across the globe today. To mitigate the spread of mosquito borne diseases, it is vital to combat the spread of mosquitoes. Of critical importance in this mission is the identification of species prevalent in an area of interest. This is important because there are close to 4,000 different species of mosquitoes present in the world today, spread across 40 or so genera [1], and with increasing globalization and warming, the species are spreading to newer locations, with some of them acting as vectors for several diseases. In any given location, multiple species are usually found at the same time (some being vectors for disease and some not). However, the process of genus and species identification is not at all easy.</p><p id="p-0005" num="0004">As of today, to derive populations of mosquitoes in any area, trained professionals lay traps, and pick them up soon after to sort trapped specimens. Sometimes, hundreds of mosquitoes can be trapped in a single day. Then, to identify each specimen trapped, it is placed under a microscope, and visually identified (to determine genus and species), which takes hours each day for all specimens. Depending on location and time of year, this process can repeat multiple times in a single week, and is cognitively demanding. Such kinds of mosquito control facilities are expensive to manage, and they are few even in advanced countries. In low economy countries, where mosquitoes pose a greater danger, such facilities are even more scarce. With rising temperatures and population migrations, mosquitoes are believed to be invading newer areas across the world, and detecting them early is a huge challenge today.</p><p id="p-0006" num="0005">Experts at mosquito control facilities acknowledge that, depending on location and time of the year, they can receive hundreds of calls each day from concerned citizens about mosquitoes in their neighborhoods. Due to limited resources, knowledge of mosquito genus and species types can play a vital role in prioritizing schedules for trap placement and spraying repellents during peak times, since different mosquito species are vectors for different diseases. In general, the deadliest mosquitoes belong to three genus types: <i>Aedes, Anopheles </i>and <i>Culex</i>. Within these genera, the species that are most deadly include <i>Aedes aegypti </i>and <i>Aedes albopictus </i>(dengue, chikungunya, yellow fever and Zika fever); <i>Anopheles stephensi, Anopheles funestus </i>and <i>Anopheles gambiae </i>(malaria); <i>Culex nigripalpus, Culex pipiens </i>and <i>Culex quinquefasciatus </i>(St. Louis encephalitis, West Nile fever, eastern equine encephalitis). But note that the ones above are the only ones that spread diseases. There are other species with these three genus types, and other there are ones in other genus also that spread diseases. Sadly, despite citizens willing to assist in the process of mosquito identification, there is no way to enable that now. One practice recommended by experts is to ask citizens to collect a few mosquitoes (after spraying insecticide on them), and store them in a transparent bag for the experts to identify them later. But this process is cumbersome, and the need for technology-based solutions to empower citizens in this effort has become clear [33].</p><p id="p-0007" num="0006">Overview of Proposed Solutions in Background Literature</p><p id="p-0008" num="0007">a). Image Based Techniques Using Digital Cameras: In [10], a solution is proposed to detect <i>Aedes aegypti </i>species using images taken from a 500&#xd7; optical zoom camera and utilizing a computerized support vector machine classification algorithm. Using a sample of 40 images, seven textural features, and a support vector machine classification algorithm, an accuracy of 92.5% was demonstrated in classifying <i>Aedes aegypti </i>species from others. This solution though is expensive, and addresses a binary classification problem only. Work in [14] and [13] discusses machine learning techniques to classify mosquitoes from insects like flies and bees using images taken from digital cameras. The problem addressed in these papers is too generic though. In a recent paper [32], the authors address a problem similar to ours, but sufficiently different. Specifically, twelve (12) adult mosquito specimens from three genera (<i>Aedes, Anopheles </i>and <i>Culex</i>) were collected, and the right wing of each specimen was photographed using a sophisticated digital camera coupled with a microscope. Then, using coordinates at intersections of wing veins as a feature, followed by a Neighbor Joining Tree classification method, the accuracy in genus identification (among three) was 90%. This technique again is expensive and requires expertise.</p><p id="p-0009" num="0008">b). Using Techniques other than Imaging: In [8], the authors attempt to use optical (rather than acoustic) sensors to record the &#x201c;sound&#x201d; of insect flight from a small distance, and then design a Bayesian classifier to identify four species of mosquitoes (<i>Aedes aegypti, Culex </i>quinque <i>fasciatus, Culex stigmatosoma</i>, and <i>Culex tarsalis</i>), and achieve an accuracy of 96%. Similarly, the work in [23] also leverages smart-phone microphones to capture and process sound, or acoustics, data of mosquito flight, along with location and time of observation. The claim is that these features are unique to classify mosquito species. More innovative techniques like hydrogel-based low-cost microfluidic chips, baited with odorants to capture saliva droplets of mosquitoes are being designed by researchers in order to serve as a test for vector species and pathogens. All of these techniques require &#x201c;live&#x201d; and &#x201c;mobile&#x201d; mosquitoes, with sensing devices placed close to them. They are not suited for ubiquitous and in-home use by both scientists as well as common citizens.</p><p id="p-0010" num="0009">c). Other Related Work: A survey on imaging techniques to classify insects is presented in [19]. However, mosquitoes are not classified there. In [26], the authors ask citizens to use smart-phones for imaging and reporting about mosquitoes they encounter, but species classification is not discussed. In [24], Munoz et. al. propose a deep learning framework to classify larvae of mosquitoes from larvae of other insects, with smart-phone images. In [5], intensity of red blood cells computed from thin blood smear images were used to identify the presence of malarial (<i>plasmodium</i>) parasites in blood samples. Microsoft's &#x201c;Project Premonition&#x201d; is an ambitious effort to use drones and DNA sequencing techniques to identify mosquito species in hot-spots [4]. These recent works highlight important, but orthogonal tech-based solutions to combat mosquito-borne diseases, but ubiquitous and easy to use solutions for identifying mosquitoes species are not yet there.</p><p id="p-0011" num="0010">To summarize, tech-based solutions to combat the spread of mosquito-borne diseases is an important need of the hour. However, there is no system yet that enables common citizens to participate in mosquito identification. This disclosure fills the gap by designing a computerized process, such as one enabled in a smart-phone based system, that enables anyone to take images of a still mosquito that is alive or dead (after possibly spraying or trapping), but still retaining its physical form, and then processes the images for species identification. This disclosure addresses the need for a system that is cheap, ubiquitous, and easily expandable to include more mosquito species beyond the current nine classified and discussed herein. The problem of identifying mosquito species from images is much harder than the certain others related to plants or larger animals, since there are no obvious (and un-aided) visually discernible markers across species types perceptible to the naked eye. In fact, public health workers with decades of experience still need a microscope and careful analysis to identify the species type of a mosquito specimen, hence demonstrating the complexity of the problem addressed here.</p><p id="p-0012" num="0011">Based on the facts mentioned above and coupled with the increasing global spread of mosquito-borne diseases, public health experts are highly receptive to any technology-based solution for mosquito species identification and recording that is accurate, comfortable and fast, so that a) human resources in public health can be utilized more effectively, and b) citizens can be better informed and, hence, better served. To this extent, this disclosure addresses a need for a computer-based system that processes digital images, such as those gathered by a smart phone. The apparatuses, systems, and methods disclosed herein utilize commonly available processing hardware of computers, mobile computers, personal devices, and smart telephones and enable anyone to take images of a still mosquito that is alive or dead (after possibly spraying or trapping), but still retaining its physical form, and then processes the captured images for genus and species identification.</p><heading id="h-0004" level="1">BRIEF SUMMARY OF THE DISCLOSURE</heading><p id="p-0013" num="0012">A computer implemented method of identifying an insect specimen with software storing instructions implementing computerized steps that include gathering a plurality of digital images of multiple insect specimens, wherein the digital images have a plurality of image orientations relative to a common axis; segmenting respective digital images into body pixels and image background pixels by detecting edges of the body pixels; extracting features of the insect specimens from the respective digital images by calculating variance of pixel intensity across the respective digital images; clustering the digital images according to the features; and classifying the insect specimens by species according to respective clusters.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE FIGURES</heading><p id="p-0014" num="0013">The patent application file or the patent issuing therefrom contains at least one drawing executed in color. Copies of this patent or patent application publication with the color drawing(s) will be provided by the Office upon request and payment of the necessary fee. Reference will now be made to the accompanying drawings, which are not necessarily drawn to scale.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a series of photographs (a) to (i) of nine species of mosquitos (three across three genus types) considered in one non-limiting example of this disclosure. <figref idref="DRAWINGS">FIG. <b>1</b>(<i>a</i>)</figref> is a photograph of an insect specimen mosquito of the species <i>Aedes aegypti</i>. <figref idref="DRAWINGS">FIG. <b>1</b>(<i>b</i>)</figref> is a photograph of an insect specimen mosquito of the species <i>Aedes </i>in fir matus. <figref idref="DRAWINGS">FIG. <b>1</b>(<i>c</i>)</figref> is a photograph of an insect specimen mosquito of the species <i>Aedes taeniorhynchus</i>. <figref idref="DRAWINGS">FIG. <b>1</b>(<i>d</i>)</figref> is a photograph of an insect specimen mosquito of the species <i>Anopheles crucians</i>. <figref idref="DRAWINGS">FIG. <b>1</b>(<i>e</i>)</figref> is a photograph of an insect specimen mosquito of the species <i>Coquillettidia perturbans</i>. <figref idref="DRAWINGS">FIG. <b>1</b>(<i>f</i>)</figref> is a photograph of an insect specimen mosquito of the species <i>Culex nigripalpus</i>. <figref idref="DRAWINGS">FIG. <b>1</b>(<i>g</i>)</figref> is a photograph of an insect specimen mosquito of the species <i>Mansonia titillans</i>. <figref idref="DRAWINGS">FIG. <b>1</b>(<i>h</i>)</figref> is a photograph of an insect specimen mosquito of the species <i>Psorophora columbiae</i>. <figref idref="DRAWINGS">FIG. <b>1</b>(<i>i</i>)</figref> is a photograph of an insect specimen mosquito of the species <i>Psorophora ferax. </i></p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a series of photographs (a) to (d) showing edge contrast in legs of different mosquito species. <figref idref="DRAWINGS">FIG. <b>2</b>(<i>a</i>)</figref> is a photograph of an insect specimen mosquito of the species <i>Aedes aegypti</i>. <figref idref="DRAWINGS">FIG. <b>2</b>(<i>b</i>)</figref> is a photograph of an insect specimen mosquito of the species <i>Aedes taeniorhynchus</i>. <figref idref="DRAWINGS">FIG. <b>2</b>(<i>c</i>)</figref> is a photograph of an insect specimen mosquito of the species <i>Coquillettidia perturbans</i>. <figref idref="DRAWINGS">FIG. <b>2</b>(<i>d</i>)</figref> is a photograph of an insect specimen mosquito of the species <i>Psorophora columbiae. </i></p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a series of photographs (a) to (d) showing color contrast in wings of different mosquito species. <figref idref="DRAWINGS">FIG. <b>3</b>(<i>a</i>)</figref> is a photograph of an insect specimen mosquito of the species <i>Aedes aegypti</i>. <figref idref="DRAWINGS">FIG. <b>3</b>(<i>b</i>)</figref> is a photograph of an insect specimen mosquito of the species <i>Aedes taeniorhynchus</i>. <figref idref="DRAWINGS">FIG. <b>3</b>(<i>c</i>)</figref> is a photograph of an insect specimen mosquito of the species <i>Coquillettidia perturbans</i>. <figref idref="DRAWINGS">FIG. <b>3</b>(<i>d</i>)</figref> is a photograph of an insect specimen mosquito of the species <i>Psorophora columbiae. </i></p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic representation of selected results of background segmentation procedures set forth in this disclosure, beginning with an original image taken in a pink background, an additional image showing segmentation with significant contours, and a final image showing segmentation with integration of significant contours and a Gaussian mixture model.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic representation of a local binary pattern calculation for a single pixel of image data as disclosed herein.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic representation of three clusters of species identified after expectation maximization (EM) clustering as disclosed herein.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic representation of data results showing comparative graphs of precision, recall, and F1-Measure for a 10-fold cross validation method for seven species.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is an accuracy graph of the top two results for a 10-fold cross validation method for seven species.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic diagram of an example computer environment configured to implement the computerized methods of this disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0024" num="0023">In some aspects, the present disclosure relates to computerized apparatuses, computer implemented methods, and computerized systems that use digital image analysis to identify species of insect specimens, such as, but not limited to mosquitos. The disclosure presents a system wherein a user (expert or an ordinary citizen) takes a photo of a mosquito using a smart-phone, and then the image is immediately sent to a central server along with GPS of the smart-phone. The server will implement algorithms described in this disclosure to a) identify the genus of the mosquito; b) identify the species of the mosquito; c) separate the body parts of the image into objects of interest like wings, legs, proboscis, abdomen, scutum etc.; d) give feedback on species and genus back to user, along with info as to what diseases the species carry, and more interesting information like flight range etc. Potential uses are in mosquito identification, since it is a painful/cognitively demanding problem now. School districts could also use this app to teach kids about biology and other areas of science, given that these kids of scientific analysis skill may eventually be mandatory for schools in many areas). Defense and Homeland Security agencies and other government agencies may see a need for the computerized application described herein.</p><p id="p-0025" num="0024">Although example embodiments of the present disclosure are explained in detail herein, it is to be understood that other embodiments are contemplated. Accordingly, it is not intended that the present disclosure be limited in its scope to the details of construction and arrangement of components set forth in the following description or illustrated in the drawings. The present disclosure is capable of other embodiments and of being practiced or carried out in various ways. For example, the test results and examples all pertain to identification of genus and species of mosquitos from the mosquito traits and features extracted from digital images. The techniques and concepts utilized and claimed in this disclosure, however, are not limited to mosquitos, but can be used with other kinds of identification processes for other animals, humans, plants and the like.</p><p id="p-0026" num="0025">It must also be noted that, as used in the specification and the appended claims, the singular forms &#x201c;a,&#x201d; &#x201c;an&#x201d; and &#x201c;the&#x201d; include plural referents unless the context clearly dictates otherwise. By &#x201c;comprising&#x201d; or &#x201c;containing&#x201d; or &#x201c;including&#x201d; is meant that at least the named compound, element, particle, or method step is present in the composition or article or method, but does not exclude the presence of other compounds, materials, particles, method steps, even if the other such compounds, material, particles, method steps have the same function as what is named.</p><p id="p-0027" num="0026">Ranges may be expressed herein as from &#x201c;about&#x201d; or &#x201c;approximately&#x201d; one particular value to &#x201c;about&#x201d; or &#x201c;approximately&#x201d; another particular value. When such a range is expressed, exemplary embodiments include from the one particular value to the other particular value. As used herein, &#x201c;about&#x201d; or &#x201c;approximately&#x201d; generally can mean within 20 percent, preferably within 10 percent, and more preferably within 5 percent of a given value or range, and can also include the exact value or range. Numerical quantities given herein can be approximate, meaning the term &#x201c;about&#x201d; or &#x201c;approximately&#x201d; can be inferred if not expressly stated.</p><p id="p-0028" num="0027">In describing example embodiments, terminology will be resorted to for the sake of clarity. It is intended that each term contemplates its broadest meaning as understood by those skilled in the art and includes all technical equivalents that operate in a similar manner to accomplish a similar purpose. It is also to be understood that the mention of one or more steps of a method does not preclude the presence of additional method steps or intervening method steps between those steps expressly identified. Steps of a method may be performed in a different order than those described herein without departing from the scope of the present disclosure. Similarly, it is also to be understood that the mention of one or more components in a device or system does not preclude the presence of additional components or intervening components between those components expressly identified.</p><p id="p-0029" num="0028">In prior work related to the embodiments of this disclosure, as disclosed in below noted reference [22], the utilized techniques leverage smart-phone images to identify a total of seven mosquito species. However, the technique in reference [22] had limitations stemming from poorer accuracy, inability to handle images taken in different backgrounds, and is also computationally very expensive to process on a smartphone (due to the processing of many features). In one non-limiting, improved system proposed in the embodiments of this disclosure, the number of genera identified is six and the number of species identified is nine, but the systems and methods described herein can directly be applied for more genus and species types across the globe. An improved system includes background segmentation that compensates for images taken in differing backgrounds; and is computationally much more efficient to enable processing on a smart-phone.</p><p id="p-0030" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 1</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Relevant Details on Dataset of Mosquito Species</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="5"><colspec colname="1" colwidth="49pt" align="left"/><colspec colname="2" colwidth="35pt" align="center"/><colspec colname="3" colwidth="42pt" align="center"/><colspec colname="4" colwidth="42pt" align="left"/><colspec colname="5" colwidth="49pt" align="left"/><tbody valign="top"><row><entry/><entry/><entry>No. of Image</entry><entry/><entry/></row><row><entry/><entry/><entry>Samples</entry></row><row><entry/><entry>No. of</entry><entry>(3 per</entry><entry>Disease</entry><entry>Geographical</entry></row><row><entry>Species</entry><entry>Specimens</entry><entry>Specimen)</entry><entry>Spread</entry><entry>Location</entry></row><row><entry namest="1" nameend="5" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="5"><colspec colname="1" colwidth="49pt" align="left"/><colspec colname="2" colwidth="35pt" align="char" char="."/><colspec colname="3" colwidth="42pt" align="char" char="."/><colspec colname="4" colwidth="42pt" align="left"/><colspec colname="5" colwidth="49pt" align="left"/><tbody valign="top"><row><entry><i>Aedes aegypti</i></entry><entry>11</entry><entry>33</entry><entry>Zika fever,</entry><entry>South America,</entry></row><row><entry/><entry/><entry/><entry>Dengue,</entry><entry>North America,</entry></row><row><entry/><entry/><entry/><entry>Chikungunya</entry><entry>Asia and</entry></row><row><entry/><entry/><entry/><entry/><entry>Africa</entry></row><row><entry><i>Aedes</i></entry><entry>10</entry><entry>30</entry><entry>Eastern</entry><entry>South America</entry></row><row><entry><i>infirmatus</i></entry><entry/><entry/><entry>Equine</entry><entry>and North</entry></row><row><entry/><entry/><entry/><entry>Encephalitis</entry><entry>America,</entry></row><row><entry/><entry/><entry/><entry>(EEE)</entry></row><row><entry><i>Aedes</i></entry><entry>8</entry><entry>24</entry><entry>West Nile</entry><entry>South America</entry></row><row><entry><i>taeniorhynchus</i></entry><entry/><entry/><entry>Virus</entry><entry>and North</entry></row><row><entry/><entry/><entry/><entry/><entry>America,</entry></row><row><entry><i>Anopheles</i></entry><entry>15</entry><entry>45</entry><entry>Malaria</entry><entry>South America,</entry></row><row><entry><i>Crucians</i></entry><entry/><entry/><entry/><entry>North America,</entry></row><row><entry/><entry/><entry/><entry/><entry>and Africa</entry></row><row><entry><i>Coquillettidia</i></entry><entry>14</entry><entry>42</entry><entry>West Nile</entry><entry>South America</entry></row><row><entry><i>perturbans</i></entry><entry/><entry/><entry>Virus</entry><entry>and North</entry></row><row><entry/><entry/><entry/><entry/><entry>America,</entry></row><row><entry><i>Culex</i></entry><entry>10</entry><entry>30</entry><entry>West Nile</entry><entry>South America,</entry></row><row><entry><i>nigripalpus</i></entry><entry/><entry/><entry>Virus</entry><entry>North America,</entry></row><row><entry/><entry/><entry/><entry/><entry>and Africa</entry></row><row><entry><i>Mansonia</i></entry><entry>11</entry><entry>33</entry><entry>Venezuelan</entry><entry>South America,</entry></row><row><entry><i>titillans</i></entry><entry/><entry/><entry>Equine</entry><entry>North America,</entry></row><row><entry/><entry/><entry/><entry>Encephalitis</entry><entry>and Africa</entry></row><row><entry/><entry/><entry/><entry>(VEE)</entry></row><row><entry><i>Psorophora</i></entry><entry>11</entry><entry>33</entry><entry>Venezuelan</entry><entry>South America,</entry></row><row><entry><i>columbiae</i></entry><entry/><entry/><entry>Equine</entry><entry>North America,</entry></row><row><entry/><entry/><entry/><entry>Encephalitis</entry><entry>and Africa</entry></row><row><entry/><entry/><entry/><entry>(VEE)</entry></row><row><entry><i>Psorophora</i></entry><entry>11</entry><entry>33</entry><entry>West Nile</entry><entry>South America,</entry></row><row><entry><i>ferox</i></entry><entry/><entry/><entry>Virus</entry><entry>North America,</entry></row><row><entry/><entry/><entry/><entry/><entry>And Africa</entry></row><row><entry namest="1" nameend="5" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0031" num="0029">In one experiment set up, this disclosure explains that the Hillsborough County, Florida area where the methods disclosed herein collected specimens from, there is a dedicated mosquito control board for trapping, collecting, and manually identifying mosquito species. In this county alone, up to 40 species of mosquitoes across numerous genus types are prevalent, not all of them at the same time though. Every week, personnel lay traps for mosquitoes in selected areas, and dead specimens are collected the next day, brought to the lab, and each specimen is visually identified using a microscope, and population results of genus and species are logged. The early collection of specimens is important because, once dead, they decay fast, making visual identification harder if delayed. During a couple of months between Fall 2016 and Spring 2017, those involved in this disclosure participated in multiple such efforts and were given a total of 101 female mosquito specimens from a total of nine different mosquito species, which were the ones most prevalent that time of the year in that county. Each specimen was carefully identified for genus and species and labeled by experts in the board to get the ground truth data.</p><p id="p-0032" num="0030">Table 1 presents details on one example data set. A Samsung Galaxy S5 phone was then used to capture an image of each specimen under the same indoor light conditions, with the camera located one foot above each specimen without flash. Three images of each specimen (<b>100</b>A-<b>100</b>I) were captured in a different phone orientation, on top of one of three backgrounds. In non-limiting examples, this disclosure illustrates using a relatively white background (<b>125</b>), a yellow background (not shown) and a pink background (<b>485</b>). In total, 303 images were captured. <figref idref="DRAWINGS">FIGS. <b>1</b> (<i>a</i>) to (<i>i</i>)</figref> present one representative smart-phone image of each of the nine species (<b>100</b>A-<b>100</b>I) which are classified in this paper, when captured in a relatively white background (<b>125</b>). Features of the smartphone camera used in one non-limiting embodiment, are presented in Table 2. Note that multiple smartphones, computers, cameras, and other equipment that detects and gathers digital information, along with multiple backgrounds, could also be used, and the technique described will not change. All kinds of digital image equipment with corresponding hardware, used to gather specimen images, are within the scope of this disclosure.</p><p id="p-0033" num="0031">a). Utility of Images Captured: Upon seeing the images generated, colleagues at the Mosquito Control Board indicated that they were sufficiently rich for a trained expert to visually identify the species from the images. This motivated researchers to achieve the same via learning techniques, that could be implemented on a smart-phone so that common citizens can do the same.</p><p id="p-0034" num="0032">b). A Note on Gender of Specimens in our Dataset: Note here that all of the 101 mosquito specimens collected for one non-limiting example study were female. Among mosquitoes, only females engage in a blood meal (to provide nutrients for egg production), while males only feed on plant nectar. As such, only female species are disease vectors. In the traps that were laid for the experiments, carbon dioxide (CO<sub>2</sub>) was used as a bait, which is typical. The presence of CO<sub>2 </sub>tricks a female mosquito into believing that there is a blood meal present, and hence gets trapped [20]. Capturing male mosquitoes would have require separate traps with &#x2018;nectar&#x2019; baits, which was beyond the scope of these non-limiting experimental setups. Nevertheless, it is generally true that external morphological characteristics of both males and females for any particular mosquito species are visually similar (with males consistently having a feather like proboscis [18]), and hence proposed techniques herein can be easily adapted to detect genus, species and genders, and is part of future efforts, with more experiments.</p><p id="p-0035" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 2</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Example Experimental Equipment -</entry></row><row><entry>Samsung Galaxy S5 Camera Features</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="21pt" align="left"/><colspec colname="1" colwidth="91pt" align="left"/><colspec colname="2" colwidth="105pt" align="left"/><tbody valign="top"><row><entry/><entry>Camera Details</entry><entry>Specifications</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry>Sensor Resolution</entry><entry>16 MP</entry></row><row><entry/><entry>Aperture size</entry><entry>F2.2</entry></row><row><entry/><entry>Focal length</entry><entry>31 mm</entry></row><row><entry/><entry>Shooting Mode</entry><entry>High Dynamic Range mode</entry></row><row><entry/><entry>Camera Light Source</entry><entry>Daylight</entry></row><row><entry/><entry>Background</entry><entry>White, Yellow &#x26; Pink</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0036" num="0033">This section presents a technical approach to classify mosquito species from smart-phone images. The term &#x201c;smart-phone images&#x201d; is not limiting of the disclosure, as noted above, because all kinds of digital imagery equipment is within the scope of this disclosure. There is a sequence of steps in the approach&#x2014;image resizing, noise removal, background segmentation, feature extraction, dimensionality reduction, unsupervised clustering and classification. The techniques are the same irrespective of phones used or light conditions or backgrounds etc.</p><p id="p-0037" num="0034">In one non-limiting case, a single smart-phone image contains 2988&#xd7;5322 pixels. This is large, and will be computationally prohibitive for the phone during image processing and features extraction, and even more so when there are multiple images. For practicality, in non-limiting embodiments described herein, this disclosure shows resizing each image captured to a size of 256&#xd7;256 pixels. This reduced the image size from around 3 MB to 16 KB, making processing much more practical and fast during model development and also run-time execution, without compromising accuracy.</p><p id="p-0038" num="0035">This disclosure also includes implementing a median filter to reduce noise. Median filter [17] is a nonlinear technique, where each pixel value in a window of size n&#xd7;n pixels is replaced by the median of all pixel values in that window. In one non-limiting embodiment case, the chosen example is n=3. In other filtering techniques like mean filter, pixels are replaced by mean values in a window, and in some cases, the mean value computed is not one that is actually there in the image, resulting in poorer retention of image fidelity, which also compromises edge and color preservation. Median filters avoid this problem, since median values of pixels are computed and retained during noise removal. For insect specimen identification, edge and color preservation are crucial since textural patterns of a mosquito that make up the edges (e.g., legs and wings), and their colors, aid in classification. For example, from <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the photographs show that the legs <b>210</b>A of <i>Aedes aegypti </i>and <b>210</b>D <i>Psorophora columbiae </i>have a combination of black and white color patterns; and the legs <b>210</b>B of <i>Aedes taeniorhynchus </i>and <b>210</b>C of <i>Coquillettidia perturbans </i>have yellowish and black patterns. But the white and black patches <b>225</b>D in the case of <i>Psorophora columbiae </i>are thinner than the patches (<b>225</b>A) of <i>Aedes aegypti</i>. Similar techniques can be used to differentiate the color formations (<b>225</b>B, <b>225</b>C) of the other species. Similarly, from observation of <figref idref="DRAWINGS">FIG. <b>3</b></figref> focusing on species wings (<b>300</b>A, <b>300</b>B, <b>300</b>C, <b>300</b>D), one can see that the wings (<b>300</b>A) of <i>Aedes aegypti </i>are slightly whiter compared to others; the wings (<b>300</b>D) of <i>Psorophora columbiae </i>are slightly blacker than others; and those of <i>Aedes taeniorhynchus </i>and <i>Coquillettidia perturbans </i>(<b>300</b>B, <b>300</b>C) are more brown. There are distinct color/textural patterns even in the scales (<b>325</b>) and shapes of contours (<b>318</b>A, <b>318</b>B, <b>318</b>C, <b>318</b>D) of the wings of various species, hence demonstrating the importance of edge and color preservation, and the importance for median filters to remove noise.</p><p id="p-0039" num="0036">The next step is background segmentation. Researchers anticipate mosquito images to be captured in a variety of backgrounds, so compensating for differing backgrounds is vital. The technical challenge here is automatically segmenting out all of the background information, while retaining only the region of interest (i.e., the mosquito). In one non-limiting technique, this disclosure employs a 2-step process. The first step is to detect the edges (<b>425</b>-<b>431</b>) of the mosquito in the image to find contours (<b>318</b>A, <b>318</b>B, <b>318</b>C, <b>318</b>D). <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref> are best viewed in color that actually encompass a significant part of the image [6]. Following which, the process identifies image portions (<b>460</b>) within the image that need to be categorized as background by comparing images before and after contour detection. To do so, the example embodiments implemented Sobel edge detection algorithm for the segmenting problem, where the algorithm takes the derivative of each pixel intensity (retrieved after converting image to gray scale) with respect to its neighboring pixel [29]. The derivative of the image is discrete as it consists of a 2D array and it is necessary to take it in two directions: x-axis and y-axis. For example, the derivative of any arbitrary pixel in the x-axis will be calculated by taking the difference of pixel intensities between its left and right neighbor. The same applies to compute the derivative in y-axis. Whenever there is edge, there is a prominent change in pixel intensity. This will cause significant change in derivative value. This significant change denotes the presence of an edge (<b>425</b>-<b>431</b>). In order to identify contours (<b>318</b>A-<b>318</b>D), the system needs to know edge intensity and its direction. Direction of the edge, &#x3b8;, is calculated as &#x3b8;=tan&#x2212;1 g<sub>x</sub>/g<sub>y</sub>, where g<sub>x </sub>and g<sub>y </sub>are the derivatives of each pixel intensity in x and y axis while edge intensity is calculated as, Edge_Intensity=&#x221a;g<sup>2</sup><sub>x</sub>+g<sup>2</sup><sub>y</sub>. After retrieving direction and intensity, interim results show many contours enclosed within the edges. The significant contours encompass the largest number of (x,y) coordinates. Then the system compares the locations of each pixel of the significant contours with the locations of pixels in the original image. The pixel intensity at locations defining background pixels which are not in the significant contour are considered as background sections (<b>471</b>). While this may look like it solves a segmenting problem, there is one issue. For those portions of the background that are enclosed within identified edges (e.g., interior background pixels (<b>456</b>, <b>457</b>, <b>458</b>) within mosquito legs)), those are not segmented out, and are considered a part of the mosquito still. Such problems do not exist in regular image processing applications like face detection. However, correcting this issue is accomplished in the next step. Now that certain portions (<b>471</b>) of the background (<b>485</b>) are extracted, the next step is to create a probabilistic model which assumes that the background pixels (<b>485</b>) are generated from a Gaussian mixture [3] [30] [31]. In this step, the embodiments create different Gaussian mixtures for known background pixels (RGB color space background pixels retrieved from the first step). For accurately segmenting the background from the mosquito image, this disclosure introduces a threshold called T. In the set-up, if the probability that the intensity of any pixel belongs to the Gaussian mixture is higher than T, that pixel is considered as background and is segmented out. In case of images with many background portions, only a few of them will be considered as background if T is set too low, while if it is too high, then it will treat portions of the foreground image as background. The example embodiments initialize T with a random number between 0 to 1, and with repeated trial and error, identifies that setting T=0.65 gives the best results.</p><p id="p-0040" num="0037">In the identification methods herein, researchers expect a relatively uniform background, since the smart-phone needs to be close to the mosquito during imaging, and overall focus area is less. As such, these parameter settings are general across backgrounds. Note that, since the distribution of pixels in the background is known a priori, shadows, and other portions of the background enclosed within edges are also removed in this technique. The effectiveness of our proposed 2-step approach in segmenting the background from an <i>Aedes aegypti </i>mosquito image taken in a pink background from our dataset is shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0041" num="0038">The next step in the system is feature extraction. Unfortunately, in one non-limiting implementation, the standard RGB color space did not give good results since the perceptible color differences across species is minimal there. The steps were then executed with the Lab color space [27], that also considers lightness as a factor for determining color, and provides superior color perception [2]. This color space has three dimensions where, L represents lightness, and a and b represent the color opponents ranging from green-red and blue-yellow.</p><p id="p-0042" num="0039">In order to extract features after transforming images to Lab color space, the experiments focused on textures. Recall from <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref> the importance of textures (patterns of scales <b>325</b> and colors in legs <b>225</b> and wings) in aiding species identification. Furthermore, textural patterns do not change much as the mosquito grows, and interacts with nature in the wild. Essentially, in texture analysis, one derives the dependency of intensity or variance across pixels in the image. This can be done in two ways. One is structural that captures dependencies among neighboring pixels, that enables superior perception of textures as primitives (spots, edges, curves and edge ends). The other is statistical, that computes local features by analyzing the spatial distribution of gray values of an image [16].</p><p id="p-0043" num="0040">Local Binary Pattern [12] procedures, as shown at <b>510</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, are popular approaches that extract a combination of structural and statistical properties of an image. In this technique, textures are extracted on the basis of local patterns formed by each pixel. To do so, each pixel is labeled by thresholding the 3&#xd7;3 neighborhood (<b>512</b>) of each pixel with the center pixel value (<b>511</b>). In other words, for each pixel of an image, the steps herein compare the pixel value of their 8 neighbors either clockwise (shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>) or counter-clockwise, a different option. As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> at <b>515</b>, if the neighbor pixel value is greater than center's pixel value, the procedure replaces it with 1, otherwise with 0. This will give 8 binary digits, which are converted to decimal values <b>518</b>, which will replace the value in the center pixel <b>511</b>. The process repeats for all pixels in the image. The range of decimal values lies from 0 to 255. In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, this disclosure shows a representative instance of determining Local Binary Patterns.</p><p id="p-0044" num="0041">The experimental analysis also comprises deriving a histogram with 26 bins for the number of decimal values in each pixel in the range of 0 to 9; 10 to 19 and so on, up to 250 to 255. The number of values in each of the 26 bins is a feature. Essentially, when the number of bins with non-zero entries is less, it indicates fewer textural patterns, and when it is more, it is an indicator of more textural patterns.</p><p id="p-0045" num="0042">While Local Binary Patterns do yield structural and statistical information on local textures, they cannot capture spatial dependencies among textures, which contrast mosquito species (e.g., alternating black and white patches in legs, variations in thickness of patches etc.). To capture these on a global scale, the system derives Haralick textural features, which employ higher order statistics to capture neighborhood properties of textures.</p><p id="p-0046" num="0043">The basis of Haralick features [15] is a gray-level co-occurrence matrix, where gray-level indicates the intensity of a pixel in two dimensions. At the start, a square matrix of dimensions G=N<sub>g</sub>&#xd7;N<sub>g </sub>is constructed, where N<sub>g </sub>denotes the number of gray levels in an image. An Element [i,j] in the matrix is generated by counting the number of times a pixel with value i is adjacent to a pixel with value j, and then dividing the entire matrix by the total number of such comparisons made. Each entry in the matrix is thus the probability that a pixel with value i will be found adjacent to a pixel of value j. Subsequently, using the pixel intensity dependencies identified in Matrix G, the system computes 13 Haralick features to capture spatial dependencies across textural patterns in the image. Table 3 presents these features, and how to compute them from the Matrix G below, where p(i, j) is defined as the probability that a pixel with value i will be found adjacent to a pixel of value j.</p><p id="p-0047" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mi>G</mi>  <mo>=</mo>  <mrow>   <mrow>    <mo>[</mo>    <mtable>     <mtr>      <mtd>       <mrow>        <mi>p</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mn>1</mn>         <mo>,</mo>         <mn>1</mn>        </mrow>        <mo>)</mo>       </mrow>      </mtd>      <mtd>       <mrow>        <mi>p</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mn>1</mn>         <mo>,</mo>         <mn>2</mn>        </mrow>        <mo>)</mo>       </mrow>      </mtd>      <mtd>       <mrow>        <mi>p</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mn>1</mn>         <mo>,</mo>         <mn>3</mn>        </mrow>        <mo>)</mo>       </mrow>      </mtd>      <mtd>       <mo>&#x2026;</mo>      </mtd>      <mtd>       <mrow>        <mi>p</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mn>1</mn>         <mo>,</mo>         <msub>          <mi>N</mi>          <mi>g</mi>         </msub>        </mrow>        <mo>)</mo>       </mrow>      </mtd>     </mtr>     <mtr>      <mtd>       <mrow>        <mi>p</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mn>2</mn>         <mo>,</mo>         <mn>1</mn>        </mrow>        <mo>)</mo>       </mrow>      </mtd>      <mtd>       <mrow>        <mi>p</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mn>2</mn>         <mo>,</mo>         <mn>2</mn>        </mrow>        <mo>)</mo>       </mrow>      </mtd>      <mtd>       <mrow>        <mi>p</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mn>2</mn>         <mo>,</mo>         <mn>3</mn>        </mrow>        <mo>)</mo>       </mrow>      </mtd>      <mtd>       <mo>&#x2026;</mo>      </mtd>      <mtd>       <mrow>        <mi>p</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mn>2</mn>         <mo>,</mo>         <msub>          <mi>N</mi>          <mi>g</mi>         </msub>        </mrow>        <mo>)</mo>       </mrow>      </mtd>     </mtr>     <mtr>      <mtd>       <mo>&#x22ee;</mo>      </mtd>      <mtd>       <mo>&#x22ee;</mo>      </mtd>      <mtd>       <mo>&#x22ee;</mo>      </mtd>      <mtd>       <mo>&#x22f1;</mo>      </mtd>      <mtd>       <mo>&#x22ee;</mo>      </mtd>     </mtr>     <mtr>      <mtd>       <mrow>        <mi>p</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <msub>          <mi>N</mi>          <mi>g</mi>         </msub>         <mo>,</mo>         <mn>1</mn>        </mrow>        <mo>)</mo>       </mrow>      </mtd>      <mtd>       <mrow>        <mi>p</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <msub>          <mi>N</mi>          <mi>g</mi>         </msub>         <mo>,</mo>         <mn>2</mn>        </mrow>        <mo>)</mo>       </mrow>      </mtd>      <mtd>       <mrow>        <mi>p</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <msub>          <mi>N</mi>          <mi>g</mi>         </msub>         <mo>,</mo>         <mn>3</mn>        </mrow>        <mo>)</mo>       </mrow>      </mtd>      <mtd>       <mo>&#x2026;</mo>      </mtd>      <mtd>       <mrow>        <mi>p</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <msub>          <mi>N</mi>          <mi>g</mi>         </msub>         <mo>,</mo>         <msub>          <mi>N</mi>          <mi>g</mi>         </msub>        </mrow>        <mo>)</mo>       </mrow>      </mtd>     </mtr>    </mtable>    <mo>]</mo>   </mrow>   <mo>.</mo>  </mrow> </mrow></math></maths></p><p id="p-0048" num="0044">Recall now that the results above have extracted 39 features from each mosquito image: 26 LBP and 13 Haralick Features. To make the solution computationally efficient, one non-limiting procedure employed Linear Discriminant analysis [21] for dimensionality reduction, where the aim is to find a linear combination of the 39 features by projecting them into a lower dimensional sub-space to avoid computational cost and over fitting, while the identified subspace maintains class variability and reduced correlation among features. To do so, let us assume, that there are K classes and each having mean &#x3bc;<sub>i</sub>, and covariance &#x3a3;, where i=1, 2, 3, . . . K. Then, the scatter between class variability is defined using sample covariance of the class means as:</p><p id="p-0049" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <munder>       <mo>&#x2211;</mo>       <mi>b</mi>      </munder>      <mrow>       <mo>=</mo>       <mrow>        <mfrac>         <mn>1</mn>         <mi>K</mi>        </mfrac>        <mo>&#x2062;</mo>        <mrow>         <munderover>          <mo>&#x2211;</mo>          <mrow>           <mi>i</mi>           <mo>=</mo>           <mn>1</mn>          </mrow>          <mi>K</mi>         </munderover>         <mrow>          <mrow>           <mo>(</mo>           <mrow>            <msub>             <mi>&#x3bc;</mi>             <mi>i</mi>            </msub>            <mo>-</mo>            <mi>&#x3bc;</mi>           </mrow>           <mo>)</mo>          </mrow>          <mo>&#x2062;</mo>          <msup>           <mrow>            <mo>(</mo>            <mrow>             <msub>              <mi>&#x3bc;</mi>              <mi>i</mi>             </msub>             <mo>-</mo>             <mi>&#x3bc;</mi>            </mrow>            <mo>)</mo>           </mrow>           <mi>T</mi>          </msup>         </mrow>        </mrow>       </mrow>      </mrow>     </mrow>     <mo>,</mo>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>1</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0050" num="0000">where &#x3bc; is the mean of the all class means. The separation of class in a direction {right arrow over (w)}, which is an eigenvector of</p><p id="p-0051" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mrow>  <mrow>   <mover>    <mo>&#x2211;</mo>    <mrow>     <mo>-</mo>     <mn>1</mn>    </mrow>   </mover>   <mtext> </mtext>   <munder>    <mo>&#x2211;</mo>    <mi>b</mi>   </munder>  </mrow>  <mtext>  </mtext>  <mo>,</mo>  <mo>,</mo> </mrow></math></maths></p><p id="p-0052" num="0000">is computed as,</p><p id="p-0053" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>S</mi>     <mo>=</mo>     <mrow>      <mfrac>       <mrow>        <msup>         <mover>          <mi>w</mi>          <mo>&#x2192;</mo>         </mover>         <mi>T</mi>        </msup>        <mo>&#x2062;</mo>        <mrow>         <msub>          <mo>&#x2211;</mo>          <mi>b</mi>         </msub>         <mover>          <mi>w</mi>          <mo>&#x2192;</mo>         </mover>        </mrow>       </mrow>       <mrow>        <msup>         <mover>          <mi>w</mi>          <mo>&#x2192;</mo>         </mover>         <mi>T</mi>        </msup>        <mo>&#x2062;</mo>        <mrow>         <mo>&#x2211;</mo>         <mover>          <mi>w</mi>          <mo>&#x2192;</mo>         </mover>        </mrow>       </mrow>      </mfrac>      <mo>.</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><heading id="h-0007" level="2">If</heading><p id="p-0054" num="0045"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mrow>  <mrow>   <mover>    <mo>&#x2211;</mo>    <mrow>     <mo>-</mo>     <mn>1</mn>    </mrow>   </mover>   <mtext> </mtext>   <munder>    <mo>&#x2211;</mo>    <mi>b</mi>   </munder>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0055" num="0000">is diagonalizable, the variability between features will be contained in the subspace spanned by the eigenvectors corresponding to the K&#x2212;1 largest eigenvalues (since</p><p id="p-0056" num="0000"><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <munder>  <mo>&#x2211;</mo>  <mi>b</mi> </munder></math></maths></p><p id="p-0057" num="0000">is of rank K&#x2212;1 at most). These K&#x2212;1 values will be the features for classification. In certain cases, since the non-limiting experiments have nine classes of mosquito species, eight final features are returned after LDA, that will be used for model development.</p><p id="p-0058" num="0046">The first attempt to classify mosquito species is to investigate the efficacy of the eight features extracted as above, by checking to see if an unsupervised learning algorithm can by itself cluster image samples. To do so, work in this disclosure included designing as Expectation-Maximization (EM) algorithm [7] for clustering unlabeled mosquito images, where the idea is to estimate the Maximum Likelihood (ML) parameters from the observed samples. Assuming that each image is sampled from a mixture of Gaussian distributions, the EM algorithm attempts to find the model parameters of each Gaussian distribution from which the sample most likely is observed, while increasing the likelihood of the parameters in each iteration. It comprises of two steps in <figref idref="DRAWINGS">FIG. <b>6</b></figref>: Three Clusters Identified after EM Clustering each iteration. In the expectation, or E-step, model parameters are estimated based on observed samples. This is achieved using the conditional expectation. In the M-step, the likelihood function of model parameters is maximized under assumption that the observed sample is sampled from the estimated parameter. The iteration goes until convergence. Convergence is guaranteed since the algorithm is bound to increase the likelihood function at each iteration. With this clustering technique, the system illustrated very good performance when the number of clusters selected were 3, and with top 2 LDA features having highest variance. <figref idref="DRAWINGS">FIG. <b>6</b></figref> presents results, where all samples belonging to <i>Aedes aegypti </i>and <i>Psorophora columbiae </i>were each clustered separately using just 2 features. This is a very interesting result from unsupervised clustering that justifies our selection of features as representative. However, all samples in 7 other species were clustered separately. These species are identified in Table 4.</p><p id="p-0059" num="0047">With two of the three species already identified via clustering, the experiment described herein presents the final step of classifying the remaining 7 species. To do so, researchers use Support Vector Machines [9], which is an established supervised classification and regression machine learning algorithm, and requires minimal overhead to train and test. It gives fast and high performance with very little tuning of parameters. The main aim in SVM is to maximize the margin between classes to be identified by determining training instances that are called as support vectors which are used to define class boundaries. The middle of the margin is the optimal separating hyperplane between two classes. While testing, users of the computerized system and method described herein calculate the probability of each sample belonging to particular species and output the one that has highest probability.</p><p id="p-0060" num="0048">Recall that, in one non-limiting embodiment, the apparatus, system and method of this disclosure are taking three smart-phone images of each mosquito specimen in different orientations. As such, three images will be given for classification in each instance. Since the number of species to be identified is only seven (after Clustering), for features from these samples alone, the steps include reapplying LDA to identify six features for classification. When implementing the SVM algorithm for this set (of 3 images each per specimen to be identified), the procedures compute the average probabilities of each species as identified from the SVM algorithm for each of the 3 images, and output the one with the highest average probability among all species classified.</p><p id="p-0061" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 3</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Formulas for Haralick's 13 features</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="49pt" align="left"/><colspec colname="2" colwidth="168pt" align="left"/><tbody valign="top"><row><entry>Features</entry><entry>Formula</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row><row><entry>Angular</entry><entry>&#x3a3;i &#x3a3;j p(i, j)<sup>2</sup>, where p(i, j) is defined as the probability</entry></row><row><entry>Second</entry><entry>that a pixel with value i will be found adjacent to a pixel</entry></row><row><entry>Moment</entry><entry>of value j</entry></row><row><entry>Contrast</entry><entry>&#x3a3;<sub>n=0</sub><sup>Ng&#x2212;1 </sup>n<sup>2 </sup>{&#x3a3;<sub>i=1</sub><sup>Ng </sup>&#x3a3;<sub>j=1</sub><sup>Ng </sup>p(i, j)}, | i &#x2212; j | = n</entry></row><row><entry>Correlation</entry><entry>(&#x3a3;i &#x3a3;j (i, j)p(i, j) &#x2212; u<sub>x</sub>u<sub>y</sub>) &#xf7; &#x3c3;<sub>x </sub>&#x3c3;<sub>y</sub>, where x and y are</entry></row><row><entry/><entry>the row and column of an entry in co-occurrence matrix</entry></row><row><entry/><entry>G, and u<sub>x</sub>, u<sub>y</sub>, &#x3c3;<sub>x</sub>, &#x3c3;<sub>y </sub>are the means and standard</entry></row><row><entry/><entry>deviations of px, py which is partial probability density</entry></row><row><entry/><entry>functions of pixel x and y respectively</entry></row><row><entry>Sum of</entry><entry>&#x3a3;i &#x3a3;j (i &#x2212; &#x3bc;)<sup>2 </sup>p(i, j)</entry></row><row><entry>Squares:</entry></row><row><entry>Variance</entry></row><row><entry>Inverse</entry><entry>&#x3a3;i &#x3a3;j (1 &#xf7; (1 + (i &#x2212; j)<sup>2</sup>)) &#xd7; p(i, j)</entry></row><row><entry>Difference</entry></row><row><entry>Moment</entry></row><row><entry>Sum Average</entry><entry>&#x3a3;<sub>i=2</sub><sup>2Ng </sup>(p<sub>x+y </sub>(i)) where p<sub>x+y </sub>(i) is the probability of</entry></row><row><entry/><entry>co-occurrence matrix coordinates summing to x + y</entry></row><row><entry>Sum Entropy</entry><entry>&#x3a3;<sub>i=2</sub><sup>2Ng </sup>p<sub>x+y </sub>(i) log{p<sub>x+y </sub>(i)} = fs</entry></row><row><entry>Sum Variance</entry><entry>&#x3a3;<sub>i=2</sub><sup>2Ng </sup>(i &#x2212; fs)<sup>2 </sup>p<sub>x+y </sub>(i)</entry></row><row><entry>Entropy</entry><entry>&#x2212;&#x3a3;i &#x3a3;j p(i, j)log(p(i, j))</entry></row><row><entry>Difference</entry><entry>&#x3a3;<sub>i=0</sub><sup>Ng&#x2212;1 </sup>i<sup>2 </sup>p<sub>x&#x2212;y </sub>(i)</entry></row><row><entry>Variance</entry></row><row><entry>Difference</entry><entry>&#x3a3;<sub>i=0</sub><sup>Ng&#x2212;1 </sup>p<sub>x&#x2212;y</sub>(i)log{p<sub>x&#x2212;y</sub>(i)}</entry></row><row><entry>Entropy</entry></row><row><entry>Information</entry><entry>(HXY &#x2212; HXY1) &#xf7; max {HX, HY}, where HXY = &#x2212;&#x3a3;i</entry></row><row><entry>Measure of</entry><entry>&#x3a3;j p(i, j), HX, HY are the entropies of px, py,</entry></row><row><entry>Correlation 1</entry><entry>HXY1 = &#x2212;&#x3a3;i &#x3a3;j p(i, j) log{p<sub>x</sub>(i)p<sub>y</sub>(j)}</entry></row><row><entry>Information</entry><entry>(1 &#x2212; exp[&#x2212;2(HXY2 &#x2212; HXY])<sup>1/2</sup>, where HXY2 =</entry></row><row><entry>Measure of</entry><entry>&#x3a3;i Ej p<sub>y</sub>(j) log{p<sub>x</sub>(i)p<sub>y</sub>(j)}</entry></row><row><entry>Correlation 2</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0062" num="0000"><tables id="TABLE-US-00004" num="00004"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 4</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Cluster Results</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="77pt" align="center"/><colspec colname="2" colwidth="140pt" align="left"/><tbody valign="top"><row><entry>Cluster</entry><entry>Species</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row><row><entry>1</entry><entry><i>Aedes infirmatus, Aedes</i></entry></row><row><entry/><entry><i>taeniorhynchus, Anopheles crucians,</i></entry></row><row><entry/><entry><i>Coquillettidia perturbans, Culex</i></entry></row><row><entry/><entry><i>nigripalpus, Mansonia titillans, </i>and</entry></row><row><entry/><entry><i>Psorophora ferox</i></entry></row><row><entry>2</entry><entry><i>Psorophora columbiae</i></entry></row><row><entry>3</entry><entry><i>Aedes aegypti</i></entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0063" num="0049">a). Overview of Evaluation Methods: Recall that for two species, namely <i>Aedes aegypti </i>and <i>Psorophora columbiae</i>, the classification accuracy was 100% with Clustering alone. For the other seven species, the techniques described herein evaluate the ability of our SVM algorithm for classification under 10-fold cross validation technique, which is standard for our problem scope.</p><p id="p-0064" num="0050">b). Results and Interpretations: <figref idref="DRAWINGS">FIG. <b>7</b></figref> presents results in terms of Precision, Recall and F1-Measure for seven species, wherein for each specimen, the average classification probability for all 3 images of that specimen are computed, and the highest one is returned. The accuracy in this case for these seven species is 71.07%. Combined with 100% accuracy for two other species, the overall accuracy of the system for all nine species is 77.5%. In another non-limiting embodiment, the system attempts to output two species which have the top two highest classification probabilities from SVM, instead of only the top most (as shown above in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). In other words, one way to evaluate accuracy of the system is if the actual species is among the top two species outputted from the algorithm. <figref idref="DRAWINGS">FIG. <b>8</b></figref> presents results, and the accuracy naturally improves to 87.15% for the 7 species, resulting in an overall accuracy for nine species as 90.03%.</p><p id="p-0065" num="0051">Interestingly, by aiming to identify each image of each specimen separately (without considering them as part of a set), the accuracy is only 47.16%. This result reveals the importance of capturing images in multiple orientations for enhanced accuracy to identify mosquito species. This procedure is quite practical for implementation as a computerized application, where citizens engage in the imaging/species identification process. In fact, for visual identification under a microscope, usually one orientation is not sufficient, and multiple orientations are needed for species identification even for experts.</p><p id="p-0066" num="0052">c). Complexity of Execution: In one non-limiting embodiment, training the expectation&#x2014;maximization (&#x201c;EM&#x201d;) clustering and support vector machine (SVM) classification model has been implemented on a machine with Intel Core i7 CPU @2.6 GHz with 16 GB RAM configuration. Training the model took less than a few minutes in this example implementation, provided here for experimental disclosure. The entire process of classification (image preprocessing, feature extraction, LDA, Clustering and Classification algorithm) has been implemented as an application on a Samsung Galaxy S5 Smart-phone. The average time it took to classify a species was less than 2 seconds, with negligible energy consumption. Total memory consumed by the application in the phone was 23 MB.</p><p id="p-0067" num="0053">d). Difficulties in Designing Deep and Transfer Learning Techniques to Identify Mosquito Species: We understand that deep-learning is state-of-art in object recognition. However, for effective model development using deep learning, tens of thousands of images are needed, since deep learning enables automatic feature extraction from the dataset. Generating 303 images in this paper was itself a challenge. Generating tens of thousands of mosquito images requires much more resources. Data Augmentation in one approach to create larger datasets via flipping, blurring, zooming and rotating images [25]. But this was not effective for us, because these are regularization techniques that have applicability when images classes are more diverse. But since there is minimal diversity in the physical appearance (and hence images) among various species of mosquitoes, this approach will likely introduce more noise, resulting in poorer accuracies. Our attempt in generating a dataset of 2000 mosquito images from the original 303, using augmentation, followed by species classification yielded an accuracy of only 55%. Enhancing our dataset size using open source images (e.g., Google Images) are not possible because there were not enough images tagged with the name of species, and even then we cannot guarantee that they were correctly tagged.</p><p id="p-0068" num="0054">Another more recent technique is Transfer Learning, where the idea is to extend an existing model already trained to identify certain classes, in order to identify newer classes. Unfortunately, even the most popular VGGNet model [28] trained to recognize 1000 classes of images using the ImageNet database [11] fetched us only 47% accuracy. Primarily, no class among the 1000 in ImageNet were even remotely representative of mosquitoes, hence explaining low accuracy in species classification using Transfer Learning.</p><p id="p-0069" num="0055">The embodiments of this disclosure show a system that allows any citizen to take image(s) of a still mosquito that is either alive or dead (via spraying or trapping), but still retaining its physical form, and subsequently processes the image(s) to identify the species type in real time.</p><p id="p-0070" num="0056">a). Practical Impact: At peak times, hundreds of requests come daily from people complaining of mosquitoes in their neighborhoods. Deciding where to divert resources for trap laying and spraying is a constant problem for public health workers. In fact, in Florida, during the Zika Virus scare in 2016, the lack of information about species type during calls from concerned citizens was a huge problem for public health workers we spoke to. With knowledge on species type and density, reported by citizens themselves using our system, urgent needs can be better prioritized. Furthermore, with a system like ours in place available at mosquito control facilities, the process of species identification and logging is much faster. Expertise of public health workers can hence be shifted from the cognitively demanding task of species identification via a microscope, to more useful tasks in combating mosquitoes spread.</p><p id="p-0071" num="0057">b). Future Work: We are now generating images of more mosquito specimens (male and female) in the Hillsborough County. With more species and specimens, and using more smart-phones for imaging, we hope to demonstrate superior validity of our system. The process of data collection though is very laborious, requiring months of laying traps, and tagging/imaging specimens. We are now working with public health experts to design a user-friendly smart-phone app that citizens can use for imaging, classification and reporting of mosquitoes. After testing, we will release it for public use in the Hillsborough county, and evaluate it. Images collected and tagged in this manner will also be publicly shared. Expanding our results to beyond Florida, and possibly beyond the US is also on our agenda, but is very challenging&#x2014;technically and logistically.</p><p id="p-0072" num="0058">As shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the systems and methods described herein may be implanted on commonly used computer hardware that is readily accessible by the general public. The computer <b>200</b> includes a processing unit <b>202</b> (&#x201c;CPU&#x201d;), a system memory <b>204</b>, and a system bus <b>206</b> that couples the memory <b>204</b> to the CPU <b>202</b>. The computer <b>200</b> further includes a mass storage device <b>212</b> for storing program modules. The program modules may be operable to perform associated with embodiments illustrated in one or more of <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>8</b></figref> discussed herein. The program modules may include an imaging application for causing a system to perform data acquisition, and/or for performing processing functions as described herein, for example to acquire and/or process image data corresponding to imaging of a region of interest (ROI). The computer <b>200</b> can include a data store for storing data that may include imaging-related data such as acquired data from the implementation in accordance with various embodiments of the present disclosure.</p><p id="p-0073" num="0059">The mass storage device is connected to the CPU <b>202</b> through a mass storage controller (not shown) connected to the bus <b>206</b>. The mass storage device and its associated computer-storage media provide non-volatile storage for the computer <b>200</b>. Although the description of computer-storage media contained herein refers to a mass storage device, such as a hard disk or CD-ROM drive, it should be appreciated by those skilled in the art that computer-storage media can be any available computer storage media that can be accessed by the computer <b>200</b>. &#x201c;Computer storage media&#x201d;, &#x201c;computer-readable storage medium&#x201d; or &#x201c;computer-readable storage media&#x201d; as described herein do not include transitory signals.</p><p id="p-0074" num="0060">According to various embodiments, the computer <b>200</b> may operate in a networked environment using connections to other local or remote computers through a network via a network interface unit <b>210</b> connected to the bus <b>206</b>. The network interface unit <b>210</b> may facilitate connection of the computing device inputs and outputs to one or more suitable networks and/or connections such as a local area network (LAN), a wide area network (WAN), the Internet, a cellular network, a radio frequency (RF) network, a Bluetooth-enabled network, a Wi-Fi enabled network, a satellite-based network, or other wired and/or wireless networks for communication with external devices and/or systems. The computer <b>200</b> may also include an input/output controller <b>208</b>A, <b>208</b>B for receiving and processing input from any of a number of input devices. Input devices may include one or more of keyboards, mice, stylus, touchscreens, microphones, audio capturing devices, and image/video capturing devices. An end user may utilize the input devices to interact with a user interface, for example a graphical user interface, for managing various functions performed by the computer <b>200</b>. The bus <b>206</b> may enable the processing unit <b>202</b> to read code and/or data to/from the mass storage device or other computer-storage media.</p><p id="p-0075" num="0061">Using the computerized technology described above, non-limiting experimental models have been developed and based on 20,000+ images of 19 vector species in an example location, e.g., Tampa, Fla. With more data from other geographies, this disclosure can be used to expand the database of images and feature classification for species identification. For future classification, the proposed technique will isolate key features of a mosquito's morphology&#x2014;wings, legs, abdomen, proboscis, and then use anatomically inspired deep learning techniques for species classification.</p><p id="p-0076" num="0062">Additional details of the disclosure are set forth in the claims following citations to the following references used in this work.</p><heading id="h-0008" level="1">REFERENCES</heading><p id="p-0077" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0063">[1] 2018. CDC&#x2014;Malaria&#x2014;About Malaria&#x2014;Biology&#x2014;Mosquitoes&#x2014;<i>Anopheles </i>Mosquitoes. https://www.cdc.gov/malaria/about/biology/mosquitoes/. (Accessed on Mar. 2, 2018).</li>    <li id="ul0001-0002" num="0064">[2] 2018. Explanation of the LAB Color Space. https://www.aces.edu/dept/fisheries/education/pond to plate/documents/ExplanationoftheLABColorSpace.pdf:. (Accessed on Feb. 24, 2018).</li>    <li id="ul0001-0003" num="0065">[3] 2018. Image Derivative &#x22c5; Chris McCormick. http://mccormickml.com/2013/02/26/image-derivative/. (Accessed on Mar. 1, 2018).</li>    <li id="ul0001-0004" num="0066">[4] 2018. Project Premonition&#x2014;Microsoft Research. https://www.microsoft.com/en-us/research/project/project-premonition/. (Accessed on Feb. 23, 2018).</li>    <li id="ul0001-0005" num="0067">[5] Ahmedelmubarak Bashir, Zeinab A Mustafa, Islah Abdelhameid, and Rimaz Ibrahem. 2017. Detection of malaria parasites using digital image processing. In Communication, Control, Computing and Electronics Engineering (ICCCCEE), 2017 International Conference on. IEEE, 1-5.</li>    <li id="ul0001-0006" num="0068">[6] D Baswaraj, A Govardhan, and P Premchand. 2012. Active contours and image segmentation: The current state of the art. Global Journal of Computer Science and Technology (2012).</li>    <li id="ul0001-0007" num="0069">[7] Sean Borman. 2004. The expectation maximization algorithm&#x2014;a short tutorial. Submitted for publication (2004), 1-9.</li>    <li id="ul0001-0008" num="0070">[8] Yanping Chen, Adena Why, Gustavo Batista, Agenor Mafra-Neto, and Eamonn Keogh. 2014. Flying insect detection and classification with inexpensive sensors. Journal of visualized experiments: JoVE 92 (2014).</li>    <li id="ul0001-0009" num="0071">[9] Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine learning 20, 3 (1995), 273-297.</li>    <li id="ul0001-0010" num="0072">[10] Anna MonicaMDe Los Reyes, Anna Camille A Reyes, Jumelyn L Torres, Dionis A Padilla, and Jocelyn Villaverde. 2016. Detection of <i>Aedes Aegypti </i>mosquito by digital image processing techniques and support vector machine. In Region 10 Conference (TENCON), 2016 IEEE. IEEE, 2342-2345.</li>    <li id="ul0001-0011" num="0073">[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 248-255.</li>    <li id="ul0001-0012" num="0074">[12] Anita Dixit and Nagaratna P Hegde. 2013. Image texture analysis-survey. In Advanced Computing and Communication Technologies (ACCT), 2013 Third International Conference on. IEEE, 69-76.</li>    <li id="ul0001-0013" num="0075">[13] Colin Favret and Jeffrey M Sieracki. 2016. Machine vision automated species identification scaled towards production levels. Systematic Entomology 41, 1 (2016), 133-143.</li>    <li id="ul0001-0014" num="0076">[14] Masataka Fuchida, Thejus Pathmakumar, Rajesh Elara Mohan, Ning Tan, and Akio Nakamura. 2017. Vision-based perception and classification of mosquitoes using support vector machine. Applied Sciences 7, 1 (2017), 51.</li>    <li id="ul0001-0015" num="0077">[15] Robert M Haralick, Karthikeyan Shanmugam, et al. 1973. Textural features for image classification. IEEE Transactions on systems, man, and cybernetics 6 (1973), 610-621.</li>    <li id="ul0001-0016" num="0078">[16] P S Hiremath and Rohini A Bhusnurmath. 2013. Texture Image Classification Using Nonsubsampled Contourlet Transform and Local Directional Binary Patterns. International Journal 3, 7 (2013).</li>    <li id="ul0001-0017" num="0079">[17] T Huang, GJTGY Yang, and G Tang. 1979. A fast two-dimensional median filtering algorithm. IEEE Transactions on Acoustics, Speech, and Signal Processing 27, 1 (1979), 13-18.</li>    <li id="ul0001-0018" num="0080">[18] PG Jupp. 1996. Mosquitoes of Southern Africa: Culicinae and Toxorhynchitinae. Hartebeespoort.</li>    <li id="ul0001-0019" num="0081">[19] Maxime Martineau, Donatello Conte, Romain Raveaux, Ingrid Arnault, Damien Munier, and Gilles Venturini. 2017. A survey on image-based insect classification. Pattern Recognition 65 (2017), 273-284.</li>    <li id="ul0001-0020" num="0082">[20] Conor J McMeniman, Roman A Corfas, Benjamin J Matthews, Scott A Ritchie, and Leslie B Vosshall. 2014. Multimodal integration of carbon dioxide and other sensory cues drives mosquito attraction to humans. Cell 156, 5 (2014), 1060-1071.</li>    <li id="ul0001-0021" num="0083">[21] Sebastian Mika, Gunnar Ratsch, Jason Weston, Bernhard Scholkopf, and Klaus-Robert Mullers. 1999. Fisher discriminant analysis with kernels. In Neural Networks for Signal Processing IX, 1999. Proceedings of the 1999 IEEE Signal Processing Society Workshop. IEEE, 41-48.</li>    <li id="ul0001-0022" num="0084">[22] Mona Minakshi, Pratool Bharti, and Sriram Chellappan. 2017. Identifying mosquito species using smart-phone cameras. In Networks and Communications (EuCNC), 2017 European Conference on. IEEE, 1-6.</li>    <li id="ul0001-0023" num="0085">[23] Haripriya Mukundarajan, Felix J H Hol, Erica A Castillo, Cooper Newby, and Manu Prakash. 2016. Using Mobile Phones as Acoustic Sensors for the Surveillance of Spatio-temporal Mosquito Ecology. (2016).</li>    <li id="ul0001-0024" num="0086">[24] J. P. Munoz, R. Boger, S. Dexter, R. Low, and J. Li. 2018. Image Recognition of Disease-Carrying Insects: A System for Combating Infectious Diseases Using Image Classification Techniques and Citizen Science. (2018).</li>    <li id="ul0001-0025" num="0087">[25] Luis Perez and Jason Wang. 2017. The effectiveness of data augmentation in image classification using deep learning. arXiv preprint arXiv:1712.04621 (2017).</li>    <li id="ul0001-0026" num="0088">[26] Antonio Rodr&#xed;guez Garc&#xed;a, Frederic Bartumeus, and Ricard Gavald&#xe0; Mestre. 2016. Machine learning assists the classification of reports by citizens on diseasecarrying mosquitoes. In SoGood 2016: Data Science for Social Good: Proceedings of the First Workshop on Data Science for Social Good co-located with European Conference on Machine Learning and Principles and Practice of Knowledge Dicovery in Databases (ECML-PKDD 2016): Riva del Garda, Italy, Sep. 19, 2016. CEURWS. org, 1-11.</li>    <li id="ul0001-0027" num="0089">[27] Michael W Schwarz, William B Cowan, and John C Beatty. 1987. An experimental comparison of RGB, YIQ, LAB, HSV, and opponent color models. ACM Transactions on Graphics (TOG) 6, 2 (1987), 123-158.</li>    <li id="ul0001-0028" num="0090">[28] Karen Simonyan and AndrewZisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).</li>    <li id="ul0001-0029" num="0091">[29] Irwin Sobel. 2014. An Isotropic 3&#xd7;3 Image Gradient Operator. (February 2014).</li>    <li id="ul0001-0030" num="0092">[30] Chris Stauffer andWEric L Grimson. 1999. Adaptive background mixture models for real-time tracking. In Computer Vision and Pattern Recognition, 1999. IEEE Computer Society Conference on., Vol. 2. IEEE, 246-252.</li>    <li id="ul0001-0031" num="0093">[31] Brandyn White and Mubarak Shah. 2007. Automatically tuning background subtraction parameters using particle swarm optimization. In Multimedia and Expo, 2007 IEEE International Conference on. IEEE, 1826-1829.</li>    <li id="ul0001-0032" num="0094">[32] Andr&#xe9; Barretto Bruno Wilke, Rafael de Oliveira Christe, Laura Cristina Multini, Paloma Oliveira Vidal, Ramon Wilk-da Silva, Gabriela Cristina de Carvalho, and Mauro Toledo Marrelli. 2016. Morphometric wing characters as a tool for mosquito identification. PloS one 11, 8 (2016), e0161643.</li>    <li id="ul0001-0033" num="0095">[33] Mona Minakshi, Pratool Bharti, Sriram Chellappan. 2018. Leveraging Smart-Phone Cameras and Image Processing Techniques to Classify Mosquito Species. MobiQuitous '18 Proceedings of the 15th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services, Pages 77-86, Nov. 5-7, 2018.</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230004756A1-20230105-M00001.NB"><img id="EMI-M00001" he="13.04mm" wi="76.20mm" file="US20230004756A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230004756A1-20230105-M00002.NB"><img id="EMI-M00002" he="8.13mm" wi="76.20mm" file="US20230004756A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230004756A1-20230105-M00003.NB"><img id="EMI-M00003" he="8.13mm" wi="76.20mm" file="US20230004756A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230004756A1-20230105-M00004.NB"><img id="EMI-M00004" he="9.14mm" wi="76.20mm" file="US20230004756A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230004756A1-20230105-M00005.NB"><img id="EMI-M00005" he="8.13mm" wi="76.20mm" file="US20230004756A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006" nb-file="US20230004756A1-20230105-M00006.NB"><img id="EMI-M00006" he="6.01mm" wi="76.20mm" file="US20230004756A1-20230105-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer implemented method of identifying an insect specimen with software storing instructions implementing computerized steps, comprising:<claim-text>gathering a plurality of digital images of multiple insect specimens, wherein the digital images comprise a plurality of image orientations relative to a common axis;</claim-text><claim-text>segmenting respective digital images into body pixels and image background pixels by detecting edges of the body pixels;</claim-text><claim-text>extracting features of the insect specimens from the respective digital images by calculating variance of pixel intensity across the respective digital images;</claim-text><claim-text>clustering the digital images according to the features;</claim-text><claim-text>classifying the insect specimens by species according to respective clusters.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computerized method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising resizing the digital images to a smaller number of pixels.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computerized method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising implementing a median filter on the digital images to reduce noise, wherein each pixel in a window is replaced by a median value of all pixel values in the window.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computerized method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the segmenting of the body pixels and the background pixels comprises detecting contours in the digital images at edges of the body pixels.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computerized method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the segmenting further comprises converting the digital images to grey scale and segmenting the digital images with a Sobel edge detection algorithm operated in both the x and y directions of a two dimensional array.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computerized method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the segmenting further comprises detecting a respective direction of the edges and intensity of corresponding pixels of the edges.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computerized method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising removing enclosed background pixels located within identified edges enclosing the body pixels of the digital images.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The computerized method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein removing the enclosed background pixels comprises labeling respective pixels as background pixels according to a probability threshold that a given pixel is one of the background pixels.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computerized method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein extracted features are calculated after transforming the digital images to a selected color space.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The computerized method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the extracted features are textures of the insect specimen detected by at least one of a structural identification method using dependency of intensity values among neighboring pixels and a statistical identification method analyzing spatial distribution of gray values within the digital images.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The computerized method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein extracting features comprises identifying local binary patterns in the digital image.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The computerized method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein extracting features comprises deriving Haralick textural features from respective probabilities that corresponding pixels with selected gray levels are side by side in the digital images and calculating spatial dependencies within the digital images.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The computerized method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein extracting features comprises reducing dimensionality of sets of extracted features by projecting the extracted features into a lower dimension sub-space and calculating a linear combination of the extracted features that maintains class variability and reduces correlation among the extracted features.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computerized method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising extracting the features in a Lab color space, where L represents lightness, and a and b represent color opponents ranging from green-red and blue-yellow.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The computerized method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the clustering comprises sampling the body pixels of the digital images with a plurality of Gaussian distributions and identifying model parameters by which an unsupervised learning algorithm finds each Gaussian distribution from which respective samples originate.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computerized method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising classifying clusters according to the model parameters and calculating a probability that the respective samples belong to a particular species.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A computer program product implemented on a personal communications device having a camera for acquiring digital images, the computer program product configured for storing in memory and executed by at least one processor on the personal communications device, causing the processor to implement steps of a computerized method comprising:<claim-text>gathering a plurality of digital images of multiple insect specimens positioned within a respective set of image backgrounds;</claim-text><claim-text>extracting image portions from each digital image;</claim-text><claim-text>converting the image portions into a selected color space data set; and</claim-text><claim-text>using trained neural networks to classify the digital images with algorithms stored in software that cause the processor to:</claim-text><claim-text>segment respective digital images into body pixels and image background pixels by detecting edges of the body pixels;</claim-text><claim-text>extracting features of the insect specimens from the respective digital images by calculating variance of pixel intensity across the respective digital images;</claim-text><claim-text>clustering the digital images according to the features; and</claim-text><claim-text>classifying the insect specimens by species according to respective clusters.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computer program product of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising network communications software in data communication with the computer program product for implementing cloud based processing and identification.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The computer program product of <claim-ref idref="CLM-00018">claim 18</claim-ref>, further comprising clustering with an unsupervised learning algorithm implemented by the neural network.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computer program product of <claim-ref idref="CLM-00019">claim 19</claim-ref>, further comprising classifying the insect specimens with at least one support vector machine.</claim-text></claim></claims></us-patent-application>