<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007342A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007342</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17363300</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>442</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>14</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>439</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>4223</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>4788</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>44218</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>1454</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00335</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00389</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>439</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>4223</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>4788</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">METHOD AND APPARATUS FOR SHARED VIEWING OF MEDIA CONTENT</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>Rovi Guides, Inc.</orgname><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Shah</last-name><first-name>Akshay Chetan</first-name><address><city>Mumbai</city><country>IN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Chandrashekar</last-name><first-name>Padmassri</first-name><address><city>Bangalore</city><country>IN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Emmanuel</last-name><first-name>Daina</first-name><address><city>Bangalore</city><country>IN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">In systems and methods for enhancing group watch experiences, a first user's reaction is detected using multiple sensors, e.g., at least one camera and a microphone, and may be combined with context information to determine an action to perform at user equipment devices of other users participating in the group watch to convey the first user's reaction. Images from the at least one camera can be used to determine a portion of the screen to which the user's reaction is directed and/or another user to whom the reaction is directed. The reaction may be conveyed using one or more of an audio effect, a visual effect, haptic effect or text, e.g., to highlight the determined portion or user, display an icon and/or output an audio or video clip. A signal for providing haptic feedback may be transmitted to the user equipment device of the determined user.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="104.82mm" wi="158.75mm" file="US20230007342A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="238.08mm" wi="176.61mm" file="US20230007342A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="243.67mm" wi="176.11mm" file="US20230007342A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="234.27mm" wi="141.14mm" file="US20230007342A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="233.93mm" wi="166.20mm" file="US20230007342A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="243.59mm" wi="174.41mm" file="US20230007342A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="234.27mm" wi="141.14mm" file="US20230007342A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="243.50mm" wi="175.85mm" file="US20230007342A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="233.85mm" wi="141.14mm" file="US20230007342A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="242.74mm" wi="176.02mm" file="US20230007342A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="121.75mm" wi="174.41mm" file="US20230007342A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="235.46mm" wi="169.76mm" file="US20230007342A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">Embodiments of the disclosure relate generally to methods and systems for group watching live or on-demand media content or other shared viewing activities.</p><p id="p-0003" num="0002">Consumption of media content in home environments has risen in recent times. This rise has been driven, in part, by increases in the number of channels available through broadcast, cable and satellite systems and in the number of streaming services. It is not always possible, however, for a group of viewers to gather together to view the content. For instance, a group of friends might like to watch a sports event or movie together but cannot gather in the same physical location, for example, due to travelling distances between their locations and/or restrictions on indoor gatherings. While it may be possible to use screen-sharing or videoconferencing to allow a group of viewers at different locations to watch the same content together, applications and functionality dedicated to shared viewing have become available.</p><p id="p-0004" num="0003">In a shared viewing activity, such as a group watch session, a plurality of viewers can watch media content at the same time, regardless of their respective locations. At least some degree of synchronization between the playback of the content on the devices used by the viewers to view the content is provided, for example using a group watch application implemented on the viewers' respective media devices. In particular, playback operations instigated by one or more of the viewers, such as pausing, rewinding, fast-forwarding or skipping content, is replicated in the playback of the content to the other viewers in the group.</p><p id="p-0005" num="0004">When using screen-sharing, videoconferencing or group watch applications, interactions between the viewers in the group are limited. For example, where screen-sharing is used, the viewers may need to rely on separate communication methods, such as e-mails, text messages, or group calls, in order to communicate with one another, while a group watch application may limit interaction between users to, say, a chat window. The effectiveness with which the above techniques emulate an experience of multiple viewers interacting with one another while watching a program in the same physical location is therefore limited.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0006" num="0005">Embodiments of this disclosure include methods and systems for transmitting user actions and providing feedback during a shared viewing activity to convey live user reactions to the media content being watched between users in a group. Such methods and systems may use a shared viewing application implemented on user devices to view the content, such as a group watch application. The shared viewing application may be a stand-alone application or may be a software module that is part of another application, such as an interactive television application, media guidance application, videocall application or videoconferencing application.</p><p id="p-0007" num="0006">The shared viewing application or software module uses the output from one or more sensors monitoring a first user in the group and, optionally, context information regarding the first user and/or the media content, to determine a reaction of the first user to be conveyed to one or more other users in the group. The sensors may, for example, detect one or more of the user's speech, gestures, verbal cues, or facial expressions to determine a reaction to be conveyed and, optionally, one or more other users in the group to whom the reaction may be directed. Such embodiments may facilitate enhanced interaction between the users taking part in the shared viewing activity.</p><p id="p-0008" num="0007">A plurality of sensors may be used to capture visual data and audio data of a user reaction, such as speech, a physical gesture, verbal cue or facial expression, of at least one of the users in the group. A corresponding action is determined based on the captured data and, optionally, context information, and the corresponding action is then performed at a user device of at least one of the other users in the group. For example, a first user in a group viewing a televised soccer match may point at a portion of a display in which a particular player is shown and make a verbal remark giving an opinion that the ball should be passed to that player. That pointing action is detected by a plurality of cameras. Images output from the plurality of cameras may be used to derive three-dimensional visual data for determining the portion of the display to which the first user is pointing. An audio sensor receives the first user's verbal remark and outputs a corresponding signal. The signal output from the audio sensor is processed to interpret the remark by determining the opinion given by the first user and, optionally, a user in the group to which the remark is to be directed or the name of the player. Based on the three-dimensional visual data and audio data, that portion of the display in which that player is shown may be highlighted on the display devices of one or more other users in the group to provide additional context for the first user's opinion, in a similar manner to how the first user pointing at the display would convey that reaction to other viewers if they had gathered at the first user's physical location, while the first user's opinion is conveyed in audio or visual form.</p><p id="p-0009" num="0008">In some embodiments, gestures of one of the users in the group are identified from the captured three-dimensional visual data, and/or other data indicative of that user's movements, and a corresponding emoji, text, audio clip, video filter, video clip, image or meme is presented to the other users in the group. For example, if the first user were to cheer a goal in a soccer match, an audio or visual indication of a celebration, such as an audio clip of cheering, a celebratory emoji, or a celebratory message may be presented on the displays of one or more of the other users. For example, a celebratory image may be presented to selected users in the group based on whether their profile information indicates that those users support the team that has scored a goal.</p><p id="p-0010" num="0009">The plurality of sensors may alternatively, or additionally, be used to identify a second user to whom the first user is referring or directing a comment. For example, images or avatars of the users in the group may be displayed alongside the content. Where the first user wishes to direct a comment to a second user in the group, the first user may point to the image of that user, and three-dimensional visual data obtained from the plurality of cameras may be used to identify which of the other users is being pointed to. In another embodiment, the first user may be watching the content on a media device having a touch-screen display, and may indicate the second user by tapping on their image. A comment or reaction from the first user may then be conveyed to that second user. Alternatively, or additionally, based on the context information or other input from the first user, the second user may be highlighted in the displays of the other users in the group, for example by applying an image filter to an image, video or avatar of the second user.</p><p id="p-0011" num="0010">In some embodiments, if the first user wished to mock one of the other users, for example, a second user who supports another team, the first user's reaction to an event such as that team missing a penalty kick may take the form of sending an icon, image, meme, message, audio clip or video clip to the user device of the second user and, optionally, user devices of the other users in the group. For example, a mocking message may be presented to selected users in the group based on whether their profile information indicates that those users support the other team.</p><p id="p-0012" num="0011">In some embodiments, a haptic device may be used to convey a tactile reaction from one user to another. For example, where a first user makes a gesture of nudging another user or tapping the other user on the shoulder to get their attention, a haptic device may be used to convey a corresponding physical sensation to the other user. The haptic device may be a device worn by the second user, such as a smartwatch, a device that the second user is watching the content on, such as a tablet, or another device associated with the second user, such as a smartphone.</p><p id="p-0013" num="0012">The application or software module may also provide betting/game functionality, in where the first and second users can assert different outcomes of an event, such as a first team to score a goal in the match or what the outcome of a particular play might be. These assertions can be detected by processing the output signal of the audio sensor to identify keywords or concepts relating to the bet. The application or software module may then determine the outcome from metadata or through analyzing audio or video components of the media content and display or output reactions to at least the first and second users indicating which of them made a correct assertion.</p><p id="p-0014" num="0013">Such methods and systems may be used to enhance shared viewing activities such as group watch sessions, in which media content is played to multiple users of respective user equipment devices. The playing of the media content may be synchronized. Playback operations requested by one, some or all of the users, such as rewinding, pausing, skipping, fast-forwarding or other trickplay functions, are performed by all of the respective user equipment devices. The media content may be live media content or on-demand media content. Other shared viewing activities in which the above methods and systems may be used include videocalls, videoconferences, screen-sharing or multi-player games.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE FIGURES</heading><p id="p-0015" num="0014">The above and other objects and advantages of the disclosure will be apparent upon consideration of the following detailed description, taken in conjunction with the accompanying drawings, in which like reference characters refer to like parts throughout, and in which:</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts a system for shared viewing activity in accordance with some embodiments of the invention;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of a user equipment device in the system of <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIGS. <b>3</b>A &#x26; <b>3</b>B</figref> depict an example of a display of media content enhanced with a user reaction;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIGS. <b>4</b> &#x26; <b>5</b></figref> are flowcharts of procedures for detecting and outputting a user reaction as shown in <figref idref="DRAWINGS">FIGS. <b>3</b>A &#x26; <b>3</b>B</figref> respectively;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIGS. <b>6</b>A &#x26; <b>6</b>B</figref> depict an example of a display of media content enhanced with a user reaction directed at a portion of the displayed content;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of a procedure for detecting and responding to a user reaction as shown in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIGS. <b>8</b>A &#x26; <b>8</b>B</figref> depict an example of a display of media content enhanced with a user reaction directed at a particular user;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart of a procedure for detecting a user reaction as shown in <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIGS. <b>10</b>A, <b>10</b>B &#x26; <b>10</b>C</figref> depict an example of a display of media content enhanced with a competition function between two users; and</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart of a procedure for providing a competition function as shown in <figref idref="DRAWINGS">FIGS. <b>10</b>A, <b>10</b>B &#x26; <b>10</b>C</figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0026" num="0025">Example methods and systems for transmitting user feedback and actions in a shared viewing activity will now be described.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts an example of a system <b>100</b> for providing shared viewing of media content in accordance with embodiments of the invention in which a group of users <b>102</b><i>a</i>-<i>d </i>are watching media content on respective user equipment devices <b>104</b><i>a</i>-<i>d</i>. Examples of suitable user equipment devices <b>104</b><i>a</i>-<i>d </i>include, but are not limited to, a smart television, a tablet device, a smartphone, a device such as a set-top box or streaming device connected to a display device, a 3D headset or virtual reality display equipment.</p><p id="p-0028" num="0027">The user equipment devices <b>104</b><i>a</i>-<i>d </i>receive the same media content from a content source <b>106</b> via a communication network <b>108</b>. Examples of content sources <b>106</b> include video-on-demand servers, streaming services, network digital video recorders or other device that can communicate with the user equipment devices <b>104</b><i>a</i>-<i>d </i>via the network <b>108</b>. Examples of media content include a television program, a recording of media content, streamed media content or an online video game. In this example, the communication network <b>108</b> is the Internet.</p><p id="p-0029" num="0028">Although only one communications network <b>108</b> is shown in the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in other embodiments the user equipment devices <b>104</b><i>a</i>-<i>d </i>may receive the media content via a first communication network <b>108</b> and communicate with other user equipment devices <b>104</b><i>a</i>-<b>104</b><i>d </i>via a second communication network (not shown). For example, the user equipment devices <b>104</b><i>a</i>-<i>d </i>may receive the media content via a first communications network, such as a cable or broadcast network, and communicate with each other via a second communication network, such as the Internet.</p><p id="p-0030" num="0029">An example of a user equipment device <b>200</b> for use in the system <b>100</b> is depicted in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The user equipment device <b>200</b> includes control circuitry <b>202</b>, which comprises processing circuitry <b>204</b> and a memory <b>206</b> that stores, at least, a computer program that, when executed by the processing circuitry <b>204</b>, provides a shared viewing application. The processing circuitry <b>204</b> may be based on one or more microprocessors, microcontrollers, digital signal processors, programmable logic devices, etc. The memory <b>206</b> may be random-access memory, read-only memory, or any other suitable memory.</p><p id="p-0031" num="0030">The control circuitry <b>202</b> is arranged to receive media content via the communication network <b>108</b> through input/output path <b>208</b>, and generates for display a video component of the media content. In addition, the control circuitry <b>202</b> is arranged to generate and send data conveying reactions of the user of the user equipment device <b>200</b> to other users in the group and to receive, and generate for display, data conveying user reactions from other user equipment devices <b>104</b><i>b</i>-<i>d </i>in the group via the input/output path <b>208</b>.</p><p id="p-0032" num="0031">The control circuitry <b>202</b> is arranged to provide the video component and received data conveying the reactions of other users for display via display output <b>210</b>. The display output <b>210</b> may be configured to be connected, via a wired or wireless connection, to an external display device, such as a television or monitor (not shown), or may be an integrated display, such as a touch-screen display.</p><p id="p-0033" num="0032">The control circuitry <b>202</b> is also arranged to generate for output, via audio output <b>212</b>, an audio component of the media content. The display output <b>210</b> may be configured to be connected, via a wired or wireless connection, to an external audio output device, such as a television, monitor, speaker or headphones (not shown), and/or one or more speakers integrated into the user equipment device <b>200</b>.</p><p id="p-0034" num="0033">The control circuitry <b>202</b> is also arranged to receive input from a plurality of sensors. In the example shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the user equipment device <b>200</b> includes a microphone input <b>214</b> that is arranged to receive audio input signals via an integrated or external microphone <b>216</b>. The control circuitry <b>202</b> is also arranged to receive still and/or video images via at least one input <b>218</b>, <b>220</b>, <b>222</b> from a respective camera <b>224</b>, <b>226</b>, <b>228</b>. The camera, or cameras, may be integrated into the user equipment device <b>200</b>, external cameras connected to the user equipment device <b>200</b>, or a combination thereof.</p><p id="p-0035" num="0034">The user equipment device <b>200</b> also includes a user input interface <b>230</b> for receiving commands and requests from a user, for example, to control playing and selection of media content using a remote control device (not shown). Such a remote control device may be connected to the user equipment device <b>200</b> via a wireless connection, such as an infra-red, Wi-Fi, BLUETOOTH or other suitable connection. Alternatively, or additionally, the microphone <b>216</b> and microphone input <b>214</b> may be used to receive voice input for controlling the user equipment device <b>200</b>, in which case the processing circuitry <b>204</b> may perform natural language processing to determine the user's command from the voice input and perform a corresponding action.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> depicts an example of a display screen for use in a shared viewing experience and a user reaction, according to some embodiments. In this example, a group of users are participating in a group watch session of media content in the form of a soccer match. The display screen, shown on a user equipment device <b>300</b> of a first user <b>302</b> in the group, presents the media content in a main display portion <b>304</b> and a gallery <b>306</b> of images <b>308</b>, <b>310</b>, <b>312</b>, <b>314</b> showing video or avatars of the users in the group.</p><p id="p-0037" num="0036">In the example shown in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, the first user <b>302</b> is cheering in response to a goal in the soccer match. The reaction of the first user <b>302</b> is detected by the user equipment <b>300</b>. For example, an audible cheer or exclamation <b>316</b> from the first user <b>302</b> may be detected by a microphone <b>318</b> that is connected to, or integrated into, the user equipment <b>300</b>.</p><p id="p-0038" num="0037">The user equipment device <b>300</b> includes, or is connected to, one or more cameras <b>320</b>, <b>322</b>, <b>324</b>. One of these cameras may be used to obtain the video <b>308</b> of the first user <b>302</b> shown in the gallery of images <b>308</b>, <b>310</b>, <b>312</b>, <b>314</b>. The video of the first user <b>302</b> captured by the one or more cameras <b>320</b>, <b>322</b>, <b>324</b> is analyzed to detect certain physical gestures. In the example shown in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, a gesture in which the first user <b>302</b> raises his arms is detected from the captured videos.</p><p id="p-0039" num="0038">In this particular example, movements of the first user <b>302</b> are also monitored based on data received from a device <b>325</b> worn, or held, by the first user <b>302</b>. For example, the first user <b>302</b> may be wearing a smartwatch that includes an accelerometer or gyroscope that outputs data indicative of the first user's movements and transmits it to the user equipment device <b>300</b>, for example, via the communication network <b>108</b> or via another connection such as a Wi-Fi or Bluetooth link. Alternatively, or additionally, the first user <b>302</b> may be holding a smartphone, not shown, that includes an accelerometer or gyroscope that can provide data indicative of the first user's movements to the user equipment device <b>300</b> in a similar manner. The control circuitry of the user equipment <b>300</b> then uses the video captured by the cameras <b>320</b>, <b>322</b>, <b>324</b> and/or data from other sensors, and combines it with context information to determine whether to cause an action to be performed at the user equipment devices of some, or all, of the other users in the group based on the first user's reaction. In this particular case, the videos of the first user <b>302</b> raising his hands, or data indicative of such a movement received from a wearable or handheld device <b>325</b>, may be combined with one or more of the audible cheer or exclamation from the first user <b>302</b>, metadata provided with the media content, analysis of the video component of the media content, analysis of the user's gestures of facial expression, or user profile information indicating that the first user <b>302</b> supports one of the teams playing in the soccer match to determine that the first user's gesture is a celebration of an event in the soccer match. Such user profile information may be, for example, a viewing history of the first user <b>302</b>, a social media profile of the first user <b>302</b>, or other profile information. In this example, if the user profile information indicates that the user supports Team A and it can be determined, from a change to the score displayed in a scoreboard <b>336</b> shown in the display screen, text in a ticker included in the media content, the exclamation from the first user <b>302</b> or from metadata accompanying the media content that Team A has just scored, then the control circuitry may determine that the first user <b>302</b> is celebrating a goal and that data corresponding to that reaction is to be sent to the user equipment devices of the other users in the group. Alternatively, or additionally, the control circuitry may undertake natural language processing or other voice processing to extract a keyword, such as &#x201c;Goal!&#x201d; from the first user's cheer or exclamation <b>316</b>, and that keyword may be included in the context information.</p><p id="p-0040" num="0039">The control circuitry may then determine, based on the received video input and/or audio or data received via other sensors, and based on the context information, an action to be performed at the user equipment devices of the other users in the group. In this particular example, the control circuitry determines that celebratory text, such as &#x201c;GOAL!&#x201d;, should be overlaid onto the media content <b>304</b> to convey the first user's reaction. Optionally, the control circuitry may determine an audio clip of the exclamation <b>316</b> of user <b>1</b> should be played if the audio of the first user's exclamation has not already been conveyed to the other user equipment devices as part of the shared viewing activity. In other embodiments, a different audio clip, such as celebratory music, may be included in the message or identified by a title or location in the message so that the other user equipment devices can retrieve the clip from local or external storage and play it. A .gif file, emoji, icon, image or video clip may be provided instead of, or as well as, the celebratory text.</p><p id="p-0041" num="0040">The control circuitry of the user equipment device <b>300</b> then sends a message requesting presentation of an indication of the first user's reaction to at least some of the other user equipment devices via the communication network. The message might specify the indication, such as a visual effect, audio effect, haptic effect or combination thereof. For example, the message may include an audio or visual clip, icon, emoji, image or text for presentation to other users, or an indication of a name or location of a stored clip, icon, image or text from which the other user equipment devices may retrieve the desired effect. Alternatively, or additionally, the message may specify a context of the first user's reaction or an intention of the first user, based on the context information. The message may also include coordinates determined from the outputs of the cameras <b>320</b>, <b>322</b>, <b>324</b> indicating a position to be highlighted or indicated to the other users.</p><p id="p-0042" num="0041">The message may be sent to some or all of the user equipment devices participating in the shared viewing activity. For example, the users in the group may be arranged into sub-groups. In the example shown in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, users 1 and 2 are in a first sub-group, which, optionally, may be indicated by the arrangement of their respective video images <b>308</b>, <b>310</b> on the screen or by a visual indication such as a colored border <b>326</b> around their videos <b>308</b>, <b>310</b>. The sub-group may be defined by one of the users manually or based on their respective user profile information. For example, users 1 and 2 may have been placed in the same sub-group based on their user profiles indicating their support for Team B, whereas users 3 and 4 might support Team A. In such a scenario, the user equipment <b>300</b> may send the message to only those user equipment devices of the users in the same sub-group as the first user <b>302</b>.</p><p id="p-0043" num="0042">Alternatively, the user equipment device <b>300</b> may send the message to all of the other user equipment devices for presentation to all the users participating in the group watch activity. In some embodiments, the other user equipment devices may determine whether or not to present the audio clip and celebratory text based on user profile information of their respective users. For example, if user equipment <b>300</b> sends the message to the user equipment devices of users 2-4, then the user equipment device of user <b>2</b> may determine, for example based on the inclusion of user <b>2</b> in the same sub-group as user <b>1</b> or on user profile information indicating that user <b>2</b> supports Team B, that the first user's reaction should be reflected on its display of the media content, while the user equipment device of user <b>3</b> may determine that the first user's reaction should not be presented to user <b>3</b>.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> depicts the presentation of the first user's reaction on the user equipment device <b>328</b> of another user in the group in response to receipt of the message from the user equipment device <b>300</b>. In the example shown in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, a display screen of the user equipment device <b>328</b> presents the media content in a main display portion <b>304</b> and the gallery of user images <b>308</b>, <b>310</b>, <b>312</b>, <b>314</b>. The celebratory text is displayed, for example by overlaying a banner <b>330</b> on the media content.</p><p id="p-0045" num="0044">The user equipment device <b>328</b> may determine a position within the display to present the banner <b>330</b> by determining a portion of the main display <b>304</b> that is relatively unimportant. In the example shown in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, the banner <b>330</b> is overlaid on a portion of the main display <b>304</b> that does not obscure the players or ball. Control circuitry of the user equipment device <b>328</b> may determine the position in which to display the banner <b>330</b> or other visual effect based on the interests of the other user. For example, the position may be determined based on user profile information indicating the other user's interest in the teams, particular players, or other objects shown in the display screen.</p><p id="p-0046" num="0045">Optionally, an audio clip of the first user's exclamation <b>316</b> is played through a speaker <b>332</b> connected to, or integrated into, the user equipment device <b>328</b>. Optionally, a second visual indicator highlighting the first user <b>302</b> is also provided, such as a border <b>334</b> around the first user's video <b>308</b>.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIGS. <b>4</b> and <b>5</b></figref> are flowcharts of processes performed by the control circuitry of the user equipment devices <b>300</b>, <b>328</b> respectively, to convey the first user's reaction in the example of <figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref>. Beginning at step <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, based on an instruction received from the first user <b>302</b>, for example, through the user input interface or a voice command, the control circuitry of the user equipment <b>300</b> joins a group watch session (step <b>402</b>). The group watch session may be initiated by the user equipment device <b>300</b> based on the instruction or, alternatively, the user equipment device <b>300</b> may join an existing group watch session initiated by another user.</p><p id="p-0048" num="0047">The user equipment <b>300</b> then begins presenting the media content. In this example, four user equipment devices <b>300</b>, <b>328</b> are presenting a soccer match to users 1-4 in a group watch session, as shown in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, and more than one user may be viewing the content at any one of the user equipment devices. Video of the first user <b>302</b> may then be captured through the one or more cameras <b>320</b>, <b>322</b>, <b>324</b> and transmitted to the other user equipment devices connected to the group viewing session for display in the gallery <b>306</b> portion of their respective display screens. Optionally, audio of the first user <b>302</b> may be captured through the microphone <b>318</b> and transmitted to the other user equipment devices instead of, or as well as, the video of the first user <b>302</b> to allow the users to converse with one another. The users may be divided into sub-groups and messages, reactions or chat may optionally be directed only to members of a particular sub-group.</p><p id="p-0049" num="0048">The group watch application may include a setting that allows the first user <b>302</b> to activate an enhanced interaction mode, in which the first user's reactions are monitored and conveyed to one or more other users in the group viewing session. Alternatively, such a setting may be associated with the group viewing session, rather than set by individual users, or may be a default mode of the group watch application. If an enhanced interaction mode is activated (step <b>404</b>), then the captured video and/or audio is monitored to detect gestures or sounds, and/or other actions from the first user <b>302</b> indicative of a reaction to the media content (step <b>406</b>). For example, the control circuitry may perform a gesture recognition on captured video of the user <b>302</b> to detect physical gestures such as facial expressions, waving, pointing, a &#x201c;high-five,&#x201d; raising a hand, or other movements of the first user <b>302</b>. For example, the control circuitry may determine one or more reaction characteristics, such as a direction, a magnitude, and a type of a movement. Such characteristics may be determined based on the video captured by the one or more cameras <b>320</b>, <b>322</b>, <b>324</b> and, where multiple cameras <b>320</b>, <b>322</b>, <b>324</b> are provided, comparing the captured videos, and/or from analysis of data received from a device <b>325</b> worn by the user <b>302</b> or held by the user <b>302</b> indicative of the first user's movements, such as a smartwatch or cellphone including an accelerometer or gyroscope. The control circuitry may then access a database that lists movement characteristics and types of movement characteristics together with corresponding reactions. Alternatively, or additionally, the control circuitry may parse audio input received via the microphone <b>318</b> to identify verbal cues, sounds or keywords in the first user's speech indicative of a reaction to determine one or more reaction characteristics, and those characteristics to corresponding reactions.</p><p id="p-0050" num="0049">The control circuitry determines, based on the analysis of the captured video and/or audio, whether a reaction from the first user <b>302</b> is detected (step <b>408</b>). If no reaction is detected, then the process returns to monitoring the user at step <b>406</b>. If a reaction is detected, then the control circuitry determines a context of the reaction (step <b>410</b>). The context may be determined based on the media content. For example, the control circuitry may determine a context based on metadata accompanying the media content, or on recognition of objects or audio cues in the media content. In <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, where the media content is a soccer match, the context may be determined based on detection of cheering, the word &#x201c;goal&#x201d; appearing in oral commentary in an audio component of the media content, in text of a ticker included in the media content, or in closed caption data accompanying the media content. The control circuitry may thus determine that the first user is reacting to a goal in the soccer match. Alternatively, or additionally, the control circuitry may detect a keyword &#x201c;goal&#x201d; in a verbal cue extracted from the captured audio or a cheer from the first user and determine the context to be a goal or based on recognition of a change in the score shown on a scoreboard <b>336</b> in the media content. Another option that may be combined with the use of the captured audio and/or video is to use the user profile information in the context determination. For instance, the control circuitry may determine that the first user <b>302</b> supports Team B, based on one or more of a viewing history of soccer matches involving Team B, an indication in a user profile, such as a media guidance user profile or a social media profile of the first user <b>302</b>, previous social media posts by the first user <b>302</b> and/or the first user <b>302</b> belonging to a group of Team B supporters in a social network. For example, the control circuitry may determine that a goal has been scored based on the media content or accompanying data and determine, based on the profile information of the first user <b>302</b>, that the goal was scored by Team B, resulting in a context of a Team B goal.</p><p id="p-0051" num="0050">At step <b>412</b>, the control circuitry transmits a message to at least one other user equipment device <b>328</b> participating in the shared viewing session. The message may indicate an intent of the first user <b>302</b>, such as celebrating, and a context, such as a goal for Team B, from which the other user equipment device can determine a corresponding action to perform to convey the first user's reaction to another user. Alternatively, the control circuitry may determine an action to be performed by the other user equipment devices to convey the first user's reaction, such as the display of the banner <b>330</b>, a celebration emoji, playing an audio clip of cheering, etc., and indicate that action in the message, for example by correlating the reaction and context with entries in a database listing corresponding actions and/or effects. In some embodiments, the message may optionally identify a file or location of a file containing audio or video data for display or may include the file itself. The message may be, or include, a JavaScript Object Notation (JSON) format file.</p><p id="p-0052" num="0051">The control circuitry then continues operating in the enhanced mode (step <b>404</b>) and returns to monitoring the first user's actions at step <b>406</b> until either the enhanced mode is deactivated (step <b>404</b>) or the viewing session finishes (step <b>414</b>), ending the process (step <b>416</b>).</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts a process performed by the second user equipment device <b>328</b>. Beginning at step <b>420</b>, the second user equipment device <b>328</b> joins the group viewing session (step <b>422</b>). If the enhanced mode is activated (step <b>424</b>) then, when the message from the user equipment <b>300</b> is received (step <b>426</b>), the control circuitry of the second user equipment <b>328</b> determines one or more effects to be presented, based on the message (step <b>428</b>). As noted above, the message may specify a particular effect determined by the user equipment device <b>300</b>. Alternatively, the control circuitry of the second user equipment device <b>328</b> may determine the one or more effects to be presented based on the information contained in the message, for example by mapping information about the reaction and context to database entries matching such information to particular actions and/or effects.</p><p id="p-0054" num="0053">The control circuitry of the second user equipment device <b>328</b> then performs actions based on the message by presenting the determined effects. If the one or more determined effects include a visual effect (step <b>430</b>) then the effect is displayed (step <b>432</b>), such as the display of the banner <b>330</b>, a video clip, an icon, a meme, or emoji. If the one or more determined effects include an audio effect (step <b>434</b>) then the effect is output (step <b>436</b>), for example by playing an audio clip, part of the captured audio from the first user <b>302</b> or a sound effect. If the one or more determined effects include a haptic effect (step <b>438</b>), then an instruction to provide a haptic effect is transmitted to a haptic device in communication with the second user equipment <b>328</b> (step <b>400</b>). For example, the second user equipment may transmit instructions to a smartwatch of the user to cause it to vibrate.</p><p id="p-0055" num="0054">The example method shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> includes decisions <b>430</b>, <b>434</b>, <b>438</b> whether to provide visual, audio and haptic effects and steps <b>432</b>, <b>436</b>, <b>440</b> that may be performed to provide such effects. In other embodiments, however, the steps relating to one or more of these effects may be omitted. For example, a method according to another embodiment may include the steps <b>430</b>, <b>432</b>, <b>434</b>, <b>436</b> relating to providing a visual effect and/or audio effect, but omit the steps <b>438</b>, <b>440</b> relating to a haptic effect. A method according to yet another embodiment might include only the steps <b>430</b>, <b>432</b> relating to a visual effect and omit the steps <b>434</b>, <b>436</b>, <b>438</b>, <b>440</b> relating to audio and haptic effects, and so on.</p><p id="p-0056" num="0055">The control circuitry of the second user equipment device then continues with the group viewing session, awaiting further messages and optionally monitoring actions of the second user in a similar manner to the monitoring in step <b>406</b> of <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, until either the enhanced mode is deactivated (step <b>424</b>) or the viewing session finishes (step <b>442</b>), ending the process (step <b>444</b>).</p><p id="p-0057" num="0056">Although the processes of <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>5</b></figref> have been described with reference to a particular group watch session, it will be understood that these methods may be implemented in group watching of live content or group watching of on-demand content, or in other shared viewing experiences such as a videocall, a videoconference, a multi-player game, or when screen-sharing. In addition, the examples of visual, audio and haptics effects are not limiting. In other embodiments, different effects may be presented instead of, or as well as, the effects described above.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> depicts another example of a user reaction and corresponding effect, in which a visual effect is used to enhance a verbal cue from a first user. A display screen, shown on a user equipment device <b>600</b> of the first user <b>602</b> in a group viewing session, presents media content in a main display portion <b>604</b>. Also presented is a gallery <b>606</b> of images <b>608</b>, <b>610</b>, <b>612</b>, <b>614</b> showing video or avatars of other users in the group viewing session. In the example shown in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, the media content is a soccer match and the first user <b>602</b> is commenting that the ball should be passed to a particular player <b>616</b>. The comment by the first user <b>602</b> is detected by a microphone <b>618</b> that is connected to, or integrated into, the user equipment <b>600</b>. In some embodiments, if the first user <b>602</b> has mentioned the player's name, nickname, position or squad number in his comment <b>620</b>, then the context of the first user's comment <b>620</b> could be determined by extracting that information as a keyword from the audio detected by the microphone <b>618</b>. In this particular example, however, the comment <b>620</b> from the first user <b>602</b> does not identify the player <b>616</b>, and so the context of the comment cannot be determined from the comment alone.</p><p id="p-0059" num="0058">The user equipment device <b>600</b> includes, or is in communication with, two or more cameras <b>620</b>, <b>622</b>, <b>624</b>. One of these cameras may be used to obtain the video <b>608</b> of the first user <b>602</b> shown in the gallery <b>606</b>, in addition to providing video for monitoring the first user's actions. The video of the first user <b>602</b> captured by the two or more cameras <b>620</b>, <b>622</b>, <b>624</b> is analyzed to detect certain gestures, such as facial expressions, physical gestures and movements. In this example, the control circuitry uses gesture recognition to determine that the first user <b>602</b> is pointing towards the display screen.</p><p id="p-0060" num="0059">The control circuitry of the user equipment <b>600</b> then compares the images captured by the cameras <b>620</b>, <b>622</b>, <b>624</b> to determine a portion of the display screen to which the first user <b>602</b> is pointing. For example, the control circuitry may determine coordinates of the portion based on orientations of the first user's finger as shown in the multiple images.</p><p id="p-0061" num="0060">The control circuitry of the user equipment <b>600</b> then generates a message for transmission to a second user equipment <b>626</b> participating in the shared viewing session. The message includes the context, i.e., co-ordinates or other information identifying the portion of the display screen. The message may optionally include the first user's comment as an audio clip. Alternatively, the user's comment may already have been conveyed to the users as part of the shared viewing activity, and need not be included with the message. In some embodiments, the message may specify a particular action to be performed by the other user equipment device <b>626</b>, such as visually highlighting the portion or player <b>616</b>, in a similar manner to that described above in relation to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In other embodiments, the second user equipment <b>626</b> may determine the action to be performed based on the information in the message, as described above in relation to <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>6</b>B</figref> depicts the presentation of the first user's reaction on the second user equipment device <b>626</b>. A display screen of the second user equipment device <b>626</b> presents the media content in a main display portion <b>604</b> and the gallery <b>606</b> of user images <b>608</b>, <b>610</b>, <b>612</b>, <b>614</b>. A first visual effect <b>628</b> is provided indicating the player identified by the first user <b>602</b> to supplement the audio of the first user's comment played through a speaker <b>630</b> connected to, or integrated into, the second user equipment device <b>626</b>. Optionally, a second visual indicator <b>632</b> highlighting the first user <b>602</b> is also provided. In this particular example, the first visual effect <b>628</b> is an arrow pointing at the portion of the display screen in which the player <b>616</b> is located and the second visual indicator <b>632</b> is a border around the image <b>608</b> of the first user <b>602</b>.</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of a process performed by the control circuitry of the first user equipment device <b>600</b> to detect the first user's reaction and transmit a message based on that reaction to the second user equipment device <b>626</b>. Beginning at step <b>700</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the control circuitry of the user equipment <b>600</b> joins a group watch session (step <b>702</b>), in a similar manner to that described above in relation to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, and begins presenting the media content. Video of the first user <b>602</b> is captured through one of multiple cameras <b>620</b>, <b>622</b>, <b>624</b> and transmitted to other user equipment devices connected to the group viewing session for display in the gallery <b>606</b> on their respective display screens. Audio of the first user <b>602</b> is captured through the microphone <b>618</b> and transmitted to the other user equipment devices instead of, or as well as, the video of the first user <b>602</b>, to allow the users to converse with one another. The users may be divided into sub-groups and messages, reaction or chat may optionally be directed only to members of a particular sub-group.</p><p id="p-0064" num="0063">If an enhanced interaction mode is activated (step <b>704</b>), then the captured video and audio is monitored to detect gestures and verbal cues from the first user <b>602</b> indicative of a reaction to the media content (step <b>706</b>). The control circuitry performs a gesture recognition on captured video of the user <b>602</b> to detect physical gestures made by the first user <b>602</b>. In addition, the control circuitry parses audio input received via the microphone <b>618</b> to identify verbal cues or keywords in the first user's speech indicative of a reaction.</p><p id="p-0065" num="0064">The control circuitry determines, based on the analysis of the captured video and/or audio, whether a reaction from the first user <b>602</b> is detected (step <b>708</b>). If no reaction is detected, then the process returns to monitoring the user at step <b>706</b>. If a reaction is detected, then the control circuitry may determine a portion of the screen to which the first user <b>602</b> is pointing (step <b>710</b>), for use in determining a context of the reaction. In the example shown in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, the subject of the first user's comment is not identified in the audio input and so the context includes the identity of the player <b>616</b> and/or the portion of the display screen that includes that player <b>616</b>. In this example, the context is determined based on captured video. The control circuitry of the user equipment device <b>600</b> compares the images from the multiple cameras <b>620</b>, <b>622</b>, <b>626</b> and determines a portion of the display screen to which the first user <b>602</b> is pointing. For example, the control circuitry may determine from an orientation and size of the first user's finger in the captured video coordinates of a portion of the display screen to which the first user <b>602</b> is pointing. The control circuitry of the user equipment device may be configured to obtain additional information, such as information about the players currently shown on screen, by performing an object recognition process and/or based on metadata of the media content or by extracting keywords in audio cues, text in a ticker included in the media content or closed caption data of the media content.</p><p id="p-0066" num="0065">At step <b>712</b>, the control circuitry transmits a message to at least one other user equipment device <b>626</b> participating in the shared viewing session. The message indicates, at least, context information that identifies the portion of the display screen that the first user <b>602</b> is pointing towards. The message may specify a visual effect <b>628</b> to indication the portion, such as highlighting the corresponding portion of a display screen viewed by another user in the group viewing session, for example by overlaying an arrow, as shown in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>, or placing a colored border around the portion. Alternatively, the control circuitry of the second user equipment device <b>626</b> may determine the visual effect <b>628</b> to be displayed and display that effect based on the information in the message that identifies the portion. The message may provide the context information in a JavaScript Object Notation (JSON) format file.</p><p id="p-0067" num="0066">The control circuitry then returns to monitoring the first user's actions at step <b>706</b> until either the enhanced mode is deactivated (step <b>704</b>) or the viewing session finishes (step <b>714</b>), ending the process (step <b>716</b>).</p><p id="p-0068" num="0067">The process described above in relation to <figref idref="DRAWINGS">FIG. <b>7</b></figref> may be performed by the second user equipment device <b>626</b> to receive the message and provide a visual effect to indicate the corresponding portion of the display screen.</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIGS. <b>8</b>A and <b>8</b>B</figref> depict another example of a display of media content enhanced with a user reaction directed at a particular user in a shared viewing session. A display screen, shown on a user equipment device <b>800</b> of the first user <b>802</b> in a group viewing session, presents media content in a main display portion <b>804</b> together with a gallery <b>806</b> of images <b>808</b>, <b>810</b>, <b>812</b>, <b>814</b> showing video or avatars of other users in the group viewing session. In the example shown in <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>, the media content is a soccer match, and the first user <b>802</b> is making a comment <b>828</b> directed at the other user corresponding to image <b>812</b>. The context to be determined in this case includes the identity of the other user to which the comment is directed.</p><p id="p-0070" num="0069">In this example, the comment by the first user <b>802</b> is detected by a microphone <b>818</b> that is connected to, or integrated into, the user equipment <b>800</b>. In other examples, the first user <b>802</b> may, instead, direct a physical gesture such as a high-five, at the other user. If the user has mentioned the others user's name or nickname, then the other user may be determined by extracting that information as a keyword from the audio detected by the microphone <b>818</b>. In this particular example, however, the comment from the first user <b>802</b> does not identify the other user <b>816</b>, and so they cannot be determined from the comment alone.</p><p id="p-0071" num="0070">The user equipment device <b>800</b> includes, or is in communication with, two or more cameras <b>820</b>, <b>822</b>, <b>824</b>. One of these cameras may be used to obtain the video <b>808</b> of the first user <b>802</b> shown in the gallery of images <b>808</b>, <b>810</b>, <b>812</b>, <b>814</b>, in addition to providing video for monitoring the first user's actions. The video of the first user <b>802</b> captured by the two or more of the cameras <b>820</b>, <b>822</b>, <b>824</b> is analyzed to detect certain gestures, such as facial expressions, physical gestures and movements. In this example, the control circuitry uses gesture recognition to determine that the first user <b>802</b> is pointing at the display screen.</p><p id="p-0072" num="0071">The control circuitry of the user equipment <b>500</b> then compares the images captured by the cameras <b>820</b>, <b>822</b>, <b>824</b> to determine a portion of the display screen to which the first user <b>802</b> is pointing. For example, the control circuitry may determine coordinates of the portion based on orientations and sizes of the first user's finger shown in the multiple images.</p><p id="p-0073" num="0072">The control circuitry of the user equipment <b>800</b> may then determine, based on such coordinates, that the first user <b>802</b> is pointing at the third image <b>812</b> in displayed gallery <b>806</b> of user images <b>808</b>, <b>810</b>, <b>812</b>, <b>814</b>. The control circuitry of the user equipment <b>800</b> may, based on this determination, direct the first user's reaction to the other user, for example by generating and transmitting a message only to the user equipment of the other user, generating and transmitting a message only to members of a sub-group to which the other user belongs, or by transmitting a message to the other user equipment devices having context information that indicates that the reaction is directed to that other user. If the first user's gesture was accompanied by other input, such as audio input received via the microphone <b>818</b>, then the message generated by the control circuitry of the user equipment device <b>800</b> may specify an action based on that input or may include the context of the other input.</p><p id="p-0074" num="0073">As described above in relation to <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, the first user <b>802</b> may be wearing or holding a device, such as a smartwatch <b>836</b>, that includes an accelerometer or gyroscope that provides data indicative of the first user's movements. Such data may allow a more precise determination of the first user's gesture. For example, in <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>, the first user <b>802</b>, may be trying to get user <b>3</b>'s attention by making a &#x201c;poking&#x201d; motion. The data from the smartwatch <b>836</b> may allow the user equipment <b>800</b> to distinguish between such a &#x201c;poking&#x201d; motion and a movement in which the first user <b>802</b> is simply pointing at the image <b>812</b> of user <b>3</b>.</p><p id="p-0075" num="0074">The user equipment devices may receive the message and determine, based on the context information regarding the other user, whether or not to perform an action based on the message. For example, even if a reaction is directed to another user, a user equipment device receiving the message might still perform an action to convey the first user's reaction and display a visual effect <b>832</b> highlighting the image <b>812</b> in the gallery corresponding to the user to whom the reaction is directed. Alternatively, if the reaction is a &#x201c;poke&#x201d; action directed to user <b>3</b>, the user equipment devices of users 2 and 4 may determine that no action is to be performed based on the received message.</p><p id="p-0076" num="0075">The message may include context that is further based on the media content. For example, the control circuitry of the user equipment device <b>800</b> may determine that Team A has just missed a penalty kick in the soccer match and may have determined, from the first user's user profile information, that the first user <b>802</b> supports Team B. The control circuitry of the user equipment device <b>800</b> may further determine, based on attributes of the shared viewing session, that the other user supports Team A, for example, based on the first user <b>802</b> and other user belonging to different sub-groups. In such an example, the control circuitry of the user equipment device <b>800</b> may determine that the first user's reaction is mocking the other user's team, and may include an action to provide a mocking visual effect, such as a &#x201c;crying/laughing&#x201d; emoji <b>826</b>; a mocking audio effect, such as an audio clip of sad violin music to be played through a speaker <b>838</b> of the user equipment device <b>830</b> of the other user; or a video filter to adapt the displayed image <b>812</b> of the other user. Alternatively, the message generated by the control circuitry of the user equipment device <b>800</b> may include an indication in that the reaction mocks the other user's team, and the control circuitry of the user equipment device <b>830</b> of the other user may determine a visual and/or audio effect to present to the other user based on that context.</p><p id="p-0077" num="0076">This example may additionally, or alternatively, include a haptic effect. In response to determining that the first user <b>802</b> is pointing at the other user, the control circuitry of the user equipment device <b>800</b> may include, in the message, an indication that a haptic effect is to be output, for example to get the other user's attention. In another example, the control circuitry of the user equipment device of the other user may determine that a haptic effect should be output, based on the message including context indicating that the reaction is directed to the other user.</p><p id="p-0078" num="0077">Such a haptic effect may be provided by the control circuitry of the user equipment device <b>830</b> transmitting an instruction to another device of the other user. For example, the other device may be a smartwatch <b>840</b> worn by the other user and the instruction may cause the smartwatch <b>840</b> to vibrate. In another example, the other device may be a cellphone <b>834</b> of the other user and the instruction may cause the cellphone <b>834</b> to vibrate. If the user equipment device <b>830</b> of the other user includes a haptic output device, for example, a touch-screen device, such as a tablet, arranged to provide haptic feedback, then the user equipment device <b>830</b> may generate the haptic effect itself.</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart of a process performed by the control circuitry of the first user equipment device <b>800</b> to detect the first user's reaction and transmit a message based on that reaction to the second user equipment device <b>830</b>. Beginning at step <b>900</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the control circuitry of the user equipment <b>800</b> joins a group watch session (step <b>902</b>), in a similar manner to that described above in relation to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, and begins presenting the media content. Video of the first user <b>802</b> is captured through one of multiple cameras <b>820</b>, <b>822</b>, <b>824</b> and transmitted to other user equipment devices connected to the group viewing session for display in the gallery <b>806</b> portion of their respective display screens. Audio of the first user <b>802</b> is captured through the microphone <b>818</b> and transmitted to the other user equipment devices instead of, or as well as, the video of the first user <b>802</b>, to allow the users to converse with one another. The users may be divided into sub-groups, and messages, reaction or chat may optionally be directed only to members of a particular sub-group.</p><p id="p-0080" num="0079">If an enhanced interaction mode is activated (step <b>904</b>), then the captured video and audio are monitored to detect gestures and verbal cues from the first user <b>802</b> indicative of a reaction to the media content (step <b>906</b>). The control circuitry performs a gesture recognition process on captured video of the user <b>802</b> to detect physical gestures made by the first user <b>802</b>. In addition, the control circuitry parses audio input received via the microphone <b>818</b> to identify verbal cues or keywords in the first user's speech indicative of a reaction.</p><p id="p-0081" num="0080">The control circuitry determines, based on the analysis of the captured video and/or audio, whether a reaction from the first user <b>802</b> is detected (step <b>908</b>). If no reaction is detected, then the process returns to monitoring the user at step <b>806</b>. If a reaction is detected, then the control circuitry determines a context of the reaction (step <b>910</b>). In the example shown in <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>, the other user to whom the first user's reaction is directed is not identified in the audio input, and so the context includes the identity of that other user to whom the first user <b>802</b> is gesturing and/or a portion of the display screen that includes the image <b>812</b> corresponding to the other user. The context is determined based on captured video. The control circuitry of the user equipment device <b>800</b> compares the images from the multiple cameras <b>820</b>, <b>822</b>, <b>824</b> and determines a portion of the display screen to which the first user <b>802</b> is pointing. For example, the control circuitry may determine, from an orientation and size of the first user's finger in the captured video, coordinates of a portion of the display screen to which the first user <b>802</b> is pointing. The context may also include other information based on the media content, such as a recent missed penalty kick by Team A and/or an indication of the intent behind the first user's reaction. For example, based on user profile information of the first user <b>802</b> indicating that they support Team B, the control circuitry of the user equipment device <b>800</b> may determine that the first user's reaction is mocking the team supported by the other user, Team A, and include an indication of the first user's mocking intent in the determined context.</p><p id="p-0082" num="0081">Optionally, at step <b>912</b>, the control circuitry of the user equipment device <b>800</b> may determine which of the other user equipment devices should receive the message. For example, the control circuitry of the user equipment device <b>800</b> may determine that the message should be sent only to the other user, for example, if the first user's reaction correlates to nudging or poking the other user to get their attention. Alternatively, the control circuitry of the user equipment device <b>800</b> may determine that the message should be sent to all users in the same sub-group as the other user, or to all of the users in the shared viewing session. Alternatively, the control circuitry of the user equipment device <b>800</b> may send the message to all of the user equipment devices participating in the shared viewing session and the receiving user equipment devices may determine, based on the context information, whether to perform an action based on that message.</p><p id="p-0083" num="0082">At step <b>914</b>, the control circuitry of the user equipment device <b>800</b> transmits a message to at least one other user equipment device <b>830</b> participating in the shared viewing session. The message indicates, at least, the context information that identifies the other user or a portion of the display screen in which the other user's image <b>812</b> is shown. The message may specify one or more effects to be presented to the other user, such as a visual effect to indicate the portion, such as highlighting the corresponding portion of a display screen viewed by another user in the group viewing session, for example by overlaying an arrow pointing at the other user, or by placing a colored border around the portion, or activating a video filter for adapting the image <b>812</b> of the other user, an audio and/or visual effect to convey the first user's reaction, such as an emoji to display to the other user, an audio clip or effect to play to the user and/or a haptic effect. Alternatively, the control circuitry of the second user equipment device <b>830</b> may determine the one or more effects to be presented based on the context information in the message. The message may provide the context information in a JavaScript Object Notation (JSON) format file.</p><p id="p-0084" num="0083">The control circuitry then returns to monitoring the first user's actions at step <b>906</b> until either the enhanced mode is deactivated (step <b>904</b>) or the viewing session finishes (step <b>914</b>), ending the process (step <b>916</b>).</p><p id="p-0085" num="0084">The process described above in relation to <figref idref="DRAWINGS">FIG. <b>5</b></figref> may be performed by the second user equipment device <b>830</b> to receive the message and provide the one or more effects based on that message.</p><p id="p-0086" num="0085"><figref idref="DRAWINGS">FIGS. <b>10</b>A and <b>10</b>B</figref> depict an example of a display of media content enhanced with a competition function, based on monitored actions of a first user <b>1002</b>. A display screen, shown on a user equipment device <b>1000</b> of the first user <b>1002</b> in a group viewing session, presents media content in a main display portion <b>1004</b> together with a gallery <b>1006</b> of images <b>1008</b>, <b>1010</b>, <b>1112</b>, <b>1114</b> showing video or avatars of other users in the group viewing session. In the example shown in <figref idref="DRAWINGS">FIG. <b>10</b>A</figref>, the media content is a cricket match and the first user <b>1002</b> is making a comment <b>1020</b> relating to a possible future event in the cricket match. The comment <b>1020</b> is detected by a microphone <b>1018</b> that is connected to, or integrated with, the user equipment device <b>1000</b>.</p><p id="p-0087" num="0086">Audio output from the microphone <b>1018</b> is processed and parsed by the control circuitry of the user equipment device <b>1000</b> to detect keywords, in a similar manner to the embodiments described above. In this particular case, the control circuitry of the user equipment device <b>1000</b> determines that the first user <b>1002</b> has made a comment <b>1020</b> predicting the outcome of an event in the cricket match.</p><p id="p-0088" num="0087">The control circuitry of the user equipment device <b>1000</b> may determine that the outcome is one that can be verified by monitoring the media content. In this particular example, the first user's comment <b>1020</b> provides an opinion that player <b>1016</b> will score a century in the cricket match, and the control circuitry monitors one or more of the scoreboard <b>1022</b>, audio commentary, ticker text or closed caption data to determine whether the player <b>1016</b> achieves that score. Using techniques similar to those described in relation to <figref idref="DRAWINGS">FIGS. <b>6</b>A, <b>6</b>B and <b>7</b></figref>, the player <b>1016</b> may be identified based on a name or nickname included in the comment <b>1020</b> or from identifying a portion of the display screen that the first user is gesturing towards.</p><p id="p-0089" num="0088">The user's comment <b>1020</b> may be relayed to other users in the shared viewing session, and those users may choose to disagree or agree with the first user <b>1002</b>. The user equipment devices of those users may then detect respective comments from those users regarding the outcome of the event and monitor the media content to determine whether the outcome is consistent with their opinions. This allows the first user <b>1002</b> to compete with other users in the viewing sessions, for example, by making bets relating to the media content. Alternatively, the first user <b>1002</b> may set up such a competition with another user, identifying the user by name or by pointing at their image <b>1012</b> in the gallery <b>1006</b> in a similar manner to that discussed above in relation to <figref idref="DRAWINGS">FIGS. <b>8</b>A, <b>8</b>B and <b>9</b></figref>.</p><p id="p-0090" num="0089"><figref idref="DRAWINGS">FIG. <b>10</b>B</figref> is an example of a display screen that might be displayed if the first user's comment <b>1020</b> is consistent with the outcome of the event. In this example, the outcome of the event may be determined based on detecting keywords in the audio commentary, ticker text and/or closed caption data. In response to detecting an outcome consistent with the comment <b>1020</b>, the control circuitry of the user equipment device <b>1000</b> generates for output one or more effects. In the examples shown in <figref idref="DRAWINGS">FIG. <b>10</b>B</figref>, the one or more effects include a visual effect, such as a celebratory message <b>1024</b> for presentation on the display screen, and an audio effect, such as an audio clip of cheering, to be output through a speaker <b>1030</b>. The control circuitry of the user equipment device <b>1000</b> may additionally transmit a message to other user equipment devices participating the shared viewing session so that an effect confirming that the first user's comment <b>1020</b> was correct can be provided to the other users.</p><p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. <b>10</b>C</figref> is an example of a display screen that might be generated for display by control circuitry of a user equipment device of another user who has predicted the outcome of the event incorrectly. In this example, a consolation message <b>1044</b>, icon or emoji is displayed and/or a corresponding audio effect played through a speaker <b>1046</b>. In embodiments where a message confirming the first user's successful prediction of the outcome is transmitted to the other user equipment devices, a visual or audio effect <b>1048</b> conveying the first user's success may be presented to the other user.</p><p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart of a procedure for providing a competition function such as that depicted in <figref idref="DRAWINGS">FIGS. <b>10</b>A-<b>10</b>C</figref>. Starting at step <b>1100</b>, the user equipment device <b>1000</b> joins a group watch session (step <b>1102</b>) and, if an enhanced interaction mode is activated (step <b>1102</b>), monitors the actions of the first user (step <b>1106</b>), for example, by analyzing audio detected by the microphone <b>1018</b>. In particular, the control circuitry of the user equipment device may process the detected audio to extract words from a comment <b>1020</b> by the first user <b>1002</b>.</p><p id="p-0093" num="0092">The control circuitry of the user equipment device <b>1000</b> then determines whether the detected audio includes a verbal cue relating to an outcome of a future event (step <b>1108</b>), for example, based on keywords such as players' names, scores, types of play, and times extracted from the detected audio. Step <b>1108</b> may also include determining whether the future event is one that can be verified by monitoring the media content. For example, the control circuitry of the user equipment device <b>1000</b> may have access to a database listing keywords associated with certain types of event and techniques for verifying the outcome of such events and may determine whether the outcome of the future event can be verified based on that information. For example, events such as final scores in a match, timing of certain events such as a first goal in a soccer match, identity of a player who scores the first or next goal may be monitored by, for example, monitoring keywords in closed caption data accompanying the media content, monitoring keywords in a commentary provided in an audio component of the media content, or detecting text in a scoreboard <b>1022</b> or player information in a video component of the media content.</p><p id="p-0094" num="0093">If the control circuitry of the user equipment device <b>1000</b> determines that the first user's comment <b>1020</b> is predicting an outcome of an event that can be verified in such a manner, the control circuitry then monitors the media content (step <b>1110</b>) to determine whether the first user's comment <b>1020</b> is consistent with the outcome of the event. In the example depicted in <figref idref="DRAWINGS">FIGS. <b>10</b>A, <b>10</b>B and <b>10</b>C</figref>, the outcome of the event can be determined based on detecting keywords such as the player's name, &#x201c;century&#x201d; or &#x201c;one hundred&#x201d; in the audio commentary and/or closed caption data or based on text displayed in the media content confirming that the player <b>1016</b> has scored a century, and keywords such as &#x201c;out&#x201d; to determine whether the player <b>1016</b> is out without having scored a century.</p><p id="p-0095" num="0094">In response to detecting an outcome consistent with the comment <b>1020</b> (step <b>1112</b>), the control circuitry of the user equipment device <b>1000</b> generates for output one or more effects corresponding to a successful bet or prediction (step <b>1114</b>). If, instead, it is determined that the first user's comment was not consistent with the outcome of the event, then one or more effects corresponding to an unsuccessful bet or prediction may be generated for display (step <b>1116</b>).</p><p id="p-0096" num="0095">Optionally, a message may be transmitted to other user equipment devices in the group watch session to confirm whether or not the first user's comment <b>1020</b> was consistent with the outcome (step <b>1118</b>), so that the other user equipment devices <b>1040</b> may display a corresponding message <b>1048</b>. As discussed above, the other users may have corresponding bets and predictions relating to the same event and may be competing with the first user <b>1002</b>, and the corresponding message <b>1048</b> may confirm the result of that competition.</p><p id="p-0097" num="0096">The control circuitry then returns to monitoring the first user's actions at step <b>1106</b> until either the enhanced mode is deactivated (step <b>1104</b>) or the viewing session finishes (step <b>1120</b>), ending the process (step <b>1122</b>).</p><p id="p-0098" num="0097">The foregoing description, for purposes of explanation, used specific nomenclature to provide a thorough understanding of the disclosure. However, it will be apparent to one skilled in the art that the specific details are not required to practice the methods and systems of the disclosure. For example, while <figref idref="DRAWINGS">FIGS. <b>4</b>, <b>5</b>, <b>7</b>, <b>9</b> and <b>11</b></figref> refer to a group watch session, the procedures set out in those flowcharts may be applied in other types of shared viewing activity, such as a videocall, videoconference, multi-player game or screen-sharing session. As another example, some of the embodiments described above include determining a portion of a display screen or a user to whom the first user's reaction relates by analyzing captured video of the first user but, in other embodiments, the user equipment device of the first user may include a touch-screen display, and the portion or user may be identified based on a position at which the first user touches the touch-screen display.</p><p id="p-0099" num="0098">The foregoing descriptions of specific embodiments of the present invention are, therefore, presented for purposes of illustration and description. They are not intended to be exhaustive or to limit the invention to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. The embodiments were chosen and described in order to best explain the principles of the invention and its practical applications, to thereby enable others skilled in the art to best utilize the methods and systems of the disclosure and various embodiments with various modifications as are suited to the particular use contemplated. Additionally, different features of the various embodiments, disclosed or otherwise, can be mixed and matched or otherwise combined so as to create further embodiments contemplated by the disclosure.</p><p id="p-0100" num="0099">This specification discloses embodiments which include, but are not limited to, the following:</p><p id="p-0101" num="0000">1. A method comprising:</p><p id="p-0102" num="0100">generating for display, using control circuitry of a first user equipment device, media content in a shared viewing session;</p><p id="p-0103" num="0101">monitoring, using the control circuitry, actions by a first user of the first user equipment device;</p><p id="p-0104" num="0102">detecting, using the control circuitry, a reaction of the first user based on the monitored actions;</p><p id="p-0105" num="0103">determining, using the control circuitry, a context of the reaction based on the media content and/or user profile information of the first user; and</p><p id="p-0106" num="0104">transmitting, to at least a second user equipment device participating in the shared viewing session, a message based on the reaction of the first user and the determined context.</p><p id="p-0107" num="0000">2. The method of embodiment 1, wherein monitoring the actions of the first user comprises:</p><p id="p-0108" num="0105">monitoring, based on output from one or more cameras, physical gestures by the first user; and/or monitoring, via an audio input device, sounds from the first user.</p><p id="p-0109" num="0000">3. The method of embodiment 2, wherein:</p><p id="p-0110" num="0106">monitoring the actions of the first user comprises determining, based on the output from one or more cameras, that the first user is pointing at a first portion of a display screen displaying the media content; and</p><p id="p-0111" num="0107">including, in the message, an indication of the first portion of the display screen or an indication of particular content shown in the first portion of the display screen.</p><p id="p-0112" num="0000">4. The method of embodiment 3, further comprising:</p><p id="p-0113" num="0108">receiving, using control circuitry of the second user equipment device, the message from the first user equipment device; and</p><p id="p-0114" num="0109">generating for output, using the control circuitry of the second user equipment device, a visual effect highlighting a second portion of the media content corresponding to the first portion indicated by the message.</p><p id="p-0115" num="0000">5. The method of embodiment 1, further comprising:</p><p id="p-0116" num="0110">receiving, using control circuitry of the second user equipment device, the message from the first user equipment device;</p><p id="p-0117" num="0111">generating for output, using the control circuitry of the second user equipment device, one or more of a visual effect, an audio effect or a haptic effect based on the message.</p><p id="p-0118" num="0000">6. The method of embodiment 1, further comprising:</p><p id="p-0119" num="0112">determining, using the control circuitry, at least one of a visual effect, an audio effect or a haptic effect corresponding to the reaction of the first user and the determined context; and</p><p id="p-0120" num="0113">including, in the message, an indication the at least one determined effect.</p><p id="p-0121" num="0000">7 The method of embodiment 5, further comprising:</p><p id="p-0122" num="0114">receiving, using control circuitry of the second user equipment device, the message from the first user equipment device;</p><p id="p-0123" num="0115">determining, using the control circuitry of the second user equipment device, based on the received message, at least one of a visual effect, an audio effect or a haptic effect corresponding to the reaction of the first user and the determined context; and</p><p id="p-0124" num="0116">generating for output, using the control circuitry of the second user equipment device, the at least one determined effect.</p><p id="p-0125" num="0000">8. The method of embodiment 2, further comprising:</p><p id="p-0126" num="0117">generating for display one or more images of respective other users participating in the shared viewing session;</p><p id="p-0127" num="0118">determining, based on the output from one or more cameras, that the first user is pointing at a third portion of a display screen displaying one of the one or more images; and</p><p id="p-0128" num="0119">determining that the image corresponds to a user of the second user equipment device; and</p><p id="p-0129" num="0120">wherein the message is transmitted to the second user equipment device in response to determining that the third portion of the display screen corresponds to the user of the second user equipment device.</p><p id="p-0130" num="0000">9. The method of embodiment 2, further comprising:</p><p id="p-0131" num="0121">determining, using the control circuitry, that sounds from the first user received via the audio input include a remark regarding a future event in the media content;</p><p id="p-0132" num="0122">monitoring, using the control circuitry, the media content to determine an outcome of the event;</p><p id="p-0133" num="0123">determining, using the control circuitry, whether the outcome of the event is consistent with the remark;</p><p id="p-0134" num="0124">generating for output, using the control circuitry receiving, an audio effect and/or a visual effect based on whether the outcome is consistent with the remark; and</p><p id="p-0135" num="0125">transmitting, to at least a second user equipment device participating in the shared viewing session, a message based on whether the outcome is consistent with the remark.</p><p id="p-0136" num="0000">10. The method of embodiment 4, wherein generating for output the effect comprises transmitting, to a haptic device, an instruction to provide a haptic effect.<br/>11. The method of embodiment 1, wherein the shared viewing session is a group watch session of live media content, a group watch session of on-demand video content, a videocall, a videoconference, a multi-player game or screen-sharing session.<br/>12. A non-transitory computer-readable medium on which are stored computer-readable instructions for:</p><p id="p-0137" num="0126">generating for display, using control circuitry of a first user equipment device, media content in a shared viewing session;</p><p id="p-0138" num="0127">monitoring, using the control circuitry, actions by a first user of the first user equipment device;</p><p id="p-0139" num="0128">detecting, using the control circuitry, a reaction of the first user based on the monitored actions;</p><p id="p-0140" num="0129">determining, using the control circuitry, a context of the reaction based on the media content and/or user profile information of the first user; and</p><p id="p-0141" num="0130">transmitting, to at least a second user equipment device participating in the shared viewing session, a message based on the reaction of the first user and the determined context.</p><p id="p-0142" num="0000">13. The computer-readable medium of embodiment 12, wherein the instructions for monitoring the actions of the first user comprise:</p><p id="p-0143" num="0131">instructions for monitoring, based on output from one or more cameras, physical gestures by the first user; and/or</p><p id="p-0144" num="0132">instructions for monitoring, via an audio input device, sounds from the first user.</p><p id="p-0145" num="0000">14. The computer-readable medium of embodiment 13, wherein:</p><p id="p-0146" num="0133">the instructions for monitoring the actions of the first user comprise instructions for determining, based on the output from one or more cameras, that the first user is pointing at a first portion of a display screen displaying the media content; and</p><p id="p-0147" num="0134">the instructions include instructions for including, in the message, an indication of the first portion of the display screen or an indication of particular content shown in the first portion of the display screen.</p><p id="p-0148" num="0000">15. The computer-readable medium of embodiment 12, further comprising:</p><p id="p-0149" num="0135">instructions for determining, using the control circuitry, at least one of a visual effect, an audio effect or a haptic effect corresponding to the reaction of the first user and the determined context; and</p><p id="p-0150" num="0136">instructions to include in the message an indication of the at least one determined effect.</p><p id="p-0151" num="0000">16. The computer-readable medium of embodiment 12, further comprising:</p><p id="p-0152" num="0137">instructions for generating for display one or more images of respective other users participating in the shared viewing session;</p><p id="p-0153" num="0138">instructions for determining, based on the output from one or more cameras, that the first user is pointing at a third portion of a display screen displaying one of the one or more images; and</p><p id="p-0154" num="0139">instructions for determining that the image corresponds to a user of the second user equipment device; and wherein the instructions for transmitting specify that the message is to be transmitted to the second user equipment device in response to determining that the third portion of the display screen corresponds to the user of the second user equipment device.</p><p id="p-0155" num="0000">17. The computer-readable medium of embodiment 13, further comprising:</p><p id="p-0156" num="0140">instructions for determining, using the control circuitry, that sounds from the first user received via the audio input include a remark regarding a future event in the media content;</p><p id="p-0157" num="0141">instructions for monitoring, using the control circuitry, the media content to determine an outcome of the event;</p><p id="p-0158" num="0142">instructions for determining, using the control circuitry, whether the outcome of the event is consistent with the remark;</p><p id="p-0159" num="0143">instructions for generating for output, using the control circuitry, an audio effect and/or a visual effect based on whether the outcome is consistent with the remark; and</p><p id="p-0160" num="0144">instructions for transmitting, to at least the second user equipment device, a message based on whether the outcome is consistent with the remark.</p><p id="p-0161" num="0000">18. The computer-readable medium of embodiment 12, wherein the computer readable instructions include instructions for participating in a shared viewing session that is a group watch session of live media content, a group watch session of on-demand video content, a videocall, a videoconference, a multi-player game or screen-sharing session.<br/>19. An apparatus comprising:</p><p id="p-0162" num="0145">a first user equipment device comprising control circuitry configured to:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0146">generate for display media content in a shared viewing session;</li>        <li id="ul0002-0002" num="0147">monitor actions by a first user of the first user equipment device;</li>        <li id="ul0002-0003" num="0148">detect a reaction of the first user based on the monitored actions;</li>        <li id="ul0002-0004" num="0149">determine a context of the reaction based on the media content and/or user profile information of the first user; and</li>        <li id="ul0002-0005" num="0150">transmit, to at least a second user equipment device participating in the shared viewing session, a message based on the reaction of the first user and the determined context.<br/>20. The apparatus of embodiment 19, further comprising:</li>    </ul>    </li></ul></p><p id="p-0163" num="0151">one or more cameras arranged to capture images of physical actions by the first user; and/or</p><p id="p-0164" num="0152">an audio input device configured to capture verbal cues from the first user.</p><p id="p-0165" num="0000">21. The apparatus of embodiment 20, wherein the control circuitry is further configured to:</p><p id="p-0166" num="0153">determine, based on the output from one or more cameras, that the first user is pointing at a first portion of a display screen displaying the media content; and</p><p id="p-0167" num="0154">include, in the message, an indication of the first portion of the display screen or an indication of particular content shown in the first portion of the display screen.</p><p id="p-0168" num="0000">22. A system comprising:</p><p id="p-0169" num="0155">the apparatus of embodiment 19; and</p><p id="p-0170" num="0156">the second user equipment device, comprising second control circuitry configured to:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0157">receive, using control circuitry of the second user equipment device, the message from the first user equipment device; and</li>        <li id="ul0004-0002" num="0158">generate for output, using the control circuitry of the second user equipment device, a visual effect highlighting a second portion of the media content corresponding to the first portion indicated by the message.<br/>23. The apparatus of embodiment 20, wherein the control circuitry is further configured to:</li>    </ul>    </li></ul></p><p id="p-0171" num="0159">generate for display one or more images of respective other users participating in the shared viewing session;</p><p id="p-0172" num="0160">determine, based on the output from one or more cameras, that the first user is pointing at a third portion of a display screen displaying one of the one or more images; and</p><p id="p-0173" num="0161">determine that the image corresponds to a user of the second user equipment device; and</p><p id="p-0174" num="0162">wherein the message is transmitted to the second user equipment device in response to determining that the third portion of the display screen corresponds to the user of the second user equipment device.</p><p id="p-0175" num="0000">24. A system comprising:</p><p id="p-0176" num="0163">the apparatus of embodiment 19; and</p><p id="p-0177" num="0164">a second apparatus comprising second control circuitry configured to receive the message from the first user equipment device and generate for output one or more of a visual effect, an audio effect or a haptic effect based on the message.</p><p id="p-0178" num="0000">25. The system of embodiment 24, further comprising:</p><p id="p-0179" num="0165">receiving, using control circuitry of the second user equipment device, the message from the first user equipment device;</p><p id="p-0180" num="0166">determining, using the control circuitry of the second user equipment device, based on the received message, the one or more of a visual effect, an audio effect or a haptic effect corresponding to the reaction of the first user and the determined context.</p><p id="p-0181" num="0000">26. The apparatus of embodiment 20, wherein the control circuitry is further configured to:</p><p id="p-0182" num="0167">determine that verbal cues from the first user received via the audio input include a remark regarding a future event in the media content;</p><p id="p-0183" num="0168">monitor the media content to determine an outcome of the event;</p><p id="p-0184" num="0169">determine, using the control circuitry, whether the outcome of the event is consistent with the remark;</p><p id="p-0185" num="0170">generate for output an audio effect and/or a visual effect based on whether the outcome is consistent with the remark; and</p><p id="p-0186" num="0171">transmit, to at least a second user equipment device participating in the shared viewing session, a message based on whether the outcome is consistent with the remark.</p><p id="p-0187" num="0000">27. A method comprising:</p><p id="p-0188" num="0172">generating for display, using control circuitry of a first user equipment device, media content in a shared viewing session;</p><p id="p-0189" num="0173">monitoring, using the control circuitry, actions by a first user of the first user equipment device;</p><p id="p-0190" num="0174">detecting, using the control circuitry, a reaction of the first user based on the monitored actions;</p><p id="p-0191" num="0175">determining, using the control circuitry, a context of the reaction based on the media content and/or user profile information of the first user; and</p><p id="p-0192" num="0176">transmitting, to at least a second user equipment device participating in the shared viewing session, a message based on the reaction of the first user and the determined context.<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0177">28. The method of embodiment 27, wherein monitoring the actions of the first user comprises:</li></ul></p><p id="p-0193" num="0178">monitoring, based on output from one or more cameras, physical gestures by the first user; and/or monitoring, via an audio input device, sounds from the first user.</p><p id="p-0194" num="0000">29. The method of embodiment 28, further comprising:</p><p id="p-0195" num="0179">generating for display one or more images of respective other users participating in the shared viewing session;</p><p id="p-0196" num="0180">determining, based on the output from one or more cameras, that the first user is pointing at a third portion of a display screen displaying one of the one or more images; and</p><p id="p-0197" num="0181">determining that the image corresponds to a user of the second user equipment device;</p><p id="p-0198" num="0182">wherein the message is transmitted to the second user equipment device in response to determining that the third portion of the display screen corresponds to the user of the second user equipment device.</p><p id="p-0199" num="0000">30. The method of embodiment 28, wherein:</p><p id="p-0200" num="0183">monitoring the actions of the first user comprises determining, based on the output from one or more cameras, that the first user is pointing at a first portion of a display screen displaying the media content; and</p><p id="p-0201" num="0184">including, in the message, an indication of the first portion of the display screen or an indication of particular content shown in the first portion of the display screen.</p><p id="p-0202" num="0000">31. The method of embodiment 29, further comprising:</p><p id="p-0203" num="0185">receiving, using control circuitry of the second user equipment device, the message from the first user equipment device; and</p><p id="p-0204" num="0186">generating for output, using the control circuitry of the second user equipment device, a visual effect highlighting a second portion of the media content corresponding to the first portion indicated by the message.</p><p id="p-0205" num="0000">32. The method of any of embodiments 27-31, further comprising:</p><p id="p-0206" num="0187">receiving, using control circuitry of the second user equipment device, the message from the first user equipment device;</p><p id="p-0207" num="0188">generating for output, using the control circuitry of the second user equipment device, one or more of a visual effect, an audio effect or a haptic effect based on the message.</p><p id="p-0208" num="0000">33. The method of any of embodiments 27-31, further comprising:</p><p id="p-0209" num="0189">determining, using the control circuitry, at least one of a visual effect, an audio effect or a haptic effect corresponding to the reaction of the first user and the determined context;</p><p id="p-0210" num="0190">wherein the message includes an indication of the at least one determined effect.</p><p id="p-0211" num="0000">34. The method of any of embodiments 27-30, further comprising:</p><p id="p-0212" num="0191">receiving, using control circuitry of the second user equipment device, the message from the first user equipment device;</p><p id="p-0213" num="0192">determining, using the control circuitry of the second user equipment device, based on the received message, at least one of a visual effect, an audio effect or a haptic effect corresponding to the reaction of the first user and the determined context; and</p><p id="p-0214" num="0193">generating for output, using the control circuitry of the second user equipment device, the at least one determined effect.</p><p id="p-0215" num="0000">35. The method of embodiment 28, further comprising:</p><p id="p-0216" num="0194">determining, using the control circuitry, that sounds from the first user received via the audio input include a remark regarding a future event in the media content;</p><p id="p-0217" num="0195">monitoring, using the control circuitry, the media content to determine an outcome of the event;</p><p id="p-0218" num="0196">determining, using the control circuitry, whether the outcome of the event is consistent with the remark;</p><p id="p-0219" num="0197">generating for output, using the control circuitry receiving, an audio effect and/or a visual effect based on whether the outcome is consistent with the remark; and</p><p id="p-0220" num="0198">transmitting, to at least a second user equipment device participating in the shared viewing session, a message based on whether the outcome is consistent with the remark.</p><p id="p-0221" num="0000">36. The method of embodiment 32 or 34, wherein generating for output the effect comprises transmitting, to a haptic device, an instruction to provide a haptic effect.<br/>37. The method of any of embodiments 27-36, wherein the shared viewing session is a group watch session of live media content, a group watch session of on-demand video content, a videocall, a videoconference, a multi-player game or screen-sharing session.<br/>38. A computer program comprising computer readable instructions that, when executed by processing circuitry, causes the processing circuitry to perform the method of any of embodiments 27-37.<br/>39. An apparatus comprising a first user equipment device including:</p><p id="p-0222" num="0199">means for generating for display media content in a shared viewing session;</p><p id="p-0223" num="0200">means for monitoring actions by a first user of the first user equipment device;</p><p id="p-0224" num="0201">means for detecting a reaction of the first user based on the monitored actions;</p><p id="p-0225" num="0202">means for determining a context of the reaction based on the media content and/or user profile information of the first user; and</p><p id="p-0226" num="0203">means for transmitting, to at least a second user equipment device participating in the shared viewing session, a message based on the reaction of the first user and the determined context.</p><p id="p-0227" num="0000">40. The apparatus of embodiment 39, further comprising:</p><p id="p-0228" num="0204">one or more cameras arranged to capture images of physical actions by the first user; and/or</p><p id="p-0229" num="0205">an audio input device configured to capture verbal cues from the first user.</p><p id="p-0230" num="0000">41. The apparatus of embodiment 40, wherein the first user equipment further comprises:</p><p id="p-0231" num="0206">means for determining, based on the output from one or more cameras, that the first user is pointing at a first portion of a display screen displaying the media content;</p><p id="p-0232" num="0207">means for including, in the message, an indication of the first portion of the display screen or an indication of particular content shown in the first portion of the display screen.</p><p id="p-0233" num="0000">42. The apparatus of embodiment 41, further comprising the second user equipment device, the second user equipment device including:<ul id="ul0006" list-style="none">    <li id="ul0006-0001" num="0000">    <ul id="ul0007" list-style="none">        <li id="ul0007-0001" num="0208">means for receiving the message from the first user equipment device; and</li>        <li id="ul0007-0002" num="0209">means for generating for output a visual effect highlighting a second portion of the media content corresponding to the first portion indicated by the message.<br/>43. The apparatus of embodiment 40 or 41, wherein the first user equipment device comprises:</li>    </ul>    </li></ul></p><p id="p-0234" num="0210">means for generating for display one or more images of respective other users participating in the shared viewing session;</p><p id="p-0235" num="0211">means for determining, based on the output from one or more cameras, that the first user is pointing at a third portion of a display screen displaying one of the one or more images; and</p><p id="p-0236" num="0212">means for determining that the image corresponds to a user of the second user equipment device;</p><p id="p-0237" num="0213">wherein the means for transmitting is configured to transmit the message to the second user equipment device in response to determining that the third portion of the display screen corresponds to the user of the second user equipment device.</p><p id="p-0238" num="0000">44. The apparatus of any of embodiments 39-43, further comprising:</p><p id="p-0239" num="0214">the second user equipment device, comprising means for receiving the message from the first user equipment device and means for generating for output one or more of a visual effect, an audio effect or a haptic effect based on the message.</p><p id="p-0240" num="0000">45. The apparatus of embodiment 44, wherein the second user equipment device further comprises:</p><p id="p-0241" num="0215">means for receiving the message from the first user equipment device; and</p><p id="p-0242" num="0216">means for determining, based on the received message, the one or more of a visual effect, an audio effect or a haptic effect corresponding to the reaction of the first user and the determined context.</p><p id="p-0243" num="0000">46. The apparatus of embodiment 40, further comprising:</p><p id="p-0244" num="0217">means for determining that verbal cues from the first user received via the audio input include a remark regarding a future event in the media content;</p><p id="p-0245" num="0218">means for monitoring the media content to determine an outcome of the event;</p><p id="p-0246" num="0219">means for determining, using the control circuitry, whether the outcome of the event is consistent with the remark;</p><p id="p-0247" num="0220">means for generating for output an audio effect and/or a visual effect based on whether the outcome is consistent with the remark; and</p><p id="p-0248" num="0221">means for transmitting, to at least the second user equipment device, a message based on whether the outcome is consistent with the remark.</p><p id="p-0249" num="0000">47. The apparatus of any of embodiments 39-46, wherein the shared viewing session is a group watch session of live media content, a group watch session of on-demand video content, a videocall, a videoconference, a multi-player game or screen-sharing session.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>generating for display, using control circuitry of a first user equipment device, media content in a shared viewing session;</claim-text><claim-text>monitoring, using the control circuitry, actions by a first user of the first user equipment device;</claim-text><claim-text>detecting, using the control circuitry, a reaction of the first user based on the monitored actions;</claim-text><claim-text>determining, using the control circuitry, a context of the reaction based on the media content and/or user profile information of the first user; and</claim-text><claim-text>transmitting, to at least a second user equipment device participating in the shared viewing session, a message based on the reaction of the first user and the determined context.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein monitoring the actions of the first user comprises:<claim-text>monitoring, based on output from one or more cameras, physical gestures by the first user; and/or</claim-text><claim-text>monitoring, via an audio input device, sounds from the first user.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein:<claim-text>monitoring the actions of the first user comprises determining, based on the output from one or more cameras, that the first user is pointing at a first portion of a display screen displaying the media content; and</claim-text><claim-text>including, in the message, an indication of the first portion of the display screen or an indication of particular content shown in the first portion of the display screen.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising:<claim-text>receiving, using control circuitry of the second user equipment device, the message from the first user equipment device; and</claim-text><claim-text>generating for output, using the control circuitry of the second user equipment device, a visual effect highlighting a second portion of the media content corresponding to the first portion indicated by the message.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>receiving, using control circuitry of the second user equipment device, the message from the first user equipment device;</claim-text><claim-text>generating for output, using the control circuitry of the second user equipment device, one or more of a visual effect, an audio effect or a haptic effect based on the message.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>determining, using the control circuitry, at least one of a visual effect, an audio effect or a haptic effect corresponding to the reaction of the first user and the determined context;</claim-text><claim-text>wherein the message includes an indication of the at least one determined effect.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising:<claim-text>receiving, using control circuitry of the second user equipment device, the message from the first user equipment device;</claim-text><claim-text>determining, using the control circuitry of the second user equipment device, based on the received message, at least one of a visual effect, an audio effect or a haptic effect corresponding to the reaction of the first user and the determined context; and</claim-text><claim-text>generating for output, using the control circuitry of the second user equipment device, the at least one determined effect.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:<claim-text>generating for display one or more images of respective other users participating in the shared viewing session;</claim-text><claim-text>determining, based on the output from one or more cameras, that the first user is pointing at a third portion of a display screen displaying one of the one or more images; and</claim-text><claim-text>determining that the image corresponds to a user of the second user equipment device;</claim-text><claim-text>wherein the message is transmitted to the second user equipment device in response to determining that the third portion of the display screen corresponds to the user of the second user equipment device.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:<claim-text>determining, using the control circuitry, that sounds from the first user received via the audio input include a remark regarding a future event in the media content;</claim-text><claim-text>monitoring, using the control circuitry, the media content to determine an outcome of the event;</claim-text><claim-text>determining, using the control circuitry, whether the outcome of the event is consistent with the remark;</claim-text><claim-text>generating for output, using the control circuitry receiving, an audio effect and/or a visual effect based on whether the outcome is consistent with the remark; and</claim-text><claim-text>transmitting, to at least a second user equipment device participating in the shared viewing session, a message based on whether the outcome is consistent with the remark.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein generating for output the effect comprises transmitting, to a haptic device, an instruction to provide a haptic effect.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the shared viewing session is a group watch session of live media content, a group watch session of on-demand video content, a videocall, a videoconference, a multi-player game or screen-sharing session.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A non-transitory computer-readable medium on which are stored computer-readable instructions for performing the method of <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. An apparatus comprising:<claim-text>control circuitry configured to:<claim-text>generate for display media content in a shared viewing session;</claim-text><claim-text>monitor actions by a first user of the first user equipment device;</claim-text><claim-text>detect a reaction of the first user based on the monitored actions;</claim-text><claim-text>determine a context of the reaction based on the media content and/or user profile information of the first user; and</claim-text><claim-text>transmit, to at least a user equipment device participating in the shared viewing session, a message based on the reaction of the first user and the determined context.</claim-text></claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:<claim-text>one or more cameras arranged to capture images of physical actions by the first user; and/or</claim-text><claim-text>an audio input device configured to capture verbal cues from the first user.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The apparatus of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the control circuitry is further configured to:<claim-text>determine, based on the output from one or more cameras, that the first user is pointing at a first portion of a display screen displaying the media content; and</claim-text><claim-text>include, in the message, an indication of the first portion of the display screen or an indication of particular content shown in the first portion of the display screen.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A system comprising:<claim-text>the apparatus of <claim-ref idref="CLM-00015">claim 15</claim-ref>; and</claim-text><claim-text>the user equipment device, comprising second control circuitry configured to:<claim-text>receive, using control circuitry of the user equipment device, the message from the first user equipment device; and</claim-text><claim-text>generate for output, using the control circuitry of the apparatus, a visual effect highlighting a second portion of the media content corresponding to the first portion indicated by the message.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The apparatus of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the control circuitry is further configured to:<claim-text>generate for display one or more images of respective other users participating in the shared viewing session;</claim-text><claim-text>determine, based on the output from one or more cameras, that the first user is pointing at a third portion of a display screen displaying one of the one or more images; and</claim-text><claim-text>determine that the image corresponds to a user of the user equipment device;</claim-text><claim-text>wherein the means for transmitting is configured to transmit the message to the user equipment device in response to determining that the third portion of the display screen corresponds to the user of the user equipment device.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:<claim-text>the user equipment device, comprising second control circuitry configured to receive the message and generate for output one or more of a visual effect, an audio effect or a haptic effect based on the message.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The apparatus of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the second control circuitry is further configured to:<claim-text>receive, using the second control circuitry, the message from the first user equipment device; and</claim-text><claim-text>determine, using the control circuitry of the second user equipment device, based on the received message, the one or more of a visual effect, an audio effect or a haptic effect corresponding to the reaction of the first user and the determined context.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The apparatus of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the control circuitry is further configured to:<claim-text>determine that verbal cues from the first user received via the audio input include a remark regarding a future event in the media content;</claim-text><claim-text>monitor the media content to determine an outcome of the event;</claim-text><claim-text>determine, using the control circuitry, whether the outcome of the event is consistent with the remark;</claim-text><claim-text>generate for output an audio effect and/or a visual effect based on whether the outcome is consistent with the remark; and</claim-text><claim-text>transmit, to at least the user equipment device participating in the shared viewing session, a message based on whether the outcome is consistent with the remark.</claim-text></claim-text></claim></claims></us-patent-application>