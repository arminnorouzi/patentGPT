<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007005A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007005</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17366209</doc-number><date>20210702</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>29</main-group><subgroup>06</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>63</main-group><subgroup>102</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>63</main-group><subgroup>0815</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>63</main-group><subgroup>0272</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>63</main-group><subgroup>0421</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>2463</main-group><subgroup>082</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEMS AND METHODS FOR ANONYMOUS PASS-PHRASE AUTHENTICATION</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>VERIZON PATENT AND LICENSING INC.</orgname><address><city>Basking Ridge</city><state>NJ</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KONERU</last-name><first-name>Rahul</first-name><address><city>Ashburn</city><state>VA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>VERIZON PATENT AND LICENSING INC.</orgname><role>02</role><address><city>Basking Ridge</city><state>NJ</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Disclosed are systems and methods for anonymous, hands-free voice authentication to network resources. The framework can operate and provide a secure authenticated operating environment for any type of computerized platform, device and/or service while preserving anonymity of both the user and the user's login credentials. Once authenticated, the user is then permitted to perform desired operations, such as, CRUD (create, read, update, delete) operations. The disclosed framework operates in a three stage process, which involves single-sign on (SSO)/virtual private network (VPN) connectivity, which is then followed by a 4way voice matching user-device integrated &#x201c;conversation&#x201d; and a proof of work macro-micro problem verification step. The framework enables a user to login and access a system by responding to randomly verifiable requests output by the system dependent on the user's current surroundings.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="91.36mm" wi="158.75mm" file="US20230007005A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="179.32mm" wi="140.38mm" orientation="landscape" file="US20230007005A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="111.51mm" wi="134.79mm" orientation="landscape" file="US20230007005A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="246.30mm" wi="144.86mm" file="US20230007005A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="206.16mm" wi="149.35mm" orientation="landscape" file="US20230007005A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="138.68mm" wi="101.52mm" file="US20230007005A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="231.06mm" wi="158.24mm" file="US20230007005A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND INFORMATION</heading><p id="p-0002" num="0001">Electronic devices house and enable access to sensitive and securely held data. This data, for example, can include personally identifiable data (PID) of a user that can be located on the user's device or accessible via the device's secure connections to network resources. The data can also include, but is not limited to, privileged, privately held and/or classified information related to the resources the device is accessing, which can relate to other users and/or the applications or enterprises hosting and/or facilitating access to such resources.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0002" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0003" num="0002">The features, and advantages of the disclosure will be apparent from the following description of embodiments as illustrated in the accompanying drawings, in which reference characters refer to the same parts throughout the various views. The drawings are not necessarily to scale, emphasis instead being placed upon illustrating principles of the disclosure:</p><p id="p-0004" num="0003"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating an example of a network configuration within which the systems and methods disclosed herein could be implemented according to some embodiments of the present disclosure;</p><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts a non-limiting example embodiment of authentication engine <b>200</b> according to some embodiments of the present disclosure;</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a non-limiting example of a work flow performed by authentication engine <b>200</b> according to some embodiments of the present disclosure;</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a non-limiting example of audible information matching performed by authentication engine <b>200</b> according to some embodiments of the present disclosure;</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram of an example network architecture according to some embodiments of the present disclosure; and</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram illustrating a computing device used in various embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0003" level="1">DETAILED DESCRIPTION OF EXAMPLE EMBODIMENTS</heading><p id="p-0010" num="0009">The disclosed systems and methods provide a novel framework for anonymous authentication. The framework can provide a secure authenticated operating environment for any type of computerized platform, device and/or service. For example, the framework can enable authenticated sessions for users in and/or on, but not limited to, artificial intelligence (AI) chat bots, web-portals, voice enabled mobile devices, internet-of-things (IoT) devices, smart cars, drones and any other type of smart device (e.g., smart speakers, smart digital assistants and voice enabled virtual assistants powered by AI technology), and the like.</p><p id="p-0011" num="0010">The disclosed framework provides systems and methods that enable user authentication while preserving anonymity of both the user and the login credentials (e.g., username, password, biometrics, PINs, and the like) that are used. Once authenticated, the user is then permitted to perform desired operations such as, but not limited to, CRUD (create, read, update, delete) operations on an enterprise system or an IoT device, for example.</p><p id="p-0012" num="0011">According to some embodiments, the framework operates in a three-stage process. The Stages include 1) a single sign-on (SSO) and virtual private network (VPN) stage; 2) a &#x201c;4way&#x201d; match stage; and 3) a validation stage. In some embodiments, as discussed below, the framework can be executed by an authentication engine <b>200</b> (of <figref idref="DRAWINGS">FIG. <b>2</b></figref>), whereby Stage 1 can be executed by SSO/VPN module <b>202</b>, Stage 2 by pass-phrase module <b>204</b>, and Stage 3 by Anonymous Voice Authentication (AVA) module <b>206</b>. In some embodiments, as discussed below, Stage 3 (and in some embodiments, Stage 2) may additionally involve execution of the multi-factor authentication (MFA) module <b>208</b>. Configurations, operating environments and network integration of engine <b>200</b> are depicted in <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>2</b></figref>, and discussed below in more detail.</p><p id="p-0013" num="0012">According to some embodiments, the framework can involve operations performed on a user's device that enable authentication of the user's identity. In some embodiments, each of the three-stages can be executed by engine <b>200</b> operating on the user's device. In some embodiments, the framework can be configured as a web-based application or executable file (or extension) that a user device accesses over a network in order for each (or at least a portion of) the three-stages to be performed. In some embodiments, the framework (e.g., engine <b>200</b>) can be hosted by a network resource (e.g., a server) that provides networked authentication capabilities.</p><p id="p-0014" num="0013">As discussed in more detail below, Stage 1, the initial stage, is where a user provides SSO credentials to establish a VPN connection in order to begin the authentication processing discussed herein. In some embodiments, the SSO/VPN stage occurs at the device level. For example, the user can &#x201c;login&#x201d; or gain access to his/her device, whereby Stage 1 is presumed cleared and processing proceeds to Stage 2.</p><p id="p-0015" num="0014">In some embodiments, Stage 2 involves 4way matching where voice pass-phrases are matched and authenticated. In some embodiments, a sequential combination of a predetermined number of alternating spoken and audibly output pass-phrases (or keys or terms, used interchangeably) are utilized to confirm the user's &#x201c;proof of identity&#x201d; (POI), i.e., they &#x201c;are who they are representing they are.&#x201d; As discussed below, the pass-phrases can be predefined and according to an order that must be spoken by the user and/or provided by the framework in order for POI to be confirmed.</p><p id="p-0016" num="0015">In some embodiments, as discussed herein, 4 pass-phrases can be utilized, but this should not be construed as limiting, as a variation of the number of exchanged phrases can also and/or alternatively be utilized, depending on an operating environment of the user, and/or user, device, application, resource and/or framework preferences.</p><p id="p-0017" num="0016">As discussed in more detail below according to some embodiments, 4way matching involves the interplay between spoken terms by a user and audibly output terms by an AI model's natural language processing (NLP) layer of the framework and/or of a service provider (e.g., Alexa&#xae; from Amazon&#xae; or Siri&#xae; from Apple&#xae;, for example). 4way matching includes an initial phrase being spoken by the user (referred to as an &#x201c;invocation word&#x201d;), whereby the framework audibly responds with its counterpart, then the framework audibly outputs another phrase which has its own counterpart term that the user must provide, for example by speaking it (referred to as a &#x201c;closing word&#x201d;).</p><p id="p-0018" num="0017">According to embodiments of the instant disclosure, the invocation word provided (or input) by the user provides an indication as to the type of &#x201c;scene&#x201d; the user is operating in. That is, according to some embodiments, the disclosed framework is adaptable for operating in different types of real-world and/or digital environments (e.g., referred to as &#x201c;scenes&#x201d; as discussed in more detail below). In some embodiments, three types of scenes can exist: public, semi-public and private.</p><p id="p-0019" num="0018">In some embodiments, a public scene refers to situations where a user is attempting to login to a secure resource while in public (e.g., at the airport, at the mall, or any other location where there is a majority of strangers in a proximity that could adversely hear and/or obtain login credentials). In some embodiments, a semi-public scene refers to a family setting, or a setting where a user is physically located proximate to friends, family and/or other acquaintances where the risk of the user's credentials being pirated are relatively low (e.g., driving in a car with a family member or friend, or having dinner at home with a guest). In some embodiments, a private scene refers to a setting where no-one else is remotely around (e.g., driving in a car solo, being at home alone, being in an office alone, and the like). In exemplary embodiments, scenes may be configurable by users.</p><p id="p-0020" num="0019">Therefore, for example, according to some embodiments as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, item <b>402</b> (and discussed in more detail below), after passing Stage 1 (e.g., gaining access to his/her device), the user is prompted and speaks the phrase &#x201c;It's raining&#x201d;. This indicates to the framework that the user is in &#x201c;Scene 1&#x201d; (e.g., a public scene, for example). In response, the framework audibly outputs &#x201c;warm&#x201d;; the framework then audibly outputs &#x201c;coffee&#x201d;, whereby the proper response (e.g., closing word) to be spoken by the user is &#x201c;tea.&#x201d; This &#x201c;request-receive&#x201d; back-and-forth exchange between the user and the framework confirms POI for the user and enables processing to proceed to Stage 3. Moreover, the processing of Stage 2 ensures that the application system the user is intending to interact with is the same system he/she is interacting with.</p><p id="p-0021" num="0020">It should be understood that while the discussion herein focuses on words or phrases being spoken or otherwise output to the user, it should not be construed as limiting, as any other type of information can be audibly or otherwise received and/or output to the user or entered by the user (e.g., numbers, sounds, shapes, haptic effects and the like) without departing from the scope of the instant disclosure.</p><p id="p-0022" num="0021">Once the Stage 2 is concluded (e.g., the user provides the proper closing word), Stage 3 is triggered, which involves validation of the user's &#x201c;proof of work&#x201d; (POW) via an Anonymous Voice Authentication (AVA) model (e.g., AVA module <b>206</b> of authentication engine <b>200</b>, as discussed below).</p><p id="p-0023" num="0022">According to some embodiments, the AVA model can include a series or set of mathematical, binary and/or alpha (macro) patterns. The patterns can be predefined and/or preselected by a user, and in some embodiments, can be stored in designated AVA configuration tables associated with and/or accessible by an application or portal, and engine <b>200</b>.</p><p id="p-0024" num="0023">Each macro pattern corresponds to a type of scene. In some embodiments, a scene can be mapped to a number of micro patters to ensure anonymity and to avoid any risk of potential pattern decoding by intruders. For example, as discussed in more detail below, a simple mathematical POW pattern can correspond to a private scene, whereas a complex binary POW pattern can corresponds to a public scene.</p><p id="p-0025" num="0024">Each macro pattern includes a set of micro patterns that engine <b>200</b> can randomly select for solving by the user, as discussed below. For example, as discussed below, mathematical micro patterns can include, but are not limited to, index based additions, subtractions, multiplications, squares, exponents, simple equations and the like. Examples of binary micro patterns can include, but are not limited to, adding, multiplying and dividing binary numbers, and the like. Examples of alpha micro patterns can include, but are not limited to, string shifting and string substitutions, and the like. These patterns represent POW problems that the user must solve (or provide the correct solution or response to) in order for the user's POW to be validated.</p><p id="p-0026" num="0025">According to some embodiments, each micro pattern has a predefined manipulation or calculation associated with it. For example, if the micro pattern is for addition, the user can set a value of 2; therefore, when presented with a number &#x201c;10&#x201d;, for example, the user's addition of &#x201c;2&#x201d; resulting in &#x201c;12&#x201d; provides a correct answer thereby confirming the user's POW. In some embodiments, the manipulation or calculation can be set by the user, as mentioned above. Therefore, when a user is presented with a micro pattern, how it is solved can be anticipated despite the micro pattern being randomly selected by engine <b>200</b>.</p><p id="p-0027" num="0026">By way of a non-limiting example, a POW problem pattern can provide the user with a set of decimal numbers whereby the user is expected to perform decimal addition of a decimal value. In another example, a binary value can be provided whereby binary multiplication is expected/request. Examples of an addition pattern are provided in the below table:</p><p id="p-0028" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="119pt" align="left"/><colspec colname="1" colwidth="28pt" align="center"/><colspec colname="2" colwidth="70pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="2" rowsep="1">TABLE 1</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry>Binary</entry><entry>Decimal</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="offset" colwidth="21pt" align="left"/><colspec colname="1" colwidth="98pt" align="left"/><colspec colname="2" colwidth="28pt" align="char" char="."/><colspec colname="3" colwidth="70pt" align="char" char="."/><tbody valign="top"><row><entry/><entry>Random AVA input:</entry><entry>1010</entry><entry>10</entry></row><row><entry/><entry>Value to be added:</entry><entry>0011</entry><entry>3</entry></row><row><entry/><entry>Expected user input:</entry><entry>1101</entry><entry>13</entry></row><row><entry/><entry namest="offset" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0029" num="0027">Table 1 provides &#x201c;Random AVA input&#x201d; which is the randomly selected micro pattern for binary and decimal macro patterns. &#x201c;Value to be added&#x201d; corresponds to the predetermined manipulation/calculation value set by the user; and &#x201c;expected user input&#x201d; corresponds to the correct answer engine <b>200</b> expects to receive in order to confirm POW.</p><p id="p-0030" num="0028">In Table 1's example, the AVA model can output the binary value of 1010, and the user is expected to provide the value resulting from the addition of binary 0011. Similarly, the AVA model can provide the decimal value of 10, whereby the user is expected to provide the solution resultant from adding decimal 3. As mentioned above, in some embodiments, the value being added is the value the user predetermines, thereby a random value being provided by the AVA model can be used for POW since the expected result/solution provided by the user can be checked/confirmed against the user's predetermined value being received. In some embodiments, for any input generated randomly by the AVA model, there may be 1 or more acceptable POW values, as the user can apply 1 or more micro patterns and all the valid responses will be computed and accepted by the AVA model.</p><p id="p-0031" num="0029">In another example, a POW problem pattern can involve an alpha pattern shift of places. For example, as illustrated in the below table:</p><p id="p-0032" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="105pt" align="left"/><colspec colname="1" colwidth="112pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="1" rowsep="1">TABLE 2</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row><row><entry/><entry>Alpha Pattern</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="28pt" align="left"/><colspec colname="1" colwidth="77pt" align="left"/><colspec colname="2" colwidth="112pt" align="center"/><tbody valign="top"><row><entry/><entry>Random AVA input:</entry><entry>ACB</entry></row><row><entry/><entry>Value to be added:</entry><entry>222</entry></row><row><entry/><entry>Expected user input:</entry><entry>CED</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0033" num="0030">Table 2 provides &#x201c;Random AVA input&#x201d; which is the randomly selected micro pattern for alpha patterns. &#x201c;Value to be added&#x201d; corresponds to the predetermined manipulation set by the user; and &#x201c;expected user input&#x201d; corresponds to the correct alpha-shifting engine <b>200</b> expects to receive in order to confirm POW.</p><p id="p-0034" num="0031">In Table 2's example, the AVA model randomly provided the user with the character string &#x201c;ACB&#x201d;. It is expected (from, for example, the user's predefined setting) that the output will shift each character in the 3 character string 2 places&#x2014;&#x201c;222&#x201d;. That is, for example the user can predefine that when presented with a 3 character string, each character will shift 2 places (e.g., 222). In some embodiments, a user setting can indicate any length of string is to be shifted n places. Therefore, in this example, the correct alpha-shifting of the &#x201c;ACB&#x201d; input is &#x201c;CED&#x201d;, where the &#x201c;A&#x201d; shifts 2 characters to the &#x201c;C&#x201d;, the &#x201c;C&#x201d; shifts 2 characters to the &#x201c;E&#x201d;, and the &#x201c;B&#x201d; shifts 2 characters to the &#x201c;D&#x201d;. Another non-limiting example can be scrambling or swapping 1 or more positions of the string of characters or numbers.</p><p id="p-0035" num="0032">With reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, system <b>100</b> is depicted which includes user equipment (UE) <b>502</b>, network <b>102</b>, cloud system <b>104</b> and authentication engine <b>200</b>. UE <b>502</b> can be any type of device, such as, but not limited to, a mobile phone, tablet, laptop, sensor, IoT device, autonomous machine, and any other device equipped with a cellular or wireless or wired transceiver. Further discussion of UE <b>502</b> is provided below in reference to <figref idref="DRAWINGS">FIGS. <b>5</b>-<b>6</b></figref>.</p><p id="p-0036" num="0033">Network <b>102</b> can be any type of network, such as, but not limited to, a wireless network, cellular network, the Internet, and the like. Network <b>102</b> facilitates connectivity of the components of system <b>100</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. A further discussion of the network configuration and type of network is provided below in reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0037" num="0034">Cloud system <b>104</b> can be any type of cloud operating platform and/or network based system upon which operations, applications, and/or other forms of network resources can be located. For example, system <b>104</b> can be a third party service provider and/or network provider. In some embodiments, cloud system <b>104</b> can include a server(s) and/or a database of information which is accessible over network <b>102</b>, whereby such access is granted via the authentication processing discussed herein. In some embodiments, a database (not shown) of cloud system <b>104</b> can store a dataset of data and metadata associated with local and/or network information related to a user(s) of UE <b>502</b> and the UE <b>502</b>, and the services, applications, content rendered and/or executed by UE <b>502</b>.</p><p id="p-0038" num="0035">In some embodiments, as discussed above and in more detail below, system <b>104</b> can provide AI NLP (e.g., host or provide access to the NLP layer) upon which AVA processing is performed. In some embodiments, system <b>104</b> can integrate and/or connect with another cloud or network-based system that provides AI NLP for AVA processing, as discussed below.</p><p id="p-0039" num="0036">Authentication engine <b>200</b>, as discussed above, includes components for performing the 3 Stage authentication processing discussed herein. Authentication engine <b>200</b> can be a special purpose machine or processor and could be hosted by UE <b>502</b>. In some embodiments, engine <b>200</b> can be hosted by a peripheral device connected to UE <b>502</b>.</p><p id="p-0040" num="0037">According to some embodiments, as discussed above, engine <b>200</b> can function as an application installed on UE <b>502</b>. In some embodiments, such application can be a web-based application accessed by UE <b>502</b> over network <b>102</b> (e.g., as indicated by the connection between network <b>102</b> and engine <b>200</b>, and/or the dashed line between cloud system <b>104</b> and engine <b>200</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>). In some embodiments, engine <b>200</b> can be configured and/or installed as an augmenting script, program or application (e.g., a plug-in or extension) to another application and/or AI/ML model.</p><p id="p-0041" num="0038">According to some embodiments, authentication engine <b>200</b> includes SSO/VPN module <b>202</b>, pass-phrase module <b>204</b>, AVA module <b>206</b> and MFA module <b>208</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. It should be understood that the engine(s) and modules discussed herein are non-exhaustive, as additional or fewer engines and/or modules (or sub-modules) may be applicable to the embodiments of the systems and methods discussed. More detail of the operations, configurations and functionalities of engine <b>200</b> and each of its modules, and their role within embodiments of the present disclosure will be discussed below in relation to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0042" num="0039"><figref idref="DRAWINGS">FIG. <b>3</b></figref> provides Process <b>300</b> which details non-limiting example embodiments of an authentication process for a user requesting access to a network resource (e.g., an enterprise platform, for example). According to some embodiments, Steps <b>302</b>-<b>304</b> of Process <b>300</b> can be performed by SSO/VPN module <b>202</b> of authentication engine <b>200</b>; Steps <b>306</b>-<b>308</b> can be performed by pass-phrase module <b>204</b>; Steps <b>310</b>-<b>318</b> can be performed by AVA module <b>206</b>; and Steps <b>320</b>-<b>322</b> can be performed by MFA module <b>208</b>.</p><p id="p-0043" num="0040">Process <b>300</b> begins with Step <b>302</b> where SSO authentication for a user is performed. As discussed above, this can involve the user logging in to his/her device and/or another account, which can be associated with the user's device and/or a network resource. For example, unlocking the device by receiving a PIN or biometric input.</p><p id="p-0044" num="0041">In Step <b>304</b>, a VPN connection over a network (e.g., network <b>102</b>) is established based on the SSO log-in from Step <b>302</b>. The VPN connection by the user's device (e.g., UE <b>502</b>) enables an encrypted virtual connection over network <b>104</b> which ensures the data communicated during the subsequent steps of Process <b>300</b> (e.g., steps related to Stages 2 and 3) are secure and protected. In some embodiments, the VPN connection enables a network connection with a desired resource (or platform). For example, if a user is desiring to login to his/her enterprise mail, a VPN connection can be established with that system (e.g., system <b>104</b>) over a network (e.g., network <b>102</b>), from which the POI and POW steps can be executed, as discussed below.</p><p id="p-0045" num="0042">In Step <b>306</b>, engine <b>200</b> can prompt the user for input of the invocation word, as discussed above. Such prompt can be an audible request or can be displayed on the user's device. As discussed above, the invocation word commences the 4way matching (Stage 2) of the authentication process. In some embodiments, in a case of a digital assistant using a voice channel, the user's device may be in a listening mode waiting for the user to speak an invocation word.</p><p id="p-0046" num="0043">According to some embodiments, invocation words are key words from a multitude of predefined private lists of pass-phrases. Each list corresponds to a type of scene, as discussed herein. Invocation words are the initial terms in a predetermined sequence of terms that are alternatively spoken and/or output by the user and/or engine <b>200</b>, respectively.</p><p id="p-0047" num="0044">In Step <b>308</b>, engine <b>200</b> determines the scene of the user. The processing operations by engine <b>200</b> for Step <b>308</b> involve the reception of the invocation word, the identification of the pass-phrase list for a particular scene (that includes the invocation word), then the iterative &#x201c;request-receive&#x201d; operation of audibly outputting and receiving pass-phrase terms from the user.</p><p id="p-0048" num="0045">For example, turning now to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, example <b>400</b> includes Scene 1, item <b>402</b> (discussed above); Scene 2, item <b>404</b>; and Scene 3, item <b>406</b>. Each scene can correspond to a type of operating environment/setting of the user. For example, Scene 1 can correspond to a list of pass-phrases to use in a public setting; Scene 2 can be used for semi-public settings, and Scene 3 can correspond to a private setting.</p><p id="p-0049" num="0046">The notation of the 4way matching involves the 4 keys being shared between a user and engine <b>200</b>. As depicted in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, UK1 is a user provided key, and represents the invocation word. Sk1 is a system key (e.g., word or phrase) that is provided by engine <b>200</b> in response to receiving UK1. In some embodiments, Sk1 can be stored in a cloud NLP layer. Sk2 is a second system key (e.g., word or phrase) that is audibly output by engine <b>200</b>. In some embodiments, Sk2 can be stored and retrieved from the platform that is being attempted to be accessed (e.g., an enterprise system). And, Uk2 is another user provided key that is spoken by the user in response to receiving (e.g., hearing) Sk2. Uk2 represents a &#x201c;closing word&#x201d; that concludes the 4way pass-phrase matching.</p><p id="p-0050" num="0047">By way of a non-limiting example depicted as item <b>404</b>, Scene 2, a user speaks &#x201c;Snowy&#x201d;. This is used by engine <b>200</b> to identify the list of terms &#x201c;Snowy, winter, morning, shovel&#x201d;. This also indicates to engine <b>200</b> that the scene, for example, is semi-public (e.g., a family setting). In response, engine <b>300</b> audibly outputs &#x201c;winter&#x201d;, then audibly outputs &#x201c;morning&#x201d;, whereby the user is expected to respond with &#x201c;shovel&#x201d;.</p><p id="p-0051" num="0048">In another non-limiting example depicted as item <b>406</b>, Scene 3, a user speaks or enters &#x201c;Very&#x201d;. This is used by engine <b>200</b> to identify the list of terms &#x201c;Very, sunny, day, coke&#x201d;. This also indicates to engine <b>200</b> that the scene, for example, is private (e.g., driving alone in a car). In response, engine <b>300</b> audibly outputs &#x201c;sunny&#x201d;, then audibly outputs &#x201c;day&#x201d;, whereby the user is expected to respond with or enter &#x201c;coke&#x201d;.</p><p id="p-0052" num="0049">In some embodiments, if the user's closing word is not correct, MFA processing can be triggered and performed in a similar manner as discussed below in relation to Step <b>320</b>. In some embodiments, in addition to or alternatively to MFA, the system can request another invocation word to perform the processing of Steps <b>306</b>-<b>308</b> again. In some embodiments, if the user fails the 4way matching and/or MFA a predetermined number of times, then the user's authentication request is denied. In some embodiments, the VPN connection is disconnected, yet the SSO login by the user can remain.</p><p id="p-0053" num="0050">In some embodiments, when engine <b>200</b> receives the invocation word (in Step <b>306</b>), engine <b>200</b> analyzes the stored set of pass-phrase lists, and identifies a list that begins with the invocation word. This also provides an indication of the type of scene. Engine <b>200</b> can then retrieve information that indicates the next three keys that are to be provided. In some embodiments, the information indicates the location for retrieval of Sk1 and Sk2. Upon receiving Uk2 (e.g., closing word), engine <b>200</b> confirms the POI of the user and the type of scene and proceeds to Step <b>310</b>.</p><p id="p-0054" num="0051">In some embodiments, the determination of a type of scene can also or alternatively be based a type of network the user is operating on or connected to, and/or the signal characteristics of that network. For example, if the user is connected to a public Wi-Fi network, then this can indicate the user is in a public or semi-public setting.</p><p id="p-0055" num="0052">In some embodiments, the determination of a type of scene can also or alternatively be based on a derived (or determined) risk. In some embodiments, the derived risk can be determined by engine <b>200</b> based on the scene opted by the user (e.g., via the invocation word) and the data being requested access to. For example, if a user indicates that she is in a public setting, but wishes to access privileged information, this can indicate and/or result in a high risk to the privileged data. Therefore, a derived risk computation can be performed based on the type of scene the user is operating in and the type of data the user is requesting.</p><p id="p-0056" num="0053">In some embodiments, the derived risk can be computed as following:</p><p id="p-0057" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Derived Risk=(Vulnerability Score)&#xd7;(Data Risk Score).<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0058" num="0054">In some embodiments, according to a non-limiting example, Vulnerability Scores can correspond to a scene/setting of the user, and can be set as follows:</p><p id="p-0059" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="35pt" align="left"/><colspec colname="1" colwidth="70pt" align="left"/><colspec colname="2" colwidth="112pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry>Vulnerability Score</entry><entry>Measure</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>Low</entry><entry>1</entry></row><row><entry/><entry>Medium</entry><entry>2</entry></row><row><entry/><entry>High</entry><entry>3</entry></row><row><entry/><entry>Very High</entry><entry>4</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0060" num="0055">For example, a user's car (e.g., a private setting) may be &#x201c;low&#x201d;, and a user being at the airport terminal waiting to board a flight (e.g., a public setting) can be &#x201c;very high.&#x201d;</p><p id="p-0061" num="0056">In some embodiments, according to a non-limiting example, a Data Risk Score can correspond to a type of data being requested, and can be set as follows:</p><p id="p-0062" num="0000"><tables id="TABLE-US-00004" num="00004"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="42pt" align="left"/><colspec colname="1" colwidth="56pt" align="left"/><colspec colname="2" colwidth="119pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry>Data Risk Score</entry><entry>Measure</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>Low</entry><entry>1</entry></row><row><entry/><entry>Medium</entry><entry>2</entry></row><row><entry/><entry>High</entry><entry>3</entry></row><row><entry/><entry>Very High</entry><entry>4</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0063" num="0057">For example, access to a user's mail account can be a &#x201c;high&#x201d; risk, and access to a user's home page for a web portal (e.g., ESPN+) may be &#x201c;low&#x201d; risk.</p><p id="p-0064" num="0058">In some embodiments, the computed derived risk can be compared against a risk threshold, and if it surpasses and/or satisfies the threshold, it can be indicated that the assumed risk to the data is too high, and a private scene is determined, for example.</p><p id="p-0065" num="0059">Upon determining the scene of the user (e.g., where the user is), Process <b>300</b> proceeds to Step <b>310</b> where a POW problem is identified. In some embodiments, Step <b>310</b> can involve identifying a type of POW macro pattern based on the scene, from which a micro pattern included therein can be randomly selected by engine <b>200</b>, as discussed above.</p><p id="p-0066" num="0060">For example, if the scene is private, the POW macro problem can be a simple mathematical problem, where engine <b>200</b> randomly selects addition of decimals in a similar manner to Table 1 discussed above. In another example, if the scene is semi-public, then the POW macro problem can be an alpha pattern shifting manipulation similar to the example in Table 2 above. As discussed above, the POW problem includes a macro category depending on the complexity needed for the type of scene, and a randomly selected or generated pattern/value that the user must correctly answer/solve.</p><p id="p-0067" num="0061">In Step <b>312</b>, the POW problem is presented to the user, and in response to receiving the user's response, the user's answer/solution is analyzed, as in Step <b>314</b>. In some embodiments, engine <b>200</b> can implement any type of known or to be known statistical or probability machine learning (ML) or AI machine or model to compute the accuracy of the response. As discussed above, Step <b>314</b> involves the analysis of whether the correct calculation and/or pattern shift was provided by the user. In Step <b>316</b>, a determination is made by engine <b>200</b> regarding whether the POW response input by the user is correct. If it is, the Process <b>300</b> proceeds to Step <b>318</b> where the user is granted access to the platform.</p><p id="p-0068" num="0062">If Step <b>316</b> indicates that that the user's response is not correct, then engine <b>200</b> can implement a MFA (or two-factor) processing, as in Step <b>320</b>. For example, the system can send an email to an account of the user, provide and request a one-time password, and/or trigger any type of known or to be known MFA processing to verify the user's identity. Upon MFA verification in Step <b>320</b>, Step <b>322</b> can be executed where the user is granted access to the platform.</p><p id="p-0069" num="0063">In some embodiments, a predetermined number of POW problems (e.g., 3) can be presented to the user prior to proceeding to MFA processing in Step <b>320</b>. In such embodiments, engine <b>200</b> can determine whether the user has failed to correctly provide a POW response a predetermined number of times before triggering the MFA module <b>208</b>. In some embodiments, the type of MFA can be based on the type of scene of the user (from Step <b>308</b>).</p><p id="p-0070" num="0064">According to some embodiments, upon receiving a logout instruction/request from the user, access to the platform is ended, however, the user's SSO/VPN connection can be maintained. In some embodiments, the SSO can remain, whereby the VPN connection is severed, and must be reestablished via another SSO iteration in order to re-engage Stage's 2 and 3 of engine <b>200</b>'s processing.</p><p id="p-0071" num="0065"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram of an example network architecture according to some embodiments of the present disclosure. In the illustrated embodiment, UE <b>502</b> accesses a data network <b>508</b> via an access network <b>504</b> and a core network <b>506</b>. In the illustrated embodiment, UE <b>502</b> comprises any computing device capable of communicating with the access network <b>504</b>. As examples, UE <b>502</b> may include mobile phones, tablets, laptops, sensors, IoT devices, autonomous machines, and any other devices equipped with a cellular or wireless or wired transceiver. One example of a UE is provided in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0072" num="0066">In the illustrated embodiment, the access network <b>504</b> comprises a network allowing over-the-air network communication with UE <b>502</b>. In general, the access network <b>504</b> includes at least one base station that is communicatively coupled to the core network <b>506</b> and wirelessly coupled to zero or more UE <b>502</b>.</p><p id="p-0073" num="0067">In some embodiments, the access network <b>504</b> comprises a cellular access network, for example, a fifth-generation 5G network or a fourth-generation (4G) network. In one embodiment, the access network <b>504</b> and UE <b>502</b> comprise a NextGen Radio Access Network (NG-RAN). In an embodiment, the access network <b>504</b> includes a plurality of next Generation Node B (gNodeB) base stations connected to UE <b>502</b> via an air interface. In one embodiment, the air interface comprises a New Radio (NR) air interface. For example, in a 5G network, individual user devices can be communicatively coupled via an X2 interface.</p><p id="p-0074" num="0068">In the illustrated embodiment, the access network <b>504</b> provides access to a core network <b>506</b> to the UE <b>502</b>. In the illustrated embodiment, the core network may be owned and/or operated by a mobile network operator (MNO) and provides wireless connectivity to UE <b>502</b>. In the illustrated embodiment, this connectivity may comprise voice and data services.</p><p id="p-0075" num="0069">At a high-level, the core network <b>506</b> may include a user plane and a control plane. In one embodiment, the control plane comprises network elements and communications interfaces to allow for the management of user connections and sessions. By contrast, the user plane may comprise network elements and communications interfaces to transmit user data from UE <b>502</b> to elements of the core network <b>506</b> and to external network-attached elements in a data network <b>508</b> such as the Internet.</p><p id="p-0076" num="0070">In the illustrated embodiment, the access network <b>504</b> and the core network <b>506</b> are operated by an MNO. However, in some embodiments, the networks (<b>504</b>, <b>506</b>) may be operated by a private entity and may be closed to public traffic. For example, the components of the network <b>506</b> may be provided as a single device, and the access network <b>504</b> may comprise a small form-factor base station. In these embodiments, the operator of the device can simulate a cellular network, and UE <b>502</b> can connect to this network similar to connecting to a national or regional network.</p><p id="p-0077" num="0071">In some embodiments, the access network <b>504</b>, core network <b>506</b> and data network <b>508</b> can be configured as a multi-access edge computing (MEC) network, where MEC or edge nodes are embodied as each UE <b>502</b>, and are situated at the edge of a cellular network, for example, in a cellular base station or equivalent location. In general, the MEC or edge nodes may comprise UEs that comprise any computing device capable of responding to network requests from another UE <b>502</b> (referred to generally as a client) and is not intended to be limited to a specific hardware or software configuration a device.</p><p id="p-0078" num="0072"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram illustrating a computing device showing an example of a client or server device used in the various embodiments of the disclosure.</p><p id="p-0079" num="0073">The computing device <b>600</b> may include more or fewer components than those shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, depending on the deployment or usage of the device <b>600</b>. For example, a server computing device, such as a rack-mounted server, may not include audio interfaces <b>652</b>, displays <b>654</b>, keypads <b>656</b>, illuminators <b>658</b>, haptic interfaces <b>662</b>, GPS receivers <b>664</b>, or cameras/sensors <b>666</b>. Some devices may include additional components not shown, such as graphics processing unit (GPU) devices, cryptographic co-processors, artificial intelligence (AI) accelerators, or other peripheral devices.</p><p id="p-0080" num="0074">As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the device <b>600</b> includes a central processing unit (CPU) <b>622</b> in communication with a mass memory <b>630</b> via a bus <b>624</b>. The computing device <b>600</b> also includes one or more network interfaces <b>650</b>, an audio interface <b>652</b>, a display <b>654</b>, a keypad <b>656</b>, an illuminator <b>658</b>, an input/output interface <b>660</b>, a haptic interface <b>662</b>, an optional global positioning systems (GPS) receiver <b>664</b> and a camera(s) or other optical, thermal, or electromagnetic sensors <b>666</b>. Device <b>600</b> can include one camera/sensor <b>666</b> or a plurality of cameras/sensors <b>666</b>. The positioning of the camera(s)/sensor(s) <b>666</b> on the device <b>600</b> can change per device <b>600</b> model, per device <b>600</b> capabilities, and the like, or some combination thereof.</p><p id="p-0081" num="0075">In some embodiments, the CPU <b>622</b> may comprise a general-purpose CPU. The CPU <b>622</b> may comprise a single-core or multiple-core CPU. The CPU <b>622</b> may comprise a system-on-a-chip (SoC) or a similar embedded system. In some embodiments, a GPU may be used in place of, or in combination with, a CPU <b>622</b>. Mass memory <b>630</b> may comprise a dynamic random-access memory (DRAM) device, a static random-access memory device (SRAM), or a Flash (e.g., NAND Flash) memory device. In some embodiments, mass memory <b>630</b> may comprise a combination of such memory types. In one embodiment, the bus <b>624</b> may comprise a Peripheral Component Interconnect Express (PCIe) bus. In some embodiments, the bus <b>624</b> may comprise multiple busses instead of a single bus.</p><p id="p-0082" num="0076">Mass memory <b>630</b> illustrates another example of computer storage media for the storage of information such as computer-readable instructions, data structures, program modules, or other data. Mass memory <b>630</b> stores a basic input/output system (&#x201c;BIOS&#x201d;) <b>640</b> for controlling the low-level operation of the computing device <b>600</b>. The mass memory also stores an operating system <b>641</b> for controlling the operation of the computing device <b>600</b>.</p><p id="p-0083" num="0077">Applications <b>642</b> may include computer-executable instructions which, when executed by the computing device <b>600</b>, perform any of the methods (or portions of the methods) described previously in the description of the preceding Figures. In some embodiments, the software or programs implementing the method embodiments can be read from a hard disk drive (not illustrated) and temporarily stored in RAM <b>632</b> by CPU <b>622</b>. CPU <b>622</b> may then read the software or data from RAM <b>632</b>, process them, and store them to RAM <b>632</b> again.</p><p id="p-0084" num="0078">The computing device <b>600</b> may optionally communicate with a base station (not shown) or directly with another computing device. Network interface <b>650</b> is sometimes known as a transceiver, transceiving device, or network interface card (NIC).</p><p id="p-0085" num="0079">The audio interface <b>652</b> produces and receives audio signals such as the sound of a human voice. For example, the audio interface <b>652</b> may be coupled to a speaker and microphone (not shown) to enable telecommunication with others or generate an audio acknowledgment for some action. Display <b>654</b> may be a liquid crystal display (LCD), gas plasma, light-emitting diode (LED), or any other type of display used with a computing device. Display <b>654</b> may also include a touch-sensitive screen arranged to receive input from an object such as a stylus or a digit from a human hand.</p><p id="p-0086" num="0080">Keypad <b>656</b> may comprise any input device arranged to receive input from a user. Illuminator <b>658</b> may provide a status indication or provide light.</p><p id="p-0087" num="0081">The computing device <b>600</b> also comprises an input/output interface <b>660</b> for communicating with external devices, using communication technologies, such as USB, infrared, Bluetooth&#x2122;, or the like. The haptic interface <b>662</b> provides tactile feedback to a user of the client device.</p><p id="p-0088" num="0082">The optional GPS transceiver <b>664</b> can determine the physical coordinates of the computing device <b>600</b> on the surface of the Earth, which typically outputs a location as latitude and longitude values. GPS transceiver <b>664</b> can also employ other geo-positioning mechanisms, including, but not limited to, triangulation, assisted GPS (AGPS), E-OTD, CI, SAI, ETA, BSS, or the like, to further determine the physical location of the computing device <b>600</b> on the surface of the Earth. In one embodiment, however, the computing device <b>600</b> may communicate through other components, provide other information that may be employed to determine a physical location of the device, including, for example, a MAC address, IP address, or the like.</p><p id="p-0089" num="0083">The present disclosure has been described with reference to the accompanying drawings, which form a part hereof, and which show, by way of non-limiting illustration, certain example embodiments. Subject matter may, however, be embodied in a variety of different forms and, therefore, covered or claimed subject matter is intended to be construed as not being limited to any example embodiments set forth herein; example embodiments are provided merely to be illustrative. Likewise, a reasonably broad scope for claimed or covered subject matter is intended. Among other things, for example, subject matter may be embodied as methods, devices, components, or systems. Accordingly, embodiments may, for example, take the form of hardware, software, firmware or any combination thereof (other than software per se). The following detailed description is, therefore, not intended to be taken in a limiting sense.</p><p id="p-0090" num="0084">Throughout the specification and claims, terms may have nuanced meanings suggested or implied in context beyond an explicitly stated meaning. Likewise, the phrase &#x201c;in some embodiments&#x201d; as used herein does not necessarily refer to the same embodiment and the phrase &#x201c;in another embodiment&#x201d; as used herein does not necessarily refer to a different embodiment. It is intended, for example, that claimed subject matter include combinations of example embodiments in whole or in part.</p><p id="p-0091" num="0085">In general, terminology may be understood at least in part from usage in context. For example, terms, such as &#x201c;and&#x201d;, &#x201c;or&#x201d;, or &#x201c;and/or,&#x201d; as used herein may include a variety of meanings that may depend at least in part upon the context in which such terms are used. Typically, &#x201c;or&#x201d; if used to associate a list, such as A, B or C, is intended to mean A, B, and C, here used in the inclusive sense, as well as A, B or C, here used in the exclusive sense. In addition, the term &#x201c;one or more&#x201d; as used herein, depending at least in part upon context, may be used to describe any feature, structure, or characteristic in a singular sense or may be used to describe combinations of features, structures or characteristics in a plural sense. Similarly, terms, such as &#x201c;a,&#x201d; &#x201c;an,&#x201d; or &#x201c;the,&#x201d; again, may be understood to convey a singular usage or to convey a plural usage, depending at least in part upon context. In addition, the term &#x201c;based on&#x201d; may be understood as not necessarily intended to convey an exclusive set of factors and may, instead, allow for existence of additional factors not necessarily expressly described, again, depending at least in part on context.</p><p id="p-0092" num="0086">The present disclosure has been described with reference to block diagrams and operational illustrations of methods and devices. It is understood that each block of the block diagrams or operational illustrations, and combinations of blocks in the block diagrams or operational illustrations, can be implemented by means of analog or digital hardware and computer program instructions. These computer program instructions can be provided to a processor of a general purpose computer to alter its function as detailed herein, a special purpose computer, ASIC, or other programmable data processing apparatus, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, implement the functions/acts specified in the block diagrams or operational block or blocks. In some alternate implementations, the functions/acts noted in the blocks can occur out of the order noted in the operational illustrations. For example, two blocks shown in succession can in fact be executed substantially concurrently or the blocks can sometimes be executed in the reverse order, depending upon the functionality/acts involved.</p><p id="p-0093" num="0087">For the purposes of this disclosure, a non-transitory computer readable medium (or computer-readable storage medium/media) stores computer data, which data can include computer program code (or computer-executable instructions) that is executable by a computer, in machine readable form. By way of example, and not limitation, a computer readable medium may comprise computer readable storage media, for tangible or fixed storage of data, or communication media for transient interpretation of code-containing signals. Computer readable storage media, as used herein, refers to physical or tangible storage (as opposed to signals) and includes without limitation volatile and non-volatile, removable and non-removable media implemented in any method or technology for the tangible storage of information such as computer-readable instructions, data structures, program modules or other data. Computer readable storage media includes, but is not limited to, RAM, ROM, EPROM, EEPROM, flash memory or other solid state memory technology, optical storage, cloud storage, magnetic storage devices, or any other physical or material medium which can be used to tangibly store the desired information or data or instructions and which can be accessed by a computer or processor.</p><p id="p-0094" num="0088">To the extent the aforementioned implementations collect, store, or employ personal information of individuals, groups, or other entities, it should be understood that such information shall be used in accordance with all applicable laws concerning the protection of personal information. Additionally, the collection, storage, and use of such information can be subject to the consent of the individual to such activity, for example, through well known &#x201c;opt-in&#x201d; or &#x201c;opt-out&#x201d; processes as can be appropriate for the situation and type of information. Storage and use of personal information can be in an appropriately secure manner reflective of the type of information, for example, through various access control, encryption, and anonymization techniques (for especially sensitive information).</p><p id="p-0095" num="0089">In the preceding specification, various example embodiments have been described with reference to the accompanying drawings. However, it will be evident that various modifications and changes may be made thereto, and additional embodiments may be implemented without departing from the broader scope of the disclosed embodiments as set forth in the claims that follow. The specification and drawings are accordingly to be regarded in an illustrative rather than restrictive sense.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>receiving, by a device, a set of words provided by a user, a portion of the received words being in response to a portion of a set of words provided by the device;</claim-text><claim-text>determining, by the device, a scene of the user based at least on a portion of the set of words received by the user, the scene indicating the current surroundings of the user;</claim-text><claim-text>identifying, by the device, a proof of work (POW) problem based on the determined scene;</claim-text><claim-text>presenting, by the device, the POW problem to the user;</claim-text><claim-text>receiving, by the device, a solution from the user to the POW problem;</claim-text><claim-text>analyzing, by the device, the solution, and determining, based on the analysis, whether access to a network resource is granted; and</claim-text><claim-text>enabling access, by the device over a network, to the network resource based on the access determination, wherein access is enabled when the solution is determined to be acceptable.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein receiving the set of words provided by the user comprises receiving an invocation word and a closing word, wherein the set of words output by the device comprise a first system word and a second system word.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:<claim-text>receiving, by the device from the user, the invocation word;</claim-text><claim-text>outputting, by the device in response to the invocation word, the first system word;</claim-text><claim-text>outputting, by the device, after outputting the first system word, the second system word; and</claim-text><claim-text>receiving, by the device from the user, the closing word.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the set of words provided by the user and the set of words provided by the device are associated with a predefined list of words, wherein the list is part of a collection of lists accessible over the network, wherein the list is identifiable by the invocation word.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein each list corresponds to a type of scene, wherein the type of scene is one of a public scene, semi-public scene and private scene.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein receiving the set of words provided by the user comprises audibly detecting spoken words by the user, wherein the set of words output by the device are audibly output by the device.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>identifying, by the device, a type of POW problem based on the scene; and</claim-text><claim-text>randomly selecting, by the device, a specific POW problem from a set of POW problems of the identified type.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the POW problem requests a calculation or manipulation by the user, wherein a value of the calculation or manipulation is preset by the user.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the determination related to the solution is based on the value.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>performing multi-factor authentication (MFA) processing when the solution is determined to be not acceptable, wherein access to the network resource is enabled when the MFA processing is successfully completed.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>receiving a logout instruction from the user related to the network resource, wherein access to the device is maintained despite the user being logged out of the network resource.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the enabled access to the network resource enables the user to perform create, read, update, delete (CRUD) operations in relation to data provided by the network resource.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>performing single sign-on (SSO) on the device such that access to the device is based on the SSO; and</claim-text><claim-text>establishing a virtual private network (VPN) connection over the network based on the SSO.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the network resource is an enterprise platform.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A device comprising:<claim-text>a processor configured to:<claim-text>receive a set of words provided by a user, a portion of the received words being in response to a portion of a set of words provided by the device;</claim-text><claim-text>determine a scene of the user based at least on a portion of the set of words received by the user, the scene indicating the current surroundings of the user;</claim-text><claim-text>identify a proof of work (POW) problem based on the determined scene;</claim-text><claim-text>present the POW problem to the user;</claim-text><claim-text>receive a solution from the user to the POW problem;</claim-text><claim-text>analyze the solution, and determine, based on the analysis, whether access to a network resource is granted; and</claim-text><claim-text>enable access, over a network, to the network resource based on the access determination, wherein access is enabled when the solution is determined to be acceptable.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The device of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein receiving the set of words provided by the user comprises receiving an invocation word and a closing word, wherein the set of words output by the device comprise a first system word and a second system word, wherein the processor is further configured to:<claim-text>receive, from the user, the invocation word;</claim-text><claim-text>output, in response to the invocation word, the first system word;</claim-text><claim-text>output, after outputting the first system word, the second system word; and</claim-text><claim-text>receive, from the user, the closing word.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The device of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the processor is further configured to:<claim-text>identify a type of POW problem based on the scene; and</claim-text><claim-text>randomly select a specific POW problem from a set of POW problems of the identified type.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. A non-transitory computer-readable medium tangibly encoded with instructions, that when executed by a processor of a device, perform a method comprising:<claim-text>receiving, by the device, a set of words provided by a user, a portion of the received words being in response to a portion of a set of words provided by the device;</claim-text><claim-text>determining, by the device, a scene of the user based at least on a portion of the set of words received by the user, the scene indicating the current surroundings of the user;</claim-text><claim-text>identifying, by the device, a proof of work (POW) problem based on the determined scene;</claim-text><claim-text>presenting, by the device, the POW problem to the user;</claim-text><claim-text>receiving, by the device, a solution from the user to the POW problem;</claim-text><claim-text>analyzing, by the device, the solution, and determining, based on the analysis, whether access to a network resource is granted; and</claim-text><claim-text>enabling access, by the device over a network, to the network resource based on the access determination, wherein access is enabled when the solution is determined to be acceptable.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable medium tangibly of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein receiving the set of words provided by the user comprises receiving an invocation word and a closing word, wherein the set of words output by the device comprise a first system word and a second system word, wherein the method further comprises:<claim-text>receiving, by the device from the user, the invocation word;</claim-text><claim-text>outputting, by the device in response to the invocation word, the first system word;</claim-text><claim-text>outputting, by the device, after outputting the first system word, the second system word; and</claim-text><claim-text>receiving, by the device from the user, the closing word.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable medium tangibly of <claim-ref idref="CLM-00018">claim 18</claim-ref>, further comprising:<claim-text>identifying, by the device, a type of POW problem based on the scene; and</claim-text><claim-text>randomly selecting, by the device, a specific POW problem from a set of POW problems of the identified type.</claim-text></claim-text></claim></claims></us-patent-application>