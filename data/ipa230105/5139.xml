<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005140A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005140</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17899232</doc-number><date>20220830</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>11</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0012</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>11</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30096</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>2201</main-group><subgroup>031</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">AUTOMATED DETECTION OF TUMORS BASED ON IMAGE PROCESSING</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/US2021/021910</doc-number><date>20210311</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17899232</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62989261</doc-number><date>20200313</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Genentech, Inc.</orgname><address><city>South San Francisco</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>FERL</last-name><first-name>Gregory Zelinsky</first-name><address><city>South San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>CARANO</last-name><first-name>Richard Alan Duray</first-name><address><city>South San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>BARCK</last-name><first-name>Kai Henrik</first-name><address><city>South San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>PATIL</last-name><first-name>Jasmine</first-name><address><city>South San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Genentech, Inc.</orgname><role>02</role><address><city>South San Francisco</city><state>CA</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Methods and systems disclosed herein relate generally to processing images to estimate whether at least part of a tumor is represented in the images. A computer-implemented method includes accessing an image of at least part of a biological structure of a particular subject, processing the image using a segmentation algorithm to extract a plurality of image objects depicted in the image, determining one or more structural characteristics associated with an image object of the plurality of image objects, processing the one or more structural characteristics using a trained machine-learning model to generate estimation data corresponding to an estimation of whether the image object corresponds to a lesion or tumor associated with the biological structure, and outputting the estimation data for the particular subject.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="120.65mm" wi="158.75mm" file="US20230005140A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="199.47mm" wi="159.68mm" file="US20230005140A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="240.37mm" wi="183.22mm" orientation="landscape" file="US20230005140A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="201.68mm" wi="157.99mm" orientation="landscape" file="US20230005140A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="223.60mm" wi="130.81mm" file="US20230005140A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="222.42mm" wi="162.90mm" orientation="landscape" file="US20230005140A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="249.00mm" wi="168.99mm" orientation="landscape" file="US20230005140A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="242.57mm" wi="174.50mm" orientation="landscape" file="US20230005140A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="212.94mm" wi="175.77mm" orientation="landscape" file="US20230005140A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="242.65mm" wi="174.24mm" orientation="landscape" file="US20230005140A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="255.95mm" wi="188.98mm" orientation="landscape" file="US20230005140A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="259.25mm" wi="179.24mm" orientation="landscape" file="US20230005140A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="219.37mm" wi="191.09mm" orientation="landscape" file="US20230005140A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="243.08mm" wi="167.81mm" orientation="landscape" file="US20230005140A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="243.92mm" wi="181.19mm" orientation="landscape" file="US20230005140A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="240.45mm" wi="181.10mm" orientation="landscape" file="US20230005140A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="250.53mm" wi="180.76mm" orientation="landscape" file="US20230005140A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="249.43mm" wi="191.85mm" orientation="landscape" file="US20230005140A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="242.23mm" wi="185.42mm" orientation="landscape" file="US20230005140A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="244.26mm" wi="150.96mm" file="US20230005140A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="254.08mm" wi="169.84mm" orientation="landscape" file="US20230005140A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="163.91mm" wi="179.58mm" orientation="landscape" file="US20230005140A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="249.09mm" wi="191.69mm" orientation="landscape" file="US20230005140A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="250.44mm" wi="194.56mm" orientation="landscape" file="US20230005140A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="250.44mm" wi="193.21mm" orientation="landscape" file="US20230005140A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="239.95mm" wi="144.78mm" file="US20230005140A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="231.22mm" wi="138.43mm" file="US20230005140A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The present application is a continuation of International Application PCT/US2021/021910, entitled &#x201c;AUTOMATED DETECTION OF TUMORS BASED ON IMAGE PROCESSING&#x201d; and filed Mar. 11, 2021, which claims the benefit of and priority to U.S. Provisional Patent Application 62/989,261, filed on Mar. 13, 2020, which is hereby incorporated by reference in its entirety for all purposes.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">Methods and systems disclosed herein relate generally to methods and systems for processing images to estimate whether at least part of a tumor is represented in the images.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Recent developments of medical imaging technologies have resulted in image outputs having much improved image quality. The images associated with higher quality have allowed medical practitioners to more accurately identify symptoms associated with a subject and diagnose various types of diseases. In addition, image processing systems have begun to use machine-learning techniques to gain better insight into these images. For example, a conventional approach may include using a trained machine-learning model to determine a tissue type based on features associated with the images.</p><p id="p-0005" num="0004">Although using trained machine-learning models may assist in segmenting an image to identify image objects corresponding to tissues and/or tumors, it becomes challenging to train these machine-learning models. This is because training machine-learning models typically involves a time-consuming process of manually labeling individual images. Further, manual labeling of images may require a large number of experts to ensure accuracy. Training the machine-learning models can also be difficult when a particular set of features within training images can confound the labeling process. For example, a training image depicting a large unknown mass may obscure other features within the training image that may be relevant for diagnosis of diseases. This may subsequently frustrate the training process of the machine-learning models, either by increasing the time needed to perform the analysis or increase the error rate of the machine-learning classification.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">In some embodiments, a computer-implemented method includes accessing an image of at least part of a biological structure of a particular subject. In some instances, the image is a three-dimensional image depicting a part of the biological structure. The computer-implemented method can also include processing the image using a segmentation algorithm to extract a plurality of image objects depicted in the image. The computer-implemented method can also include determining one or more structural characteristics associated with an image object of the plurality of image objects. The computer-implemented method can also include processing the one or more structural characteristics using a trained machine-learning model to generate estimation data corresponding to an estimation of whether the image object corresponds to a lesion or tumor associated with the biological structure. The trained machine-learning model can be trained with a three-dimensional model constructed from a set of training images. The computer-implemented method can also include outputting the estimation data for the particular subject.</p><p id="p-0007" num="0006">In some instances, at least part of the biological structure includes at least part of a lung. The image can depict skeletal structures surrounding at least part of the biological structure. The image can depict a transverse plane of at least part of the biological structure. The image can include or may have been derived from image data captured using a computed tomography scanner, which may be a micro-computed tomography scanner. The one or more structural characteristics can include shape, location, surface area, and/or longest diameter of the image object.</p><p id="p-0008" num="0007">In some instances, processing the image using the segmentation algorithm includes determining one or more structural characteristics associated with another image object. Processing the image using the segmentation algorithm can also include processing the one or more structural characteristics of the other image object using the trained machine-learning model to generate estimation data corresponding to an estimation of whether the other image object corresponds a type of the biological structure. In some instances, the trained machine-learning model processes the one or more structural characteristics to identify a level of tumor burden shown in the image. Processing the image using the segmentation algorithm can also include outputting the estimation data associated with the other image object. The type of the biological structure can include blood vessel, lung, heart, and/or liver.</p><p id="p-0009" num="0008">In some instances, the segmentation algorithm is a watershed transformation algorithm. The trained machine-learning model can be a trained support vector machine (SVM).</p><p id="p-0010" num="0009">In some instances, the computer-implemented method includes preprocessing the image using a trained image-preprocessing machine-learning model to generate a filter. The filter can be applied to isolate one or more image regions from the image, thereby generating a filtered image. The isolated image regions may indicate the biological structure. In some instances, the trained image-preprocessing machine-learning model includes a U-Net or V-Net model. The computer-implemented method can also include processing the isolated image regions of the filtered image using the segmentation algorithm to extract the plurality of image objects depicted in the image. The trained image-preprocessing machine-learning model can be a trained convolutional-neural-network machine-learning model.</p><p id="p-0011" num="0010">In some instances, the computer-implemented method includes processing the image using an image filter to generate a filtered image that excludes one or more skeletal structures surrounding the at least part of the biological structure. The filtered image can be used in lieu of the image to separate the image object from the plurality of image objects.</p><p id="p-0012" num="0011">In some instances, the computer-implemented method includes processing the image using co-registration to align the image with a reference image. The aligned image can be used to in lieu of the image to extract the image object from the plurality of image objects.</p><p id="p-0013" num="0012">In some instances, the computer-implemented method includes transforming the image into a binary image. The computer-implemented method can also include converting one or more pixels of the binary image by using a fill operation. The computer-implemented method can also include performing an erosion and dilation operation to reduce image noise from the binary image.</p><p id="p-0014" num="0013">In some instances, processing the image using a segmentation algorithm includes applying a negative distance transform function to the image to identify boundaries between two or more overlapping image objects of the plurality of image objects.</p><p id="p-0015" num="0014">In some embodiments, a system is provided that includes one or more data processors and a non-transitory computer readable storage medium containing instructions which, when executed on the one or more data processors, cause the one or more data processors to perform part or all of one or more methods disclosed herein.</p><p id="p-0016" num="0015">In some embodiments, a computer-program product is provided that is tangibly embodied in a non-transitory machine-readable storage medium and that includes instructions configured to cause one or more data processors to perform part or all of one or more methods disclosed herein.</p><p id="p-0017" num="0016">Some embodiments of the present disclosure include a system including one or more data processors. In some embodiments, the system includes a non-transitory computer readable storage medium containing instructions which, when executed on the one or more data processors, cause the one or more data processors to perform part or all of one or more methods and/or part or all of one or more processes disclosed herein. Some embodiments of the present disclosure include a computer-program product tangibly embodied in a non-transitory machine-readable storage medium, including instructions configured to cause one or more data processors to perform part or all of one or more methods and/or part or all of one or more processes disclosed herein.</p><p id="p-0018" num="0017">The terms and expressions which have been employed are used as terms of description and not of limitation, and there is no intention in the use of such terms and expressions of excluding any equivalents of the features shown and described or portions thereof, but it is recognized that various modifications are possible within the scope of the invention claimed. Thus, it should be understood that although the present invention as claimed has been specifically disclosed by embodiments and optional features, modification and variation of the concepts herein disclosed may be resorted to by those skilled in the art, and that such modifications and variations are considered to be within the scope of this invention as defined by the appended claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0019" num="0018">The present disclosure is described in conjunction with the appended figures:</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example computing environment for processing images to estimate whether at least part of a tumor is represented in the images according to some embodiments.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an exemplary schematic diagram of an image processing system for processing images to estimate whether at least part of a tumor is represented in the images according to some embodiments.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an exemplary U-Net according to some embodiments.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> shows a residual block according to some embodiments.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> shows a pyramidal layer according to some embodiments.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example schematic diagram of a &#x2212;V-Net architecture used for image-preprocessing according to some embodiments.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows an example set of graphs that indicate performance of the trained image-preprocessing machine-learning model.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIGS. <b>7</b>A-B</figref> illustrate exemplary schematic diagrams of image filter subsystems that identify biological structures from input images according to some embodiments.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an example schematic diagram of a binary converter subsystem that transforms the filtered images into binary images according to some embodiments.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIGS. <b>9</b>A-B</figref> illustrate example schematic diagrams of image registration subsystem that registers binary images into a single spatial coordinate system according to some embodiments.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an example schematic diagram of an image segmentation subsystem that extracts a set of image objects from a registered image according to some embodiments.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates an example schematic diagram of a feature extractor subsystem that identifies a set of structural characteristics associated with an image object according to some embodiments.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>12</b></figref> shows example sets of box plots corresponding to distributions of centroid locations of image objects detected by an image segmentation subsystem.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows example sets of box plots corresponding to distributions of orientations of image objects detected by an image segmentation subsystem.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>14</b></figref> shows example sets of box plots corresponding to distributions of principal axis lengths of image objects detected by an image segmentation subsystem.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>15</b></figref> shows example sets of box plots corresponding to distributions of sizes of image objects detected by an image segmentation subsystem.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>16</b></figref> shows example sets of box plots corresponding to distributions of shapes of image objects detected by an image segmentation subsystem.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>17</b></figref> shows example sets of box plots corresponding to distributions of voxel intensities of image objects detected by an image segmentation subsystem.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>18</b></figref> illustrates an exemplary schematic diagram of a support vector machine for estimating whether an image object corresponds to a tumor according to some embodiments.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>19</b></figref> shows an example of a graph that identifies a relationship between detected lung volumes from images and total tumor volumes.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIGS. <b>20</b>A-C</figref> show example screenshots of a user interface for labeling training image objects corresponding to three-dimensional image data according to some embodiments.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>21</b></figref> illustrates a process for processing images to estimate whether at least part of a tumor is represented in the images according to some embodiments.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>22</b></figref> illustrates a process for processing images to extract a set of image objects according to some embodiments.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0043" num="0042">In the appended figures, similar components and/or features can have the same reference label. Further, various components of the same type can be distinguished by following the reference label by a dash and a second label that distinguishes among the similar components. If only the first reference label is used in the specification, the description is applicable to any one of the similar components having the same first reference label irrespective of the second reference label.</p><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><heading id="h-0007" level="2">I. Overview</heading><p id="p-0044" num="0043">Techniques can relate to processing images to estimate whether at least part of a tumor is represented in the images. In particular, the techniques can identify individual tumors in a three-dimensional image even when multiple tumors are present in such image. An image (e.g., a three-dimensional, micro-computed tomography (micro-CT) image) depicting a biological structure may be accessed by an image processing system. The image processing system may process the image through a series of image transformation operations, including filter operations, binarize operations, image registration operations, and segmentation operations. As a result, the image processing system can define boundaries in the image that can be used to differentiate regions of the image. Based on the identified boundaries, a set of image objects can be identified from the image. A classification system may process each image object of the extracted image objects to determine a set of structural characteristics. The set of structural characteristics can be processed using a trained machine-learning model (e.g., a support vector machine). Based on outputs from the trained machine-learning model, the classification system can identify, for each of the extracted image objects, information that indicates whether the image object corresponds to a tumor, a lesion. or a normal tissue. In some instances, the classification system processes the sets of structural characteristics to identify a level of tumor burden corresponding to the biological structure depicted in the image.</p><p id="p-0045" num="0044">The trained machine-learning model can be trained based on training data including a set of training images. Each training image of the set may include multiple training image objects. Each training image object of the multiple training image objects may be associated with a label that identifies a biological structure corresponding to the training image object. To generate labels corresponding to the training image objects, a training system may render the three-dimensional training image into a set of two-dimensional CT images depicting slices of the biological structure, such that image objects corresponding to the three-dimensional training image can be labeled. The image can also be rendered as interactive three-dimensional image data which may include a set of three-dimensional image objects corresponding to respective portions of the biological structure. A label can be assigned for each object of the set of three-dimensional image objects, including (for example) a tumor, a heart, a liver, a lung, a mediastinum, and/or blood vessels. The training data (that includes the training images and the labels) can be used to generate sets of structural characteristics that can be used to train the machine-learning models.</p><p id="p-0046" num="0045">The classification system can receive the trained machine-learning model from the training system. The classification system may receive an unlabeled image, from which a set of image objects can be extracted by the image processing system. For each unlabeled image object, the classification system may identify its structural characteristics. The classification system may use the trained machine-learning model to identify whether the structural characteristics of a particular image object correspond to structural characteristics associated with a particular biological structure (e.g., a tumor, a lung, blood vessels). Based on the identification, the classification system may estimate whether the particular image corresponds to a tumor, a lesion, or a normal tissue. In addition to the classification, the trained machine-learning model can identify a level of tumor burden from the image. For example, the trained machine-learning model can predict that a given image of a particular subject includes a tumor and further predict a medium tumor burden in the given image. The trained machine-learning model can include a discriminant-analysis model, a kernel classification model, a k-nearest neighbor model, a linear classification model, a Naive Bayes classifier, a support vector machine, a gradient-boosted Ensemble classification algorithm, and/or one or more classification trees.</p><p id="p-0047" num="0046">Accordingly, embodiments of the present disclosure provide a technical advantage over conventional systems that analyze images to detect presence of tumors in a tissue. Although conventional systems may simply identify whether any tumor is present in an image, but cannot identify types of individual tumors based on regions depicted in the image. Moreover, tumor identification by conventional systems require defining rules on image intensities, morphology and relations to other anatomical features that then need to be implemented programmatically, which is often difficult to implement. The techniques of pre-processing the images through filtering, registration, and segmentation operations can further improve the training (e.g., unsupervised training) and performance of the trained classifier systems, such that individual tumors can be identified and classified. In addition, automated generation of the three dimensional data constructed from the training data significantly improves the rate at which the training images can be processed and labeled for training the machine-learning models.</p><heading id="h-0008" level="2">II. Techniques for Automated Detection of Tumors Based on Image Processing</heading><p id="p-0048" num="0047">II.A. Example Computing Environment</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example computing environment <b>100</b> for processing images to estimate whether at least part of a tumor is represented in the images according to some embodiments. The computing environment <b>100</b> may include a tissue classification system <b>105</b>, which in turn may include an image processing subsystem <b>110</b>, a classification subsystem <b>112</b>, and a training subsystem <b>115</b>. The image processing subsystem <b>110</b> can extract image objects from an image corresponding to an individual subject, by processing the image through different types of image processing operations (e.g., automated alignments, image segmentation, and/or feature extraction). The classification subsystem <b>112</b> may process each of the extracted image objects to generate a set of structural characteristics, which in turn can be processed using a trained machine-learning model (e.g., a support vector machine). Based on outputs from the trained machine-learning model, the classification subsystem <b>112</b> can generate information corresponding to each image object, the information indicating whether the image object corresponds to a tumor, a lesion, or a normal tissue.</p><p id="p-0050" num="0049">The image processing subsystem <b>110</b> can access an image (e.g., a three-dimensional micro-CT image) showing one or more biological structures. A biological structure may refer to one or more types of tissues that are represented in the image. For example, biological structures may include an individual organ such a lung, heart, or liver, various types of tissues (e.g., bone, blood vessels, nerves, tumors), and/or any structure that indicates changes to at least part of the biological structure (e.g., lesions). The image processing subsystem <b>110</b> may process the image using a series of image transformations to extract multiple image objects and derive structural characteristics from each of the multiple image objects.</p><p id="p-0051" num="0050">The training subsystem <b>115</b> can train the machine-learning model and transmit it to the classification subsystem <b>112</b>. The training subsystem <b>115</b> may use the training data (e.g., structural characteristics corresponding to a labeled training image object) to learn parameter values of the machine-learning model. Various machine-learning techniques may be used to train the machine-learning models. A machine-learning technique may include decision-tree learning, association-rule learning, an artificial neural network, deep learning, inductive logic programming, a support vector machine, clustering, a Bayesian network, reinforcement learning, representation learning, similarity and metric learning, sparse dictionary learning, a genetic algorithm, and/or rule-based machine learning. A machine-learning technique can include an ensemble technique, which learns inter-ensemble weights to apply to results produced from various underlying techniques (such as two or more of those previously mentioned). The inter-ensemble weights may be identified based on (for example) accuracy, speed and/or resource usage associated with the underlying techniques.</p><p id="p-0052" num="0051">The training data can include sets of structural characteristics corresponding to image objects of training images and labels corresponding to each of the image objects. The training subsystem <b>115</b> can receive labels corresponding to each image object of the training images. The labels associated with the training data may have been received or may be derived from data received from the provider system(s) <b>120</b>, each of which may be associated with (for example) a physician, lab technician, nurse, hospital, CT scan technician, etc. The training data may be generated based on communications from a medical-provider device associated with the provider system <b>120</b>. The communications from the medical-provider device can include medical records corresponding to the particular subject. The medical records can include (for example) a professional's diagnosis or characterization that indicates whether at least part of a tumor is represented in the CT images.</p><p id="p-0053" num="0052">The training subsystem <b>115</b> can also collect the labels based on three-dimensional training images. The training subsystem <b>115</b> can perform a segmentation operation (e.g., watershed segmentation, k-means) to define boundaries in the three-dimensional training image, then render the two-dimensional slice images. Based on the segmentation operation, the training subsystem <b>115</b> can define each of a set of three-dimensional image objects in the image, in which each image object corresponds to a respective portion of the biological structure. A label can be assigned for each object of the set of three-dimensional image objects. For example, each image object can be labeled as (for example) a tumor, a heart, a liver, a lung, a mediastinum, and/or blood vessels.</p><p id="p-0054" num="0053">The training subsystem <b>115</b> may additionally or alternatively present two-dimensional slice images rendered from the three-dimensional training images, in order to facilitate the collection of labels based on the three-dimensional training images. The two-dimensional slice images correspond to consecutive and/or adjacent slices of the biological structure depicted in the training image. Each of the two-dimensional training images may include regions that were identified based on boundaries generated from the segmentation operation. Using visualizations of the two-dimensional images, labeling of the regions can be facilitated.</p><p id="p-0055" num="0054">The labeled image objects can be processed to generate their corresponding structural characteristics. Such data can be used to train the machine-learning models. Specifically, a set of structural characteristics corresponding to each labeled training image object can be generated based on morphological features identified from the labeled training image object. For example, the set of structural characteristics may include diameter, surface area, a shape, centrally convex volume corresponding to each labeled training image object. For each labeled training image object, the set of structural characteristics can be stored in a data structure including a two-dimensional array. The data structure storing the set of structural characteristics of the image object can be inputted to train the machine-learning model, which may be trained to estimate whether an image object corresponds to a tumor based on the determined structural characteristics.</p><p id="p-0056" num="0055">A user device <b>130</b> can be used to assign the labels to the set of three-dimensional image objects. For example, the user device <b>130</b> can include a user interface. The user interface can be used to label each of the set of three-dimensional image objects. The training subsystem <b>115</b> can avail (e.g., locally present or transmit configuration data for) the user interface of the user device <b>130</b> to represent the training image with the corresponding set of the three-dimensional image objects. As such, the three-dimensional images can be loaded into the user interface (e.g., a user interface <b>1000</b> of FIGS, <b>10</b>A-C), such that the user interface may allow access of an image object of a set of image objects corresponding to each of the loaded three-dimensional images. In this manner, a user using the user device <b>130</b> may assign a tissue label (e.g., the tumor, the liver, the heart) to each image object, thereby generating training data used to train the machine-learning models.</p><p id="p-0057" num="0056">The classification subsystem <b>112</b> can receive the trained machine-learning model from the training subsystem <b>115</b>. The classification subsystem <b>112</b> may receive an input image, from which a set of unlabeled image objects can be extracted by the image processing subsystem <b>110</b>. The input image may be preprocessed using a trained image-preprocessing machine-learning model to identify regions of interest (ROIs) including organs and tumors. The ROIs can be processed using binarization, registration, and segmentation (for example) to extract the set of unlabeled image objects. For each unlabeled image object, the classification subsystem <b>112</b> may identify or otherwise access its structural characteristics. The structural characteristics may refer to morphological features associated with an image object that can be measured using any techniques known to one ordinarily skilled in the art. For example, the structural characteristics may include diameter, surface area, shape, centrally convex volume, equivalent diameter, orientation, solidity, and/or volume associated with the image object. The classification subsystem <b>112</b> may use the trained machine-learning model to identify whether the structural characteristics of a particular image object correspond to pre-identified structural characteristics associated with a particular biological structure (e.g., a tumor, a lung, blood vessels). Based on the identification, the classification subsystem <b>112</b> may estimate whether the particular image corresponds to a tumor or a lesion.</p><p id="p-0058" num="0057">At least part of the images processed by the trained machine-learning models may include or may have been derived from data collected using and received from an imaging system <b>120</b>. The imaging system <b>120</b> can include a CT system, which may include a tomographic imager and/or a micro-CT component (or micro-tomosynthesis component). The tomographic imager may provide global information for determination of a region/volume of interest, extraction of the surface of the head as the reference, and assistance of local micro-tomography/local micro-tomosynthesis. The micro-CT (or micro-tomosynthesis) component may be integrated within the tomographic imager or separated from it, to acquire high-resolution data of the region/volume of interest. Based on the determined region/volume of interest, the CT system may capture the images and transmit the captured images to the tissue classification system <b>105</b>.</p><p id="p-0059" num="0058">II.B. Example Schematic Diagram of an Image Processing System</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an exemplary schematic diagram <b>200</b> of an image processing system for processing images to estimate whether at least part of a tumor is represented in the images according to some embodiments. Input images <b>205</b> may be accessed by an image processing subsystem <b>210</b>. The input images <b>205</b> may be three-dimensional images derived from data captured using a CT system, including a micro-CT system. Each of the input images <b>205</b> may depict a three dimensional representation corresponding to the biological structure. Each of the input images <b>205</b> may be captured at a specific point in time. For example, an input image of the input images <b>205</b> may be captured at a time point at which no lesions or tumors are detected. Another input image of the input images <b>205</b> can be captured at another time point at which the lesions or tumors are visible in the image. The input images <b>205</b> may or may not be captured using a respiratory gating operation. For example, an imaging system (e.g., the imaging system <b>130</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) can use the respiratory gating operation to identify a specific phase (e.g., inhale, exhale) of the subject's respiratory cycle. The imaging system may then capture each of the input images <b>205</b> during a particular phase that corresponds to the pre-identified phase. In some instances, the input images <b>205</b> are images depicting ROIs, in which an image region depicting the biological structure including heart, lung, and tumor is cropped from an original input image.</p><p id="p-0061" num="0060">Filters can be applied to each of the input images <b>205</b> to produce the filtered images. A first filter may be applied to each of the input images <b>205</b> to isolate a biological structure of interest from the image. In some instances, the first filter includes an image mask (e.g., a lung mask) specifically used to isolate a particular biological structure (e.g., the lung). Additionally or alternatively, the first filter can include another type of image mask (e.g., a thoracic-cavity mask) that can be used to isolate multiple biological structures of interest (e.g., the lung, heart, liver, mediastinum, tumor, blood vessels). A second filter may be additionally used for each of the input images to identify particular skeletal structures appearing in the input image. Image portions corresponding to the particular skeletal structures can be removed from each of the input images <b>205</b>. The identified skeletal structures can be alternatively used as boundaries surrounding the biological structure, at which the boundaries can be used to adjust the size of a region corresponding to the biological structure of interest.</p><p id="p-0062" num="0061">In some instances, a trained image-preprocessing machine-learning model is applied to the input image to generate the filters <b>215</b>. For example, a trained V-Net can be used to process the input image to identify the biological structure of interest (e.g., the heart). In another example, a trained U-Net can be used to process the input image to identify the biological structure of interest. The trained image-preprocessing model can thus be used to generate one or more filters (e.g., a lung mask), to isolate the biological structure (e.g., the lung) from other regions of the input image (e.g., rib cage, blood vessels).</p><p id="p-0063" num="0062">A binary converter <b>220</b> may transform the filtered images into binary images. The binary converter <b>220</b> may convert each pixel of at least part of a filtered image into either 0 or 1 pixel value. For example, the filtered image can be a grayscale image, which can be converted into the binary image by the binary converter <b>220</b>. The binary converter <b>220</b> may additionally convert binary values corresponding to a subset of pixels (e.g., 0 pixel surrounded by 1 pixels) in the binary images based on a fill operation. The binary converter <b>220</b> may thus remove background signals and image objects in the filtered image. In effect, by transforming the filtered images into binary images, borders surrounding each image object can be sharpened and may thus generate an output that is optimized for subsequent image processing operations. For example, image registration operations may better align the images as a result of the transformation. This is because the binary images with sharpened borders can be properly aligned with those of a reference image. In another example, the generation of binary images can allow image segmentation algorithms to produce a more accurate set of image objects, due to the image objects being more clearly defined based on their sharpened borders. Erosion-dilation operations can be applied to further reduce image noise of the binary images. In some instances, the binary converter <b>220</b> transforms the filtered images into the binary images after a registration operation is performed.</p><p id="p-0064" num="0063">An image registration subsystem <b>225</b> can process the binary images using an image registration operation to produce registered images. The image registration operation (e.g., rotation, scale, translation, shear) may include associating the binary images to a reference image, in order to align the binary images into a single spatial coordinate system. Because the registered images are aligned in the single spatial coordinate system, the subsequent operations (e.g., an image segmentation algorithm) may use similar location parameters (e.g., x, y, and z coordinates) for the set. Further, training machine-learning models with registered images may result in an increase of accuracy during deployment. Otherwise, a location of each image object being expressed by x, y, and z coordinates may correspond differently from one image to another, unless all images are registered within the same coordinate system. The image registration subsystem <b>225</b> may register the binary images using a reference filter to derive transformation parameters from the filtered images. For example, the bone mask filter of a reference image can be used as the reference filter which can be applied to bone mask filters corresponding to the set of filtered images. As a result, a first set of transformation parameters (e.g., tform) can be identified. For each of the binary images, the first set of transformation parameters can be applied by the image registration subsystem <b>225</b> such that the binary images can be warped towards the location coordinates corresponding to the bone mask filter. The filtered images can be warped using the first set of transformation parameters as well, such that they can be used to generate additional sets of transformation parameters.</p><p id="p-0065" num="0064">The image registration subsystem <b>225</b> may additionally apply a second registration operation to the warped images (e.g., warped images based on the first set of transformation parameters) using the reference image. Each of the warped filtered images can be aligned by the image registration subsystem <b>225</b> to corresponding regions of the reference image. Once the alignment is completed, a second set of transformation parameters can be identified (e.g., tform_grayscale). The warped images generated based on the reference filter can be warped again using the second set of transformation parameters to produce the registered images. The second registration operation using the reference image can allow the registered images to be further calibrated towards the single coordinate system, such that training of the machine-learning models as well as other image processing operations can become more efficient.</p><p id="p-0066" num="0065">An image segmentation subsystem <b>230</b> can process the registered images using a segmentation algorithm to extract image objects corresponding to each registered image. The segmentation algorithm can enable identification of morphological features of each of the registered images and by defining boundaries of the image objects by using the morphological features. The image segmentation subsystem <b>230</b> can use the identified boundaries to extract each image object from the registered image. Various techniques can be used to extract the image objects from the registered images, including watershed segmentation algorithms, graph partitioning algorithms, and model-based segmentation algorithms.</p><p id="p-0067" num="0066">For each image object of the extracted image objects, a feature extractor <b>235</b> may identify its structural characteristics. The feature extractor <b>235</b> may identify other types of characteristics from each of the image objects including, but not limited to, location coordinates of the center of mass of the image object, Eigenvalues associated with the voxels forming the image object, and Euler angles associated with the image object.</p><p id="p-0068" num="0067">II.C. Example Schematic Diagram of Applying Image Filters to Input Images</p><p id="p-0069" num="0068">As described herein, filters can be applied to each of the input images to produce the filtered images. A first filter may be applied to each of the input images to isolate a biological structure of interest from the image. A trained U-Net (for example) can be used to generate the first filter, such that a particular organ such as the lung can be identified and isolated. In some instances, the first filter includes an image mask (e.g., a lung mask) specifically used to isolate a particular biological structure (e.g., the lung). Additionally or alternatively, the first filter can include another type of image mask (e.g., a thoracic-cavity mask) that can be used to isolate multiple biological structures of interest (e.g., the lung, heart, liver, mediastinum, tumor, blood vessels). A second filter may be additionally used for each of the input images to identify particular skeletal structures appearing in the input image. Image portions corresponding to the particular skeletal structures can be removed from each of the input images. The identified skeletal structures can be alternatively used as boundaries surrounding the biological structure, at which the boundaries can be used to adjust the size of a region corresponding to the biological structure of interest.</p><p id="p-0070" num="0069">II.C.1 Generating Image Filters Using Image-Preprocessing Machine-Learning Models</p><p id="p-0071" num="0070">In some embodiments, a trained image-preprocessing machine-learning model is used to process each input image to generate one or more filters, in which the filters can be used to identify organs, tissues, tumors, and blood vessels depicted in the input image. The trained machine-learning model may additionally or alternatively be used for organ-specific identification for one or more organs such as the spleen, liver, lungs, and kidneys. Based on the identification, the generated filters can be applied to each of the input images, such that subsequent image processing can be focused on image regions corresponding to ROIs identified by the filters.</p><p id="p-0072" num="0071">II.C.1.a Example Training Data Set for Training the Image Preprocessing Machine-Learning Model</p><p id="p-0073" num="0072">The image-preprocessing machine-learning model can be trained using a training data set that includes a plurality of training images. The training images correspond to images, in which ROIs corresponding to the biological structure were identified. In some instances, an ROI of a training image is identified as follows: (i) a technician marks multiple 2-dimensional regions that correspond to the biological structure (e.g., the lungs); (ii) the 2-dimensional regions are propagated into a continuous 3-dimensional ROI; (iii) the 3-dimensional ROI is segmented from the training image; and (iv) tissue volume is calculated from the 3-dimensional ROI. An example training data set can include 3520 CT scans with lung ROIs, in which a subset of the training images can be discarded in which the ROIs are not accurately identified (e.g., missing ROI, corrupted file, inaccurate identification of the biological structure).</p><p id="p-0074" num="0073">II.C.1.b Generating Image Filters Using U-Net</p><p id="p-0075" num="0074">The three-dimensional training images can be used to train the machine-learning model (e.g., a U-Net), which in turn can generate the filters for identifying biological structures of interest. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a U-Net <b>300</b> may include a contracting path <b>305</b> and an expansive path <b>310</b>, which gives it a u-shaped architecture. The contracting path <b>305</b> is a CNN network that includes repeated application of convolutions (e.g., 3&#xd7;3 convolutions (unpadded convolutions)), each followed by a rectified linear unit (ReLU) and a max pooling operation (e.g., a 2&#xd7;2 max pooling with stride 2) for downsampling. At each downsampling step or pooling operation, the number of feature channels may be doubled. During the contraction, the spatial information of the image data is reduced while feature information is increased. The expansive path <b>310</b> is a CNN network that combines the feature and spatial information from the contracting path <b>305</b> (upsampling of the feature map from the contracting path <b>305</b>). The upsampling of the feature map is followed by a sequence of up-convolutions (upsampling operators) that halves the number of channels, concatenations with a correspondingly cropped feature map from the contracting path <b>305</b>, repeated application of convolutions (e.g., two 3&#xd7;3 convolutions) that are each followed by a rectified linear unit (ReLU), and a final convolution (e.g., one 1&#xd7;1 convolution) to generate the two-dimensional tumor masks. In order to localize, the high-resolution features from the contracting path <b>305</b> are combined with the upsampled output from the expansive path <b>310</b>. The U-Net <b>300</b> uses the valid part of each convolution without any fully connected layers, i.e., the segmentation map only contains the pixels for which the full context is available in the input image, and uses skip connections that link the context features learned during a contracting block and the localization features learned in an expansion block.</p><p id="p-0076" num="0075">In conventional U-Net architecture, convolutional blocks are composed of convolutional layers (e.g., typically two or three layers) for performing the convolutions. However, in accordance with various embodiments, the convolutional blocks and convolutional layers are replaced with residual blocks <b>315</b> with separable convolutions performed in pyramidal layers <b>320</b> (a single convolutional layer may be replaced with two or more pyramidal layers <b>320</b>) at one or more levels of dilation. (e.g., stacked filtered images). <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> illustrates the layer structure of one of the residual blocks <b>315</b> illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. As shown, a residual block <b>400</b> may comprise multiple pyramidal layers <b>405</b>. In a network (e.g., a ResNet) comprising residual blocks <b>400</b>, each pyramidal layer <b>405</b> feeds into the next layer (A, B, C . . . ) and directly into the layers about 2-3 layers away (D, E . . . ). The use of residual blocks <b>400</b> in the network helps to overcome a degradation problem that occurs from increasing the number of pyramidal layers (if the number of layers keeps increasing, accuracy will increase at first but will start to saturate at one point and eventually degrade). The residual blocks <b>400</b> skip some of these additional pyramidal layers using the skip-connections or residual connections, which ultimately propagates larger gradients to initial pyramidal layers. Skipping effectively simplifies the network, using fewer pyramidal layers in the initial training stages. This speeds learning by reducing the impact of vanishing gradients, as there are fewer layers to propagate through (i.e., multi-speed residual learning). The network then gradually restores the skipped layers as it learns the feature space.</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> illustrates a single pyramidal layer <b>405</b> of <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, in accordance with various embodiments. As shown in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, the pyramidal layer <b>405</b> may use dilated (atrous) separable convolutions at multiple different scales (&#x2018;dilation blocks&#x2019;), in this example four levels. The pyramidal layer <b>405</b> comprises the same image at the multiple different scales in order to increase accuracy in detecting objects (e.g., a tumor). A dilated (atrous) convolution refers to a filter with a &#x201c;spread out&#x201d; receptive field, which increases the size of the receptive field relative to the kernel size. In some embodiments, the one or more levels of dilation is four levels of dilation. In other embodiments, greater or fewer levels of dilation could be used, for example, six levels of dilation. The convolutional layer output <b>415</b> are output of the dilation blocks <b>420</b> (here labeled as Dilations <b>1</b>, <b>2</b>, <b>4</b>, and <b>8</b>). The illustrated example of <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> assumes four dilation blocks and that each dilation block outputs two channels (of the same color), so the total number of channels output is eight. The number of channels output by each dilation block may vary depending on the residual block in question. The example of <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> illustrates the top left or top right residual block <b>315</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In some embodiments, the number of each of the channels output by each dilation block <b>415</b> in a pyramidal layer <b>410</b> of a residual block <b>405</b> is equal to the k number of filters on the residual block <b>405</b> divided by four.</p><p id="p-0078" num="0077">Empirical evidence shows that the residual blocks allow a gain of accuracy and an easier optimization. Separable convolutions, depthwise convolutions followed by pointwise convolutions, have also shown a large gain in convergence speed and a significant reduction of the model size. Dilated convolutions expand the receptive field without loss of resolution allowing hence to aggregate multi-scale contextual information down sampling. The redesign of the convolutional blocks allows for extracting very localized and rare information in the image.</p><p id="p-0079" num="0078">II.C.1.c Generating Image Filters Using V-Nets</p><p id="p-0080" num="0079">A model (e.g., a three-dimensional convolutional neural network such as a V-Net for three-dimensional segmentation) may comprise downsampling and upsampling subnetworks, with skip connections to propagate higher resolution information to the final segmentation. In some instances, the downsampling subnetwork may be a sequence of multiple dense feature stacks connected by downsampling convolutions, each skip connection may be a single convolution of the corresponding dense feature stack output, and the upsampling network comprises bilinear upsampling to the final segmentation resolution.</p><p id="p-0081" num="0080">The trained image pre-processing machine-learning model can be used to extract features from the input image (e.g., micro-CT image) using a convolutional neural network (CNN) system that includes multiple distinct submodels to identify the ROIs in the image. As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a trained V-Net <b>500</b> may be used to refine the input image. The trained V-Net <b>500</b> can perform a series of operations that form a single convolutional layer for several types of CNN architecture elements: (1) convolution; (2) non-linearity conversion (e.g., ReLU); (3) pooling or sub sampling; and (4) classification (Fully Connected Layer). In some instances, convolution operation of the trained V-Net <b>500</b> preserves the spatial relationship between pixels or voxels within at least one of the two-dimensional or three-dimensional scans or across two-dimensional or three-dimensional scans by learning image features using small squares of input data. For example, the input image can be considered as a matrix of pixel and voxel values, in which each pixel and voxel area of the matrix can be assigned with values. In addition, the input image can include black-and-white images having pixel or voxel values ranging from 0 and 1. The input image may alternatively or additionally include color images that have three assigned RGB pixel or voxel values ranging from 0 and 255.</p><p id="p-0082" num="0081">After the input image is accessed, the trained V-Net <b>500</b> may perform convolutions on the input image to extract features corresponding to anatomical regions depicted in the image. The left side of the trained V-Net <b>500</b> may include a compression path <b>510</b> for downsampling, and the right side may include a decompression path <b>515</b> for upsampling that decompresses the signal until its original size is reached. The compression path <b>510</b> may be divided in different stages that operate at different resolutions. Each stage may comprise one or multiple convolutional layers. Convolutions within each of the layers may be applied with appropriate padding. Each stage may be configured such that it learns a residual function via a residual connection: the input of each stage is (i) used in the convolutional layers and processed through the non-linearities, and (ii) added to the output of the last convolutional layer of that stage in order to enable learning a residual function. The convolutions performed in each stage use volumetric kernels having a predetermined size such as 5&#xd7;5&#xd7;5 voxels. As the data proceeds through different stages along the compression path <b>510</b>, its resolution may be reduced. Each stage along the compression path <b>510</b> can be performed through convolution with predetermined size kernels such as 2&#xd7;2&#xd7;2 voxel wide kernels applied with an appropriate stride (e.g., a slide of 2). Since the second operation extracts features by considering only non-overlapping volume patches, the size of the resulting feature maps can be halved (sub-sampled). This strategy can serve a similar purpose as pooling layers. Replacing pooling operations with convolutional ones can result in a network with a smaller memory footprint, because no switches mapping the output of pooling layers back to their inputs are needed for back-propagation. Each of the stages of the compression path <b>510</b> may compute a number of features which is multiple times higher than the number of feature from the previous layer.</p><p id="p-0083" num="0082">The decompression path <b>515</b> can extract features and expands the spatial support of the lower resolution feature maps in order to gather and assemble the necessary information to output a two channel volumetric segmentation corresponding to the ROIs of the image. After each stage of the decompression path <b>515</b>, a de-convolution operation may be employed in order increase the size of the inputs followed by one or multiple convolutional layers involving half the number of kernels such as 5&#xd7;5&#xd7;5 kernels employed in the previous layer. Similar to the compression path <b>510</b>, residual functions may be learned in the convolutional stages of decompression path <b>515</b>. Additionally, the features extracted from early stages of the compression path <b>510</b> may be forwarded to the decompression path <b>515</b>, as shown by the horizontal connections <b>520</b>. The two feature maps computed by the very last convolutional layer, having an appropriate kernel size such as 1&#xd7;1&#xd7;1 kernel size and producing outputs of the same size as the input volume (two volumes having the same resolution as the original input data), may be processed through a soft-max layer which outputs the probability of each voxel belonging to foreground and to background max voxelwise. Based on the probability outputted by the softmax layer, the trained image pre-processing machine-learning model can indicate a probability as to whether a particular voxel corresponds to an image region of the ROIs. Accordingly, the outputs generated by the trained V-Net <b>500</b> for all voxels in the input image may indicate multiple image regions corresponding to the ROIs, which may include a tissue, blood vessels, a tumor, and a mediastinum.</p><p id="p-0084" num="0083">After the ROIs are identified, the ROIs can be used to generate the filters for isolating the biological structure from the input images. The use of the trained image pre-processing machine-learning model may improve accuracy of classification of images, since only relevant portions of the image are being classified.</p><p id="p-0085" num="0084">II.C.1.d Example Results</p><p id="p-0086" num="0085"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows an example set of graphs <b>600</b> that indicate performance of the trained image-preprocessing machine-learning model. In a linear regression graph <b>602</b>, each plot point represents, for a corresponding image, an amount of tissue volume identified manually by tracing the image (x-axis) and an amount of tissue volume detected by a trained V-Net (y-axis). In the graph <b>602</b>, the coefficient of determination (R2) is 0.96, which demonstrates accuracy of the V-Net in detecting tissue volumes across images that include different amounts of tissue volume.</p><p id="p-0087" num="0086">In a linear regression graph <b>604</b>, each plot point represents, for a corresponding image, an amount of tissue volume identified manually by tracing the image (x-axis) and an amount of tissue volume detected the image by a rules-based analysis (y-axis). For this example, the rules-based analysis is implemented using techniques discussed in Barck, Kai H et al. &#x201c;Quantification of Tumor Burden in a Genetically Engineered Mouse Model of Lung Cancer by Micro-CT and Automated Analysis.&#x201d; Translational oncology vol. 8,2 (2015): 126-35. In the graph <b>604</b>, the coefficient of determination (R2) is 0.72. Using the manually-traced tissue volume as a point of reference, the trained V-Net detected tissue volumes more accurately than the tissue volumes detected by the rules-based analysis. Such accuracy can indicate an improved performance of V-Nets in generating filters for isolating the tissue (e.g., the lungs) from the corresponding images.</p><p id="p-0088" num="0087">II.C.2 Applying Image Filters</p><p id="p-0089" num="0088"><figref idref="DRAWINGS">FIGS. <b>7</b>A-B</figref> illustrate exemplary schematic diagrams <b>700</b> of image filter subsystems that identify biological structures from input images according to some embodiments. Image filters <b>715</b> of an image processing system (e.g., the image processing system <b>310</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>) can access a set of input images <b>705</b> that depict a biological structure of a subject (e.g., an organ, blood vessels, a tumor). For each of the set of input images <b>705</b>, the image filters <b>715</b> may apply image masks and generate a set of filtered images. Each of the generated filtered images depicts the biological structure of interest (e.g., the lung) being distinguished from background portions of the input image.</p><p id="p-0090" num="0089">In <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, a first image filter <b>720</b> may generate a first set of image masks corresponding to the set of input images <b>705</b>. In each image mask of the first set of image masks, the first image filter <b>720</b> can specify one or more biological structures that can be identified in the image, while image regions that do not correspond to the specified biological structures (e.g., the background portions) can be obscured or removed. The first image filter <b>720</b> may apply each generated image mask of the first set of image masks to each of the input images <b>705</b>. By applying the first set of image masks on the set of input images <b>705</b>, the first image filter <b>720</b> may generate a first set of filtered images <b>730</b>. For each filtered image in the set of filtered images, at least a part of the filtered image (e.g., the lung, thoracic cavity) can be distinguished from other portions the filtered image (e.g., the rib cage, air). Additional details regarding the use of the image masking operations to identify the biological structures as disclosed herein are set forth in Barck K H, et al. &#x201c;Quantification of Tumor Burden in a Genetically Engineered Mouse Model of Lung Cancer by Micro-CT and Automated Analysis.&#x201d; Transl Oncol. 2015; 8:126-135 and Wyatt S K, et al. &#x201c;Fully-automated, high-throughput micro-computed tomography analysis of body composition enables therapeutic efficacy monitoring in preclinical models.&#x201d; Int J Obes (Lond). June 2015, which are both incorporated herein by reference in their entirety for all purposes.</p><p id="p-0091" num="0090">In <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, a second image filter <b>725</b> may also process each of the set of input images <b>705</b> to generate a second set of image masks. By using the second image mask, the second image filter <b>725</b> can generate a second set of filtered images <b>740</b>. For each of the second set of filtered images <b>740</b>, different parts of the filtered image can be identified for removal. For example, a filtered image of the second set of filtered images <b>740</b> can indicate image regions corresponding to the particular skeletal structures which can removed, whereas other portions of the filtered image remain (e.g., the lung). The first image filter <b>720</b> can also use the second set of image masks to draw boundaries that define the biological structure of interest (e.g., the lung) and generate the first set of filtered images <b>735</b>, such that image regions outside the boundaries can be removed. The second set of filtered images can be transmitted to a binary converter subsystem <b>735</b>.</p><p id="p-0092" num="0091">II.E. Example Schematic Diagram of a Binary Converter of an Image Processing System</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an example schematic diagram <b>800</b> of a binary converter subsystem that transforms the filtered images into binary images according to some embodiments. A binary converter subsystem <b>810</b> (e.g., the binary converter subsystem <b>835</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>) can access a set of filtered images <b>830</b> which were outputted from image filters (e.g., the image filters <b>815</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>). The binary converter subsystem <b>810</b> can binarize each pixel of each filtered image of the filtered images <b>830</b> based on the pixel's corresponding radiodensity value. For example, a pixel associated with an HU above a determined threshold can be configured as 1 pixel value, and another pixel associated with an HU below the threshold can be configured as 0 pixel value. The binary converter subsystem <b>810</b> can use a binarize operation, in which a threshold range can be determined so as to minimize variance of the black and white pixels. For example, the binary converter subsystem <b>810</b> can set the threshold range between &#x2212;200 HU and 400 HU, at which any pixels associated with the HU outside of the threshold range can be converted into the 0 pixel value. As a result, a set of binary images <b>835</b> can be outputted in which pixel values of each binary image are configured as either 1 or 0.</p><p id="p-0094" num="0093">The binary converter subsystem <b>810</b> may additionally perform a fill operation for each of the set of binary images <b>835</b> to remove holes within a region of the image. For example, the binary image can include a group of pixels having 0 values (e.g., hole) that are enclosed by a larger group of pixels having 1 values (e.g., region surrounding the hole). The binary converter system can perform the fill operation on the group of 0-value pixels to output a new binary image in which the two groups of pixels are merged into a new group of pixels having 1 values (e.g., region without holes).</p><p id="p-0095" num="0094">Each of the set of binary images <b>835</b> can additionally be processed through an erosion operation <b>815</b> and/or a dilation operation <b>820</b>. By performing the erosion operation <b>815</b> and the dilation operation <b>820</b>, small image objects that cannot be accurately classified can be removed from each binary image of the set of binary images <b>835</b>. Removing smaller image objects from classification can improve performance of subsequent segmentation operations. The erosion operation <b>815</b> may access a binary image of the set of binary images and initialize an image-processing kernel (e.g., 5&#xd7;5 kernel) at a first pixel within the binary image. As the image-processing kernel traverses through each pixel of the binary image, a pixel with a value of 1 (e.g., white) may be converted into 0 (e.g., black) if at least one neighboring pixel covered by the kernel has a value of 0. In effect, the erosion operation <b>815</b> erodes the boundaries of the image object appearing in the binary image. In addition, the erosion operation <b>815</b> may remove any salt noise depicted in the binary image, specifically any sparsely occurring white pixels that are not a part of the image object.</p><p id="p-0096" num="0095">The dilation operation <b>820</b> may access the binary image processed by the erosion operation <b>815</b>. Similar to the erosion operation <b>815</b>, the image-processing kernel is set on the first pixel of the binary image. As the image-processing kernel traverses through each pixel of the processed binary image, a pixel with a value of 0 (e.g., black) may be converted into 1 (e.g., white) if at least one neighboring pixel covered by the kernel has a value of 1. In effect, the dilation operation <b>820</b> may gradually enlarge the boundaries corresponding to the biological structures depicted in the processed binary image. In addition, similar to a hole filling operation, the dilation operation <b>820</b> may remove any pepper noise depicted in binary image, specifically any sparsely occurring black pixels that are within the image object.</p><p id="p-0097" num="0096">Performing the erosion operation <b>815</b> followed by the dilation operation <b>820</b> can remove salt noise from each of the set of binary images <b>835</b>. The sequence of such operations can be referred to as an opening operation. Conversely, performing the dilation operation <b>820</b> followed by the erosion operation can remove pepper noise from each of the set of binary images <b>835</b>. The sequence of dilation followed by erosion can be referred to as closing operation. By performing the erosion operation <b>815</b> and/or the dilation operation <b>820</b> in different order, the binary converter subsystem <b>810</b> can remove any sharp and sudden disturbances in each of the set of binary images <b>835</b>. The set of binary images can be provided to the image registration subsystem.</p><p id="p-0098" num="0097">II.F. Example Schematic Diagram of an Image Registration Subsystem of an Image Processing System</p><p id="p-0099" num="0098"><figref idref="DRAWINGS">FIGS. <b>9</b>A-B</figref> illustrate example schematic diagrams <b>900</b> of image registration subsystem that registers binary images into a single spatial coordinate system according to some embodiments. Specifically, the image registration subsystem <b>910</b> may process these sets of images using a series of image registration operations. An image registration operation can refer to an optimization process, where the &#x201c;moving&#x201d; image is warped such that pairwise distance between all voxels in the &#x201c;moving&#x201d; image relative to those in the &#x201c;fixed&#x201d; or reference image is minimized. The image registration operations can be performed such that the processed images are aligned on corresponding regions of the reference image (e.g., skeletal structures, lungs). Each image registration operation can result in a set of output images that are better scaled, aligned, and rotated to fit in a single coordinate system. Subsequent image registration operations may calibrate and consequently improve the alignment of the set of output images. By using the registered images for training machine-learning models, the image registration operations can improve accuracy and performance of the trained machine-learning models.</p><p id="p-0100" num="0099">In <figref idref="DRAWINGS">FIG. <b>9</b>A</figref>, an image registration subsystem <b>910</b> may access a set of filtered images <b>905</b> (e.g., the first and second sets of filtered images of <figref idref="DRAWINGS">FIG. <b>6</b></figref>) and a set of binary images <b>915</b> (e.g., the binary images <b>835</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>). During a first registration operation <b>920</b>, the image registration subsystem <b>910</b> may initially select a reference image from the set of filtered images <b>905</b>. The reference image may refer to a filtered image that includes location parameters, such that the set of binary images <b>915</b> can be aligned based on the location parameters. From the reference image, a reference filter can be generated that corresponds to image regions of the reference image that depict skeletal structures (e.g., a reference bone mask). Filters of the set of filtered images <b>905</b> (e.g., bone mask filters) corresponding to the reference filter can be registered such that the filtered images of the first subset can be aligned, as shown in a first set of aligned images <b>925</b>. By registering the filters, a first set of transformation parameters (e.g., tform) can be determined. The first set of transformation parameters can indicate an estimated set of numerical values corresponding to affine or linear transformation (e.g., rotation, scale, translation, shear) applied to each filtered image of the set of filtered images <b>905</b> to the reference image.</p><p id="p-0101" num="0100">The image registration subsystem <b>910</b> may apply the first set of transformation parameters to each of the set of filtered images <b>905</b> and the set of binary images <b>915</b> using an image warping operation <b>930</b>. For each of the set of binary images <b>915</b>, the image warping operation <b>930</b> may warp the binary image using the first set of transformation parameters to adjust its position and orientation. Each filtered image of the set of filtered images <b>905</b> can also be warped using the first transformation parameters, such that the warped filtered images can be used to generate transformation parameters for subsequent registration operations. The image registration subsystem <b>910</b> may output the warped set of filtered images <b>905</b> and the set of binary images <b>915</b> as a set of warped images <b>935</b>.</p><p id="p-0102" num="0101">In <figref idref="DRAWINGS">FIG. <b>9</b>B</figref>, the image registration subsystem <b>910</b> may access and perform a second registration operation <b>940</b> on each of the set of warped images <b>935</b>. During the second registration operation <b>940</b>, the image registration subsystem <b>910</b> may use the same reference image that was used by the first registration operation <b>920</b> of <figref idref="DRAWINGS">FIG. <b>9</b>A</figref>. The reference image can include a reference filtered grayscale image. The reference image can be used to register each filtered image (e.g., a filtered grayscale image) of the set of filtered images <b>905</b>, such that the filtered images of the set of filtered images <b>905</b> can be aligned towards the reference image, as shown in a second set of aligned images <b>950</b>. By registering the filtered images of the set of filtered images <b>905</b>, a second set of transformation parameters can be determined (e.g., tform grayscale). The second set of transformation parameters can indicate an estimated set of numerical values corresponding to affine or linear transformation (e.g., rotation, scale, translation, shear) applied to each filtered image of the set of warped images <b>935</b> to the reference image.</p><p id="p-0103" num="0102">The image registration subsystem <b>910</b> may apply the second set of transformation parameters to each of the set of warped images <b>935</b> using the image warping operation <b>930</b>. For each of the set of warped images <b>935</b>, the image warping operation <b>930</b> may warp the warped image again using the second set of transformation parameters to adjust its position and orientation. The image registration subsystem <b>910</b> may output the warped set of filtered images <b>905</b> and the set of binary images <b>915</b> as a set of registered images <b>955</b>. In some instances, only the first registration operation <b>920</b> is performed to generate the set of registered images <b>955</b>. For example, the set of warped images <b>935</b> are the set of registered images <b>955</b> that can be subsequently processed by the image segmentation subsystem <b>1005</b>.</p><p id="p-0104" num="0103">II.G. Example Schematic Diagram of an Image Segmentation Subsystem of an Image Processing System</p><p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an example schematic diagram <b>1000</b> of an image segmentation subsystem that extracts a set of image objects from a registered image according to some embodiments. An image segmentation subsystem <b>1005</b> may access a set of registered images <b>1010</b>. The set of registered images <b>1010</b> may include one or more binary images (e.g., the set of binary images <b>835</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>) applied with one or more registration operations (e.g., the first registration operation <b>920</b> of <figref idref="DRAWINGS">FIG. <b>9</b>A</figref>, the second operation <b>940</b> of <figref idref="DRAWINGS">FIG. <b>9</b>B</figref>) For each of the set of registered images <b>1010</b>, the image segmentation subsystem <b>1005</b> may perform a watershed segmentation to extract a set of image objects. To perform the watershed segmentation, the image segmentation system <b>1005</b> may erode each registered image of the set of registered images <b>1010</b> and identify new image objects in the registered image as they become separated and vanish. The image segmentation system <b>1005</b> may then create seed points for each of the new image objects, at which each seed point of the seed points can be conditionally dilated (layers added) until the registered image is filled. These operations can be repeated, such that the set of image objects can be extracted from the registered image.</p><p id="p-0106" num="0105">Additionally or alternatively, the image segmentation subsystem <b>1005</b> may reverse pixels of each registered image of the set of registered images <b>1010</b> to transform the registered image into its complement image. For example, black pixels corresponding to a part of the registered image can be converted into white pixels, and white pixels corresponding to another part of the registered image can be converted into black pixels. As the watershed segmentation continues, the image segmentation subsystem <b>1005</b> may process each complement image corresponding to the set of registered images <b>1010</b> by using a negative distance transform operation. For each pixel corresponding to each complement image, the negative distance transform operation may be performed to identify a distance value from the pixel to a nearest nonzero-valued pixel, compute a negative value of the identified distance value, and transform the pixel based on the computed negative value. As a result of transforming the pixels corresponding to the complement images, the image segmentation subsystem <b>1005</b> may generate a set of transformed images and process the set of transformed images through a segmentation algorithm (e.g., a watershed algorithm). The segmentation algorithm may generate a set of segmented images <b>1015</b>, in which a set of lines can be depicted in each of the set of segmented images <b>1015</b>. For each segmented image of the set of segmented images <b>1015</b>, the image segmentation subsystem <b>1005</b> may extract a set of image objects by using the set of lines as boundaries corresponding to each image object represented in the segmented image.</p><p id="p-0107" num="0106">II.H. Example Schematic Diagram of a Feature Extractor Subsystem of an Image Processing System</p><p id="p-0108" num="0107">II.H.1 Identifying Structural Characteristics of Image Objects</p><p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates an example schematic diagram <b>1100</b> of a feature extractor subsystem that identifies a set of structural characteristics associated with an image object according to some embodiments. A set of segmented images <b>1105</b> (e.g., the set of segmented images <b>1015</b>) can be accessed. For each segmented image of the set of segmented images <b>1105</b>, a set of image objects can be identified based on a set of lines depicted in the segmented image. A feature extractor <b>1110</b> may access the set of image objects and determine the set of structural characteristics corresponding to each image object of the set of image objects. Each structural characteristic of the set of structural characteristics includes a value that corresponds to a particular structural-characteristic category (e.g., diameter, surface area, a shape, centrally convex volume, voxel intensity). For each image object, the feature extractor <b>1110</b> may store the set of structural characteristics corresponding to the image object in a data structure <b>1115</b> (e.g., an array), in which an identifier may be assigned to identify the image object. The data structure <b>1115</b> storing the set of structural characteristics of the image object can be transmitted to a trained machine-learning model, which may estimate whether the image object corresponds to a tumor based on the determined structural characteristics.</p><p id="p-0110" num="0109">Various types of the set of structural characteristics can be determined by the feature extractor <b>1110</b>, which can be processed and considered by the trained machine-learning model. The trained machine-learning model can include various types of machine-learning models, such as a discriminant-analysis model, a kernel classification model, a k-nearest neighbor model, a linear classification model, a Naive Bayes classifier, a support vector machine, a gradient-boosted Ensemble classification algorithm, and/or one or more classification trees. For example, the set of structural characteristics may include, but are not limited to, structural characteristics corresponding to the following structural-characteristic categories:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0110">&#x201c;Boundingbox&#x201d;&#x2014;Smallest cuboid containing the region, returned as a 1-by-6 vector of the form [ulf_x ulf_y ulf_z width_z width_y width_z]. ulf_x, ulf_y, and ulf_z specify the upper-left front corner of the cuboid. width_z, width_y, and width_z specify the width of the cuboid along each dimension;</li>        <li id="ul0002-0002" num="0111">&#x201c;Centroid&#x201d;&#x2014;Center of mass or the region, returned as a 1-by-3 vector of the form [centroid_x centroid_y and centroid_z]. The first element centroid_x is the horizontal coordinate (or x-coordinate) of the center of mass. The second element, centroid_y is the vertical coordinate (or y-coordinate). The third element, centroid_z, is the planar coordinate (or z-coordinate);</li>        <li id="ul0002-0003" num="0112">&#x201c;ConvexHull&#x201d;&#x2014;Smallest convex polygon that can contain the region, returned as a p-by-3 matrix. Each row of the matrix contains the x-, y-, and z-coordinates of one vertex of the polygon;</li>        <li id="ul0002-0004" num="0113">&#x201c;ConvexImage&#x201d;&#x2014;Image of the convex hull, returned as a volumetric binary image (logical) with all voxels within the hull filled in (set to &#x2018;on&#x2019;). The image is the size of the bounding box of the region;</li>        <li id="ul0002-0005" num="0114">&#x201c;ConvexVolume&#x201d;&#x2014;Number of voxels in &#x201c;ConvexImage&#x201d;, returned as a scalar;</li>        <li id="ul0002-0006" num="0115">&#x201c;EigenValues&#x201d;&#x2014;Eigenvalues of the voxels representing a region, returned as a 3-by-1 vector. For example, regionprops3 operation may use the eigenvalues to calculate the principal axes lengths;</li>        <li id="ul0002-0007" num="0116">&#x201c;EigenVectors&#x201d;&#x2014;Eigenvectors or the voxels representing a region, returned as a 3-by-3 vector. For example, regionprops3 operation may use the eigenvectors to calculate the orientation of the ellipsoid that has the same normalized second central moments as the region;</li>        <li id="ul0002-0008" num="0117">&#x201c;EquivDiameter&#x201d;&#x2014;Diameter of a sphere with the same volume as the region, returned as a scalar. Computed as (6*Volume/pi){circumflex over (&#x2003;)}(1/3);</li>        <li id="ul0002-0009" num="0118">&#x201c;Extent&#x201d;&#x2014;Ratio of voxels in the regions to voxels in the total bounding box, returned as a scalar. Computed as the value of Volume divided by the volume of the bounding box. [Volume/(bounding box width*bounding box height*bounding box depth)];</li>        <li id="ul0002-0010" num="0119">&#x201c;Image&#x201d;&#x2014;Bounding box of the region, returned as a volumetric binary image (logical) that is the same size as the bounding box of the region. The &#x2018;on&#x2019; voxels correspond to the region, and all other voxels are &#x2018;off&#x2019;;</li>        <li id="ul0002-0011" num="0120">&#x201c;Orientation&#x201d; (x, y, and z values)&#x2014;Euler angles, returned as a 1-by-3 vector. The angles are based on the right-hand rule. For example, regionprops3 operation may interpret the angles by looking at the origin along the x-, y-, and z-axis representing roll, pitch, and yaw, respectively. A positive angle represents a rotation in the counterclockwise direction. Rotation operations are not commutative so they can be applied in the correct order to have the intended effect;</li>        <li id="ul0002-0012" num="0121">&#x201c;PrincipalAxisLength&#x201d; (x, y, and z values)&#x2014;Length (in voxels) of the major axes of the ellipsoid that have the same normalized second central moments as the region, returned as 1-by-3 vector. For example, regionprops3 operation may sort the values from highest to lowest;</li>        <li id="ul0002-0013" num="0122">&#x201c;Solidity&#x201d;&#x2014;Proportion of the voxels In the convex hull that are also in the region, returned as a scalar. Computed as Volume/ConvexVolume;</li>        <li id="ul0002-0014" num="0123">&#x201c;SubarrayIdx&#x201d;&#x2014;Indices used to extract elements inside the object bounding box, returned as a cell array such that L(idx{:}) extracts the elements of L inside the object bounding box;</li>        <li id="ul0002-0015" num="0124">&#x201c;SurfaceArea&#x201d;&#x2014;Distance around the boundary of the region, returned as a scalar;</li>        <li id="ul0002-0016" num="0125">&#x201c;Volume&#x201d;&#x2014;Count of the actual number of &#x2018;on&#x2019; voxels in the region, returned as a scalar. Volume represents the metric or measure of the number of voxels in the regions within the volumetric binary image, BW;</li>        <li id="ul0002-0017" num="0126">&#x201c;VoxelIdxList&#x201d;&#x2014;Linear Indices of the voxels in the region, returned as a p-element vector; and</li>        <li id="ul0002-0018" num="0127">&#x201c;VoxelList&#x201d;&#x2014;Locations of voxels in the region, returned as a p-by-3 matrix. Each row of the matrix has the form [x y z] and specifies the coordinates of one voxel in the region.</li>    </ul>    </li></ul></p><p id="p-0111" num="0128">II.H.2 Example Structural Characteristics of Image Objects</p><p id="p-0112" num="0129"><figref idref="DRAWINGS">FIGS. <b>12</b> to <b>17</b></figref> show example sets of box plots that identify structural characteristics of image objects detected by an image segmentation subsystem. The box plots of <figref idref="DRAWINGS">FIGS. <b>12</b> to <b>17</b></figref> show a data corresponding to feature arrays (e.g., a feature array generated by the feature extractor <b>1110</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>). The feature arrays represented the training images used to train the machine-learning models for automatically detecting at least part of a tumor from images. For example, the plots show object features for objects of the training images that were manually labeled using a labeling application (e.g., <figref idref="DRAWINGS">FIGS. <b>20</b>A-C</figref>). As the difference between the Tumor and Non-tumor box plots increases across FIGS. <b>12</b>-<b>17</b>, the machine-learning model (e.g., an SVM) will be increasingly likely to be able to distinguish between tumor and non-tumor objects, after being trained. For each of the box plots in <figref idref="DRAWINGS">FIGS. <b>12</b> to <b>17</b></figref>, horizontal line represents a median value (e.g., object volume) representing each class (e.g., tumor, non-tumor), and edges of the &#x201c;box&#x201d; represent values that belong within the 25% and 75% quantiles. The vertical, dashed lines (whiskers) indicate the total range of the values corresponding to each class.</p><p id="p-0113" num="0130"><figref idref="DRAWINGS">FIG. <b>12</b></figref> shows example sets of box plots <b>1200</b> corresponding to distributions of centroid locations of image objects detected by an image segmentation subsystem. Each of box plots <b>1202</b>, <b>1204</b>, and <b>1206</b> shows a first distribution of centroid locations for a first set of image objects classified as tumor regions and a second distribution of centroid locations for a second set of image objects classified as non-tumor regions. In addition, the box plots <b>1202</b> show distributions of centroid locations of the image objects along the x-axis, the box plots <b>1204</b> show distributions of centroid locations of the image objects along the y-axis, and the box plots <b>1206</b> show distributions of centroid locations of the image objects along the z-axis. As shown in the box plots <b>1202</b>, <b>1204</b>, and <b>1206</b>, there are similar distributions of centroid locations between the image objects classified as tumor regions and the image objects classified as non-tumor regions, which may indicate that centroid location features are less likely to be predictive for detecting tumor regions from images, relative to other features.</p><p id="p-0114" num="0131"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows example sets of box plots <b>1300</b> corresponding to distributions of orientations of image objects detected by an image segmentation subsystem. Each of box plots <b>1302</b>, <b>1304</b>, and <b>1306</b> shows a first distribution of orientations for a first set of image objects classified as tumor regions and a second distribution of orientations for a second set of image objects classified as non-tumor regions. A magnitude of orientation is expressed in Euler angle units. In addition, the box plots <b>1302</b> show distributions of orientations of the image objects along the x-axis, the box plots <b>1304</b> show distributions of orientations of the image objects along the y-axis, and the box plots <b>1306</b> show distributions of orientations of the image objects along the z-axis. As shown in the box plots <b>1302</b>, <b>1304</b>, and <b>1306</b>, there are similar distributions of orientations between the image objects classified as tumor regions and the image objects classified as non-tumor regions, which may indicate that object orientation features are less likely to be predictive for detecting tumor regions from images, relative to other features. For example, the box plot <b>1302</b> showed that orientation of objects along the x-axis (plot <b>1302</b> in <figref idref="DRAWINGS">FIG. <b>13</b></figref>) appear similar for Tumor and Non-tumor classes. The box plot <b>1032</b> thus indicates that object orientation is likely not as predictive of tissue class (although it still may be informative), relative to other features (e.g., object volume).</p><p id="p-0115" num="0132"><figref idref="DRAWINGS">FIG. <b>14</b></figref> shows example sets of box plots <b>1400</b> corresponding to distributions of principal axis lengths of image objects detected by an image segmentation subsystem. Each of box plots <b>1402</b>, <b>1404</b>, and <b>1406</b> shows a first distribution of principal axis lengths for a first set of image objects classified as tumor regions and a second distribution of principal axis lengths for a second set of image objects classified as non-tumor regions. In addition, the box plots <b>1402</b> show distributions of principal axis lengths of the image objects along the x-axis, the box plots <b>1404</b> show distributions of principal axis lengths of the image objects along the y-axis, and the box plots <b>1406</b> show distributions of principal axis lengths of the image objects along the z-axis. As shown in the box plots <b>1402</b>, <b>1404</b>, and <b>1406</b>, the principal axis lengths corresponding to the image objects classified as tumor regions are, on average, greater than the principal axis lengths corresponding to image objects classified as non-tumor regions. Such observation may indicate that principal axis lengths are more likely to be predictive for detecting tumor regions from images, relative to other features (e.g., centroid locations shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>).</p><p id="p-0116" num="0133"><figref idref="DRAWINGS">FIG. <b>15</b></figref> shows example sets of box plots <b>1500</b> corresponding to distributions of sizes of image objects detected by an image segmentation subsystem. Each of box plots <b>1502</b>, <b>1504</b>, <b>1506</b>, and <b>1508</b> shows a first distribution of sizes for a first set of image objects classified as tumor regions and a second distribution of sizes for a second set of image objects classified as non-tumor regions. In addition, the box plots <b>1502</b> show distributions of tissue volumes of the image objects expressed in voxel units, the box plots <b>1504</b> show distributions of surface areas of the image objects expressed in distance around boundary of a corresponding image object, the box plots <b>1506</b> show distributions of convex volume of the image objects expressed in volume of smallest convex polygon containing a corresponding image object, and the box plots <b>1508</b> show distributions of equivalent diameters of the image objects expressed in diameter of sphere with same number of voxels measured for a corresponding image object. As shown in the box plots <b>1502</b>, <b>1504</b>, <b>1506</b>, and <b>1508</b>, the size characteristics (e.g., volume, surface area, equivalent diameter) corresponding to the image objects classified as tumor regions are, on average, greater than the size characteristics corresponding to image objects classified as non-tumor regions. Such observation may indicate that size characteristic features are more likely to be predictive for detecting tumor regions from images, relative to other features. For example, the box plot <b>1502</b> in <figref idref="DRAWINGS">FIG. <b>15</b></figref> showed that objects labeled &#x201c;Tumor&#x201d; have an average volume approximately 10 times higher than the non-tumor objects. The box plot <b>1502</b> thus indicates that an object volume is likely to be a meaningful feature and highly predictive to which class the object belongs, respect to using the trained machine-learning model to predict object classes for a new image that is not part of the training set.</p><p id="p-0117" num="0134"><figref idref="DRAWINGS">FIG. <b>16</b></figref> shows example sets of box plots <b>1600</b> corresponding to distributions of shapes of image objects detected by an image segmentation subsystem. Each of box plots <b>1602</b>, <b>1604</b>, and <b>1606</b> shows a first distribution of shapes for a first set of image objects classified as tumor regions and a second distribution of shapes for a second set of image objects classified as non-tumor regions. In addition, the box plots <b>1602</b> show distributions of fractional anisotropies of the image objects. While the median values corresponding to fractional anisotropies were similar for image objects in both tumor and non-tumor regions, the interquartile range of the distribution was greater in images objects classified as non-tumor regions. The box plots <b>1604</b> show distributions of extents of the image objects expressed in tissue volume divided by volume of bounding box. The extents across tissue class were greater in image objects classified as tumor regions. The box plots <b>1606</b> show distributions of solidities of the image objects expressed in tissue volume divided by volume of Convex Hull. The distributions of solidities were greater in image objects classified as non-tumor regions. As shown in the box plots <b>1602</b>, <b>1604</b>, and <b>1606</b>, there are pronounced differences between shapes (e.g., extents, solidities) of the image objects classified as tumor regions and shapes of the image objects classified as non-tumor regions. Such observation may indicate that object shape features are more likely to be predictive for detecting tumor regions from images, relative to other features.</p><p id="p-0118" num="0135"><figref idref="DRAWINGS">FIG. <b>17</b></figref> shows example sets of box plots <b>1700</b> corresponding to distributions of voxel intensities of image objects detected by an image segmentation subsystem. Each of box plots <b>1702</b> and <b>1704</b> shows a first distribution of voxel intensities for a first set of image objects classified as tumor regions and a second distribution of voxel intensities for a second set of image objects classified as non-tumor regions. In addition, the box plots <b>1702</b> show distributions of mean voxel intensities of the image objects expressed in Hounsfield units, and the box plots <b>1704</b> show distributions of maximum voxel intensities of the image objects in Hounsfield units. As shown in the box plots <b>1702</b> and <b>1704</b>, the voxel intensities corresponding to the image objects classified as tumor regions are greater than the voxel intensities corresponding to image objects classified as non-tumor regions, which may indicate that voxel intensity features are more likely to be predictive for detecting tumor regions from images, relative to other features (e.g., centroid locations shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>).</p><p id="p-0119" num="0136">II.I. Example Schematic Diagram of a Classification Subsystem with a Trained Support Vector Machine</p><p id="p-0120" num="0137"><figref idref="DRAWINGS">FIG. <b>18</b></figref> illustrates an exemplary schematic diagram <b>1800</b> of a support vector machine for estimating whether an image object corresponds to a tumor according to some embodiments. A training subsystem <b>1805</b> can train a support vector machine <b>1810</b> into a trained support vector machine <b>1815</b>. To train the support vector machine <b>1810</b>, the training subsystem <b>1805</b> may use training data <b>1820</b>, which may include sets of structural characteristics that correspond to each training image object. Each set of structural characteristics can be respectively assigned with a corresponding label of a set of labels <b>1825</b><i>a</i>-<i>n. </i>For example, a first set of structural characteristics corresponding to a first training image object of the training data <b>1820</b> may correspond to a label <b>1825</b><i>a, </i>and a second set of structural characteristics corresponding to a second training image object of the training data <b>1820</b> may correspond to a label <b>1825</b><i>b. </i>Each label of the set of labels <b>1825</b><i>a</i>-<i>n </i>may indicate whether its corresponding set of structural characteristics indicates a tumor. The sets of structural characteristics can thus be used to characterize the training image in tumor-by-tumor basis in the training image. In effect, the trained machine-learning model can be used to detect individual tumor regions based on their respective structural characteristics (e.g., tissue volume). In some instances, these individual tumor regions are aggregated to identify a measured value for the entire training image, such as level of tumor burden.</p><p id="p-0121" num="0138">II.I.1 Example Training Data for Training the Support Vector Machine</p><p id="p-0122" num="0139">As described herein, the training data <b>1820</b> can correspond to structural characteristics that correspond to each training image object of the training images. The training data <b>1820</b> can represent a plurality of training images that correspond to scanned images of subjects. Each of the training images can be labeled as having no tumor, low tumor burden, medium tumor burden, or high tumor burden. For example, the training data <b>1820</b> included 71 scanned images, which were comprised of 21 images that indicate no tumor, 20 images that indicate low tumor burden, 15 images that indicate medium tumor burden, and 15 images that indicate high tumor burden. In this example, a level of tumor burden can be labeled for each training image based on the following criteria:</p><p id="p-0123" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 1</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Example criteria for tumor burden labeling</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="42pt" align="left"/><colspec colname="2" colwidth="28pt" align="left"/><colspec colname="3" colwidth="98pt" align="left"/><colspec colname="4" colwidth="49pt" align="left"/><tbody valign="top"><row><entry/><entry>Tumor</entry><entry/><entry/></row><row><entry>Labels</entry><entry>level</entry><entry>Blood vessel volume</entry><entry>Tumor volume</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row><row><entry>No tumor</entry><entry>0%</entry><entry>Mean (&#xb1;SD) blood vessel</entry><entry>N/A</entry></row><row><entry/><entry/><entry>volume (mL) = 0.023 &#xb1; 0.0056</entry><entry/></row><row><entry/><entry/><entry>Mean (&#xb1;SD) blood vessel vol</entry><entry/></row><row><entry/><entry/><entry>(% total lung vol) = 10 &#xb1; 2.2</entry><entry/></row><row><entry>Low</entry><entry>&#x3c;20 %</entry><entry>Mean (&#xb1;SD) blood vessel</entry><entry>Mean (&#xb1;SD) </entry></row><row><entry>tumor</entry><entry>of total</entry><entry>volume (mL) = 0.028 &#xb1; 0.0061</entry><entry>tumor volume</entry></row><row><entry>burden</entry><entry>lung</entry><entry>Mean (&#xb1;SD) blood vessel vol</entry><entry>(mL) = 0.018 &#xb1;</entry></row><row><entry/><entry>volume</entry><entry>(% total lung vol) = 11 &#xb1; 2.4</entry><entry>0.014</entry></row><row><entry/><entry/><entry/><entry>Mean (&#xb1;SD)</entry></row><row><entry/><entry/><entry/><entry>tumor vol (%</entry></row><row><entry/><entry/><entry/><entry>total lung vol) = </entry></row><row><entry/><entry/><entry/><entry>6.9 &#xb1; 5.4</entry></row><row><entry>Medium</entry><entry>20-40%</entry><entry>Mean (&#xb1;SD) blood vessel</entry><entry>Mean (&#xb1;SD) </entry></row><row><entry>tumor</entry><entry>of total</entry><entry>volume (mL) = 0.029 &#xb1; 0.012</entry><entry>tumor volume</entry></row><row><entry>burden</entry><entry>lung</entry><entry>Mean (&#xb1;SD) blood vessel vol</entry><entry>(mL) = 0.1 &#xb1;</entry></row><row><entry/><entry>volume</entry><entry>(% total lung vol) = 8.4 &#xb1; 3.5</entry><entry>0.032</entry></row><row><entry/><entry/><entry/><entry>Mean (&#xb1;SD) </entry></row><row><entry/><entry/><entry/><entry>tumor vol (%</entry></row><row><entry/><entry/><entry/><entry>total lung vol) = </entry></row><row><entry/><entry/><entry/><entry>30 &#xb1; 5.2</entry></row><row><entry>High</entry><entry>40-70%</entry><entry>Mean (&#xb1;SD) blood vessel</entry><entry>Mean (&#xb1;SD) </entry></row><row><entry>tumor</entry><entry>of total</entry><entry>volume (mL) = 0.010 &#xb1; 0.008</entry><entry>tumor volume</entry></row><row><entry>burden</entry><entry>lung</entry><entry>Mean (&#xb1;SD) blood vessel vol</entry><entry>(mL) = 0.29 &#xb1; </entry></row><row><entry/><entry>volume</entry><entry>(% total lung vol) = 2.1 &#xb1; 1.8</entry><entry>0.05</entry></row><row><entry/><entry/><entry/><entry>Mean (&#xb1;SD) </entry></row><row><entry/><entry/><entry/><entry>tumor vol (%</entry></row><row><entry/><entry/><entry/><entry>total lung vol) = </entry></row><row><entry/><entry/><entry/><entry>56 &#xb1; 10</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0124" num="0140">From the 71 scanned images, 12 images were identified as hold-out set for testing. In addition, the training data <b>1820</b> can include 1941 training image objects, of which 364 training image objects were identified as the hold-out set for testing.</p><p id="p-0125" num="0141">II.I.2 Feature Selection</p><p id="p-0126" num="0142">The support vector machine <b>1810</b> (or other machine-learning models such as a gradient-boosted algorithms) can be trained using a subset of the set of structural characteristics associated with each training image object. Such feature selection technique reduces the number of input variable for training the machine-learning model to enable efficient use of computing resources and potentially improve performance of the trained machine-learning model by reducing noise.</p><p id="p-0127" num="0143">For example, the subset of structural characteristics can be selected by processing the sets of structural characteristics of the training data <b>1820</b> and generating, for a structural-characteristic category (e.g., orientation, centroid location), a first distribution of structural-characteristic values corresponding a tumor region and a second distribution of structural-characteristic values corresponding to a non-tumor region. A statistical difference between the two distributions can be identified. For instance, the statistical difference corresponds to a difference of median or mean values between the first and second distributions. The statistical difference of two distributions can be analyzed to determine whether the structural-characteristic value of the corresponding category can be added as an element of the subset of structural characteristics. If so, the structural characteristic of the image object that is associated with the corresponding category can be added as an element of the subset of structural characteristics for the image object. In some instances, the subset of structural characteristics includes values corresponding to structural-characteristic categories such as volume, surface area, equivalent diameter, and voxel intensity of the image object.</p><p id="p-0128" num="0144">Additionally or alternatively, the subset of structural characteristics can be selected by processing the sets of structural characteristics of the training data <b>1820</b> to generate, for each structural-characteristic category, a correlation metric (e.g., Pearson correlation coefficient). The correlation metric can be compared to a predefined threshold (e.g., 0.7). In response to determining that the correlation metric exceeds the predefined threshold, the structural characteristic value of the image object that is associated with the corresponding structural-characteristic category can be added as an element of the subset. In some instances, the subset of structural characteristics can be selected by generating a SHapley Additive exPlanations plot that identifies contribution of each structural characteristic category for reaching the tumor classification.</p><p id="p-0129" num="0145">II.I.3 Support Vector Machine</p><p id="p-0130" num="0146">For each set of structural characteristics of the training data <b>1820</b>, the training subsystem <b>1805</b> may use values from the set of structural characteristics (e.g., shape, volume) to plot the set on a multi-dimensional graph that represents the support vector machine <b>1810</b>. As a result, the training image objects (with the associated labels) corresponding to the sets of structural characteristics can be identified in the multi-dimensional graph corresponding to the support vector machine <b>1810</b>. The training subsystem <b>1805</b> may then train the support vector machine <b>1810</b> by generating an optimal hyperplane that separates sets of structural characteristics assigned with tumor labels and sets of structural characteristics assigned with non-tumor labels. In some instances, a K-fold cross validation technique is used when optimizing one or more hyperparameters of the support vector machine. For example, a Bayesian hyperparameter optimization technique can be used when optimizing the one or more hyperparameters. Examples of hyperparameters can include the following:</p><p id="p-0131" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 2</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Examples of Hyperparameters</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="56pt" align="left"/><colspec colname="2" colwidth="28pt" align="left"/><colspec colname="3" colwidth="133pt" align="left"/><tbody valign="top"><row><entry/><entry>Optimal</entry><entry/></row><row><entry>Hyperparameter</entry><entry>Value</entry><entry>Description</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row><row><entry>PCA (%)</entry><entry>98</entry><entry>Reduces dimensionality of feature array from</entry></row><row><entry/><entry/><entry>16 to 12 (prevents overfitting)</entry></row><row><entry>Kernel Function</entry><entry>gaussian</entry><entry>Specifies type of hyperplane that forms decision</entry></row><row><entry/><entry/><entry>boundaries</entry></row><row><entry>Polynomial Order</entry><entry>n/a</entry><entry>Higher order polynomial leads to overfitting</entry></row><row><entry>Kernel Scale</entry><entry>15</entry><entry/></row><row><entry>Box Constraint</entry><entry>500</entry><entry>Regularization parameter (loss function penalty</entry></row><row><entry/><entry/><entry>term; prevents overfitting)</entry></row><row><entry>Standardize Data</entry><entry>1</entry><entry>Data normalization; improves optimization</entry></row><row><entry/><entry/><entry>efficiency</entry></row><row><entry>Multiclass Method</entry><entry>onevsall</entry><entry>Primarily impacts computation time</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0132" num="0147">The trained support vector machine with the optimal hyperplane can then be tested using the hold-out data set (e.g., 21 images from the 71 total scanned images). As a result of generating the optimal hyperplane, the training subsystem <b>1805</b> may determine that the support vector machine <b>1810</b> has been adequately trained for classification. The trained support vector machine <b>1815</b> with the optimal hyperplane can then be used by the classification subsystem <b>1830</b>.</p><p id="p-0133" num="0148">The classification subsystem <b>1830</b> may process information stored in a data structure <b>1835</b> by using the trained support vector machine <b>1815</b> and estimate whether an image object indicated in the data structure <b>1835</b> corresponds to a tumor. For the data structure <b>1835</b>, the classification subsystem <b>1830</b> may access a set of structural characteristics corresponding to the image object and plot the values corresponding to the structural characteristics in the multi-dimensional graph having the optimal hyperplane for the trained support vector machine <b>1815</b>. In some instances, a feature vector is generated to plot the corresponding set of structural characteristics in the multi-dimensional graph. Once the set of structural characteristics is plotted, the trained support vector machine <b>1815</b> may generate an estimation data indicating whether the image corresponds to a tumor based on a graph position of the data structure <b>1835</b> in relation to the optimal hyperplane of the trained support vector machine <b>1815</b>. The classification subsystem <b>1830</b> may collect the estimation data corresponding to the image objects indicated in the data structure <b>1835</b> and generate an output <b>1840</b> that includes information indicating whether each of the image objects corresponds to a tumor, a lesion, or normal tissue. In some instances, the output <b>1840</b> identifies the image object as either a tumor or a non-tumor region. Additionally or alternatively, the output <b>1840</b> can identify the image object as a tumor, a vessel, or another object type.</p><p id="p-0134" num="0149">The classification system <b>1830</b> can generate another output (not shown) that identifies a level of tumor burden corresponding to the biological structure depicted in the input image. The level of tumor burden can be determined by aggregating the image objects classified as having a tumor and generating a structural-characteristic value that represents the aggregated image objects (e.g., tumor volume). The generated structural-characteristic value can then be compared to a reference structural-characteristic value that represents all image objects segmented and identified from the image (e.g., lung volume). Such comparison can identify a proportion and/or ratio of the aggregated image objects relative to all image objects of the image. In some instances, the level of tumor burden identifies a low tumor burden (e.g., &#x3c;20% of total lung volume), a medium tumor burden (e.g., 20-40% of total lung volume), or a high tumor burden (e.g., &#x3e;40% of total lung volume). Additionally or alternatively, the classification system <b>1830</b> can identify a numerical value that represents the level of tumor burden of the image, such as a percentage of lung volume. The other output can be used to diagnose of level of cancer of the patient, identify a type of treatment for the subject corresponding to the image, and/or determine diagnostic or prognostic factors for the subject.</p><p id="p-0135" num="0150">II.I.4 Example Results</p><p id="p-0136" num="0151"><figref idref="DRAWINGS">FIG. <b>19</b></figref> shows an example of a graph <b>1900</b> that identifies a relationship between detected lung volumes from images and total tumor volumes. As shown in the graph <b>1900</b>, each plot point represents, for a corresponding image, a measured amount of tumor volume (x-axis) and an amount of lung volume detected by the trained 3D U-Net (y-axis). The y-intercept is shown as 0.24 mL, the slope is 0.92, and the R2 is 0.79. Each image is classified as having no tumor burden, low tumor burden (&#x3c;20% of lung volume), medium tumor burden (20-40% of lung volume), or high tumor burden (&#x3e;40% of lung volume). The graph <b>1900</b> shows that the amount of lung volume detected by the trained 3D U-Net increases approximately proportionally to the increase of tumor burden. Such increase can indicate that lungs significantly expand in volume to accommodate tumor growth. The graph <b>1900</b> also demonstrates that the trained machine-learning model can be used to consistently capture changes in total lung volume under different physiological/disease conditions. The consistent and accurate performance of the machine-learning model can thus be reliably used for tracking cancer and other non-oncology disease progression (for example).</p><p id="p-0137" num="0152">II.J. Example Schematic Diagram of a User Interface for Labeling Training Data</p><p id="p-0138" num="0153"><figref idref="DRAWINGS">FIGS. <b>20</b>A-C</figref> show example screenshots of a user interface <b>2000</b> for labeling training image objects corresponding to three-dimensional image data according to some embodiments. As described herein, a training subsystem (e.g., the training subsystem <b>115</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) may identify boundaries associated with each region represented in a training image of training data. The region may correspond to a three-dimensional image object of the training image. Each three-dimensional image object of the set can be labeled through the user interface <b>2000</b> and used as the training data for training a machine-learning model.</p><p id="p-0139" num="0154">In the user interface <b>2000</b>, each object of the set of three-dimensional image objects corresponding to the three-dimensional training image can be represented by a distinct color. A two-dimensional portion (e.g., the slice) rendered from the three-dimensional training image can be concurrently presented with the three-dimensional training image. Additionally or alternatively, the user interface <b>2000</b> can be used to scroll through an entire stack of a plurality of two-dimensional portions rendered from the three-dimensional training image. For example, the three-dimensional training image can be displayed on a first part of the user interface <b>2000</b>, and a two-dimensional portion can be concurrently displayed on a second part of the user interface <b>2000</b>. The two-dimensional portions may indicate regions that correspond to one or more image objects of the three-dimensional training image. For example a region corresponding to an image object can be specified (e.g., via a highlighting and/or border overlay) over a part of the two-dimensional portion. <figref idref="DRAWINGS">FIG. <b>20</b>A</figref> shows a first screenshot of the user interface <b>2000</b>, which depicts a three-dimensional training image corresponding to a subject with low tumor burden. <figref idref="DRAWINGS">FIG. <b>20</b>B</figref> shows a second screenshot of the user interface <b>2000</b>, which depicts a three-dimensional training image corresponding to a subject with medium to high tumor burden. <figref idref="DRAWINGS">FIG. <b>20</b>C</figref> shows a third screenshot of the user interface <b>2000</b>, which depicts a three-dimensional training image corresponding to a subject with a very high tumor burden.</p><p id="p-0140" num="0155">II.K. Example Processes for Automated Detection of Tumors Based on Image Processing</p><p id="p-0141" num="0156"><figref idref="DRAWINGS">FIG. <b>21</b></figref> illustrates a process <b>2100</b> for processing images to estimate whether at least part of a tumor is represented in the images according to some embodiments. Process <b>2100</b> begins at block <b>2105</b>, at which an image showing at least part of a biological structure is accessed. The biological structure may refer to one or more types tissues that are represented in the image. For example, the biological structure may include an individual organ such a lung, heart, or liver, various types of tissues (e.g., bone, blood vessels, tumors), and/or any structure that indicates changes to at least part of the biological structure (e.g., lesions).</p><p id="p-0142" num="0157">The accessed image may include or may have been derived from data collected using and received from an imaging system. The imaging system can include a CT system, which may include a tomographic imager and/or a micro-CT component (or micro-tomosynthesis component).</p><p id="p-0143" num="0158">At block <b>2110</b>, the image is processed using a segmentation algorithm to extract an image object from a set of image objects depicted in the image. Each of the set of image object may depict a particular type of the biological structure (e.g., tissue, tumor, blood vessels). The image may be pre-processed (e.g., negative distance transform operation) to identify boundaries within the image and use the boundaries to extract the set of image objects from the image. Various segmentation algorithms can be used to extract the image objects from the registered images, including watershed segmentation algorithms, graph partitioning algorithms, and model-based segmentation algorithms.</p><p id="p-0144" num="0159">At block <b>2115</b>, structural characteristics associated with each image object is determined. The structural characteristics may refer to morphological features associated with an image object that can be measured using any techniques known to one ordinarily skilled in the art. For example, the structural characteristics may include diameter, surface area, shape, centrally convex volume, equivalent diameter, orientation, solidity, and/or volume associated with the image object. The structural characteristics can be stored in a data structure, in which an identifier may be assigned to identify the image object.</p><p id="p-0145" num="0160">At block <b>2120</b>, the structural characteristics are processed using a trained machine-learning model to generate a classification metric corresponding to an estimation of whether the image object corresponds to a lesion or tumor. The trained machine-learning model may identify whether the structural characteristics of the image object correspond to pre-identified structural characteristics associated with a particular biological structure (e.g., a tumor, a lung, blood vessels). Based on the identification, the classification metric may be generated to estimate whether the particular image corresponds to a tumor or a lesion.</p><p id="p-0146" num="0161">At block <b>2125</b>, the classification metric is output. For example, the classification metric may be locally presented or transmitted to another device. The classification metric may be output along with an identifier of the image. The classification metric can be output with a probability metric corresponding to a confidence level associated with the classification metric.</p><p id="p-0147" num="0162"><figref idref="DRAWINGS">FIG. <b>22</b></figref> illustrates a process <b>2200</b> for processing images to extract a set of image objects according to some embodiments. Process <b>2200</b> begins at block <b>2205</b>, at which an image showing at least part of a biological structure is accessed. The biological structure may refer to one or more types of tissues that are represented in the image. For example, the biological structure may include an individual organ such a lung, heart, or liver, various types of tissues (e.g., bone, blood vessels, tumors), and/or any structure that indicates changes to at least part of the biological structure (e.g., lesions).</p><p id="p-0148" num="0163">The accessed image may include or may have been derived from data collected using and received from an imaging system. The imaging system can include a CT system, which may include a tomographic imager and/or a micro-CT component (or micro-tomosynthesis component).</p><p id="p-0149" num="0164">At block <b>2210</b>, filters are applied to the image to isolate a biological structure of interest from background of the image. For example, a first filter may be a lung mask that can be applied to isolate the biological structure (e.g., a lung) from the image. Background can include regions depicted in the image that exclude the biological structure of interest (e.g., bones). To apply the filters, at least part of the background can be used as boundaries surrounding the biological structure, at which the boundaries can be used to adjust the size of a region corresponding to the biological structure of interest. In some instances, a trained image pre-processing machine-learning model is used to process the accessed image to identify or isolate the ROIs, in which the ROI may include organs, tissues, tumors, and blood vessels depicted in the accessed image.</p><p id="p-0150" num="0165">At block <b>2215</b>, the filtered image is transformed into a binary image. Each pixel of at least part of the filtered image can be converted into either 0 or 1 pixel value. For example, the filtered image can be a grayscale image, which can be converted into the binary image. Binary values corresponding to a subset of pixels (e.g., 0 pixel surrounded by 1 pixels) can be converted based on a fill operation. Erosion-dilation and/or hole-filling operations can be applied to further reduce image noise of the binary image.</p><p id="p-0151" num="0166">At block <b>2220</b>, a registration operation is performed to align the binary image to a reference image. The filtered image can be registered based on a reference filter corresponding to the reference image, such that a set of transformation parameters are generated. An image warping operation can then be performed by applying the set of transformation parameters to the binary image, such that its position and orientation are adjusted. A subsequent registration operation can be performed on the warped binary image, based on a different set of transformation parameters generated based on aligning the filtered image to the reference image.</p><p id="p-0152" num="0167">At block <b>2225</b>, a segmentation algorithm is used to extract a set of image objects from the registered image. To extract the set of image objects, the registered image can be eroded to identify a set of image objects, at which seed points can be created for each image object and conditionally dilated until the registered image can be filled.</p><p id="p-0153" num="0168">The transformed registered image can be processed by a segmentation algorithm to generate a segmented image that identifies a set of lines that can be used as boundaries for the set of image objects depicted in the registered image. Based on the boundaries, the set of image objects can be extracted from the segmented image.</p><p id="p-0154" num="0169">At block <b>2230</b>, the set of image objects is output. For example, the set of image objects may be locally presented or transmitted to another device. The set of image objects may be output along with an identifier of the image. The set of image objects may also be processed by a trained machine-learning model to generate a classification metric corresponding to an estimation of whether the image object corresponds to a lesion or tumor.</p><heading id="h-0009" level="2">III. Additional Considerations</heading><p id="p-0155" num="0170">Some embodiments of the present disclosure include a system including one or more data processors. In some embodiments, the system includes a non-transitory computer readable storage medium containing instructions which, when executed on the one or more data processors, cause the one or more data processors to perform part or all of one or more methods and/or part or all of one or more processes disclosed herein. Some embodiments of the present disclosure include a computer-program product tangibly embodied in a non-transitory machine-readable storage medium, including instructions configured to cause one or more data processors to perform part or all of one or more methods and/or part or all of one or more processes disclosed herein.</p><p id="p-0156" num="0171">The terms and expressions which have been employed are used as terms of description and not of limitation, and there is no intention in the use of such terms and expressions of excluding any equivalents of the features shown and described or portions thereof, but it is recognized that various modifications are possible within the scope of the invention claimed. Thus, it should be understood that although the present invention as claimed has been specifically disclosed by embodiments and optional features, modification and variation of the concepts herein disclosed may be resorted to by those skilled in the art, and that such modifications and variations are considered to be within the scope of this invention as defined by the appended claims.</p><p id="p-0157" num="0172">The ensuing description provides preferred exemplary embodiments only, and is not intended to limit the scope, applicability or configuration of the disclosure. Rather, the ensuing description of the preferred exemplary embodiments will provide those skilled in the art with an enabling description for implementing various embodiments. It is understood that various changes may be made in the function and arrangement of elements without departing from the spirit and scope as set forth in the appended claims.</p><p id="p-0158" num="0173">Specific details are given in the following description to provide a thorough understanding of the embodiments. However, it will be understood that the embodiments may be practiced without these specific details. For example, circuits, systems, networks, processes, and other components may be shown as components in block diagram form in order not to obscure the embodiments in unnecessary detail. In other instances, well-known circuits, processes, algorithms, structures, and techniques may be shown without unnecessary detail in order to avoid obscuring the embodiments.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method comprising:<claim-text>accessing an image of at least part of a biological structure of a particular subject;</claim-text><claim-text>preprocessing the image using a trained image-preprocessing machine-learning model to generate a filter for identifying one or more image regions that correspond to the biological structure;</claim-text><claim-text>applying the filter to the image to isolate the one or more image regions;</claim-text><claim-text>processing the isolated image regions of the image using a segmentation algorithm to extract a plurality of image objects depicted in the image;</claim-text><claim-text>determining one or more structural characteristics associated with an image object of the plurality of image objects;</claim-text><claim-text>processing the one or more structural characteristics using a trained machine-learning model to generate estimation data corresponding to an estimation of whether the image object corresponds to a lesion or tumor associated with the biological structure, the trained machine-learning model trained with a three-dimensional model constructed from a set of training images; and</claim-text><claim-text>outputting the estimation data for the particular subject.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least part of the biological structure includes at least part of a lung.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the image depicts skeletal structures surrounding the at least part of the biological structure.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the image depicts a transverse plane of the at least part of the biological structure.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the image includes or has been derived from image data captured using a micro-computed tomography scanner.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more structural characteristics identify shape, location, surface area, and/or longest diameter of the image object.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein processing the image using the segmentation algorithm further comprises:<claim-text>determining one or more structural characteristics associated with another image object;</claim-text><claim-text>processing the one or more structural characteristics of the other image object using the trained machine-learning model to generate estimation data corresponding to an estimation of whether the other image object corresponds a type of the biological structure; and</claim-text><claim-text>outputting the estimation data associated with the other image object.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The computer-implemented method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the type of the biological structure includes blood vessel, lung, heart, and/or liver.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the segmentation algorithm is a watershed transformation algorithm, and the trained machine-learning model is a trained support vector machine (SVM).</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising processing the image using an image filter to generate a filtered image that excludes one or more skeletal structures surrounding the at least part of the biological structure, wherein the filtered image is used in lieu of the image to separate the image object from the plurality of image objects.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising processing the image using co-registration to align the image with a reference image, wherein the aligned image is used to in lieu of the image to extract the image object from the plurality of image objects.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>transforming the image into a binary image;</claim-text><claim-text>converting one or more pixels of the binary image by using a fill operation; and</claim-text><claim-text>performing an erosion and dilation operation to reduce image noise from the binary image.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein processing the image using a segmentation algorithm further comprises applying a negative distance transform function to the image to identify boundaries between two or more overlapping image objects of the plurality of image objects.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising performing the steps of determining, processing, and outputting for each of remaining image objects of the plurality of image objects, thereby generating estimation data for the plurality of image objects.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The computer-implemented method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising determining, based on the estimation data of the plurality of image objects, a level of tumor burden associated with the at least part of the biological structure.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computer-implemented method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the level of tumor burden corresponds to a ratio between a first structural-characteristic value derived from one or more image objects of the plurality of image objects that were classified as having the tumor and a second structural-characteristic value derived from all of the plurality of image objects.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The computer-implemented method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein:<claim-text>the one or more structural characteristics correspond to a set of structural characteristics of the image object; and</claim-text><claim-text>processing the one or more structural characteristics using the trained machine-learning model further comprises selecting a subset of structural characteristics from the set of structural characteristics, wherein the subset of structural characteristics is selected by:<claim-text>generating, for a structural-characteristic category associated with a particular structural characteristic of the set of structural characteristics, a first distribution of structural-characteristic values that correspond to a first set of image objects identified as having the tumor and a second distribution of structural-characteristic values that correspond to a second set of image objects identified as not having the tumor, wherein the plurality of image objects include the first and second sets of image objects;</claim-text><claim-text>identifying a statistical difference between the first distribution and the second distribution;</claim-text><claim-text>determining, based on the statistical difference, that the structural-characteristic category is to be added as a category of the subset of the structural characteristics; and</claim-text><claim-text>in response to determining that the structural-characteristic category is to be added as the category of the subset of the structural characteristics, adding the particular structural characteristic as an element of the subset of structural characteristics.</claim-text></claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computer-implemented method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the subset of structural characteristics identify volume, surface area, equivalent diameter, and voxel intensity of the image object.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A system comprising:<claim-text>one or more data processors; and</claim-text><claim-text>a non-transitory computer readable storage medium containing instructions which, when executed on the one or more data processors, cause the one or more data processors to perform operations comprising:<claim-text>accessing an image of at least part of a biological structure of a particular subject;</claim-text><claim-text>preprocessing the image using a trained image-preprocessing machine-learning model to generate a filter for identifying one or more image regions that correspond to the biological structure;</claim-text><claim-text>applying the filter to the image to isolate the one or more image regions;</claim-text><claim-text>processing the isolated image regions of the image using a segmentation algorithm to extract a plurality of image objects depicted in the image;</claim-text><claim-text>determining one or more structural characteristics associated with an image object of the plurality of image objects;</claim-text><claim-text>processing the one or more structural characteristics using a trained machine-learning model to generate estimation data corresponding to an estimation of whether the image object corresponds to a lesion or tumor associated with the biological structure, the trained machine-learning model trained with a three-dimensional model constructed from a set of training images; and</claim-text><claim-text>outputting the estimation data for the particular subject.</claim-text></claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein processing the image using the segmentation algorithm further comprises:<claim-text>determining one or more structural characteristics associated with another image object;</claim-text><claim-text>processing the one or more structural characteristics of the other image object using the trained machine-learning model to generate estimation data corresponding to an estimation of whether the other image object corresponds a type of the biological structure; and</claim-text><claim-text>outputting the estimation data associated with the other image object.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. A computer-program product tangibly embodied in a non-transitory machine-readable storage medium, including instructions configured to cause one or more data processors to perform operations comprising:<claim-text>accessing an image of at least part of a biological structure of a particular subject;</claim-text><claim-text>preprocessing the image using a trained image-preprocessing machine-learning model to generate a filter for identifying one or more image regions that correspond to the biological structure;</claim-text><claim-text>applying the filter to the image to isolate the one or more image regions;</claim-text><claim-text>processing the isolated image regions of the image using a segmentation algorithm to extract a plurality of image objects depicted in the image;</claim-text><claim-text>determining one or more structural characteristics associated with an image object of the plurality of image objects;</claim-text><claim-text>processing the one or more structural characteristics using a trained machine-learning model to generate estimation data corresponding to an estimation of whether the image object corresponds to a lesion or tumor associated with the biological structure, the trained machine-learning model trained with a three-dimensional model constructed from a set of training images; and</claim-text><claim-text>outputting the estimation data for the particular subject.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The computer-program product of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the instructions are further configured to cause one or more data processors to perform operations comprising:<claim-text>processing the image using an image filter to generate a filtered image that excludes one or more skeletal structures surrounding the at least part of the biological structure, wherein the filtered image is used in lieu of the image to separate the image object from the plurality of image objects.</claim-text></claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The computer-program product of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the instructions are further configured to cause one or more data processors to perform operations comprising:<claim-text>processing the image using co-registration to align the image with a reference image, wherein the aligned image is used to in lieu of the image to extract the image object from the plurality of image objects.</claim-text></claim-text></claim></claims></us-patent-application>