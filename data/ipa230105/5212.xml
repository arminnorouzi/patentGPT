<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005213A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005213</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17782851</doc-number><date>20210108</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2020-008936</doc-number><date>20200123</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>20</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>17</main-group><subgroup>05</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>205</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>17</main-group><subgroup>05</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2200</main-group><subgroup>24</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">IMAGING APPARATUS, IMAGING METHOD, AND PROGRAM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SONY GROUP CORPORATION</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>TANAKA</last-name><first-name>Hisao</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>ITAKURA</last-name><first-name>Eisaburo</first-name><address><city>Kanagawa</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>OKA</last-name><first-name>Shinichi</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>SEIMIYA</last-name><first-name>Hiroshi</first-name><address><city>Kanagawa</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>SONY GROUP CORPORATION</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2021/000445</doc-number><date>20210108</date></document-id><us-371c12-date><date>20220606</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">There is provided an imaging apparatus, an imaging method, and a program, capable of easily obtaining an image captured from a desired position. By using distance information from an imaging position to a subject and model information, a virtual image obtained by imaging the subject from a virtual imaging position different from the imaging position is generated from a captured image obtained by imaging the subject from the imaging position. The present technology can be applied to, for example, an imaging apparatus that images a subject.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="102.95mm" wi="158.75mm" file="US20230005213A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="236.81mm" wi="159.26mm" orientation="landscape" file="US20230005213A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="236.81mm" wi="159.26mm" orientation="landscape" file="US20230005213A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="238.34mm" wi="159.26mm" orientation="landscape" file="US20230005213A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="236.81mm" wi="159.26mm" orientation="landscape" file="US20230005213A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="99.99mm" wi="79.16mm" file="US20230005213A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="100.67mm" wi="114.89mm" orientation="landscape" file="US20230005213A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="107.61mm" wi="104.65mm" orientation="landscape" file="US20230005213A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="236.81mm" wi="159.26mm" orientation="landscape" file="US20230005213A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="110.91mm" wi="83.65mm" orientation="landscape" file="US20230005213A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="181.27mm" wi="110.32mm" file="US20230005213A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="227.33mm" wi="157.65mm" orientation="landscape" file="US20230005213A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="236.81mm" wi="159.26mm" orientation="landscape" file="US20230005213A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="150.45mm" wi="105.07mm" orientation="landscape" file="US20230005213A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="203.03mm" wi="150.11mm" file="US20230005213A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="236.81mm" wi="159.26mm" orientation="landscape" file="US20230005213A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="236.81mm" wi="160.02mm" orientation="landscape" file="US20230005213A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="171.28mm" wi="130.30mm" file="US20230005213A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="192.96mm" wi="146.30mm" orientation="landscape" file="US20230005213A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="154.52mm" wi="158.16mm" orientation="landscape" file="US20230005213A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="171.37mm" wi="146.98mm" file="US20230005213A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="230.21mm" wi="160.10mm" orientation="landscape" file="US20230005213A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="236.81mm" wi="159.26mm" orientation="landscape" file="US20230005213A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="238.34mm" wi="159.26mm" orientation="landscape" file="US20230005213A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="143.34mm" wi="100.75mm" orientation="landscape" file="US20230005213A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="234.44mm" wi="152.15mm" orientation="landscape" file="US20230005213A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="233.60mm" wi="141.39mm" orientation="landscape" file="US20230005213A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="188.72mm" wi="107.95mm" orientation="landscape" file="US20230005213A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present technology relates to an imaging apparatus, an imaging method, and a program, and particularly relates to, for example, an imaging apparatus, an imaging method, and a program capable of easily obtaining an image captured from a desired position.</p><p id="p-0003" num="0002">BACKGROUND ART</p><p id="p-0004" num="0003">For example, Patent Document 1 describes, as a technique for obtaining a virtual image captured from a virtual virtual imaging position different from an actual imaging position, a technique of imaging a subject from various imaging positions using a large number of imaging apparatuses and generating highly accurate three-dimensional data from the captured images obtained by imaging.</p><heading id="h-0002" level="1">CITATION LIST</heading><heading id="h-0003" level="1">Patent Document</heading><p id="p-0005" num="0004">Patent Document 1: Japanese Patent Application Laid-Open No. 2019-103126</p><heading id="h-0004" level="1">SUMMARX OF THE INVENTION</heading><heading id="h-0005" level="1">Problems to be Solved by the Invention</heading><p id="p-0006" num="0005">In the technique described in Patent Document 1, it is necessary to arrange a large number of imaging apparatuses at various positions. Therefore, there are many cases where the technique cannot be easily achieved due to the cost, the labor required for installation, and the like of the imaging apparatuses.</p><p id="p-0007" num="0006">Furthermore, in a case where a large number of imaging apparatuses are arranged, it is necessary to consider that a certain imaging apparatus is captured by another imaging apparatus or that the subject does not hit the imaging apparatus when the subject is a moving object, and the imaging apparatuses cannot be necessarily installed at any positions.</p><p id="p-0008" num="0007">The present technology has been made in view of such circumstances to easily obtain an image captured from a desired position.</p><heading id="h-0006" level="1">Solutions to Problems</heading><p id="p-0009" num="0008">An imaging apparatus or program of the present technology is an imaging apparatus including: a generation unit that uses distance information from an imaging position to a subject and model information to generate a virtual image obtained by imaging the subject from a virtual imaging position different from the imaging position from a captured image obtained by imaging the subject from the imaging position or a program for causing a computer to function as such an imaging apparatus.</p><p id="p-0010" num="0009">An imaging method of the present technology is an imaging method including: using distance information from an imaging position to a subject and model information to generate a virtual image obtained by imaging the subject from a virtual imaging position different from the imaging position from a captured image obtained by imaging the subject from the imaging position.</p><p id="p-0011" num="0010">In an imaging apparatus, imaging method, and program of the present technology, by using distance information from an imaging position to a subject and model information, a virtual image obtained by imaging the subject from a virtual imaging position different from the imaging position is generated from a captured image obtained by imaging the subject from the imaging position.</p><p id="p-0012" num="0011">Note that the imaging apparatus may be an independent apparatus, or may be an internal block constituting a single apparatus.</p><p id="p-0013" num="0012">Furthermore, the program can be provided by being transferred via a transfer medium or by being recorded on a recording medium</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0007" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating an example of an imaging situation.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating an example of an imaging situation and a captured image captured in the imaging situation.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating another example of an imaging situation and a captured mace captured in the imaging situation.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating another example of an imaging situation.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating a captured image obtained by imaging a person and a building from above in front of the person.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a top diagram illustrating an example of an imaging situation in which imaging from a position of a long distance from a person cannot be performed.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram describing perspective projection transformation when imaging is performed by an imaging apparatus.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram illustrating an example of an imaging situation of imaging a subject existing on a single object plane.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a top diagram illustrating a state of wide-angle imaging of imaging a subject from an imaging position close to the subject using a wide-angle lens.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a top diagram illustrating a state of telephoto imaging of imaging a subject from an imaging position far from the subject using a telephoto lens.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram describing an example of a process of obtaining a virtual image.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating an example of an imaging situation in a case where subjects exist on a plurality of object planes.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a top diagram illustrating a state of wide-angle imaging of imaging a subject from an imaging position close to the subject using a wide-angle lens.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a top diagram illustrating a state of telephoto imaging of imaging a subject from an imaging position far from the subject using a telephoto lens.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a diagram illustrating a state of short-distance imaging and a captured image obtained by the short-distance imaging.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a diagram illustrating a state of long-distance imaging and a captured image obtained by the long-distance imaging.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a top diagram illustrating a state of imaging.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram describing mapping of pixel values in a case of generating a virtual image obtained by long-distance imaging, which is virtual imaging, on the basis of a captured image obtained by short-distance imaging, which is actual imaging.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is another diagram describing mapping of pixel values in a case of generating a virtual image obtained by long-distance imaging, which is virtual imaging, on the basis of a captured image obtained by short-distance imaging, which is actual imaging.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a diagram describing an example of an occlusion portion complementing method that complements a pixel of an occlusion portion.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a diagram describing another example of a process of obtaining a virtual image obtained by virtual imaging on the basis of information obtained by actual imaging.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a plan diagram illustrating an example of a captured image and a virtual image.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a diagram describing a method of expressing a virtual imaging position in a case of performing virtual imaging.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a plan diagram illustrating an example of a UI operated in a case where a user designates a virtual imaging position.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a block diagram illustrating a configuration example of an embodiment of an imaging apparatus to which the present technology has been applied.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>26</b></figref> a flowchart describing an example of processing of a generation unit.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a block diagram illustrating a configuration example of an embodiment of a computer to which the present technology has been applied.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0008" level="1">MODE FOR CARRYING OUT THE INVENTION</heading><p id="p-0041" num="0040">&#x3c;Relationship Between Imaging Distance and Captured Image&#x3e;</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating an example of an imaging situation by an imaging apparatus.</p><p id="p-0043" num="0042">In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the imaging situation is illustrated by third angle projection.</p><p id="p-0044" num="0043">In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a person stands in front of a building when viewed from the imaging apparatus side, and the person and the building are imaged by the imaging apparatus from the front side of the person.</p><p id="p-0045" num="0044">Hereinafter, an image (captured image) actually captured by the imagine apparatus in the imaging situation of <figref idref="DRAWINGS">FIG. <b>1</b></figref> will be described.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating an example of an imaging situation of the imaging apparatus and a captured image captured in the imaging situation.</p><p id="p-0047" num="0046">A of <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a cop diagram illustrating an imaging situation, and B of <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a captured image captured in the imaging situation of A of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0048" num="0047">In A of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the two-dot chain lines indicate an angle of view of the imaging apparatus, and a space within the angle of view is imaged by the imaging apparatus. In A of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the broken. Lines indicate an angle of view occupied by a person who is a main subject (main subject).</p><p id="p-0049" num="0048">In the imaging situation of A of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the distance between the person and the building is a relatively long distance with respect to the distance (imaging distance) between the imaging apparatus and the person when imaging is performed by the imaging apparatus. Therefore, actually, the width of the building existing behind the person is wider than the width of the person, but in the captured image, the width of the building appears narrower than the width of the person. This is because a distant object looks small by so-called perspective.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating another example of an imaging situation of the imaging apparatus and a captured image captured in the imaging situation.</p><p id="p-0051" num="0050">A of <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a top diagram illustrating an imaging situation, and B of <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a captured image captured in the imaging situation of A of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0052" num="0051">In A of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, similarly to A of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the two-dot chain lines indicate an angle of view of the imaging apparatus, and the broken lines indicate an angle of view occupied by the person.</p><p id="p-0053" num="0052">In A of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the person and the building, which are the same subjects as in A of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, are imaged from an imaging position farther from the subject using a telephoto lens with a narrow angle of view (or a zoom lens with a long focal distance) than in the case of A of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0054" num="0053">In the imaging situation of A of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, as illustrated in B of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a captured image in which the width of the building appears to be wider than the width of the person is captured, similarly to the actual situation. This is because, in the imaging situation of A of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the imaging distance between the imaging apparatus and the person is larger than that in the case of A of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the distance between the person and the building becomes relatively small, and the perspective is reduced.</p><p id="p-0055" num="0054">As described above, even in the imaging of the same subjects (person and building), the content (composition) of the obtained captured image differs depending on the imaging distance between the subject and the imaging apparatus.</p><p id="p-0056" num="0055">The fact that (the content of) the captured image differs depending on the imaging distance has an important meaning in video representation. In a simple example, in a case where it is desired to obtain an image with a landscape such as vast mountains as a background, it is necessary to capture an image by approaching the subject using a wide-angle lens. On the other hand, in a case where it is desired to obtain an image in which a miscellaneous background is not captured as much as possible, it is necessary to capture an image away from the subject using a more telephoto lens.</p><p id="p-0057" num="0056">Note that, in principle, when imaging is performed from infinity, the ratio of the sizes of the person and the building appearing in the captured image is equal to the actual ratio. Therefore, in building applications, academic applications, and the like, in order to obtain a captured image correctly reflecting the ratio of the actual size, it is necessary to perform imaging from a farther distance.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating another example of an imaging situation by an imaging apparatus.</p><p id="p-0059" num="0058">In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, similarly to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the imaging situation is illustrated by third angle projection.</p><p id="p-0060" num="0059">In the imaging situation of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the optical axis direction of the imaging apparatus substantially coincides with the direction from the person to the building. In this case, as illustrated in B of <figref idref="DRAWINGS">FIG. <b>2</b></figref> and B of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the captured image captured by the imaging apparatus is an image in which the sense of distance between the person and the building is hardly expressed.</p><p id="p-0061" num="0060">In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the person and the building are imaged from above in front of the person with the optical axis of the imaging apparatus facing the person. In this case, the optical axis direction of the imaging apparatus is a direction different from the direction from the person to the building, and the captured image expressing the sense of distance between the person and the building can be obtained.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating a captured image obtained by imaging a person and a building from above in front of the person.</p><p id="p-0063" num="0062">By imaging the person and the building from above in front of the person with the optical axis of the imaging apparatus facing the person, it is possible to obtain a bird's-eye view image expressing the sense of distance between the person and the building in which the person and the building are looked down from above as a captured image.</p><p id="p-0064" num="0063">In order to perform video representation according to a purpose, it is required to capture images of a subject from various positions.</p><p id="p-0065" num="0064">However, in reality, imaging is not necessarily performed from a free position. For example, even in a case where it is desired to capture an image from a position of a long distance from a person as in A of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in reality, there is a case where imaging from a position of a long distance from a person cannot be performed.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a top diagram illustrating an example of an imaging situation in which imaging from a position of a long distance from a person cannot be performed.</p><p id="p-0067" num="0066">In <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a wall exists on the front side of the person. Therefore, in a case where the person is imaged from the front, since the imaging apparatus cannot be physically moved to the back side of the wall surface on the front side of the person, the person cannot be imaged from a long distance position.</p><p id="p-0068" num="0067">Furthermore, as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, in a case where the person and the building are imaged from above in front of the person, imaging can be performed from above to some extent by using a tripod or a stepladder. However, in the case of using a tripod or a stepladder, imaging can be performed from only a few meters above at most. Moreover, mobility ate, an imaging site is reduced by using a tripod or a stepladder.</p><p id="p-0069" num="0068">In recent years, an image can be captured from almost immediately above a subject by using a drone, but a flight time of the drone and eventually an imaging time are limited according to the capacity of a battery mounted on the drone.</p><p id="p-0070" num="0069">Furthermore, the operation of the drone is not necessarily easy, and is affected by weather such as rain and wind outdoors. Moreover, the drone cannot be used in a place where the flight of the drone is restricted or a place where the flight of the drone is prohibited due to concentration of people.</p><p id="p-0071" num="0070">In the present technology, even in a case where a free imaging position cannot be taken, an image obtained by imaging a subject from a desired position can be easily obtained. In the present technology, for example, from the captured mage in B of <figref idref="DRAWINGS">FIG. <b>2</b></figref> captured in the imaging situation in A of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, it is possible to generate an image obtained by imaging the subject from the imaging position in the imaging situation in, for example, A of <figref idref="DRAWINGS">FIG. <b>3</b></figref> or <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0072" num="0071">Note that Patent Document 1 describes a technique of generating a virtual image obtained by (seemingly) imaging the subject from an arbitrary virtual virtual imaging position from three-dimensional data generated by imaging a subject from various imaging positions using a large number of imaging apparatuses and using the captured images obtained by the imaging.</p><p id="p-0073" num="0072">However, in the technology described in Patent Document 1, it is necessary to arrange a large number of imaging apparatuses at various positions, and it is often impossible to easily realize the imaging situation as described in Patent Document 1 due to the cost, the labor required for installation, and the like of the imaging apparatuses.</p><p id="p-0074" num="0073">Moreover, in a case where a large number of imaging apparatuses are arranged, it is necessary to prevent a certain imaging apparatus from being captured by another imaging apparatus or it is necessary to consider that the subject does not hit the imaging apparatus when the subject is a moving object. Therefore, it is not always possible to install the imaging apparatuses at an arbitrary position.</p><p id="p-0075" num="0074">In the present technology, from a captured image obtained by imaging a subject from an imaging position, a virtual image obtained by imaging the subject from a virtual imaging position different from the imaging position is generated by using distance information from the imaging position to the subject and coping model information. Therefore, in the present technology, it is possible to easily obtain a virtual image obtained by imaging a subject from a desired virtual imaging position without installing a large number of imaging apparatuses.</p><p id="p-0076" num="0075">Hereinafter, a method of generating a virtual image obtained by imaging a subject from a desired virtual imaging position from a captured image obtained by imaging the subject from a certain imaging position will be described. The method of generating a virtual image obtained by imaging a subject from a desired virtual imaging position from a captured image obtained by imaging the subject from a certain imaging position is, for example, a method of generating, as a virtual image, a captured image captured from an imaging position at a long distance from the subject using a telephoto lens as in the imaging situation of A of <figref idref="DRAWINGS">FIG. <b>3</b></figref> from a captured image captured from an imaging position at a short distance from the subject using a wide-angle lens (or a zoom lens with a short focal distance) as in the imaging situation of A of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0077" num="0076">&#x3c;Perspective Projection Transformation&#x3e;</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram describing perspective projection transformation when imaging is performed by an imaging apparatus.</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a relationship between an actual subject on an object plane where the subject exists and an image on an imaging plane of an image sensor that performs photoelectric conversion of the imaging apparatus.</p><p id="p-0080" num="0079">Note that <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a top diagram of a subject standing vertically on the ground surface as viewed from above, and the horizontal direction represents a position in a horizontal direction (lateral direction) horizontal to the ground surface. The following description is similarly applied to a perpendicular direction (vertical direction) perpendicular to the ground surface, which is represented by a side diagram of a subject standing vertically on the ground surface as viewed from the side.</p><p id="p-0081" num="0080">The distance from the object plane to the lens of the imaging apparatus (imaging distance between the subject on the object plane and the imaging apparatus) is referred to as an object distance, and is represented by L<sub>obj</sub>. The distance from the lens to the imaging plane is referred to as an image distance, and is represented by L<sub>img</sub>. The position on the object plane, that is, the distance from the optical axis of the imaging apparatus on the object plane is represented by X<sub>obj</sub>. The position on the imaging plane, that is, the distance from the optical axis of the imaging apparatus on the imaging plane is represented by X<sub>img</sub>.</p><p id="p-0082" num="0081">Formula (1) holds for the object distance L<sub>obj</sub>, the image distance L<sub>img</sub>, the distance (position) X<sub>obj</sub>, and the distance (position) X<sub>img</sub>.</p><p id="p-0083" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i><sub>img</sub><i>/X</i><sub>obj</sub><i>=L</i><sub>img</sub><i>/L</i><sub>obj </sub>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0084" num="0082">From Formula (1), the position X<sub>img </sub>on the imaging plane corresponding to the position X<sub>obj </sub>of the subject on the object plane can be represented by Formula (2).</p><p id="p-0085" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i><sub>img</sub><i>=L</i><sub>img</sub><i>/L</i><sub>obj</sub><i>&#xd7;X</i><sub>obj </sub>&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0086" num="0083">Formula (2) represents transformation called a perspective projection transformation.</p><p id="p-0087" num="0084">The perspective projection transformation of Formula (2) is performed so to speak physically (optically) at the time of actual imaging of the subject by the imaging apparatus.</p><p id="p-0088" num="0085">Furthermore, from Formula (1), the position X<sub>obj </sub>of the subject on the object plane corresponding to the position X<sub>img </sub>on the imaging plane can he represented by Formula (3).</p><p id="p-0089" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i><sub>obj</sub><i>=L</i><sub>obj</sub><i>/L</i><sub>img</sub><i>&#xd7;X</i><sub>img </sub>&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0090" num="0086">Formula (3) represents the inverse transformation of the perspective projection transformation (perspective projection inverse transformation).</p><p id="p-0091" num="0087">In order to perform the perspective projection inverse transformation of Formula (3), the object distance L<sub>obj</sub>, the image distance L<sub>img</sub>, and the position X<sub>img </sub>of the subject on the imaging plane are required.</p><p id="p-0092" num="0088">The imaging apparatus that images the subject can recognize (acquire) the image distance L<sub>img </sub>and the position X<sub>img </sub>of the subject on the imaging plane.</p><p id="p-0093" num="0089">Therefore, in order to perform the perspective projection inverse transformation of Formula (3), it is necessary to recognize the object distance L<sub>obj </sub>(distance information) in some way.</p><p id="p-0094" num="0090">In order to obtain the position X<sub>obj </sub>of the subject on the object plane with respect to each pixel of the imaging plane, the object distance L<sub>obj </sub>having a resolution in pixel units or close thereto is required.</p><p id="p-0095" num="0091">As a method of obtaining the object distance L<sub>obj</sub>, any method can be adopted. For example, a so-called stereo method of calculating a distance to a subject from parallax obtained by using a plurality of image sensors that performs photoelectric conversion can be adopted. Furthermore, for example, a method of irradiating the subject with a determined optical pattern and calculating the distance to the subject from the shape of the optical pattern projected on the subject can be adopted. Furthermore, a method called time of flight (ToF) for calculating the distance to the subject from the time from laser light irradiation to the return of reflected light from the subject can be adopted. Moreover, it is possible to adopt a method of calculating the distance to the subject using an image plane phase difference method, which is one of so-called autofocus methods. In addition, the distance to the subject can be calculated by combining a plurality of the above methods.</p><p id="p-0096" num="0092">Hereinafter, on the assumption that the object distance L<sub>obj </sub>can be recognized by some method, a method of generating a virtual image obtained (that would be imaged) by imaging the subject from a virtual imaging position separated from the subject by a distance different from the actual object distance L<sub>obj </sub>by perspective projection transformation and perspective projection inverse transformation will be described.</p><p id="p-0097" num="0093">&#x3c;Virtual Image Generation Method&#x3e;</p><p id="p-0098" num="0094"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram illustrating an example of an imaging situation of imaging a subject existing on a single object plane.</p><p id="p-0099" num="0095">In <figref idref="DRAWINGS">FIG. <b>8</b></figref>, similarly to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the imaging situation is illustrated by third angle projection.</p><p id="p-0100" num="0096">In the imaging situation of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the subject exists on a single object plane, and the object plane is parallel to the imaging plane of the imaging apparatus. Therefore, the object plane is orthogonal to the optical axis of the imaging apparatus.</p><p id="p-0101" num="0097"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a top diagram illustrating a state of wide-angle imaging of imaging a subject from an imaging position close to the subject using a wide-angle lens in the imaging situation of <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0102" num="0098">In the wide-angle imaging of <figref idref="DRAWINGS">FIG. <b>9</b></figref>, a subject whose position on the object plane is X<sub>obj </sub>is imaged from an imaging position separated by an object distance L<sub>obj_W</sub>. The image distance at the time of wide-angle imaging is L<sub>img_W</sub>, and the position of the subject on the imaging plane is X<sub>img_W</sub>.</p><p id="p-0103" num="0099"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a top diagram illustrating a state of telephoto imaging of imaging a subject from an imaging position far from the subject using a telephoto lens in the imaging situation of <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0104" num="0100">In the telephoto imaging of <figref idref="DRAWINGS">FIG. <b>10</b></figref>, a subject whose position on the object plane is X<sub>obj </sub>and that is the same as in the case of the wide-angle imaging of <figref idref="DRAWINGS">FIG. <b>9</b></figref> is imaged from an imaging position separated by an object distance L<sub>obj_T</sub>. The image distance at the time of telephoto imaging is L<sub>img_T</sub>, and the position of the subject on the imaging plane is X<sub>img_T</sub>.</p><p id="p-0105" num="0101">When Formula (3) of the perspective projection inverse transformation is applied to the wide-angle imaging of <figref idref="DRAWINGS">FIG. <b>9</b></figref>, Formula (4) of the perspective projection inverse transformation can be obtained.</p><p id="p-0106" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i><sub>obj</sub><i>=L</i><sub>obj_W</sub><i>/L</i><sub>img_W</sub><i>&#xd7;X</i><sub>img_W </sub>&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0107" num="0102">When Formula (2) of the perspective projection transformation is applied to the telephoto imaging of <figref idref="DRAWINGS">FIG. <b>10</b></figref>, Formula (5) of the perspective projection transformation can be obtained.</p><p id="p-0108" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i><sub>img_T</sub><i>=L</i><sub>img_T</sub><i>/L</i><sub>obj_T</sub><i>&#xd7;X</i><sub>obj </sub>&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0109" num="0103">Formula (6) can be obtained by substituting X<sub>obj </sub>on the left side of Formula (4) into X<sub>obj </sub>on the right side of Formula (5).</p><p id="p-0110" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i><sub>img_T</sub>=(<i>L</i><sub>img_T</sub><i>/L</i><sub>obj_T</sub>)&#xd7;(<i>L</i><sub>obj_W</sub><i>/L</i><sub>img_W</sub>)&#xd7;<i>X</i><sub>img_W </sub>&#x2003;&#x2003;(6)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0111" num="0104">Here, a coefficient k is defined by Formula (7).</p><p id="p-0112" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>k</i>=(<i>L</i><sub>img_T</sub><i>/L</i><sub>obj_T</sub>)&#xd7;(<i>L</i><sub>obj_W</sub><i>/L</i><sub>img_W</sub>) &#x2003;&#x2003;(7)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0113" num="0105">Using Formula (7), Formula (6) can be a simple proportional expression of Formula (8).</p><p id="p-0114" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i><sub>img_T</sub><i>=k&#xd7;X</i><sub>img_W </sub>&#x2003;&#x2003;(8)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0115" num="0106">By using Formula (8) (Formula (6)), it is possible to obtain the position X<sub>img_T </sub>on the imaging plane in telephoto imaging using a telephoto lens, here, long-distance imaging from a long distance from the position X<sub>img_W </sub>on the imaging plane in wide-angle imaging using a wide-angle lens, here, short-distance imaging from a short distance. In other words, in a case where it is assumed that imaging is performed in long-distance imaging on the basis of information such as a captured image obtained by actual imaging in short-distance imaging or the like, information of a virtual image that would be obtained by imaging in the long-distance imaging can be obtained.</p><p id="p-0116" num="0107">Although the short-distance imaging using the wide-angle lens and the long-distance imaging using the telephoto lens have been described above as examples of imaging from imaging positions at different distances from the subject, the above description can be applied to a case where imaging at an arbitrary distance from the subject is performed using a lens with an arbitrary focal distance.</p><p id="p-0117" num="0108">That is, according to Formula (8) (Formula (6)), on the basis of the information such as a captured image obtained by imaging from a certain imaging position or the like using a lens with a certain focal distance, information of a captured image (virtual image) obtained in a case where imaging from another imaging position (virtual imaging position) using a lens with another focal distance is performed can be obtained.</p><p id="p-0118" num="0109">Here, since imaging from a certain imaging position using a lens with a certain focal distance is imaging that is actually performed, it is also referred to as actual imaging. On the other hand, since imaging from another imaging position (virtual imaging position) using a lens with another focal distance is not imaging that is actually performed, it is also referred to as virtual imaging.</p><p id="p-0119" num="0110"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram describing an example of a process of obtaining a virtual image obtained by virtual imaging on the basis of information obtained by actual imaging using Formula (8).</p><p id="p-0120" num="0111">Here, a conceptual meaning of obtaining Formula (6) from Formulae (4) and (5) is as described below.</p><p id="p-0121" num="0112">The position X<sub>img_W </sub>of the subject on the imaging plane is a position of a point obtained by perspective projection of a point on the subject in a three-dimensional space on the imaging plane of the image sensor, which is a two-dimensional plane. The position X<sub>obj </sub>of the point on the subject in a three-dimensional space (object plane) can be obtained by performing the perspective projection inverse transformation of Formula (4) on the position X<sub>img_W </sub>of the subject on the imaging plane.</p><p id="p-0122" num="0113">By performing the perspective projection transformation of Formula (5) on the position X<sub>obj </sub>on the subject in a three-dimensional space obtained in this manner, it is possible to obtain information of a virtual image obtained in a case where imaging is performed from a virtual imaging position different from an imaging position separated from the subject by the object distance L<sub>obj_W</sub>, that is, a virtual imaging position separated from the subject by the object distance L<sub>obj_T</sub>.</p><p id="p-0123" num="0114">Formula (6) is transformation from the position X<sub>img_W </sub>on the imaging plane of the subject at the time of wide-angle imaging as a certain two-dimensional plane to the position X<sub>img_T </sub>on the imaging plane of the subject at the time of telephoto imaging as another two-dimensional plane while the (variable indicating) position X<sub>obj </sub>of the point on the subject in the three-dimensional space is apparently removed. However, in the process of deriving Formula (6) from Formulae (4) and (5), the position X<sub>obj </sub>on the subject in the three-dimensional space is once determined.</p><p id="p-0124" num="0115">As illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a process of obtaining a virtual image obtained by virtual imaging on the basis of information obtained by actual imagine includes actual imaging, generation of a virtual subject (model), and virtual imaging.</p><p id="p-0125" num="0116">In actual imaging, a subject in a physical space (three-dimensional space) is subjected to the perspective projection transformation on an image sensor by an optical system (physical lens optical system) such as a physical lens or the like in an imaging apparatus, and a captured image (actual captured image) that is a two-dimensional image is generated. The perspective projection transformation in the actual imaging is optically performed using a physical imaging position (physical imaging position) of the imaging apparatus as a parameter.</p><p id="p-0126" num="0117">In the generation of the virtual subject, the perspective projection inverse transformation of Formula (4) is performed by calculation on the captured image obtained by the actual imaging using distance information from the imaging position to the subject obtained separately by measurement or the like, and (the subject model of) the subject in the three-dimensional space is virtually reproduced (generated). The virtually reproduced subject is also referred to as a virtual subject (model).</p><p id="p-0127" num="0118">In the virtual imaging, the perspective projection transformation of Formula (5) is performed by calculation on the virtual subject, so that the virtual subject is (virtually) imaged and a virtual image (virtual captured image) is generated. In the virtual imaging, a virtual imaging position at the time of imaging the virtual subject is designated as a parameter, and the virtual subject is imaged from the virtual imaging position.</p><p id="p-0128" num="0119">&#x3c;Positions of Subjects on Imaging Plane in a Case Where Subjects Exist on a Plurality of Object Planes&#x3e;</p><p id="p-0129" num="0120"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating an example of an imaging situation in a case where subjects exist on a plurality of object planes.</p><p id="p-0130" num="0121">In <figref idref="DRAWINGS">FIG. <b>12</b></figref>, similarly to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the imaging situation is illustrated by third angle projection.</p><p id="p-0131" num="0122">In the imaging situation of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, it is assumed that the object plane of the subject is single, but in actual imaging, subjects often exist on a plurality of object planes. <figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates an imaging situation in a case where subjects exist on a plurality of object planes as described above.</p><p id="p-0132" num="0123">In the imaging situation of <figref idref="DRAWINGS">FIG. <b>12</b></figref>, when viewed from the imaging apparatus side, behind a first subject corresponding to the subject of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, a second subject, which is another subject, exists.</p><p id="p-0133" num="0124">For the first subject, for example, the position X<sub>img_W </sub>of the subject on the imaging plane in the short-distance imaging, which is actual imaging, can be transformed into, for example, the position X<sub>img_T </sub>of the subject on the imaging plane is the long-distance imaging, which is virtual imaging, by using Formula (6) (Formula (8)).</p><p id="p-0134" num="0125">Similar transformation can be performed for the second subject.</p><p id="p-0135" num="0126"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a top diagram illustrating a state of wide-angle imaging of imaging a subject from an imaging position close to the subject using a wide-angle lens in the imaging situation of <figref idref="DRAWINGS">FIG. <b>12</b></figref>.</p><p id="p-0136" num="0127"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a top diagram illustrating a state of telephoto imaging of imaging a subject from an imaging position far from the subject using a telephoto lens in the imaging situation of <figref idref="DRAWINGS">FIG. <b>12</b></figref>.</p><p id="p-0137" num="0128"><figref idref="DRAWINGS">FIGS. <b>13</b> and <b>14</b></figref> are diagrams in which an object plane and an imaging plane are added regarding the second subject as compared with <figref idref="DRAWINGS">FIGS. <b>9</b> and <b>10</b></figref>.</p><p id="p-0138" num="0129">In <figref idref="DRAWINGS">FIGS. <b>13</b> and <b>14</b></figref>, a first object plane is the object plane of the first subject, and a second object plane is the object plane of the second subject. Since the second subject is imaged simultaneously with the first subject as the background of the first subject, imaging planes of the first subject and the second subject are the same in each of <figref idref="DRAWINGS">FIGS. <b>13</b> and <b>14</b></figref>.</p><p id="p-0139" num="0130">In the wide-angle imaging of <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the second subject whose position on the second object plane is X<sub>obj2 </sub>is imaged from an imaging position separated by an object distance L<sub>obj_W2</sub>. The image distance at the time of wide-angle imaging is L<sub>img_W</sub>, and the position of the second subject on the imaging plane is X<sub>img_W2</sub>. Since the first subject and the second subject are simultaneously imaged, the image distance at the time of wide-angle imaging is L<sub>img_W</sub>, which is the same as in the case of <figref idref="DRAWINGS">FIG. <b>9</b></figref>. Note that when a distance between the first object plane and the second object plane is d, d is expressed by equation d=L<sub>obj_W2</sub>&#x2212;L<sub>obj_W</sub>.</p><p id="p-0140" num="0131">In the telephoto imaging of <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the second subject whose position on the second object plane is X<sub>obj2 </sub>is imaged from an imaging position separated by an object distance L<sub>obj_T2</sub>. The image distance at the time of telephoto imaging is L<sub>img_T</sub>, and the position of the second subject on the imaging plane is X<sub>img_T2</sub>. Since the first subject and the second subject are simultaneously imaged, the image distance at the time of telephoto imaging is L<sub>img_T</sub>, which is the same as in the case of <figref idref="DRAWINGS">FIG. <b>10</b></figref>. Note that when a distance between the first object plane and the second object plane is d, d is expressed by equation d=L<sub>obj_T2</sub>&#x2212;L<sub>obj_T</sub>.</p><p id="p-0141" num="0132">When Formula (3) of the perspective projection inverse transformation is applied to the wide-angle imaging of <figref idref="DRAWINGS">FIG. <b>13</b></figref>, Formula (9) of the perspective projection inverse transformation can be obtained.</p><p id="p-0142" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i><sub>obj2</sub><i>=L</i><sub>obj_W2</sub><i>/L</i><sub>img_W</sub><i>&#xd7;X</i><sub>img_W2 </sub>&#x2003;&#x2003;(9)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0143" num="0133">When Formula (2) of the perspective projection transformation is applied to the telephoto imaging of <figref idref="DRAWINGS">FIG. <b>14</b></figref>, Formula (10) of the perspective projection transformation can be obtained.</p><p id="p-0144" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i><sub>img_T2</sub><i>=L</i><sub>img_T</sub><i>/L</i><sub>obj_T2</sub><i>&#xd7;X</i><sub>obj2 </sub>&#x2003;&#x2003;(10)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0145" num="0134">Formula (11) can be obtained by substituting X<sub>obj2 </sub>on the left side of Formula (9) into X<sub>obj2 </sub>on the right side of Formula (10).</p><p id="p-0146" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>X<sub>img_T2</sub>=(<i>L</i><sub>img_T</sub><i>/L</i><sub>obj_T2</sub>)&#xd7;(<i>L</i><sub>obj_W2</sub><i>/L</i><sub>img_W</sub>)&#xd7;<i>X</i><sub>img_W2 </sub>&#x2003;&#x2003;(11)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0147" num="0135">Here, a coefficient k<sub>2 </sub>is defined by Formula (12).</p><p id="p-0148" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>k</i><sub>2</sub>=(L<sub>img_T</sub><i>/L</i><sub>obj_T2</sub>)&#xd7;(<i>L</i><sub>obj_W2</sub><i>/L</i><sub>img_W</sub>) &#x2003;&#x2003;(12)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0149" num="0136">Using Formula (12), Formula (11) can be a simple proportional expression of Formula (13).</p><p id="p-0150" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i><sub>img_T2</sub><i>=k</i><sub>2</sub><i>&#xd7;X</i><sub>img_W2 </sub>&#x2003;&#x2003;(13)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0151" num="0137">By using Formula (13) (Formula (11)), it is possible to obtain the position X<sub>img_T2 </sub>on the imaging plane in telephoto imaging using a telephoto lens, here, long-distance imaging from a long distance from the position X<sub>img_W2 </sub>on the imaging plane in wide-angle imaging using a wide-angle lens, here, short-distance imaging from a short distance.</p><p id="p-0152" num="0138">Therefore, by applying Formula (8) to a pixel in which the first subject on the first object plane appears and applying Formula (13) to a pixel in which the second subject on the second object plane appears among pixels of a captured image obtained by, for example, short-distance imaging, which is actual imaging, it is possible to map pixels of a captured image obtained by short-distance imaging to pixels of a virtual image obtained, for example, by long-distance imaging, which is virtual imaging.</p><p id="p-0153" num="0139">&#x3c;Occlusion&#x3e;</p><p id="p-0154" num="0140"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a diagram illustrating a state of short-distance imaging and a captured image obtained by the short-distance imaging in the imaging situation of <figref idref="DRAWINGS">FIG. <b>12</b></figref>.</p><p id="p-0155" num="0141">That is, A of <figref idref="DRAWINGS">FIG. <b>15</b></figref> is a top diagram illustrating a state of short-distance imaging of imaging a subject from an imaging position close to the subject using a wide-angle lens in the imaging situation of <figref idref="DRAWINGS">FIG. <b>12</b></figref>. B of <figref idref="DRAWINGS">FIG. <b>15</b></figref> is a plan diagram illustrating a captured image obtained by the short-distance imaging of A of <figref idref="DRAWINGS">FIG. <b>15</b></figref>, and is equivalent to a front diagram in which the imaging plane is viewed from the front.</p><p id="p-0156" num="0142">A of <figref idref="DRAWINGS">FIG. <b>15</b></figref> is a diagram in which the dotted lines as auxiliary lines passing the lens center from the end points of the first subject and the second subject are added to <figref idref="DRAWINGS">FIG. <b>13</b></figref>.</p><p id="p-0157" num="0143"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a diagram illustrating a state of long-distance imaging and a captured image or by the long-distance imaging in the imaging situation of <figref idref="DRAWINGS">FIG. <b>12</b></figref>.</p><p id="p-0158" num="0144">That is, A of <figref idref="DRAWINGS">FIG. <b>16</b></figref> is a top diagram illustrating a state of long-distance imaging of imaging a subject from an imaging position far from the subject using a telephoto lens in the imaging situation of <figref idref="DRAWINGS">FIG. <b>12</b></figref>. B of <figref idref="DRAWINGS">FIG. <b>16</b></figref> is a plan diagram illustrating a captured image obtained by the long-distance imaging of A of <figref idref="DRAWINGS">FIG. <b>16</b></figref>, and is equivalent to a front diagram in which the imaging plane is viewed from the front similarly to the case of <figref idref="DRAWINGS">FIG. <b>15</b></figref>.</p><p id="p-0159" num="0145">A of <figref idref="DRAWINGS">FIG. <b>16</b></figref> is a diagram in which the dotted lines as auxiliary lines passing the lens center from the end points of the first subject and the second subject are added to <figref idref="DRAWINGS">FIG. <b>14</b></figref>.</p><p id="p-0160" num="0146">Now, in order to simplify the description, it is assumed that the imaging is performed such that the sizes of the first subjects on the imaging plane (captured image) are the same in the short-distance imaging and the long-distance imaging.</p><p id="p-0161" num="0147">The size of the second subject on the imaging plane (captured image) is larger in the long-distance imaging of <figref idref="DRAWINGS">FIG. <b>16</b></figref> than in the short-distance imaging of <figref idref="DRAWINGS">FIG. <b>15</b></figref>. As described above, the phenomenon that the size of the second subject on the imaging plane becomes larger in the long-distance imaging than in the short-distance imaging is caused by perspective, similarly to the case described with reference to <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref>.</p><p id="p-0162" num="0148"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a top diagram illustrating a state of imaging in which the top diagram of A of <figref idref="DRAWINGS">FIG. <b>15</b></figref> and the top diagram of A of <figref idref="DRAWINGS">FIG. <b>16</b></figref> are superimposed with a part omitted.</p><p id="p-0163" num="0149">In <figref idref="DRAWINGS">FIG. <b>17</b></figref>, portions M of the second subject are imaged in the long-distance imaging, but are not imaged behind the first subject in the short-distance imaging.</p><p id="p-0164" num="0150">In a case where subjects exist on a plurality of object planes, occlusion, that is, a state in which a first subject, which is a subject on the front side, hides a second subject, which is a subject on the back side, and makes the second subject invisible may occur.</p><p id="p-0165" num="0151">The portions M of the second subject are visible in the long-distance imaging, but become occlusion hidden behind the first subject and invisible in the short-distance imaging. It is also referred to as an occlusion. portion (missing portion) of the portion M of the second subject that is occlusion as described above.</p><p id="p-0166" num="0152">In the short-distance imaging, which is actual imaging, the portions M of the second subject as the occlusion portions are riot imaged. Therefore, in a case where a virtual image obtained by long-distance imaging, which is virtual imaging, is generated using Formulae (8) and (13) on the basis of a captured image obtained by short-distance imaging, in the virtual image, a pixel value cannot be obtained for the portions M of the second subject as the occlusion portions, and thus is missing.</p><p id="p-0167" num="0153"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram describing mapping of pixel values in a case of generating a virtual image obtained by long-distance imaging, which is virtual imaging, on the basis of a captured image obtained by short-distance imaging, which is actual imaging.</p><p id="p-0168" num="0154">In a captured image (short-distance captured image) obtained by short-distance imaging, which is actual imaging, on the upper side of <figref idref="DRAWINGS">FIG. <b>18</b></figref>, the portions M of the second subject are behind the first subject and are occlusion portions.</p><p id="p-0169" num="0155">In a case where a virtual image obtained by long-distance imaging, which is virtual imaging, on the lower side of <figref idref="DRAWINGS">FIG. <b>18</b></figref> is generated on the basis of the captured image on the upper side of <figref idref="DRAWINGS">FIG. <b>18</b></figref>, pixel values of pixels at the position of the first subject and the position of the second subject in the captured image (short-distance captured image) are mapped as pixel values of pixels at the position X<sub>img_T </sub>of the first subject and the position X<sub>img_Tw </sub>of the second subject in the virtual image (long-distance captured image) using Formulae (8) and (13), respectively.</p><p id="p-0170" num="0156">In the virtual image on the lower side of <figref idref="DRAWINGS">FIG. <b>18</b></figref>, the pixel values of the pixels in which the portions M of the second subject appear should be mapped to the hatched portions. However, in the captured image on the upper side of <figref idref="DRAWINGS">FIG. <b>18</b></figref>, the portions M of the second subject are not imaged, and the pixel values of the portions M of the second subject cannot be obtained. Therefore, in the virtual image on the lower side of <figref idref="DRAWINGS">FIG. <b>18</b></figref>, the pixel values of the pixels of the portions M of the second subject cannot be mapped to the hatched portions, and the pixel values are missing.</p><p id="p-0171" num="0157">As described above, in a case where the subjects exist on a plurality of object planes, missing of pixel values occurs for an occlusion portion that is occlusion, such as the portions M of the second subject.</p><p id="p-0172" num="0158"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is another diagram describing mapping of pixel values in a case of generating a virtual image obtained by long-distance imaging, which is virtual imaging, on the basis of a captured image obtained by short-distance imaging, which is actual imaging.</p><p id="p-0173" num="0159">In <figref idref="DRAWINGS">FIG. <b>19</b></figref>, an image picW is a captured image obtained by short-distance imaging, which is actual imaging, and an image picT is a virtual image obtained by long-distance imaging, which is virtual imaging.</p><p id="p-0174" num="0160">Furthermore, in <figref idref="DRAWINGS">FIG. <b>19</b></figref>, the horizontal axis of two-dimensional coordinates represents the horizontal positions X<sub>img_W </sub>and X<sub>img_W2 </sub>of the captured image picW, and the vertical axis represents the horizontal positions X<sub>img_T </sub>and X<sub>img_W2 </sub>of the virtual image picT.</p><p id="p-0175" num="0161">Moreover, in <figref idref="DRAWINGS">FIG. <b>19</b></figref>, a straight line L<b>1</b> represents Formula (8), and a straight line <b>12</b> represents Formula (13).</p><p id="p-0176" num="0162">(The pixel value of) the pixel at the position X<sub>img_W </sub>of the first subject in the captured image picW is mapped to (the pixel value of) the pixel at the position X<sub>img_T </sub>of the first subject in the virtual image picT obtained by Formula (8) with the position X<sub>img_W </sub>as an input.</p><p id="p-0177" num="0163">The pixel at the position X<sub>img_W2 </sub>of the second subject in the captured image picW is mapped to the pixel at the position X<sub>img_T2 </sub>of the second subject in the virtual image picT obtained by Formula (13) with the position X<sub>img_W2 </sub>g as an input.</p><p id="p-0178" num="0164">In the virtual image picT, the hatched portions are occlusion portions in which the corresponding portions do not appear in the captured image picW, and pixels (pixel values) are missing.</p><p id="p-0179" num="0165">&#x3c;Complementation of Occlusion Portion&#x3e;</p><p id="p-0180" num="0166"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a diagram describing an example of an occlusion portion complementing method that complements a pixel of an occlusion portion.</p><p id="p-0181" num="0167">As a method of complementing the occlusion portion, various methods can be adopted.</p><p id="p-0182" num="0168">As a method of complementing the occlusion portion, for example, there is a method of interpolating (a pixel value of) a pixel of the occlusion portion using a pixel in the vicinity of the occlusion portion. As a method of interpolating a pixel, for example, any method such as a nearest neighbor method, a bilinear method, a bicubic method, or the like can be adopted.</p><p id="p-0183" num="0169">In the nearest neighbor method, a pixel value of a neighboring pixel is used as a pixel value of a pixel of an occlusion portion as it is. In the bilinear method, an average value of pixel values of peripheral pixels around a pixel of an occlusion portion is used as a pixel value of a pixel of the occlusion portion. In the bicubic method, an interpolation value obtained by performing three-dimensional interpolation using pixel values of peripheral pixels around a pixel of an occlusion portion is used as a pixel value of a pixel of the occlusion portion.</p><p id="p-0184" num="0170">For example, in a case where the occlusion portion is an image of a monotonous wall surface, by complementing the occlusion portion by interpolation using a pixel in the vicinity of the occlusion portion, it is possible to generate a virtual image (substantially) similar to an image obtained in a case where imaging is performed from a virtual imaging position where the virtual image is imaged. In a case where a virtual image similar to an image obtained in a case where imaging is performed from a virtual imaging position is generated, the virtual image is also referred to as a virtual image with high reproducibility.</p><p id="p-0185" num="0171">Note that, in addition, as a method of interpolating a pixel of the occlusion portion using a pixel in the vicinity of the occlusion portion, for example, in a case where the occlusion portion is an image having a texture such as a rough wall surface or the like, a method of interpolating the occlusion portion with a duplicate of a region having a certain area of the periphery of the occlusion portion can be adopted.</p><p id="p-0186" num="0172">The method of interpolating a pixel of an occlusion portion using a pixel in the vicinity of the occlusion portion is based on the premise that the estimation that the occlusion portion will be an image similar to the vicinity of the occlusion portion is correct.</p><p id="p-0187" num="0173">Therefore, in a case where the occlusion portion is not an image similar to the vicinity of the occlusion portion (in a case where the occlusion portion is singular as compared with the vicinity of the occlusion portion), there is a possibility that a virtual image with high reproducibility cannot be obtained by the method of interpolating a pixel of the occlusion portion using a pixel in the vicinity of the occlusion portion.</p><p id="p-0188" num="0174">For example, in a case where a graffiti portion of a wall partially having graffiti is an occlusion portion, by a method of interpolating a pixel of the occlusion portion using a pixel in the vicinity of the occlusion portion, the graffiti cannot be reproduced, and a virtual image with high reproducibility cannot be obtained.</p><p id="p-0189" num="0175">In a case where the occlusion portion is not an image similar to the vicinity of the occlusion portion, in order to obtain a virtual image with high reproducibility, in addition to main imaging (original imaging), as actual imaging, auxiliary imaging can be performed from an imaging position different from the imaging position of the main imaging such that the occlusion portion generated in the main imaging appears. Then, the occlusion portion generated in the main imaging can be complemented by using the captured image obtained by the auxiliary imaging.</p><p id="p-0190" num="0176"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a top diagram describing main imaging and auxiliary imaging performed as actual imaging of the first subject and the second subject.</p><p id="p-0191" num="0177">In <figref idref="DRAWINGS">FIG. <b>20</b></figref>, actual imaging with a position p<b>201</b> as an imaging position is performed as main imaging, and each actual imaging with positions p<b>202</b> and p<b>203</b> displaced to the right and left from the position p<b>201</b> as imaging positions is performed as auxiliary imaging.</p><p id="p-0192" num="0178">In this case, the main imaging from the imaging position p<b>201</b> cannot image the portions M of the second subject that are occlusion portions. However, in the auxiliary imaging from the imaging positions p<b>202</b> and p<b>203</b>, the portions M of the second subject that are occlusion portions in the main imaging can be imaged.</p><p id="p-0193" num="0179">Therefore, a virtual image obtained by virtual imaging is generated on the basis of the captured image obtained by main imaging from the imaging position p<b>201</b>, and, in the virtual image, (the pixel value of) the portions M of the second subject that are occlusion portions are complemented using the captured image obtained by auxiliary imaging from the imaging positions p<b>202</b> and p<b>203</b>, so that a virtual image with high reproducibility can be obtained.</p><p id="p-0194" num="0180">The main imaging and the auxiliary imaging can be performed simultaneously or at different timings using a plurality of imaging apparatuses.</p><p id="p-0195" num="0181">Furthermore, the main imaging and the auxiliary imaging can be performed using a single imaging apparatus such as a multi-camera having a plurality of imaging systems.</p><p id="p-0196" num="0182">Moreover, the main imaging and the auxiliary imaging can be performed at different timings using a single imaging apparatus having a single imaging system. For example, for a subject that does not move, the auxiliary imaging can be performed before or after the main imaging.</p><p id="p-0197" num="0183">Complementation of the occlusion portion can be performed using only a part of information such as color, texture, or the like of a captured image obtained by auxiliary imaging. Moreover, the occlusion portion can be complemented by being used in combination with another method.</p><p id="p-0198" num="0184">As described above, the occlusion portion can be complemented using a captured image obtained by auxiliary imaging, or using a captured image obtained by another main imaging, for example, a captured image obtained by main imaging performed in the past.</p><p id="p-0199" num="0185">For example, in a case where the second subject as the background of the first subject is famous architecture (construction) such as Tokyo Tower or the like, for such famous architecture, captured images captured from various imaging positions in the past may be accumulated in an image library such as a stock photo service or the like.</p><p id="p-0200" num="0186">In a case where famous (or well-known) architecture appears in a captured image by actual imaging (a captured image obtained by actual imaging), and a portion where the famous architecture appears is an occlusion portion, complementation of the occlusion portion can be performed using a captured image in which the same famous architecture appears, which has been captured in the past and accumulated in an image library. In addition, the occlusion portion can be complemented using an image published on a network such as the Internet or the like, for example, a photograph published on a website that provides a map search service.</p><p id="p-0201" num="0187">The complementation of the occlusion portion can be performed using an image or can be performed using data (information) other than the image.</p><p id="p-0202" num="0188">For example, in a case where the second subject serving as the background of the first subject is architecture, when information such as the shape of the architecture, a surface finishing method, a coating color, and the like is disclosed and available on a web server or the like as building data regarding the building of the architecture, the occlusion portion can be complemented by estimating a pixel value of the occlusion portion using such building data</p><p id="p-0203" num="0189">In a case where a portion where the architecture appears is an occlusion portion, when complementation of the occlusion portion is performed using a captured image captured in the past and accumulated in an image library or using building data, it is necessary to specify the architecture, that is, here, the second subject. The second subject can be specified by, for example, performing image recognition targeted at a captured image in which the second subject appears or specifying a position where actual imaging to capture the captured image has been performed. The position where the actual imaging has been performed can be specified by referring to metadata of the, captured image such as exchangeable image file format (EXIF) information or the like.</p><p id="p-0204" num="0190">Note that actual imaging is performed, for example, in a situation where the subject is illuminated by a light source such as sunlight.</p><p id="p-0205" num="0191">On the other hand, in a case where the occlusion portion is complemented using a past captured image (captured image captured in the past) or building data, (illumination by) a light source at the time of actual imaging is not reflected in the occlusion portion.</p><p id="p-0206" num="0192">Therefore, for example, in a case where complementation of the occlusion portion of the captured image by actual imaging performed under sunlight is performed using a past captured image or building data, the color of (the portion that was) the occlusion portion may become an unnatural color as compared with the color of another portion.</p><p id="p-0207" num="0193">Therefore, in a case where the occlusion portion of the captured image by actual imaging performed under sunlight is complemented using a past captured image or building data, when weather data regarding weather can be obtained, the occlusion portion can be complemented using the past captured image or building data, and then the color tone of the occlusion portion can be corrected using the weather data.</p><p id="p-0208" num="0194">In actual imaging performed under sunlight, the intensity and color temperature of light illuminating the subject are affected by the weather. In a case where weather data can be obtained, weather at the time of actual imaging can be specified from the weather data, and illumination light information such as intensity or color temperature of light illuminating the subject at the time of actual imaging performed under sunlight can be estimated from the weather.</p><p id="p-0209" num="0195">Then, the occlusion portion can be complemented. using a past captured image or building data, and the color tone of the occlusion portion can be corrected such that the color of the occlusion portion becomes the color when the subject is illuminated with the light indicated by the illumination light information.</p><p id="p-0210" num="0196">By correcting the color tone as described above, the color of the occlusion portion can be set to a natural color as compared with the color of another portion, and therefore, a virtual image with high reproducibility can be obtained.</p><p id="p-0211" num="0197">In addition, the complementation of the occlusion portion can be performed using, for example, a learning model subjected to machine learning.</p><p id="p-0212" num="0198">For example, in a case where both the short-distance imaging and the long-distance imaging can be actually performed, it is possible to perform learning of a learning model so as to output the image of the occlusion portion of the virtual image obtained by the long-distance imaging performed as the virtual imaging by using, as an input, for example, the captured image obtained by the short-distance imaging performed as actual imaging using the captured image obtained by actually performing the short-distance imaging and the long-distance imaging as learning data.</p><p id="p-0213" num="0199">In this case, by inputting a captured image obtained by short-distance imaging, which is actual imaging, to the learning model after learning, an image of an occlusion portion of virtual image obtained by the long-distance imaging, which is virtual imaging, is obtained, and the occlusion portion can be complemented by the image.</p><p id="p-0214" num="0200">A complementation method of complementing the occlusion portion is riot particularly limited. However, by adopting a complementation method that can be performed by a single imaging apparatus or even a plurality of, but a small number of, imaging apparatuses, it is possible to suppress a reduction in mobility at the imaging site and easily obtain an image (virtual image) captured from a desired position (virtual imaging position). In particular, by adopting a complementation method that can be performed by a single imaging apparatus, the mobility at the imaging site can be maximized.</p><p id="p-0215" num="0201"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a diagram describing another example of a process of obtaining a virtual image obtained by virtual imaging on the basis of information obtained by actual imaging.</p><p id="p-0216" num="0202">In <figref idref="DRAWINGS">FIG. <b>21</b></figref>, similarly to the case of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a process of obtaining a virtual image obtained by virtual imaging on the basis of information obtained by actual imaging includes actual imaging, generation of a virtual subject (model), and virtual imaging. However, in <figref idref="DRAWINGS">FIG. <b>21</b></figref>, complementation of the occlusion portion is added to the case of <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0217" num="0203">In the actual imaging, similarly to the case of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a captured image (actual captured image) that is a two-dimensional image is generated (captured).</p><p id="p-0218" num="0204">In the generation of the virtual subject, the virtual subject as a corrected model is reproduced (generated) from the captured image obtained by the actual imaging using the distance information from the imaging position of the actual imaging to the subject and the coping model information.</p><p id="p-0219" num="0205">The coping model information is knowledge information for coping with occlusion, and includes, for example, one or more of a captured image captured in the past (past captured image), a captured image obtained by auxiliary imaging (auxiliary captured image), building data, weather data, and the like.</p><p id="p-0220" num="0206">In the generation of the virtual subject, first, similarly to the case of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the perspective projection inverse transformation of the captured image obtained by the actual imaging is performed using the distance information so as to generate the virtual subject.</p><p id="p-0221" num="0207">Moreover, in the generation of the virtual subject, the virtual imaging position is given as a parameter, and an imaged portion of the virtual subject imaged from the virtual imaging position is specified in the virtual imaging to be performed later.</p><p id="p-0222" num="0208">Then, by complementing, using the coping model information, a missing part in which (a pixel value of) a pixel of the captured image is missing in the imaged portion of the virtual subject, in other words, an occlusion portion that is occlusion when the virtual subject is viewed from the virtual imaging position, the virtual subject after the complementation is generated as a corrected model obtained by correcting the virtual subject.</p><p id="p-0223" num="0209">In the virtual imaging, similarly to the case of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a virtual image is generated by perspective projection transformation.</p><p id="p-0224" num="0210">However, the virtual imaging of <figref idref="DRAWINGS">FIG. <b>21</b></figref> is different from the case of <figref idref="DRAWINGS">FIG. <b>11</b></figref> in that the target of the perspective projection transformation is not the virtual subject itself generated by performing the perspective projection inverse transformation of the captured image obtained by the actual imaging, but the virtual subject, which is the corrected model in which the occlusion portion of the virtual subject has been complemented.</p><p id="p-0225" num="0211">In the virtual imaging of <figref idref="DRAWINGS">FIG. <b>21</b></figref>, the corrected model is (virtually) imaged from the virtual imaging position by performing the perspective projection transformation on the corrected model, and a virtual image (virtual captured image) is generated.</p><p id="p-0226" num="0212">With respect to the complementation of the occlusion portion, the range of the complementation can be suppressed to a necessary minimum by performing the complementation only on the occlusion portion that is occlusion when the virtual subject is viewed from the virtual imaging position. Therefore, in a case where the auxiliary imaging is performed in addition to the main imaging, the auxiliary imaging can be performed in the minimum necessary range, and it is possible to suppress a reduction in mobility at the time of imaging.</p><p id="p-0227" num="0213">Note that, in the auxiliary imaging, by imaging a range slightly wider than the necessary minimum, the virtual imaging position can be finely corrected after imaging.</p><p id="p-0228" num="0214">&#x3c;Another Virtual Image Generation Method&#x3e;</p><p id="p-0229" num="0215"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a plan diagram illustrating an example of a captured image and a virtual image.</p><p id="p-0230" num="0216">In the above-described case, the generation method of generating a virtual image captured in a case where imaging is performed in one of the short-distance imaging and the long-distance imaging on the basis of a captured image actually captured in the other of the short-distance imaging and the long-distance imaging has been described as an example. That is, the generation method of generating a virtual image captured from a virtual imaging position on the basis of a captured image, the virtual imaging position being a position that is moved along the optical axis of an imaging apparatus at the time of imaging from an imaging position where the captured image is actually captured and different only in imaging distance has been described.</p><p id="p-0231" num="0217">The virtual image generation method described above can also be applied to a case where a position moved in a direction not along the optical axis of the imaging apparatus from the imaging position of the captured image is set as the virtual imaging position. That is, the above-described generation of the virtual image can be applied not only to the case of generating a virtual image obtained by imaging the subject from a position moved along the optical axis of the imaging apparatus from the imaging position of the captured image, but also to the case of generating a virtual image (another virtual image) obtained by imaging the subject from a position moved in a direction not along the optical axis of the imaging apparatus.</p><p id="p-0232" num="0218">In a case where the subject is imaged with the optical axis of the imaging apparatus facing the subject, when a position moved along the optical axis of the imaging apparatus at the time of actual imaging from the imaging position of the captured image is set as a virtual imaging position, the optical axis of the imaging apparatus coincides between the actual imaging and the virtual imaging.</p><p id="p-0233" num="0219">On the other hand, when a position moved in a direction not along the optical axis of the imaging apparatus at the time of actual imaging from the imaging position of the captured image is set as the virtual imaging position, the optical axis of the imaging apparatus is different between the actual imaging and the virtual imaging.</p><p id="p-0234" num="0220">The case where the position moved in the direction not along the optical axis of the imaging apparatus from the imaging position of the captured image is set as the virtual imaging position corresponds to, for example, a case where the actual imaging is performed in the imaging situation of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and the virtual imaging is performed in the imaging situation of <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0235" num="0221">A of <figref idref="DRAWINGS">FIG. <b>22</b></figref> illustrates a captured image obtained by the actual imaging in the imaging situation of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0236" num="0222">B of <figref idref="DRAWINGS">FIG. <b>22</b></figref> illustrates a bird's-eve view image as a captured image obtained by the actual imaging in the imaging situation of <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0237" num="0223">C of <figref idref="DRAWINGS">FIG. <b>22</b></figref> illustrates a virtual image obtained (generated) by performing the virtual imaging on the virtual subject generated using the distance information on the basis of the captured image of A of <figref idref="DRAWINGS">FIG. <b>22</b></figref> with the imaging position in the imaging situation of <figref idref="DRAWINGS">FIG. <b>4</b></figref> as the virtual imaging position.</p><p id="p-0238" num="0224">In a virtual image obtained by performing the virtual imaging on a virtual subject itself generated using the distance information on the basis of the captured image of A of <figref idref="DRAWINGS">FIG. <b>22</b></figref>, the upper portions of a person and a building that do not appear in the captured image in the imaging situation of <figref idref="DRAWINGS">FIG. <b>1</b></figref> become occlusion portions as illustrated by hatching in C of <figref idref="DRAWINGS">FIG. <b>22</b></figref>.</p><p id="p-0239" num="0225">By complementing the occlusion portions, a virtual image close to the captured image of B of <figref idref="DRAWINGS">FIG. <b>22</b></figref> can be obtained.</p><p id="p-0240" num="0226">That is, within the imaged portion of the virtual subject, an occlusion portion that is occlusion when the virtual subject is viewed from the virtual imaging position is complemented using the coping model information, and perspective projection transformation of the corrected model that is the virtual subject after the complementation is performed, so that a virtual image close to the captured image of B of <figref idref="DRAWINGS">FIG. <b>22</b></figref> can be obtained.</p><p id="p-0241" num="0227">As a method of complementing the occlusion portion, a method of interpolating the occlusion portion using a pixel in the vicinity of the occlusion portion, a method of using a captured image obtained by auxiliary imaging, a method of using a captured image captured in the past, a method of using a learning model learned by machine learning, a method using building data, and the like described above can be adopted.</p><p id="p-0242" num="0228">&#x3c;Virtual Imaging UT&#x3e;</p><p id="p-0243" num="0229"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a diagram describing a method of expressing a virtual imaging position in a case of performing virtual imaging.</p><p id="p-0244" num="0230">In <figref idref="DRAWINGS">FIG. <b>23</b></figref>, the imaging situation is illustrated by third angle projection.</p><p id="p-0245" num="0231">For the imaging apparatus, the imaging position of the actual imaging is determined by physically (actually) installing the imaging apparatus. In the present technology, in addition to the imaging position of the actual imaging, a virtual imaging position is required, and it is necessary to designate the virtual imaging position.</p><p id="p-0246" num="0232">As a designation method of designating the virtual imaging position, for example, a method of automatically designating a position moved by a predetermined distance in a predetermined direction with respect to the imaging position as the virtual imaging position can be adopted.</p><p id="p-0247" num="0233">Furthermore, in addition, as a designation method of designating the virtual imaging position, for example, a method of causing a user to perform designation can be adopted.</p><p id="p-0248" num="0234">Hereinafter, a UI in a case where the user designates the virtual imaging position will be described, but before that, a method of expressing the virtual imaging position will be described.</p><p id="p-0249" num="0235">In the present embodiment, as illustrated in <figref idref="DRAWINGS">FIG. <b>23</b></figref>, the virtual imaging position is expressed by (the coordinates of) a spherical coordinate system with the position of the subject (intersecting the optical axis of the imaging apparatus) as the center (origin).</p><p id="p-0250" num="0236">Here, the intersection between the optical axis of the imaging apparatus (physically existing physical imaging apparatus), that is, the optical axis of (the optical system of) the imaging apparatus and the subject is referred to as the center of the subject. The optical axis of the imaging apparatus passes through the center of the image sensor of the imaging apparatus and coincides with a straight line perpendicular to the image sensor.</p><p id="p-0251" num="0237">The optical axis connecting the center of the image sensor of the imaging apparatus and the center of the subject (optical axis of the imaging apparatus) is referred to as a physical optical axis, and the optical axis connecting the virtual imaging position and the center of the subject is referred to as a virtual optical axis.</p><p id="p-0252" num="0238">When the center of the subject is set as the center of the spherical coordinate system, in the spherical coordinate system, the virtual imaging position can be expressed by a rotation amount (azimuth angle) &#x3c6;<sub>v </sub>in the azimuth angle direction with respect to the physical optical axis of the virtual optical axis, a rotation amount (elevation angle) &#x3b8;<sub>v </sub>in the elevation angle direction, and a distance r<sub>v </sub>between the subject on file virtual optical axis and the virtual imaging position.</p><p id="p-0253" num="0239">Note that, in <figref idref="DRAWINGS">FIG. <b>23</b></figref>, a distance r<sub>r </sub>represents the distance between the subject on the physical optical axis and the imaging position.</p><p id="p-0254" num="0240">Furthermore, in <figref idref="DRAWINGS">FIG. <b>23</b></figref>, a distance r<sub>v </sub>illustrated in the top diagram represents the distance along the virtual optical axis, not a distance component on the plane.</p><p id="p-0255" num="0241"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a plan diagram illustrating an example of a UI operated in a case where a user designates a virtual imaging position.</p><p id="p-0256" num="0242">In <figref idref="DRAWINGS">FIG. <b>24</b></figref>, the UI includes operation buttons such as a C button, a TOP button, a BTM button, a LEFT button, a RIGHT button, a SHORT button, a LONG button, a TELE button, and a WIDE button.</p><p id="p-0257" num="0243">Note that the UI can be configured using an operation unit such as a rotary dial, a joystick, or a touch panel in addition to the operation buttons. Furthermore, in a case where the UI is configured using the operation buttons, the arrangement of the operation buttons is not limited to the arrangement of <figref idref="DRAWINGS">FIG. <b>24</b></figref>.</p><p id="p-0258" num="0244">The imaging apparatus to which the present technology is applied can generate in real time a virtual image similar to a captured image obtained by imaging a subject from a virtual imaging position and output in real time the virtual image to a display unit such as a viewfinder. In this case, the display, unit can display, the virtual image in real time as a so-called through image. By viewing the virtual image displayed as a through image on the display unit, the user of the imaging apparatus can enjoy feeling as if the user is imaging the subject from the virtual imaging position.</p><p id="p-0259" num="0245">In the spherical coordinate system, it is necessary to determine the center of the spherical coordinate system in order to express the virtual imaging position.</p><p id="p-0260" num="0246">In the imaging apparatus, for example, when the C button of the UI is operated, the position of the point at which the optical axis of the imaging apparatus and the subject intersect is determined as the center of the spherical coordinate system. Then, the virtual imaging position is set to the imaging position of the actual imaging, that is, the azimuth angle &#x3c6;<sub>v</sub>=0, the elevation angle &#x3b8;<sub>v</sub>=0, and the distance r<sub>v</sub>=r<sub>r</sub>.</p><p id="p-0261" num="0247">The azimuth angle &#x3c6;<sub>v </sub>can be designated by operating the LEFT button or the RIGHT button. In the imaging apparatus, when the LEFT button is pressed, the azimuth angle &#x3c6;<sub>v </sub>changes in the negative direction by a predetermined constant amount. Furthermore, when the RIGHT button is pressed, the azimuth angle &#x3c6;<sub>v </sub>changes in the positive direction by a predetermined constant amount.</p><p id="p-0262" num="0248">The elevation angle &#x3b8;<sub>v </sub>can be designated by operating the TOP button or the BTM button. When the TOP button is pressed, the elevation angle &#x3b8;<sub>v </sub>changes in the positive direction by a predetermined constant amount. Furthermore, when the BTM button is pressed, the elevation angle &#x3b8;<sub>v </sub>changes in the negative direction by a predetermined constant amount.</p><p id="p-0263" num="0249">The distance r<sub>v </sub>can be designated by operating the SHORT button or the LONG button. When the SHORT button is pressed, the distance r<sub>v </sub>changes in the negative direction by a predetermined constant amount or by a constant magnification. Furthermore, when the LONG button is pressed, the distance r<sub>v </sub>changes in the positive direction by a predetermined constant amount or by a constant magnification.</p><p id="p-0264" num="0250">In <figref idref="DRAWINGS">FIG. <b>24</b></figref>, as described above, in addition to the C button, the TOP button, the BTM button, the LEFT button, the RIGHT button, the SHORT button, and the LONG button related to the designation of the virtual imaging position, the UI is provided with the TELE button and the WIDE button for designating the focal distance (hereinafter, also referred to as a virtual focal distance) of the virtual imaging apparatus when the virtual imaging from the virtual imaging position is performed.</p><p id="p-0265" num="0251">When the TELE button is pressed, the virtual focal distance changes in a direction in which the virtual focal distance increases by a predetermined constant amount or by a constant magnification. Furthermore, when the WIDE button is pressed, the virtual focal distance changes in a direction in which the virtual focal distance decreases by a predetermined constant amount or by a constant magnification.</p><p id="p-0266" num="0252">For example, the image distances L<sub>img_W </sub>and L<sub>img_T </sub>in Formulae (4) and (5) are determined according to the virtual focal distance.</p><p id="p-0267" num="0253">Note that the manner of changing the azimuth angle &#x3c6;<sub>v </sub>or the like with respect to the operation of the operation buttons of the UI is not limited to those described above. For example, in a case where the operation button is pressed for a long time, convenience can be enhanced by continuously changing the virtual imaging position and the virtual focal distance such as the azimuth angle &#x3c6;<sub>v </sub>while the operation button is pressed for a long time, or changing the change amount of the virtual imaging position and the virtual focal distance such as the azimuth angle &#x3c6;<sub>v </sub>according to the time when the operation button is pressed for a long time.</p><p id="p-0268" num="0254">Furthermore, the method of designating the virtual imaging position is not limited to the method of operating the UI. For example, a method of detecting the line of sight of the user, detecting a gaze point at which the user is gazing from the result of detection of the line of sight, and designating a virtual imaging position, or the like can be adopted. In this case, the position of the gaze point is designated (set) as the virtual imaging position.</p><p id="p-0269" num="0255">Moreover, in a case where a virtual image obtained by virtual imaging from a virtual imaging position designated by the operation of the or the like is displayed in real time, the imaging apparatus can display the occlusion portion so that the user can recognize the occlusion portion.</p><p id="p-0270" num="0256">Here, in the virtual image generated by complementing the occlusion portion, there is a possibility that accuracy of information of the complemented portion where the occlusion portion is complemented is inferior to an image obtained by actual imaging from the virtual imaging position.</p><p id="p-0271" num="0257">Therefore, after the virtual imaging position is designated, the imaging apparatus can display the virtual image on the display unit so that the user can recognize the occlusion portion that is occlusion in the virtual image obtained by virtual imaging from the virtual imaging position.</p><p id="p-0272" num="0258">In this case, the user of the imaging apparatus can recognize which portion of the subject is an occlusion portion by viewing the virtual image displayed on the display unit. Then, by recognizing which portion of the subject is an occlusion portion, the user of the imaging apparatus can consider the imaging position of the actual imaging so that an important portion of the subject for the user does not become an occlusion portion. That is, the imaging position can be considered such that an important portion of the subject for the user appears in the captured image obtained by the actual imaging.</p><p id="p-0273" num="0259">Moreover, by recognizing which portion of the subject is an occlusion portion, the user of the imaging apparatus can perform the auxiliary imaging such that the portion of the subject that is an occlusion portion appears in a case where the main imaging and the auxiliary imaging described in <figref idref="DRAWINGS">FIG. <b>20</b></figref> are performed.</p><p id="p-0274" num="0260">As a display method of displaying the virtual image on the display unit so that the user can recognize the occlusion portion that is occlusion in the virtual image, for example, a method of displaying the occlusion portion in the virtual image in a specific color, a method of reversing the gradation of the occlusion portion at a predetermined cycle such as one second, or the like can be adopted.</p><p id="p-0275" num="0261">&#x3c;Embodiment of the Imaging Apparatus to Which the Present Technology has Been Applied&#x3e;</p><p id="p-0276" num="0262"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a block diagram illustrating a configuration example of an embodiment of an imaging apparatus such as a digital camera or the like to which the present technology has been applied.</p><p id="p-0277" num="0263">In <figref idref="DRAWINGS">FIG. <b>25</b></figref>, an imaging apparatus <b>100</b> includes an imaging optical system <b>2</b>, an image sensor <b>3</b>, a distance sensor <b>5</b>, an inverse transformation unit <b>7</b>, a correction unit <b>9</b>, a transformation unit <b>11</b>, a display unit <b>13</b>, a UI <b>15</b>, a storage unit <b>17</b>, recording units <b>21</b> to <b>23</b>, and an output unit <b>24</b>. The imaging apparatus <b>100</b> can be applied to imaging of both a moving image and a still image.</p><p id="p-0278" num="0264">The imaging optical system <b>2</b> condenses light from a subject on the image sensor <b>3</b> to form an image. Therefore, the subject in the three-dimensional space is subjected to the perspective projection transformation on the image sensor <b>3</b>.</p><p id="p-0279" num="0265">The image sensor <b>3</b> receives light from the imaging optical system <b>2</b> and performs photoelectric conversion to generate a captured image <b>4</b> that is a two-dimensional image having a pixel value corresponding to the amount of received light, and supplies the captured image <b>4</b> to the inverse transformation unit <b>7</b>.</p><p id="p-0280" num="0266">The distance sensor <b>5</b> measures distance information <b>6</b> to each point of the subject and outputs the distance information <b>6</b>. The distance information <b>6</b> output from the distance sensor <b>5</b> is supplied to the inverse transformation unit <b>7</b>.</p><p id="p-0281" num="0267">Note that the distance information <b>6</b> of the subject can be measured by an external apparatus and supplied to the inverse transformation unit <b>7</b>. In this case, the imaging apparatus <b>100</b> can be configured without providing the distance sensor <b>5</b>.</p><p id="p-0282" num="0268">The inverse transformation unit <b>7</b> performs the perspective projection inverse transformation of the captured image <b>4</b> from the image sensor <b>3</b> using the distance information <b>6</b> from the distance sensor <b>5</b>, and generates and outputs a virtual subject, which is three-dimensional data <b>8</b>.</p><p id="p-0283" num="0269">The correction unit <b>9</b> complements the occlusion portion of the virtual subject, which is the three-dimensional data <b>8</b>, output by the inverse transformation unit <b>7</b>, and outputs the complemented virtual subject as a corrected model <b>10</b>.</p><p id="p-0284" num="0270">The transformation unit <b>11</b> performs the perspective projection transformation of the corrected model <b>10</b> output by the correction unit <b>9</b>, and outputs a resultant virtual image <b>12</b>, which is a two-dimensional image.</p><p id="p-0285" num="0271">The display unit <b>13</b> displays the virtual image <b>12</b> output by the transformation snit <b>11</b>. In a case where the transformation unit <b>11</b> outputs the virtual image <b>12</b> in real time, the display unit <b>13</b> can display the virtual image <b>12</b> in real time.</p><p id="p-0286" num="0272">The UI <b>15</b> is configured as illustrated, for example, in <figref idref="DRAWINGS">FIG. <b>24</b></figref>, and is operated by, for example, a user who is an imaging person of the imaging apparatus <b>100</b>. The user can perform an operation on the UI <b>15</b> for designating a virtual imaging position <b>16</b> while viewing the virtual image displayed on the display unit <b>13</b>.</p><p id="p-0287" num="0273">The UI <b>15</b> sets and outputs the virtual imaging position <b>16</b> according to the operation of the user.</p><p id="p-0288" num="0274">The correction unit <b>9</b> complements an occlusion portion that becomes occlusion when the virtual subject is viewed from the virtual imaging position <b>16</b> output by the UI <b>15</b>.</p><p id="p-0289" num="0275">That is, the correction unit <b>9</b> specifies an occlusion portion that becomes occlusion when the virtual subject is viewed from the virtual imaging position <b>16</b>.</p><p id="p-0290" num="0276">Thereafter, in the, correction unit <b>9</b>, the occlusion portion is complemented, and the complemented virtual subject is output as the corrected model <b>10</b>.</p><p id="p-0291" num="0277">In the transformation unit <b>11</b>, the virtual image <b>12</b>, which is a two-dimensional image, obtained by imaging the corrected model <b>10</b> output by the correction unit <b>9</b> from the virtual imaging position <b>16</b> output by the UI <b>15</b>, is generated by the perspective projection transformation of the corrected model <b>10</b>.</p><p id="p-0292" num="0278">Therefore, on the display unit <b>13</b>, the virtual image <b>12</b> obtained by imaging the corrected model <b>10</b> from the virtual imaging position <b>16</b> set according to the operation of the UI <b>15</b> by the user is displayed in real time. Therefore, the user can designate the virtual imaging position <b>16</b> at which the desired virtual image <b>12</b> can be obtained by operating the UI <b>15</b> while viewing the virtual image <b>12</b> displayed on the display unit <b>13</b>.</p><p id="p-0293" num="0279">Note that, in the correction unit <b>9</b>, the complementation of the occlusion portion can be performed by interpolating the occlusion portion using a pixel in the vicinity of the occlusion portion. Furthermore, for example, a past captured image <b>18</b>, building data <b>19</b>, weather data <b>20</b>, and the like as the coping model information are obtained from the outside, and the complementation of the occlusion portion can be performed using the coping model information.</p><p id="p-0294" num="0280">Moreover, the occlusion portion can be complemented using a captured image obtained by auxiliary imaging, a machine-learned learning model, or the like as another coping model information.</p><p id="p-0295" num="0281">In a case where the auxiliary imaging is performed, when the auxiliary imaging is performed prior to the main imaging, a virtual subject, which is the three-dimensional data <b>8</b>, generated from the captured image <b>4</b> obtained by the auxiliary imaging in the inverse transformation unit <b>7</b> is stored in the storage unit <b>17</b>.</p><p id="p-0296" num="0282">That is, the storage unit <b>17</b> stores the virtual subject, which is the three-dimensional data <b>8</b>, generated from the captured image <b>4</b> obtained by the auxiliary imaging in the inverse transformation unit <b>7</b>.</p><p id="p-0297" num="0283">The virtual subject, which is the three-dimensional data <b>8</b>, generated from the captured image <b>4</b> obtained by the auxiliary imaging stored in the storage unit <b>17</b> can be used in the correction unit <b>9</b> to complement an occlusion portion that is occlusion when the virtual subject, which is the three-dimensional data <b>8</b>, generated from the captured image <b>4</b> obtained by the main imaging performed after the auxiliary imaging is viewed from the virtual imaging position <b>16</b>.</p><p id="p-0298" num="0284">In a case where the auxiliary imaging is performed, when the auxiliary imaging is performed after the main imaging, a virtual subject, which is the three-dimensional data <b>8</b>, generated from the captured image <b>4</b> obtained by the main imaging in the inverse transformation unit <b>7</b> is stored in the recording unit <b>23</b>.</p><p id="p-0299" num="0285">That is, the recording unit <b>23</b> stores the virtual subject, which is the three-dimensional data <b>8</b>, generated from the captured image <b>4</b> obtained by the main imaging in the inverse transformation unit <b>7</b>.</p><p id="p-0300" num="0286">The complementation of the occlusion portion for the virtual subject, which is the three-dimensional data <b>8</b>, that is generated from the captured image <b>4</b> obtained by the main imaging and recorded in the recording unit <b>23</b>, can be performed by the correction unit <b>9</b> using the virtual subject, which is the three-dimensional data <b>8</b> generated from the, captured image <b>4</b> obtained by the auxiliary imaging performed after the main imaging.</p><p id="p-0301" num="0287">Thus, in a case where the auxiliary imaging is performed, when the auxiliary imaging is performed after the main imaging, after waiting for the auxiliary imaging to be performed after the main imaging, the occlusion portion for the virtual subject, which is the three-dimensional data <b>8</b>, generated from the captured image obtained by the main imaging is complemented.</p><p id="p-0302" num="0288">In a case where such complementation of the occlusion portion is performed, it is difficult to generate the virtual image <b>12</b> from the captured image <b>4</b> obtained by the main imaging in real time. Therefore, in a case where generation of a virtual image in real time is required, the auxiliary imaging needs to be performed prior to the main imaging, not after the main imaging.</p><p id="p-0303" num="0289">In <figref idref="DRAWINGS">FIG. <b>25</b></figref>, the recording unit <b>21</b> records the virtual image <b>12</b> output by the transformation unit <b>11</b>. The virtual image <b>12</b> recorded in the recording unit <b>21</b> can be output to the display unit <b>13</b> and the output unit <b>24</b>.</p><p id="p-0304" num="0290">The recording unit <b>22</b> records the corrected model <b>10</b> output by the correction unit <b>9</b>.</p><p id="p-0305" num="0291">For example, the correction unit <b>9</b> can complement a wide range portion (a portion including a portion of a virtual subject that becomes a new occlusion portion when the virtual imaging position <b>16</b> is slightly changed and the virtual subject is viewed from the virtual imaging position <b>16</b> after the change) that is slightly wider than the occlusion portion including the occlusion portion that is occlusion when the virtual subject is viewed from the virtual imaging position <b>16</b> from the UI <b>15</b>. In the recording unit <b>22</b>, it is possible to record the corrected model <b>10</b> in which such a wide range portion is complemented.</p><p id="p-0306" num="0292">In this case, the virtual image <b>12</b> in which the virtual imaging position <b>16</b> is finely corrected (finely adjusted) can be generated using the corrected model <b>10</b>, which is recorded in the recording unit <b>22</b> and in which the wide range portion has been complemented, as a target of the perspective projection transformation of the transformation unit <b>11</b>. Therefore, it is possible to generate the virtual image <b>12</b> in which the virtual imaging position <b>16</b> is finely corrected by using the corrected model <b>10</b>, which is recorded in the recording unit <b>22</b> and in which the wide range portion has been complemented, after the captured image <b>4</b> that is the basis of the corrected model <b>10</b> in which the wide range portion has been complemented is captured.</p><p id="p-0307" num="0293">The recording unit <b>23</b> records the virtual subject, which is the three-dimensional data <b>8</b>, output by the inverse transformation unit <b>7</b>, that is, the virtual subject before the occlusion portion is complemented by the correction unit <b>9</b>. For example, in a case where the virtual image <b>12</b> is used for news or the like and the authenticity of a part of the virtual image <b>12</b> is questioned, the virtual subject recorded in the recording unit <b>23</b> can be referred to as unprocessed, that is, true data for confirming the authenticity.</p><p id="p-0308" num="0294">Note that the recording unit <b>23</b> can record the captured image <b>4</b> that is the basis of generation of the virtual subject, which is the three-dimensional data <b>8</b>, together with the virtual subject, which is the three-dimensional data <b>8</b>, or instead of the virtual subject, which is the three-dimensional data <b>8</b>.</p><p id="p-0309" num="0295">The output unit <b>24</b> is an interface (I/F) that outputs data to the outside of the imaging apparatus <b>100</b>, and outputs the virtual image <b>12</b> output by the transformation unit <b>11</b> to the outside in real time.</p><p id="p-0310" num="0296">In a case where an external apparatus, which is not illustrated, is connected to the output unit <b>24</b>, when the transformation unit <b>11</b> outputs the virtual image <b>12</b> real time, the virtual image <b>12</b> can be distributed in real time from the output unit <b>24</b> to the external apparatus.</p><p id="p-0311" num="0297">For example, in a case where an external display unit, which is not illustrated, is connected to the output unit <b>24</b>, when the transformation unit <b>11</b> outputs the virtual image <b>12</b> in real time, the virtual image <b>12</b> is output in real time from the output unit <b>24</b> to the external display unit, and the virtual image <b>12</b> is displayed in real time on the external display unit.</p><p id="p-0312" num="0298">In the imaging apparatus <b>100</b> configured as described above, the inverse transformation unit <b>7</b> performs the perspective projection inverse transformation of the captured image <b>4</b> from the image sensor <b>3</b> using the distance information <b>6</b> from the distance sensor <b>5</b> to generate the virtual subject, which is the three-dimensional data <b>8</b>.</p><p id="p-0313" num="0299">The correction unit <b>9</b> uses the coping model information such as the past captured image <b>18</b> or the like to complement an occlusion portion that is occlusion when the virtual subject, which is the three-dimensional data <b>8</b>, generated by the inverse transformation unit <b>7</b> is viewed from the virtual imaging position <b>16</b> from the UI <b>15</b>, and obtains a virtual subject after the complementation, which is the corrected model <b>10</b>, obtained by correcting the virtual subject.</p><p id="p-0314" num="0300">Using the corrected model <b>10</b> obtained by the correction unit <b>9</b>, the transformation unit <b>11</b> generates the virtual image <b>12</b> obtained by imaging the corrected model <b>10</b> from the virtual imaging position <b>16</b> by the perspective projection transformation.</p><p id="p-0315" num="0301">Thus, it can be said that the inverse transformation unit <b>7</b>, the correction unit <b>9</b>, and the transformation unit <b>11</b> configure the generation unit that generates the virtual image <b>12</b> obtained by imaging the subject from the virtual imaging position <b>16</b> different from the imaging position from the captured image <b>4</b> obtained by imaging the subject from the imaging position by using the distance information <b>6</b> from the imaging position to the subject and the coping model information.</p><p id="p-0316" num="0302"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a flowchart describing an example of processing of a generation unit.</p><p id="p-0317" num="0303">In step S<b>1</b>, the generation unit generates the virtual image <b>12</b>, which is different from the captured image <b>4</b> and captured from the virtual imaging position <b>16</b>, from the captured image <b>4</b> by using the distance information <b>6</b> and the coping model information (knowledge information) for coping with occlusion such as the past captured image <b>18</b> or the like.</p><p id="p-0318" num="0304">Specifically, in step S<b>11</b>, the inverse transformation unit of the generation unit generates the virtual subject, which is the three-dimensional data <b>8</b>, by performing the perspective projection inverse transformation of the captured image <b>4</b> using the distance information <b>6</b>, and the processing proceeds to step S<b>12</b>.</p><p id="p-0319" num="0305">In step S<b>12</b>, the correction unit <b>9</b> uses the coping model information such as the past captured image <b>18</b> or the like to complement an occlusion portion that is occlusion when the virtual subject, which is the three-dimensional data <b>8</b>, generated by the inverse transformation unit <b>7</b> is viewed from the virtual imaging position <b>16</b> to generate the corrected model <b>10</b> (three-dimensional data <b>8</b> in which the occlusion portion is complemented) obtained by correcting the virtual subject, which is the three-dimensional data <b>8</b>, and the processing proceeds to step S<b>13</b>.</p><p id="p-0320" num="0306">In step S<b>13</b>, using the corrected model <b>10</b> generated by the correction unit <b>9</b>, the transformation unit <b>11</b> generates the virtual image obtained by imaging the corrected model <b>10</b> from the virtual imaging position <b>16</b> by the perspective projection transformation.</p><p id="p-0321" num="0307">With the imaging apparatus <b>100</b>, for example, even in the case of a situation where it is difficult to image a subject from a desired imaging position (viewpoint), by using a captured image captured from a certain imaging position (viewpoint) at which imaging can be performed, distance information from the imaging position to the subject, and coping model information that is auxiliary information other than separately obtained distance information, it is possible to generate a virtual image captured in a pseudo manner from a virtual imaging position, which is a desired imaging position, different from an actual imaging position. Therefore, an image (virtual image) captured from a desired position can be easily obtained.</p><p id="p-0322" num="0308">With the imaging apparatus <b>100</b>, for example, as illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, in an imaging situation where a wall exists in front of a person, it is possible to generate a virtual image as if imaging is performed from a position behind the wall surface in front of the person.</p><p id="p-0323" num="0309">Furthermore, with the imaging apparatus <b>100</b>, for example, in an imaging situation in which the user of the imaging apparatus <b>100</b> cannot approach the subject, such as an imaging situation in which the user captures an image of the outside through a window in a room or on a vehicle, it is possible to generate a virtual image as if the user has approached the subject and captured the image.</p><p id="p-0324" num="0310">Moreover, with the imaging apparatus <b>100</b>, for example, a virtual image such as the bird's-eye view image illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref> can be generated without using a stepladder, a drone, or the like.</p><p id="p-0325" num="0311">Furthermore, with the imaging apparatus <b>100</b>, for example, in a case where the subject is a person and the eye line of the person is not directed to the imaging apparatus, it is possible to generate a so-called virtual image with the camera being looked at by setting the position ahead of the eye line as the virtual imagine position.</p><p id="p-0326" num="0312">Moreover, with the imaging apparatus <b>100</b>, by setting the virtual imaging position to the position of the eyeball of the head of the user who is an imaging person, it is possible to generate a virtual image showing a state viewed from the viewpoint of the user. By displaying such a virtual image on a glasses-type display, it is possible to configure electronic glasses having no parallax.</p><p id="p-0327" num="0313">&#x3c;Description of a Computer to Which the Present Technology has Been Applied&#x3e;</p><p id="p-0328" num="0314">Next, a series of processing of the inverse transformation unit <b>7</b>, the correction unit <b>9</b>, and the transformation unit <b>11</b> constituting the generation unit described above can he performed by hardware or software. In a case where the series of processing is performed by software, a program constituting the software is installed is a general-purpose computer or the like.</p><p id="p-0329" num="0315"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a block diagram illustrating a configuration example of an embodiment of a computer in which a program that executes the series of processing described above is installed.</p><p id="p-0330" num="0316">The program may be preliminarily recorded on a hard disk <b>905</b> or ROM <b>903</b>, which is a recording medium incorporated in a computer.</p><p id="p-0331" num="0317">Alternatively, the program can be stored (recorded) in a removable recording medium <b>911</b> driven by a drive <b>909</b>. Such a removable recording medium <b>911</b> can be provided as so-called package software. Here, examples of the removable recording medium <b>911</b> include a flexible disc, a compact disc read only memory (CD-ROM), a magneto optical (MO) disc, a digital versatile disc (DVD), a magnetic disc, a semiconductor memory, or the like.</p><p id="p-0332" num="0318">Note that the program may not only be installed in a computer from the removable recording medium <b>911</b> described above, but also be downloaded into a computer and installed in the incorporated hard disk <b>905</b> via a communication network or a broadcast network. In other words, for example, the program can be wirelessly transferred to a computer from a download site via an artificial satellite for digital satellite broadcast, or can be transferred to a computer by wire via a network, e.g., a local area network (LAN) or the Internet.</p><p id="p-0333" num="0319">The computer incorporates a central processing unit (CPU) <b>902</b>. An input/output interface <b>910</b> is connected to the CPU <b>902</b> via a bus <b>901</b>.</p><p id="p-0334" num="0320">When a command is input by an operation or the like of an input unit <b>907</b> by the user via the input/output interface <b>910</b>, the CPU <b>902</b> executes the program stored in the read only memory (ROM) <b>903</b> accordingly. Alternatively, the CPU <b>902</b> loads the program stored in the hard disk <b>905</b> into random access memory (RAN) <b>904</b> and executes the program.</p><p id="p-0335" num="0321">Therefore, the CPU <b>902</b> performs the processing following the aforementioned flowchart or the processing performed by the configuration of the aforementioned block diagram. Then, the CPU <b>902</b> causes the processing result to be, output from an output unit <b>906</b>, transmitted from a communication unit <b>908</b>, recorded by the hard disk <b>905</b>, or the like, for example, via the input/output interface <b>910</b>, as needed.</p><p id="p-0336" num="0322">Note that the input unit <b>907</b> includes a keyboard, a mouse, a microphone, or the like. Furthermore, the output unit <b>906</b> includes a liquid crystal display (LCD), a speaker, or the like.</p><p id="p-0337" num="0323">Here, in the present specification, the processing performed by the computer according to the program is not necessarily needed to be performed in chronological order along the procedure described as the flowchart. In other words, the processing performed by the computer according to the program also includes processing that is executed in parallel or individually (e.g., parallel processing or processing by an object).</p><p id="p-0338" num="0324">Furthermore, the program may be processed by a single computer (processor) or may be processed in a distributed manner by a plurality of computers. Moreover, the program may be transferred to and executed by a remote computer.</p><p id="p-0339" num="0325">Moreover, in the present specification, a system means a cluster of a plurality of constituent elements (apparatuses, modules (parts), etc.) and it does not matter whether or not all the constituent elements are in the same casing. Therefore, a plurality of apparatuses that is housed in different enclosures and connected via a network, and a single apparatus in which a plurality of modules is housed in a single enclosure are both the system.</p><p id="p-0340" num="0326">Note that the embodiment of the present technology is not limited to the aforementioned embodiments, but various changes may be made within the scope not departing from the gist of the present technology.</p><p id="p-0341" num="0327">For example, the present technology can adopt a configuration of cloud computing in which one function is shared and jointly processed by a plurality of apparatuses via a network.</p><p id="p-0342" num="0328">Furthermore, each step described in the above-described flowcharts can be executed by a single apparatus or shared and executed by a plurality of apparatuses.</p><p id="p-0343" num="0329">Moreover, in a case where a single step includes a plurality of pieces of processing, the plurality of pieces of processing included in the single step can be executed by a single device or can be shared and executed by a plurality of devices.</p><p id="p-0344" num="0330">Furthermore, the effects described in the present specification are merely illustrative and are not limitative, and other effects may be provided.</p><p id="p-0345" num="0331">Note that the present technology may be configuration as below.</p><p id="p-0346" num="0332">&#x3c;1&#x3e;</p><p id="p-0347" num="0333">An imaging apparatus including:</p><p id="p-0348" num="0334">a generation unit that uses distance information from an imaging position to a subject and model information to generate a virtual image obtained by imaging the subject from a virtual imaging position different from the imaging position from a captured image obtained by imaging the subject from the imaging position.</p><p id="p-0349" num="0335">&#x3c;2&#x3e;</p><p id="p-0350" num="0336">The imaging apparatus according to &#x3c;1&#x3e;, in which</p><p id="p-0351" num="0337">the generation unit generates a corrected model from the captured image using the distance information and the model information, and generates the virtual image using the corrected model.</p><p id="p-0352" num="0338">&#x3c;3&#x3e;</p><p id="p-0353" num="0339">The imaging apparatus according to &#x3c;1&#x3e; or &#x3c;2&#x3e;, in which</p><p id="p-0354" num="0340">the model information includes knowledge information for coping with occlusion.</p><p id="p-0355" num="0341">&#x3c;4&#x3e;</p><p id="p-0356" num="0342">The imaging apparatus according to &#x3c;3&#x3e;, in which</p><p id="p-0357" num="0343">the generation unit:</p><p id="p-0358" num="0344">generates a virtual subject by performing perspective projection inverse transformation of the captured image using the distance information;</p><p id="p-0359" num="0345">generates a corrected model in which the virtual subject is corrected by complementing an occlusion portion that is occlusion when the virtual subject is viewed from the virtual imaging position using the model information; and</p><p id="p-0360" num="0346">generates the virtual image obtained by imaging the corrected model from the virtual imaging position by perspective projection transformation using the corrected model.</p><p id="p-0361" num="0347">&#x3c;5&#x3e;</p><p id="p-0362" num="0348">The imaging apparatus according to &#x3c;4&#x3e;, further including:</p><p id="p-0363" num="0349">a recording unit that records the virtual subject or the corrected model.</p><p id="p-0364" num="0350">&#x3c;6&#x3e;</p><p id="p-0365" num="0351">The imaging apparatus according to any of &#x3c;3&#x3e; to &#x3c;5&#x3e;, in which the model information includes one or more of the captured image captured in the past, building data related to building, and weather data related to weather.</p><p id="p-0366" num="0352">&#x3c;7&#x3e;</p><p id="p-0367" num="0353">The imaging apparatus according to &#x3c;1&#x3e;, further including:</p><p id="p-0368" num="0354">a user interface (UI) that designates the virtual imaging position.</p><p id="p-0369" num="0355">&#x3c;8&#x3e;</p><p id="p-0370" num="0356">The imaging apparatus according to any of &#x3c;1&#x3e; to &#x3c;7&#x3e;, in which</p><p id="p-0371" num="0357">the virtual image is output to a display unit in real time.</p><p id="p-0372" num="0358">&#x3c;9&#x3e;</p><p id="p-0373" num="0359">The imaging apparatus according to &#x3c;7&#x3e;, in which</p><p id="p-0374" num="0360">the UI includes:</p><p id="p-0375" num="0361">a first operation unit that is operated when a center of a spherical coordinate system expressing the virtual imaging position is determined;</p><p id="p-0376" num="0362">a second operation unit that is operated when an azimuth angle of the virtual imaging position in the spherical coordinate system is changed;</p><p id="p-0377" num="0363">a third operation unit that is operated when an elevation angle of the virtual imaging position in the spherical coordinate system is changed; and</p><p id="p-0378" num="0364">a fourth operation unit that is operated when a distance between the center of the spherical coordinate system and the virtual imaging position is changed.</p><p id="p-0379" num="0365">&#x3c;10&#x3e;</p><p id="p-0380" num="0366">The imaging apparatus according to &#x3c;9&#x3e;, in which</p><p id="p-0381" num="0367">the UI further includes a fifth operation unit that is operated when a focal distance of a virtual imaging apparatus when virtual imaging from the virtual imaging position is performed is changed.</p><p id="p-0382" num="0368">&#x3c;11&#x3e;</p><p id="p-0383" num="0369">The imaging apparatus according to &#x3c;10&#x3e;, in which</p><p id="p-0384" num="0370">The UI continuously changes the virtual imaging position or the focal distance while any one of the first to fifth operation units is being operated.</p><p id="p-0385" num="0371">&#x3c;12&#x3e;</p><p id="p-0386" num="0372">The imaging apparatus according to &#x3c;10&#x3e;, in which</p><p id="p-0387" num="0373">the UI changes a change amount of the virtual imaging position or the focal distance according to a time during which any one of the first to fifth operation units is being operated.</p><p id="p-0388" num="0374">&#x3c;13&#x3e;</p><p id="p-0389" num="0375">The imaging apparatus according to any of &#x3c;1&#x3e; to &#x3c;12&#x3e;, in which</p><p id="p-0390" num="0376">the UI designates a gaze point at which a user is gazing as the virtual imaging position.</p><p id="p-0391" num="0377">&#x3c;14&#x3e;</p><p id="p-0392" num="0378">An imaging method including:</p><p id="p-0393" num="0379">using distance information from an imaging position to a subject and model information to generate a virtual image obtained by imaging the subject from a virtual imaging position different from the imaging position from a captured image obtained by imaging the subject from the imaging position.</p><p id="p-0394" num="0380">&#x3c;15&#x3e;</p><p id="p-0395" num="0381">A program for causing a computer to function as:</p><p id="p-0396" num="0382">a generation unit that uses distance information from an imaging position to a subject and model information to generate a virtual image obtained by imaging the subject from a virtual imaging position different from the imaging position from a captured image obtained by imaging the subject from the imaging position.</p><heading id="h-0009" level="1">REFERENCE SIGNS LIST</heading><p id="p-0397" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0383"><b>2</b> Imaging optical system</li>    <li id="ul0001-0002" num="0384"><b>3</b> Image sensor</li>    <li id="ul0001-0003" num="0385"><b>5</b> Distance sensor</li>    <li id="ul0001-0004" num="0386"><b>7</b> Inverse transformation unit</li>    <li id="ul0001-0005" num="0387"><b>9</b> Correction unit</li>    <li id="ul0001-0006" num="0388"><b>11</b> Transformation unit</li>    <li id="ul0001-0007" num="0389"><b>13</b> Display unit</li>    <li id="ul0001-0008" num="0390"><b>15</b> UI</li>    <li id="ul0001-0009" num="0391"><b>17</b> Storage unit</li>    <li id="ul0001-0010" num="0392"><b>21</b> to <b>23</b> Recording unit</li>    <li id="ul0001-0011" num="0393"><b>24</b> Output unit</li>    <li id="ul0001-0012" num="0394"><b>901</b> Bus</li>    <li id="ul0001-0013" num="0395"><b>902</b> CPU</li>    <li id="ul0001-0014" num="0396"><b>903</b> ROM</li>    <li id="ul0001-0015" num="0397"><b>904</b> RAM</li>    <li id="ul0001-0016" num="0398"><b>905</b> Hard disk</li>    <li id="ul0001-0017" num="0399"><b>906</b> Output unit</li>    <li id="ul0001-0018" num="0400"><b>907</b> Input unit</li>    <li id="ul0001-0019" num="0401"><b>908</b> Communication unit</li>    <li id="ul0001-0020" num="0402"><b>909</b> Drive</li>    <li id="ul0001-0021" num="0403"><b>910</b> Input/output interface</li>    <li id="ul0001-0022" num="0404"><b>911</b> Removable recording medium</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An imaging apparatus comprising:<claim-text>a generation unit that uses distance information from an imaging position to a subject and model information to generate a virtual image obtained by imaging the subject from a virtual imaging position different from the imaging position from a captured image obtained by imaging the subject from the imaging position.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The imaging apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the generation unit generates a corrected model from the captured image using the distance information and the model information, and generates the virtual image using the corrected model.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The imaging apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the model information includes knowledge information for coping with occlusion.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The imaging apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the generation unit:</claim-text><claim-text>generates a virtual subject by performing perspective projection inverse transformation of the captured image using the distance information;</claim-text><claim-text>generates a corrected model in which the virtual subject is corrected by complementing an occlusion portion that is occlusion when the virtual subject is viewed from the virtual imaging position using the model information; and</claim-text><claim-text>generates the virtual image obtained by imaging the corrected model from the virtual imaging position by perspective projection transformation using the corrected model.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The imaging apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising:<claim-text>a recording unit than records the virtual subject or the corrected model.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The imaging apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the model information includes one or more of the captured image captured in the past, building data related to building, and weather data related to weather.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The imaging apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a user interface (UI) that designates the virtual imaging position.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The imaging apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the virtual image is output to a display unit in real time.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The imaging apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein<claim-text>the UI includes:</claim-text><claim-text>a first operation unit that is operated when a center of a spherical coordinate system expressing the virtual imaging position is determined;</claim-text><claim-text>a second operation unit that is operated when an azimuth angle of the virtual imaging position in the spherical coordinate system is changed;</claim-text><claim-text>a third operation unit that is operated when an elevation angle of the virtual imaging position in the spherical coordinate system is changed; and</claim-text><claim-text>a fourth operation unit that is operated when a distance between the center of the spherical coordinate system and the virtual imaging position is changed.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The imaging apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>the UI further includes a fifth operation unit that is operated when a focal distance of a virtual imaging apparatus when virtual imaging from the virtual imaging position is performed is changed.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The imaging apparatus according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein<claim-text>the UI continuously changes the virtual imaging position or the focal distance while any one of the first to fifth operation units is being operated.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The imaging apparatus according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein<claim-text>the UI changes a change amount of the virtual imaging position or the focal distance according to a time during which any one of the first to fifth operation units is being operated.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The imaging apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the UI designates a gaze point at which a user is gazing as the virtual imaging position.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. An imaging method comprising:<claim-text>using distance information from an imaging position to a subject and model information to generate a virtual image obtained by imaging the subject from a virtual imaging position different from the imaging position from a captured image obtained by imaging the subject from the imaging position.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A program for causing a computer to function as:<claim-text>a generation unit that uses distance information from an imaging position to a subject and model information to generate a virtual image obtained by imaging the subject from a virtual imaging position different from the imaging position from a captured image obtained by imaging the subject from the imaging position.</claim-text></claim-text></claim></claims></us-patent-application>