<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005255A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005255</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17939137</doc-number><date>20220907</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>98</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>75</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>98</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>084</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>751</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">ANALYSIS DEVICE AND COMPUTER-READABLE RECORDING MEDIUM STORING ANALYSIS PROGRAM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/JP2020/017823</doc-number><date>20200424</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17939137</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>FUJITSU LIMITED</orgname><address><city>Kawasaki-shi</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KUBOTA</last-name><first-name>Tomonori</first-name><address><city>Kawasaki</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Nakao</last-name><first-name>Takanori</first-name><address><city>Kawasaki</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Murata</last-name><first-name>Yasuyuki</first-name><address><city>Shizuoka</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>FUJITSU LIMITED</orgname><role>03</role><address><city>Kawasaki-shi</city><country>JP</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An analysis device includes a processor configured to: execute a first learning process on a generative model for images such that the images that bring a recognition result of an image recognition process into a preassigned state are generated; execute a second learning process on the generative model on which the first learning process has been executed, while gradually changing recognition accuracy of the images generated by the generative model on which the first learning process has been executed, to desired recognition accuracy; acquire each piece of information on back-error propagation calculated by executing the image recognition process, for the images with each level of the recognition accuracy generated through a course of the second learning process; and generate evaluation information indicating each of image parts that cause erroneous recognition at each level of the recognition accuracy, based on the acquired each piece of the information on the back-error propagation.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="187.54mm" wi="158.75mm" file="US20230005255A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="216.58mm" wi="162.31mm" file="US20230005255A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="116.67mm" wi="168.99mm" file="US20230005255A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="213.44mm" wi="156.63mm" orientation="landscape" file="US20230005255A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="231.73mm" wi="154.01mm" orientation="landscape" file="US20230005255A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="218.52mm" wi="152.99mm" orientation="landscape" file="US20230005255A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="206.67mm" wi="156.29mm" file="US20230005255A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="225.30mm" wi="151.55mm" orientation="landscape" file="US20230005255A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="254.17mm" wi="148.76mm" orientation="landscape" file="US20230005255A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="239.10mm" wi="156.29mm" file="US20230005255A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="233.60mm" wi="162.31mm" file="US20230005255A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="218.52mm" wi="154.35mm" orientation="landscape" file="US20230005255A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="212.77mm" wi="147.91mm" orientation="landscape" file="US20230005255A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="243.25mm" wi="142.24mm" orientation="landscape" file="US20230005255A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="235.88mm" wi="146.81mm" orientation="landscape" file="US20230005255A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="249.09mm" wi="156.63mm" file="US20230005255A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="251.63mm" wi="163.75mm" file="US20230005255A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="219.20mm" wi="155.02mm" orientation="landscape" file="US20230005255A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="238.68mm" wi="138.43mm" orientation="landscape" file="US20230005255A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="256.03mm" wi="144.36mm" orientation="landscape" file="US20230005255A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="199.05mm" wi="121.41mm" file="US20230005255A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="242.74mm" wi="149.86mm" orientation="landscape" file="US20230005255A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="256.29mm" wi="161.97mm" orientation="landscape" file="US20230005255A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="236.64mm" wi="121.92mm" file="US20230005255A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="223.01mm" wi="126.58mm" orientation="landscape" file="US20230005255A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="224.03mm" wi="139.36mm" orientation="landscape" file="US20230005255A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="139.19mm" wi="121.41mm" file="US20230005255A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is a continuation application of International Application PCT/JP2020/017823 filed on Apr. 24, 2020 and designated the U.S., the entire contents of which are incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">The embodiments discussed herein are related to an analysis device and an analysis program.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Traditionally, in image recognition processing using a convolutional neural network (CNN), an analysis technique for analyzing an image part that causes erroneous recognition when erroneous recognition has happened has been known. As an example, a score maximization method (activation maximization) and the like can be mentioned.</p><p id="p-0005" num="0004">Japanese Laid-open Patent Publication No. 2018-097807, Japanese Laid-open Patent Publication No. 2018-045350 and Ramprasaath R. Selvaraju, et al.: <i>Grad</i>-<i>cam: Visual explanations from deep networks via gradient</i>-<i>based localization. The IEEE International Conference on Computer Vision </i>(<i>ICCV</i>), pp. 618-626, 2017 are disclosed as related art.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">According to an aspect of the embodiments, an analysis device includes: a memory; and a processor coupled to the memory and configured to: execute a first learning process on a generative model for images such that the images that bring a recognition result of an image recognition process into a preassigned state are generated; execute a second learning process on the generative model on which the first learning process has been executed, while gradually changing recognition accuracy of the images generated by the generative model on which the first learning process has been executed, to desired recognition accuracy; acquire each piece of information on back-error propagation calculated by executing the image recognition process, for the images with each level of the recognition accuracy generated through a course of the second learning process; and generate evaluation information that indicates each of image parts that cause erroneous recognition at each level of the recognition accuracy, based on the acquired each piece of the information on the back-error propagation.</p><p id="p-0007" num="0006">The object and advantages of the invention will be realized and attained by means of the elements and combinations particularly pointed out in the claims.</p><p id="p-0008" num="0007">It is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory and are not restrictive of the invention.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating an example of the functional configuration of an analysis device;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating an example of the hardware configuration of the analysis device;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating an example of the functional configuration of an image refiner initialization unit;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a first diagram illustrating an example of the functional configuration of a refined image generation unit;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a first diagram illustrating an example of the functional configuration of a map generation unit;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a first flowchart illustrating a flow of an erroneous recognition cause extraction process;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a second diagram illustrating an example of the functional configuration of a refined image generation unit;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a second diagram illustrating an example of the functional configuration of a map generation unit;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a second flowchart illustrating a flow of an erroneous recognition cause extraction process;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a second diagram illustrating an example of the functional configuration of an analysis device;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a first diagram illustrating an example of the functional configuration of a specifying unit;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating a specific example of processing of a superpixel dividing unit;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram illustrating a specific example of processing of an important superpixel designation unit;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram illustrating a specific example of processing of an area extraction unit and a compositing unit;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a third flowchart illustrating a flow of an erroneous recognition cause extraction process;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a flowchart illustrating a flow of a changeable area specifying process;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a second diagram illustrating an example of the functional configuration of a specifying unit;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a first diagram illustrating an example of the functional configuration of a detailed cause analysis unit;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a first diagram illustrating a specific example of processing of the detailed cause analysis unit;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a first flowchart illustrating a flow of a detailed cause analysis process;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a second diagram illustrating an example of the functional configuration of a detailed cause analysis unit;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a second diagram illustrating a specific example of processing of the detailed cause analysis unit;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a second flowchart illustrating a flow of a detailed cause analysis process;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a third diagram illustrating an example of the functional configuration of a detailed cause analysis unit;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a third diagram illustrating a specific example of processing of the detailed cause analysis unit; and</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a third flowchart illustrating a flow of a detailed cause analysis process.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0035" num="0034">According to the score maximization method, by changing the input image such that the score is maximized and generating a refined image, the changed portion of the generated refined image from the input image can be visualized as an image part that causes the erroneous recognition.</p><p id="p-0036" num="0035">However, in the case of the score maximization method, the image part after the change is completed is clearly indicated, but the image parts in the middle of the course of changing are not clearly indicated. Therefore, a user can grasp the image part affecting the maximum score, but is not allowed to grasp which image part has influence at scores in the middle of the course (recognition accuracy in the middle of the course) (for example, the degree of influence of each image part in the middle of the course).</p><p id="p-0037" num="0036">One aspect aims to visualize the degree of influence of each image part that causes erroneous recognition.</p><p id="p-0038" num="0037">Hereinafter, each embodiment will be described with reference to the accompanying drawings. Note that, in the present specification and the drawings, constituent elements having substantially the same functional configuration are denoted by the same reference sign, and redundant description will be omitted.</p><heading id="h-0007" level="1">First Embodiment</heading><p id="p-0039" num="0038">&#x3c;Functional Configuration of Analysis Device&#x3e;</p><p id="p-0040" num="0039">First, a functional configuration of an analysis device according to a first embodiment will be described. <figref idref="DRAWINGS">FIG. <b>1</b></figref> is a first diagram illustrating an example of the functional configuration of the analysis device. An analysis program is installed in the analysis device <b>100</b>, and when the program is executed, the analysis device <b>100</b> functions as an image recognition unit <b>110</b>, an erroneous recognition image extraction unit <b>120</b>, and an erroneous recognition cause extraction unit <b>140</b>.</p><p id="p-0041" num="0040">The image recognition unit <b>110</b> performs an image recognition process using a trained CNN. For example, the image recognition unit <b>110</b> executes the image recognition process in response to the input of an input image <b>10</b> and outputs a recognition result (for example, a label) indicating the type of an object (in the present embodiment, the type of vehicle) included in the input image <b>10</b>.</p><p id="p-0042" num="0041">The erroneous recognition image extraction unit <b>120</b> determines whether or not the recognition result included in the input image <b>10</b> (for example, a label indicating the type of the object (known)) and the recognition result by the image recognition unit <b>110</b> (for example, a label) coincide with each other. In addition, the erroneous recognition image extraction unit <b>120</b> extracts the input image when it is determined that the recognition results do not coincide with each other (when the erroneous recognition result is output), as &#x201c;erroneous recognition image&#x201d;, and stores the extracted erroneous recognition image in an erroneous recognition image storage unit <b>130</b>.</p><p id="p-0043" num="0042">The erroneous recognition cause extraction unit <b>140</b> specifies each image part that causes erroneous recognition at each level of recognition accuracy for the erroneous recognition image and, by outputting erroneous recognition cause information (an example of evaluation information) indicating each specified image part at each level of recognition accuracy, visualizes the degree of influence of each image part.</p><p id="p-0044" num="0043">For example, the erroneous recognition cause extraction unit <b>140</b> includes an image refiner initialization unit <b>141</b>, a refined image generation unit <b>142</b>, and a map generation unit <b>143</b>.</p><p id="p-0045" num="0044">The image refiner initialization unit <b>141</b> is an example of a first learning unit. The image refiner initialization unit <b>141</b> reads the erroneous recognition image stored in the erroneous recognition image storage unit <b>130</b> and executes a first learning process for initializing an image refiner unit, by inputting the read erroneous recognition image.</p><p id="p-0046" num="0045">The image refiner unit is a generative model that uses a CNN to change the erroneous recognition image and generate a refined image with a predetermined level of recognition accuracy. The image refiner initialization unit <b>141</b> initializes the image refiner unit by executing the first learning process and updating model parameters of the generative model.</p><p id="p-0047" num="0046">The refined image generation unit <b>142</b> is an example of a second learning unit, and the image refiner unit initialized by the image refiner initialization unit <b>141</b> is applied. The refined image generation unit <b>142</b> reads the erroneous recognition image stored in the erroneous recognition image storage unit <b>130</b>, executes a second learning process on the image refiner unit such that the recognition results have each level of recognition accuracy, and generates refined images with each level of recognition accuracy. The refined image generation unit <b>142</b> generates the refined images with each level of recognition accuracy while gradually raising the recognition accuracy to the desired recognition accuracy. Note that, among the refined images with each level of recognition accuracy, the refined image with the maximized recognition accuracy (the refined image with the desired recognition accuracy) will be referred to as &#x201c;recognition accuracy-maximized refined image&#x201d;.</p><p id="p-0048" num="0047">The map generation unit <b>143</b> is an example of a generation unit. The map generation unit <b>143</b> uses a traditional analysis technique for analyzing the cause of erroneous recognition, and the like to separately generate maps indicating each image part that causes erroneous recognition at each level of recognition accuracy. The map generation unit <b>143</b> visualizes the degree of influence of each image part by outputting each generated map as the erroneous recognition cause information.</p><p id="p-0049" num="0048">In this manner, the analysis device <b>100</b> visualizes the degree of influence of each image part that causes erroneous recognition by separately generating and outputting maps indicating each image part that causes erroneous recognition at each level of recognition accuracy.</p><p id="p-0050" num="0049">&#x3c;Hardware Configuration of Analysis Device&#x3e;</p><p id="p-0051" num="0050">Next, a hardware configuration of the analysis device <b>100</b> will be described. <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating an example of the hardware configuration of the analysis device. As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the analysis device <b>100</b> includes a central processing unit (CPU) <b>201</b>, a read only memory (ROM) <b>202</b>, and a random access memory (RAM) <b>203</b>. The CPU <b>201</b>, the ROM <b>202</b>, and the RAM <b>203</b> form a so-called computer.</p><p id="p-0052" num="0051">In addition, the analysis device <b>100</b> includes an auxiliary storage device <b>204</b>, a display device <b>205</b>, an operation device <b>206</b>, an interface (I/F) device <b>207</b>, and a drive device <b>208</b>. Note that the respective pieces of hardware of the analysis device <b>100</b> are interconnected via a bus <b>209</b>.</p><p id="p-0053" num="0052">The CPU <b>201</b> is an arithmetic device that executes various programs (such as the analysis program as an example) installed in the auxiliary storage device <b>204</b>. Note that, although not illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, an accelerator (such as a graphics processing unit (GPU) as an example) may be combined as an arithmetic device.</p><p id="p-0054" num="0053">The ROM <b>202</b> is a nonvolatile memory. The ROM <b>202</b> functions as a main storage device that stores various programs, data, and the like demanded by the CPU <b>201</b> to execute various programs installed in the auxiliary storage device <b>204</b>. For example, the ROM <b>202</b> functions as a main storage device that stores, for example, a boot program such as the Basic Input/Output System (BIOS) or the Extensible Firmware Interface (EFI).</p><p id="p-0055" num="0054">The RAM <b>203</b> is a volatile memory such as a dynamic random access memory (DRAM) or a static random access memory (SRAM). The RAM <b>203</b> functions as a main storage device that provides a work area into which various programs installed in the auxiliary storage device <b>204</b> are loaded when executed by the CPU <b>201</b>.</p><p id="p-0056" num="0055">The auxiliary storage device <b>204</b> is an auxiliary storage device that stores various programs and information used when various programs are executed. For example, the erroneous recognition image storage unit <b>130</b> is implemented in the auxiliary storage device <b>204</b>.</p><p id="p-0057" num="0056">The display device <b>205</b> is a display device that displays various display screens containing the erroneous recognition cause information and the like. The operation device <b>206</b> is an input device for a user of the analysis device <b>100</b> to input various instructions to the analysis device <b>100</b>.</p><p id="p-0058" num="0057">The I/F device <b>207</b> is, for example, a communication device for connecting to a network (not illustrated).</p><p id="p-0059" num="0058">The drive device <b>208</b> is a device to which a recording medium <b>210</b> is set. The recording medium <b>210</b> mentioned here includes a medium that optically, electrically, or magnetically records information, such as a compact disc read only memory (CD-ROM), a flexible disk, or a magneto-optical disk. In addition, the recording medium <b>210</b> may include a semiconductor memory or the like that electrically records information, such as a ROM or a flash memory.</p><p id="p-0060" num="0059">Note that various programs to be installed in the auxiliary storage device <b>204</b> are installed, for example, when the distributed recording medium <b>210</b> is set to the drive device <b>208</b>, and the various programs recorded in the recording medium <b>210</b> are read by the drive device <b>208</b>. Alternatively, various programs to be installed in the auxiliary storage device <b>204</b> may be downloaded from a network (not illustrated) to be installed.</p><p id="p-0061" num="0060">&#x3c;Functional Configuration of Erroneous Recognition Cause Extraction Unit&#x3e;</p><p id="p-0062" num="0061">Next, among the functions implemented in the analysis device <b>100</b> according to the first embodiment, details of each unit (the image refiner initialization unit <b>141</b>, the refined image generation unit <b>142</b>, and the map generation unit <b>143</b>) of the erroneous recognition cause extraction unit <b>140</b> will be described. Note that, hereinafter, in explaining the details of each unit, the recognition accuracy is assumed as &#x201c;score&#x201d;, and the refined images with each level of recognition accuracy are</p><p id="p-0063" num="0062">assumed as<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0063">a refined image with a target score of 70%,</li>        <li id="ul0002-0002" num="0064">a refined image with a target score of 80%,</li>        <li id="ul0002-0003" num="0065">a refined image with a target score of 90%, and</li>        <li id="ul0002-0004" num="0066">a refined image with a target score of 100% (score-maximized refined image). However, the recognition accuracy is not limited to &#x201c;score&#x201d; (recognition accuracy other than &#x201c;score&#x201d; may be used as long as the recognition result is represented). In addition, the setting of the target scores with an incremental margin of 10% in the range of 70% to 100% is also merely an example, and it is assumed that an optional range and an optional incremental margin can be set.</li>    </ul>    </li></ul></p><p id="p-0064" num="0067">(1) Details of Image Refiner Initialization Unit</p><p id="p-0065" num="0068">First, the details of the image refiner initialization unit <b>141</b> will be described. <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating an example of the functional configuration of the image refiner initialization unit. As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the image refiner initialization unit <b>141</b> includes an image refiner unit <b>301</b> and a comparison/change unit <b>302</b>.</p><p id="p-0066" num="0069">Among these, as described above, the image refiner unit <b>301</b> is a generative model that uses the CNN to change the erroneous recognition image and generate a refined image with a predetermined level of recognition accuracy. The image refiner initialization unit <b>141</b> executes the first learning process on the image refiner unit <b>301</b>.</p><p id="p-0067" num="0070">For example, the image refiner initialization unit <b>141</b> inputs the erroneous recognition image to the image refiner unit <b>301</b> and the comparison/change unit <b>302</b>. This prompts the image refiner unit <b>301</b> to output a refined image. In addition, the refined image output from the image refiner unit <b>301</b> is input to the comparison/change unit <b>302</b>.</p><p id="p-0068" num="0071">The comparison/change unit <b>302</b> calculates the difference (image difference value) between the refined image output from the image refiner unit <b>301</b> and the erroneous recognition image input by the image refiner initialization unit <b>141</b>. In addition, the comparison/change unit <b>302</b> updates the model parameters of the image refiner unit <b>301</b> by back-error propagation of the calculated image difference value.</p><p id="p-0069" num="0072">In this manner, by executing the first learning process on the image refiner unit <b>301</b>, the model parameters are updated in the image refiner unit <b>301</b> such that an erroneous recognition image in the same state as the input erroneous recognition image is output.</p><p id="p-0070" num="0073">In the description of the present embodiment, the erroneous recognition image in the same state mentioned here will be assumed as referring to the same image as the input erroneous recognition image. However, the whole image does not necessarily have to be the same, and an image that will have the same recognition result when the image recognition process is executed may be adopted.</p><p id="p-0071" num="0074">For example, the image refiner unit <b>301</b> is initialized by updating the model parameters such that the erroneous recognition image in the same state as each erroneous recognition image is output even when any kind of erroneous recognition image is input.</p><p id="p-0072" num="0075">Note that the image refiner unit whose model parameters have been updated by executing the first learning process (first trained generative model) is applied to the refined image generation unit <b>142</b>. This allows the second learning process to be executed using the image refiner unit in a predetermined state, without using the image refiner unit in a state in which the model parameters are initialized by random numbers and the history is unknown as in the traditional case.</p><p id="p-0073" num="0076">(2) Details of Refined Image Generation Unit</p><p id="p-0074" num="0077">Next, the details of the refined image generation unit <b>142</b> will be described. <figref idref="DRAWINGS">FIG. <b>4</b></figref> is a first diagram illustrating an example of the functional configuration of the refined image generation unit.</p><p id="p-0075" num="0078">As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the refined image generation unit <b>142</b> includes an image refiner unit <b>401</b>, an image error calculation unit <b>402</b>, an image recognition unit <b>403</b>, and a recognition error calculation unit <b>404</b>.</p><p id="p-0076" num="0079">The image refiner unit <b>401</b> is a first trained generative model in which the model parameters have been updated by the image refiner initialization unit <b>141</b> when the first learning process was executed. The refined image generation unit <b>142</b> executes the second learning process on the image refiner unit <b>401</b> and generates refined images with each target score from the erroneous recognition image.</p><p id="p-0077" num="0080">For example, the refined image generation unit <b>142</b> inputs the erroneous recognition image to the image refiner unit <b>401</b> and the image error calculation unit <b>402</b>. This prompts the image refiner unit <b>401</b> to generate a refined image. In addition, the image refiner unit <b>401</b> changes the erroneous recognition image such that the scores of the correct answer labels match each target score when the image recognition process is executed using the generated refined images. Furthermore, the image refiner unit <b>401</b> generates a refined image such that the amount of change from the erroneous recognition image (the difference between the generated refined image and the erroneous recognition image) becomes smaller. Consequently, according to the image refiner unit <b>401</b>, an image (refined image) that is visually close to the image (erroneous recognition image) before the change may be generated.</p><p id="p-0078" num="0081">For example, the refined image generation unit <b>142</b> executes the second learning process at each target score and updates the model parameters of the image refiner unit <b>401</b> such that<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0082">the error (score error) between the score when the image recognition process is executed using the generated refined image and the target score of the correct answer label, and</li>        <li id="ul0004-0002" num="0083">the image difference value, which is the difference between the generated refined image and the erroneous recognition image,</li>    </ul>    </li></ul></p><p id="p-0079" num="0084">are minimized.</p><p id="p-0080" num="0085">The image error calculation unit <b>402</b> calculates the difference between the erroneous recognition image and the refined image generated by the image refiner unit <b>401</b> through the course of the second learning process, and inputs the image difference value to the image refiner unit <b>401</b>. The image error calculation unit <b>402</b> calculates the image difference value by performing, for example, a difference (L1 difference) or structural similarity (SSIM) calculation for each pixel, and inputs the calculated image difference value to the image refiner unit <b>401</b>.</p><p id="p-0081" num="0086">The image recognition unit <b>403</b> is a trained CNN that performs the image recognition process with the refined image generated by the image refiner unit <b>401</b> as an input, and outputs the recognition result (the score of the label). Note that the recognition error calculation unit <b>404</b> is notified of the score output by the image recognition unit <b>403</b>.</p><p id="p-0082" num="0087">The recognition error calculation unit <b>404</b> calculates the error between the score notified by the image recognition unit <b>403</b> and the target score and notifies the image refiner unit <b>401</b> of the recognition error (score error).</p><p id="p-0083" num="0088">The second learning process for the image refiner unit <b>401</b> is</p><p id="p-0084" num="0089">performed<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0090">a preassigned number of times of learning (for example, the maximum number of times of learning=N times), or</li>        <li id="ul0006-0002" num="0091">until the score of the correct answer label exceeds a predetermined threshold value with respect to the target score, or</li>        <li id="ul0006-0003" num="0092">until the score of the correct answer label exceeds the predetermined threshold value with respect to the target score and the image difference value becomes smaller than a predetermined threshold value.</li>    </ul>    </li></ul></p><p id="p-0085" num="0093">Note that the map generation unit <b>143</b> is notified of structural information of the image recognition unit <b>403</b> when the image recognition process was performed by the image recognition unit <b>403</b> on the refined images with each target score generated by the image refiner unit <b>401</b>. In the present embodiment, the structural information of the image recognition unit <b>403</b></p><p id="p-0086" num="0094">includes<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0095">image recognition unit structural information when the image recognition process was performed on the refined image with a target score of 70%,</li>        <li id="ul0008-0002" num="0096">image recognition unit structural information when the image recognition process was performed on the refined image with a target score of 80%,</li>        <li id="ul0008-0003" num="0097">image recognition unit structural information when the image recognition process was performed on the refined image with a target score of 90%, and</li>        <li id="ul0008-0004" num="0098">image recognition unit structural information when the image recognition process was performed on the refined image with a target score of 100%.</li>    </ul>    </li></ul></p><p id="p-0087" num="0099">(3) Details of Map Generation Unit</p><p id="p-0088" num="0100">Next, the details of the map generation unit <b>143</b> will be described. <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a first diagram illustrating an example of the functional configuration of the map generation unit.</p><p id="p-0089" num="0101">As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the map generation unit <b>143</b> includes an important feature map generation unit <b>511</b> and a difference map generation unit <b>512</b>.</p><p id="p-0090" num="0102">The important feature map generation unit <b>511</b> acquires the structural information of the image recognition unit <b>403</b> from the refined image generation unit <b>142</b>. In addition, the important feature map generation unit <b>511</b> generates &#x201c;important feature map&#x201d;, based on the structural information of the image recognition unit <b>403</b> by using a back propagation (BP) method, a guided back propagation (GBP) method, or a selective BP method. The important feature map is a map that visualizes the feature portion that reacted during the image recognition process.</p><p id="p-0091" num="0103">Note that the BP method is a method in which the error of each label with respect to the target score is computed from a classification probability obtained by performing the image recognition process on the refined images with the target scores, and the feature portion is visualized by forming an image of the magnitude of a gradient obtained by back-error propagation to the input layer. In addition, the GBP method is a method in which the feature portion is visualized by forming an image of only the positive values of the gradient information as the feature portion.</p><p id="p-0092" num="0104">Furthermore, the selective BP method is a method in which the error between the score of the correct answer label and the target score is computed and the processing is performed using the BP method or the GBP method. In the case of the selective BP method, the feature portion to be visualized is the feature portion that affects only the target score of the correct answer label.</p><p id="p-0093" num="0105">The important feature map generation unit <b>511</b> outputs an important feature map <b>520</b> corresponding to a target score of 70% among the generated important feature maps, as one piece of the erroneous recognition cause information. In addition, the important feature map generation unit <b>511</b> notifies the difference map generation unit <b>512</b> of the generated important feature maps.</p><p id="p-0094" num="0106">The difference map generation unit <b>512</b> generates a plurality of difference maps by calculating the differences between the important feature maps generated by the important feature map generation unit <b>511</b>. For example, the difference map generation unit <b>512</b>:<ul id="ul0009" list-style="none">    <li id="ul0009-0001" num="0000">    <ul id="ul0010" list-style="none">        <li id="ul0010-0001" num="0107">generates a difference map <b>521</b> by calculating the image difference value between the important feature map corresponding to a target score of 70% and the important feature map corresponding to a target score of 80%;</li>        <li id="ul0010-0002" num="0108">generates a difference map <b>522</b> by calculating the image difference value between the important feature map corresponding to a target score of 80% and the important feature map corresponding to a target score of 90%; and</li>        <li id="ul0010-0003" num="0109">generates a difference map <b>523</b> by calculating the image difference value between the important feature map corresponding to a target score of 90% and the important feature map corresponding to a target score of 100%.</li>    </ul>    </li></ul></p><p id="p-0095" num="0110">In addition, the difference map generation unit <b>512</b>:<ul id="ul0011" list-style="none">    <li id="ul0011-0001" num="0000">    <ul id="ul0012" list-style="none">        <li id="ul0012-0001" num="0111">outputs an important feature map obtained by adding the difference map <b>521</b> to the important feature map <b>520</b> corresponding to a target score of 70%, as one piece of the erroneous recognition cause information;</li>        <li id="ul0012-0002" num="0112">outputs an important feature map obtained by adding the difference map <b>521</b> and the difference map <b>522</b> to the important feature map <b>520</b> corresponding to a target score of 70%, as one piece of the erroneous recognition cause information; and</li>        <li id="ul0012-0003" num="0113">outputs an important feature map obtained by adding the difference map <b>521</b>, the difference map <b>522</b>, and the difference map <b>523</b> to the important feature map <b>520</b> corresponding to a target score of 70%, as one piece of the erroneous recognition cause information.</li>    </ul>    </li></ul></p><p id="p-0096" num="0114">&#x3c;Flow of Erroneous Recognition Cause Extraction Process&#x3e;</p><p id="p-0097" num="0115">Next, the flow of an erroneous recognition cause extraction process by the erroneous recognition cause extraction unit <b>140</b> will be described. <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a first flowchart illustrating a flow of the erroneous recognition cause extraction process. When the erroneous recognition image is newly stored in the erroneous recognition image storage unit <b>130</b>, the erroneous recognition cause extraction process illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is started.</p><p id="p-0098" num="0116">In step S<b>601</b>, the erroneous recognition cause extraction unit <b>140</b> acquires the erroneous recognition image from the erroneous recognition image storage unit <b>130</b>.</p><p id="p-0099" num="0117">In step S<b>602</b>, the image refiner initialization unit <b>141</b> executes the first learning process in order to initialize the image refiner unit <b>301</b> (generative model) and generates the first trained generative model.</p><p id="p-0100" num="0118">In step S<b>603</b>, the refined image generation unit <b>142</b> sets the initial target score (70%) and the incremental margin (10%) of the target score.</p><p id="p-0101" num="0119">In step S<b>604</b>, the refined image generation unit <b>142</b> executes the second learning process on the image refiner unit <b>401</b> (first trained generative model) such that the current target score is reached. This prompts the image refiner unit <b>401</b> to generate a refined image with the current target score.</p><p id="p-0102" num="0120">In step S<b>605</b>, the map generation unit <b>143</b> acquires the structural information of the image recognition unit <b>403</b> when the image recognition unit <b>403</b> performed the image recognition process by inputting the refined image with the current target score.</p><p id="p-0103" num="0121">In step S<b>606</b>, the refined image generation unit <b>142</b> determines whether or not the current target score has reached the maximum score (100%). When it is determined in step S<b>606</b> that the current target score has not reached the maximum score (in the case of NO in step S<b>606</b>), the process proceeds to step S<b>607</b>.</p><p id="p-0104" num="0122">In step S<b>607</b>, the refined image generation unit <b>142</b> adds the incremental margin to the current target score and returns to step S<b>604</b>.</p><p id="p-0105" num="0123">On the other hand, when it is determined in step S<b>606</b> that the current target score has reached the maximum score (in the case of YES in step S<b>606</b>), the process proceeds to step S<b>608</b>.</p><p id="p-0106" num="0124">In step S<b>608</b>, the map generation unit <b>143</b> generates the important feature maps corresponding to each target score, based on the structural information of the image recognition unit <b>403</b> corresponding to each target score.</p><p id="p-0107" num="0125">In step S<b>609</b>, the map generation unit <b>143</b> generates the difference maps based on the important feature maps corresponding to each target score.</p><p id="p-0108" num="0126">In step S<b>610</b>, the map generation unit <b>143</b> outputs the important feature map corresponding to the initial target score as one piece of the erroneous recognition cause information. In addition, the map generation unit <b>143</b> sequentially adds the difference maps to the important feature map corresponding to the initial target score and outputs each of the added important feature maps as one piece of the erroneous recognition cause information.</p><p id="p-0109" num="0127">As is clear from the above description, the analysis device <b>100</b> according to the first embodiment executes the first learning process for initializing the image refiner unit, by inputting the erroneous recognition image, and generates the first trained generative model. In addition, the analysis device <b>100</b> according to the first embodiment generates the refined images with each level of recognition accuracy (each target score), using the first trained generative model, and generates the important feature maps based on the structural information when the image recognition process was performed on the refined images with each level of recognition accuracy. Furthermore, the analysis device <b>100</b> according to the first embodiment outputs the important feature map corresponding to the initial recognition accuracy, as one piece of the erroneous recognition cause information. Additionally, the analysis device <b>100</b> according to the first embodiment sequentially adds the difference maps between the important feature maps corresponding to each level of recognition accuracy to the important feature map corresponding to the initial recognition accuracy and outputs each of the added important feature maps, as one piece of the erroneous recognition cause information.</p><p id="p-0110" num="0128">As described above, in the analysis device according to the first embodiment, with the recognition accuracy in the middle of the course, it may be possible to visualize which image part among the image parts that cause erroneous recognition has influence (degree of influence), by outputting the important feature maps corresponding to each level of recognition accuracy.</p><heading id="h-0008" level="1">Second Embodiment</heading><p id="p-0111" num="0129">In the above first embodiment, each of the important feature maps generated based on the structural information when the image recognition process was performed on the refined images with each level of recognition accuracy is output as the erroneous recognition cause information. However, the map output as the erroneous recognition cause information is not limited to the important feature map. A second embodiment will be described below focusing on differences from the first embodiment described above.</p><p id="p-0112" num="0130">&#x3c;Functional Configuration of Erroneous Recognition Cause Extraction Unit&#x3e;</p><p id="p-0113" num="0131">(1) Details of Refined Image Generation Unit</p><p id="p-0114" num="0132"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a second diagram illustrating an example of the functional configuration of a refined image generation unit. The difference from the refined image generation unit <b>142</b> described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref> in the above first embodiment is that, in the case of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a score-maximized refined image storage unit <b>710</b> is included.</p><p id="p-0115" num="0133">The score-maximized refined image storage unit <b>710</b> stores the refined image with a target score of 100% (score-maximized refined image) among the refined images generated by an image refiner unit <b>401</b>.</p><p id="p-0116" num="0134">(2) Details of Map Generation Unit</p><p id="p-0117" num="0135">Next, the details of a map generation unit <b>143</b> will be described. <figref idref="DRAWINGS">FIG. <b>8</b></figref> is a second diagram illustrating an example of the functional configuration of the map generation unit.</p><p id="p-0118" num="0136">As illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the map generation unit <b>143</b> includes a deterioration scale map generation unit <b>801</b> and a superimposition unit <b>802</b> in addition to an important feature map generation unit <b>511</b> and a difference map generation unit <b>512</b>.</p><p id="p-0119" num="0137">The deterioration scale map generation unit <b>801</b> acquires the score-maximized refined image stored in the score-maximized refined image storage unit <b>710</b>. In addition, the deterioration scale map generation unit <b>801</b> acquires the erroneous recognition image. Furthermore, the deterioration scale map generation unit <b>801</b> calculates the difference between the score-maximized refined image and the erroneous recognition image and generates a deterioration scale map <b>810</b>.</p><p id="p-0120" num="0138">For example, the deterioration scale map is a map indicating changed portions and the extent of change of each changed portion when the score-maximized refined image is generated from the erroneous recognition image.</p><p id="p-0121" num="0139">The superimposition unit <b>802</b> generates an important feature index map <b>820</b> corresponding to a target score of 70%, by superimposing an important feature map <b>520</b> generated by the important feature map generation unit <b>511</b> and the deterioration scale map <b>810</b> generated by the deterioration scale map generation unit <b>801</b>. In addition, the superimposition unit <b>802</b> outputs the generated important feature index map <b>820</b> corresponding to a target score of 70%, as one piece of the erroneous recognition cause information.</p><p id="p-0122" num="0140">Furthermore, the superimposition unit <b>802</b> sequentially adds difference maps <b>521</b>, <b>522</b>, and <b>523</b> to the important feature index map <b>820</b> corresponding to a target score of 70% and outputs each of a plurality of important feature index maps including<ul id="ul0013" list-style="none">    <li id="ul0013-0001" num="0000">    <ul id="ul0014" list-style="none">        <li id="ul0014-0001" num="0141">an important feature index map <b>821</b> corresponding to a target score of 80%,</li>        <li id="ul0014-0002" num="0142">an important feature index map <b>822</b> corresponding to a target score of 90%, and</li>        <li id="ul0014-0003" num="0143">an important feature index map <b>823</b> corresponding to a target score of 100%,</li>    </ul>    </li></ul></p><p id="p-0123" num="0144">as one piece of the erroneous recognition cause information.</p><p id="p-0124" num="0145">&#x3c;Flow of Erroneous Recognition Cause Extraction Process&#x3e;</p><p id="p-0125" num="0146">Next, the flow of an erroneous recognition cause extraction process by an erroneous recognition cause extraction unit <b>140</b> will be described. <figref idref="DRAWINGS">FIG. <b>9</b></figref> is a second flowchart illustrating a flow of the erroneous recognition cause extraction process. The differences from the erroneous recognition cause extraction process described with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref> in the above first embodiment are steps S<b>901</b> to S<b>904</b>.</p><p id="p-0126" num="0147">In step S<b>901</b>, the map generation unit <b>143</b> acquires the score-maximized refined image generated by the image refiner unit <b>401</b>.</p><p id="p-0127" num="0148">In step S<b>902</b>, the map generation unit <b>143</b> calculates the difference between the score-maximized refined image and the erroneous recognition image and generates the deterioration scale map.</p><p id="p-0128" num="0149">In step S<b>903</b>, the map generation unit <b>143</b> generates the important feature index map corresponding to the initial target score, by superimposing the important feature map corresponding to the initial target score onto the deterioration scale map, and outputs the generated important feature index map, as one piece of the erroneous recognition cause information.</p><p id="p-0129" num="0150">In step S<b>904</b>, the map generation unit <b>143</b> sequentially adds the difference maps to the important feature index map corresponding to the initial target score and generates the important feature index maps corresponding to each target score. In addition, the map generation unit <b>143</b> outputs each of the important feature index maps corresponding to each target score, as one piece of the erroneous recognition cause information.</p><p id="p-0130" num="0151">As is clear from the above description, an analysis device <b>100</b> according to the second embodiment further includes the deterioration scale map generation unit, in addition to the functions provided in the analysis device <b>100</b> according to the above first embodiment, and generates the deterioration scale map. In addition, the analysis device <b>100</b> according to the second embodiment further includes the superimposition unit, generates the important feature index map by superimposing the important feature map corresponding to the initial recognition accuracy onto the deterioration scale map, and outputs the generated important feature index map as one piece of the erroneous recognition cause information. Furthermore, the analysis device <b>100</b> according to the second embodiment sequentially adds the difference maps between the important feature maps corresponding to each level of recognition accuracy to the important feature index map corresponding to the initial recognition accuracy and outputs each of the added important feature index maps, as one piece of the erroneous recognition cause information.</p><p id="p-0131" num="0152">As described above, in the analysis device according to the second embodiment, with the recognition accuracy in the middle of the course, it may be possible to visualize which image part among the image parts that cause erroneous recognition has influence (degree of influence), by outputting the important feature index maps corresponding to each level of recognition accuracy.</p><heading id="h-0009" level="1">Third Embodiment</heading><p id="p-0132" num="0153">In the above first and second embodiments, the important feature maps corresponding to each level of recognition accuracy or the important feature index maps corresponding to each level of recognition accuracy are output as the erroneous recognition cause information. In contrast to this, in a third embodiment, the combinations of superpixels (changeable areas) at each level of recognition accuracy specified based on the important feature index maps corresponding to each level of recognition accuracy are output as the erroneous recognition cause information. The third embodiment will be described below focusing on differences from the first and second embodiments described above.</p><p id="p-0133" num="0154">&#x3c;Functional Configuration of Analysis Device&#x3e;</p><p id="p-0134" num="0155"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a second diagram illustrating an example of the functional configuration of an analysis device. The difference from the functional configuration of the analysis device <b>100</b> described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref> in the above first embodiment is that, in the case of <figref idref="DRAWINGS">FIG. <b>10</b></figref>, an erroneous recognition cause extraction unit <b>140</b> includes a specifying unit <b>1001</b>.</p><p id="p-0135" num="0156">The specifying unit <b>1001</b> replaces a changeable area in the erroneous recognition image defined based on the generated important feature index map with the generated refined image. In addition, the specifying unit <b>1001</b> executes an image recognition process by inputting the erroneous recognition image in which the changeable area is replaced with the refined image, and determines the effect of the replacement from the output recognition result (the score of the label).</p><p id="p-0136" num="0157">Furthermore, the specifying unit <b>1001</b> repeats the image recognition process while modifying the dimensions of the changeable area and specifies, from the recognition result (the score of the label), a combination of superpixels (changeable area) that causes erroneous recognition at each level of recognition accuracy (each target score). Additionally, the specifying unit <b>1001</b> outputs the combinations of superpixels (changeable areas) that cause erroneous recognition, which have been specified at each level of recognition accuracy, as the erroneous recognition cause information.</p><p id="p-0137" num="0158">In this manner, by referring to the effect of the replacement when the changeable area is replaced with the refined image, each image part that causes erroneous recognition at each level of recognition accuracy (each target score) may be accurately specified.</p><p id="p-0138" num="0159">&#x3c;Functional Configuration of Specifying Unit&#x3e;</p><p id="p-0139" num="0160">Next, a functional configuration of the specifying unit <b>1001</b> will be described. <figref idref="DRAWINGS">FIG. <b>11</b></figref> is a first diagram illustrating an example of the functional configuration of the specifying unit. As illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the specifying unit <b>1001</b> includes a superpixel dividing unit <b>1101</b>, an important superpixel designation unit <b>1102</b>, an image recognition unit <b>1103</b>, and an important superpixel evaluation unit <b>1104</b>.</p><p id="p-0140" num="0161">The superpixel dividing unit <b>1101</b> divides the erroneous recognition image into &#x201c;superpixels&#x201d;, which are areas for each component of the object (a vehicle in the present embodiment) included in the erroneous recognition image, and outputs superpixel division information. Note that, in dividing the erroneous recognition image into superpixels, an existing dividing function is used, or a CNN or the like trained so as to divide for each component of the vehicle is used.</p><p id="p-0141" num="0162">The important superpixel designation unit <b>1102</b> separately adds, for each superpixel,<ul id="ul0015" list-style="none">    <li id="ul0015-0001" num="0000">    <ul id="ul0016" list-style="none">        <li id="ul0016-0001" num="0163">the value of each pixel of the important feature index map corresponding to a target score of 70%,</li>        <li id="ul0016-0002" num="0164">the value of each pixel of the important feature index map corresponding to a target score of 80%,</li>        <li id="ul0016-0003" num="0165">the value of each pixel of the important feature index map corresponding to a target score of 90%, and</li>        <li id="ul0016-0004" num="0166">the value of each pixel of the important feature index map corresponding to a target score of 100%,</li>    </ul>    </li></ul></p><p id="p-0142" num="0167">which have been generated by a superimposition unit <b>802</b> based on the superpixel division information output by the superpixel dividing unit <b>1101</b>.</p><p id="p-0143" num="0168">In addition, among respective superpixels, the important superpixel designation unit <b>1102</b> extracts a superpixel whose additional value of respective added pixels is equal to or higher than a predetermined threshold value (important feature index threshold value) for each target score. Furthermore, the important superpixel designation unit <b>1102</b> defines superpixels selected from among the superpixels extracted for each target score and combined, as a changeable area, and defines the superpixels other than the combined superpixels as an unchangeable area.</p><p id="p-0144" num="0169">Additionally, the important superpixel designation unit <b>1102</b> extracts the image portion corresponding to the unchangeable area from the erroneous recognition image, extracts the image portion corresponding to the changeable area from the refined image, and generates a composite image by compositing the two extracted image portions. Since<ul id="ul0017" list-style="none">    <li id="ul0017-0001" num="0000">    <ul id="ul0018" list-style="none">        <li id="ul0018-0001" num="0170">the refined image with a target score of 70%,</li>        <li id="ul0018-0002" num="0171">the refined image with a target score of 80%,</li>        <li id="ul0018-0003" num="0172">the refined image with a target score of 90%, and</li>        <li id="ul0018-0004" num="0173">the refined image with a target score of 100%,</li>    </ul>    </li></ul></p><p id="p-0145" num="0174">are output from an image refiner unit <b>401</b>, the important superpixel designation unit <b>1102</b> generates<ul id="ul0019" list-style="none">    <li id="ul0019-0001" num="0000">    <ul id="ul0020" list-style="none">        <li id="ul0020-0001" num="0175">the composite image corresponding to a target score of 70%,</li>        <li id="ul0020-0002" num="0176">the composite image corresponding to a target score of 80%,</li>        <li id="ul0020-0003" num="0177">the composite image corresponding to a target score of 90%, and</li>        <li id="ul0020-0004" num="0178">the composite image corresponding to a target score of 100%,</li>    </ul>    </li></ul></p><p id="p-0146" num="0179">for each of the refined images.</p><p id="p-0147" num="0180">Note that the important superpixel designation unit <b>1102</b> increases the number of superpixels to be extracted (expands the changeable area and narrows down the unchangeable area), by slowly lowering the important feature index threshold value used when defining the changeable area and the unchangeable area. In addition, the important superpixel designation unit <b>1102</b> updates the changeable area and the unchangeable area while modifying the combination of superpixels selected from among the extracted superpixels.</p><p id="p-0148" num="0181">The image recognition unit <b>1103</b>, which has the same function as the function of the image recognition unit <b>403</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, performs the image recognition process by inputting each composite image generated by the important superpixel designation unit <b>1102</b> and outputs the recognition result (the score of the label).</p><p id="p-0149" num="0182">The important superpixel evaluation unit <b>1104</b> acquires the recognition result (the score of the label) output from the image recognition unit <b>1103</b>. As described above, for each of the target scores, the important superpixel designation unit <b>1102</b> generates a number of composite images according to the number of times the important feature index threshold value is lowered and the number of combinations of superpixels. Therefore, the important superpixel evaluation unit <b>1104</b> acquires a number of scores according to the number, for each of the target scores. In addition, the important superpixel evaluation unit <b>1104</b> specifies the combination of superpixels (changeable area) that causes erroneous recognition at each of the target scores, based on the recognition result, and outputs the specified combination as the erroneous recognition cause information.</p><p id="p-0150" num="0183">&#x3c;Specific Example of Processing of Each Unit of Specifying Unit&#x3e;</p><p id="p-0151" num="0184">Next, a specific example of processing of each unit (here, the superpixel dividing unit <b>1101</b> and the important superpixel designation unit <b>1102</b>) of the specifying unit <b>1001</b> will be described.</p><p id="p-0152" num="0185">(1) Specific Example of Processing of Superpixel Dividing Unit</p><p id="p-0153" num="0186">First, a specific example of processing of the superpixel dividing unit <b>1101</b> will be described. <figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating a specific example of processing of the superpixel dividing unit. As illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the superpixel dividing unit <b>1101</b> includes, for example, a simple linear iterative clustering (SLIC) unit <b>1210</b> that performs SLIC processing. The SLIC unit <b>1210</b> divides the erroneous recognition image into superpixels, which are partial images for each component of the vehicle included in the erroneous recognition image. In addition, the superpixel dividing unit <b>1101</b> outputs the superpixel division information about the erroneous recognition image generated by the SLIC unit <b>1210</b> dividing the erroneous recognition image into superpixels.</p><p id="p-0154" num="0187">(2) Specific Example of Processing of Important Superpixel Designation Unit</p><p id="p-0155" num="0188">Next, a specific example of processing of the important superpixel designation unit <b>1102</b> will be described. <figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram illustrating a specific example of processing of the important superpixel designation unit.</p><p id="p-0156" num="0189">As illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the important superpixel designation unit <b>1102</b> includes an area extraction unit <b>1310</b> and a compositing unit <b>1311</b>.</p><p id="p-0157" num="0190">The important superpixel designation unit <b>1102</b></p><p id="p-0158" num="0191">overlays<ul id="ul0021" list-style="none">    <li id="ul0021-0001" num="0000">    <ul id="ul0022" list-style="none">        <li id="ul0022-0001" num="0192">the important feature index maps corresponding to a target score of 70% to a target score of 100% output from the superimposition unit <b>802</b> (here, the important feature index map corresponding to a target score X % is assumed for simplification of explanation), and</li>        <li id="ul0022-0002" num="0193">the superpixel division information output from the superpixel dividing unit <b>1101</b>. This prompts the important superpixel designation unit <b>1102</b> to generate an important superpixel image <b>1301</b> corresponding to the target score X %.</li>    </ul>    </li></ul></p><p id="p-0159" num="0194">In addition, the important superpixel designation unit <b>1102</b> adds the value of each pixel of the important feature index map corresponding to the target score X % for each of the superpixels in the generated important superpixel image <b>1301</b>.</p><p id="p-0160" num="0195">Furthermore, the important superpixel designation unit <b>1102</b> determines whether or not the additional value for each superpixel is equal to or higher than the important feature index threshold value and extracts the superpixel determined to have an additional value equal to or higher than the important feature index threshold value. Note that, in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, an important superpixel image <b>1302</b> corresponding to the target score X % clearly indicates an example of the additional values for each superpixel.</p><p id="p-0161" num="0196">In addition, the important superpixel designation unit <b>1102</b> defines superpixels selected from among the extracted superpixels and combined, as a changeable area, and defines the superpixels other than the combined superpixels as an unchangeable area. Furthermore, the important superpixel designation unit <b>1102</b> notifies the area extraction unit <b>1310</b> of the defined changeable area and unchangeable area.</p><p id="p-0162" num="0197">The area extraction unit <b>1310</b> extracts the image portion corresponding to the unchangeable area from the erroneous recognition image.</p><p id="p-0163" num="0198">In addition, the area extraction unit <b>1310</b> extracts the image portions corresponding to the changeable area from the refined images with a target score of 70% to a target score of 100% (here, the refined image with the target score X % is assumed for simplification of explanation).</p><p id="p-0164" num="0199">The compositing unit <b>1311</b> composites the image portion corresponding to the changeable area extracted from the refined image with the target score X % and the image portion corresponding to the unchangeable area extracted from the erroneous recognition image, and generates a composite image corresponding to the target score X %.</p><p id="p-0165" num="0200"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram illustrating a specific example of processing of the area extraction unit and the compositing unit. In <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the upper part illustrates a situation in which the area extraction unit <b>1310</b> extracts the image portion (the white portion of an image <b>1402</b>) corresponding to the changeable area from a refined image <b>1401</b> with the target score X %.</p><p id="p-0166" num="0201">Meanwhile, in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the lower part illustrates a situation in which the area extraction unit <b>1310</b> extracts the image portion (the white portion of an image <b>1402</b>&#x2032;) corresponding to the unchangeable area from an erroneous recognition image <b>1411</b>. Note that the image <b>1402</b>&#x2032; is an image obtained by inverting the white portion and the black portion of the image <b>1402</b> (for convenience of explanation, the white portion in the lower part of <figref idref="DRAWINGS">FIG. <b>14</b></figref> is assumed as the image portion corresponding to the unchangeable area).</p><p id="p-0167" num="0202">As illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the compositing unit <b>1311</b> composites<ul id="ul0023" list-style="none">    <li id="ul0023-0001" num="0000">    <ul id="ul0024" list-style="none">        <li id="ul0024-0001" num="0203">an image portion <b>1403</b> corresponding to the changeable area of the refined image <b>1401</b> with the target score X %, and</li>        <li id="ul0024-0002" num="0204">an image portion <b>1413</b> corresponding to the unchangeable area of the erroneous recognition image <b>1411</b>,</li>    </ul>    </li></ul></p><p id="p-0168" num="0205">which have been output from the area extraction unit <b>1310</b>, and generates a composite image <b>1420</b> corresponding to the target score X %.</p><p id="p-0169" num="0206">In this manner, when generating the composite image <b>1420</b>, the specifying unit <b>1001</b> adds the value of each pixel of the important feature index map corresponding to the target score X % in superpixel units. Consequently, according to the specifying unit <b>1001</b>, the area to be replaced with the refined image with the target score X % may be specified in superpixel units.</p><p id="p-0170" num="0207">&#x3c;Flow of Erroneous Recognition Cause Extraction Process&#x3e;</p><p id="p-0171" num="0208">Next, the flow of an erroneous recognition cause extraction process by the erroneous recognition cause extraction unit <b>140</b> will be described. <figref idref="DRAWINGS">FIG. <b>15</b></figref> is a third flowchart illustrating a flow of the erroneous recognition cause extraction process. The differences from the erroneous recognition cause extraction process described with reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref> in the above second embodiment are steps S<b>1501</b> and S<b>1502</b>.</p><p id="p-0172" num="0209">In step S<b>1501</b>, a map generation unit <b>143</b> sequentially adds the difference maps to the important feature index map corresponding to the initial target score and generates the important feature index maps corresponding to each target score.</p><p id="p-0173" num="0210">In step S<b>1502</b>, the specifying unit <b>1001</b> executes a changeable area specifying process that outputs the changeable areas at each level of recognition accuracy specified based on<ul id="ul0025" list-style="none">    <li id="ul0025-0001" num="0000">    <ul id="ul0026" list-style="none">        <li id="ul0026-0001" num="0211">the erroneous recognition image,</li>        <li id="ul0026-0002" num="0212">the refined images with each target score, and</li>        <li id="ul0026-0003" num="0213">the important feature index maps corresponding to each target score,</li>    </ul>    </li></ul></p><p id="p-0174" num="0214">as the erroneous recognition cause information. Note that the details of the changeable area specifying process will be described later.</p><p id="p-0175" num="0215">&#x3c;Flow of Changeable Area Specifying Process&#x3e;</p><p id="p-0176" num="0216">Next, the flow of the changeable area specifying process (step S<b>1502</b> in <figref idref="DRAWINGS">FIG. <b>15</b></figref>) will be described. <figref idref="DRAWINGS">FIG. <b>16</b></figref> is a flowchart illustrating a flow of the changeable area specifying process.</p><p id="p-0177" num="0217">In step S<b>1601</b>, the superpixel dividing unit <b>1101</b> divides the erroneous recognition image into superpixels and generates the superpixel division information.</p><p id="p-0178" num="0218">In step S<b>1602</b>, the important superpixel designation unit <b>1102</b> adds the value of each pixel of the important feature index map corresponding to the current target score in superpixel units. Note that, at the start of the changeable area specifying process, it is assumed that the initial target score (70%) is set as the default value for &#x201c;current target score&#x201d;.</p><p id="p-0179" num="0219">In step S<b>1603</b>, the important superpixel designation unit <b>1102</b> extracts a superpixel whose additional value is equal to or higher than the important feature index threshold value and defines the changeable area by combining superpixels selected from among the extracted superpixels. In addition, the important superpixel designation unit <b>1102</b> defines the superpixels other than the combined superpixels as the unchangeable area.</p><p id="p-0180" num="0220">In step S<b>1604</b>, the important superpixel designation unit <b>1102</b> reads the refined image with the current target score.</p><p id="p-0181" num="0221">In step S<b>1605</b>, the important superpixel designation unit <b>1102</b> extracts the image portion corresponding to the changeable area from the refined image with the current target score.</p><p id="p-0182" num="0222">In step S<b>1606</b>, the important superpixel designation unit <b>1102</b> extracts the image portion corresponding to the unchangeable area from the erroneous recognition image.</p><p id="p-0183" num="0223">In step S<b>1607</b>, the important superpixel designation unit <b>1102</b> composites the image portion corresponding to the changeable area extracted from the refined image and the image portion corresponding to the unchangeable area extracted from the erroneous recognition image, and generates a composite image corresponding to the current target score.</p><p id="p-0184" num="0224">In step S<b>1608</b>, the image recognition unit <b>1103</b> performs the image recognition process by inputting the composite image corresponding to the current target score and calculates the score of the correct answer label. In addition, the important superpixel evaluation unit <b>1104</b> acquires the score of the correct answer label calculated by the image recognition unit <b>1103</b>.</p><p id="p-0185" num="0225">In step S<b>1609</b>, the important superpixel designation unit <b>1102</b> determines whether or not the important feature index threshold value has reached a lower limit value. When it is determined in step S<b>1609</b> that the lower limit value has not been reached (in the case of NO in step S<b>1609</b>), the process proceeds to step S<b>1610</b>.</p><p id="p-0186" num="0226">In step S<b>1610</b>, the important superpixel designation unit <b>1102</b> lowers the important feature index threshold value and then returns to step S<b>1603</b>.</p><p id="p-0187" num="0227">On the other hand, when it is determined in step S<b>1609</b> that the lower limit value has been reached (in the case of YES in step S<b>1609</b>), the process proceeds to step S<b>1611</b>.</p><p id="p-0188" num="0228">In step S<b>1611</b>, the important superpixel evaluation unit <b>1104</b> specifies the combination of superpixels (changeable area) that causes erroneous recognition at the current target score, based on the acquired score of the correct answer label, and outputs the specified combination of superpixels (changeable area) as one piece of the erroneous recognition cause information.</p><p id="p-0189" num="0229">In step S<b>1612</b>, the specifying unit <b>1001</b> determines whether or not the current target score has reached the maximum score (100%). When it is determined in step S<b>1612</b> that the current target score has not reached the maximum score (in the case of NO in step S<b>1612</b>), the process proceeds to step S<b>1613</b>.</p><p id="p-0190" num="0230">In step S<b>1613</b>, the specifying unit <b>1001</b> adds the incremental margin to the current target score and returns to step S<b>1602</b>.</p><p id="p-0191" num="0231">On the other hand, when it is determined in step S<b>1612</b> that the current target score has reached the maximum score (in the case of YES in step S<b>1612</b>), the changeable area specifying process is ended.</p><p id="p-0192" num="0232">As is clear from the above description, the analysis device <b>100</b> according to the third embodiment further includes the specifying unit <b>1001</b>, in addition to the functions provided in the analysis device <b>100</b> according to the above second embodiment. In addition, the analysis device <b>100</b> according to the third embodiment outputs the combinations of superpixels (changeable areas) at each level of recognition accuracy specified by the specifying unit <b>1001</b> based on the important feature index maps corresponding to each level of recognition accuracy, as the erroneous recognition cause information.</p><p id="p-0193" num="0233">As described above, in the analysis device according to the third embodiment, with the recognition accuracy in the middle of the course, it may be possible to visualize which image part among the image parts that cause erroneous recognition has influence (degree of influence), by outputting the changeable areas corresponding to each level of recognition accuracy.</p><heading id="h-0010" level="1">Fourth Embodiment</heading><p id="p-0194" num="0234">In the above third embodiment, the combinations of superpixels (changeable areas) corresponding to each level of recognition accuracy have been described as being output as the erroneous recognition cause information. However, the method of outputting the erroneous recognition cause information is not limited to this, and for example, an important portion in the changeable area may be output in pixel units. A fourth embodiment will be described below focusing on differences from the third embodiment described above.</p><p id="p-0195" num="0235">&#x3c;Functional Configuration of Specifying Unit&#x3e;</p><p id="p-0196" num="0236">First, a functional configuration of a specifying unit in an analysis device <b>100</b> according to the fourth embodiment will be described. <figref idref="DRAWINGS">FIG. <b>17</b></figref> is a second diagram illustrating an example of the functional configuration of the specifying unit <b>1001</b>. The difference from the functional configuration of the specifying unit <b>1001</b> illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref> is that a detailed cause analysis unit <b>1701</b> is included.</p><p id="p-0197" num="0237">The detailed cause analysis unit <b>1701</b> calculates an important portion in the changeable area, using the erroneous recognition image and the refined images with each target score, and outputs the calculated important portion as an action result image.</p><p id="p-0198" num="0238">&#x3c;Functional Configuration of Detailed Cause Analysis Unit&#x3e;</p><p id="p-0199" num="0239">Next, a functional configuration of the detailed cause analysis unit <b>1701</b> will be described. <figref idref="DRAWINGS">FIG. <b>18</b></figref> is a first diagram illustrating an example of the functional configuration of the detailed cause analysis unit. As illustrated in FIG. <b>18</b>, the detailed cause analysis unit <b>1701</b> includes an image difference calculation unit <b>1801</b>, an SSIM calculation unit <b>1802</b>, a cutout unit <b>1803</b>, and an action unit <b>1804</b>.</p><p id="p-0200" num="0240">The image difference calculation unit <b>1801</b> calculates the differences in pixel units between the erroneous recognition image and the refined images with each target score (here, the refined image with the target score X % is assumed for simplification of explanation), and output a difference image.</p><p id="p-0201" num="0241">The SSIM calculation unit <b>1802</b> outputs an SSIM image by performing an SSIM calculation using the erroneous recognition image and the refined image with the target score X %.</p><p id="p-0202" num="0242">The cutout unit <b>1803</b> cuts out the image portion for the changeable area corresponding to the target score X % from the difference image. In addition, the cutout unit <b>1803</b> cuts out the image portion for the changeable area corresponding to the target score X % from the SSIM image. Furthermore, the cutout unit <b>1803</b> generates a multiplied image by multiplying the difference image and the SSIM image obtained by cutting out the image portions for the changeable area at the target score X %.</p><p id="p-0203" num="0243">The action unit <b>1804</b> generates the action result image corresponding to the target score X %, based on the erroneous recognition image and the multiplied image.</p><p id="p-0204" num="0244">&#x3c;Specific Example of Processing of Detailed Cause Analysis Unit&#x3e;</p><p id="p-0205" num="0245">Next, a specific example of processing of the detailed cause analysis unit <b>1701</b> will be described. <figref idref="DRAWINGS">FIG. <b>19</b></figref> is a diagram illustrating a specific example of processing of the detailed cause analysis unit.</p><p id="p-0206" num="0246">As illustrated in <figref idref="DRAWINGS">FIG. <b>19</b></figref>, first, in the image difference calculation unit <b>1801</b>, the difference between the erroneous recognition image (A) and the refined image (B) with the target score X % (=(A)&#x2212;(B)) is calculated, and the difference image is output. The difference image contains pixel correction information at each image part that causes erroneous recognition at the target score X %.</p><p id="p-0207" num="0247">Subsequently, in the SSIM calculation unit <b>1802</b>, the SSIM calculation is performed based on the erroneous recognition image (A) and the refined image (B) with the target score X % (y=SSIM((A), (B)). Furthermore, in the SSIM calculation unit <b>1802</b>, the result of the SSIM calculation is inverted (y&#x2032;=255&#x2212;(y&#xd7;255)), whereby the SSIM image is output. The SSIM image is an image in which each image part that causes erroneous recognition at the target score X % is located with high accuracy and represents that the difference is larger when the pixel value is higher, and that the difference is smaller when the pixel value is lower. Note that the process of inverting the result of the SSIM calculation may be performed, for example, by calculating y&#x2032;=1&#x2212; y.</p><p id="p-0208" num="0248">Subsequently, in the cutout unit <b>1803</b>, the image portion is cut out from the difference image for the changeable area corresponding to the target score X %, and a cutout image (C) is output. Similarly, in the cutout unit <b>1803</b>, the image portion is cut out from the SSIM image for the changeable area corresponding to the target score X %, and a cutout image (D) is output.</p><p id="p-0209" num="0249">Here, the changeable area corresponding to the target score X % is obtained by specifying an area of the image portion that causes erroneous recognition at the target score X %, and the detailed cause analysis unit <b>1701</b> aims to further analyze the cause at the granularity of pixels in the specified area.</p><p id="p-0210" num="0250">Therefore, the cutout unit <b>1803</b> multiplies the cutout image (C) and the cutout image (D) and generates a multiplied image (G). The multiplied image (G) is nothing but pixel correction information in which the pixel correction information at each image part that causes erroneous recognition at the target score X % is located with higher accuracy.</p><p id="p-0211" num="0251">In addition, the cutout unit <b>1803</b> performs an enhancement process on the multiplied image (G) and outputs an enhanced multiplied image (H). Note that the cutout unit <b>1803</b> calculates the enhanced multiplied image (H) based on the following formula.</p><p id="p-0212" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Enhanced Multiplied Image (<i>H</i>)=255&#xd7;(<i>G</i>)/(max(<i>G</i>)&#x2212;min(<i>G</i>))&#x2003;&#x2003;(Formula 3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0213" num="0252">Subsequently, the action unit <b>1804</b> visualizes the important portion by subtracting the enhanced multiplied image (H) from the erroneous recognition image (A) and generates an action result image corresponding to the target score X %.</p><p id="p-0214" num="0253">Note that the method for the enhancement process illustrated in <figref idref="DRAWINGS">FIG. <b>19</b></figref> is merely an example, and the enhancement process may be performed by another method as long as the method makes it easier to identify the important portion when visualized.</p><p id="p-0215" num="0254">&#x3c;Flow of Detailed Cause Analysis Process&#x3e;</p><p id="p-0216" num="0255">Next, the flow of a detailed cause analysis process by the detailed cause analysis unit <b>1701</b> will be described. <figref idref="DRAWINGS">FIG. <b>20</b></figref> is a first flowchart illustrating a flow of the detailed cause analysis process.</p><p id="p-0217" num="0256">In step S<b>2001</b>, the image difference calculation unit <b>1801</b> calculates the difference image between the erroneous recognition image and the refined image with the target score X %.</p><p id="p-0218" num="0257">In step S<b>2002</b>, the SSIM calculation unit <b>1802</b> calculates the SSIM image based on the erroneous recognition image and the refined image with the target score X %.</p><p id="p-0219" num="0258">In step S<b>2003</b>, the cutout unit <b>1803</b> cuts out the difference image for the changeable area corresponding to the target score X %.</p><p id="p-0220" num="0259">In step S<b>2004</b>, the cutout unit <b>1803</b> cuts out the SSIM image for the changeable area corresponding to the target score X %.</p><p id="p-0221" num="0260">In step S<b>2005</b>, the cutout unit <b>1803</b> multiplies the cut-out difference image and the cut-out SSIM image and generates the multiplied image.</p><p id="p-0222" num="0261">In step S<b>2006</b>, the cutout unit <b>1803</b> performs the enhancement process on the multiplied image. In addition, the action unit <b>1804</b> subtracts the multiplied image that has undergone the enhancement process, from the erroneous recognition image, and outputs the action result image corresponding to the target score X %.</p><p id="p-0223" num="0262">As is clear from the above description, the analysis device <b>100</b> according to the fourth embodiment generates the difference images and the SSIM images based on the erroneous recognition image and the refined images with each level of recognition accuracy and outputs the important portions by cutting out and multiplying the changeable areas corresponding to each level of recognition accuracy.</p><p id="p-0224" num="0263">As described above, in the analysis device according to the fourth embodiment, by outputting the important portion in the changeable area in pixel units, the degree of influence of each image part that causes erroneous recognition may be visualized in pixel units.</p><heading id="h-0011" level="1">Fifth Embodiment</heading><p id="p-0225" num="0264">In the above fourth embodiment, a case has been described in which the degree of influence of each image part that causes erroneous recognition is visualized in pixel units, using the difference images and the SSIM images generated based on the erroneous recognition image and the refined images with each level of recognition accuracy.</p><p id="p-0226" num="0265">In contrast to this, in a fifth embodiment, the degree of influence of each image part that causes erroneous recognition is visualized in pixel units, by further using important feature maps corresponding to each level of recognition accuracy. The fifth embodiment will be described below focusing on differences from the fourth embodiment described above.</p><p id="p-0227" num="0266">&#x3c;Functional Configuration of Detailed Cause Analysis Unit&#x3e;</p><p id="p-0228" num="0267">First, a functional configuration of a detailed cause analysis unit in an analysis device <b>100</b> according to the fifth embodiment will be described. <figref idref="DRAWINGS">FIG. <b>21</b></figref> is a second diagram illustrating an example of the functional configuration of the detailed cause analysis unit. The difference from the functional configuration of the detailed cause analysis unit illustrated in <figref idref="DRAWINGS">FIG. <b>19</b></figref> is that, in the case of <figref idref="DRAWINGS">FIG. <b>21</b></figref>, an important feature map generation unit <b>2101</b> is included.</p><p id="p-0229" num="0268">The important feature map generation unit <b>2101</b> acquires image recognition unit structural information corresponding to each target score (here, for simplification of explanation, the image recognition unit structural information corresponding to the target score X %) from an image recognition unit <b>403</b>. In addition, the important feature map generation unit <b>2101</b> generates an important feature map corresponding to the target score X %, based on the image recognition unit structural information corresponding to the target score X % by using the selective BP method.</p><p id="p-0230" num="0269">In the present embodiment, using the difference image, the SSIM image, and the important feature map corresponding to the target score X % generated based on<ul id="ul0027" list-style="none">    <li id="ul0027-0001" num="0000">    <ul id="ul0028" list-style="none">        <li id="ul0028-0001" num="0270">the erroneous recognition image,</li>        <li id="ul0028-0002" num="0271">the refined image with the target score X %, and</li>        <li id="ul0028-0003" num="0272">the image recognition unit structural information corresponding to the target score X %,</li>    </ul>    </li></ul></p><p id="p-0231" num="0273">the detailed cause analysis unit <b>1701</b> visualizes the important portion in the changeable area and outputs the visualized important portion as the action result image corresponding to the target score X %.</p><p id="p-0232" num="0274">Note that, in the present embodiment, the difference image, the SSIM image, and the important feature map corresponding to the target score X % that are used by the detailed cause analysis unit <b>1701</b> to output the action result image corresponding to the target score X % have attributes as follows.<ul id="ul0029" list-style="none">    <li id="ul0029-0001" num="0000">    <ul id="ul0030" list-style="none">        <li id="ul0030-0001" num="0275">Difference image: difference information for each pixel, which is information having positive and negative values indicating how much the pixel is supposed to be corrected in order to raise the classification probability of the located label from the erroneous recognition state.</li>        <li id="ul0030-0002" num="0276">SSIM image: difference information that takes into account the shift statuses of the entire image and local areas, which is information having less artifacts (unintended noise) than the difference information for each pixel. For example, this is more accurate difference information (however, is information with only positive values).</li>        <li id="ul0030-0003" num="0277">Important feature map corresponding to the target score X %: a map that visualizes a feature portion of the correct answer label that affects the image recognition process.</li>    </ul>    </li></ul></p><p id="p-0233" num="0278">&#x3c;Specific Example of Processing of Detailed Cause Analysis Unit&#x3e;</p><p id="p-0234" num="0279">Next, a specific example of processing of the detailed cause analysis unit <b>1701</b> will be described. <figref idref="DRAWINGS">FIG. <b>22</b></figref> is a second diagram illustrating a specific example of processing of the detailed cause analysis unit. Note that the differences from the specific example of processing of the detailed cause analysis unit <b>1701</b> in <figref idref="DRAWINGS">FIG. <b>19</b></figref> is that the important feature map generation unit <b>2101</b> performs an important feature map generation process based on image recognition unit structural information (I) corresponding to the target score X % to generate the important feature map. In addition, a cutout unit <b>1803</b> cuts out the image portion for the changeable area corresponding to the target score X % from the important feature map corresponding to the target score X % and outputs a cutout image (J). Furthermore, the cutout unit <b>1803</b> multiplies a cutout image (C), a cutout image (D), and the cutout image (J) to generate a multiplied image (G).</p><p id="p-0235" num="0280">&#x3c;Flow of Detailed Cause Analysis Process&#x3e;</p><p id="p-0236" num="0281">Next, the flow of a detailed cause analysis process by the detailed cause analysis unit <b>1701</b> will be described. <figref idref="DRAWINGS">FIG. <b>23</b></figref> is a second flowchart illustrating a flow of the detailed cause analysis process. The differences from the flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>20</b></figref> are steps S<b>2301</b>, S<b>2302</b>, and S<b>2303</b>.</p><p id="p-0237" num="0282">In step S<b>2301</b>, the important feature map generation unit <b>2101</b> acquires, from the image recognition unit <b>403</b>, the image recognition unit structural information corresponding to the target score X % when the image recognition process was performed with the refined image with the target score X % as an input. In addition, the important feature map generation unit <b>2101</b> generates the important feature map corresponding to the target score X %, based on the image recognition unit structural information corresponding to the target score X % by using the selective BP method.</p><p id="p-0238" num="0283">In step S<b>2302</b>, the cutout unit <b>1803</b> cuts out the image portion for the changeable area corresponding to the target score X % from the important feature map corresponding to the target score X %.</p><p id="p-0239" num="0284">In step S<b>2303</b>, the cutout unit <b>1803</b> multiplies the difference image, the SSIM image, and the important feature map corresponding to the target score X %, which have been obtained by cutting out the image portions for the changeable area corresponding to the target score X %, and generate the multiplied image.</p><p id="p-0240" num="0285">As is clear from the above description, the analysis device <b>100</b> according to the fifth embodiment generates the difference images, the SSIM images, and the important feature maps corresponding to each level of recognition accuracy, based on<ul id="ul0031" list-style="none">    <li id="ul0031-0001" num="0000">    <ul id="ul0032" list-style="none">        <li id="ul0032-0001" num="0286">the erroneous recognition image,</li>        <li id="ul0032-0002" num="0287">the refined images with each level of recognition accuracy, and</li>        <li id="ul0032-0003" num="0288">the image recognition unit structural information corresponding to each level of recognition accuracy, and</li>    </ul>    </li></ul></p><p id="p-0241" num="0289">outputs the important portions by cutting out and multiplying the changeable areas corresponding to each level of recognition accuracy.</p><p id="p-0242" num="0290">As described above, in the analysis device according to the fifth embodiment, by outputting the important portion in the changeable area in pixel units, the degree of influence of each image part that causes erroneous recognition may be visualized in pixel units.</p><heading id="h-0012" level="1">Sixth Embodiment</heading><p id="p-0243" num="0291">In a sixth embodiment, an embodiment in which the degree of influence of each image part that causes erroneous recognition is visualized in pixel units, using difference images generated based on the erroneous recognition image and the refined images with each level of recognition accuracy (an embodiment different from the above fourth embodiment) will be described. The sixth embodiment will be described below focusing on differences from the fourth embodiment described above.</p><p id="p-0244" num="0292">&#x3c;Functional Configuration of Detailed Cause Analysis Unit&#x3e;</p><p id="p-0245" num="0293">First, a functional configuration of a detailed cause analysis unit in an analysis device <b>100</b> according to the sixth embodiment will be described. <figref idref="DRAWINGS">FIG. <b>24</b></figref> is a third diagram illustrating an example of the functional configuration of the detailed cause analysis unit. The difference from the functional configuration of the detailed cause analysis unit <b>1701</b> illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref> is that, in the case of <figref idref="DRAWINGS">FIG. <b>24</b></figref>, the SSIM calculation unit <b>1802</b> is not included.</p><p id="p-0246" num="0294">In the present embodiment, a detailed cause analysis unit <b>1701</b> visualizes the important portion in the changeable area, using the difference image generated based on<ul id="ul0033" list-style="none">    <li id="ul0033-0001" num="0000">    <ul id="ul0034" list-style="none">        <li id="ul0034-0001" num="0295">the erroneous recognition image, and</li>        <li id="ul0034-0002" num="0296">the refined image with the target score X %,</li>    </ul>    </li></ul></p><p id="p-0247" num="0297">and outputs the visualized important portion as the action result image corresponding to the target score X %.</p><p id="p-0248" num="0298">Note that, in the present embodiment, the difference image used by the detailed cause analysis unit <b>1701</b> to output the action result image corresponding to the target score X % has attributes as follows.<ul id="ul0035" list-style="none">    <li id="ul0035-0001" num="0000">    <ul id="ul0036" list-style="none">        <li id="ul0036-0001" num="0299">Difference image: difference information for each pixel, which is information having positive and negative values indicating how much the pixel is supposed to be corrected in order to raise the classification probability of the located label from the erroneous recognition state.</li>    </ul>    </li></ul></p><p id="p-0249" num="0300">&#x3c;Specific Example of Processing of Detailed Cause Analysis Unit&#x3e;</p><p id="p-0250" num="0301">Next, a specific example of processing of the detailed cause analysis unit <b>1701</b> will be described. <figref idref="DRAWINGS">FIG. <b>25</b></figref> is a third diagram illustrating a specific example of processing of the detailed cause analysis unit. Note that the differences from the specific example of processing of the detailed cause analysis unit <b>1701</b> in <figref idref="DRAWINGS">FIG. <b>19</b></figref> are that there is no description regarding the cutout image (D) cut out by the SSIM calculation unit <b>1802</b> and there is no description regarding the multiplication process with the cutout image (C).</p><p id="p-0251" num="0302">&#x3c;Flow of Detailed Cause Analysis Process&#x3e;</p><p id="p-0252" num="0303">Next, the flow of a detailed cause analysis process by the detailed cause analysis unit <b>1701</b> will be described. <figref idref="DRAWINGS">FIG. <b>26</b></figref> is a third flowchart illustrating a flow of the detailed cause analysis process. The differences from the flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>20</b></figref> are that the respective processes in steps S<b>2002</b>, S<b>2004</b>, and S<b>2005</b> are not provided, and the process in step S<b>2401</b> is executed instead of step S<b>2006</b>.</p><p id="p-0253" num="0304">As illustrated in <figref idref="DRAWINGS">FIG. <b>26</b></figref>, in step S<b>2001</b>, an image difference calculation unit <b>1801</b> calculates the difference image between the erroneous recognition image and the refined image with the target score X %.</p><p id="p-0254" num="0305">In step S<b>2003</b>, a cutout unit <b>1803</b> cuts out the changeable area corresponding to the target score X % from the difference image.</p><p id="p-0255" num="0306">In step S<b>2401</b>, the cutout unit <b>1803</b> performs an enhancement process on the cut-out difference image. In addition, an action unit <b>1804</b> subtracts the difference image that has undergone the enhancement process, from the erroneous recognition image, and outputs the action result image corresponding to the target score X %.</p><p id="p-0256" num="0307">As is clear from the above description, the analysis device <b>100</b> according to the sixth embodiment generates the difference images based on the erroneous recognition image and the refined images with each level of recognition accuracy and outputs the important portions by cutting out and enhancing the changeable areas corresponding to each level of recognition accuracy.</p><p id="p-0257" num="0308">As described above, in the analysis device according to the sixth embodiment, by outputting the important portion in the changeable area in pixel units, the degree of influence of each image part that causes erroneous recognition may be visualized in pixel units.</p><heading id="h-0013" level="1">OTHER EMBODIMENTS</heading><p id="p-0258" num="0309">In each of the above embodiments, a case where the refined image generation unit <b>142</b>, the map generation unit <b>143</b>, and the specifying unit <b>1001</b> perform processing using the erroneous recognition image has been described. However, the refined image generation unit <b>142</b>, the map generation unit <b>143</b>, and the specifying unit <b>1001</b> may perform processing using the refined image generated by the image refiner initialization unit <b>141</b> executing the first learning process, instead of the erroneous recognition image.</p><p id="p-0259" num="0310">In addition, in each of the above embodiments, the recognition accuracy has been described as a score, but recognition accuracy other than the score may be used. For example, the recognition accuracy other than the score mentioned here includes the position and dimensions, existence probability, intersection over union (IoU), segment, other information regarding the output of deep learning, and the like.</p><p id="p-0260" num="0311">Furthermore, in each of the above embodiments, a case where one object is included in the erroneous recognition image has been described, but a plurality of objects may be included. In this case, the erroneous recognition cause information may be output for each object, or the erroneous recognition cause information including a plurality of objects may be output.</p><p id="p-0261" num="0312">In addition, in each of the above embodiments, it has been described that the first learning process is executed such that the erroneous recognition image in the same state as the input erroneous recognition image is generated. However, the method for the first learning process is not limited to this.</p><p id="p-0262" num="0313">The purpose of executing the first learning process on the image refiner unit <b>301</b> is to learn model parameters to a predefined initial state instead of an unknown initial state before performing the second learning process. Accordingly, in the first learning process, apart from the method of updating the model parameters such that the erroneous recognition image in the same state as the input erroneous recognition image is generated, a predetermined targeted score may be predefined to perform initialization such that an image that outputs the score is generated.</p><p id="p-0263" num="0314">In this case, the score of the first learning process does not necessarily have to be a score lower than the score when the image recognition process is executed on the refined image generated by executing the second learning process. For example, the first learning process may be executed on the image refiner unit <b>301</b> such that an image that gives the score=100% is generated, and the refined images that give the scores=90%, 80%, and 70% may be generated in the second learning process. Alternatively, the first and second learning processes may be executed in accordance with other fluctuation patterns of the score.</p><p id="p-0264" num="0315">In addition, the coefficient for performing the enhancement process in the above fourth to sixth embodiments may be selected so as to adjust the action result image or the strength of the action on the refined image. For example, when it is difficult to distinguish the magnitude of the pixel value indicating the cause of erroneous recognition, the coefficient may be selected so as to promote the enhancement. Alternatively, the coefficient may be selected such that the scale of the pixel value changed by the action of multiplication is optimally adjusted, or the coefficient may be selected so as not to perform the enhancement process.</p><p id="p-0265" num="0316">In addition, in the first learning process of learning such that the recognition accuracy of the image generated by the generative model matches the desired recognition accuracy, the output of the hidden layer of deep learning may be used together with the information regarding the output of deep learning mentioned above or the like (or may be used alone).</p><p id="p-0266" num="0317">For example, when a feature map is also used together as the output of the hidden layer, the first learning process may be executed such that the information regarding the output of deep learning (image recognition unit) to be analyzed and the information regarding the output of the hidden layer of deep learning (image recognition unit) to be analyzed</p><p id="p-0267" num="0318">have the same state<ul id="ul0037" list-style="none">    <li id="ul0037-0001" num="0000">    <ul id="ul0038" list-style="none">        <li id="ul0038-0001" num="0319">when the input erroneous recognition image is processed, and</li>        <li id="ul0038-0002" num="0320">when the image generated by the first learning process is processed.</li>    </ul>    </li></ul></p><p id="p-0268" num="0321">When the information regarding the output of the hidden layer of deep learning (image recognition unit) to be analyzed is evaluated, for example,</p><p id="p-0269" num="0322">evaluation may be made by executing some processing for evaluating whether the same state is achieved, such as<ul id="ul0039" list-style="none">    <li id="ul0039-0001" num="0000">    <ul id="ul0040" list-style="none">        <li id="ul0040-0001" num="0323">L1/L2/SSIM,</li>        <li id="ul0040-0002" num="0324">Neural Style Transfer loss, or</li>        <li id="ul0040-0003" num="0325">Max Pooling or Average Pooling.</li>    </ul>    </li></ul></p><p id="p-0270" num="0326">Note that the embodiments are not limited to the configurations described here and may include, for example, combinations of the configurations or the like described in the above embodiments with other elements. These points may be changed without departing from the spirit of the embodiments and may be appropriately assigned according to application modes thereof.</p><p id="p-0271" num="0327">All examples and conditional language provided herein are intended for the pedagogical purposes of aiding the reader in understanding the invention and the concepts contributed by the inventor to further the art, and are not to be construed as limitations to such specifically recited examples and conditions, nor does the organization of such examples in the specification relate to a showing of the superiority and inferiority of the invention. Although one or more embodiments of the present invention have been described in detail, it should be understood that the various changes, substitutions, and alterations could be made hereto without departing from the spirit and scope of the invention.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An analysis device comprising:<claim-text>a memory; and</claim-text><claim-text>a processor coupled to the memory and configured to:</claim-text><claim-text>execute a first learning process on a generative model for images such that the images that bring a recognition result of an image recognition process into a preassigned state are generated;</claim-text><claim-text>execute a second learning process on the generative model on which the first learning process has been executed, while gradually changing recognition accuracy of the images generated by the generative model on which the first learning process has been executed, to desired recognition accuracy;</claim-text><claim-text>acquire each piece of information on back-error propagation calculated by executing the image recognition process, for the images with each level of the recognition accuracy generated through a course of the second learning process; and</claim-text><claim-text>generate evaluation information that indicates each of image parts that cause erroneous recognition at each level of the recognition accuracy, based on the acquired each piece of the information on the back-error propagation.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The analysis device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor:<claim-text>executes the first learning process on the generative model for the images such that the images in a same state as input images are generated, and</claim-text><claim-text>executes the second learning process on the generative model on which the first learning process has been executed, while gradually raising the recognition accuracy of the images generated by the generative model on which the first learning process has been executed, to the desired recognition accuracy.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The analysis device according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the processor:<claim-text>separately generates important feature maps that visualize feature portions that reacted during the image recognition process, based on the acquired each piece of the information on the back-error propagation;</claim-text><claim-text>generates a plurality of difference maps by calculating differences between the separately generated important feature maps; and</claim-text><claim-text>among the separately generated important feature maps, generates a predetermined important feature map and each of added important feature maps obtained by sequentially adding the plurality of difference maps to the predetermined important feature map, as the evaluation information.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The analysis device according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the processor<claim-text>generates an important feature index map in which a deterioration scale map obtained by calculating the differences between the input images or the images generated by executing the first learning process, and the images that are generated by executing the second learning process and have the desired recognition accuracy is superimposed on the predetermined important feature map, and each of added important feature index maps obtained by sequentially adding the plurality of difference maps to the important feature index map, as the evaluation information.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The analysis device according to <claim-ref idref="CLM-00004">claim 4</claim-ref>,<claim-text>wherein the processor:</claim-text><claim-text>divides the input images or the images generated by executing the first learning process for each of superpixels; and</claim-text><claim-text>adds a value of each pixel of the important feature index map for each of the superpixels, and generates areas indicated by combinations of the superpixels whose additional values are equal to or higher than a predetermined threshold value, as the evaluation information.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The analysis device according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the processor<claim-text>composites the input images or the images generated by executing the first learning process, and the images generated by executing the second learning process, based on the combinations of the superpixels whose additional values are equal to or higher than the predetermined threshold value, and specifies the combinations of the superpixels, based on a result of the image recognition process executed on composite images.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The analysis device according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the processor<claim-text>calculates the differences in pixel units between the input images or the images generated by executing the first learning process, and the images generated by executing the second learning process, which are the images included in the areas indicated by the specified combinations of the superpixels, and generates the images obtained from the calculated differences in pixel units, as the evaluation information.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A non-transitory computer-readable recording medium storing an analysis program causing a computer a processing of:<claim-text>executing a first learning process on a generative model for images such that the images that bring a recognition result of an image recognition process into a preassigned state are generated;</claim-text><claim-text>executing a second learning process on the generative model on which the first learning process has been executed, while gradually changing recognition accuracy of the images generated by the generative model on which the first learning process has been executed, to desired recognition accuracy; and</claim-text><claim-text>acquiring each piece of information on back-error propagation calculated by executing the image recognition process, for the images with each level of the recognition accuracy generated through a course of the second learning process; and</claim-text><claim-text>generating evaluation information that indicates each of image parts that cause erroneous recognition at each level of the recognition accuracy, based on the acquired each piece of the information on the back-error propagation.</claim-text></claim-text></claim></claims></us-patent-application>