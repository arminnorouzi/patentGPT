<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004588A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004588</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364919</doc-number><date>20210701</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>33</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>284</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>3334</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>284</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>025</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>0454</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Resource-Efficient Sequence Generation with Dual-Level Contrastive Learning</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Microsoft Technology Licensing, LLC</orgname><address><city>Redmond</city><state>WA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>JIAO</last-name><first-name>Jian</first-name><address><city>Bellevue</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>GONG</last-name><first-name>Yeyun</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>DUAN</last-name><first-name>Nan</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>ZHANG</last-name><first-name>Ruofei</first-name><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Microsoft Technology Licensing, LLC</orgname><role>02</role><address><city>Redmond</city><state>WA</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A training system produces a resource-efficient machine-trained model via a training architecture that employs plural processing paths. Some of the processing paths incorporate the use of auxiliary information that imparts external knowledge about source items being processed. The training architecture also employs contrastive learning that operates at different respective levels within the training architecture. For instance, the training architecture uses encoder-level contrastive learning to compare output information generated by different encoders within the training architecture. The training architecture uses decoder-level contrastive learning to compare output information produced by different decoders within the training architecture. An inference-stage system performs an application task using the model produced by the training system.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="219.96mm" wi="158.75mm" file="US20230004588A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="236.14mm" wi="167.39mm" file="US20230004588A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="231.48mm" wi="147.83mm" file="US20230004588A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="244.26mm" wi="165.02mm" file="US20230004588A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="191.77mm" wi="144.86mm" file="US20230004588A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="201.42mm" wi="158.58mm" file="US20230004588A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="230.97mm" wi="162.64mm" file="US20230004588A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="234.78mm" wi="141.14mm" file="US20230004588A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="234.70mm" wi="164.00mm" file="US20230004588A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="168.23mm" wi="164.00mm" file="US20230004588A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="227.58mm" wi="164.85mm" file="US20230004588A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="229.28mm" wi="163.83mm" file="US20230004588A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">Developers continually strive to increase the accuracy and versatility of natural language processing (NLP) models. The developers may attempt to accomplish these goals by increasing the complexity of the NLP models. While these enhanced models may exhibit improved accuracy, they may also impose inference-stage costs that limit their effectiveness. For instance, an improved NLP model may consume a significant amount of system resources (e.g., processor resources, memory resources, etc.). This factor may make an application that uses the improved NLP model unsuitable for implementation on a resource-constrained computing platform. An improved NLP model may also increase the amount of time it takes for an application to produce its output results. This factor may make an application that uses the improved NLP model unsuitable for use in an application that demands real-time responses to user inputs.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0003" num="0002">A training system is described herein for producing a resource-efficient machine-trained model. In some non-limiting implementations, the technology uses a training architecture that includes plural training paths that operate on plural respective instances of input information. Some of the training paths accept input information that incorporates external knowledge about sources items being processed, beyond that imparted by the source items themselves. Further, the training system uses contrastive learning at different levels within the training architecture. For instance, the training architecture uses encoder-level contrastive learning to compare output information generated by different encoders within the training architecture. The training architecture uses decoder-level contrastive learning to compare output information produced by different decoders within the training architecture.</p><p id="p-0004" num="0003">According to some non-limiting implementations, the training system performs its constative learning based on information stored in buffer memory. The information in buffer memory is updated on a first-in-first-out (FIFO) basis.</p><p id="p-0005" num="0004">According to some non-limiting implementations, the training system uses back-projection to update training weights in some parts of training architecture. It updates the training weights in other parts of the training architecture in an indirect manner, based on the training weights that have been computed by back-projection.</p><p id="p-0006" num="0005">Also described herein is an inference-stage system that employs the model produced by the training system. The inference-stage system generates a sequence using auto-regression, given an input source item.</p><p id="p-0007" num="0006">The training system described above produces a machine-trained model that exhibits high accuracy relative to some competing models. The model achieves high accuracy, in part, based on the training system's incorporation of external knowledge in the training process, and through the training system's use of encoder-level contrastive learning and decoder-level contrastive learning. At the same time, the model produced by the training system makes efficient use of computer resources and exhibits good latency-related performance. These factors enable developers to use the model in resource-constrained computing platforms, and in applications that demand low-latency responses (such applications that demand real-time responses to user inputs).</p><p id="p-0008" num="0007">According to another technical benefit, the training system's use of FIFO buffer memory and its selective use of back-projection allows it to produce the machine-trained model in a resource-efficient manner. The use of buffer memory also allows the training system to increase the amount of data that is used to perform contrastive learning, which positively contributes to the accuracy of the resultant model produced by the training system.</p><p id="p-0009" num="0008">The above-summarized elements can be manifested in various types of systems, devices, components, methods, computer-readable storage media, data structures, graphical user interface presentations, articles of manufacture, and so on.</p><p id="p-0010" num="0009">This Summary is provided to introduce a selection of concepts in a simplified form; these concepts are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an illustrative training system for producing a machine-trained model.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows an illustrative example-generation system for producing training examples to be processed by the training system of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows one implementation of a training architecture used by the training system of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows buffer memories used by the training system of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in generating encoder-level loss information.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows buffer memories used by the training system of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in generating decoder-level loss information.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a first-in-first-out manner of operation of the buffer memories of <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>5</b></figref>.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an inference-level system that uses a machine-trained model produced by the training system of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows an illustrative transformer that can be used to implement the inference-level system of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, and different parts of the training system of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The transformer includes an encoder and a decoder.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows an illustrative encoder block for use in the encoder of <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows an illustrative decoder block for use in the decoder of <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart that shows an overview of one manner of operation of the training system of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart that expands on the implementation of one of the processing blocks of <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart that shows an overview of one manner of operation of the inference-stage system of <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>14</b></figref> shows computing equipment that can be used to implement the training system of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and the inference-stage system of <figref idref="DRAWINGS">FIG. <b>7</b></figref></p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>15</b></figref> shows an illustrative type of computing system that can be used to implement any aspect of the features shown in the foregoing drawings.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0026" num="0025">The same numbers are used throughout the disclosure and figures to reference like components and features. Series 100 numbers refer to features originally found in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, series 200 numbers refer to features originally found in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, series 300 numbers refer to features originally found in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, and so on.</p><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0027" num="0026">This disclosure is organized as follows. Section A describes a training system for producing a machine-trained model, and an inference-stage system for applying the machine-trained model produced by the training system. Section B sets forth illustrative methods that explain the operation of the systems of Section A. And Section C describes illustrative computing functionality that can be used to implement any aspect of the features described in Sections A and B.</p><heading id="h-0005" level="1">A. Illustrative Computing Systems</heading><heading id="h-0006" level="1">A.1. Training System</heading><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an illustrative training system <b>102</b> for producing a machine-trained model <b>104</b>. The machine-trained model <b>104</b> is configured to process an instance of linguistic input information expressed as a series of input tokens, where the tokens may correspond to words, n-grams, or other linguistic units. As will be explained in greater detail below, the machine-trained model <b>104</b> is configured to use auto-regression to successively convert the input information into a sequence of linguistic tokens. For example, the input information may specify a plurality terms that convey a plurality of concepts. The machine-trained model <b>104</b> can convert the input information into a coherent input sentence that incorporates the plural terms. In other cases, the input information expresses a query submitted by a user. The machine-trained model <b>104</b> can convert the query into a set of key terms.</p><p id="p-0029" num="0028">This subsection will explain the training system <b>102</b> used to produce the machine-trained model <b>104</b>. The next subsection (A.2) will set forth an inference-stage system that applies the machine-trained model <b>104</b> produced by the training system <b>102</b> to perform an application task.</p><p id="p-0030" num="0029">With reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the training system <b>102</b> operates on training examples in a data store <b>106</b> on a batch-by-batch basis. Each batch has n training examples (where n is an environment-specific number chosen by the model developer). In the training process, the training system <b>102</b> updates training weights after processing each batch. In some implementations, the training system <b>102</b> begins its training based on a pre-trained language model <b>108</b> produced by a preliminary training process (not shown). The training system <b>102</b> then successively refines the training weights of the pre-trained language model <b>108</b>. In other cases, the training system <b>102</b> can produce the machine-trained model <b>104</b> from &#x201c;scratch,&#x201d; that is, without the use of the pre-trained model <b>108</b>.</p><p id="p-0031" num="0030">In some implementations, the training system <b>102</b> uses back-propagation in combination with stochastic gradient descent to update its training weights. In some implementations, the training system <b>102</b> uses momentum learning to update training weights in a resource-efficient manner. Additional details regarding the use of momentum learning are set forth below.</p><p id="p-0032" num="0031">An example-generation system <b>110</b> generates the training examples in the data store <b>106</b>. <figref idref="DRAWINGS">FIG. <b>2</b></figref> shows one implementation of the example-generation system <b>110</b>, and will be described in greater detail below. Suffice it to say here that any given training example includes plural data items, such as: a) a source item; b) a target item; and c) two or more auxiliary items. The source item pertains to an instance of linguistic input information, such as a set of terms associated with a plurality of respective concepts. The target item is a stipulated transformation of the source item that is deemed correct (for the case of a positive training example) or incorrect (for the case of a negative training example). For instance, the target item for a positive example may correspond to a sentence that combines the concepts conveyed by the source item in a manner that is deemed valid. An auxiliary item is information that imparts external knowledge about the source item. The knowledge is considered &#x201c;external&#x201d; and &#x201c;supplemental&#x201d; because it is obtained from a source (or sources) other than the source item itself. For example, consider a source item that sets forth a set of concept-related terms. An auxiliary item for that source item may originate from a knowledge base that provides information regarding the concepts and/or which describes the connections among the concepts, etc.</p><p id="p-0033" num="0032">The training system <b>102</b> uses a training architecture <b>112</b> that includes plural processing paths. This description will use the qualifiers &#x201c;first,&#x201d; &#x201c;second&#x201d; and &#x201c;third&#x201d; to describe these paths and the components used in the respective paths. However, note that these qualifiers are arbitrary designations intended to facilitate explanation by unambiguously identifying elements, and do not imply an ordering of parts or processing steps.</p><p id="p-0034" num="0033">The training architecture <b>112</b> will be explained below with reference to the processing a single training example i that includes a source item S<sub>i</sub>, a corresponding target item T<sub>i</sub>, a first auxiliary item A<sub>i</sub>, and a second auxiliary item However, as will be clarified below, the training architecture <b>112</b> can alternatively process a batch of several training examples at the same time. The training system <b>102</b> can use a processing architecture that includes plural graphic processing units (GPUs) (not shown) and/or plural neural processing units (NPUs) operating in parallel to process the training examples in the batch.</p><p id="p-0035" num="0034">A first processing path <b>114</b> includes a first-path encoder <b>116</b> and a first-path decoder <b>118</b>. When training is finished, the fully trained first-path encoder <b>116</b> and first-path decoder <b>118</b> serve as the machine-trained model <b>104</b>. <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates this point by showing an arrow connecting the first processing path <b>114</b> to the machine-trained model <b>104</b>.</p><p id="p-0036" num="0035">In operation, the first-path encoder <b>116</b> converts a first instance of input information <b>120</b> into first-path encoder output information <b>122</b>. The first-path decoder <b>118</b> converts the first-path encoder output information <b>122</b> into first-path decoder output information <b>124</b>. The first instance of input information <b>120</b> includes the source item S<sub>i </sub>combined (e.g., concatenated) with the first auxiliary item A<sub>i</sub>. The first-path decoder output information <b>124</b> includes a generated sequence G<sub>i </sub>that is the counterpart of the target item T<sub>i </sub>in the training example. If the first processing path <b>114</b> produces an accurate result, G<sub>i </sub>will be close to T<sub>i </sub>in vector space, when both are mapped into the same vector space.</p><p id="p-0037" num="0036">A second processing path <b>126</b> uses a second-path encoder <b>128</b> to map a second instance of input information <b>130</b> into second-path encoder output information <b>132</b>. The second instance of input information <b>130</b> includes the source item S<sub>i </sub>combined (e.g., concatenated) with the target item T<sub>i</sub>.</p><p id="p-0038" num="0037">A third processing path <b>134</b> uses a third-path encoder <b>136</b> to map a third instance of input information <b>138</b> into third-path encoder output information (not labeled in <figref idref="DRAWINGS">FIG. <b>1</b></figref>). It then uses a third-path decoder <b>140</b> to map the third-path encoder output information into third-path decoder output information <b>142</b>, which includes another generated sequence denoted by G<sub>i</sub>&#x2032;. The third instance of input information <b>138</b> includes the source item S<sub>i </sub>combined with the second auxiliary item A<sub>i</sub>&#x2032;. Thus, the third processing path <b>134</b> can be viewed as the same as the first processing path <b>114</b>, with the exception that these two processing paths (<b>114</b>, <b>134</b>) operate on different respective auxiliary items (A<sub>i</sub>, A<sub>i</sub>&#x2032;).</p><p id="p-0039" num="0038">An auto-regressive loss (ARL) computer <b>144</b> compares the first-path decoder output information <b>124</b> with the target item T<sub>i</sub>, to generate a first measure of loss L<sub>AR</sub>. An encoder-level contrastive learning (ECL) computer <b>146</b> compares the first-path encoder output information <b>122</b> with the second-path encoder output information <b>132</b> to generate a second loss measure L<sub>EC</sub>. A decoder-level contrastive learning (DCL) computer <b>148</b> compares the first-path decoder output information <b>124</b> with the third-path decoder output information <b>142</b> to generate a third loss measure L<sub>DC</sub>.</p><p id="p-0040" num="0039">The training system <b>102</b> updates the training weights of the training architecture <b>112</b> based on the combination of the above-described three loss measures (L<sub>AR</sub>, L<sub>EC</sub>, L<sub>DC</sub>) More specifically, the training system <b>102</b> can use momentum contrastive learning to update the training weights used in the first training path <b>114</b> based on the loss information computed as described above, e.g., by propagating gradients associated with the loss information through the first processing path <b>114</b> via back-propagation. The arrows (<b>150</b>, <b>152</b>) denote these back-propagation operations. In contrast, the training system <b>102</b> can update the training weights in the second processing path <b>126</b> and the third processing path <b>134</b> as a mathematical function of the updated weights that have already been computed for the first processing path <b>114</b>, without the separate use of back-propagation. This manner of operation simplifies the weight-updating operation, and correspondingly reduces the training system's consumption of computer resources. Additional detail regarding this updating operation will be set forth below when describing <figref idref="DRAWINGS">FIGS. <b>3</b>-<b>6</b></figref>, particularly with respect to the manner in which the ECL computer <b>146</b> and the DCL computer <b>148</b> leverage respective buffer memories (<b>154</b>, <b>156</b>).</p><p id="p-0041" num="0040">More generally stated, contrastive learning enables the training system <b>102</b> to produce a machine-trained model <b>104</b> of high accuracy by promoting the similarity between items that compose a positive pairing of two items (q, k)<sup>+</sup> while simultaneously confirming the dissimilarity between each of a plurality of negative training examples (q, k)<sup>&#x2212;</sup>. A positive pairing describes a case in which the two items (q and k) are related to each other, and a negative pairing describes a case in which the two items (q and k) are not related to each other. The symbol q (a particular &#x201c;query&#x201d;) and the symbol k (a particular &#x201c;key&#x201d;) are used herein as shorthand generic designators to refer to any two items being compared. For instance, in one application, the query q may represent an expression of the first-path encoder output information <b>122</b> and the key k may represent an expression of the second-path encoder output information <b>132</b>. To repeat, additional detail regarding how the training system <b>102</b> performs contrastive learning will be set forth below.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows one implementation of the example-generation system <b>110</b> introduced in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. As stated, this system <b>110</b> generates the training examples that the training system <b>102</b> uses to train the machine-trained model <b>104</b>. The example-generation system <b>110</b> can operate in different ways in different respective environments, based, in part, on what data sources are available for use by a model developer. In some implementations, the developer already has a corpus of pre-generated pairs of data items, each of which includes a particular source item and a corresponding target item. Here, the role of the example-generation system <b>110</b> is to identify one or more auxiliary items for each source item. In other implementations, the developer does not have access to pre-generated pairs of source items and target items. Here, the example-generation system <b>110</b> may perform the additional preliminary task of identifying pairs of source items and respective target items. To facilitate explanation, the following explanation will assume that the former scenario applies, and, accordingly, the principal role that the example-generation system <b>110</b> performs is to identify auxiliary items, given respective source items.</p><p id="p-0043" num="0042">The example-generation system <b>110</b> can use a retrieval engine <b>202</b> to retrieve auxiliary items from one or more data sources <b>204</b>, given specified source items. One or more systems <b>206</b> can generate or otherwise supply the items in the data sources <b>204</b>. The retrieval engine <b>202</b> can operate using different environment-specific retrieval strategies. In some implementations, assume that one or more data sources <b>204</b> explicitly associate source items with respective auxiliary items. Here, the retrieval engine <b>202</b> uses each source item as a lookup key to interrogate the data source(s). The retrieval engine then retrieves whatever knowledge is imparted by the data source(s) for the specified source item. Data sources of this type can be structured as dictionaries, digests, lookup tables, knowledge graphs, etc.</p><p id="p-0044" num="0043">In addition, or alternatively, the retrieval engine <b>202</b> can perform a feature-based search to find a matching auxiliary item, given a specified source item. For instance, the retrieval engine <b>202</b> can identify the features of a given source item. The retrieval engine <b>202</b> can then consult a search index (not shown) to find another information item having matching features. The features used to perform this matching can include any combination of lexical features, TD-IDF features, semantic vectors, etc.</p><p id="p-0045" num="0044">For example, given a set of concepts associated with a candidate source item, the retrieval engine <b>202</b> can consult a search index to identify a Wikipedia article that matches those concepts. The retrieval engine <b>202</b> can then select one or more sentences from this article (and/or the title of this article) as appropriate auxiliary items with respect to the specified source item. In some implementations, the retrieval engine <b>202</b> can perform this search by matching keywords in the source item with the same keywords in the Wikipedia article. Alternatively, or in addition, the retrieval engine <b>202</b> can perform this search by mapping the source item into a source vector, and then finding the Wikipedia article having a corresponding article vector that is within a prescribed distance to the source vector in a shared vector space. The distance between vectors can be measured by cosine similarity or any other distance metric.</p><p id="p-0046" num="0045">In addition, or alternatively, the retrieval engine <b>202</b> can provide an interface that allows one or more users to manually specify auxiliary items for specified source items. For example, the retrieval engine <b>202</b> can provide a crowdsourcing platform that allows users to specify auxiliary items, given respective source items.</p><p id="p-0047" num="0046">The above-described implementations of the retrieval engine <b>202</b> are set forth here in the spirit of illustration, not limitation; other environments can use other implementations of the retrieval engine <b>202</b>. In any event, the retrieval engine <b>202</b> produces initial example information <b>208</b>.</p><p id="p-0048" num="0047">A filter <b>210</b> removes training examples that fail to satisfy a prescribed quality metric. For example, the filter <b>210</b> can use a machine-trained classification model to generate a score for a given pair that includes a candidate source item and a candidate auxiliary item. The score reflects an extent to which the candidate auxiliary item provides supplemental information that is relevant to candidate source item. The filter <b>210</b> can remove any pairing of items having a score below an environment-specific threshold value. Without limitation, the classification model can be implemented as any of a logistic regression model, a transformer-based model, a decision tree model, and so on.</p><p id="p-0049" num="0048">The example-generation system <b>110</b> provides a plurality of source items (in a data store <b>212</b>) (which may optionally be given), a plurality of target items (in a data store <b>214</b>) (which may optionally be given), and a plurality of auxiliary items (in a data store <b>216</b>). The example-generation system <b>110</b> can include information <b>218</b> that describes the links between matching source items and respective target items (which may optionally be given), and information <b>220</b> that describes the links between matching source items and respective auxiliary items. This information collectively composes a plurality of training examples <b>222</b> for processing by the training system <b>102</b>.</p><p id="p-0050" num="0049">As noted above, in other implementations, the example-generation system <b>110</b> can perform a more expansive role in generating the training examples <b>222</b>. For example, the example-generation system <b>110</b> can generate the target items, given respective source items. The example-generation system <b>110</b> can perform this task using one more other machine-trained models. For example, the example-generation system <b>110</b> can use a first machine-trained model to map images into respective sets of concepts (corresponding to candidate source items). The example-generation system <b>110</b> can use a second machine-trained model to map the same images into respective sentences (corresponding to respective target items). Or it can adopt whatever captions are already associated with the images. For a given image, the concept set and its sentence correspond to a pair composed of a source item S<sub>i </sub>and a target item T<sub>i</sub>. One example of technology for extracting linguistic information from images is set forth by Microsoft Corporation of Redmond, Wash., in FANG, et al., &#x201c;From Captions to Visual Concepts and Back,&#x201d; arXiv:1411.4952v3 [cs.CV], Apr. 14, 2015, 10 pages. Further, the example-generation system <b>110</b> can use the filter <b>210</b> to remove pairs of source items and target items that fail to satisfy prescribed quality metrics.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows one implementation of the training architecture <b>112</b> introduced before. Elements in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>3</b></figref> that have the same reference numbers correspond to the same parts. As will be described below in greater detail, the training architecture <b>112</b> generates some of its output information in successive fashion, e.g., by generating one token at a time. However, so as not to overburden the explanation at this stage, the iterative nature of the training architecture <b>112</b> will not be specifically emphasized in the description of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0052" num="0051">Referring first to the first processing path (PP) <b>114</b> in the middle of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the first instance of input information <b>120</b> incudes the source item S<sub>i </sub>and the first auxiliary item A<sub>i</sub>. The first instance of input information <b>120</b> optionally includes a special beginning-of-sequence &#x3c;BOS&#x3e; token that demarcates the beginning of the sequence of input information <b>120</b>, and a special end-of-sequence &#x3c;EOS&#x3e; token that demarcates the end of the input information <b>120</b>. The first-path encoder <b>116</b> uses a neural network (described below) to map the first instance of input information <b>120</b> into the first-path encoder output information <b>122</b>. The first-path encoder output information <b>122</b> includes hidden state information associated with each part of the first instance of input information <b>120</b>. For instance, the hidden state information &#x3c;EOS&#x3e;<sup>e </sup><b>302</b> denotes the hidden state counterpart of the &#x3c;EOS&#x3e; token in the first instance of input information <b>120</b> (where the &#x201c;e&#x201d; in this context represents encoded information). The first-path encoder <b>116</b> uses the same trained weights to produce each part of the first-path encoder output information <b>122</b>.</p><p id="p-0053" num="0052">The first-path decoder <b>118</b> uses another neural network (described below) to transform the first-path encoded output information <b>122</b> into the first-path decoder output information <b>124</b>. The first-path decoder output information <b>124</b>, in turn, includes the generated sequence G<sub>i </sub>that represents the transformed counterpart of the source item S<sub>i </sub>(and its encoded counterpart). It also includes information &#x3c;EOS&#x3e;<sup>d </sup><b>304</b> that denotes the transformed version of the &#x3c;EOS&#x3e; token in the first instance of input information <b>120</b> (and its encoded counterpart). The symbol &#x201c;d&#x201d; represents that this is information generated by a decoder. The first-path decoder <b>118</b> uses the same trained weights to produce all parts of the first-path decoder output information <b>124</b>. As such, the first-path decoder <b>118</b> uses the same trained weights to produce the information &#x3c;EOS&#x3e;<sup>d </sup><b>304</b> and the generated sequence G<sub>i</sub>.</p><p id="p-0054" num="0053">The second processing path <b>126</b> uses the second-path encoder <b>128</b> to map the second instance of input information <b>130</b> into the second-path output information <b>132</b>. The second-path output information <b>132</b>, in turn, includes information &#x3c;EOS&#x3e;<sup>e </sup><b>306</b> that represents the hidden state encoded counterpart of an &#x3c;EOS&#x3e; token in the second instance of input information <b>130</b>. As previously explained, the second instance of input information <b>130</b> includes the source item S<sub>i </sub>and its corresponding target item T<sub>i</sub>.</p><p id="p-0055" num="0054">The third processing path <b>134</b> uses the third-path encoder <b>136</b> and the third-path decoder <b>140</b> to map the third instance of input information <b>138</b> into the third-path decoder output information <b>142</b>. The third-path decoder output information <b>142</b>, in turn, incudes a generated sequence G<sub>i</sub>&#x2032;. It also includes information &#x3c;EOS&#x3e;<sup>d </sup><b>308</b> that denotes the hidden state decoded counterpart of an &#x3c;EOS&#x3e; token in the third instance of input information <b>138</b>. As previously explained, the third instance of input information <b>138</b> incudes the source item S<sub>i </sub>in combination with the second auxiliary item A<sub>i</sub>&#x2032;.</p><p id="p-0056" num="0055">The auto-regressive loss (ARL) computer <b>144</b> can generate a measure of the auto-regressive generation loss L<sub>AR </sub>over a set of training examples using any loss function, such as a cross-entropy loss function. The ARL computer <b>144</b> computes cross-entropy for a predicted token in the generated sequence G<sub>i </sub>(given the tokens that have already been predicted in the sequence G<sub>i</sub>) by taking the log of the predicted token's probability (which can be calculated using a softmax operation, also known as a normalized exponential function), and then forming the product of that log with whatever token is expected (as defined by the corresponding target item T<sub>i</sub>). The ARL sums the above measure over plural comparisons between generated and expected results.</p><p id="p-0057" num="0056">The encoder-level contrastive learning (ECL) computer <b>146</b> includes a first neural network (NN) <b>310</b> for mapping the information &#x3c;EOS&#x3e;<sup>e </sup><b>302</b> produced by the first-path encoder <b>116</b> into a vector z<sub>q</sub>. The ECL computer <b>146</b> includes a second neural network <b>312</b> for mapping the information &#x3c;EOS&#x3e;<sup>e </sup><b>306</b> produced by the second-path encoder <b>128</b> into another vector z<sub>k</sub>, in the same vector space as the vector z<sub>q</sub>. The neural networks (<b>310</b>, <b>312</b>) can be constructed using any number of layers, and can use any activation function(s). In some implementations, they may be implemented as multilevel perceptron networks (MLPs). A similarity computer <b>314</b> generates any type of measure of the distance between z<sub>q </sub>and z<sub>k</sub>. Overall, the ECL computer <b>146</b> generates the encoder-level contrastive loss measure L<sub>EC </sub>based on a summation of the above similarity measures computed over plural comparisons.</p><p id="p-0058" num="0057">Similarly, the decoder contrastive learning (DCL) computer <b>148</b> includes a first neural network <b>316</b> for mapping the information &#x3c;EOS&#x3e;<sup>d </sup><b>304</b> produced by the first-path decoder <b>118</b> into a vector z<sub>q</sub>. The DCL computer <b>148</b> includes a second neural network <b>318</b> for mapping the information &#x3c;EOS&#x3e;<sup>d </sup><b>308</b> produced by the third-path decoder <b>140</b> into a vector z<sub>k </sub>within the same vector space as the vector z<sub>q</sub>. The neural networks (<b>316</b>, <b>318</b>) can be constructed using any number of layers, and can use any activation function(s). In some implementations, they may be implemented as multilevel perceptron networks (MLPs). A similarity computer <b>320</b> generates any type of measure of the distance between z<sub>q </sub>and z<sub>k</sub>. Overall, the DLC computer <b>148</b> generates the decoder-level constative loss measure L<sub>DC </sub>based on a summation of the above similarity measures computer over plural comparisons.</p><p id="p-0059" num="0058">More specifically, the ECL computer <b>146</b> and the DCL computer <b>148</b> can use the following non-limiting equation to calculate contrastive loss, denoted generically below as L<sub>c</sub>:</p><p id="p-0060" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>L</mi>      <mi>c</mi>     </msub>     <mo>=</mo>     <mrow>      <mo>-</mo>      <mrow>       <munder>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>&#x2208;</mo>         <mi>I</mi>        </mrow>       </munder>       <mrow>        <munder>         <mo>&#x2211;</mo>         <mrow>          <mi>p</mi>          <mo>&#x2208;</mo>          <mrow>           <mi>P</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mi>i</mi>           <mo>)</mo>          </mrow>         </mrow>        </munder>        <mrow>         <mfrac>          <mn>1</mn>          <mrow>           <semantics definitionURL="">            <mo>&#x2758;</mo>            <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>           </semantics>           <mrow>            <mi>P</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <mi>i</mi>            <mo>)</mo>           </mrow>           <semantics definitionURL="">            <mo>&#x2758;</mo>            <annotation encoding="Mathematica">"\[RightBracketingBar]"</annotation>           </semantics>          </mrow>         </mfrac>         <mo>&#x2062;</mo>         <mrow>          <mrow>           <mi>log</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mfrac>            <mrow>             <mi>exp</mi>             <mo>&#x2061;</mo>             <mo>(</mo>             <mrow>              <mrow>               <msub>                <mi>z</mi>                <msub>                 <mi>q</mi>                 <mi>i</mi>                </msub>               </msub>               <mo>&#xb7;</mo>               <msub>                <mi>z</mi>                <msub>                 <mi>k</mi>                 <mi>p</mi>                </msub>               </msub>              </mrow>              <mo>/</mo>              <mi>&#x3c4;</mi>             </mrow>             <mo>)</mo>            </mrow>            <mrow>             <msub>              <mo>&#x2211;</mo>              <mrow>               <mi>j</mi>               <mo>&#x2208;</mo>               <mi>M</mi>              </mrow>             </msub>             <mrow>              <mi>exp</mi>              <mo>&#x2061;</mo>              <mo>(</mo>              <mrow>               <mrow>                <msub>                 <mi>z</mi>                 <msub>                  <mi>q</mi>                  <mi>i</mi>                 </msub>                </msub>                <mo>&#xb7;</mo>                <msub>                 <mi>z</mi>                 <msub>                  <mi>k</mi>                  <mi>j</mi>                 </msub>                </msub>               </mrow>               <mo>/</mo>               <mi>&#x3c4;</mi>              </mrow>              <mo>)</mo>             </mrow>            </mrow>           </mfrac>           <mo>)</mo>          </mrow>          <mo>.</mo>         </mrow>        </mrow>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>1</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0061" num="0059">In Equation (1), i represents an index of a particular sample in a batch I under consideration. P(i) represents a set of all positive samples for index i, and |P(i)| represents the ordinality of the set. A positive sample (k<sub>p</sub>) represents a correct (ground-truth) counterpart of the sample at index i. &#x3c4; represents a temperature hyper-parameter.</p><p id="p-0062" num="0060">More generally, the outer summation of Equation (1) steps through each sample i in the batch I. The inner summation of Equation (1) steps through each positive sample p with respect to a sample i under consideration. With reference to the term enclosed by the innermost parentheses, the numerator expresses the dot product of the vector z<sub>q </sub>for the sample i with the vector z<sub>k </sub>for a particular positive example p under consideration. The dot product expresses the difference between these two vectors. The exponential of this product is normalized by a sum of similarly-computed dot products, which is expressed by the denominator of the equation.</p><p id="p-0063" num="0061">The training system <b>102</b> generates a final loss measure as a weighted sum of L<sub>AR</sub>, L<sub>EC</sub>, and L<sub>DC</sub>, e.g., as L<sub>AR</sub>+&#x3bb;<sub>EC</sub>L<sub>EC</sub>+&#x3bb;<sub>DC</sub>L<sub>DC</sub>, where &#x3bb;<sub>EC </sub>and &#x3bb;<sub>DC </sub>are scalar weighting parameters. The training system <b>102</b> then proceeds to update the training weights in the training architecture <b>112</b> based on the calculated weights. More specifically, assume that the training weights of the first-path encoder <b>116</b> are generically denoted by &#x3b8;<sub>q</sub><sup>e1 </sup>and the training weights of the first-path decoder <b>118</b> are generically denoted by &#x3b8;<sub>q</sub><sup>d1</sup>. The training system <b>102</b> updates these training weights using back-projection.</p><p id="p-0064" num="0062">In contrast, the training system <b>102</b> updates the weights in the second processing path <b>126</b> and the third processing path <b>134</b> in indirect fashion, based on the training weights that have been calculated for the first processing path <b>114</b> (without separately performing back-projection). Let &#x3b8;<sub>k</sub><sup>e2 </sup>represent the training weights used by the second-path encoder <b>128</b>. The training system <b>102</b> can update these weights using &#x3b8;<sub>k</sub><sup>e2</sup>&#x2190;m&#x3b8;<sub>k</sub><sup>e2</sup>+(1&#x2212;m)&#x3b8;<sub>q</sub><sup>e1</sup>. Here, &#x3b8;<sub>k</sub><sup>e2 </sup>on the right side of the equation represents the current training weights used by the second-path encoder <b>128</b>, while &#x3b8;<sub>k</sub><sup>e2 </sup>on the left side of the equation represents the updated training weights. The symbol m represents a constant value that is close to 1.0 (e.g., 0.9). By choosing a value of m close to 1.0, the training system <b>102</b> slows the change in the existing training weights of the second-path encoder <b>128</b>. The training system <b>102</b> updates the training weights of the third-path decoder <b>140</b> in the same manner set forth above for the second-path encoder <b>128</b>. That is, the training system <b>102</b> updates the training weights of the third-path decoder using the equation: &#x3b8;<sub>k</sub><sup>d3</sup>&#x2190;m&#x3b8;<sub>k</sub><sup>d3</sup>+(1&#x2212;m)&#x3b8;<sub>q</sub><sup>d1</sup>.</p><p id="p-0065" num="0063">By virtue of this indirect manner of updating training weights, the training system <b>102</b> can simplify the training operation and reduce the consumption of computing resources in the training operation. More specifically, the training system <b>102</b> consumes a significant amount of processing and memory resources in performing back-projection. By reducing the amount of training weights that are updated via back-projection, the training system <b>102</b> can reduce the consumption of computing resources. The use of above-described update strategy also ensures that the training system <b>102</b> learns the training weights in a controlled fashion, e.g., by minimizing large oscillations of values in the training weights.</p><p id="p-0066" num="0064"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a buffer memory <b>154</b> used by the ECL computer <b>146</b> in the course of generating the encoder-level loss information L<sub>EC</sub>. This buffer memory <b>154</b> contains a set of the last m entries that have been processed by the second-path encoder <b>128</b>, generically referred to as keys, each of which includes a pairing of a particular encoded source item S<sup>e </sup>and a particular target item T<sup>e </sup>produced by the second-path encoder <b>128</b>. The training system <b>102</b> also maintains a data structure <b>402</b> that identifies the correlation between each query q and the keys (k<sub>1</sub>, k<sub>2</sub>, . . . k<sub>m</sub>) in the buffer memory <b>154</b>. Here, the query q represents a pairing of a particular encoded source item Se and a particular first auxiliary item A<sup>e </sup>produced by the first-path encoder <b>116</b>. That is, the data structure <b>402</b> identifies a matching query and key as a positive training example (+), and an unmatched query and key as a negative training example (&#x2212;). This correlation is identified for the n queries in a batch, where n&#x3c;m. Although <figref idref="DRAWINGS">FIG. <b>4</b></figref> shows that each query has a single positive key, any given query can match plural keys, and any given key can match plural queries.</p><p id="p-0067" num="0065">Similarly, <figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a buffer memory <b>156</b> used by the DCL computer <b>148</b> in the course of generating the decoder-level loss information L<sub>DC</sub>. This buffer memory <b>156</b> contains a set of the last m entries that have been processed by the third-path decoder <b>140</b>, generically referred to as keys, each of which includes a pairing of a particular decoded source item S<sup>d </sup>and a particular second auxiliary item A&#x2032;<sup>d </sup>produced by the third-path encoder <b>140</b>. The training system <b>102</b> also maintains a data structure <b>502</b> that identifies the correlation between each query q and the keys (k<sub>1</sub>, k<sub>2</sub>, . . . k<sub>m</sub>) in the buffer memory <b>156</b>. Here, the query q represents a pairing of a particular decoded source item S<sup>d </sup>and a particular decoded first auxiliary item A<sup>d </sup>produced by the first-path decoder <b>118</b>. This correlation is identified for the n queries in a batch, where n&#x3c;m. Although <figref idref="DRAWINGS">FIG. <b>5</b></figref> shows that each query has a single positive key, any given query can match plural keys, and any given key can match plural queries.</p><p id="p-0068" num="0066"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates how the training system <b>102</b> can update each buffer memory described above, generically represented in <figref idref="DRAWINGS">FIG. <b>6</b></figref> as buffer memory <b>602</b>. The training system <b>102</b> commences each training iteration by packaging together a block of new entries to be processed by the training architecture <b>112</b>. The training system <b>102</b> can expresses this block as the concatenation of plural instances of the kind of input information illustrated in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>3</b></figref>. The training system <b>102</b> updates each buffer memory <b>602</b> by ejecting the oldest block of entries (which was stored last) in the buffer memory <b>602</b> and storing the block of new entries (which is designated as the newest entry).</p><p id="p-0069" num="0067">Generally, the training system <b>102</b> relies on the buffer memories (<b>154</b>, <b>156</b>) shown in <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>5</b></figref> to increase the number of entries that are used to compute L<sub>EC </sub>and L<sub>DC</sub>, e.g., to encompass more entries than are present in any given batch. The training system <b>102</b> specifically relies on the buffer memories (<b>154</b>, <b>156</b>) to increase the number of negative examples that are used to compute the loss information. This ability, in turn, enables the training system <b>102</b> to increase the accuracy of the machine-trained model <b>104</b> that it produces. The training system <b>102</b> relies on the FIFO strategy shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> to ensure that the buffer memories (<b>154</b>. <b>156</b>) store entries that are computed using training weights that are relatively up-to-date (because they were recently updated), while not overburdening the training system <b>102</b> by performing more frequent memory update operations.</p><p id="p-0070" num="0068">Other implementations of the training system <b>102</b> can adopt other strategies for managing memory. For example, other implementations of the training system <b>102</b> can rely on a fixed dictionary of entries, or a dictionary of entries that is updated on some other basis than the FIFO strategy described above. Other implementations can use back-projection to also update the encoders (<b>128</b>, <b>136</b>) of the second processing path <b>126</b> and the third processing path <b>134</b>, and to update the decoder <b>140</b> of the third processing path <b>134</b> (rather than indirectly computing the training weights of these components in the manner described above).</p><heading id="h-0007" level="1">A.2. Inference-Stage System</heading><p id="p-0071" num="0069"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an inference-stage system <b>702</b> that uses the machine-trained model <b>104</b> produced by the training system <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Assume that the inference-stage system <b>702</b> receives a particular input source item S<sub>i </sub><b>704</b>. An item retrieval system <b>706</b> retrieves at least one auxiliary item A<sub>i </sub><b>708</b> that is deemed related to the input source item from at least one data source of auxiliary items. Like the previously-described retrieval engine <b>202</b> (of <figref idref="DRAWINGS">FIG. <b>2</b></figref>), the inference-stage item retrieval system <b>706</b> can use various techniques to perform this task. For example, the item retrieval system <b>706</b> can use the source item as a lookup key to retrieve an auxiliary item that a data source explicitly identifies as being related to the source item. Data sources that can serve this role include lookup tables, dictionaries, digests, knowledge graphs, etc. In other implementations, the item retrieval system <b>706</b> can use any kind of feature-based search mechanism to identify an auxiliary item that matches a specified source item. For example, the item retrieval system <b>706</b> can map a given source item into a source vector, and consult a search index to find an auxiliary item that has a corresponding auxiliary vector that is within a prescribed distance to the source vector. The item retrieval system <b>706</b> can assess distance using any distance metric, such as cosine similarity. These examples of the of the item retrieval system <b>706</b> are described here in the spirit of illustration, not limitation; other implementations of the item retrieval system <b>706</b> can adopt yet other retrieval strategies.</p><p id="p-0072" num="0070">The inference-stage system <b>702</b> can generate an instance of input information that mirrors that received by the first processing path <b>114</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. That is, the inference-stage system <b>702</b> can concatenate the source item <b>704</b> with the retrieved auxiliary item <b>708</b>, and bracket this concatenated result with a &#x3c;BOS&#x3e; token and an &#x3c;EOS&#x3e; token.</p><p id="p-0073" num="0071">A transformer <b>710</b> uses a model encoder <b>712</b> and a model decoder <b>714</b> to process the input information. The model encoder <b>712</b> is the trained counterpart of the first-path encoder <b>116</b> introduced in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, while the model decoder <b>714</b> is the trained counterpart of the first-path decoder <b>118</b> introduced in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. More specifically, the model encoder <b>712</b> maps the input information to encoder output information, and the model decoder <b>714</b> maps the encoder output information to decoder output information. As will be explained in greater detail below, the transformer <b>710</b> actually generates the decoder output information in an auto-regressive manner, e.g., token by token. When an end-of-sequence token is predicted, the transformer <b>710</b> outputs a fully generated sequence <b>716</b>. Any type of optional post-processing component(s) <b>718</b> can perform any application-specific task(s) on the basis of the generated sequence <b>716</b>.</p><p id="p-0074" num="0072">In a first example, the source item corresponds to a set of concept terms, such as the concept terms (&#x201c;jump,&#x201d; &#x201c;rider,&#x201d; &#x201c;air,&#x201d; &#x201c;bike&#x201d;). The transformer <b>710</b> produces a generated sequence that corresponds to a coherent sentence that uses these terms, such as the sentence &#x201c;The rider is jumping in the air on his bike.&#x201d; A post-processing component can perform any application-specific task based on this generated sentence. For instance, assume that the concept terms originate from a classification engine that performs topic analysis on an image, e.g., by identifying regions of interest in the image, and then classifying the topics associated with the regions of interest. The post-processing component can annotate the image with the generated sentence.</p><p id="p-0075" num="0073">In a second example, the source item corresponds to a query submitted by a user to a search engine via a browser application, or through some other kind of application. The transformer <b>710</b> produces a set of keywords based on the query, e.g., which have been previously specified by an advertiser. A post-processing component can match an advertisement with the identified keywords, and then serve the advertisement to the user who submitted the query.</p><p id="p-0076" num="0074"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows an illustrative transformer <b>802</b> that can be used by the inference-stage system <b>702</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>. The transformer <b>802</b> includes an encoder <b>804</b> and a decoder <b>806</b>. Note that each encoder of the training system <b>102</b> can use the same architecture as the representative encoder <b>804</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, and each decoder of the training system <b>102</b> can use the same architecture as the representative decoder <b>806</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>. But the transformer <b>802</b> will be explained below primarily in the context of its inference-stage manifestation.</p><p id="p-0077" num="0075">The encoder <b>804</b> receives encoder input information in the form of a series of input vectors. An input encoding component (not shown) produces the input vectors by converting a series of tokens that compose the linguistic information supplied to the transformer <b>802</b> into respective vectors (e.g., using a lookup table, machine-trained model, etc.), and then adding position information to the respective vectors which describes their position within the sequence of input vectors. The encoder <b>804</b> maps the encoder input information into encoder output information using a pipeline of encoder blocks (<b>808</b>, <b>810</b>, . . . , <b>812</b>), with each encoder block receiving its input information from a preceding encoder block (if any). The encoder blocks (<b>808</b>, <b>810</b>, . . . , <b>812</b>) include respective attention mechanisms (<b>814</b>, <b>816</b>, . . . , <b>818</b>).</p><p id="p-0078" num="0076">The decoder <b>806</b> performs its processing based on both decoder input information and the encoder output information. Since the decoder <b>806</b> produces the generated sequence using auto-regression, the decoder input information includes, at any given time, the set of tokens that have been predicted thus far. The decoder <b>806</b> uses a pipeline of decoder blocks (<b>820</b>, <b>822</b>, . . . , <b>824</b>) to produce decoder output information, with each decoder block receiving input information from a preceding decoder block (if any). The decoder blocks (<b>820</b>, <b>822</b>, . . . , <b>824</b>) can include respective attention mechanisms (<b>826</b>, <b>828</b>, <b>830</b>).</p><p id="p-0079" num="0077">An output probability generation component <b>832</b> can use a combination of a linear transformation operation and the softmax function to map the decoder output information into a probability distribution. The probability distribution identifies the probability associated with each word in an identified vocabulary. A search heuristic component can use any search heuristic to select from among the candidate tokens. In a greedy search heuristic, the search heuristic component selects the token having the highest probability at each time step. In a beam search heuristic, the search heuristic component selects a plurality of tokens having the highest probabilities.</p><p id="p-0080" num="0078">Consider the operation of auto-regression for the case in which the greedy search heuristic is used. Upon predicting the next token, the decoder <b>806</b> adds this predicted token to the decoder input information, to produce updated decoder input information. The decoder <b>806</b> then repeats the above-described operations on the basis of the updated decoder input information, to produce a next token in the generated sequence. The decoder <b>806</b> adds this next token to the decoder input information, to produce yet another instance of updated decoder input information. The decoder <b>806</b> continues in this recursive manner until the output probability generation component <b>832</b> predicts that the next token is an end-of-sequence token. For the case in which the beam search heuristic is used, the decoder <b>806</b> performs the above tasks with respect to plural paths through a token search space.</p><p id="p-0081" num="0079"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows an illustrative and non-limiting encoder block <b>902</b>. It includes a self-attention mechanism <b>904</b>, an add-&#x26;-normalize component <b>906</b>, a feed-forward component <b>908</b>, and another add-&#x26;-normalize component <b>910</b>. The self-attention mechanism performs self-attention. The first add-&#x26;-normalize component <b>906</b> adds the input information fed to the self-attention mechanism <b>904</b> to the output information provided by the self-attention mechanism <b>904</b> (thus forming a residual connection), and then performs layer-normalization on that result. Layer normalization entails adjusting values in a layer based on the mean and deviation of those values in the layer. The feed-forward component <b>908</b> uses one or more fully connected neural network layers to map input information to output information. The second add-&#x26;-normalize component <b>910</b> performs the same function as the first add-&#x26;-normalize component <b>906</b>.</p><p id="p-0082" num="0080">In some implementation, each attention mechanism in the self-attention mechanism <b>904</b> generates attention information using the following equation:</p><p id="p-0083" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>attn</mi>      <mo>&#x2062;</mo>      <mrow>       <mo>(</mo>       <mrow>        <mi>Q</mi>        <mo>,</mo>        <mi>K</mi>        <mo>,</mo>        <mi>V</mi>       </mrow>       <mo>)</mo>      </mrow>     </mrow>     <mo>=</mo>     <mrow>      <mi fontstyle="normal">softmax</mi>      <mo>&#x2062;</mo>      <mtext fontstyle="normal">    </mtext>      <mrow>       <mo>(</mo>       <mfrac>        <mrow>         <mi>Q</mi>         <mo>&#x2062;</mo>         <msup>          <mi>K</mi>          <mi>T</mi>         </msup>        </mrow>        <msqrt>         <mi>d</mi>        </msqrt>       </mfrac>       <mo>)</mo>      </mrow>      <mo>&#x2062;</mo>      <mrow>       <mi>V</mi>       <mo>.</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0084" num="0081">Query information Q is produced by multiplying the input vectors associated with the encoder input information by a query weighting matrix W<sup>Q</sup>. Key information K and value information V are produced by multiplying the same input vectors by a key weighting matrix W<sup>K </sup>and a value weighting matrix W<sup>V</sup>, respectively. (For clarity, note that the query information Q and key information K in Equation (2) represents different information items than the query q and the key k that appear in Equation (1)). Equation (2) involves taking the dot product of Q by the transpose of K, and then dividing that dot product by a scaling factor &#x221a;{square root over (V)}, where d may represent the dimensionality of the machine-learned model. This yields a scaled result. Equation (2) then involves computing the softmax of the scaled result, and then multiplying the result of the softmax operation by V. From a more general perspective, the self-attention mechanism <b>904</b> uses Equation (2) to determine the amount of focus (attention) that should be placed on each part of the input information, when processing a particular part of the input information under consideration.</p><p id="p-0085" num="0082"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows an illustrative and non-limiting example of a decoder block <b>1002</b>. The decoder block <b>1002</b> includes a self-attention mechanism <b>1004</b>, an add-&#x26;-normalize component <b>1006</b>, encoder-decoder attention mechanism <b>1008</b>, another add-&#x26;-normalize component <b>1010</b>, a feed-forward component <b>1012</b>, and another add-&#x26;-normalize component <b>1014</b>. The self-attention mechanism <b>1004</b> performs masked self-attention on the decoder input information fed to it using Equation (2). The self-attention mechanism <b>1004</b> performs masking so that positions in a sequence after a last-predicted token (which are unknown at this time) do not bias its results.</p><p id="p-0086" num="0083">On the other hand, the encoder-decoder attention mechanism <b>1008</b> performs cross-attention based on the output information generated by the encoder <b>804</b> and the output information supplied by the preceding component in the decoder block <b>1002</b> (i.e., the add-&#x26;-normalize component <b>1006</b>). The encoder-decoder attention mechanism <b>1008</b> also uses Equation (2) to perform its functions, but the encoder-decoder attention mechanism <b>1008</b> uses the encoder output information to generate the key formation K and the value information V, and uses the output information fed to it by the add-&#x26;-normalize component <b>1006</b> to generate the query information Q. The add-&#x26;-normalize components (<b>1006</b>, <b>1010</b>, <b>1014</b>) and the feed-forward component <b>1012</b> perform the same functions described above for the encoder block <b>902</b>.</p><p id="p-0087" num="0084">The decoders that are used in the training system <b>102</b> also operate in an auto-regressive manner as described above. But each training-stage decoder successively processes the tokens that make up the target item rather than the tokens that are predicted by the output probability component <b>932</b>.</p><p id="p-0088" num="0085">In conclusion to Section A, the machine-trained model <b>104</b> produced by the training system <b>102</b> exhibits high accuracy relative to some competing models. The machine-trained model <b>104</b> achieves high accuracy, in part, based on the training system's incorporation of external knowledge in the training process, and through the training system's use of encoder-level contrastive learning and decoder-level contrastive learning. At the same time, the machine-trained model <b>104</b> produced by the training system <b>102</b> makes efficient use of computer resources and exhibits good latency-related performance. These factors enable developers to use the model in resource-constrained computing platforms, and in applications that demand low-latency responses (such applications that demand real-time responses to user inputs).</p><p id="p-0089" num="0086">In other words, instead of increasing the complexity of the machine-trained model <b>104</b> itself (e.g., by adding additional layers and functions to a transformer-based architecture), the training system <b>102</b> uses a training architecture <b>112</b> that improves the accuracy of a resultant transformer-based model. The transformer-based model, because it does not incorporate additional layers or functions, can be expected to consume no more computer resources than some other transformer-based models. The transformer-based model can also be expected to offer latency-related performance that is no worse than some other transformer-based models.</p><p id="p-0090" num="0087">According to another technical benefit, the training system's use of FIFO buffer memory and its selective use of back-projection allows it to produce the machine-trained model in a resource-efficient manner. The use of buffer memory also allows the training system <b>102</b> to increase the amount of data that is used to perform contrastive learning, which positively contributes to the accuracy of the resultant model produced by the training system <b>102</b>.</p><heading id="h-0008" level="1">B. Illustrative Processes</heading><p id="p-0091" num="0088"><figref idref="DRAWINGS">FIGS. <b>11</b>-<b>13</b></figref> show processes that explain the operation of the training system <b>102</b> and the inference-stage system <b>702</b> of Section A in flowchart form. Since the principles underlying the operation of the systems (<b>102</b>, <b>702</b>) have already been described in Section A, certain operations will be addressed in summary fashion in this section. Each flowchart is expressed as a series of operations performed in a particular order. But the order of these operations is merely representative, and can be varied in other implementations. Further, any two or more operations described below can be performed in a parallel manner. In one implementation, the blocks shown in the flowcharts that pertain to processing-related functions can be implemented by the hardware logic circuitry described in Section C, which, in turn, can be implemented by one or more hardware processors and/or other logic units that include a task-specific collection of logic gates.</p><p id="p-0092" num="0089"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows a process <b>1102</b> that represents an overview of the operation of the training system <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In block <b>1104</b>, for a particular training example of linguistic information, the training system <b>102</b> obtains a source item, a target item, a first auxiliary item, and a second auxiliary item. The first and second auxiliary items convey knowledge about the source item that is supplemental to any information conveyed by the source item itself, and the target item represents a transformed counterpart of the source item. In block <b>1106</b>, in the first processing path <b>114</b>, the training system <b>102</b>: forms the first instance of input information <b>120</b> by combining the source item and the first auxiliary item; transforms the first instance of input information <b>120</b> into first-path encoder output information <b>122</b> using the first-path encoder <b>116</b>; and transforms the first-path encoder output information <b>122</b> into first-path decoder information <b>124</b> using the first-path decoder <b>118</b>. In block <b>1108</b>, in the second processing path <b>126</b>, the training system <b>102</b>: forms the second instance of input information <b>130</b> by combing the source item and the target item; and transforms the second instance of input information <b>130</b> into second-path encoder output information <b>132</b> using the second-path encoder <b>128</b>. In block <b>1110</b>, in the third processing path <b>134</b>, the training system <b>102</b>: forms the third instance of input information <b>134</b> by combining the source item and the second auxiliary item; and transforms the third instance of input information <b>134</b> into third-path decoder output information <b>142</b> using the third-path encoder <b>136</b> and the third-path decoder <b>140</b>. In block <b>1112</b>, the training system <b>102</b> updates training weights based on loss information generated using the first processing path <b>114</b>, the second processing path <b>126</b>, and the third processing path <b>134</b>. The process <b>1102</b> is repeating for additional training examples in a training data set. The machine-trained model <b>104</b> that is produced by the process <b>1102</b> corresponds to a trained counterpart of the first-path encoder <b>116</b> and the first-path decoder <b>118</b>.</p><p id="p-0093" num="0090"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart that provides additional detail regarding one implementation of the updating operation of block <b>1112</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>. In block <b>1202</b>, the training system <b>102</b> generates first loss information based on a comparison of the first-path decoder output information <b>124</b> and the target item. In block <b>1204</b>, the training system <b>102</b> generates, based on encoder-level contrastive learning, second loss information based on comparison of the first-path encoder output information <b>122</b> and the second-path encoder output information <b>132</b>. In block <b>1206</b>, the training system <b>102</b> generates, based on decoder-level contrastive learning, third loss information based on a comparison of the first-path decoder output information <b>124</b> and the third-path decoder output information <b>142</b>. In block <b>1208</b>, the training system <b>102</b> updates the machine-trained model <b>104</b> based on the first loss information, the second loss information, and the third loss information.</p><p id="p-0094" num="0091"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows a process <b>1302</b> that represents one manner of operation of the inference-stage system <b>702</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>. In block <b>1304</b>, the inference-stage system <b>702</b> obtains the input source item <b>704</b>. In block <b>1306</b>, the inference-stage system <b>702</b> obtains a particular auxiliary item <b>708</b> that conveys knowledge about the input source item <b>704</b> that is supplemental to any information conveyed by the input source item <b>794</b> itself. In block <b>1308</b>, the inference-stage system <b>702</b> forms a particular instance of input information based on a combination of the input source item <b>704</b> and the particular auxiliary item <b>708</b>. In block <b>1310</b>, the inference-stage system <b>702</b> uses the machine-trained model <b>104</b> to map the particular instance of input information into the particular generated sequence <b>716</b>. The machine-trained model <b>104</b> includes the model encoder <b>712</b> that produces model encoder output information based on the particular instance of input information, and the model decoder <b>714</b> that produces model decoder output information based on the model encoder output information. The machine-trained model <b>104</b> is produced by a training process that involves encoder-level contrastive learning and decoder-level contrastive learning. The encoder-level contrastive learning involves comparing the model encoder output information with other encoder output information (e.g., <b>132</b>) that is produced by another encoder (e.g., the second-path encoder <b>128</b>). The decoder-level contrastive learning involves comparing the model decoder output information with other decoder output information (e.g., <b>142</b>) that is produced by another decoder (e.g., the third-path decoder <b>140</b>).</p><heading id="h-0009" level="1">C. Representative Computing Functionality</heading><p id="p-0095" num="0092"><figref idref="DRAWINGS">FIG. <b>14</b></figref> shows computing equipment that can be used to implement the training system <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and the inference-stage system <b>702</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>. The computing equipment includes a set of user computing devices <b>1402</b> coupled to a set of servers <b>1404</b> via a computer network <b>1406</b>. Each user computing device can correspond to any device that performs a computing function, including a desktop computing device, a laptop computing device, a handheld computing device of any type (e.g., a smartphone, a tablet-type computing device, etc.), a mixed reality device, a wearable computing device, an Internet-of-Things (IoT) device, a gaming system, and so on. The computer network <b>1406</b> can be implemented as a local area network, a wide area network (e.g., the Internet), one or more point-to-point links, or any combination thereof.</p><p id="p-0096" num="0093"><figref idref="DRAWINGS">FIG. <b>14</b></figref> also indicates that the training system <b>102</b> and the inference-stage system <b>702</b> can be spread across the user computing devices <b>1402</b> and/or the servers <b>1404</b> in any manner. For instance, in some cases, the inference-stage system <b>702</b> is entirely implemented by one or more of the servers <b>1404</b>. Each user may interact with the servers <b>1404</b> via a browser application or other programmatic interface provided by a user computing device. In other cases, the inference-stage system <b>702</b> is entirely implemented by a user computing device in local fashion, in which case no interaction with the servers <b>1404</b> is necessary. In other cases, the functionality associated with the inference-stage system <b>702</b> is distributed between the servers <b>1404</b> and each user computing device in any manner</p><p id="p-0097" num="0094"><figref idref="DRAWINGS">FIG. <b>15</b></figref> shows a computing system <b>1502</b> that can be used to implement any aspect of the mechanisms set forth in the above-described figures. For instance, the type of computing system <b>1502</b> shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref> can be used to implement any user computing device or any server shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref>. In all cases, the computing system <b>1502</b> represents a physical and tangible processing mechanism.</p><p id="p-0098" num="0095">The computing system <b>1502</b> can include one or more hardware processors <b>1504</b>. The hardware processor(s) <b>1504</b> can include, without limitation, one or more Central Processing Units (CPUs), and/or one or more Graphics Processing Units (GPUs), and/or one or more Application Specific Integrated Circuits (ASICs), and/or one or more Neural Processing Units (NPUs), etc. More generally, any hardware processor can correspond to a general-purpose processing unit or an application-specific processor unit.</p><p id="p-0099" num="0096">The computing system <b>1502</b> can also include computer-readable storage media <b>1506</b>, corresponding to one or more computer-readable media hardware units. The computer-readable storage media <b>1506</b> retains any kind of information <b>1508</b>, such as machine-readable instructions, settings, data, etc. Without limitation, the computer-readable storage media <b>1506</b> may include one or more solid-state devices, one or more magnetic hard disks, one or more optical disks, magnetic tape, and so on. Any instance of the computer-readable storage media <b>1506</b> can use any technology for storing and retrieving information. Further, any instance of the computer-readable storage media <b>1506</b> may represent a fixed or removable unit of the computing system <b>1502</b>. Further, any instance of the computer-readable storage media <b>1506</b> may provide volatile or non-volatile retention of information.</p><p id="p-0100" num="0097">More generally, any of the storage resources described herein, or any combination of the storage resources, may be regarded as a computer-readable medium. In many cases, a computer-readable medium represents some form of physical and tangible entity. The term computer-readable medium also encompasses propagated signals, e.g., transmitted or received via a physical conduit and/or air or other wireless medium, etc. However, the specific term &#x201c;computer-readable storage medium&#x201d; expressly excludes propagated signals per se in transit, while including all other forms of computer-readable media.</p><p id="p-0101" num="0098">The computing system <b>1502</b> can utilize any instance of the computer-readable storage media <b>1506</b> in different ways. For example, any instance of the computer-readable storage media <b>1506</b> may represent a hardware memory unit (such as Random Access Memory (RAM)) for storing transient information during execution of a program by the computing system <b>1502</b>, and/or a hardware storage unit (such as a hard disk) for retaining/archiving information on a more permanent basis. In the latter case, the computing system <b>1502</b> also includes one or more drive mechanisms <b>1510</b> (such as a hard drive mechanism) for storing and retrieving information from an instance of the computer-readable storage media <b>1506</b>.</p><p id="p-0102" num="0099">The computing system <b>1502</b> may perform any of the functions described above when the hardware processor(s) <b>1504</b> carry out computer-readable instructions stored in any instance of the computer-readable storage media <b>1506</b>. For instance, the computing system <b>1502</b> may carry out computer-readable instructions to perform each block of the processes described in Section B.</p><p id="p-0103" num="0100">Alternatively, or in addition, the computing system <b>1502</b> may rely on one or more other hardware logic units <b>1512</b> to perform operations using a task-specific collection of logic gates. For instance, the hardware logic unit(s) <b>1512</b> may include a fixed configuration of hardware logic gates, e.g., that are created and set at the time of manufacture, and thereafter unalterable. Alternatively, or in addition, the other hardware logic unit(s) <b>1512</b> may include a collection of programmable hardware logic gates that can be set to perform different application-specific tasks. The latter category of devices includes, but is not limited to Programmable Array Logic Devices (PALs), Generic Array Logic Devices (GALs), Complex Programmable Logic Devices (CPLDs), Field-Programmable Gate Arrays (FPGAs), etc.</p><p id="p-0104" num="0101"><figref idref="DRAWINGS">FIG. <b>15</b></figref> generally indicates that hardware logic circuitry <b>1514</b> includes any combination of the hardware processor(s) <b>1504</b>, the computer-readable storage media <b>1506</b>, and/or the other hardware logic unit(s) <b>1512</b>. That is, the computing system <b>1502</b> can employ any combination of the hardware processor(s) <b>1504</b> that execute machine-readable instructions provided in the computer-readable storage media <b>1506</b>, and/or one or more other hardware logic unit(s) <b>1512</b> that perform operations using a fixed and/or programmable collection of hardware logic gates. More generally stated, the hardware logic circuitry <b>1514</b> corresponds to one or more hardware logic units of any type(s) that perform operations based on logic stored in and/or otherwise embodied in the hardware logic unit(s). Further, in some contexts, each of the terms &#x201c;component,&#x201d; &#x201c;module,&#x201d; &#x201c;engine,&#x201d; &#x201c;system,&#x201d; &#x201c;mechanism,&#x201d; and &#x201c;tool&#x201d; refers to a part of the hardware logic circuitry <b>1514</b> that performs a particular function or combination of functions.</p><p id="p-0105" num="0102">In some cases (e.g., in the case in which the computing system <b>1502</b> represents a user computing device), the computing system <b>1502</b> also includes an input/output interface <b>1516</b> for receiving various inputs (via input devices <b>1518</b>), and for providing various outputs (via output devices <b>1520</b>). Illustrative input devices include a keyboard device, a mouse input device, a touchscreen input device, a digitizing pad, one or more static image cameras, one or more video cameras, one or more depth camera systems, one or more microphones, a voice recognition mechanism, any position-determining devices (e.g., GPS devices), any movement detection mechanisms (e.g., accelerometers, gyroscopes, etc.), and so on. One particular output mechanism may include a display device <b>1522</b> and an associated graphical user interface presentation (GUI) <b>1524</b>. The display device <b>1522</b> may correspond to a liquid crystal display device, a light-emitting diode display (LED) device, a cathode ray tube device, a projection mechanism, etc. Other output devices include a printer, one or more speakers, a haptic output mechanism, an archival mechanism (for storing output information), and so on. The computing system <b>1502</b> can also include one or more network interfaces <b>1526</b> for exchanging data with other devices via one or more communication conduits <b>1528</b>. One or more communication buses <b>1530</b> communicatively couple the above-described units together.</p><p id="p-0106" num="0103">The communication conduit(s) <b>1528</b> can be implemented in any manner, e.g., by a local area computer network, a wide area computer network (e.g., the Internet), point-to-point connections, etc., or any combination thereof. The communication conduit(s) <b>1528</b> can include any combination of hardwired links, wireless links, routers, gateway functionality, name servers, etc., governed by any protocol or combination of protocols.</p><p id="p-0107" num="0104"><figref idref="DRAWINGS">FIG. <b>15</b></figref> shows the computing system <b>1502</b> as being composed of a discrete collection of separate units. In some cases, the collection of units corresponds to discrete hardware units provided in a computing device chassis having any form factor. <figref idref="DRAWINGS">FIG. <b>15</b></figref> shows illustrative form factors in its bottom portion. In other cases, the computing system <b>1502</b> can include a hardware logic unit that integrates the functions of two or more of the units shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. For instance, the computing system <b>1502</b> can include a system on a chip (SoC or SOC), corresponding to an integrated circuit that combines the functions of two or more of the units shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref>.</p><p id="p-0108" num="0105">The following summary provides a non-exhaustive set of illustrative examples of the technology set forth herein.</p><p id="p-0109" num="0106">(A1) Some implementations of the technology described herein include a method (e.g., the process <b>1102</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>), using a computing system (e.g., the computing system <b>1502</b>), for producing a machine-trained model (e.g., the machine-trained model <b>104</b>). For a particular training example of linguistic information, the method involves obtaining (e.g., in block <b>1104</b>) a source item, a target item, a first auxiliary item, and a second auxiliary item. The first and second auxiliary items convey knowledge about the source item that is supplemental to any information conveyed by the source item itself, and the target item represents a transformed counterpart of the source item. In a first processing path (e.g., the first processing path <b>114</b>), the method involves (e.g., in block <b>1106</b>): forming a first instance of input information (e.g., <b>120</b>) by combining the source item and the first auxiliary item; transforming the first instance of input information into first-path encoder output information (e.g., <b>122</b>) using a first-path encoder (e.g., the first-path encoder <b>116</b>); and transforming the first-path encoder output information into first-path decoder information (e.g., <b>124</b>) using a first-path decoder (e.g., the first-path decoder <b>118</b>). In a second processing path (e.g., the second processing path <b>126</b>), the method involves (e.g., in block <b>1108</b>): forming a second instance of input information (e.g., <b>130</b>) by combing the source item and the target item; and transforming the second instance of input information into second-path encoder output information (e.g., <b>132</b>) using a second-path encoder (e.g., the second-path encoder <b>128</b>). In a third processing path (e.g., the third processing path <b>134</b>), the method involves (e.g., in block <b>1110</b>): forming a third instance of input information (e.g., <b>138</b>) by combining the source item and the second auxiliary item; and transforming the third instance of input information into third-path decoder output information (e.g., <b>142</b>) using a third-path encoder (e.g., the third-path encoder <b>136</b>) and a third-path decoder (e.g., the third-path decoder <b>140</b>). The method then involves (e.g., in block <b>1112</b>) updating training weights based on loss information generated using the first processing path, the second processing path, and the third processing path. The method is repeated for additional training examples in a training data set. The machine-trained model that is produced by the method corresponds to a trained counterpart of the first-path encoder and the first-path decoder.</p><p id="p-0110" num="0107">The method described above produces a machine-trained model that exhibits high accuracy relative to some competing models. The model achieves high accuracy, in part, based on the method's incorporation of auxiliary information in the training process. At the same time, the model produced by the training system makes efficient use of computer resources and exhibits good latency-related performance. These factors enable developers to use the model in resource-constrained computing platforms, and in applications that demand low-latency responses (such applications that demand real-time responses to user inputs).</p><p id="p-0111" num="0108">(A2) According some implementations of the method of A1, the method further comprises: generating an initial set of training examples; and filtering the initial set of training examples to eliminate one or more initial training examples that fail to satisfy a prescribed quality metric, to produce the training examples in the training data set.</p><p id="p-0112" num="0109">(A3) According to some implementations of the method of A2, the operation of filtering further includes: using a machine-trained classification model to generate a score for a particular initial training example, the particular initial training example including a candidate source item and a candidate auxiliary item, the score identifying how closely the candidate source item matches the candidate auxiliary item; determining that the score fails to satisfy the prescribed quality metric; and eliminating the particular initial training example in response to the determining operation.</p><p id="p-0113" num="0110">(A4) According to some implementations of any of the methods of A1-A3, the operation of updating includes: generating first loss information based on a comparison of the first-path decoder output information and the target item; generating, based on encoder-level contrastive learning, second loss information based on comparison of the first-path encoder output information and the second-path encoder output information; generating, based on decoder-level contrastive learning, third loss information based on a comparison of the first-path decoder output information and the third-path decoder output information; and updating the machine-trained model based on the first loss information, the second loss information, and the third loss information. The use of multi-level contrastive learning further improves the accuracy of the machine-trained model, and accommodates the production of a resource-efficient machine-trained model.</p><p id="p-0114" num="0111">(A5) According to some implementations of the method of A4, the encoder-level contrastive learning generates the second loss information based on a plurality of encoder output information pairs that have been stored in buffer memory, the method updating the buffer memory on a first-in-first-out basis upon receiving a new batch of training examples.</p><p id="p-0115" num="0112">(A6) According to some implementations of the method of A4, the decoder-level contrastive learning generates the third loss information based on a plurality of decoder output information pairs that have been stored in buffer memory, the method updating the buffer memory on a first-in-first-out basis upon receiving a new batch of training examples.</p><p id="p-0116" num="0113">(A7) According to some implementations of the method of A4, the method further includes: updating training weights in the first processing path using back-projection based on the first loss information, the second loss information, and third loss information after processing a batch of training examples; determining updates to training weights to be applied to the second processing path and the third processing path as a function of the training weights that have been updated in the first processing path; and updating the training weights in the second processing path and the third process path based on the determining operation.</p><p id="p-0117" num="0114">(A8) According to some implementations of any of the methods A1-A7, the first-path encoder, the second-path encoder, and the third-path encoder are each transformer-based neural network encoders. Further, the first-path decoder and the third-path decoder are each transformer-based neural network decoders.</p><p id="p-0118" num="0115">(A9) According to some implementations of any of the methods of A1-A8, the first-path decoder successively generates tokens in the first-path decoder output information using auto-regression. Further, the third-path decoder successively generates tokens in the third-path decoder output information using auto-regression.</p><p id="p-0119" num="0116">(A10) According to some implementations of any of the methods of A1-A9, the method further includes, in an inference-stage system: obtaining an input source item; obtaining a particular auxiliary item that conveys knowledge about the input source item that is supplemental to any information conveyed by the input source item itself; forming a particular instance of input information based on a combination of the input source item and the particular auxiliary item; and using the machine-trained model to map the particular instance of input information into a particular generated sequence.</p><p id="p-0120" num="0117">(A11) According to some implementations of the method of A10, the input source item includes a set of terms that describe respective concepts, and wherein the particular generated sequence includes a sentence that relates to the concepts.</p><p id="p-0121" num="0118">(A12) According to some implementations of the method of A10, the input source item includes a query submitted by a user, and wherein the particular generated sequence includes a set of key terms that relate to the query.</p><p id="p-0122" num="0119">(B1) Some aspects of the technology described herein describe a method (e.g., the method <b>1302</b> of <figref idref="DRAWINGS">FIG. <b>13</b></figref>) for using a computing system (e.g., the computing system <b>1502</b> of <figref idref="DRAWINGS">FIG. <b>15</b></figref>) to apply a machine-trained model (e.g., the machine-trained model <b>104</b>). The method includes: obtaining (e.g., in block <b>1304</b>) an input source item (e.g., <b>704</b>); obtaining (e.g., in block <b>1306</b>) a particular auxiliary item (e.g., <b>708</b>) that conveys knowledge about the input source item that is supplemental to any information conveyed by the input source item itself; forming (e.g., in block <b>1308</b>) a particular instance of input information based on a combination of the input source item and the particular auxiliary item; and using (e.g., in block <b>1310</b>) the machine-trained model to map the particular instance of input information into a particular generated sequence (e.g., <b>716</b>). The machine-trained model includes a model encoder (e.g., the model encoder <b>712</b>) that produces model encoder output information based on the particular instance of input information, and a model decoder (e.g., the model decoder <b>714</b>) that produces model decoder output information based on the model encoder output information. The machine-trained model is produced by a training process (e.g., the process <b>1102</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>) that involves encoder-level contrastive learning and decoder-level contrastive learning. The encoder-level contrastive learning involves comparing the model encoder output information with other encoder output information (e.g., <b>132</b>) that is produced by another encoder (e.g., the second- path encoder <b>128</b>). The decoder-level contrastive learning involves comparing the model decoder output information with other decoder output information (e.g., <b>142</b>) that is produced by another decoder (e.g., the third-path decoder <b>140</b>). The use of multi-level contrastive learning improves the accuracy of the machine-trained model, and accommodates the production of a resource-efficient machine-trained model.</p><p id="p-0123" num="0120">(B2) According to some implements of the method of B1, the input source item includes a set of terms that describe respective concepts, and the particular generated sequence includes a sentence that relates to the concepts.</p><p id="p-0124" num="0121">(B3) According to some implementations of the method of B1, the input source item includes a query submitted by a user, and the particular generated sequence includes a set of key terms that relate to the query.</p><p id="p-0125" num="0122">(B4) According to some implementations of any of the methods of B1-B3, the model encoder is a transformer-based neural network encoder, and the model decoder is a transformer-based neural network decoder.</p><p id="p-0126" num="0123">(B5) According to some implementations of any of the methods of B1-B4, the model decoder successively generates tokens in the generated sequence using auto-regression.</p><p id="p-0127" num="0124">(B6) According to some implementations of any of the methods of B1-B5, a particular training example incudes a training-example source item, a training-example target item, a first training-example auxiliary item, and a second training-example auxiliary item. The first and second training-example auxiliary items convey knowledge about the training-example source item that is supplemental to any information conveyed by the training-example source item itself, and the training-example target item represents a transformed counterpart of the target-example source item. The model encoder output information and the model decoder output information are produced, for the particular training example, based a combination of the training-example source item and the first training-example auxiliary item. The other encoder output information is produced, for the particular training example, based a combination of the training-example source item and the training-example target item. The other decoder output information is produced, for the particular training example, based on a combination of the training example source item and the second training-example auxiliary item.</p><p id="p-0128" num="0125">In yet another aspect, some implementations of the technology described herein include a computing system (e.g., computing system <b>1502</b>). The computing system includes hardware logic circuitry (e.g., the hardware logic circuitry <b>1514</b>) that is configured to perform any of the methods set forth herein (e.g., any of the methods of A1-A12, or any of the methods of B1-B5).</p><p id="p-0129" num="0126">In yet another aspect, some implementations of the technology described herein include a computer-readable storage medium (e.g., the computer-readable storage medium <b>1506</b>) for storing computer-readable instructions (e.g., the computer-readable instructions <b>1508</b>). The computer-readable instructions, when executed by one or more hardware processors (e.g., hardware processors <b>1504</b>), perform any of the methods described herein (e.g., methods A1-12, or any of the methods of B1-B5).</p><p id="p-0130" num="0127">More generally stated, any of the individual elements and steps described herein can be combined, without limitation, into any logically consistent permutation or subset. Further, any such combination can be manifested, without limitation, as a method, device, system, computer-readable storage medium, data structure, article of manufacture, graphical user interface presentation, etc. The technology can also be expressed as a series of means-plus-format elements in the claims, although this format should not be considered to be invoked unless the phase &#x201c;means for&#x201d; is explicitly used in the claims.</p><p id="p-0131" num="0128">As to terminology used in this description, the phrase &#x201c;configured to&#x201d; encompasses various physical and tangible mechanisms for performing an identified operation. The mechanisms can be configured to perform an operation using the hardware logic circuity <b>1514</b> of Section C. The term &#x201c;logic&#x201d; likewise encompasses various physical and tangible mechanisms for performing a task. For instance, each processing-related operation illustrated in the flowcharts of Section B corresponds to a logic component for performing that operation.</p><p id="p-0132" num="0129">This description may have identified one or more features as &#x201c;optional.&#x201d; This type of statement is not to be interpreted as an exhaustive indication of features that may be considered optional; that is, other features can be considered as optional, although not explicitly identified in the text. Further, any description of a single entity is not intended to preclude the use of plural such entities; similarly, a description of plural entities is not intended to preclude the use of a single entity. Further, while the description may explain certain features as alternative ways of carrying out identified functions or implementing identified mechanisms, the features can also be combined together in any combination. Further, the term &#x201c;plurality&#x201d; refers to two or more items, and does not necessarily imply &#x201c;all&#x201d; items of a particular kind, unless otherwise explicitly specified. Further, the descriptors &#x201c;first,&#x201d; &#x201c;second,&#x201d; &#x201c;third,&#x201d; etc. are used to distinguish among different items, and do not imply an ordering among items, unless otherwise noted. The phrase &#x201c;A and/or B&#x201d; means A, or B, or A and B. Further, the terms &#x201c;comprising,&#x201d; &#x201c;including,&#x201d; and &#x201c;having&#x201d; are open-ended terms that are used to identify at least one part of a larger whole, but not necessarily all parts of the whole. Finally, the terms &#x201c;exemplary&#x201d; or &#x201c;illustrative&#x201d; refer to one implementation among potentially many implementations.</p><p id="p-0133" num="0130">In closing, the description may have set forth various concepts in the context of illustrative challenges or problems. This manner of explanation is not intended to suggest that others have appreciated and/or articulated the challenges or problems in the manner specified herein. Further, this manner of explanation is not intended to suggest that the subject matter recited in the claims is limited to solving the identified challenges or problems; that is, the subject matter in the claims may be applied in the context of challenges or problems other than those described herein.</p><p id="p-0134" num="0131">Although the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230004588A1-20230105-M00001.NB"><img id="EMI-M00001" he="9.48mm" wi="76.20mm" file="US20230004588A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230004588A1-20230105-M00002.NB"><img id="EMI-M00002" he="7.03mm" wi="76.20mm" file="US20230004588A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method for generating a machine-trained model, comprising:<claim-text>for a particular training example of linguistic information, obtaining a source item, a target item, a first auxiliary item, and a second auxiliary item,</claim-text><claim-text>the first and second auxiliary items conveying knowledge about the source item that is supplemental to any information conveyed by the source item itself, and the target item representing a transformed counterpart of the source item;</claim-text><claim-text>in a first processing path: forming a first instance of input information by combining the source item and the first auxiliary item; transforming the first instance of input information into first-path encoder output information using a first-path encoder; and transforming the first-path encoder output information into first-path decoder information using a first-path decoder;</claim-text><claim-text>in a second processing path: forming a second instance of input information by combing the source item and the target item; and transforming the second instance of input information into second-path encoder output information using a second-path encoder;</claim-text><claim-text>in a third processing path: forming a third instance of input information by combining the source item and the second auxiliary item; and transforming the third instance of input information into third-path decoder output information using a third-path encoder and a third-path decoder; and</claim-text><claim-text>updating training weights based on loss information generated using the first processing path, the second processing path, and the third processing path,</claim-text><claim-text>the method being repeating for additional training examples in a training data set,</claim-text><claim-text>the machine-trained model that is produced by the method corresponding to a trained counterpart of the first-path encoder and the first-path decoder.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>generating an initial set of training examples; and</claim-text><claim-text>filtering the initial set of training examples to eliminate one or more initial training examples that fail to satisfy a prescribed quality metric, to produce the training examples in the training data set.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein said filtering comprises:<claim-text>using a machine-trained classification model to generate a score for a particular initial training example, the particular initial training example including a candidate source item and a candidate auxiliary item, the score identifying how closely the candidate source item matches the candidate auxiliary item;</claim-text><claim-text>determining that the score fails to satisfy the prescribed quality metric; and</claim-text><claim-text>eliminating the particular initial training example in response to said determining.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said updating comprises:<claim-text>generating first loss information based on a comparison of the first-path decoder output information and the target item;</claim-text><claim-text>generating, based on encoder-level contrastive learning, second loss information based on comparison of the first-path encoder output information and the second-path encoder output information;</claim-text><claim-text>generating, based on decoder-level contrastive learning, third loss information based on a comparison of the first-path decoder output information and the third-path decoder output information; and</claim-text><claim-text>updating the machine-trained model based on the first loss information, the second loss information, and the third loss information.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the encoder-level contrastive learning generates the second loss information based on a plurality of encoder output information pairs that have been stored in buffer memory, the method updating the buffer memory on a first-in-first-out basis upon receiving a new batch of training examples.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the decoder-level contrastive learning generates the third loss information based on a plurality of decoder output information pairs that have been stored in buffer memory, the method updating the buffer memory on a first-in-first-out basis upon receiving a new batch of training examples.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein method further includes:<claim-text>updating training weights in the first processing path using back-projection based on the first loss information, the second loss information, and the third loss information after processing a batch of training examples;</claim-text><claim-text>determining updates to training weights to be applied to the second processing path and the third processing path as a function of the training weights that have been updated in the first processing path; and</claim-text><claim-text>updating the training weights in the second processing path and the third process path based on said determining.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the first-path encoder, the second-path encoder, and the third-path encoder are each transformer-based neural network encoders, and</claim-text><claim-text>wherein the first-path decoder and the third-path decoder are each transformer-based neural network decoders.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the first-path decoder successively generates tokens in the first-path decoder output information using auto-regression, and</claim-text><claim-text>wherein the third-path decoder successively generates tokens in the third-path decoder output information using auto-regression.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further including, in an inference-stage system:<claim-text>obtaining an input source item;</claim-text><claim-text>obtaining a particular auxiliary item that conveys knowledge about the input source item that is supplemental to any information conveyed by the input source item itself;</claim-text><claim-text>forming a particular instance of input information based on a combination of the input source item and the particular auxiliary item; and</claim-text><claim-text>using the machine-trained model to map the particular instance of input information into a particular generated sequence.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the input source item includes a set of terms that describe respective concepts, and wherein the particular generated sequence includes a sentence that relates to the concepts.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the input source item includes a query submitted by a user, and wherein the particular generated sequence includes a set of key terms that relate to the query.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A computing system for applying a machine-trained model, comprising:<claim-text>hardware logic circuitry, the hardware logic circuitry corresponding to: (a) one or more hardware processors that perform operations by executing machine-readable instructions stored in a memory, and/or (b) one or more other hardware logic units that perform the operations using a collection of logic gates, the operations including:</claim-text><claim-text>obtaining an input source item;</claim-text><claim-text>obtaining a particular auxiliary item that conveys knowledge about the input source item that is supplemental to any information conveyed by the input source item itself;</claim-text><claim-text>forming a particular instance of input information based on a combination of the input source item and the particular auxiliary item; and</claim-text><claim-text>using the machine-trained model to map the particular instance of input information into a particular generated sequence,</claim-text><claim-text>the machine-trained model including a model encoder that produces model encoder output information based on the particular instance of input information, and a model decoder that produces model decoder output information based on the model encoder output information,</claim-text><claim-text>the machine-trained model being produced by a training process, implemented by the hardware logic circuitry, that involves encoder-level contrastive learning and decoder-level contrastive learning,</claim-text><claim-text>the encoder-level contrastive learning involving comparing the model encoder output information with other encoder output information that is produced by another encoder, and</claim-text><claim-text>the decoder-level contrastive learning involving comparing the model decoder output information with other decoder output information that is produced by another decoder.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computing system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the input source item includes a set of terms that describe respective concepts, and wherein the particular generated sequence includes a sentence that relates to the concepts.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The computing system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the input source item includes a query submitted by a user, and wherein the particular generated sequence includes a set of key terms that relate to the query.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computing system of <claim-ref idref="CLM-00013">claim 13</claim-ref>,<claim-text>wherein the model encoder is a transformer-based neural network encoder, and</claim-text><claim-text>wherein the model decoder is a transformer-based neural network decoder.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The computing system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the model decoder successively generates tokens in the generated sequence using auto-regression.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computing system of <claim-ref idref="CLM-00013">claim 13</claim-ref>,<claim-text>wherein a particular training example incudes a training-example source item, a training-example target item, a first training-example auxiliary item, and a second training-example auxiliary item,</claim-text><claim-text>the first and second training-example auxiliary items conveying knowledge about the training-example source item that is supplemental to any information conveyed by the training-example source item itself, and the training-example target item representing a transformed counterpart of the target-example source item,</claim-text><claim-text>wherein the model encoder output information and the model decoder output information are produced, for the particular training example, based a combination of the training-example source item and the first training-example auxiliary item,</claim-text><claim-text>wherein the other encoder output information is produced, for the particular training example, based a combination of the training-example source item and the training-example target item, and</claim-text><claim-text>wherein the other decoder output information is produced, for the particular training example, based on a combination of the training example source item and the second training-example auxiliary item.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A computer-readable storage medium for storing computer-readable instructions, the computer-readable instructions, when executed by one or more hardware processors, performing a method that comprises:<claim-text>for a particular training example of linguistic information, obtaining a source item, a target item, a first auxiliary item, and a second auxiliary item,</claim-text><claim-text>the first and second auxiliary items conveying knowledge about the source item that is supplemental to any information conveyed by the source item itself, and the target item representing a transformed counterpart of the source item.</claim-text><claim-text>in a first processing path: forming a first instance of input information by combining the source item and the first auxiliary item; transforming the first instance of input information into first-path encoder output information using a first-path encoder; and transforming the first-path encoder output information into first-path decoder information using a first-path decoder;</claim-text><claim-text>in a second processing path: forming a second instance of input information by combing the source item and the target item; and transforming the second instance of input information into second-path encoder output information using a second-path encoder;</claim-text><claim-text>in a third processing path: forming a third instance of input information by combining the source item and the second auxiliary item; and transforming the third instance of input information into a third-path decoder output information using a third-path encoder and a third-path decoder;</claim-text><claim-text>generating first loss information based on a comparison of the first-path decoder output information and the target item;</claim-text><claim-text>generating, using encoder-level contrastive learning, second loss information based on comparison of the first-path encoder output information and the second-path encoder output information;</claim-text><claim-text>generating, using decoder-level contrastive learning, third loss information based on a comparison of the first-path decoder output information and the third-path decoder output information; and</claim-text><claim-text>updating a machine-trained model based on the first loss information, the second loss information, and the third loss information,</claim-text><claim-text>the method being repeating for additional training examples in a training data set,</claim-text><claim-text>the machine-trained model that is produced by the method corresponding to a trained counterpart of the first-path encoder and the first-path decoder.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computer-readable storage medium of <claim-ref idref="CLM-00019">claim 19</claim-ref>,<claim-text>wherein the encoder-level contrastive learning generates the second loss information based on a plurality of encoder output information pairs that have been stored in a first buffer memory, the method updating the first buffer memory on a first-in-first-out basis upon receiving a new batch of training examples, and</claim-text><claim-text>wherein the decoder-level contrastive learning generates the third loss information based on a plurality of decoder output information pairs that have been stored in second buffer memory, the method updating the second buffer memory on a first-in-first-out basis upon receiving the new batch of training examples.</claim-text></claim-text></claim></claims></us-patent-application>