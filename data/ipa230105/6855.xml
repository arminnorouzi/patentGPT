<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230006856A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230006856</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17941700</doc-number><date>20220909</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>KR</country><doc-number>10-2017-0111008</doc-number><date>20170831</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>12</main-group><subgroup>28</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>04847</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0481</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0488</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>75</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>12</main-group><subgroup>282</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6253</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>04847</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>167</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0481</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0488</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>255</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>75</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>17</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD AND SYSTEM OF CONTROLLING DEVICE USING REAL-TIME INDOOR IMAGE</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16930440</doc-number><date>20200716</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11444799</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17941700</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16118904</doc-number><date>20180831</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10742440</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16930440</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>HANWHA TECHWIN CO., LTD.</orgname><address><city>Seongnam-si</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>LEE</last-name><first-name>Ho Jung</first-name><address><city>Seongnam-si</city><country>KR</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>CHO</last-name><first-name>Mi Ran</first-name><address><city>Seongnam-si</city><country>KR</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>KIM</last-name><first-name>Yeon Woo</first-name><address><city>Seongnam-si</city><country>KR</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>HANWHA TECHWIN CO., LTD.</orgname><role>03</role><address><city>Seongnam-si</city><country>KR</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A device and a method for controlling a device using a real-time image are provided. The method includes: receiving an image captured by an image capturing device connected to a network to display the image in real-time; searching for the device that is connected to the network and is controllable; designating, within the image, a setting zone corresponding to the device; receiving a user input; and controlling the device selected according to the user input. A location of the setting zone within the image may be updated according to a change in the image. The user may receive immediate visual feedback on how the devices are being controlled. The user may control a device displayed on the screen on which the real-time indoor image is displayed without having to navigate through different sub-menus for different devices.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="215.90mm" wi="148.08mm" file="US20230006856A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="130.56mm" wi="152.48mm" file="US20230006856A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="78.57mm" wi="156.21mm" file="US20230006856A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="231.39mm" wi="150.11mm" file="US20230006856A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="239.27mm" wi="154.94mm" file="US20230006856A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="89.92mm" wi="156.63mm" file="US20230006856A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="243.67mm" wi="152.65mm" file="US20230006856A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="234.10mm" wi="146.05mm" file="US20230006856A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="243.25mm" wi="117.94mm" orientation="landscape" file="US20230006856A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="237.74mm" wi="157.90mm" file="US20230006856A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="109.05mm" wi="156.21mm" file="US20230006856A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="161.71mm" wi="154.86mm" file="US20230006856A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="161.71mm" wi="125.73mm" file="US20230006856A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="191.43mm" wi="157.99mm" file="US20230006856A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="163.41mm" wi="123.19mm" file="US20230006856A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="201.34mm" wi="125.90mm" file="US20230006856A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="199.31mm" wi="129.79mm" file="US20230006856A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO THE RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is a continuation application of U.S. patent application Ser. No. 16/930,440, filed on Jul. 16, 2020, which is a continuation application of U.S. patent application Ser. No. 16/118,904, filed on Aug. 31, 2018, which claims priority from Korean Patent Application 10-2017-0111008 filed on Aug. 31, 2017 in the Korean Intellectual Property Office, the disclosures of which are incorporated herein in their entireties by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">1. Field</heading><p id="p-0003" num="0002">The present disclosure relates to a method of controlling a device, and a system for controlling a device, and more particularly, to a method of controlling a display device, and a system for controlling a device using a real-time image.</p><heading id="h-0004" level="1">2. Description of the Related Art</heading><p id="p-0004" num="0003">Home automation refers to the automatic control and management of various facilities and electronic appliances at home by utilizing indoor devices and computer systems that are connected with the Internet of Things (IoT). With the introduction of home automation, it is possible to control a plurality of indoor devices with a single remote controller in the room. Further, it is possible to control the indoor devices from a remote location in communications over a wireless network.</p><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an existing home automation system. In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, icons for controlling indoor devices are displayed on a touch screen. Previously, in order to control a device, a user would have to select an icon and then submenus for controlling the device would be displayed, so that the user may control the device. However, it is cumbersome to select from different menus for controlling different indoor devices, and it is inconvenient since the user cannot monitor the statuses of the controlled indoor devices in real time.</p><p id="p-0006" num="0005">In the related art, a plan view or a picture of a house may be displayed on a touch screen, and icons of home appliances may be generated and displayed on the touch screen. The icons displayed on touch screen may be placed in the plan view of the house according to a user's command. When the user selects an icon by touching the touch screen, a home appliance associated with the icon may be identified and controlled (as disclosed in Korean Patent No. 10-0575447).</p><p id="p-0007" num="0006">In such plan view of the house, however, the user could not see how a home appliance is being controlled in real time. Additionally, referring to FIG. 7 of Korean Patent No. 10-0575447, the walls are represented in the plan view of the house, and accordingly a user may not recognize home appliances placed behind the walls. In addition, the plan view of the house where furniture and home appliances are placed has to be updated whenever the furniture or the home appliances are physically moved, which is cumbersome.</p><p id="p-0008" num="0007">In view of the above, what is needed is a home automation system that allows a user to monitor the statuses of the indoor devices in real time without requiring the user to navigate through different submenus for different devices.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0009" num="0008">Aspects of the present disclosure provide a method and system of controlling a device without having to enter different sub-menus for different devices.</p><p id="p-0010" num="0009">Aspects of the present disclosure also provide a method and system of controlling a device that allows a user to receive immediate visual feedback on how a device is being controlled.</p><p id="p-0011" num="0010">It should be noted that objects of the present disclosure are not limited to the above-mentioned object; and other objects of the present invention will be apparent to those skilled in the art from the following descriptions.</p><p id="p-0012" num="0011">According to an aspect of the present disclosure, there is provided a method of controlling a device. The method may include: receiving an image captured by an image capturing device connected to a network to display the image in real time; searching for the device that is connected to the network and is controllable; designating, within the image, a setting zone corresponding to the device; receiving a user input; and controlling the device selected according to the user input. A location of the setting zone within the image may be updated according to a change in the image.</p><p id="p-0013" num="0012">Other particulars of the present disclosure will be described in the detailed description with reference to the accompanying drawings.</p><p id="p-0014" num="0013">According to exemplary embodiments of the present disclosure, at least following effects may be achieved:</p><p id="p-0015" num="0014">According to an exemplary embodiment of the present disclosure, it is possible for a user to monitor the status of a device immediately (e.g., in real time) and to control the device. That is to say, it is possible to provide a more intuitive device control system to the user.</p><p id="p-0016" num="0015">In addition, according to an exemplary embodiment of the present disclosure, the user may control a device on the screen on which the indoor image is displayed without having to navigate through different sub-menus for different devices.</p><p id="p-0017" num="0016">Furthermore, for a device in which a network error occurs frequently, it is possible to receive immediate feedback on the screen on how the device is operating, and thus to monitor that it is operating properly.</p><p id="p-0018" num="0017">It should be noted that effects of the present disclosure are not limited to those described above and other effects of the present disclosure will be apparent to those skilled in the art from the following descriptions.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0019" num="0018">The above and other aspects and features of the present disclosure will become more apparent by describing in detail exemplary embodiments thereof with reference to the attached drawings, in which:</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a device control system according to related arts;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of a device control system according to an exemplary embodiment of the present disclosure;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a method of registering a setting zone for controlling a smart light bulb;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a method of registering a setting zone for controlling a boiler;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows the display in which measurement information is indicated;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref> illustrate a method of controlling a smart light bulb when a user's touch input is a tap;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a method of controlling the smart light bulb precisely;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIGS. <b>9</b> and <b>10</b></figref> show a method of controlling a doorbell camera;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows a method of controlling a cleaning robot;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>12</b></figref> shows a method of controlling a TV;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows a method of adjusting the coordinates of the setting zone in accordance with the movement of the camera;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a flowchart for illustrating a method of controlling a device according to an exemplary embodiment of the present disclosure; and</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIGS. <b>15</b> and <b>16</b></figref> are flowcharts for illustrating a method of controlling a device according to an exemplary embodiment of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION OF THE EXEMPLARY EMBODIMENTS</heading><p id="p-0033" num="0032">Reference will now be made in detail to exemplary embodiments, with reference to the accompanying drawings. In the drawings, parts irrelevant to the description are omitted to clearly describe the exemplary embodiments, and like reference numerals refer to like elements throughout the specification. In this regard, the present exemplary embodiments may have different forms and should not be construed as being limited to the descriptions set forth herein.</p><p id="p-0034" num="0033">Unless otherwise defined, all terms (including technical and scientific terms) used herein have the same meaning as commonly understood by one of ordinary skill in the art to which this disclosure belongs. It will be further understood that terms, such as those defined in commonly used dictionaries, should be interpreted as having a meaning that is consistent with their meaning in the context of the relevant art and/or the present application, and will not be interpreted in an idealized or overly formal sense unless expressly so defined herein.</p><p id="p-0035" num="0034">Terms used herein are for illustrating the embodiments rather than limiting the present disclosure. As used herein, the singular forms are intended to include plural forms as well, unless the context clearly indicates otherwise. Throughout this specification, the word &#x201c;comprise&#x201d; and variations such as &#x201c;comprises&#x201d; or &#x201c;comprising,&#x201d; will be understood to imply the inclusion of stated elements but not the exclusion of any other elements.</p><p id="p-0036" num="0035">Hereinafter, exemplary embodiments of the present disclosure will be described in detail with reference to the accompanying drawings. The word &#x201c;exemplary&#x201d; is used herein to mean &#x201c;serving as an example or illustration.&#x201d; Any aspect or design described herein as &#x201c;exemplary&#x201d; is not necessarily to be construed as preferred or advantageous over other aspects or designs.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a device control system <b>1</b> according to an exemplary embodiment of the present disclosure. According to the exemplary embodiment of the present disclosure, the device control system <b>1</b> includes an image receiver <b>11</b>, a display <b>12</b>, a controller <b>13</b>, a device searcher <b>14</b>, an input interface <b>16</b>, and a setting zone creator <b>15</b>. The device control system <b>1</b> may further include a mapper, a matcher, a network, an information display, or an object recognizer. The various components, units and modules shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> and other figures may be implemented with hardware (e.g., circuits, microchips, processors, etc.), software (e.g., instructions, commands, programs, applications, firmware, etc.), or a combination of both hardware and software.</p><p id="p-0038" num="0037">The device control system <b>1</b> according to the exemplary embodiment of the present disclosure may be a home automation system and may control devices placed indoors by using a real-time indoor image acquired from a camera used as an image capturing device.</p><p id="p-0039" num="0038">The image capturing device may be a camera or an image sensor capable of capturing images. The image capturing device may be connected to a network. The devices to be controlled may be connected to the network. By using the real-time image from the image capturing device connected to the same network as the devices to be controlled, it is possible to efficiently receive the information of the devices to be controlled and the image or to efficiently control the devices including the image capturing device. Accordingly, it is possible to prevent an error that may arise when data is transmitted/received over different networks or connections due to by a difference between the networks. In addition, information or images of devices can be integrated and transmitted from a single device or server connected over a network, allowing for various network configurations.</p><p id="p-0040" num="0039">The image capturing device may capture an image of, but is not limited to, an indoor space. It is to be understood that the image capturing device may capture an image of any location where the device to be controlled is located. In the following description, the image capturing device captures an image of a room as an example.</p><p id="p-0041" num="0040">The image receiver <b>11</b> receives a real-time indoor image from the image capturing device (e.g., a camera). The image receiver <b>11</b> may receive the real-time indoor image via wired communications or wireless communications. In addition, the image receiver <b>11</b> may receive the real-time indoor image directly from the camera or may receive the image from a server after it is transmitted from the camera to the server.</p><p id="p-0042" num="0041">Additionally, when the camera has the capability of measuring the indoor environments such as temperature and humidity, the image receiver <b>11</b> can receive the measured information together with the real-time indoor image.</p><p id="p-0043" num="0042">The display <b>12</b> displays the real-time indoor image received through the image receiver <b>11</b>. When the display <b>12</b> is implemented as a touch screen, it can display an indoor image and receive input from a user. The display <b>12</b> may display a real-time indoor image fully on the screen or may display a real-time indoor image on a predetermined part of the screen while displaying icons used for controlling the indoor devices, etc. in the other part thereof.</p><p id="p-0044" num="0043">The input interface <b>19</b> receives the user's input. It may receive the user's touch input or voice. The input interface <b>19</b> may receive an input from a mouse, a keyboard, a microphone, a camera, etc. The input interface <b>19</b> may receive an input via another input device such as a keyboard and a mouse or may receive an input by way of gesture recognition or the like. When the display <b>12</b> is implemented as a touch screen, the input interface <b>19</b> may receive a touch input on the touch screen from the user. The input interface <b>19</b> may include a voice recognizer for analyzing the user's voice when the user's voice is received and identifying the user's command. By recognizing the user's voice, a selection command for selecting one from among the devices and a control command for controlling the selected device may be identified. Alternatively, a control command may be identified without the user specifically selecting a device, and a device that is to perform the derived control command may be identified. For example, a selection command for selecting a light (e.g., a light fixture) may be derived by analyzing a user's utterance of &#x201c;light,&#x201d; &#x201c;select light,&#x201d; or the like. Similar to the selection command, a control command for controlling the light may be identified by analyzing the user's utterance such as &#x201c;turn on the light,&#x201d; &#x201c;change the color,&#x201d; or the like. Alternatively, a control command for the light may be derived even without selecting the light by directly analyzing the utterance of &#x201c;turn off the light.&#x201d;</p><p id="p-0045" num="0044">A user may make an input by touching the screen with a finger, which hereinafter is referred to as a touch input. The touch input may include a tap gesture of a user touching the screen with a single finger for a short period of time, a long press gesture (also known as a tap-and-hold gesture) of a user touching the screen with a single finger for a long period of time (e.g., longer than a threshold time duration) and a multi-touch gesture of a user touching the screen with two or more fingers simultaneously.</p><p id="p-0046" num="0045">When the user's touch input received by the input interface <b>19</b> falls within a setting zone associated with an indoor device in the image displayed on the display <b>12</b>, the controller <b>13</b> generates a control signal for controlling the device. The setting zone refers to an area (e.g., a hotspot) on the display <b>12</b> designated by the user for controlling a specific device that is mapped to the area. The location of the setting zone of an indoor device to be controlled on the screen may be either the same as or different from the location of the indoor device on the screen.</p><p id="p-0047" num="0046">The controller <b>13</b> may generate a control signal for controlling an indoor device upon receiving a user's touch input. For example, when a click gesture is input on the touch screen, the controller <b>13</b> may generate a control signal for switching an indoor device from the on-state to the off-state or vice versa. Alternatively, when a multi-touch gesture is input on the touch screen, the controller <b>13</b> may generate a control signal for switching a plurality of devices from the on-state to the off-state or vice versa. The controller <b>13</b> may be a processor, a central processing unit (CPU), an application processor (AP), etc.</p><p id="p-0048" num="0047">The device searcher <b>14</b> searches for devices that can be controlled by the controller <b>13</b>. A device to be searched for may be a device located in the area captured by the image capturing device or may be a device connected to the image capturing device over the network. The device may be discovered by searching for the network. Alternatively, the device may be searched for by performing object recognition on the received image.</p><p id="p-0049" num="0048">The setting zone creator <b>15</b> may set (e.g., define, assign, designate, etc.) a setting zone for the discovered devices. In order to select a device to be controlled from the image, a setting zone may be set to select an area where the device is placed. The setting zone may be set upon receiving a user's input or by performing object recognition in the image. Alternatively, the setting zone may be set by using the location of the device which is identified through data transmission/reception over the network.</p><p id="p-0050" num="0049">The user may interact with the user interface more intuitively when the zone is defines as the location of the device. The setting zone may be set in the shape of the device identified in the image or may be set to have a size including all or part of the shape of the device. If the setting zone is too small or too large, it may be difficult to select the device or it may obstruct the selection of another device. Accordingly, the setting zone may have a predetermined size. Alternatively, the position, shape and size of the setting zone may be adjusted by the user.</p><p id="p-0051" num="0050">The setting zone creator <b>15</b> determines whether the discovered device is recognized in the image. If such a case, the location of the display device in the image is set as the setting zone of the device. In other cases, any location of the device in the image that does not overlap with the setting zones of other devices may be set as the setting zone of the device.</p><p id="p-0052" num="0051">The devices displayed in the image may be extracted by performing object recognition. The areas associated with the locations of the devices extracted by object recognition may be set as the setting zones of the extracted devices. Alternatively, the information on the devices extracted by the object recognition may be indicated on the display <b>12</b> and provided to the user, and to query whether or not to set the setting zones for the devices. In addition, the setting zone creator <b>15</b> may set a setting zone for selecting a device not identified in the image. If a device is not identified in the image depending on the capturing angle or range (e.g., field of view) of the camera, or a device is located outside the room, an arbitrary (e.g., randomly chosen) area may be set as the setting zone for the device. Candidate areas for the setting zone of the device may be displayed on the display <b>12</b> so that the user can select an area among them.</p><p id="p-0053" num="0052">The location of the setting zone within the image may be changed (e.g., updated) according to the change of the image. The change in the image may include a change in the angle of view of the image or a change in the location of a device within a given angle of view. The angle of view of the image capturing device may be changed according to panning, tilting and zooming (PTZ) control of the image capturing device or a device to be controlled may be movable like a cleaning robot (e.g., a robot vacuum cleaner) or the shape may vary, as well as when the image capturing device captures an image with the same angle of view or a device to be controlled is stationary. As a result, there may be a change in the image. When the setting zone is set to a location on the image where the device is recognized, the location of the device in the image changes according to the change of the image, and accordingly the setting zone may be changed in position or shape according to the change of the image. If the device moves out of frame from the image due to a change in the image, the setting zone may be moved to a predetermined location for the device. For example, the predetermined location may be a top-left corner or a top-right corner. Alternatively, the display <b>12</b> may display areas other than the area of the displayed image. At this time, the setting zone of the device which has moved out of frame from the image may be moved to an area other than the previously designated area of the image. Alternatively, a mini-map for the entire area or a partial area of the capturing range of the image capturing device may be displayed, such that the setting zone may be moved on the mini-map in order to continue to display the setting zone of the device which has moved out of frame from the current image.</p><p id="p-0054" num="0053">The object recognizer may extract a device from the image and recognize it or may track the location of the device in the image. The device may be extracted as the image receiver analyzes the received image and recognizes the object. When the capturing angle or range of the camera is changed by the camera operator, the setting zone in the image may be changed. When this happens, the location of the setting zone with respect to the location of the device changed in the image may be changed by performing object recognition to track the new location. In addition, if the device is a moving device (e.g., a robot vacuum cleaner), the location of the device may be changed according to the operation of the device and the location of the device in the image may be tracked by performing object recognition.</p><p id="p-0055" num="0054">The display <b>12</b> may display an image received in real-time by the image receiver to allow the user to monitor the information on the current location and status of the device and receive immediate feedback on how the devices are being operated and controlled. In addition, the display <b>12</b> may display at least a part of the setting zone of the device selected upon receiving the user's input on the image. The display <b>12</b> may display the setting zone to allow the user to select the setting zone. In addition, the display <b>12</b> may indicate at least one piece of information on the device on the setting zone. The information on the device may include identification information, function information, status information indicating an operating status of the device, measurement information indicating a value measured by the device and image information captured by a camera mounted on the device.</p><p id="p-0056" num="0055">When receiving a user's input, if the device selected in accordance with the user's input is not located in the currently displayed image, the controller <b>13</b> may control the image capturing device so that the image capturing device captures the device selected in accordance with the user's input while the device is controlled or after the device is controlled. When receiving the user's input, if the device selected in accordance with the user's input is not located in the currently displayed image, the display <b>12</b> may display the setting zone of the device on the image while the device is controlled or after the device is controlled. When the device is located in the image, the user may receive immediate feedback on how the device is being operated and controlled. However, if the device is not located on the currently displayed image, the user cannot receive immediate feedback. Therefore, the controller <b>13</b> controls the image capturing device so that the image capturing device captures the device selected in accordance with the user's input while the device is controlled or after the device is controlled, to allow the user to receive immediate feedback on how the device is being operated or controlled. For example, a voice input of &#x201c;turn on the light&#x201d; may be received from the user when the light is not located on the image. Then, the controller <b>13</b> controls the image capturing device so that it focuses on (e.g., pans, tilts, or zooms toward) the light to see the light is turned on or to see the light has been turned on.</p><p id="p-0057" num="0056">Alternatively, when receiving the user's input, if the device selected in accordance with the user's input is not located in the currently displayed image, the display <b>12</b> may display the setting zone of the device on the image while the device is controlled or after the device is controlled, allowing the user to receive immediate feedback on how the device is being operated or controlled. It is possible to display the setting zone on the currently displayed image without the control of the image capturing device, so that the user may receive immediate feedback on how the device is being operated or controlled. For the above-described light, if a voice input of &#x201c;turn on the light&#x201d; from a user when the light or the setting zone of the light is not located in the image, the setting zone of the light may be displayed so that the user may monitor whether the light is turned on or has been turned on.</p><p id="p-0058" num="0057">In order for the controller <b>13</b> to generate a control signal for controlling the device corresponding to the setting zone according to the touch input applied to the setting zone, the setting zone may be matched with the device. To this end, the mapper and the matcher may be used.</p><p id="p-0059" num="0058">The mapper may map the coordinates of the setting zone designated by the user on the screen. The matcher may match the mapped coordinates of the setting zone to the indoor device to be controlled.</p><p id="p-0060" num="0059">When the controller <b>13</b> generates a control signal for controlling the device matched in the matcher according to the touch input applied to the coordinates mapped in the mapper, the network may transmit the generated control signal to the indoor device. In addition, the network may receive the information of the indoor device from the indoor device.</p><p id="p-0061" num="0060">The network may use wireless communications for data transmission with the indoor device. The wireless communications may include Wi-Fi, Bluetooth, ZigBee, Infrared Data Association (IrDA) and radio-frequency identification (RFID).</p><p id="p-0062" num="0061">The information on the device received by the network may include the status information indicating an operating state of the device, measurement information indicating a value measured by the device, and image information captured by a camera mounted on the device. For example, if the device is a smart light bulb, the status information may include a light-off state, a light-on state, the color of the light, etc. If the device is an air conditioner, the measurement information may include room temperature, humidity, etc. measured by the air conditioner. When the device is a doorbell or an outdoor camera (e.g., closed-circuit television camera), the image information captured by a camera mounted on the doorbell or the outdoor camera may include a real-time image (e.g., still image, video footage) or recorded images.</p><p id="p-0063" num="0062">The information display may indicate on the display <b>12</b> the status information, the measurement information, the image information and the guide information for operation of the device control system <b>1</b> received from the network. In addition, the information display may indicate on the display <b>12</b> the spatial information or the indoor environment information such as temperature and humidity measured by the camera in the image receiver <b>11</b>.</p><p id="p-0064" num="0063">The information display may collect the received measurement information of the indoor devices and indicate it on the display <b>12</b>. For example, when the network receives the indoor temperature and the humidity from the air conditioner as the measurement information from an air cleaner, the information display may combine the received measurement information to indicate the indoor temperature, humidity and air quality on the display <b>12</b>.</p><p id="p-0065" num="0064">In addition, the information display may indicate the received status information of the device in the vicinity of the mapped coordinates of the setting zone matched with the device as an overlay view. For example, when the device is an air conditioner, the wind speed and direction information of the air conditioner may be indicated in the vicinity of the mapped coordinates of the setting zone matched with the air conditioner as an overlay view.</p><p id="p-0066" num="0065">In addition, the information display may display, as the guide information required for the operation of the device control system <b>1</b>, information on how to use the system, information for guiding the control menu of the indoor devices, etc.</p><p id="p-0067" num="0066">The camera operator may control the operation of the camera. When the camera for capturing a real-time indoor image is a PTZ camera, the camera operator may control the operations of the PTZ camera such as panning, tilting and zooming. The camera operator may include a swivel, a gimbal, a slider, a motor, etc. The camera operator may include a function of individually controlling the panning, tilting and zooming of the PTZ camera, a function of controlling the PTZ camera so that it has predetermined values of the panning, tilting and zooming, and a function of restoring the PTZ camera to back to the initial state. For a 360-degree camera or a fish-eye camera, it is possible to control the devices for all directions without controlling the camera's operation.</p><p id="p-0068" num="0067">When the indoor image captured by the camera changes by controlling the camera operation unit, the mapped coordinates do not change but the location of the matched device on the image changes, and thus the setting zone may be positioned at a location on the screen different from the location designated by the user. The mapping unit may adjust the mapped coordinates of the setting zone according to the movement of the camera so that the setting zone can be positioned at the location on the screen that the user has designated even if the indoor image captured by the camera is changed by the camera operation unit.</p><p id="p-0069" num="0068">When the device selected upon receiving the input includes a display, the controller <b>13</b> may generate a control signal so that the selected device displays an image signal that the user desires to display on the display of the selected device upon receiving the user's input. When the device includes a display, such as a TV and a monitor, the controller <b>13</b> may control the device so that an image signal that the user desires is displayed on the display. If the image signal is a real-time image captured by a camera mounted on a user terminal or another capturing device or the like, video call with an object located indoors may be possible. In doing so, an audio signal together with the video signal may be sent out. Alternatively, images such as pictures and videos may be displayed.</p><p id="p-0070" num="0069">When the device selected upon receiving the input is movable and the input interface <b>16</b> receives the location to which the selected device is to be moved, the controller <b>13</b> may generate a control signal for moving the selected device to the received location. By receiving the location to which the device is moved, which is movable or needs to be moved such as a robot cleaner (e.g., a robot vacuum cleaner) and an air cleaner (e.g., an air purifier), the device can be moved. In addition to the location to which the device is to be moved, the movement path may be input by a user via dragging (e.g., a tap and drag gesture) or the like.</p><p id="p-0071" num="0070">The controller <b>13</b> may generate a control signal for simultaneously controlling a plurality of devices in the image upon receiving the input. It is possible to simultaneously control a plurality of devices with one input, such as entering a power saving mode in the user's absence from home (e.g., a vacation mode). For example, the devices which do not need to operate while nobody is present at home, such as a TV, an air conditioner and light may be turned off or may enter the power saving mode altogether.</p><p id="p-0072" num="0071">In the foregoing description, the configuration of the device control system <b>1</b> according to the exemplary embodiment of the present disclosure has been described. Based on the above description, a method of operating the device control system <b>1</b> will be described below. In order to describe the method in detail, a smart light bulb will be described as an example of the indoor device. It is to be understood that the smart light bulb is merely an example and the indoor device controlled by the device control system <b>1</b> is not limited to the smart light bulb.</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a method of registering a setting zone for controlling a smart light bulb. In <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the smart light bulb is an indoor device existing in the real-time indoor image displayed on the display <b>12</b>.</p><p id="p-0074" num="0073">The method of registering a setting zone for controlling the smart light bulb will be described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Initially, guide information <b>21</b> (e.g., instruction text) for guiding the user to designate a setting zone for controlling the smart light bulb is indicated by the information display. The guide information <b>21</b> may be located at the upper end of the display <b>12</b>. The setting zone for selecting or controlling the smart light bulb may be set by performing object recognition. Alternatively, when the user makes a touch input <b>22</b> for designating the setting zone for controlling the smart light bulb according to the guide information <b>21</b>, the input interface <b>16</b> receives the touch input and transmits it to the mapper. The setting zone may be a point on the screen where the smart light bulb is positioned. The mapper maps the coordinates of the setting zone <b>23</b> designated by the user on the screen. The matching unit receives the mapped coordinates of the setting zone <b>23</b> from the mapper and matches the received coordinates with the smart light bulb.</p><p id="p-0075" num="0074">According to an exemplary embodiment of the present disclosure, the device control system <b>1</b> may control not only the indoor devices placed in the real-time indoor image but also indoor devices or outdoor devices located outside the real-time indoor image. A method of registering a setting zone to control an indoor device located outside a real-time indoor image will be described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a method of registering a setting zone for controlling a boiler.</p><p id="p-0077" num="0076">A method of registering a setting zone for controlling a boiler or a water heater that is placed outside the indoor image will be described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Initially, the information display indicates guide information <b>31</b> that guides registration of the setting zone for controlling the boiler. The user may touch an arbitrary area on the real-time indoor image displayed on the display <b>12</b> to designate the setting zone. In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the point where the left wall of the room is located in the real-time indoor image displayed on the display <b>12</b> is designated as the setting zone <b>33</b> by the user. The input interface <b>16</b> transmits a touch input <b>32</b> on the arbitrary setting zone <b>33</b> input by the user to the mapper, and the mapper maps the coordinates of the setting zone <b>33</b> on the touch screen <b>12</b>. The matcher receives the coordinates of the setting zone <b>33</b> mapped from the mapper and matches the received coordinates with the boiler.</p><p id="p-0078" num="0077">Next, a method of indicating the measurement information on the indoor devices on the display <b>12</b> by the information display will be described with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>. <figref idref="DRAWINGS">FIG. <b>5</b></figref> shows the display <b>12</b> in which measurement information is indicated.</p><p id="p-0079" num="0078">The network may receive measurement information on the temperature, humidity, air quality, etc. and the like in the room measured by the indoor devices such as an air conditioner, an air purifier, a humidifier and a thermometer, and may transmit the measurement information to the information display.</p><p id="p-0080" num="0079">The information display may combine the pieces of the transmitted measurement information <b>41</b> and indicate it on the display <b>12</b>. In doing so, the information display may display the measurement information <b>41</b> on the display <b>12</b> such that the measurement information does not overlap with the setting zone. In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the measurement information <b>41</b> may be located at the upper left portion of the screen so that it does not overlap with the setting zone.</p><p id="p-0081" num="0080">In the example shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the information measured by the indoor devices is used as the measurement information indicated on the information display. If the camera that captures the indoor image has the capability of measuring temperature, humidity, air quality, etc., it is possible to receive the measurement information measured by the camera via the image receiver <b>11</b> and indicate it.</p><p id="p-0082" num="0081">Hereinafter, a method of controlling an indoor device pursuant to a user's touch input will be described with reference to <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref>.</p><p id="p-0083" num="0082"><figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref> illustrate a method of controlling a smart light bulb when a user's touch input is a tap.</p><p id="p-0084" num="0083">Referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the input interface <b>19</b> receives a touch input <b>52</b> of tapping the setting zone <b>51</b> of the smart light bulb from the user. The controller <b>13</b> receives the touch input from the touch screen <b>12</b> and receives from the network the status information indicative of the off-state of the smart light bulb. Since the touch input is a tap and the smart light bulb is in the off-state, the controller <b>13</b> may generate a control signal for switching the smart light bulb into the on-state. The controller <b>13</b> transmits the generated control signal to the device over the network, and accordingly the smart light bulb is switched into the on-state. In doing so, when the user taps the setting zone of the smart light bulb, the sign &#x201c;ON&#x201d; <b>53</b> may be displayed by the information display for a short period of time (e.g., flash) at the lower end of the display <b>12</b> as an indication that the smart light bulb is switched into the on-state.</p><p id="p-0085" num="0084">Referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the input interface <b>19</b> receives a touch input <b>61</b> of tapping the setting zone of the smart light bulb from the user. The controller <b>13</b> receives the touch input from the touch screen <b>12</b> and receives from the network the status information indicative of the on-state of the smart light bulb. Since the touch input is a touch and the smart light bulb is in the on-state, the controller <b>13</b> may generate a control signal for switching the smart light bulb into the off-state. The controller <b>13</b> transmits the generated control signal to the device over the network, and accordingly the smart light bulb is switched into the off-state. Also in this time, in response to the input of clicking the setting zone of the smart light bulb on the display <b>12</b>, the sign &#x201c;OFF&#x201d; <b>62</b> may be displayed by the information display for a short period of time (e.g., flash) at the lower end of the display <b>12</b> as an indication that the smart light bulb is switched into the off-state.</p><p id="p-0086" num="0085">As the smart light bulb is switched into the off-state, the real-time indoor image displayed on the display <b>12</b> becomes dark, and thus it may be difficult to distinguish the locations of the indoor devices on the image one from another. In this regard, the information display may indicate information <b>63</b> (e.g., labels) on the devices matched with the mapped coordinates, so that the user can readily distinguish the devices one from another even in the dark image. The information on the indoor devices indicated by the information display includes the names of the devices, images of the devices, icons, etc.</p><p id="p-0087" num="0086">Hereinafter, a method of controlling the smart light bulb precisely will be described with reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a method of controlling the smart light bulb precisely.</p><p id="p-0089" num="0088">Referring to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, before precise control, the smart light bulb is displayed on the display <b>12</b> in the on-state. The input interface <b>19</b> receives a long press gesture from a user with as a touch input <b>71</b> in the setting zone of the smart light bulb. The input interface <b>19</b> delivers the input of the long press gesture to the information display. The information display displays related control items <b>72</b> of the smart light bulb on the display <b>12</b> accordingly for finer control of the smart light bulb. The information display may indicate the related control items <b>72</b> in the vicinity of the coordinates to which the setting zone of the smart light bulb is mapped as an overlay view. The types of related control items <b>72</b> may vary depending on the type of the indoor devices. In <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the related control items <b>72</b> of the smart light bulb includes the color, the auto-off and the illuminance. When the user taps on the color item among the related control items <b>72</b>, the information display may display a color control panel <b>73</b> on the display <b>12</b> to adjust the color of the smart light bulb. When the user taps on a desired color on the color control panel <b>73</b>, the controller <b>13</b> receives the touch input <b>74</b> of clicking from the input interface <b>19</b> and generates a control signal for changing the color of the smart light bulb according to the selected color. The generated control signal is transmitted to the smart light bulb over the network, and the color of the smart light bulb is changed accordingly. The user can receive immediate visual feedback on the color of the smart light bulb being changed via the real-time indoor image displayed on the display <b>12</b>.</p><p id="p-0090" num="0089">An image captured by another camera may be connected to the currently displayed image. By doing so, it is possible to quickly monitor an event that occurs on a single screen to control it, without having to switch the screen to the image captured by another camera or to divide the screen into sub-images captured by several cameras. A doorbell camera may be employed as an example of another camera that connects the images.</p><p id="p-0091" num="0090">Referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the doorbell camera is located outdoors. The setting zone of the doorbell camera in the image may be set as a main door or a front door <b>81</b> that leads to the outside. Any other area may also be set as the setting zone. In order to register the doorbell camera and to set the setting zone of the doorbell camera, the user may touch the screen or draw a shaped around a desired setting zone, and then the setting zone is saved. The setting zone may be selected later on to retrieve the functions associated with the doorbell camera. In addition to the way that the user designates the area on the screen manually, the location of the setting zone of the doorbell camera may be recommended to the user by performing object recognition which automatically recognizes an object (e.g., a door) displayed on the image.</p><p id="p-0092" num="0091">When the input interface <b>19</b> receives the touch input <b>82</b> for selecting the setting zone <b>81</b> of the doorbell camera from the user, an outside image <b>83</b> captured by the doorbell camera received from the doorbell camera may be displayed on the display <b>12</b>. Even if there is no input from the user, the image of the doorbell camera may be displayed on the display <b>12</b> when information indicative of an event is received from the doorbell camera. Upon receiving an input to open the door lock from the user later on, the door lock <b>84</b> may be opened, and the information <b>85</b> may be displayed on the display.</p><p id="p-0093" num="0092">As shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the image of the doorbell camera may be moved to a desired location <b>91</b> or the size of the image of the doorbell camera may be adjusted (<b>92</b>). In addition, while the image of the doorbell camera is displayed, the functions associated with the doorbell camera may be used. For example, zoom-in <b>93</b>, zoom-out <b>95</b>, face recognition <b>94</b>, object recognition, pan &#x26; tilt, two-way calling, and the like are possible. When the function of the camera is used, the information may be displayed on the display. When the image of the doorbell camera is displayed, recorded and saved image as well as real-time image may be displayed.</p><p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows a method of controlling a cleaning robot.</p><p id="p-0095" num="0094">Referring to <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the setting zone of a cleaning robot may be set to coincide with the location of the cleaning robot. An arbitrary area may be designated as the setting zone as the cleaning robot is a moving device.</p><p id="p-0096" num="0095">When the input interface <b>19</b> receives a touch input <b>102</b> for selecting the setting zone <b>101</b> of the cleaning robot from the user, the operation of the cleaning robot may be executed or interrupted. When information is received from the cleaning robot, the information <b>103</b> may be displayed on the display <b>12</b>. The information may include status information or measurement information. For example, information such as progress or completion of cleaning, battery status, filter status information and operation error may be displayed. In addition, it is possible to set a target point <b>105</b> by changing the direction of the cleaning robot, by receiving a location <b>104</b> to move, or by receiving a moving route. Further, information for locating the cleaning robot may be displayed on the display <b>12</b>. In doing so, it is possible to accurately locate and guide the robot by using object recognition. Even if the cleaning robot is hidden behind another object and not visible in the image, the location of the cleaning robot may be determined by location tracking.</p><p id="p-0097" num="0096"><figref idref="DRAWINGS">FIG. <b>12</b></figref> shows a method of controlling a TV.</p><p id="p-0098" num="0097">Referring to <figref idref="DRAWINGS">FIG. <b>12</b></figref>, when the input interface <b>19</b> receives a touch input <b>112</b> for selecting the setting zone <b>111</b> of a TV, it is possible to turn on/off the TV. In addition, it is possible to adjust the volume or change the channel, for example, by a dragging gesture.</p><p id="p-0099" num="0098">When a user is outside and a dog is at home alone, the user may monitor where the dog is located from the image displayed on the display. In addition, the user may turn on the TV to provide entertainment for the dog or check the temperature/humidity of the house and adjust it in real time.</p><p id="p-0100" num="0099">In addition, when the input interface <b>19</b> receives a touch input <b>114</b> for the user to select the setting zone <b>113</b> of a telephone, the user may make a phone call. At this time, if the TV is selected together with the telephone, a video call may be made through the TV. The information <b>115</b> indicating that the phone call and the video call are made may be displayed. The image captured by the user's mobile terminal may be transmitted to the TV so that bidirectional video communications may be performed. Video communications is possible using a device including a display other than the TV.</p><p id="p-0101" num="0100">In addition to the indoor device controlled by the touch inputs of tap and long press gestures as described above, a plurality of indoor devices may be simultaneously controlled by a multi-touch input.</p><p id="p-0102" num="0101">When a touch input of dragging down the screen from top to bottom with multiple fingertips is inputted, the controller <b>13</b> may generate a control signal for switching the plurality of indoor devices into the off-state by receiving the touch input from the input interface <b>19</b>. Alternatively, when a touch input of dragging up the screen from bottom to top with multiple fingertips is inputted, the controller <b>13</b> may generate a control signal for switching the plurality of indoor devices into the on-state by receiving the touch input from the input interface <b>19</b>.</p><p id="p-0103" num="0102">The plurality of indoor devices may include all the devices matched with the coordinates mapped to the real-time indoor image displayed on the display <b>12</b>. Alternatively, the plurality of indoor devices may be designated by the user in advance. The multi-touch gesture for controlling the indoor devices may be a touch input of touching the screen with three fingers.</p><p id="p-0104" num="0103"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows a method of adjusting the coordinates of the setting zone in accordance with the movement of the camera.</p><p id="p-0105" num="0104">Referring to <figref idref="DRAWINGS">FIG. <b>13</b></figref>, when the camera pans to the left by the camera operation unit, the indoor image displayed on the display <b>12</b> changes according to the operation of the camera. On the contrary, the mapped coordinates <b>121</b> of the setting zone are the coordinates on the screen, and thus do not change with the movement of the camera. As a result, the mapped coordinates <b>121</b> of the setting zone may not match the location where the matched device is displayed.</p><p id="p-0106" num="0105">In order to prevent such a situation, the location of the device may be recognized via object recognition, and the location of the setting zone may be set according to the location of the recognized device. Alternatively, the mapping unit may convert the mapped coordinates <b>121</b> to newly created coordinates <b>122</b> so that the mapped coordinates <b>121</b> of the setting zone are located on the matched device on the screen according to the movement of the camera. The setting zone may be located on the new coordinates <b>122</b>, and the user may touch the setting zone on the new coordinates <b>122</b> to control the matched device.</p><p id="p-0107" num="0106">In order to create the new coordinates <b>122</b> of the setting zone, the mapping unit analyzes the positional change of the matched device on the image before and after the movement of the camera by the image processing, to create the new coordinates <b>122</b> by moving the mapped coordinates <b>121</b> according to the changed location. Alternatively, the mapping unit may calculate the distance that the matched device has moved in accordance with the movement of the camera by using the moving angle of the camera and the distance between the camera and the matched device, to move the mapped coordinates <b>121</b> accordingly to create the new coordinates <b>122</b>.</p><p id="p-0108" num="0107"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a flowchart for illustrating a method of controlling a device according to an exemplary embodiment of the present disclosure. <figref idref="DRAWINGS">FIGS. <b>15</b> and <b>16</b></figref> are flowcharts for illustrating a method of controlling a device according to the exemplary embodiment of the present disclosure. The detailed description of the method corresponds to the detailed description of the device control system described above with reference to <figref idref="DRAWINGS">FIGS. <b>1</b> to <b>13</b></figref>; and, therefore, the redundant description will be omitted. It should be understood that, for any method or process discussed herein, there can be additional or alternative steps performed in similar or alternative orders, or in parallel, within the scope of the various embodiments unless otherwise stated.</p><p id="p-0109" num="0108">In operation S<b>11</b>, an image captured by the image capturing device connected to the network is received and displayed in real time. In operation S<b>12</b>, a device which is connected to the network and is able to be controlled is searched for. In operation S<b>13</b>, a setting zone of the discovered device on the image is set. In operation S<b>14</b>, a user input is received. In operation S<b>15</b>, the selected device is controlled according to the user's input. The setting zone is changed according to the change of the image. The change of the image includes one of a change in the angle of view of the image or a change of the location of the device within a given angle of view.</p><p id="p-0110" num="0109">The setting zone may be set upon receiving a user's input or by object recognition in an image. The method may further include displaying at least a part of the setting zone of the device selected upon receiving the user's input on the image.</p><p id="p-0111" num="0110">The method may further include determining whether the discovered device is recognized (e.g., visible, featured, etc.) in the image. If it is determined that the device is recognized in the image, the setting the setting zone in the image may include setting the location in the image where the device is recognized as the setting zone of for the device. If it is determined that the device is not recognized in the image, the setting the setting zone in the image may include setting a predetermined location in the image that does not overlap with the setting zones of the other devices as the setting zone of the device.</p><p id="p-0112" num="0111">If the device selected in accordance with the user input is not recognized in the currently displayed image, in operation S<b>21</b>, the image capturing device may be controlled so that it captures the device selected in accordance with the user's input while the device is controlled or after the device has been controlled. If the setting zone of the device selected in accordance with the user's input is not located in the currently displayed image, in operation S<b>31</b>, the setting zone of the device may be displayed on the image while the device selected is controlled or after the device has been controlled.</p><p id="p-0113" num="0112">At least one piece of information on the device may be displayed on the setting zone. The user input may be a touch input or a voice input.</p><p id="p-0114" num="0113">Exemplary embodiments of the present disclosure may be embodied in computer-readable code (e.g., instructions) on a computer-readable storage medium. A computer-readable storage medium may include any type of recording device in which data readable by a computer system is stored.</p><p id="p-0115" num="0114">An example of the computer-readable storage medium may include a read-only memory (ROM), a random access memory (RAM), a compact disc read-only memory (CD-ROM), a magnetic tape, a floppy disk, an optical data storage, or the like. In addition, the computer-readable storage medium may be distributed in computer systems connected with one another over a network, such that the computer-readable code may be stored and executed in a distributed manner. Functional programs, codes and code segments for embodying the present disclosure may be easily deduced by computer programmers in the art.</p><p id="p-0116" num="0115">It will be evident to those skilled in the art that various modifications and changes may be made in the exemplary embodiments of the present disclosure without departing from the technical idea or the gist of the present disclosure. Therefore, it should be understood that the above-mentioned embodiments are not limiting but illustrative in all aspects. It should be understood that the drawings and the detailed description are not intended to limit the present disclosure to the particular forms disclosed herein, but on the contrary, the intention is to cover all modifications, equivalents, and alternatives falling within the spirit and scope of the present disclosure as defined by the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of controlling a device, the method comprising:<claim-text>capturing an image and displaying the image in real time on a touch screen;</claim-text><claim-text>searching for a first device and a second device that are connected to a network and is controllable;</claim-text><claim-text>receiving a first user input within the image at a position of the first device in the image, and designating a first setting zone at the position of the first user input;</claim-text><claim-text>displaying guide information for guiding a user to designate a second setting zone corresponding to the second device that is not displayed in the image;</claim-text><claim-text>receiving a second user input at a position at which the first device is not located, within the image;</claim-text><claim-text>based on a determination that the second device is not recognized in the image, designating the second setting zone at the position of the second user input that does not overlap with the position of the first device and positions of setting zones of other devices, and displaying the second setting zone in the image after designating the second setting zone; and</claim-text><claim-text>controlling the first device and the second device according to control inputs interacting with the first setting zone and the second setting zone, respectively.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>displaying, on the image, at least a part of the second setting zone selected according to the second user input.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first setting zone is designated based on a result of object recognition within the image.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>determining whether the first device is recognized in the image.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the designating the first setting zone further comprises, based on a determination that the first device is recognized in the image, designating a location of the first device recognized in the image as a location of the first setting zone of the first device.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>updating the position of the first setting zone of the first device by changing a location of the first setting zone that is designated by the user to a different location within the image according to a change in the image in real time.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>based on the second device selected according to the second user input not being recognized in the image, controlling a image capturing device to capture the second device selected according to the second user input while the second device is controlled or after the second device has been controlled.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>after designating the second setting zone in the image, displaying the second setting zone of the second device in the image while the second device is controlled or after the second device has been controlled.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein information on the first device is displayed on the first setting zone.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second user input is one of a touch input and a voice input.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A device control system for controlling a device, the device control system comprising:<claim-text>a display;</claim-text><claim-text>an input interface;</claim-text><claim-text>a processor; and</claim-text><claim-text>a computer-readable storage medium storing instructions which, when executed by the processor, cause the processor to perform operations comprising:</claim-text><claim-text>capturing an image captured and displaying the image in real time on a touch screen;</claim-text><claim-text>searching for a first device and a second device that are connected to the network and is controllable;</claim-text><claim-text>receiving a first user input within the image at a position of the first device in the image, and designating a first setting zone at the position of the first user input;</claim-text><claim-text>displaying guide information for guiding a user to designate a second setting zone corresponding to the second device that is not displayed in the image;</claim-text><claim-text>receiving a second user input at a position at which the first device is not located, within the image;</claim-text><claim-text>based on a determination that the second device is not recognized in the image, designing the second setting zone at the position of the second user input that does not overlap with the position of the first device and positions of setting zones of other devices, and displaying the second setting zone in the image after designating the second setting zone; and</claim-text><claim-text>controlling the first device and the second device according to control inputs interacting with the first setting zone and the second setting zone, respectively.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The device control system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the operations further comprise:<claim-text>updating the position of the first setting zone of the first device by changing a location of the first setting zone that is designated by the user to a different location within the image according to a change in the image in real time.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The device control system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the change in the image comprises at least one of a change in an angle of view of the image and a change in a location of the second device within the angle of view.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The device control system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the display is configured to display, on the image, at least a part of the second setting zone of the second device selected according to the second user input.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The device control system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the second setting zone is designated based on at least one of the second user input and a result of object recognition within the image.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The device control system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the operations further comprise:<claim-text>based on the second device selected according to the second user input not being recognized in the image, controlling a camera to capture the second device selected according to the second user input while the second device is controlled or after the second device has been controlled.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The device control system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the operations further comprise:<claim-text>after designating the second setting zone, displaying the second setting zone of the second device on the image while the second device is controlled or after the second device has been controlled.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The device control system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the display is configured to display information on the first device is displayed on the first setting zone.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The device control system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the second user input is one of a touch input and a voice input.</claim-text></claim></claims></us-patent-application>