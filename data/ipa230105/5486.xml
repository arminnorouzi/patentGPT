<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005487A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005487</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17363824</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>17</main-group><subgroup>14</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>57</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>19</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>17</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>29</main-group><subgroup>06</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>17</main-group><subgroup>14</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>57</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>19</main-group><subgroup>167</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>17</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>65</main-group><subgroup>403</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">AUTOCORRECTION OF PRONUNCIATIONS OF KEYWORDS IN AUDIO/VIDEOCONFERENCES</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>Rovi Guides, Inc.</orgname><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Emmanuel</last-name><first-name>Daina</first-name><address><city>Bangalore</city><country>IN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Chandrashekar</last-name><first-name>Padmassri</first-name><address><city>Bangalore</city><country>IN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present disclosure relates to automatically correcting mispronounced keywords during a conference session. More particularly, the present invention provides methods and systems for automatically correcting audio data generated from audio input having indications of mispronounced keywords during an audio/videoconferencing system. In some embodiments, the process of automatically correcting the audio data may require a re-encoding process of the audio data at the conference server. In alternative embodiments, the process may require updating the audio data at the receiver end of the conferencing system.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="105.58mm" wi="158.75mm" file="US20230005487A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="232.66mm" wi="165.52mm" orientation="landscape" file="US20230005487A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="59.94mm" wi="169.93mm" file="US20230005487A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="215.48mm" wi="167.64mm" orientation="landscape" file="US20230005487A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="232.16mm" wi="147.32mm" orientation="landscape" file="US20230005487A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="221.49mm" wi="171.28mm" file="US20230005487A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="200.24mm" wi="170.10mm" file="US20230005487A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="243.76mm" wi="165.52mm" file="US20230005487A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD</heading><p id="p-0002" num="0001">The present disclosure relates to automatically correcting mispronounced keywords during a conference session. More particularly, the present invention provides methods and systems for automatically correcting audio data generated from audio input having indications of mispronounced keywords during an audio/videoconferencing system.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Advancements in communication technology have improved the ability of users to communicate (e.g., such as via videoconferencing) with colleagues, family and friends located in different physical locations than the user. For example, conferencing systems (e.g., Microsoft Teams&#x2122;, Zoom&#x2122;, etc.) are often used to enable colleagues in separate, geographically distributed physical locations to have a face-to-face conversation via a videoconference. In some circumstances, multiple devices in a single location may be joined to a conference (e.g., in a physical conference room at an office), and other users in different geographical locations may also be participating in the conference over a network. However, with the globalization of digital communications, it has become difficult to correct someone's mispronunciation of a word, such as a participant's name, due to technical concerns. Furthermore, the mispronunciation of words may be distracting and irritating to users participating in the conference.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0004" num="0003">According to a first aspect, a method is provided for automatically correcting pronunciations of keywords for a video or audioconference. The method comprises a step of receiving audio input from a first user at a first device and generating, at the first device, a first audio data comprising a first audio signal based on the audio input. The generated first audio data may be transmitted to the server. The method further comprises a step of identifying, at the server, one or more portions of the first audio data, each comprising an indication of a mispronounced keyword and corresponding timestamp data associated with the one or more portions of the first audio data. The method further comprises a step of generating a corrected audio portion for each of the one or more portions of the first audio data, wherein the corrected audio portion comprises a correction of the mispronounced keyword and correcting, for output at a second device, the first audio data, wherein the correction comprises correcting the one or more portions of the first audio data with the respective corrected audio portion at the corresponding timestamp.</p><p id="p-0005" num="0004">In some examples, the step of identifying, at the server, one or more portions of the first audio data comprising an indication of a mispronounced keyword comprises referencing a database comprising correct pronunciations of keywords and determining, based on referencing, for each of the one or more portions of the first audio data, the indication of the mispronounced keyword.</p><p id="p-0006" num="0005">In some examples, the correction of the first audio data is processed at the server.</p><p id="p-0007" num="0006">In some examples, the processing of the correction of the first audio data at the server comprises re-encoding, at the server, the first audio data based on the correction of the one or more portions of the first audio data and transmitting the re-encoded first audio data comprising the corrected audio portion to the second device for output.</p><p id="p-0008" num="0007">In some examples, the correcting of the first audio data is processed at the second device.</p><p id="p-0009" num="0008">In some examples, the correction of the first audio data at the second device comprises transmitting, from the server, the first audio data to the second device, wherein the first audio data comprises a reference to a second audio data for correcting the one or more portions of the first audio data with the respective corrected audio portion and transmitting, in parallel with the first audio data, the second audio data comprising the corrected audio portion for each of the one or more portions of the first audio data.</p><p id="p-0010" num="0009">In some examples, the correction of the first audio data at the second device further comprises decoding, at the second device, the first audio data and the second audio data and correcting, at the second device, the first audio data, wherein the correction comprises correcting the one or more portions of the first audio data with the respective corrected audio portion of the second audio data.</p><p id="p-0011" num="0010">In some examples, the one or more keywords comprise any one or more of: names of people, names of locations, domain specific keywords, keywords related to an organization of the first user, keywords related to an organization of the second user.</p><p id="p-0012" num="0011">In some examples, the method further comprises determining an audio signature for the first user and generating the corrected audio portion based on the determined audio signature.</p><p id="p-0013" num="0012">According to a second aspect, a system is provided for correcting incorrect pronunciations of keywords during a video or audioconference. The system comprises control circuitry to perform the steps of the method according to the first aspect.</p><p id="p-0014" num="0013">According to a further aspect, a non-transitory computer-readable medium is provided having instructions encoded thereon for carrying out a method according to the method of the first aspect.</p><p id="p-0015" num="0014">It will be appreciated that other features, aspects and variations of the present invention will be apparent from the disclosure herein of the drawings and detailed description. Additionally, it will be further appreciated that additional or alternative embodiments may be implemented within the principles set out by the present disclosure.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0016" num="0015">The above and other objects and advantages of the disclosure will be apparent upon consideration of the following detailed description, taken in conjunction with the accompanying drawings, in which:</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a flowchart of a detailed illustrative process for automatically correcting a user's mispronunciation of keywords during a conference, in accordance with some embodiments of this disclosure;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows an example of generating an audio signature, in accordance with some embodiments of this disclosure;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an example timeline of the generated audio signature. In accordance with some embodiments of this disclosure;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram of illustrative conferencing devices, in accordance with some embodiments of this disclosure;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram of an illustrative conferencing system, in accordance with some embodiments of this disclosure; and</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>6</b></figref> and <figref idref="DRAWINGS">FIG. <b>7</b></figref> show flowcharts of a detailed illustrative process for automatically correcting a user's mispronunciation of keywords during a conference, in accordance with some embodiments of this disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0023" num="0022">The figures herein depict various embodiments of the disclosed invention for purposes of illustration only. It will be appreciated that additional or alternative structures, systems and methods may be implemented within the principles set out by the present disclosure.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0024" num="0023">In view of the foregoing, the present disclosure proposes methods and systems for automatically correcting audio data generated from audio input having indications of mispronounced words/keywords for output to one or more listeners at receiving devices of an audio/videoconferencing system.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>1</b></figref>. depicts an exemplary audio/videoconference setup <b>100</b> with participants 1-3 connected with a presenter <b>102</b> via a conference session <b>106</b>. An example display screen of the presenter's user device <b>104</b> is shown, which is in communication with a conference server. <figref idref="DRAWINGS">FIG. <b>1</b></figref> further shows a flowchart of a detailed illustrative process for automatically correcting audio data having indications of mispronounced keywords during a conference session, in accordance with some embodiments of this disclosure. In addition, one or more steps of the process of <figref idref="DRAWINGS">FIG. <b>1</b></figref> may be incorporated into or combined with one or more steps of any other process or embodiment (e.g., process <b>600</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref> and/or process <b>700</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>) described herein.</p><p id="p-0026" num="0025">At step <b>110</b>, the system receives audio input from a presenter or user. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, microphone <b>204</b> may receive audio input <b>202</b> (e.g., a voice of a user in the conference, background noise, music, recorded audio, and/or other suitable audio signals). The detected audio input <b>202</b> may include frequencies in a range of 20 Hz to 20 kHz (e.g., the sound wave frequency that may be heard by the human ear). In some embodiments, audio input <b>202</b> may be sampled at regular intervals (e.g., periods of 1-2 seconds with samples taken every few milliseconds during the period). For example, audio input <b>202</b> may be various segments of the audio of the conference. It will be appreciated that any of participant shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> may alternatively be a presenter during the conference session, and the presenter <b>102</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> may alternatively be a participant of the conference session.</p><p id="p-0027" num="0026">At step <b>112</b>, the system generates, at the user's device at which the audio input was received, audio data comprising an audio signal based on the received audio input. In example embodiments, the audio input <b>202</b> may be converted into an electrical audio signal. For example, the audio signal may be generated at a first user device as a first user speaks into a microphone to communicate with one or more participants of the conference session. The electrical audio signal output by the microphone <b>204</b> may be an analog output, and may be digitized at digital signal processor <b>206</b> to facilitate further processing, for example. In some embodiments, the microphone <b>204</b> may be a MEMS microphone with a digital output. The digital signal processor <b>206</b> (e.g., included in a general-purpose microprocessor or a specialized digital signal processor) may perform various operations on the received digital signal. In some embodiments, the digital signal processor <b>206</b> may perform a fast Fourier transform operation on time-domain samples of the audio to produce a frequency-domain representation of the audio. In some embodiments, the digital signal processor <b>206</b> may employ audio compression techniques, to reduce network resources and/or computing power to process the signal. In some embodiments, noise reduction techniques may be employed (e.g., in a pre-processing stage) to filter out unwanted signals.</p><p id="p-0028" num="0027">In some examples, an audio signature of the speaker/presenter may be generated, as shown at step <b>208</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The generated audio signature <b>208</b> may comprise a time-domain and/or frequency-domain representation (e.g., a spectrogram) of the signal. In some embodiments, digital signal processing including frequency analysis, peak volume detecting, audio hashing, waveform matching, and/or any other digital signal processing method known to those skilled in the art may be used to generate an audio signature. As another example, the audio signature may comprise an audio signature or hash calculation deterministically generated from a predefined portion of the audio signal.</p><p id="p-0029" num="0028">At step <b>114</b>, the generated audio data is transmitted to the conference server. For example, the audio data transmitted to the conference server may be sent as compressed audio data. For example, the device (e.g., device <b>104</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) associated with the user (e.g., presenter <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) may transmit an encoded time/frequency-domain representation of the signal to a conference server and receive and decode a time/frequency-domain representation of the audio signal.</p><p id="p-0030" num="0029">In some embodiments, the conference server may generate a features vector based on the output of the digital signal processor <b>206</b>, and audio signature <b>208</b> may correspond to such features vector. For example, the features vector may comprise a sequence of values representing various audio or acoustic features or characteristics (e.g., amplitude, modulation, tone, frequency, volume, speed, etc.) of the signal. Audio processing circuitry may analyze audio characteristics of the signal to identify audio signatures using any suitable audio analysis technique (e.g., frequency analysis to determine a base frequency and unique harmonic pattern of a particular voice, phoneme analysis to determine an accent of a particular voice, etc.). For example, the wave amplitude of the audio signal may be used to determine the volume of the voice of user <b>202</b> and/or analyze frequency data to determine the pitch and tone of the voice. The audio processing circuitry may also identify non-vocal audio such as music, sound effects, and the like using similar frequency analysis techniques or any other suitable method of audio analysis. The identified audio characteristics may be stored in association with a timestamp of when the audio signal was received. Audio signal processing is discussed in more detail in U.S. patent application Ser. No. 16/553,410, filed Aug. 28, 2019, which is hereby incorporated by reference herein in its entirety.</p><p id="p-0031" num="0030">At step <b>116</b>, the conference server identifies one or more portions of the audio data, each portion of the audio data comprising an indication of a mispronounced keyword by the presenter, and further identifies corresponding timestamp data associated with each portion of the audio data. In some embodiments, the system may reference a database comprising correct pronunciations of keywords and determine, based on the reference to the database of keywords, for each portion of the audio data, an indication within the audio data that a keyword has been mispronounced.</p><p id="p-0032" num="0031">In example embodiments, based on speech analysis and speech recognition techniques, the server determines any indications of mispronounced keywords that form part of the audio data and replaces the portion of the audio data having an indication of a mispronounced keyword, in substantially real time, with a corrected portion of audio data by referencing the database of keywords. For example, upon receiving the audio data at the server end of the conference system, keywords and any mispronunciations thereof may be identified in portions of the audio data. For example, keywords may be identified at the server using processing techniques, such as NLP algorithms or any other suitable algorithm. During speech analysis, for example, portions of the audio data containing the mispronounced keywords may be identified and compared to an index or database of keywords, the index or database having an indication of a correct pronunciation of each of the keywords.</p><p id="p-0033" num="0032">In example embodiments, the conference server may identify timestamp data at which the mispronounced keywords occur in the audio data. The timestamp associated with each mispronounced keyword within the portion of audio data that is processed at the server may be further associated with the corrected keyword signals, thereby providing a marker suitable for replacing portions of the audio data with the corrected keyword audio portion.</p><p id="p-0034" num="0033">In some embodiments, the conference server may continuously build upon the database of correct keywords pronunciations. For example, the system may maintain a learned/cached mechanism for a frequently used set of keywords per user, industry or organization, as may be efficient. For example, the system may store and maintain in the database of keywords the organization's name, client names of the organization, employee names, director names, locations relevant to the organization such as office locations, and domain specific keywords. In some embodiments, the keyword database may further include, but not limited to, names of people/cities/states/countries. In some examples, the keywords may be specific to domain or industry where there are more complex keywords based around context. For example, some industries that may benefit from such a mechanism may include, but are not limited to, the medical industry, the pharmaceutical and life sciences industry, etc. In some embodiments, a general keyword database may be preloaded with industry-related or company related information. The system may comprise a database of keywords that may be automatically generated based on metadata associated with the conference session, e.g., meeting attendees. The database of keywords may, additionally or alternatively, be saved in the user's profile at the conference server, to be provided for access for future audio/videoconference sessions. In some examples, one or more user specific databases may be updated with keywords that are commonly mispronounced by the user.</p><p id="p-0035" num="0034">At step <b>118</b>, the system generates a corrected audio portion for each portion of the audio data having an indication of a mispronounced keyword. In example embodiments, the corrected audio portion comprises a correction of the mispronounced keyword. For example, upon identifying the portions of the audio data having indications of mispronounced keywords and respective timestamps associated with said portions, corrected audio portions having corrected pronunciations of the mispronounced keywords may be determined and generated, which can be used to replace the original portion of the audio data received at the server for output at each listener's or participant's device. In preferred embodiments, the corrected audio portion may be generated using the speaker's audio signature, for example, in order to provide a fluid video/audioconferencing experience with minimal distractions for all participants of the conference session.</p><p id="p-0036" num="0035">At step <b>120</b>, the system corrects, for output at each listener's or participant's device, the audio data having the corrected audio portions at the corresponding timestamp. In some embodiments, the process described in relation to <figref idref="DRAWINGS">FIG. <b>1</b></figref> of automatically correcting the audio data may require a re-encoding process of the audio data at the conference server, as described in further detail with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>. In alternative embodiments, the process may require updating the audio data at the receiver end of the conferencing system. In such mechanisms, the automatic correction of keywords may be processed at the receiver end of the audio/videoconference system, as described in further detail with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an illustration of an example master playlist, master playlist <b>302</b>, that may be transmitted to one or more conference session participants. The master playlist may have index location identifiers and respective timestamp identifiers associated with an audio playlist <b>304</b> of the master playlist, in accordance with some embodiments of the present disclosure.</p><p id="p-0038" num="0037">In some embodiments, the master playlist comprises at least a video playlist <b>306</b> comprising one or more bitrate variants, such as bitrate variants <b>314</b> and <b>316</b>. In example embodiments, the master playlist comprises an audio playlist <b>304</b> comprising one or more bitrate variants, such as bitrate variants <b>310</b> and <b>312</b>, and, in some embodiments, an audio keyword playlist <b>308</b>. Furthermore, the one or more bitrate variants of the audio playlist <b>304</b> may comprise index location identifiers and respective timestamp identifiers, such as identifiers <b>320</b> and <b>322</b>. In example embodiments, the audio keyword playlist <b>308</b> comprises predetermined index location identifiers and associated keyword identifiers, such as identifiers <b>324</b>, <b>326</b>, <b>328</b> and <b>330</b>, which may be used to replace the mispronounced keywords and are transmitted as part of the audio playlist <b>304</b>. It will be appreciated that, in some embodiments, instead of transmitting a separate audio keyword playlist <b>308</b> as part of the master playlist <b>302</b>, the audio playlist <b>304</b> may alternatively be transmitted as a corrected or re-encoded audio playlist, as described as part of the present disclosure.</p><p id="p-0039" num="0038">The system may output, at each participant's or listeners' device, an audio output using the corrected audio data. In some embodiments, the output audio may comprise an output audio signal having the corrected keyword signals. In some embodiments, the corrected pronunciation of keywords may be output to attendees of the audio/videoconference in the same acoustic properties of the speaker, e.g., based on the audio signature of the speaker.</p><p id="p-0040" num="0039">The system may thus be a continuously learning and feedback mechanism to improve the database over time and also to improve the recognition of speakers' audio signatures, e.g., region-specific accents and pronunciations.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows generalized embodiments of illustrative conferencing devices <b>400</b> and <b>401</b>. Any of the devices discussed in connection with the present disclosure may be implemented as conferencing devices <b>400</b> or <b>401</b>. The conferencing application may be executed on any combination of conferencing device <b>400</b> and/or device <b>401</b> (e.g., locally) and/or conferencing server <b>508</b> (e.g., remotely) of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, and the processes described herein may be performed by the conferencing application locally or remotely. As depicted, conferencing device <b>400</b> may be a smartphone or tablet, whereas conferencing device <b>401</b> may be a conferencing system that includes equipment device <b>416</b> (e.g., a PC, set-top box, CPU, video-game console, etc.). Conferencing devices <b>400</b> and <b>401</b> may receive content and data via input/output (hereinafter &#x201c;I/O&#x201d;) path <b>402</b> (e.g., I/O circuitry). I/O path <b>402</b> may provide content (e.g., Internet content, content available over a local area network (LAN) or wide area network (WAN), and/or other content) and data to control circuitry <b>404</b>, which includes processing circuitry <b>406</b> and storage <b>408</b>. Control circuitry <b>404</b> may be used to send and receive commands, requests, and other suitable data using I/O path <b>402</b>. I/O path <b>402</b> may connect control circuitry <b>404</b> (and specifically processing circuitry <b>406</b>) to one or more communications paths (described below). I/O path <b>402</b> may additionally provide circuitry to control user interface <b>410</b>. I/O functions may be provided by one or more of these communications paths but are shown as a single path in <figref idref="DRAWINGS">FIG. <b>4</b></figref> to avoid overcomplicating the drawing.</p><p id="p-0042" num="0041">Control circuitry <b>404</b> may be based on any suitable processing circuitry such as processing circuitry <b>406</b>. As referred to herein, processing circuitry should be understood to mean circuitry based on one or more microprocessors, microcontrollers, digital signal processors, programmable logic devices, field-programmable gate arrays (FPGAs), application-specific integrated circuits (ASICs), etc., and may include a multi-core processor (e.g., dual-core, quad-core, hexa-core, or any suitable number of cores) or supercomputer. In some embodiments, processing circuitry may be distributed across multiple separate processors or processing units, for example, multiple of the same type of processing units (e.g., two Intel Core i7 processors) or multiple different processors (e.g., an Intel Core i5 processor and an Intel Core i7 processor). In some embodiments, control circuitry <b>404</b> executes instructions for a conferencing system stored in memory (e.g., storage <b>408</b>). Specifically, control circuitry <b>404</b> may be instructed by the conferencing system to perform the functions discussed above and below.</p><p id="p-0043" num="0042">In client server-based embodiments, control circuitry <b>404</b> may include communications circuitry suitable for communicating with a conferencing system server (e.g., server <b>508</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>) or other networks or servers. The instructions for carrying out the above-mentioned functionality may be stored on a server. Communications circuitry may include a cable modem, an integrated service digital network (ISDN) modem, a digital subscriber line (DSL) modem, a telephone modem, Ethernet card, or a wireless modem for communications with other equipment, or any other suitable communications circuitry. Such communications may involve the Internet or any other suitable communications networks or paths. In addition, communications circuitry may include circuitry that enables peer-to-peer communication of conferencing devices, or communication of conferencing devices in locations remote from each other.</p><p id="p-0044" num="0043">As referred to herein, the phrase &#x201c;conferencing device&#x201d; and &#x201c;user device&#x201d; should be understood to mean any device for accessing the content described above, such as a television, a Smart TV, a set-top box, an integrated receiver decoder (IRD) for handling satellite television, a digital storage device, a digital media receiver (DMR), a digital media adapter (DMA), a streaming media device, a personal computer (PC), a laptop computer, a tablet, a WebTV box, a smartphone, or any other television equipment, computing equipment, or wireless device, and/or combination of the same.</p><p id="p-0045" num="0044">Memory may be an electronic storage device provided as storage <b>408</b> that is part of control circuitry <b>404</b>. As referred to herein, the phrase &#x201c;electronic storage device&#x201d; or &#x201c;storage device&#x201d; should be understood to mean any device for storing electronic data, computer software, or firmware, such as random-access memory, read-only memory, hard drives, optical drives, digital video disc (DVD) recorders, compact disc (CD) recorders, BLU-RAY disc (BD) recorders, BLU-RAY 3D disc recorders, digital video recorders (DVRs, sometimes called personal video recorders, or PVRs), solid state devices, quantum storage devices, gaming consoles, gaming media, or any other suitable fixed or removable storage devices, and/or any combination of the same. Nonvolatile memory may also be used (e.g., to launch a boot-up routine and other instructions). Cloud-based storage, described in relation to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, may be used to supplement storage <b>408</b> or instead of storage <b>408</b>.</p><p id="p-0046" num="0045">Control circuitry <b>404</b> may include audio circuitry, video circuitry, and tuning circuitry, such as one or more analog tuners, one or more MP4 decoders or other digital decoding circuitry, or any other suitable tuning or audio circuits or combinations of such circuits. Encoding circuitry (e.g., for converting over-the-air, analog, or digital signals to audio signals for storage) may also be provided. Control circuitry <b>404</b> may also include scaler circuitry for upconverting and downconverting content into the preferred output format of the conferencing device <b>400</b>. Control circuitry <b>404</b> may also include digital-to-analog converter circuitry and analog-to-digital converter circuitry for converting between digital and analog signals. The tuning and encoding circuitry may be used by the conferencing device to receive and to display, to play, or to record content. The tuning and encoding circuitry may also be used to receive guidance data. The circuitry described herein, including, for example, the tuning, audio-generating, encoding, decoding, encrypting, decrypting, scaler, and analog/digital circuitry, may be implemented using software running on one or more general purpose or specialized processors. Multiple tuners may be provided to handle simultaneous tuning functions. If storage <b>408</b> is provided as a separate device from user device <b>400</b>, the tuning and encoding circuitry (including multiple tuners) may be associated with storage <b>408</b>.</p><p id="p-0047" num="0046">A user may send instructions to control circuitry <b>404</b> using user input interface <b>410</b>. User input interface <b>410</b> may be any suitable user interface, such as a remote control, mouse, trackball, keypad, keyboard, touchscreen, touchpad, stylus input, joystick, voice recognition interface, or other user input interfaces. Display <b>412</b> may be a touchscreen or touch-sensitive display. In such circumstances, user input interface <b>410</b> may be integrated with or combined with display <b>412</b>. Display <b>412</b> may be provided as a stand-alone device or integrated with other elements of each one of conferencing device <b>400</b> and device <b>401</b>. Speakers <b>414</b> may be provided as integrated with other elements of each of conferencing device <b>400</b> and device <b>401</b>. In the case of conferencing device <b>401</b>, speakers <b>414</b> may be stand-alone units (e.g., smart speakers). The audio component of videos and other content displayed on display <b>412</b> may be played through speakers <b>414</b>. In some embodiments, the audio may be distributed to a receiver (not shown), which processes and outputs the audio via speakers <b>414</b>.</p><p id="p-0048" num="0047">The conferencing application may be implemented using any suitable architecture. For example, it may be a stand-alone application wholly implemented on conferencing device <b>400</b> and/or <b>401</b>. In such an approach, instructions of the application are stored locally (e.g., in storage <b>408</b>), and data for use by the application is downloaded on a periodic basis (e.g., from an out-of-band feed, from an Internet resource, or using another suitable approach). Control circuitry <b>404</b> may retrieve instructions of the application from storage <b>408</b> and process the instructions to generate any of the displays discussed herein. Based on the processed instructions, control circuitry <b>404</b> may determine what action to perform when input is received from input interface <b>410</b>.</p><p id="p-0049" num="0048">In some embodiments, the conferencing application is a client/server-based application. Data for use by a thick or thin client implemented on device <b>400</b> or device <b>401</b> may be retrieved on demand by issuing requests to a server (e.g., conferencing server <b>508</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>) remote to the device <b>400</b> or device <b>401</b>. In one example of a client/server-based conferencing application, control circuitry <b>404</b> runs a web browser that interprets web pages provided by a remote server (e.g., conferencing server <b>508</b>). For example, the remote server may store the instructions for the application in a storage device. The remote server may process the stored instructions using circuitry (e.g., control circuitry <b>404</b>) and generate the displays discussed above and below. The user device may receive the displays generated by the remote server and may display the content of the displays locally on device <b>400</b> or device <b>401</b>. This way, the processing of the instructions is performed remotely by the server while the resulting displays are provided locally on device <b>400</b> or device <b>401</b>. Device <b>400</b> or device <b>401</b> may receive inputs from the user via input interface <b>410</b> and transmit those inputs to the remote server for processing and generating the corresponding displays. For example, device <b>400</b> or device <b>401</b> may transmit a communication to the remote server indicating that an up/down button was selected via input interface <b>410</b>. The remote server may process instructions in accordance with that input and generate a display of the application corresponding to the input (e.g., a display that moves a cursor up/down). The generated display is then transmitted to device <b>400</b> or device <b>401</b> for presentation to the user.</p><p id="p-0050" num="0049">In some embodiments, the conferencing application is downloaded and interpreted or otherwise run by an interpreter or virtual machine (run by control circuitry <b>404</b>). In some embodiments, the application may be encoded in the ETV Binary Interchange Format (EBIF), received by control circuitry <b>404</b> as part of a suitable feed, and interpreted by a user agent running on control circuitry <b>404</b>. For example, the application may be an EBIF application. In some embodiments, the guidance application may be defined by a series of Java-based files that are received and run by a local virtual machine or other suitable middleware executed by control circuitry <b>404</b>. In some of such embodiments (e.g., those employing MPEG-2 or other digital media encoding schemes), the application may be, for example, encoded and transmitted in an MPEG-2 object carousel with the MPEG audio and video packets of a program.</p><p id="p-0051" num="0050">Conferencing device <b>400</b> and conferencing device <b>401</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> can be implemented in system <b>500</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref> as any combination of conferencing device <b>502</b>, conferencing device <b>505</b>, or conferencing device <b>506</b>. Conferencing devices, on which a conferencing system may be implemented, may function as stand-alone devices or may be part of a network of devices. Conferencing server <b>508</b> may have a similar configuration to conferencing device <b>401</b>, although conferencing server <b>508</b> may not include certain elements (e.g., a display, a user interface, speakers, etc.). Various network configurations of devices may be implemented and are discussed in more detail below.</p><p id="p-0052" num="0051">In system <b>500</b>, as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, there may be multiple conferencing devices, but only three are shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> to avoid overcomplicating the drawing. In addition, each user may utilize more than one type of conferencing device and also more than one of each type of conferencing device. Conferencing devices <b>502</b>, <b>505</b>, <b>506</b> may be coupled to communication network <b>504</b>. Communication network <b>504</b> may be one or more networks including the Internet, a mobile phone network, mobile voice or data network (e.g., a 4G or LTE network), cable network, public switched telephone network, or other types of communications network or combinations of communications networks. Conferencing server <b>508</b>, and conferencing devices <b>502</b>, <b>505</b>, <b>506</b> may be coupled to communications path <b>504</b> via one or more communications paths, such as a satellite path, a fiber-optic path, a cable path, a path that supports Internet communications (e.g., IPTV), free-space connections (e.g., for broadcast or other wireless signals), or any other suitable wired or wireless communications path or combination of such paths.</p><p id="p-0053" num="0052">Although communications paths are not drawn between conferencing devices <b>502</b>, <b>505</b>, <b>506</b>, and conferencing server <b>508</b>, these devices may communicate directly with each other via communications paths, such as short-range point-to-point communications paths, such as USB cables, IEEE 1394 cables, wireless paths (e.g., Bluetooth, infrared, IEEE 802-11x, etc.), or other short-range communication via wired or wireless paths. BLUETOOTH is a certification mark owned by Bluetooth SIG, INC. The conferencing devices may also communicate with each other through an indirect path via communication network <b>504</b>.</p><p id="p-0054" num="0053">Conferencing applications may be, for example, stand-alone applications implemented on conferencing devices. For example, the conferencing application may be implemented as software or a set of executable instructions, which may be stored in storage <b>408</b> and executed by control circuitry <b>404</b> of a conferencing device <b>502</b>, <b>505</b>, <b>506</b>. In some embodiments, conferencing systems may be client server applications where only a client application resides on the conferencing device, and a server application resides on conferencing server <b>508</b>. For example, conferencing systems may be implemented partially as a client application on control circuitry <b>404</b> of conferencing devices <b>502</b>, <b>505</b>, <b>506</b> and partially on conferencing server <b>508</b> as a server application running on control circuitry of conferencing server <b>508</b>.</p><p id="p-0055" num="0054">When executed by control circuitry of conferencing server <b>508</b>, the conferencing application may instruct the control circuitry to capture audio, generate audio signatures (e.g., based on captured audio), join a device to the conference, and generate the conferencing system output (e.g., a video feed of the conference, audio feed of the conference, text chat or other functionalities for the conference, etc.) and transmit a conference ID (e.g., to a device newly joined to a conference) and the generated output to conferencing devices <b>502</b>, <b>505</b>, <b>506</b>. The client application may instruct control circuitry of the receiving conferencing devices <b>502</b>, <b>505</b>, <b>506</b> to generate the conferencing system output.</p><p id="p-0056" num="0055">Conferencing devices <b>502</b>, <b>505</b>, <b>506</b> may operate in a cloud computing environment to access cloud services. In a cloud computing environment, various types of computing services for content sharing, storage or distribution (e.g., video sharing sites or social networking sites) are provided by a collection of network-accessible computing and storage resources, referred to as &#x201c;the cloud.&#x201d; Cloud resources may be accessed by conferencing device <b>502</b>, <b>505</b>, <b>506</b> using, for example, a web browser, a conferencing system, a desktop application, a mobile application, and/or any combination of access applications of the same. Conferencing devices <b>502</b>, <b>505</b>, <b>506</b> may be a cloud client that relies on cloud computing for application delivery, or the conferencing device may have some functionality without access to cloud resources.</p><p id="p-0057" num="0056">For example, some applications running on conferencing device <b>502</b>, <b>505</b>, <b>506</b> may be cloud applications, i.e., applications delivered as a service over the Internet, while other applications may be stored and run on the conferencing device. In some embodiments, a user device may receive content from multiple cloud resources simultaneously. For example, a user device can stream audio from one cloud resource while downloading content from a second cloud resource. Or a user device can download content from multiple cloud resources for more efficient downloading. In some embodiments, conferencing devices can use cloud resources for processing operations such as the processing operations performed by processing circuitry described in relation to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a flowchart <b>600</b> of a detailed illustrative process, implemented at a server, for automatically correcting a user's mispronunciation of keywords during a conference, in accordance with some embodiments of this disclosure. It should be noted that process <b>600</b> or any step thereof could be performed on, or provided by, any of the devices shown in <figref idref="DRAWINGS">FIGS. <b>4</b>-<b>5</b></figref>. For example, process <b>600</b> may be executed by devices <b>502</b>, <b>505</b>, <b>506</b> (e.g., via control circuitry <b>404</b>) and/or control circuitry of conferencing server <b>508</b>, as instructed by a conferencing application that may be implemented on conferencing device <b>502</b>, and/or conferencing device <b>505</b> and/or conferencing device <b>506</b> and/or conferencing server <b>508</b>, such as to distribute control of database management application operations for a target device among multiple devices. In addition, one or more steps of process <b>600</b> may be incorporated into or combined with one or more steps of any other process or embodiment (e.g., process of <figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0059" num="0058">In the example of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, corresponding to embodiments according to processing the autocorrection of keywords at the server-side, the audio data/portion needs to be re-encoded before it is transmitted in order to update the keywords with corrected keywords.</p><p id="p-0060" num="0059">In example embodiments, control circuitry of a conferencing server (e.g., server <b>508</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>) may establish a conference (e.g., conference <b>105</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) via a communication network (e.g., network <b>504</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>). For example, the conferencing server may communicate with users via one or more devices (e.g., devices <b>502</b>, <b>505</b>, <b>506</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>) to set up the conference. The conference may include multiple participants from several different physical locations, and at any of such locations there may be a single user or multiple users present (e.g., communicating via a single device in a conference room, or communicating via multiple devices). The conference may be associated with a unique conference ID stored at the conferencing server, and such conference ID may be communicated to each of the devices to enable the devices to join the conference.</p><p id="p-0061" num="0060">At step <b>602</b>, the system receives audio input from a presenter or user. For example, a microphone of each of one or more devices (e.g., devices <b>502</b>, <b>505</b>, <b>506</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>) joined to the conference may capture audio signals of voices of users in the conference.</p><p id="p-0062" num="0061">At step <b>604</b>, the system generates, at the user's device at which the audio input was received, audio data comprising an audio signal based on the received audio input. In example embodiments, the audio input may be converted into an electrical audio signal. For example, the audio signal may be generated at a first user device as a first user speaks into a microphone to communicate with one or more participants of the conference session.</p><p id="p-0063" num="0062">At step <b>606</b>, the generated audio data is transmitted to the conference server. For example, the audio data transmitted to the conference server may be sent as compressed audio data. In some embodiments, the captured audio signal may be transmitted from each of the devices <b>502</b>, <b>505</b>, <b>506</b> joined to the conference at the same time, or at various predetermined times. The captured audio signals may be transmitted to the conferencing server each time the audio is sampled by the respective devices.</p><p id="p-0064" num="0063">At step <b>608</b>, the conference server identifies one or more portions of the audio data, each portion of the audio data comprising an indication of a mispronounced keyword by the presenter, and further identifies corresponding timestamp data associated with each portion of the audio data. In some embodiments, the system may reference a database comprising correct pronunciations of keywords and determine, based on the reference to the database of keywords, for each portion of the audio data, an indication within the audio data that a keyword has been mispronounced.</p><p id="p-0065" num="0064">At step <b>610</b>, the system generates a corrected audio portion for each portion of the audio data having an indication of a mispronounced keyword. In example embodiments, the corrected audio portion comprises a correction of the mispronounced keyword. For example, upon identifying the portions of the audio data having indications of mispronounced keywords and respective timestamps associated with said portions, corrected audio portions having corrected pronunciations of the mispronounced keywords may be determined and generated, which can be used to replace the original portion of the audio data received at the server for output at each listener's or participant's device. In preferred embodiments, the corrected audio portion may be generated using the speaker's audio signature, for example, in order to provide a fluid video/audioconferencing experience with minimal distractions for all participants of the conference session.</p><p id="p-0066" num="0065">At step <b>612</b>, the system re-encodes, at the conference server, the first audio data based on the correction of the one or more portions of the first audio data. Thus, the conference server corrects, for output at each listener's or participant's device, the audio data having the corrected audio portions at the corresponding timestamp.</p><p id="p-0067" num="0066">In this example, the system generates a corrected audio portion for each portion of the audio data having an indication of a mispronounced keyword, and carries out the automatic correction at the conference server. For example, upon identifying the portions of the audio data having indications of mispronounced keywords and respective timestamps associated with said portions, corrected audio portions having corrected pronunciations of the mispronounced keywords may be determined and generated. The corrected audio portions are then used to replace the original portion of the audio data received at the server for output at each listener's or participant's device.</p><p id="p-0068" num="0067">In preferred embodiments, the corrected audio portion may be generated using the speaker's audio signature, for example, in order to provide a fluid video/audioconferencing experience with minimal distractions for all participants of the conference session. For example, the system may continuously analyze the incoming speech for mispronounced keywords and replace them with the right pronunciation, while retaining the audio signature of the user, e.g., acoustic properties of the environment and the speaker's voice and accent. The pronunciation may be corrected and applied in the context of the speaker's voice, pitch, duration or speed, intensity, voice quality and accent, etc., which can be assessed and determined as described in relation to <figref idref="DRAWINGS">FIG. <b>2</b></figref> above.</p><p id="p-0069" num="0068">For example, control circuitry of the conferencing server (and/or digital signal processor <b>206</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>) may generate audio signatures based on the audio signals (e.g., received via I/O path <b>402</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>), such as in accordance with the techniques discussed in connection with <figref idref="DRAWINGS">FIG. <b>2</b></figref>. For example, audio modulations of the received signals may be compressed into audio signatures. Various digital signal processing algorithms may be used (e.g., fast Fourier transform) in generating the audio signatures. The audio signature may be generated for each audio signal received by the conferencing server from the devices joined to the conference, and audio signals may be transmitted to the server (e.g., at predetermined times, for the purposes of generating audio signatures) until the conclusion of the conference session.</p><p id="p-0070" num="0069">In some embodiments, control circuitry of conferencing server (e.g., server <b>508</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>) may store (e.g., in storage <b>408</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) the generated audio signatures in connection with the conference ID associated with the conference. In addition to the conference ID, the generated signatures may be associated and/or tabulated with device IDs respectively associated with devices joined to the conference, and a timestamp associated with each audio signature. It should be noted that the conferencing server may store any number of conference IDs for various ongoing conferences (e.g., within particular organizations, across multiple organizations, etc.).</p><p id="p-0071" num="0070">At step <b>614</b>, the system transmits, from the conference server, the re-encoded audio data comprising the corrected audio portion to the second device for output. The encoded audio is signaled to the receiver, and the same is played back/heard as a normal audio data with no changes needed at the receiver end of the audio/videoconference system.</p><p id="p-0072" num="0071">At step <b>616</b>, the system outputs, at a second device, an output audio based on the correction. When rendered out, the audio would be with corrected keywords. Thus, in this approach, the autocorrection would take place at the server before it transmits the corrected audio portion to the receiver devices.</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a flowchart <b>700</b> of a detailed illustrative process, implemented at a server, for automatically correcting a user's mispronunciation of keywords during a conference, in accordance with some embodiments of this disclosure. It should be noted that process <b>700</b> or any step thereof could be performed on, or provided by, any of the devices shown in <figref idref="DRAWINGS">FIGS. <b>4</b>-<b>5</b></figref>. For example, process <b>700</b> may be executed by devices <b>502</b>, <b>505</b>, <b>506</b> (e.g., via control circuitry <b>404</b>) and/or control circuitry of conferencing server <b>508</b>, as instructed by a conferencing application that may be implemented on conferencing device <b>502</b>, and/or conferencing device <b>505</b> and/or conferencing device <b>506</b> and/or conferencing server <b>508</b>, such as to distribute control of database management application operations for a target device among multiple devices. In addition, one or more steps of process <b>700</b> may be incorporated into or combined with one or more steps of any other process or embodiment (e.g., process of <figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0074" num="0073">In the example of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, corresponding to embodiments according to processing auto-correction of keywords at the receiver-side, the audio data/portions are transmitted alongside separately transmitted audio keyword data/portions, e.g., as separate playlists as part of a master playlist, in order to update the keywords with corrected keywords at the receiver's device.</p><p id="p-0075" num="0074">In example embodiments, control circuitry of a conferencing server (e.g., server <b>508</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>) may establish a conference (e.g., conference <b>105</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) via a communication network (e.g., network <b>504</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>). For example, the conferencing server may communicate with users via one or more devices (e.g., devices <b>502</b>, <b>505</b>, <b>506</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>) to set up the conference. The conference may include multiple participants from several different physical locations, and at any of such locations there may be a single user or multiple users present (e.g., communicating via a single device in a conference room, or communicating via multiple devices). The conference may be associated with a unique conference ID stored at the conferencing server, and such conference ID may be communicated to each of devices to enable the devices to join the conference.</p><p id="p-0076" num="0075">At step <b>702</b>, the system receives audio input from a presenter or user. For example, a microphone of each of one or more devices (e.g., devices <b>502</b>, <b>505</b>, <b>506</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>) joined to the conference may capture audio signals of voices of users in the conference.</p><p id="p-0077" num="0076">At step <b>704</b>, the system generates, at the user's device at which the audio input was received, audio data comprising an audio signal based on the received audio input. In example embodiments, the audio input may be converted into an electrical audio signal. For example, the audio signal may be generated at a first user device as a first user speaks into a microphone to communicate with one or more participants of the conference session.</p><p id="p-0078" num="0077">At step <b>706</b>, the generated audio data is transmitted to the conference server. For example, the audio data transmitted to the conference server may be sent as compressed audio data. In some embodiments, the captured audio signal may be transmitted from each of the devices joined to the conference at the same time, or at various predetermined times. The captured audio signals may be transmitted to the conferencing server each time the audio is sampled by the respective devices.</p><p id="p-0079" num="0078">At step <b>708</b>, the conference server identifies one or more portions of the audio data, each portion of the audio data comprising an indication of a mispronounced keyword by the presenter, and further identifies a corresponding timestamp data associated with each portion of the audio data. In some embodiments, the system may reference a database comprising correct pronunciations of keywords and determine, based on the reference to the database of keywords, for each portion of the audio data, an indication within the audio data that a keyword has been mispronounced.</p><p id="p-0080" num="0079">At step <b>710</b>, the system generates a corrected audio portion for each portion of the audio data having an indication of a mispronounced keyword. In example embodiments, the corrected audio portion comprises a correction of the mispronounced keyword. For example, upon identifying the portions of the audio data having indications of mispronounced keywords and respective timestamps associated with said portions, corrected audio portions having corrected pronunciations of the mispronounced keywords may be determined and generated, which can be used to replace the original portion of the audio data received at the server for output at each listener's or participant's device. In preferred embodiments, the corrected audio portion may be generated using the speaker's audio signature, for example, in order to provide a fluid video/audioconferencing experience with minimal distractions for all participants of the conference session.</p><p id="p-0081" num="0080">At step <b>712</b>, the conference server transmits the first audio data to the second device, wherein the first audio data comprises a reference to a second audio data. At step <b>714</b>, the conference server transmits, in parallel with the first audio data, the second audio data comprising the corrected audio portion for each of the one or more portions of the first audio data. For example, the conference server may transmit the first audio data to the second device and a second audio data comprising the corrected keyword signal for correcting each of the one or more keywords at the second device. The second audio data, which comprises encoded corrected audio portions, may be signaled as a separated playlist/track for decoding (e.g., Audio Keyword Playlist <b>308</b> as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>). Along with the original audio data (e.g., Audio Playlist <b>304</b> as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>), a reference to an index to the corrected keywords may also be signaled for each portion of the audio data.</p><p id="p-0082" num="0081">At step <b>716</b>, the system decodes, at the second device, the initial audio data and the corrected audio data. Thus, at the receiver end of the conference system, both the original audio portions (without corrections) and the corrected audio portions are decoded for output. For example, at the receiver end, each portion of the Audio Playlist <b>304</b> may be parsed, and the index or keyword identifier may be retrieved from the URL present in the playlist by refencing the corrected audio portions of the Audio Keyword Playlist <b>308</b>, for example.</p><p id="p-0083" num="0082">At step <b>718</b>, the system corrects, at the second device for output, the first audio data, wherein the correction comprises correcting the one or more portions of the first audio data with the respective corrected audio portion of the second audio data. Therefore, when the audio is rendered at the receiving device, the rendering logic may be updated such that the decoded portions of audio data where the mispronounced keyword occurs are replaced with decoded portions of the corrected audio potions having corrected keyword pronunciations at the respective timestamps, as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0084" num="0083">At step <b>720</b>, the system outputs, at a second device, an output audio based on the correction. When rendered out, the audio would be with corrected keyword. Thus, in this approach, the autocorrection would take place at the receiver end of the system.</p><p id="p-0085" num="0084">The systems and processes discussed above are intended to be illustrative and not limiting. One skilled in the art would appreciate that the actions of the processes discussed herein may be omitted, modified, combined, and/or rearranged, and any additional actions may be performed without departing from the scope of the invention. More generally, the above disclosure is meant to be exemplary and not limiting. Only the claims that follow are meant to set bounds as to what the present disclosure includes. Furthermore, it should be noted that the features and limitations described in any one embodiment may be applied to any other embodiment herein, and flowcharts or examples relating to one embodiment may be combined with any other embodiment in a suitable manner, done in different orders, or done in parallel. In addition, the systems and methods described herein may be performed in real time. It should also be noted that the systems and/or methods described above may be applied to, or used in accordance with, other systems and/or methods.</p><p id="p-0086" num="0085">All of the features disclosed in this specification (including any accompanying claims, abstract and drawings), and/or all of the steps of any method or process so disclosed, may be combined in any combination, except combinations where at least some of such features and/or steps are mutually exclusive.</p><p id="p-0087" num="0086">Each feature disclosed in this specification (including any accompanying claims, abstract and drawings), may be replaced by alternative features serving the same, equivalent or similar purpose, unless expressly stated otherwise. Thus, unless expressly stated otherwise, each feature disclosed is one example only of a generic series of equivalent or similar features.</p><p id="p-0088" num="0087">The invention is not restricted to the details of any foregoing embodiments. The invention extends to any novel one, or any novel combination, of the features disclosed in this specification (including any accompanying claims, abstract and drawings), or to any novel one, or any novel combination, of the steps of any method or process so disclosed. The claims should not be construed to cover merely the foregoing embodiments, but also any embodiments that fall within the scope of the claims.</p><p id="p-0089" num="0088">Throughout the description and claims of this specification, the words &#x201c;comprise&#x201d; and &#x201c;contain&#x201d; and variations of them mean &#x201c;including but not limited to,&#x201d; and they are not intended to (and do not) exclude other moieties, additives, components, integers or steps. Throughout the description and claims of this specification, the singular encompasses the plural unless the context otherwise requires. In particular, where the indefinite article is used, the specification is to be understood as contemplating plurality as well as singularity, unless the context requires otherwise.</p><p id="p-0090" num="0089">All of the features disclosed in this specification (including any accompanying claims, abstract and drawings), and/or all of the steps of any method or process so disclosed, may be combined in any combination, except combinations where at least some of such features and/or steps are mutually exclusive. The invention is not restricted to the details of any foregoing embodiments. The invention extends to any novel one, or any novel combination, of the features disclosed in this specification (including any accompanying claims, abstract and drawings), or to any novel one, or any novel combination, of the steps of any method or process so disclosed.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of automatically correcting pronunciations of keywords for a video or audioconference, the method comprising:<claim-text>receiving audio input from a first user at a first device;</claim-text><claim-text>generating, at the first device, a first audio data comprising a first audio signal based on the audio input;</claim-text><claim-text>transmitting the generated first audio data to the server;</claim-text><claim-text>identifying, at the server, one or more portions of the first audio data each comprising an indication of a mispronounced keyword and corresponding timestamp data associated with the one or more portions of the first audio data;</claim-text><claim-text>generating a corrected audio portion for each of the one or more portions of the first audio data, wherein the corrected audio portion comprises a correction of the mispronounced keyword; and</claim-text><claim-text>correcting, for output at a second device, the first audio data, wherein the correction comprises correcting the one or more portions of the first audio data with the respective corrected audio portion at the corresponding timestamp.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of identifying, at the server, one or more portions of the first audio data comprising an indication of a mispronounced keyword comprises:<claim-text>referencing a database comprising correct pronunciations of keywords; and</claim-text><claim-text>determining, based on referencing, for each of the one or more portions of the first audio data, the indication of the mispronounced keyword.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the correction of the first audio data is processed at the server.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the correction of the first audio data at the server comprises:<claim-text>re-encoding, at the server, the first audio data based on the correction of the one or more portions of the first audio data; and</claim-text><claim-text>transmitting the re-encoded first audio data comprising the corrected audio portion to the second device for output.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the correction of the first audio data is processed at the second device.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the correction of the first audio data at the second device comprises:<claim-text>transmitting, from the server, the first audio data to the second device, wherein the first audio data comprises a reference to a second audio data for correcting the one or more portions of the first audio data with the respective corrected audio portion; and</claim-text><claim-text>transmitting, in parallel with the first audio data, the second audio data comprising the corrected audio portion for each of the one or more portions of the first audio data.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the correction of the first audio data at the second device further comprises:<claim-text>decoding, at the second device, the first audio data and the second audio data; and</claim-text><claim-text>correcting, at the second device, the first audio data, wherein the correction comprises correcting the one or more portions of the first audio data with the respective corrected audio portion of the second audio data.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more keywords comprise any one or more of: names of people; names of locations; domain specific keywords; keywords related to an organization of the first user; keywords related to an organization of the second user.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>determining an audio signature for the first user; and</claim-text><claim-text>generating the corrected audio portion based on the determined audio signature.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A non-transitory computer-readable medium having instructions encoded thereon for carrying out a method according to the method of <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A system for correcting incorrect pronunciations of keywords during a video or audioconference, the system comprising control circuitry to perform the steps of:<claim-text>receiving audio input from a first user at a first device;</claim-text><claim-text>generating, at the first device, a first audio data comprising a first audio signal based on the audio input;</claim-text><claim-text>transmitting the generated first audio data to the server;</claim-text><claim-text>identifying, at the server, one or more portions of the first audio data each comprising an indication of a mispronounced keyword and corresponding timestamp data associated with the one or more portions of the first audio data;</claim-text><claim-text>generating a corrected audio portion for each of the one or more portions of the first audio data, wherein the corrected audio portion comprises a correction of the mispronounced keyword; and</claim-text><claim-text>correcting, for output at a second device, the first audio data, wherein the correction comprises correcting the one or more portions of the first audio data with the respective corrected audio portion at the corresponding timestamp.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the step of identifying, at the server, one or more portions of the first audio data comprising an indication of a mispronounced keyword comprises:<claim-text>referencing a database comprising correct pronunciations of keywords; and</claim-text><claim-text>determining, based on referencing, for each of the one or more portions of the first audio data, the indication of the mispronounced keyword.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the correction of the first audio data is processed at the server.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the correction of the first audio data at the server comprises:<claim-text>re-encoding, at the server, the first audio data based on the correction of the one or more portions of the first audio data; and</claim-text><claim-text>transmitting the re-encoded first audio data comprising the corrected audio portion to the second device for output.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the correction of the first audio data is processed at the second device.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the correction of the first audio data at the second device comprises:<claim-text>transmitting, from the server, the first audio data to the second device, wherein the first audio data comprises a reference to a second audio data for correcting the one or more portions of the first audio data with the respective corrected audio portion; and</claim-text><claim-text>transmitting, in parallel with the first audio data, the second audio data comprising the corrected audio portion for each of the one or more portions of the first audio data.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the correction of the first audio data at the second device further comprises:<claim-text>decoding, at the second device, the first audio data and the second audio data; and</claim-text><claim-text>correcting, at the second device, the first audio data, wherein the correction comprises correcting the one or more portions of the first audio data with the respective corrected audio portion of the second audio data.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the one or more keywords comprise any one or more of: names of people; names of locations; domain specific keywords; keywords related to an organization of the first user; keywords related to an organization of the second user.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>determining an audio signature for the first user; and</claim-text><claim-text>generating the corrected audio portion based on the determined audio signature.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A non-transitory computer-readable medium having instructions encoded thereon for carrying out a method, the method comprising:<claim-text>receiving audio input from a first user at a first device;</claim-text><claim-text>generating, at the first device, a first audio data comprising a first audio signal based on the audio input;</claim-text><claim-text>transmitting the generated first audio data to the server;</claim-text><claim-text>identifying, at the server, one or more portions of the first audio data each comprising an indication of a mispronounced keyword and corresponding timestamp data associated with the one or more portions of the first audio data;</claim-text><claim-text>generating a corrected audio portion for each of the one or more portions of the first audio data, wherein the corrected audio portion comprises a correction of the mispronounced keyword; and</claim-text><claim-text>correcting, for output at a second device, the first audio data, wherein the correction comprises correcting the one or more portions of the first audio data with the respective corrected audio portion at the corresponding timestamp.</claim-text></claim-text></claim><claim id="CLM-21-28" num="21-28"><claim-text><b>21</b>-<b>28</b>. (canceled)</claim-text></claim></claims></us-patent-application>