<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004755A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004755</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17471498</doc-number><date>20210910</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>KR</country><doc-number>10-2021-0086129</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>17</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6257</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>17</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6202</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD, SYSTEM AND RECORDING MEDIUM FOR GENERATING TRAINING DATA FOR DETECTION MODEL BASED ON ARTIFICIAL INTELLIGENCE</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>AIRISS CO., LTD.</orgname><address><city>Daejeon</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KIM</last-name><first-name>Jeong Mi</first-name><address><city>Daejeon</city><country>KR</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>AIRISS CO., LTD.</orgname><role>03</role><address><city>Daejeon</city><country>KR</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">According to an aspect of the present disclosure, there is provided a method of generating training data related to an artificial intelligence-based detection model, which includes the steps of: generating three-dimensional (3D) model of a hidden target object and 3D model of a hiding tool object, respectively, and combining the 3D model of the hidden target object and the 3D model of the hiding tool object; generating a two-dimensional (2D) image by capturing, in at least one direction, the combined 3D model obtained by combining the 3D model of the hidden target object with the 3D model of the hiding tool object; and processing the generated 2D image with reference to deformation or distortion which occurs in a detection target image obtained by actually capturing a detection target object to be detected.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="116.16mm" wi="129.29mm" file="US20230004755A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="228.09mm" wi="132.93mm" file="US20230004755A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="212.34mm" wi="65.79mm" file="US20230004755A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application claims priority to Korean Patent Application No. 10-2021-0086129 filed on Jun. 30, 2021, the entire contents of which are herein incorporated by reference.</p><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to a method, a system, and a recording medium of generating training data for detection model based on an artificial intelligence.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Recently, as research on artificial intelligence-related technologies such as a machine learning and a deep learning is actively conducted, many technologies for detecting a specific object from an image using such an artificial intelligence have been introduced.</p><p id="p-0005" num="0004">Also in the field of security search, attempts are being made to detect a hidden target object (e.g., a knife, a bottle, a firearm, or the like) using such a technology based on artificial intelligence. However, techniques which has been introduced so far, including the above conventional technology, have limitations in that the techniques require a large amount of time and cost because pieces of video data for artificial intelligence learning are produced one by one through capturing. For example, when a learning video in which a pistol is hidden in a briefcase is made, it is necessary to capture an image of a briefcase while actually putting a pistol into the briefcase. In this case, the angle of the pistol may be manually adjusted one by one, which is significantly ineffective in terms of time and cost. In addition, the performance of an artificial intelligence-based detection model may vary depending on the diversity and volume of training data, and thus, in order to increase the performance of the artificial intelligence-based detection model, it is necessary to secure a large amount of training data containing various aspects.</p><p id="p-0006" num="0005">Based on the above findings, the inventor(s) of the present disclosure proposes a technique of effectively generating a large number of pieces of training data in which various aspects of a hidden target object are included.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0007" num="0006">One object of the present disclosure is to solve all the above-described problems in the related art.</p><p id="p-0008" num="0007">Another object of the present disclosure is to provide a method and a system to effectively generate a large number of pieces of training data in which various aspects of a hidden target object are included.</p><p id="p-0009" num="0008">Yet another object of the present disclosure is to enhance the performance of an artificial intelligence-based detection model by increasing the diversity and volume of training data.</p><p id="p-0010" num="0009">Representative configurations of the present disclosure are as follows.</p><p id="p-0011" num="0010">According to one aspect of the present disclosure, there is provided a method of generating training data for a detection model based on an artificial intelligence, including the steps of: generating a three-dimensional (3D) model of a hidden target object and a 3D model of a hiding tool object, respectively, and combining the 3D model of the hidden target object and the 3D model of the hiding tool object; generating a two-dimensional (2D) image by capturing, in at least one direction, the combined 3D model obtained by combining the 3D model of the hidden target object and the 3D model of the hiding tool object; and processing the generated 2D image with reference to deformation or distortion which occurs in a detection target image obtained by actually capturing a detection target object to be detected.</p><p id="p-0012" num="0011">According to another aspect of the present disclosure, there is provided a system of generating training data fora a detection model based on an artificial intelligence, which includes: a 3D model management unit configured to generate a 3D model of a hidden target object and each 3D model of a hiding tool object, respectively, and configured to combine the 3D model of the hidden target object and the 3D model of the hiding tool object; a training data generation unit configured to generate a 2D image by capturing, in at least one direction, the combined 3D model obtained by combining the 3D model of the hidden target object and the 3D model of the hiding tool object; and a training data processing unit configured to process the 2D image with reference to deformation or a distortion which occurs in a detection target image obtained by actually capturing a detection target object to be detected.</p><p id="p-0013" num="0012">Further, there is provided another method and another system for implementing the present disclosure, and a non-transitory computer-readable recording medium having stored thereon a computer program for executing the aforementioned method.</p><p id="p-0014" num="0013">According to the present disclosure, it is possible to effectively generate a large number of pieces of training data in which various aspects of a hidden target object are included.</p><p id="p-0015" num="0014">Further, according to the present disclosure, it is possible to enhance the performance of an artificial intelligence-based detection model by increasing the diversity and the volume of training data.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a detailed internal configuration of a system of generating training data for a detection model based on an artificial intelligence according to one embodiment of the present disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a three-dimensional (3D) model of a hiding tool object generated according to one embodiment of the present disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a 3D model of a hidden target object generated according to one embodiment of the present disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> shows a combined 3D model obtained by combining (or synthesizing) the 3D model of the hiding tool object with the 3D model of the hidden target object according to one embodiment of the present disclosure.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> shows a combined 3D model obtained by combining (or synthesizing) the 3D model of the hiding tool object with the 3D model of the hidden target object according to one embodiment of the present disclosure.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> shows a two-dimensional (2D) image captured from a combined 3D model according to one embodiment of the present disclosure.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> shows a two-dimensional (2D) image captured from a combined 3D model according to one embodiment of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0023" num="0022">The following detailed description of the present disclosure will be provided with reference to the accompanying drawings that show, by way of illustration, specific embodiments in which the present disclosure may be practiced. These embodiments are described in sufficient detail to enable those skilled in the art to practice the present disclosure. It is to be understood that the various embodiments of the present disclosure, although different from each other, are not necessarily mutually exclusive. For example, specific shapes, structures and characteristics described herein may be implemented as modified from one embodiment to another without departing from the spirit and scope of the present disclosure. Furthermore, it shall be understood that the positions or arrangements of individual elements within each of the embodiments may also be modified without departing from the spirit and scope of the present disclosure. Therefore, the following detailed description is not to be taken in a limiting sense, and the scope of the present disclosure is to be taken as encompassing the scope of the appended claims and all equivalents thereof. In the drawings, like reference numerals refer to the same or similar elements throughout the several views.</p><p id="p-0024" num="0023">Hereinafter, various preferred embodiments of the present disclosure will be described in detail with reference to the accompanying drawings to enable those skilled in the art to easily implement the present disclosure.</p><p id="p-0025" num="0024">Configuration of Training Data Generation System</p><p id="p-0026" num="0025">An internal configuration of a training data generation system <b>100</b> which performs primary functions for the implementation of the present disclosure and functions of elements of the training data generation system <b>100</b> will be described below.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a detailed internal configuration of the training data generation system <b>100</b> according to one embodiment of the present disclosure.</p><p id="p-0028" num="0027">As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the training data generation system <b>100</b> according to one embodiment of the present disclosure may be configured to include a three-dimensional (3D) model management unit <b>110</b>, a training data generation unit <b>120</b>, a training data processing unit <b>130</b>, a communication unit <b>140</b>, and a control unit <b>150</b>. According to one embodiment of the present disclosure, at least some of the 3D model management unit <b>110</b>, the training data generation unit <b>120</b>, the training data processing unit <b>130</b>, the communication unit <b>140</b>, and the control unit <b>150</b> may be program modules that communicates with an external system (not shown). Such program modules may be included in the training data generation system <b>100</b> in the form of operating systems, application program modules, or other program modules and may be physically stored in a variety of commonly known storage devices. Further, such program modules may also be stored in a remote storage device that may communicate with the training data generation system <b>100</b>. Meanwhile, such program modules may include, but not limited to, routines, subroutines, programs, objects, components, data structures, and the like for performing specific tasks or executing specific abstract data types which will be described below according to the present disclosure.</p><p id="p-0029" num="0028">Although the training data generation system <b>100</b> is described as above, such a description is an example. As will be understood by those skilled in the art, at least some of the elements or functions of the training data generation system <b>100</b> may be implemented inside or included in an external system (not shown) as needed.</p><p id="p-0030" num="0029">First, the 3D model management unit <b>110</b> according to one embodiment of the present disclosure may perform a function of generating 3D models of hidden target objects (e.g., a knife, a drug, a bomb, and the like) and 3D models of hiding tool objects (e.g., a footwear, a bag, and the like). For example, a 3D model of a hidden target object and a 3D model of a hiding tool object may be generated by capturing images of the hidden target object and the hiding tool object using a certain capturing apparatus. A 3D CT (Computed Tomography) scanner may be used as the such capturing apparatus. Further, for example, a 3D model generated through the capturing by the 3D CT scanner, may include visual information about an internal structure and an external structure of the hidden target object or the hiding tool object.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a 3D model of a hiding tool object generated according to one embodiment of the present disclosure.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a 3D model of a hidden target object generated according to one embodiment of the present disclosure.</p><p id="p-0033" num="0032">Referring to <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref>, a 3D model <b>210</b> of the hidden target object may be generated by 3D CT scanning a hidden object having a rectangular parallelepiped shape as a hidden target object, and a 3D model <b>220</b> of the hiding tool object may be generated by 3D CT scanning a footwear as a hiding tool object to generate.</p><p id="p-0034" num="0033">Further, the 3D model management unit <b>110</b> according to one embodiment of the present disclosure may be configured to combine or synthesize the 3D model of the hidden target object and the 3D model of the hiding tool object, which are generated in the above-described manner, according to at least one aspect.</p><p id="p-0035" num="0034">Specifically, according to one embodiment of the present disclosure, the 3D model management unit <b>110</b> may provide a user interface configured to assist a user to be able to combine or synthesize the 3D models with each other. As an example, the user may combine or synthesize the 3D model of the hidden target object and the 3D model of the hiding tool object in various aspects while manipulating the 3D model of the hidden target object and the 3D model of the hiding tool object. As another example, the user may change at least one of a size, shape, and position of the 3D model of the hidden target object, which is to be combined with the 3D model of the hiding tool object. As yet another example, when combining or synthesizing the 3D model of the hiding tool object and the 3D model of the hidden target object, the user may change a positional relationship (direction, angle, or the like) between the 3D model of the hiding tool object and the 3D model of the hidden target object in various ways.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> show combined 3D models generated by combining (or synthesizing) a 3D model of a hiding tool object and a 3D model of a hidden target object according to one embodiment of the present disclosure.</p><p id="p-0037" num="0036">As shown in <figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref>, the combined 3D models in which the 3D model <b>210</b> of the hidden target object is hidden on each of the toe side (see <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) and the heel side (see <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>) in a 3D model <b>220</b> of a footwear as the hiding tool object, may be generated.</p><p id="p-0038" num="0037">Here, configurations of the generation and combination of the 3D models according to one embodiment of the present disclosure are not limited to the above example, and may be changed in various manners as long as the objects of the present disclosure can be accomplished. Further, a 3D CT scanner is used as a capturing apparatus used to generate the 3D model, but other capturing apparatuses than the 3D CT scanner may be also used.</p><p id="p-0039" num="0038">The training data generation unit <b>120</b> according to one embodiment of the present disclosure may perform a function of generating a 2D image by capturing, in at least one direction, an image of a combined 3D model obtained by combining the 3D model of the hidden target object and the 3D model of the hiding tool object.</p><p id="p-0040" num="0039">Specifically, the 2D image may be provided such that an image obtained by revealing the combined 3D model obtained by combining the 3D model of the hiding tool object and the 3D model of the hidden target object in a perspective way is included. An aspect included in the 2D image may vary depending on the capturing direction. For example, according to one embodiment of the present disclosure, the 2D image may be an image captured through a virtual capturing method which is simulated in the same manner as in an X-RAY scanning method (to be described later).</p><p id="p-0041" num="0040">According to one embodiment of the present disclosure, the 2D image may be obtained in the form of a 16 bit-grey scale having two kinds of output energies without a separate distortion process, such as resolution degradation or RGB change.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> show 2D images captured from the combined 3D model according to one embodiment of the present disclosure.</p><p id="p-0043" num="0042">Referring to <figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref>, the training data generation unit <b>120</b> may capture a 2D perspective image of various types of combined 3D models related to a footwear in which a hidden object is hidden and may generate training data about the footwear in which the hidden object is embedded.</p><p id="p-0044" num="0043">The training data processing unit <b>130</b> according to one embodiment of the present disclosure may process the 2D image generated in the above manner with reference to deformation or distortion that may occur in a detection target image obtained by actually capturing a detection target object to be detected. According to one embodiment of the present disclosure, the deformation or distortion may include deformation or distortion that may occur in the captured image according to a positional relationship between the detection target object and a capturing apparatus for capturing the detection target object or a distance therebetween due to characteristics of the capturing apparatus that has light beams emitted in a radial manner. Due to such deformation or distortion, captured images may be displayed in different forms even for the same article.</p><p id="p-0045" num="0044">Specifically, the training data processing unit <b>130</b> according to one embodiment of the present disclosure may process a 2D image in a manner that the distortion or the deformation detected from at least one detection target image obtained by actually capturing the detection target object as described above is included in the 2D image.</p><p id="p-0046" num="0045">For example, the training data processing unit <b>130</b> according to one embodiment of the present disclosure may predict deformation or distortion that may occur in a 2D image captured from a 3D model by using a separate artificial intelligence-based prediction model which is learned based on a position of a detection target object in a detection target image, a positional relationship between the detection target object and the apparatus for capturing the detection target object, specification information of the apparatus for capturing the detection target object, and data about the deformation or distortion that occurs in the detection target image, and may process the 2D image by adding the predicted deformation or distortion to the 2D image.</p><p id="p-0047" num="0046">The training data processing unit <b>130</b> according to one embodiment of the present disclosure may process the 2D image with reference to differences between a capturing apparatus (e.g., a 3D CT scanner) used to generate the 3D model of the hidden target object and the 3D model of the hiding tool object and a capturing apparatus (e.g., an X-ray scanner) for actually capturing a detection target object.</p><p id="p-0048" num="0047">Specifically, according to one embodiment of the present disclosure, the capturing apparatus used in capturing the hidden target object and the hiding tool object may be a CT scanner, and the capturing apparatus used in capturing the detection target object may be an X-ray scanner. The CT scanner, which employs a computer tomography method, projects x-rays or ultrasonic waves onto a human body at various angles and reconfigurates the projection results with a computer to process an internal cross section of the detection target object as an image. The X-ray scanner measures a degree to which the X-rays attenuates as they pass through the detection target object to display an internal structure of the detection target object as an image. Accordingly, the types of electromagnetic waves used in both the capturing apparatuses may be different from each other, and differences may occur in the capturing techniques and the imaging results. Such differences may be used as a reference in a step of processing the 2D image.</p><p id="p-0049" num="0048">For example, the training data processing unit <b>130</b> according to one embodiment of the present disclosure may determine a visual effect to be added to a 2D image captured from a 3D model by using a separate artificial intelligence-based determination model that is learned based on data about the capturing apparatus used to generate the 3D model of the hidden target object and the 3D model of the hiding tool object, data about the capturing apparatus for actually capturing the detection target object, data about a difference between the 2D image captured from the 3D model and an image of the detection target object, and may process the 2D image by adding the determined visual effect to the 2D image.</p><p id="p-0050" num="0049">Further, the training data processing unit <b>130</b> according to one embodiment of the present disclosure may process a 2D image with reference to at least one of specification information and identification information of the capturing apparatus for capturing the detection target object.</p><p id="p-0051" num="0050">For example, the training data processing unit <b>130</b> according to one embodiment of the present disclosure may determine a visual effect to be added to a 2D image captured from a 3D model by using a separate artificial intelligence-based determination model that is learned based on data about specification information and identification information of the capturing apparatus used to generate the 3D model of the hidden target object and the 3D model of the hiding tool object, and data about a difference between the 2D image captured from the 3D model and an image of the detection target object, and may process the 2D image by adding the determined visual effect to the 2D image.</p><p id="p-0052" num="0051">Further, according to one embodiment of the present disclosure, the 2D image generated and processed as above may be used as training data of the artificial intelligence-based detection model.</p><p id="p-0053" num="0052">Specifically, according to one embodiment of the present disclosure, the artificial intelligence-based detection model may be learned to detect a hidden target object from a 2D image related to the hidden target object based on at least one 2D image (i.e., training data) in which aspects of the hidden target object and a hiding tool object are included.</p><p id="p-0054" num="0053">The communication unit <b>140</b> according to one embodiment of the present disclosure may perform a function to enable data transmission and reception from/to the 3D model management unit <b>110</b>, the training data generation unit <b>120</b>, and the training data processing unit <b>130</b>.</p><p id="p-0055" num="0054">The control unit <b>150</b> according to one embodiment of the present disclosure may function to control data flow among the 3D model management unit <b>110</b>, the training data generation unit <b>120</b>, the training data processing unit <b>130</b>, and the communication unit <b>140</b>. That is, the control unit <b>150</b> according to the present disclosure may control data flow of the training data generation system <b>100</b> from into/out of the training data generation system <b>100</b> or data flow among the elements of the training data generation system <b>100</b>, so that the 3D model management unit <b>110</b>, the training data generation unit <b>120</b>, the training data processing unit <b>130</b>, and the communication unit <b>140</b> may carry out their particular.</p><p id="p-0056" num="0055">The method of generating and processing the training date of the artificial intelligence-based detection model according to one embodiment of the present disclosure is not limited to the above-described examples, and may be varied in other various manners as long as the objects of the present disclosure can be achieved.</p><p id="p-0057" num="0056">The embodiments according to the present disclosure as described above may be implemented in the form of program instructions that can be executed by various computer components, and may be stored on a computer-readable recording medium. The computer-readable recording medium may include program instructions, data files, and data structures, separately or in combination. The program instructions stored on the computer-readable recording medium may be specially designed and configured for the present disclosure, or may also be known and available to those skilled in the computer software field. Examples of the computer-readable recording medium may include the following: magnetic media such as hard disks, floppy disks and magnetic tapes; optical media such as compact disk-read only memory (CD-ROM) and digital versatile disks (DVDs); magneto-optical media such as floptical disks; and hardware devices such as read-only memory (ROM), random access memory (RAM) and flash memory, which are specially configured to store and execute program instructions. Examples of the program instructions include not only machine language codes created by a compiler, but also high-level language codes that can be executed by a computer using an interpreter. The above hardware devices may be changed to one or more software modules to perform the processes of the present disclosure, and vice versa.</p><p id="p-0058" num="0057">Although the present disclosure has been described above in terms of specific items such as detailed elements as well as the limited embodiments and the drawings, they are only provided to help more general understanding of the present disclosure, and the present disclosure is not limited to the above embodiments. It will be appreciated by those skilled in the art to which the present disclosure pertains that various modifications and changes may be made from the above description.</p><p id="p-0059" num="0058">Therefore, the spirit of the present disclosure shall not be limited to the above-described embodiments, and the entire scope of the appended claims and their equivalents will fall within the scope and spirit of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of generating training data for detection model based on an artificial intelligence, comprising the steps of:<claim-text>generating a three-dimensional (3D) model of a hidden target object and a 3D model of a hiding tool object, respectively, and combining the 3D model of the hidden target object and the 3D model of the hiding tool object;</claim-text><claim-text>generating a two-dimensional (2D) image by capturing, in at least one direction, the combined 3D model obtained by combining the 3D model of the hidden target object and the 3D model of the hiding tool object; and</claim-text><claim-text>processing the generated 2D image with reference to deformation or distortion which occurs in a detection target image obtained by actually capturing a detection target object to be detected.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the deformation or the distortion includes deformation or distortion that occurs in the detection target image according to a positional relationship between the detection target object and a capturing apparatus for capturing the detection target object.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the combined 3D model is generated by combining the 3D model of the hidden target object and the 3D model of the hiding tool object, which are generated independently of each other, according to at least one aspect.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein at least one of a size, a shape, and a position of the 3D model of the hidden target object, which is combined with the 3D model of the hiding tool object, is changed.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein in the processing of the generated 2D image, the 2D image is processed in a manner that the distortion or the deformation is included in the 2D image.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein in the processing of the generated 2D image, the 2D image is processed with reference to a difference between a capturing apparatus used to generate the 3D model of the hidden target object and the 3D model of the hiding tool object and a capturing apparatus for capturing the detection target object.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein in the processing of the generated 2D image, the 2D image is processed with reference to at least one of specification information and identification information of a capturing apparatus for capturing the detection target object.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A non-transitory computer-readable recording medium having stored thereon a computer program for executing the method of <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A system of generating training data for detection model based on an artificial intelligence, comprising:<claim-text>a 3D model management unit configured to generate a 3D model of a hidden target object and a 3D model of a hiding tool object, respectively, and combine the 3D model of the hidden target object and the 3D model of the hiding tool object;</claim-text><claim-text>a training data generation unit configured to generate a 2D image by capturing, in at least one direction, the combined 3D model obtained by combining the 3D model of the hidden target object and the 3D model of the hiding tool object; and</claim-text><claim-text>a training data processing unit configured to process the 2D image with reference to deformation or a distortion which occurs in a detection target image obtained by actually capturing a detection target object to be detected.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the deformation or the distortion includes deformation or distortion that occurs in the detection target image according to a positional relationship between the detection target object and a capturing apparatus for capturing the detection target object.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the combined 3D model is generated by combining the 3D model of the hidden target object and the 3D model of the hiding tool object, which are generated independently of each other, according to at least one aspect.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein at least one of a size, a shape, and a position of the 3D model of the hidden target object, which is combined with the 3D model of the hiding tool object, is changed.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the training data processing unit processes the 2D image in a manner that the distortion or the deformation is included in the 2D image.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the training data processing unit processes the 2D image with reference to a difference between a capturing apparatus used to generate the 3D model of the hidden target object and the 3D model of the hiding tool object and a capturing apparatus for capturing the detection target object.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the training data processing unit processes the 2D image with reference to at least one of specification information and identification information of a capturing apparatus for capturing the detection target object.</claim-text></claim></claims></us-patent-application>