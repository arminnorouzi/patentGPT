<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004214A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004214</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17940761</doc-number><date>20220908</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>KR</country><doc-number>10-2020-0188276</doc-number><date>20201230</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>011</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>017</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>006</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">ELECTRONIC APPARATUS AND CONTROLLING METHOD THEREOF</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/KR2021/020135</doc-number><date>20211229</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17940761</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SAMSUNG ELECTRONICS CO., LTD.</orgname><address><city>Suwon-si</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Shcherbina</last-name><first-name>Artem</first-name><address><city>Kyiv</city><country>UA</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Bondarets</last-name><first-name>Ivan</first-name><address><city>Kyiv</city><country>UA</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Trunov</last-name><first-name>Oleksandr</first-name><address><city>Kyiv</city><country>UA</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Olshevskyi</last-name><first-name>Viacheslav</first-name><address><city>Kyiv</city><country>UA</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Savin</last-name><first-name>Volodymyr</first-name><address><city>Kyiv</city><country>UA</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>SAMSUNG ELECTRONICS CO., LTD.</orgname><role>03</role><address><city>Suwon-si</city><country>KR</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An electronic apparatus and a controlling method thereof are provided. The electronic apparatus providing augmented reality (AR) content includes a display, a camera and a processor configured to display augmented reality (AR) content through the display, detect a hand of a user from image obtained through the camera, and identify a first interaction of the hand with the AR content based on a size of the hand, wherein the size of the hand is obtained based on the information about an object provided through the display.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="52.24mm" wi="69.09mm" file="US20230004214A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="85.34mm" wi="50.38mm" file="US20230004214A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="78.74mm" wi="91.78mm" file="US20230004214A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="79.76mm" wi="71.12mm" file="US20230004214A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="91.02mm" wi="64.18mm" file="US20230004214A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="81.79mm" wi="82.13mm" file="US20230004214A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="104.22mm" wi="81.62mm" file="US20230004214A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="157.90mm" wi="142.24mm" file="US20230004214A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="128.95mm" wi="124.12mm" file="US20230004214A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="86.87mm" wi="70.44mm" file="US20230004214A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="86.87mm" wi="70.44mm" file="US20230004214A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="86.87mm" wi="70.44mm" file="US20230004214A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="218.10mm" wi="156.63mm" file="US20230004214A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="194.65mm" wi="154.01mm" orientation="landscape" file="US20230004214A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="112.44mm" wi="70.95mm" file="US20230004214A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="119.38mm" wi="61.47mm" file="US20230004214A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="136.57mm" wi="73.41mm" file="US20230004214A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="136.57mm" wi="73.32mm" file="US20230004214A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="222.00mm" wi="145.37mm" file="US20230004214A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="120.99mm" wi="87.21mm" file="US20230004214A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="111.42mm" wi="132.25mm" file="US20230004214A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="127.17mm" wi="135.89mm" file="US20230004214A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="104.22mm" wi="104.14mm" file="US20230004214A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading><p id="p-0002" num="0001">This application is a bypass continuation of International Application No. PCT/KR2021/020135, filed on Dec. 29, 2021, which is based on and claims priority to Korean Patent Application No. 10-2020-0188276, filed on Dec. 30, 2020, in the Korean Intellectual Property Office, the disclosures of which are incorporated by reference herein in their entireties.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">1. Field</heading><p id="p-0003" num="0002">The disclosure relates to an electronic apparatus and a controlling method thereof, and more particularly, to an electronic apparatus for estimating a distance of an object using a camera and a controlling method thereof.</p><heading id="h-0004" level="1">2. Description of the Related Art</heading><p id="p-0004" num="0003">With the development of electronic technology, augmented reality (AR) market is rapidly growing. The AR market is focusing on two sub-trends such as software (SW) and AR application (apps) development including mobile AR, AR for shopping, AR for navigation, AR for enterprise, etc., and hardware (HW) development in which the AR domain is largely expanded thanks to the emergence of a neural network processing unit (NPU), digital signal processing (DSP), and artificial intelligence (AI) technology.</p><p id="p-0005" num="0004">Recently, the size of a wearable AR device (e.g., AR glasses, etc.) is continuously becoming smaller and lightweight, and as a result of such miniaturization, there is less space for sensors and battery in an electronic apparatus. The size of electronic device and backup power decrease leads to switching from depth sensors (e.g., structured light, time of flight (ToF), etc.) to related art cameras (e.g., red, green, blue (RGB) camera), switching from stereo vision to monocular vision, and switching from global shutter to rolling shutter cameras for miniaturization and cost saving. As such, monocular cameras are getting important.</p><p id="p-0006" num="0005">When an electronic apparatus displays a virtual object such as content rendered in a three-dimensional (3D) space and a user interface (UI) element in AR environment, a user may perform interaction with the virtual object. A natural way of interaction with such objects is using gestures of moving hands of a user.</p><p id="p-0007" num="0006">However, it is difficult to accurately estimate the distance (or position of the hand) between the electronic apparatus and the hand (in particular, the moving hand) using only the two-dimensional image obtained through the monocular camera. This may lead to interaction with a different virtual object (i.e., imprecise interaction) other than an intended virtual object, or interaction failure.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0008" num="0007">Provided are an electronic apparatus for estimating a distance to an object more accurately by using a camera and a controlling method thereof.</p><p id="p-0009" num="0008">According to an aspect of the disclosure, an electronic apparatus may include a display, a camera and a processor configured to display augmented reality (AR) content thorough the display, detect a hand of a user based on image obtained through the camera, and identify a first interaction of the hand with the AR content based on a size of the hand, wherein the size of the hand is obtained based on the information about an object provided through the display.</p><p id="p-0010" num="0009">The processor may be further configured to set the size of the detected hand to a preset value, identify whether the second interaction of the hand occurs for the object provided through the display based on the set size, based on identifying that the second interaction occurs, identify the size of the hand based on the information on the object, and identify the first interaction of the hand for the AR content based on the identified size of the hand.</p><p id="p-0011" num="0010">The object may comprise at least one of a first type object included in the image obtained through the camera and a second type object included in the AR content displayed on the display.</p><p id="p-0012" num="0011">The electronic apparatus may include a memory storing feature information and size information of a reference object, wherein the processor is further configured to, based on the first type object and the hand of the user being detected from image obtained through the camera, identify whether the first type object is the reference object based on the feature information stored in the memory and feature information of the detected first type object, and based on identifying that the first type object is the reference object, identify the size of the hand based on a size of the reference object.</p><p id="p-0013" num="0012">The processor may be further configured to, based on identifying that the first type object is not the reference object based on the feature information stored in the memory and the feature information of the detected first type object, identify the size of the first type object included in consecutive image frames by using the consecutive image frames obtained through the camera, and identify the size of the hand based on the size of the first type object.</p><p id="p-0014" num="0013">The processor may be further configured to set the size of the hand to a preset value, identify whether the second interaction of the hand occurs with the second type object through the display based on the set size, and identify the size of the hand based on a depth of the second type object in which interaction of the hand is identified.</p><p id="p-0015" num="0014">In accordance with an aspect of the disclosure, a method of controlling an electronic apparatus may include displaying augmented reality (AR) content on the display, detecting a hand of a user based on image obtained through the camera and identifying a first interaction of the hand with the AR content based on a size of the hand, wherein the size of the hand is obtained based on the information about an object provided through the display.</p><p id="p-0016" num="0015">The method may further include setting the size of the detected hand to a preset value, identifying whether the second interaction of the hand occurs for the object provided through the display based on the set size and based on identifying that the second interaction occurs, identifying the size of the hand based on the information on the object.</p><p id="p-0017" num="0016">The object comprises at least one of a first type object included in the image obtained through the camera and a second type object included in the AR content displayed on the display.</p><p id="p-0018" num="0017">The method may further include based on the first type object and the hand of the user being detected from image obtained through the camera, identifying whether the first type object is a reference object based on the feature information stored in the electronic apparatus and feature information of the detected first type object and based on identifying that the first type object is the reference object, identifying the size of the hand based on a size of the reference object.</p><p id="p-0019" num="0018">The method may further include based on identifying that the first type object is not the reference object based on the feature information stored in the electronic apparatus and the feature information of the detected first type object, identifying the size of the first type object included in consecutive image frames by using the consecutive image frames obtained through the camera, and identifying the size of the hand based on the size of the first type object.</p><p id="p-0020" num="0019">The method may further include setting the size of the hand to a preset value, identifying whether the second interaction of the hand occurs with the second type object through the display based on the set size and identifying the size of the hand based on a depth of the second type object in which interaction of the hand is identified.</p><p id="p-0021" num="0020">According to various embodiments of the disclosure, an electronic apparatus for estimating a distance to an object more accurately by using a camera and a controlling method thereof are provided.</p><p id="p-0022" num="0021">According to an embodiment of the disclosure, the size of a user's hand may be accurately estimated, and a parameter for a user's hand may be accurately estimated.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0023" num="0022">The above and other aspects, features, and advantages of certain embodiments of the present disclosure will be more apparent from the following description taken in conjunction with the accompanying drawings, in which:</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> are diagrams illustrating an electronic apparatus according to an embodiment of the disclosure;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>1</b>C</figref> is a diagram illustrating a parameter of a user's hand according to an embodiment of the disclosure;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>1</b>D</figref> is a diagram illustrating a user's hand size according to an embodiment of the disclosure;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a configuration of an electronic apparatus according to an embodiment of the disclosure;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating an operation of a processor according to an embodiment of the disclosure;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating a method of identifying a size of a hand through a focal length according to an embodiment of the disclosure;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating a relationship between a focal length and a focal distance according to an embodiment of the disclosure;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIGS. <b>6</b>A, <b>6</b>B, and <b>6</b>C</figref> are diagrams illustrating a method of identifying a size of a hand through a focal length according to an embodiment of the disclosure;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram illustrating a method of identifying a hand size through an object interacting with a hand according to an embodiment of the disclosure;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> is a diagram illustrating a method of detecting an object interacting with a hand according to an embodiment of the disclosure;</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIGS. <b>8</b>B and <b>8</b>C</figref> are diagrams illustrating a method of detecting an object interacting with a hand according to an embodiment of the disclosure;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIGS. <b>9</b>A and <b>9</b>B</figref> are diagrams illustrating a method of identifying a size of a hand through an object of a first type according to an embodiment of the disclosure;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating a method of identifying a size of a hand through an object of a first type according to an embodiment of the disclosure;</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram illustrating a method of identifying a hand size through an object of a second type according to an embodiment of the disclosure;</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>12</b>A</figref> is a diagram illustrating an additional configuration of an electronic apparatus according to an embodiment of the disclosure;</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>12</b>B</figref> is a diagram illustrating an example of an electronic apparatus according to an embodiment of the disclosure; and</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram illustrating a flowchart according to an embodiment of the disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0041" num="0040">In the following description, a detailed description of known functions and configurations may be omitted when it may obscure the subject matter of the disclosure. In addition, the following embodiments may be modified in many different forms, and the scope of the technical spirit of the disclosure is not limited to the following examples. Rather, these embodiments are provided so that this disclosure will be thorough and complete, and will fully convey the technical spirit to those skilled in the art.</p><p id="p-0042" num="0041">However, it should be understood that the present disclosure is not limited to the specific embodiments described hereinafter, but includes various modifications, equivalents, and/or alternatives of the embodiments of the present disclosure. In relation to explanation of the drawings, similar drawing reference numerals may be used for similar constituent elements throughout.</p><p id="p-0043" num="0042">The terms &#x201c;first,&#x201d; &#x201c;second,&#x201d; as used in the disclosure, may be modified regardless of the order and/or importance, and the components may not be used to distinguish one element from other components but are not limited thereto.</p><p id="p-0044" num="0043">In the description, expressions such as &#x201c;A or B,&#x201d; &#x201c;at least one of A or/and B,&#x201d; or &#x201c;one or more of A or/and B&#x201d; may include all possible combinations of the items that are enumerated together. For example, the term &#x201c;at least one of A [or/and] B&#x201d; means (1) including at least one A, (2) including at least one B, or (3) including both at least one A and at least one B.</p><p id="p-0045" num="0044">A singular expression includes a plural expression, unless otherwise specified. It is to be understood that the terms such as &#x201c;comprise&#x201d; or &#x201c;include&#x201d; are used herein to designate a presence of a characteristic, number, step, operation, element, component, or a combination thereof, and not to preclude a presence or a possibility of adding one or more of other characteristics, numbers, steps, operations, elements, components or a combination thereof.</p><p id="p-0046" num="0045">If it is described that a certain element (e.g., first element) is &#x201c;operatively or communicatively coupled with/to&#x201d; or is &#x201c;connected to&#x201d; another element (e.g., second element), it should be understood that the certain element may be connected to the other element directly or through still another element (e.g., third element). On the other hand, if it is described that a certain element (e.g., first element) is &#x201c;directly coupled to&#x201d; or &#x201c;directly connected to&#x201d; another element (e.g., second element), it may be understood that there is no element (e.g., third element) between the certain element and the other element.</p><p id="p-0047" num="0046">Also, the expression &#x201c;configured to&#x201d; used in the disclosure may be interchangeably used with other expressions such as &#x201c;suitable for,&#x201d; &#x201c;having the capacity to,&#x201d; &#x201c;designed to,&#x201d; &#x201c;adapted to,&#x201d; &#x201c;made to,&#x201d; and &#x201c;capable of,&#x201d; depending on cases. Meanwhile, the term &#x201c;configured to&#x201d; does not necessarily mean that a device is &#x201c;specifically designed to&#x201d; in terms of hardware. Instead, under some circumstances, the expression &#x201c;a device configured to&#x201d; may mean that the device &#x201c;is capable of&#x201d; performing an operation together with another device or component. For example, the phrase &#x201c;a processor configured to perform A, B, and C&#x201d; may mean a dedicated processor (e.g., an embedded processor) for performing the corresponding operations, or a generic-purpose processor (e.g., a central processing unit (CPU) or an application processor) that can perform the corresponding operations by executing one or more software programs stored in a memory device.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> are diagram illustrating an electronic apparatus according to an embodiment.</p><p id="p-0049" num="0048">Referring to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>, an electronic apparatus <b>100</b> may provide content of augmented reality (AR).</p><p id="p-0050" num="0049">The electronic apparatus <b>100</b> may be implemented as a wearable device which a user may wear. A wearable device may be embodied as various types of devices such as an accessory type (e.g., a watch, a ring, a bracelet, an ankle bracelet, a necklace, a pair of glasses, a contact lens or a head-mounted-device (HMD)); a fabric or a garment-embedded type (e.g.: electronic cloth); skin-attached type (e.g., a skin pad or a tattoo); a bio-implantable circuit, or the like. The electronic apparatus <b>100</b> may be implemented as a smartphone, a tablet personal computer (PC), an e-book reader, a laptop PC, a navigator, a vehicle, or the like. The electronic apparatus <b>100</b> may be implemented as a bendable flexible device.</p><p id="p-0051" num="0050">An AR content may represent content provided in an AR environment. The AR environment may allow a virtual object to appear as if the virtual object is actually present in the surrounding environment by providing AR content, which represents a virtual object that does not actually exist in the surrounding environment of the user, along with the surrounding environment through the display. Furthermore, the AR content may be content for providing additional information (e.g., additional information for an object present in an actually existing environment, weather information, etc.).</p><p id="p-0052" num="0051">Referring to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, the electronic apparatus <b>100</b> may obtain an image frame through the camera <b>120</b>.</p><p id="p-0053" num="0052">The camera <b>120</b> may be a general camera, unlike the stereo camera and the depth camera. The camera <b>120</b> may include a monocular camera. The monocular camera may include a camera capable of obtaining an image frame that includes two-dimensional position information (e.g., position information on the x-axis representing the horizontal direction and on the y-axis representing the vertical direction). The monocular camera may be lightweight, may be miniaturized, and reduce costs compared to a stereo camera including two monocular cameras, or a depth camera capable of obtaining depth information.</p><p id="p-0054" num="0053">The image frame may include a hand <b>1</b> of a user. Specifically, the camera <b>120</b> may obtain an image frame including the hand <b>1</b> of the user by capturing the hand <b>1</b> of the user.</p><p id="p-0055" num="0054">For example, referring to <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, the electronic apparatus <b>100</b> may display the AR content to the display <b>110</b>. The electronic apparatus <b>100</b> may match a region (or pixel) of the display <b>110</b> to a region (or position) on a virtual three-dimensional space, and may display the AR content on a region of the display <b>110</b> corresponding to a 3D region. The AR content displayed on the display <b>110</b> may include three-dimensional position information.</p><p id="p-0056" num="0055">In this example, the electronic apparatus <b>100</b> may identify an interaction with the AR object displayed on the display <b>110</b> based on the hand <b>1</b> of the user (or pose of the hand <b>1</b>) included in the image frame obtained through the camera <b>120</b>.</p><p id="p-0057" num="0056">For example, the electronic apparatus <b>100</b> may detect the hand <b>1</b> of the user included in the image frame obtained through the camera <b>120</b>, compare the position information of the hand <b>1</b> (or position information of the hand <b>1</b> defined according to the pose of the hand <b>1</b>) and the position information of the AR object, and identify whether an interaction with respect to the AR object is generated. However, this is merely an example, and the electronic apparatus <b>100</b> may identify an interaction between the hand <b>1</b> and the AR object using various related art methods.</p><p id="p-0058" num="0057">The interaction may indicate at least one of an operation in which the hand of a user contacts the AR object, an operation that the hand of the user points to the AR object, and an operation in which the hand of the user approaches the AR object. For example, the electronic apparatus <b>100</b> may determine that an interaction with the AR object displayed on the display <b>110</b> is generated when the hand <b>1</b> of the user is located at a position (e.g., xyz coordinate) on which the AR object is displayed, or the distance between the hand <b>1</b> of the user and the position (e.g., xyz coordinate) where the AR object is displayed is less than a predetermined distance.</p><p id="p-0059" num="0058">The hand <b>1</b> of the user (or pose of the hand <b>1</b>) may be defined based on the parameter of the hand <b>1</b>.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>1</b>C</figref> is a diagram illustrating a parameter of a user's hand according to an embodiment.</p><p id="p-0061" num="0060">Referring to <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>, a parameter for the hand <b>1</b> may include position information (i.e., a position on the x-axis, the y-axis, and the z-axis) indicating a position of the hand <b>1</b> (or wrist, finger, etc.) on the three-dimensional space. The parameters for the hand <b>1</b> may also include rotation information (i.e., rotation for pitch, yaw, and roll) indicating the direction and the degree of rotation of the hand <b>1</b> around the central axis of the wrist of the user.</p><p id="p-0062" num="0061">As such, the parameters for the hand <b>1</b> may include three degrees of freedom (3DOF) representing three motion directions, such as position information on the x-axis, y-axis, and z-axis or rotation information of pitch, yaw, and roll. Alternatively, six degrees of freedom (6DOF) indicating six motion directions such as position information on the x-axis, y-axis, and z-axis relative to hand <b>1</b>, and rotation information of pitch, yaw, and roll may be included. However, this is merely exemplary and the parameters for the hand <b>1</b> may mean various parameters such as a finger length, a finger bending angle, and the like.</p><p id="p-0063" num="0062">The parameters for the hand <b>1</b> (i.e., 3DOF or 6DOF), may typically be calculated based on the size of the hand.</p><p id="p-0064" num="0063">According to an embodiment, the electronic apparatus <b>100</b> may identify the size of the hand <b>1</b> using the image frame obtained through the camera <b>120</b> to determine parameters for the hand <b>1</b>. The electronic apparatus <b>100</b> may identify the interaction of the hand <b>1</b> and the AR object displayed on the display <b>110</b> using parameters for the hand <b>1</b>.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>1</b>D</figref> is a diagram illustrating a user's hand size according to an embodiment.</p><p id="p-0066" num="0065">Referring to <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>, the size of the hand <b>1</b> may include at least one of a width WH (or breadth) of the hand <b>1</b>, an area (or volume) of the hand <b>1</b>, a length of a finger constituting the hand <b>1</b>, and the like. The electronic apparatus <b>100</b> may estimate the size of the hand <b>1</b> in unit of millimeter.</p><p id="p-0067" num="0066">According to an embodiment, even if the camera <b>120</b> is used, the hand <b>1</b> and the AR object displayed on the display <b>110</b> may be accurately identified based on the size of the hand <b>1</b>.</p><p id="p-0068" num="0067">Hereinafter, a method for determining the size of a hand according to various embodiments will be described in detail with reference to the accompanying drawings.</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a configuration of an electronic apparatus according to an embodiment of the disclosure.</p><p id="p-0070" num="0069">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the electronic apparatus <b>100</b> may include a display <b>110</b>, a camera <b>120</b>, and a processor <b>130</b>.</p><p id="p-0071" num="0070">The display <b>110</b> is a device that provides visual information. For example, the display <b>110</b> may display the image frame in the entire or partial area of the display area. The display area of the display <b>110</b> may include a plurality of pixels divided by different positions. The display <b>110</b> may display an image frame by emitting light having a color and a brightness value included in each pixel of the image frame for each pixel of the display <b>110</b>.</p><p id="p-0072" num="0071">According to an embodiment, the display <b>110</b> may be implemented as a liquid crystal display (LCD) that uses a separate backlight unit (e.g., a light emitting diode (LED)) as a light source and controls the molecular arrangement of a liquid crystal, thereby controlling the degree of light emitting from the backlight unit to be transmitted through the liquid crystal (brightness of light or intensity of light). According to another embodiment, the display <b>110</b> may be implemented as a display using a self-light-emitting device (e.g., a mini LED having a size of 100-200 um, a micro LED light having a size of 100 um or less, an organic LED (OLED), a quantum dot LED (QLED), etc.) as a light source without a separate backlight unit or a liquid crystal.</p><p id="p-0073" num="0072">The display <b>110</b> may be implemented in the form of a touch screen capable of sensing a touch operation of a user. As another example, the display <b>110</b> may be implemented in the form of a flexible display having a characteristic that a portion of the display <b>110</b> may be bent or folded or unfolded again, or the display <b>110</b> may be implemented as a transparent display having a characteristic that allows the display <b>110</b> to show an object located behind the display <b>110</b> to be transparently seen.</p><p id="p-0074" num="0073">The camera <b>120</b> may obtain an image frame. The camera <b>120</b> may obtain an image frame including an object by capturing an object (i.e., a subject) existing in a field of view (FoV) in a specific point of view (PoV). For example, an object included in an image frame may include an object or a user's hand, in a surrounding environment.</p><p id="p-0075" num="0074">The camera <b>120</b> may include at least one lens <b>121</b> (<figref idref="DRAWINGS">FIG. <b>5</b></figref>), an image sensor <b>123</b> (<figref idref="DRAWINGS">FIG. <b>5</b></figref>), and an image processor. The lens <b>121</b> may condense or split the light reflected from the subject to the image sensor <b>123</b>. The image sensor <b>123</b> may include a plurality of pixels arranged to be divided into different positions on a two-dimensional plane. The image sensor <b>123</b> may divide the light transmitted from the lens <b>121</b> into pixel units to detect red, green, and blue colors for each pixel to generate an electrical signal. The image processor may obtain an image frame representing the color and brightness of the subject according to the electrical signal sensed by the image sensor <b>123</b>. Here, in the image frame, a real 3D space is projected in a virtual 2D plane, i.e., the image frame may include a plurality of pixels having different 2D position information (e.g., an x-axis position, a y-axis position). Each pixel of the image frame may include a particular color and brightness value.</p><p id="p-0076" num="0075">The camera <b>120</b> may perform continuous capturing with respect to time with a frame rate indicating a capturing speed (or a capturing cycle) to sequentially (or periodically) obtain a plurality of image frames. For example, when the camera <b>120</b> captures a surrounding environment with a frame rate of 30 frame per second (fps), <b>30</b> image frames per second may be sequentially obtained.</p><p id="p-0077" num="0076">The image frame obtained through the camera <b>120</b> may include information on at least one of a frame rate, a capturing time, and a view angle captured by the camera <b>120</b>. The field of view may be determined according to the focal length of the lens <b>121</b> of the camera <b>120</b> and the size (e.g., diagonal length) of the image sensor <b>123</b> of the camera <b>120</b>, or the like. The information on at least one of the above-described frame rate, time, and field of view may be included in the image frame itself or included in metadata corresponding to the image frame.</p><p id="p-0078" num="0077">The processor <b>130</b> may control the electronic apparatus <b>100</b> or overall configuration of the electronic apparatus <b>100</b>. The processor <b>130</b> may control the electronic apparatus <b>100</b> by executing at least one instruction stored in a memory provided in the processor <b>130</b> or a memory <b>160</b> (see <figref idref="DRAWINGS">FIG. <b>13</b></figref>) existing outside the processor <b>130</b>. The memory included in the processor <b>130</b> may include a read-only memory (ROM) (e.g., NOR NAND or NAND type flash memory), a random access memory (RAM) (e.g., dynamic RAM (DRAM), a synchronous DRAM (SDRAM), a double data rate SDRAM (DDR SDRAM), a volatile memory, and the like.</p><p id="p-0079" num="0078">The processor <b>130</b> may be configured as one or a plurality of processors, and each processor may be implemented as a general-use processor such as a central processing unit (CPU), an application processor (AP), a graphics-only processor such as a graphic processing unit (GPU), a vision processing unit (VPU), or the like, or an artificial intelligence (AI)-only processor such as a neural processing unit (NPU). The plurality of processors may be implemented in the form of an integrated chipset or may be implemented in the form of a separate chipset.</p><p id="p-0080" num="0079">The GPU and CPU may perform the operation of the disclosure in association with each other. For example, the GPU may process image frames or the like in data, and the CPU may process the remaining data (e.g., instructions, code, etc.). In this example, the GPU is implemented with a structure having hundreds or thousands of cores specialized in a parallel processing method for processing various instructions or data at the same time, and the CPU may be implemented with a structure having several cores specialized in a serial processing method in which instructions or data are processed in an input order. For example, the GPU may process the image frame of the disclosure to obtain information, and the CPU may process or operate information obtained through the image frame.</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating an operation of a processor according to an embodiment of the disclosure.</p><p id="p-0082" num="0081">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in operation S<b>310</b>, the processor <b>130</b> may display AR content on the display <b>110</b>. The processor <b>130</b> may control the display <b>110</b> to display AR content.</p><p id="p-0083" num="0082">The AR content may be implemented in various types such as an image, a moving image, an animation effect, etc. For example, AR content may be a real object (e.g., a TV, a digital picture, a sound bar, a refrigerator, a washing machine, furniture, a vehicle, a building, a tree, etc.) in a two-dimensional or three-dimensional rendered image. For example, the AR content may be one of a variety of types, such as text, character, images, pictures, videos, documents, dashboard, and the like. The processor <b>130</b> may control the display <b>110</b> to display AR content by adjusting the transparency of the AR content. The AR content may be displayed on the display <b>110</b> in an opaque or translucent state.</p><p id="p-0084" num="0083">The AR content may be displayed in a location on the virtual three-dimensional space (e.g., xyz coordinates) through the display <b>110</b>. Here, the location on the virtual three-dimensional space may be mapped to a two-dimensional plane of the display <b>110</b> (e.g., flat or curved plane).</p><p id="p-0085" num="0084">For example, the processor <b>130</b> may display the AR content such that the center of the AR content is located in a pixel located at the coordinate (90, 180) of the display <b>110</b> corresponding to the coordinate (100, 200, 50) on the virtual three-dimensional space.</p><p id="p-0086" num="0085">In operation S<b>320</b>, the processor <b>130</b> may detect the user's hand from the image frames obtained through the camera <b>120</b>.</p><p id="p-0087" num="0086">In this example, programming libraries for analyzing real-time computer vision (e.g., OpenCV, Python, etc.), various color filters such as Sobel filter or Fuzzy filter, and various algorithms such as canny edge detection, color-based, template-based, and background differentiation methods may be used.</p><p id="p-0088" num="0087">For example, the processor <b>130</b> may perform a preprocessing to binarize the color of a plurality of pixels included in the image frame. The processor <b>130</b> may detect bundle (or group) of adjacent pixels having similar colors based on the binarized color (or contrast) of each pixel as one objects, and the processor <b>130</b> may detect (identify) an object having a shape, a rate and a curvature similar to the hand (or finger) of the identified object as the user's hand <b>1</b>.</p><p id="p-0089" num="0088">In operation S<b>330</b>, the processor <b>130</b> may identify the interaction of the hand <b>1</b> for the AR content based on the size of the hand <b>1</b>.</p><p id="p-0090" num="0089">Specifically, the size of the hand <b>1</b> may be set to a predetermined value. Alternatively, the size of the hand <b>1</b> may be a value obtained based on the information on the object when the interaction of the hand with respect to the object provided through the display <b>110</b> is generated. The interaction of the hand with respect to the object provided through the display <b>110</b> may be an operation that occurs before the interaction of the hand <b>1</b> with respect to the AR content described above.</p><p id="p-0091" num="0090">The processor <b>130</b> may identify the pixel size of hand <b>1</b> detected on the image frame. The pixel size may mean the number of pixels included in the area representing at least one of the width (or breadth) of the hand <b>1</b> detected in the image frame, the length of the finger constituting the hand <b>1</b>, and the area (or volume) of the hand <b>1</b>.</p><p id="p-0092" num="0091">The processor <b>130</b> may calculate the distance between the electronic apparatus <b>100</b> and the hand <b>1</b> using the size of the hand <b>1</b> and the pixel size of the hand. Here, the distance may mean a value (or depth) on the z-axis.</p><p id="p-0093" num="0092">For example, the processor <b>130</b> may calculate the distance of the hand <b>1</b> corresponding to the size of the hand <b>1</b> and the pixel size of the hand <b>1</b> according to Table 1 in which the corresponding relationship among the size of the hand <b>1</b>, distance of the hand <b>1</b>, and the pixel size of the hand <b>1</b> detected from the image frame are calculated.</p><p id="p-0094" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="91pt" align="center"/><colspec colname="2" colwidth="35pt" align="center"/><colspec colname="3" colwidth="91pt" align="center"/><thead><row><entry namest="1" nameend="3" rowsep="1">TABLE 1</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row><row><entry>Size</entry><entry>Distance</entry><entry>Pixel size</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>0.2 meters</entry><entry>0.5 meters</entry><entry>200</entry></row><row><entry>0.2 meters</entry><entry>&#x2002;1 meter</entry><entry>100</entry></row><row><entry>0.2 meters</entry><entry>&#x2002;&#x2009;2 meters</entry><entry>&#x2002;50</entry></row><row><entry>0.3 meters</entry><entry>0.5 meters</entry><entry>300</entry></row><row><entry>0.3 meters</entry><entry>&#x2002;1 meter</entry><entry>150</entry></row><row><entry>0.3 meters</entry><entry>&#x2002;&#x2009;2 meters</entry><entry>&#x2002;75</entry></row><row><entry>. . .</entry><entry>. . .</entry><entry>. . .</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0095" num="0093">The size may represent at least one of a length and an area that is obtained by physically measuring an object (i.e., an object present in a real space and may include an object such as the hand <b>1</b>, smartphone, or the like) that is a target of capturing of the camera <b>120</b>. The distance may represent a length between the electronic apparatus <b>100</b> (or the camera <b>120</b>) and an object (e.g., an object such as the hand <b>1</b>, a smartphone, etc.). The distance may be expressed as a value on the z-axis. The pixel size may represent the number of pixels included in an area representing at least one of a length and an area of an object (e.g., the hand <b>1</b>, a smartphone, etc.) present in a virtual space (e.g., an image frame).</p><p id="p-0096" num="0094">The processor <b>130</b> may identify the position of the hand <b>1</b> through the position information of the pixel representing the hand <b>1</b> detected on the image frame. Here, the identified position of the hand <b>1</b> may be two-dimensional information (e.g., xy coordinates). The processor <b>130</b> may estimate the direction of the hand <b>1</b> or the angle of the finger based on the hand <b>1</b> or finger included in the image frame. In this example, the processor <b>130</b> may estimate the coordinates of the points (e.g., points representing the joints of the hand <b>1</b>) for measuring the size of the hand <b>1</b>.</p><p id="p-0097" num="0095">The processor <b>130</b> may combine the two-dimensional position information of the hand <b>1</b> with the z-axis value to obtain three-dimensional position information (e.g., xyz coordinate).</p><p id="p-0098" num="0096">The processor <b>130</b> may compare the location on the three-dimensional space of the user's hand <b>1</b> and the location on the three-dimensional space of the AR content displayed on the display <b>110</b> to determine whether an interaction between the hand <b>1</b> and the AR content has occurred. For example, if the processor <b>130</b> compares the three-dimensional position information of the hand <b>1</b> and the three-dimensional position information of the AR content displayed on the display <b>110</b> and determines that the distance between the hand <b>1</b> and the AR content is less than a predetermined value, the processor <b>130</b> may identify that the interaction of the hand <b>1</b> with respect to the AR content is generated.</p><p id="p-0099" num="0097">Hereinafter, a method of identifying the size of the hand <b>1</b> according to an embodiment will be described in greater detail.</p><p id="p-0100" num="0098">According to an embodiment, the processor <b>130</b> may set the detected hand size to a predetermined value. In this example, the processor <b>130</b> may identify whether the interaction of the hand <b>1</b> with respect to the object provided through the display <b>110</b> is generated based on the size of the hand set to a predetermined value.</p><p id="p-0101" num="0099">The predetermined value may be a value of a physical measurement unit (e.g., cm, etc.). For example, a predetermined value may be a value output through a trained AI model. The processor <b>130</b> may input the image frame into a trained AI model to set a preset value output from the trained AI model as the size of the detected hand. The trained AI model may be trained to output the size of the hand included in the image frame when the image frame including the hand of the user and the size of the hand included in the corresponding image frame are inputted as learning data.</p><p id="p-0102" num="0100">For example, a predetermined value may be a value representing the average size of the user's hand. The users may be classified according to a specific group (e.g., gender, age, area, etc.). For example, the electronic apparatus <b>100</b> (or memory <b>160</b>) may store information about the feature information of the user's hand <b>1</b> and the size (e.g., individual size, average size, etc.) of the hand <b>1</b> of the user, for a user belonging to each group (e.g., adult male, adult female, etc.). The feature information may include information on at least one of the ratio of the length of the finger constituting the hand <b>1</b>, the roughness of the hand <b>1</b>, the wrinkle of the hand <b>1</b>, and the like. In this example, the processor <b>130</b> may compare the stored feature information of the hand <b>1</b> with the feature information of the hand <b>1</b> detected in the image frame to identify a group to which the detected hand <b>1</b> belongs. The processor <b>130</b> may determine the average size of the hand <b>1</b> of the users belonging to the identified group as a predetermined value and set the predetermined value as the size of the detected hand.</p><p id="p-0103" num="0101">The size of the hand <b>1</b> according to an embodiment may be obtained based on information about an object when an interaction of the hand <b>1</b> with respect to an object provided through the display <b>110</b> is generated. When the interaction of the hand <b>1</b> with respect to the object provided through the display <b>110</b> is generated, the processor <b>130</b> may obtain the size of the hand <b>1</b> based on the information on the object in which the interaction has occurred. The processor <b>130</b> may correct the size of the hand <b>1</b> to a value obtained at a predetermined value. In this example, the processor <b>130</b> may identify whether an interaction of the hand <b>1</b> for the object provided through the display <b>110</b> has occurred based on the size of the hand <b>1</b>.</p><p id="p-0104" num="0102">The object may include at least one of a first type object included in the image frame obtained through the camera <b>120</b> and an object of a second type included in AR content displayed on the display <b>110</b>. The object provided through the display <b>110</b> may be a first type object or a second type object.</p><p id="p-0105" num="0103">More specifically, the object of the first type may refer to an object (e.g., a smartphone, a refrigerator, an elevator button, etc.) existing in the real space that may be captured through the camera <b>120</b>.</p><p id="p-0106" num="0104">For example, the first type object may be provided to the user in the form of being transmitted through the display <b>110</b>, which is a transparent display. The display <b>110</b> may transmit light representing a first type object that exists outside the display <b>110</b>. The first type object may be located in one direction (e.g., rear) opposite to the user located in one direction (e.g., front) based on the display <b>110</b> in the real space. In this example, the user may view the first type object through the light transmitted to the display <b>110</b>.</p><p id="p-0107" num="0105">For another example, a first type object may be provided to the user in the form of an image frame being displayed on the display <b>110</b>. The display <b>110</b> may display an image frame obtained through the camera <b>120</b>. An object of the first type may be included in the image frame. In this example, the user may view the first type object through the light emitted from the display <b>110</b> displaying the image frame.</p><p id="p-0108" num="0106">The second type object may refer to a virtual object (e.g., a three-dimensional UI component, a widget, etc.) displayed on the display <b>110</b>. The object of the second type may be described in the same way as the AR content described above.</p><p id="p-0109" num="0107">The information about the object may include information about the size of the object (e.g., the horizontal length, longitudinal length, space, etc.), the distance between the object and the electronic apparatus <b>100</b>, the information on the pixel size of the object included in the image frame, and their corresponding relationship. For example, the information on the object may indicate that the information located in the same row as in Table 1 is a relationship (or mapped relationship) corresponding to each other.</p><p id="p-0110" num="0108">The interaction of the hand <b>1</b> with respect to the object may include at least one of an event in which the distance between the hand <b>1</b> and the object is below or equal to a predetermined value, an event in which the hand <b>1</b> contacts the object, an event in which the hand <b>1</b> holds the object, an event in which a screen displayed on a display (or LED, etc.) in an object existing around the position of the hand <b>1</b> is changed, or an event that the movement of the object existing around the position of the hand <b>1</b> starts or stops.</p><p id="p-0111" num="0109">In an embodiment, the electronic apparatus <b>100</b> may further include a memory <b>160</b> (<figref idref="DRAWINGS">FIG. <b>13</b></figref>). The memory <b>130</b> may store feature information and size information of the reference object. The reference object may mean an object (e.g., a known object) in which feature information and size information are prestored. The feature information indicates unique information for identifying an object, and may include at least one of texture, scratch, shape, and the like.</p><p id="p-0112" num="0110">If the object of the first type and the hand of the user are detected in the image frames obtained through the camera <b>120</b>, the processor <b>130</b> may identify whether the object of the first type is the reference object based on the feature information stored in the memory <b>160</b> and the detected feature information of the first type object.</p><p id="p-0113" num="0111">In this example, the processor <b>130</b> may identify the size of the hand <b>1</b> based on the size of the reference object if the first type object is identified as the reference object (i.e., if the object of the first type is a known object).</p><p id="p-0114" num="0112">According to another embodiment, if it is identified that the first type object does not correspond to the reference object (that is, the first type object is an unknown object), the processor <b>130</b> may identify the size of the first type object included in consecutive image frames by using consecutive image frames obtained through the camera <b>120</b>, based on the feature information stored in the memory <b>160</b> and the detected feature information of the first type object. In this example, the processor <b>130</b> may identify the size of the hand based on the size of the first type object.</p><p id="p-0115" num="0113">According to an embodiment, the processor <b>130</b> may set the detected hand size to a predetermined value. In this example, the processor <b>130</b> may identify whether an interaction of a hand for a second type object provided through the display <b>110</b> is generated based on the set size. The processor <b>130</b> may identify the size of the hand based on the depth of the second type object in which the interaction of the hand is identified.</p><p id="p-0116" num="0114">A method of identifying the size of the hand <b>1</b> according to various embodiments will be described.</p><p id="p-0117" num="0115"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating a method of identifying a size of a hand through a focal length according to an embodiment of the disclosure.</p><p id="p-0118" num="0116">Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, in operation S<b>410</b>, the processor <b>130</b> may identify whether the focal length of the camera <b>120</b> may be changed based on the specification information of the camera <b>120</b>. Here, the spec information of the camera <b>120</b> may include information on whether the focal length is variable and may be stored in the electronic apparatus <b>100</b>.</p><p id="p-0119" num="0117">If it is determined that the focal length of the camera <b>120</b> is not changeable in operation S<b>410</b>-N, the processor <b>130</b> may perform the method of step <b>2</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, which will be described later.</p><p id="p-0120" num="0118">If it is determined that the focal length of the camera <b>120</b> may be changed in operation S<b>410</b>-Y, in operation S<b>420</b>, the processor <b>130</b> may select the image frame having the highest contrast of the hand <b>1</b> included in the obtained image frame by changing the focal length. In operation S<b>430</b>, the processor <b>130</b> may calculate the distance to the hand <b>1</b> using the focal distance of the selected image. This will be described with reference to <figref idref="DRAWINGS">FIGS. <b>5</b> and <b>6</b></figref>.</p><p id="p-0121" num="0119"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating a relationship between a focal length and a focal distance according to an embodiment. <figref idref="DRAWINGS">FIGS. <b>6</b>A, <b>6</b>B, and <b>6</b>C</figref> are diagrams illustrating a method of identifying a size of a hand through a focal length according to an embodiment.</p><p id="p-0122" num="0120">Referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the camera <b>120</b> according to an embodiment may include at least one lens <b>121</b> and an image sensor <b>123</b>. In this example, the camera <b>120</b> may capture the subject <b>510</b> (e.g., the hand <b>1</b> or the first type object, etc.) to obtain an image frame.</p><p id="p-0123" num="0121">The processor <b>130</b> according to an embodiment may calculate the distance between the camera <b>120</b> and the subject <b>510</b> based on Equation (1) and Equation (2) of <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0124" num="0122">The focal distance a is the distance between the camera <b>120</b> and the subject <b>510</b> (e.g., a focused subject) (i.e., the focal distance between the lens <b>121</b> and the subject <b>510</b>), and the focal length f is the distance between the lens <b>121</b> (e.g., the principal point of the lens <b>121</b>) and the image sensor <b>123</b>. In addition, L<b>1</b> may represent the size (e.g., width or breadth) of the subject <b>510</b>, and L<b>2</b> may represent the size (e.g., width or breadth) of the image sensor <b>123</b>. Here, if the focal length f is changed, the focal distance a may also be changed. The focal length f may be adjusted by an automatic method through the driving of the motor or by a manual method by a user.</p><p id="p-0125" num="0123">Referring to <figref idref="DRAWINGS">FIGS. <b>6</b>A, <b>6</b>B, and <b>6</b>C</figref>, when it is determined that the focal length f of the camera <b>120</b> is changeable in operation S<b>410</b>-Y, the processor <b>130</b> may change the focal length f (or the focal distance a) to obtain image frames <b>610</b>, <b>620</b>, and <b>630</b>. For example, the image frame <b>610</b> of <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> may have a focal length of 10 cm, the image frame <b>620</b> of <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> may have a focal length of 18 cm, and the image frame <b>630</b> of <figref idref="DRAWINGS">FIG. <b>6</b>C</figref> may have a focal length of 23 cm.</p><p id="p-0126" num="0124">The processor <b>130</b> may select the image frame having the highest contrast among the image frames <b>610</b>, <b>620</b>, and <b>630</b> having different focal distance (or focal lengths). For example, the processor <b>130</b> may detect an area representing a hand from the image frames <b>610</b>, <b>620</b> and <b>630</b>, and compare the detected regions <b>615</b>, <b>625</b>, and <b>635</b> with a hand model previously stored in the electronic apparatus <b>100</b> to determine contrast. The contrast may mean a sharpness (or degree of haze), the higher the contrast, the hand <b>1</b> will be clearly captured, and the higher the contrast, the better the hand <b>1</b> is focused. The processor <b>130</b> may determine that the contrast ratio of the area <b>625</b> indicating the detected hand in the image frame <b>620</b> of FIG. <b>6</b>B is highest among the determined contrast, and may identify that the focus distance 18 cm of the image frame <b>620</b> selected as the image frame having the highest contrast as the distance to the hand <b>1</b>.</p><p id="p-0127" num="0125">Referring back to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, in operation S<b>440</b>, the processor <b>130</b> may calculate the size of the hand <b>1</b> using the distance to the hand <b>1</b>.</p><p id="p-0128" num="0126">For example, the processor <b>130</b> may prestore, for the hand <b>1</b> having a real measurement size (e.g., 20 cm), the corresponding relationship between the distant distance between the hand <b>1</b> and the electronic apparatus <b>100</b> (e.g., 0.5 meters . . . 1 meter) and the size of the hand <b>1</b> (e.g., pixel number 100, 50 . . . ) included in the image frame obtained through capturing from the corresponding distance in the electronic apparatus <b>100</b> or the memory <b>160</b>. In this example, if the processor <b>130</b> may, if the distance of the hand <b>1</b> (or object) is known, calculate the size of the hand <b>1</b> (or object) through the distance of the hand <b>1</b> (or object) included in the image frame based on a pre-stored corresponding relationship.</p><p id="p-0129" num="0127">In operation S<b>450</b>, the processor <b>130</b> may calculate the accuracy of the calculated size of the hand <b>1</b>. The processor <b>130</b> may update the size and accuracy of the hand <b>1</b> when the calculated accuracy is greater than or equal to a predetermined value in operation S<b>450</b>-Y. Alternatively, when the calculated accuracy is less than a predetermined value in operation S<b>450</b>-N, the method of step <b>2</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref> may be performed, which will be described later.</p><p id="p-0130" num="0128"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram illustrating a method of identifying a hand size through an object interacting with a hand according to an embodiment of the disclosure.</p><p id="p-0131" num="0129">Referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, in operation S<b>710</b>, the processor <b>130</b> may determine whether an object interacting with the hand <b>1</b> is detected among the objects provided through the display <b>110</b>. The object provided through the display <b>110</b> may include at least one of a first type object included in the image frame obtained through the camera <b>120</b> and an object of a second type included in AR content displayed on the display <b>110</b>. The interaction will be described with reference to <figref idref="DRAWINGS">FIGS. <b>8</b>A, <b>8</b>B and <b>8</b>C</figref>.</p><p id="p-0132" num="0130"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> is a diagram illustrating a method of detecting an object interacting with a hand according to an embodiment of the disclosure. <figref idref="DRAWINGS">FIGS. <b>8</b>B and <b>8</b>C</figref> is a diagram illustrating a method of detecting an object interacting with a hand according to an embodiment of the disclosure.</p><p id="p-0133" num="0131">Referring to <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>, the processor <b>130</b> according to an embodiment may sequentially obtain image frames <b>810</b>, <b>820</b>, <b>830</b>, <b>840</b> through the camera <b>120</b>.</p><p id="p-0134" num="0132">The processor <b>130</b> may detect feature information in an obtained image frame <b>810</b>, and detect regions <b>811</b><i>h</i>, <b>813</b><i>h</i>, and <b>815</b><i>h </i>representing the user's hand <b>1</b> through feature information and regions <b>811</b><i>o</i>, <b>813</b><i>o</i>, <b>815</b><i>o </i>representing the object.</p><p id="p-0135" num="0133">The image frame <b>810</b> may include at least one of the image frames <b>811</b>, <b>813</b>, and <b>815</b>. The image frame <b>811</b> indicates that the position of the hand <b>1</b> is in front of the object (i.e., if the distance between the hand <b>1</b> and the electronic apparatus <b>100</b> is greater), the image frame <b>813</b> indicates that the position of the hand <b>1</b> is behind the object (i.e., if the distance between the hand <b>1</b> and the electronic apparatus <b>100</b> is smaller), the image frame <b>815</b> indicates that the position of the hand <b>1</b> (i.e., the finger) is between the object (i.e., if the distance between the hand <b>1</b> and the electronic apparatus <b>100</b> is equal to the distance between the object and the electronic apparatus <b>100</b>). The feature information is information indicating a unique feature used to identify a hand <b>1</b> or an object in an image frame, and the feature information may include information on texture, color, shape, and the like.</p><p id="p-0136" num="0134">The processor <b>130</b> may detect feature information in the obtained image frame <b>820</b>, and detect a region <b>825</b><i>h </i>representing the user's hand <b>1</b> and an area <b>825</b><i>o </i>representing the object through the feature information. The processor <b>130</b> may compare the sequentially obtained image frame <b>810</b> and the image frame <b>820</b> to determine whether an object starts or stops movement.</p><p id="p-0137" num="0135">The processor <b>130</b> may detect feature information in the obtained image frame <b>830</b> and detect a region <b>835</b><i>h </i>indicating the user's hand <b>1</b> through the feature information. The processor <b>130</b> may compare the sequentially obtained image frame <b>820</b> and the image frame <b>830</b> to determine whether the object is located under the hand <b>1</b> of the user (e.g., a shadow portion).</p><p id="p-0138" num="0136">The processor <b>130</b> may detect feature information in the image frame <b>840</b> obtained through the camera <b>120</b> and detect a region <b>845</b><i>h </i>indicating the user's hand <b>1</b> through the feature information. The processor <b>130</b> may compare the sequentially obtained image frame <b>830</b> and the image frame <b>840</b> to determine whether the hand <b>1</b> of the user holds the object through a change in the shape of the hand <b>1</b> of the user. When it is determined that the user's hand <b>1</b> holds the object, the processor <b>130</b> may determine that the user's hand <b>1</b> interacts with the object.</p><p id="p-0139" num="0137">Referring to <figref idref="DRAWINGS">FIGS. <b>8</b>B and <b>8</b>C</figref>, the processor <b>130</b> according to an embodiment may include image frames <b>850</b>, <b>860</b> through the camera <b>120</b>.</p><p id="p-0140" num="0138">For example, the processor <b>130</b> may detect feature information in the image frame <b>850</b> of <figref idref="DRAWINGS">FIG. <b>8</b>B</figref>, and detect an area and an object (e.g., an elevator button) representing the user's hand <b>1</b> through the feature information. The processor <b>130</b> may determine that the user's hand <b>1</b> interacts with the object when the change of the region <b>855</b><i>o </i>indicating the object (e.g., the LED of the elevator button is turned on) through the obtained image frame is detected.</p><p id="p-0141" num="0139">For another example, the processor <b>130</b> may detect feature information in the image frame <b>860</b> of <figref idref="DRAWINGS">FIG. <b>8</b>C</figref>, and detect an area representing the user's hand <b>1</b> and an area <b>865</b><i>o </i>representing an object (e.g., a display of the refrigerator) through the feature information. Further, the processor <b>130</b> may determine that the user's hand <b>1</b> interacts with the object when the change of the region <b>865</b><i>o </i>representing the object (e.g., the image displayed on the display of the refrigerator is changed) is sensed through the sequentially obtained image frame.</p><p id="p-0142" num="0140">Referring back to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, when an object interacting with the hand <b>1</b> is detected in operation S<b>710</b>-Y, in operation S<b>720</b>, the processor <b>130</b> may determine whether the detected object corresponds to the first type object. Alternatively, the processor <b>130</b> may determine whether the detected object corresponds to a second type object. The object of the first type may refer to an object (e.g., a smartphone, a refrigerator, an elevator button, etc.) existing in the real space that may be captured through the camera <b>120</b>. The second type object may refer to a virtual object (e.g., a three-dimensional UI component, a widget, etc.) displayed on the display <b>110</b>.</p><p id="p-0143" num="0141">For example, the processor <b>130</b> may compare the position of the hand <b>1</b> included in the image frame and the position of the at least one first type object in an image frame obtained through the camera <b>120</b>. The processor <b>130</b> may detect a first type object having a distance equal to or less than a predetermined value from among at least one first type object as an object interaction with the hand <b>1</b>. In this example, in operation S<b>720</b>-Y, the processor <b>130</b> may determine whether the detected first type object is the reference object.</p><p id="p-0144" num="0142">If the detected type object is the first type object in operation S<b>720</b>-Y, in operation S<b>730</b>, the processor <b>130</b> may determine whether the detected first type object is the reference object. The reference object may mean an object in which the feature information and the size information are pre-stored in the memory <b>160</b>.</p><p id="p-0145" num="0143">For example, the processor <b>130</b> may compare the feature information of the plurality of reference objects stored in the memory <b>160</b> and the detected feature information of the detected first type object to calculate the similarity of each reference object and the detected first type object. The processor <b>130</b> may detect that the reference object having the highest similarity is greater than a predetermined value among the plurality of reference objects.</p><p id="p-0146" num="0144">In operation S<b>740</b>, the processor <b>130</b> may identify the size of the hand <b>1</b> based on the size of the reference object (i.e., if the object of the first type is a known object) in operation S<b>730</b>-Y. For example, the processor <b>130</b> may identify the detected first type object as the detected reference object when the reference object having the highest similarity is detected that is greater than or equal to a predetermined value. Various embodiments will be described with reference to <figref idref="DRAWINGS">FIGS. <b>9</b>A and <b>9</b>B</figref>.</p><p id="p-0147" num="0145"><figref idref="DRAWINGS">FIGS. <b>9</b>A and <b>9</b>B</figref> are diagrams illustrating a method of identifying a size of a hand through an object of a first type according to an embodiment.</p><p id="p-0148" num="0146">Referring to <figref idref="DRAWINGS">FIGS. <b>9</b>A and <b>9</b>B</figref>, the processor <b>130</b> according to an embodiment may obtain image frames <b>910</b> and <b>930</b> through the camera <b>120</b>.</p><p id="p-0149" num="0147">As shown in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref>, the processor <b>130</b> may detect feature information from the obtained image frame <b>910</b>, and detect a region <b>915</b><i>h </i>indicating the hand <b>1</b> of the user or a region <b>915</b><i>o </i>indicating the object of the first type through the feature information.</p><p id="p-0150" num="0148">In this example, if it is determined that the distance between the area <b>915</b><i>h </i>representing the hand <b>1</b> of the user and the area <b>915</b><i>o </i>representing the object of the first type is below or equal to a predetermined value, the processor <b>130</b> may determine that an interaction of user's hand <b>1</b> and the object of the first type occurs.</p><p id="p-0151" num="0149">In this example, the processor <b>130</b> may compare the feature information included in the area <b>915</b><i>o </i>representing the first type object and the feature information of the plurality of reference objects stored in the memory <b>160</b> to determine whether the object of the first type is the reference object. <figref idref="DRAWINGS">FIG. <b>9</b>A</figref> illustrates a case where an object of a first type included in an image frame is a reference object (e.g., Galaxy S7 in the size of 69.6 mm width&#xd7;142.4 mm height).</p><p id="p-0152" num="0150">In this example, the processor <b>130</b> may identify the size of the hand <b>1</b> based on the size of the reference object, the pixel size X1, Y1 of the reference object, and the pixel size <b>915</b><i>hs </i>of the hand <b>1</b>. For example, the processor <b>130</b> may calculate the size of the hand through a ratio relationship as shown in Equation (3).</p><p id="p-0153" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Reference object size/pixel size of the reference object=size of the hand/pixel size of the hand&#x2003;&#x2003;[Equation 3]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0154" num="0151">The above-described manner may be equally applied to the case of <figref idref="DRAWINGS">FIG. <b>9</b>B</figref>. That is, if it is determined that the distance between the region <b>935</b><i>h </i>indicating the hand <b>1</b> of the user included in the obtained image frame <b>930</b> and the region <b>935</b><i>o </i>representing the object of the first type is less than or equal to a predetermined value, the processor <b>130</b> may determine that the interaction of the user's hand <b>1</b> with respect to the object of the first type has occurred.</p><p id="p-0155" num="0152">In this example, the processor <b>130</b> may compare the feature information included in the area <b>935</b><i>o </i>representing the first type object and the feature information of the plurality of reference objects stored in the memory <b>160</b> to determine whether the object of the first type is the reference object. <figref idref="DRAWINGS">FIG. <b>9</b>B</figref> illustrates a case where an object of a first type included in an image frame is a display of a reference object (e.g., a display of a refrigerator having a size of 120 mm width&#xd7;205 mm height).</p><p id="p-0156" num="0153">In this example, the processor <b>130</b> may identify the size of the hand based on the size of the reference object, the pixel size X2, Y2 of the reference object, and the pixel size <b>935</b><i>hs </i>of the hand.</p><p id="p-0157" num="0154">According to an embodiment, when the image frame is obtained through the camera <b>120</b>, the processor <b>130</b> may identify at least one first type object (or reference object) included in the obtained image frame, and then, when the hand of the user is detected, may identify an object in which the interaction has occurred with the detected hand among the at least one first type object.</p><p id="p-0158" num="0155">Alternatively, referring again to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, in operation S<b>750</b> the processor <b>130</b> may identify the size of the first type object included in consecutive image frames by using the consecutive image frames obtained through the camera <b>120</b> when it is identified that the first type object does not correspond to the reference object (that is, when the first type is an unknown object) in operation S<b>730</b>-N based on the feature information stored in the memory <b>160</b> and the feature information of the detected first type object.</p><p id="p-0159" num="0156">For example, if the reference object having a similarity equal to or greater than a predetermined value is not detected, the processor <b>130</b> may identify that the object of the first type does not correspond to the reference object.</p><p id="p-0160" num="0157">In this example, the processor <b>130</b> may use consecutive image frames obtained through the camera <b>120</b> to identify a size of a first type object included in consecutive image frames. In operation S<b>755</b>, the processor <b>130</b> may identify the size of the hand based on the size of the object of the first type. Various embodiments will be described with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0161" num="0158"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating a method of identifying a size of a hand through an object of a first type according to an embodiment of the disclosure. <figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating that the first type object is not a reference object.</p><p id="p-0162" num="0159">Referring to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the processor <b>130</b> may obtain the image frames <b>1010</b>, <b>1020</b>, and <b>1030</b> through the camera <b>120</b>. The processor <b>130</b> may detect feature information from the obtained image frames <b>1010</b>, <b>1020</b>, and <b>1030</b>, and detect a region <b>1035</b><i>h </i>representing the hand <b>1</b> of the user or a region <b>1011</b><i>o</i>, <b>1012</b><i>o</i>, <b>1025</b><i>o</i>, and <b>1035</b><i>o </i>representing the first type object through the feature information.</p><p id="p-0163" num="0160">It is assumed that the processor <b>130</b> identifies that the first type object included in the image frames <b>1010</b>, <b>1020</b>, and <b>1030</b> does not correspond to the reference object. In this example, the processor <b>130</b> may measure the size of a first type object included in the image frames <b>1010</b>, <b>1020</b>, and <b>1030</b> through a simultaneous localization and mapping (SLAM) scheme.</p><p id="p-0164" num="0161">The image frame <b>1011</b> and the image frame <b>1012</b> represent a continuously obtained image frame having a very small captured time interval (e.g., 5 ms, etc.) (i.e., the relationship of the t<sup>th </sup>obtained image frame <b>1011</b> following the t&#x2212;1<sup>th </sup>obtained image frame <b>1011</b>), and the plurality of image frames <b>1010</b> through <b>1030</b> represent an image frame having a large captured time interval (e.g., 2 seconds, etc.).</p><p id="p-0165" num="0162">The processor <b>130</b> may compare the obtained two image frames <b>1011</b>, <b>1012</b> which are sequentially captured and obtained while the camera <b>120</b> is moving (or during rotation) to determine the distance of the center position of the regions <b>1011</b><i>o</i>, <b>1012</b><i>o </i>representing the first type object is moving. In this example, the processor <b>130</b> may obtain a degree of movement (e.g., a moving distance, or a rotation angle) of the camera <b>120</b> during a time of capturing of the camera <b>120</b> through a separate sensor (e.g., an acceleration sensor, etc.) provided in the electronic apparatus <b>100</b>.</p><p id="p-0166" num="0163">The processor <b>130</b> may identify a distance (e.g., a distance between the object and the electronic apparatus <b>100</b>) that corresponds to the degree of movement (e.g., a movement distance, or a rotation angle) and center position of regions <b>1011</b><i>o</i>, <b>1012</b><i>o </i>that is representing the first type object while the camera <b>120</b> is moving (or rotating). The electronic apparatus <b>100</b> may prestore information about the rotation angle of the electronic apparatus <b>100</b> and the distance which matched to moving distance of center position of the electronic apparatus <b>100</b>. When the camera <b>120</b> moves, the object may move on the image frame obtained through the camera <b>120</b>. In this case, although the camera moves at the same distance, the degree which the center position of the object is moving on the image frame may be vary.</p><p id="p-0167" num="0164">When the distance between the camera <b>120</b> and the object is identified through the image frame <b>1010</b>, the processor <b>130</b> may identify a pixel size (e.g., a horizontal pixel, a vertical pixel, etc.) of the region <b>1025</b><i>o </i>indicating the object of the first type in the obtained image frame <b>1020</b> after the distance is identified. The processor <b>130</b> may identify a size of an object (e.g., a real size of 40 mm width, 45 mm height) corresponding to a pixel size of the object and a distance of the object. In this example, the processor <b>130</b> may identify the distance of the object and the size of the object mapped to the pixel size of the object by using information about the object (e.g., the size of the object, the distance of the object, information on the pixel size of the object included in the image frame, and information on their corresponding relationship (e.g., Table 1).</p><p id="p-0168" num="0165">The processor <b>130</b> may determine that the interaction has occurred for the user's hand <b>1</b> with respect to the object when the distance between the region <b>1035</b><i>h </i>representing the hand <b>1</b> of the user and the region <b>1035</b><i>o </i>representing the object of the first type is less than a predetermined value in the image frame <b>1030</b>.</p><p id="p-0169" num="0166">In one embodiment, the processor <b>130</b> may estimate the distance (e.g., a distance between the object and the camera <b>120</b>) of the object to a distance (e.g., a distance between the hand <b>1</b> and the camera <b>120</b>) of the user. In this example, the processor <b>130</b> may identify the size of the hand <b>1</b> corresponding to the distance of the hand <b>1</b> and the pixel size of the user's hand <b>1</b> through a corresponding relationship as in Table 1.</p><p id="p-0170" num="0167">In one embodiment, the processor <b>130</b> may identify the size of the hand <b>1</b> based on the size of the object, the pixel size of the object, and the pixel size of the hand <b>1</b>, in a manner similar to Equation (3).</p><p id="p-0171" num="0168">Referring back to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, for example, the processor <b>130</b> may compare the position of the hand <b>1</b> included in the image frame obtained through the camera <b>120</b> and the position of at least one second type object displayed through the display <b>110</b>. The processor <b>130</b> may detect a second type object having a distance equal to or less than a predetermined value from among at least a second type object as an object having interaction with the hand <b>1</b>.</p><p id="p-0172" num="0169">In this example in operation S<b>720</b>-N, in operation S<b>760</b>, the processor <b>130</b> may identify the size of the hand based on the detected depth of the object of the second type.</p><p id="p-0173" num="0170">The processor <b>130</b> may set the detected hand size to a predetermined value. The depth value of the detected second type object may be considered the distance value of the hand <b>1</b> (e.g., the distance between the electronic apparatus <b>100</b> and the hand <b>1</b> (i.e., the length on the z axis)). In this example, the processor <b>130</b> may obtain the size of the hand <b>1</b> corresponding to the pixel size of the pixel representing the hand <b>1</b> among the plurality of pixels included in the image frame and the distance value of the hand <b>1</b>, as in the embodiment of Table 1. The processor <b>130</b> may obtain three-dimensional position information of the hand <b>1</b> by combining the position information (i.e., the coordinate on the xy-axis) indicating the hand <b>1</b> among the pixels of a plurality of pixels included in the image frame and the distance value (e.g., the length of the z-axis) of the hand <b>1</b>. Various embodiments will be described with reference to <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0174" num="0171"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram illustrating a method of identifying a hand size through an object of a second type according to an embodiment of the disclosure.</p><p id="p-0175" num="0172">Referring to <figref idref="DRAWINGS">FIG. <b>11</b></figref>, an image <b>1120</b> may be provided through the display <b>110</b> according to an embodiment. The image <b>1120</b> may include at least one of an image in which the image frame obtained through the camera <b>120</b> and a virtual second type object <b>1125</b><i>o </i>are overlapped and displayed on the display <b>110</b>, or an image in which external environment is transmitted on the display <b>110</b> and the virtual second type object <b>1125</b><i>o </i>is overlapped with the external environment and displayed on the display <b>110</b>.</p><p id="p-0176" num="0173">The processor <b>130</b> may control the display <b>110</b> to display the second type object (or AR content) <b>1125</b><i>o </i>on a specific position on the virtual 3D space. The second type object <b>1125</b><i>o </i>may include 3D position information (e.g., xyz coordinate).</p><p id="p-0177" num="0174">The processor <b>130</b> may obtain an image frame through the camera <b>120</b>. The processor <b>130</b> may detect feature information in the obtained image frame and detect a region <b>1125</b><i>h </i>indicating the user's hand <b>1</b> through the feature information. The processor <b>130</b> may set the detected hand size to a predetermined value (e.g., an average value for the user's hand). In this example, the processor <b>130</b> may identify the size of the user's hand <b>1</b> and the distance of the hand <b>1</b> of the user corresponding to the pixel size of the hand <b>1</b> through a corresponding relationship as in Table 1. The processor <b>130</b> may identify three-dimensional position information of the hand <b>1</b> by combining the xy coordinate and distance of the hand <b>1</b> on the image frame.</p><p id="p-0178" num="0175">The processor <b>130</b> may compare the three-dimensional position of the hand <b>1</b> and the object <b>1125</b><i>o </i>of the second type to determine that an interaction with respect to the second type object <b>1125</b><i>o </i>of the hand <b>1</b> is generated when the distance between the hand <b>1</b> and the object <b>1125</b><i>o </i>of the second type is less than a predetermined value.</p><p id="p-0179" num="0176">The processor <b>130</b> may estimate (or correct) the distance value on the z-axis of the second type object <b>1125</b><i>o </i>as a distance on the z-axis of the hand <b>1</b> if it is determined that the interaction with the second type object <b>1125</b><i>o </i>of the hand <b>1</b> has occurred. The distance (e.g., the distance on the z-axis) of the hand <b>1</b> may be readjusted.</p><p id="p-0180" num="0177">In this example, the processor <b>130</b> may identify the size of the hand <b>1</b> corresponding to the readjusted distance of the hand <b>1</b> and the pixel size of the hand <b>1</b> through the corresponding relationship in Table 1. The size of the hand <b>1</b> may be corrected to the identified value at a predetermined value.</p><p id="p-0181" num="0178">According to an embodiment, the electronic apparatus <b>100</b> may store and manage the size of the user's hand <b>1</b> for each user account. The electronic apparatus <b>100</b> may store (or update) the size of the hand <b>1</b> of a particular user together with the user account of the corresponding user. The electronic apparatus <b>100</b> may then access the user account to load the size of the hand <b>1</b> stored together in the user account. To access a particular user account, the electronic apparatus <b>100</b> may perform an authentication process. For example, the electronic apparatus <b>100</b> may access a user account corresponding to the password when a pre-registered password (e.g., character, number, symbol, pattern, gesture, etc.) is input. As another example, the electronic apparatus <b>100</b> may access a user account corresponding to the biometric information when the pre-registered biometric information (e.g., fingerprint, retina, face, hand shape, etc.) is input.</p><p id="p-0182" num="0179"><figref idref="DRAWINGS">FIG. <b>12</b>A</figref> is a block diagram illustrating an additional configuration of an electronic apparatus according to an embodiment of the disclosure.</p><p id="p-0183" num="0180">Referring to <figref idref="DRAWINGS">FIG. <b>12</b>A</figref>, in addition to the display <b>110</b>, the camera <b>120</b>, and the processor <b>130</b>, the electronic apparatus <b>100</b> may further include at least one of an input interface <b>140</b>, an output interface <b>150</b>, a memory <b>160</b>, a sensor <b>170</b>, a communicator <b>180</b>, and a power supplier <b>190</b>.</p><p id="p-0184" num="0181">The input interface <b>140</b> may receive various user commands and pass the same to the processor <b>130</b>. The processor <b>130</b> may recognize a user command input from a user through the input interface <b>140</b>. Here, the user command may be implemented in various ways, such as a user's touch input (e.g., via a touch panel), an input of pressing a key or a button, input of a voice uttered by a user, or the like.</p><p id="p-0185" num="0182">The output interface <b>150</b> may further include a speaker. The speaker may directly output various notification sound or voice messages as well as various audio data for which various processing has been performed, such as decoding or amplification, noise filtering, etc., by an audio processor as voice.</p><p id="p-0186" num="0183">The memory <b>160</b> is configured to store various data related to components of the operating system (OS) and electronic apparatus <b>100</b> to control the overall operation of the components of the electronic apparatus <b>100</b>.</p><p id="p-0187" num="0184">The memory <b>160</b> may include hardware that temporarily or permanently stores data or store. For example, the memory <b>160</b> may be implemented as at least one hardware among a non-volatile memory, a volatile memory, a flash memory, a hard disk drive (HDD) or solid state drive (SSD), RAM, ROM, or the like.</p><p id="p-0188" num="0185">The sensor <b>170</b> may be implemented with various sensors, such as a camera, a microphone, a proximity sensor, an illuminance sensor, a motion sensor, a ToF sensor, a global positioning system (GPS) sensor, and the like. For example, the camera may divide the light in pixel units, detect the intensity of light for red (R), green (G), and blue (B) for each pixel, and convert the intensity of the light into an electrical signal to obtain data representing the color, shape, contrast, or the like, of the object. At this time, the type of data may be an image having R, G, and B color values for each of the plurality of pixels. The microphone may sense a sound wave, such as a voice of a user, and convert the sound wave into an electrical signal to obtain data. At this time, the type of data may be an audio signal in various formats. The proximity sensor may sense the presence of the surrounding object to obtain data about the presence of a surrounding object or the proximity of the surrounding object. The illuminance sensor may sense light quantity (or brightness) relative to the surrounding environment of the electronic apparatus <b>100</b> to obtain data for the illuminance. The motion sensor can sense movement distance, movement direction, incline, or the like, of the electronic apparatus <b>100</b>. The motion sensor may be implemented in a combination of an acceleration sensor, a gyro sensor, a geomagnetic sensor, or the like. A TOF sensor may detect a flight time of emitting and returning the various electromagnetic waves (e.g., ultrasound, infrared, laser, ultra-wideband (UWB) etc.) having a specific speed, and may obtain data with respect to a distance (or location) with the object. The GPS sensor may receive a radio wave signal from a plurality of satellites, calculate a distance with each satellite by using a transmission time of the received signal, and obtain data for the current location of the electronic apparatus <b>100</b> by using the calculated distance through triangulation. The above-described implementation embodiment of the sensor <b>170</b> is only one embodiment, and it is not limited thereto, and it is possible to implement various types of sensors.</p><p id="p-0189" num="0186">The communicator <b>180</b> may communicate with various types of external devices according to various types of communication methods to transmit and receive various types of data. The communicator <b>180</b> is a circuit that performs various types of wireless communication, such as Bluetooth module (Bluetooth or Bluetooth low energy), Wi-Fi module (Wi-Fi method), wireless communication module (cellular such as 3rd generation (3G), <b>4</b><i>t</i><sup>h </sup>generation (4G), 5<sup>th </sup>generation (5G)), near field communication (NFC), infrared modules (infrared method), Zigbee modules (Zigbee method), Ethernet module performing wireless communication with UWB and ultrasonic modules (ultrasonic method), universal serial bus (USB) module, high definition multimedia interface (HDMI), display port (DP), D-subminiature (D-SUB), digital visual interface (DVI), Thunderbolt, or components.</p><p id="p-0190" num="0187">The power supplier <b>190</b> may supply or block power to each configuration of the electronic apparatus <b>100</b>. The power supplier <b>190</b> may include a battery for supplying power, and the battery may be charged according to a wired charging method or a wireless charging method.</p><p id="p-0191" num="0188"><figref idref="DRAWINGS">FIG. <b>12</b>B</figref> is a diagram illustrating an example of an electronic apparatus according to an embodiment of the disclosure.</p><p id="p-0192" num="0189">Referring to <figref idref="DRAWINGS">FIG. <b>12</b>B</figref>, the electronic apparatus <b>100</b> according to various embodiments may be implemented in the form of glasses. However, this is merely an example, and the electronic apparatus <b>100</b> may be implemented as various types of electronic apparatuses such as goggles, helmet, hat, smartphone, and the like.</p><p id="p-0193" num="0190">In various embodiments, the electronic apparatus <b>100</b> may be worn on a user's head portion to provide an image related to the augmented reality service to the user. According to an embodiment, the electronic apparatus <b>100</b> may provide an augmented reality service which outputs so that at least one virtual object is seen to be overlapped in an area determined as a field of view of a user. For example, a region determined as a field of view of a user may refer to an area that a user wearing the electronic apparatus <b>100</b> may recognize through the electronic apparatus <b>100</b>.</p><p id="p-0194" num="0191">The electronic apparatus <b>100</b> according to an embodiment may be divided into a support portion (e.g., a first support portion <b>101</b>, and/or a second support portion <b>102</b>) and a main body portion <b>103</b>. The main body portion <b>103</b> and the support portions <b>101</b> and <b>102</b> of the electronic apparatus <b>100</b> may be operatively connected. For example, the main body portion <b>103</b> and the support portions <b>101</b> and <b>102</b> may be operatively connected through a hinge portion <b>104</b>. The main body portion <b>103</b> may be mounted on the nose of the user, and may include at least one glass <b>111</b>, <b>112</b>, a display module <b>115</b>, and the camera <b>120</b>. The support portions <b>101</b>, <b>102</b> may include a support member mounted on the ear of the user, and may include a first support <b>101</b> mounted on the left ear and/or a second support <b>102</b> mounted to the right ear.</p><p id="p-0195" num="0192">The electronic apparatus <b>100</b> may include a plurality of glasses (e.g., the first glass <b>111</b> and the second glass <b>112</b>) corresponding to each of the user's eyes (e.g., left-eye and right-eye).</p><p id="p-0196" num="0193">For example, each of the plurality of glasses <b>111</b>, <b>112</b> may function as the display <b>110</b> described above. Each of the plurality of glasses <b>111</b> and <b>112</b> may include at least one of an LCD, a digital mirror device (DMD), a liquid crystal on silicon (LCoS), an OLED, and a micro LED. In this example, the electronic apparatus <b>100</b> may drive the display panel to display an image. Each of the plurality of glasses <b>111</b> and <b>112</b> may function as a single transparent display.</p><p id="p-0197" num="0194">In another embodiment, the display <b>110</b> may display an image by a projector scheme. The display <b>110</b> may include a display module <b>115</b> (e.g., a projector, a backlight unit, etc.) for providing light corresponding to an image to user's eyes. In this example, the display module <b>115</b> may be located on a side surface of each of the glasses <b>111</b> and <b>112</b>, a connection part of each of the glass <b>111</b> and <b>112</b>, or the like, but may be variously modified.</p><p id="p-0198" num="0195">More specifically, according to one embodiment, the electronic apparatus <b>100</b> may include at least one glass (e.g., the first glass <b>111</b> and the second glass <b>112</b>). The at least one glass (e.g., the first glass <b>111</b> and the second glass <b>112</b>) may include a condensing lens (not shown) and/or a transparent waveguide. For example, the transparent waveguide may be located in at least a portion of the glass. According to an embodiment, the light emitted from the display module <b>115</b> may be incident on one end of the glass through the first glass <b>111</b> and the second glass <b>112</b>, and the incident light may be transmitted to the user through a waveguide and/or an optical waveguide (e.g., waveguide) formed in the glass. The waveguide may be made of glass, plastic, or polymer, and may include a nano pattern formed on the inner or outer surface thereof, for example, a polygonal or curved grating structure. According to one embodiment, the incident light may be propagated or reflected inside the waveguide by the nano pattern to be provided to the user. According to one embodiment, the waveguide may include at least one of at least one diffractive element (e.g., diffractive optical element (DOE), holographic optical element (HOE)), or reflective element (e.g., reflective mirror, total internal reflection (TIR) member, etc.). According to one embodiment, the optical waveguide may induce the light emitted from the light source unit to the eye of the user using at least one diffractive element or reflective element.</p><p id="p-0199" num="0196">According to an embodiment, each of the plurality of glasses <b>111</b> and <b>112</b> may be formed of a transparent material to transmit external light. A user may view a real space or an external object through the plurality of glasses <b>111</b> and <b>112</b>. The electronic apparatus <b>100</b> may display a virtual object in at least a portion of the display <b>100</b> (e.g., at least one of the plurality of glasses <b>111</b>, <b>112</b>) such that the user may view that a virtual object is added to at least a portion of the real space.</p><p id="p-0200" num="0197">According to an embodiment, the virtual object output through the display <b>110</b> may include information related to an application program executed in the electronic apparatus <b>100</b> and/or information related to an external object located in a real space corresponding to an area determined as a field of view of the user. For example, the electronic apparatus <b>100</b> may identify an external object included in at least a part of the image information associated with a real space obtained through the camera <b>120</b> of the electronic apparatus <b>100</b>. The electronic apparatus <b>100</b> may output (or display) a virtual object associated with an external object identified by at least a part of the display area of the electronic apparatus <b>100</b> through an area determined as a field of view of a user. The external object may include objects existing in the real space.</p><p id="p-0201" num="0198">According to an embodiment, the electronic apparatus <b>100</b> may further include the camera <b>120</b> for capturing an image corresponding to a field of view of the user, and an eye tracking camera for checking the direction of the gaze viewed by the user. For example, the camera <b>120</b> may capture a front direction of the electronic apparatus <b>100</b>, and the gaze tracking camera may capture a direction opposite to the capturing direction of the camera <b>120</b> (i.e., a direction in which eyes of a user wearing the electronic apparatus <b>100</b> are located). For example, the gaze tracking camera may capture the eyes of the user.</p><p id="p-0202" num="0199">According to an embodiment, the electronic apparatus <b>100</b> may include at least one illumination LED module <b>125</b>. For example, the light emitting device <b>125</b> may emit light. The light emitting device <b>125</b> may be used as an auxiliary means for improving the accuracy of an image obtained by the camera <b>120</b> by providing light to an environment having a low illuminance.</p><p id="p-0203" num="0200">According to an embodiment, each of the first support portion <b>101</b> and the second support portion <b>102</b> may include at least one of the input interface <b>140</b>, a printed circuit board (PCB) <b>105</b>, the output interface <b>150</b> (e.g., a speaker, etc.) and the power supplier <b>190</b>. The input interface <b>140</b> (e.g., a microphone, etc.) may receive a user's voice and ambient sound. The printed circuit board <b>105</b> may transmit an electrical signal to each component of the electronic apparatus <b>100</b>. The output interface <b>150</b> may output an audio signal. The power supplier <b>190</b> may supply power required to drive each component of the electronic apparatus <b>100</b>, such as the printed circuit board <b>105</b>. Each of the first and second support portions <b>101</b> and <b>102</b> may include a hinge portion <b>104</b> for coupling to the main body portion <b>103</b> of the electronic apparatus <b>100</b>.</p><p id="p-0204" num="0201"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram illustrating a flowchart according to an embodiment of the disclosure.</p><p id="p-0205" num="0202">Referring to <figref idref="DRAWINGS">FIG. <b>13</b></figref>, a controlling method of the electronic apparatus <b>100</b> providing AR content may include displaying the AR content on the display <b>110</b> in operation S<b>1310</b>, detecting a hand of a user from image frames obtained through the camera <b>120</b> in operation S<b>1320</b>, and identifying an interaction of the hand for the AR content based on the size of the hand in operation S<b>1330</b>. The size of the hand may be obtained based on the information about the object, in response to an interaction of the hand with respect to an object provided through the display <b>110</b> occurring.</p><p id="p-0206" num="0203">In operation S<b>1310</b>, the controlling method may display the AR content on the display <b>110</b>.</p><p id="p-0207" num="0204">In operation S<b>1320</b>, the user's hand may be detected from the image frames obtained through the camera <b>120</b>.</p><p id="p-0208" num="0205">According to an embodiment, the size of the detected hand may be set to a predetermined value. In this example, when the interaction of the hand with respect to the object provided through the display <b>110</b> is generated, the size of the hand may be obtained based on the information on the object.</p><p id="p-0209" num="0206">As an embodiment, the size of the detected hand may be set to a preset value. It may be identified whether interaction of the hand occurs for an object provided through the display <b>110</b> based on the set size. Based on identification that the interaction occurs, the size of the hand may be identified based on the information about the object.</p><p id="p-0210" num="0207">According to an embodiment, the object may include at least one of an object of a first type included in an image frame obtained through the camera <b>120</b> and an object of a second type included in the AR content displayed on the display <b>110</b>.</p><p id="p-0211" num="0208">As an embodiment, the method may further include, based on a first type object and the hand of the user being detected from image frames obtained through the camera <b>120</b>, identifying whether the first type object is the reference object based on feature information of the reference object stored in the electronic apparatus <b>100</b> and feature information of the detected first type object; and based on identification that the first type object is the reference object, identifying the size of the hand based on the size of the reference object stored in the electronic apparatus <b>100</b>.</p><p id="p-0212" num="0209">According to an embodiment, the method may further include, based on identification that the first type object is not the reference object based on the feature information stored in the electronic apparatus <b>100</b> and the feature information of the detected first type object, identifying the size of the first type object included in consecutive image frames using the consecutive image frames obtained through the camera <b>120</b> and identifying the size of the hand based on the identified size of the first type object.</p><p id="p-0213" num="0210">According to an embodiment, the method may further include setting the size of the detected hand as a preset value; identifying whether the interaction of the hand occurs for the second type object through the display based on the set size; and identifying the size of the hand based on depth of the second type object in which interaction of the hand is identified.</p><p id="p-0214" num="0211">In operation S<b>1330</b>, the interaction of the hand with respect to the AR content may be identified based on the size of the hand in operation S<b>1330</b>.</p><p id="p-0215" num="0212">According to various embodiments of the disclosure as described above, an electronic apparatus for estimating a distance to an object more accurately by using a camera and a control method thereof are provided.</p><p id="p-0216" num="0213">According to an embodiment of the present disclosure, it is possible to accurately estimate the size of the user's hand, and accurately estimate the parameter for the user's hand.</p><p id="p-0217" num="0214">Various embodiments may be implemented as software that includes instructions stored in machine-readable storage media readable by a machine (e.g., a computer). A device may call instructions from a storage medium and operate in accordance with the called instructions, including an electronic apparatus (e.g., the electronic apparatus <b>100</b>). When the instruction is executed by a processor, the processor may perform the function corresponding to the instruction, either directly or under the control of the processor, using other components. The instructions may include a code generated by a compiler or a code executable by an interpreter. The machine-readable storage medium may be provided in the form of a non-transitory storage medium. The &#x201c;non-transitory&#x201d; storage medium may not include a signal and is tangible, but does not distinguish whether data is permanently or temporarily stored in a storage medium.</p><p id="p-0218" num="0215">According to embodiments, a method disclosed herein may be provided in a computer program product. A computer program product may be traded between a seller and a purchaser as a commodity. A computer program product may be distributed in the form of a machine-readable storage medium (e.g., a CD-ROM) or distributed online through an application store (e.g., PLAYSTORE&#x2122;). In the case of on-line distribution, at least a portion of the computer program product may be stored temporarily in a storage medium, such as a manufacturer's server, a server in an application store, a memory in a relay server, and the like, or temporarily generated.</p><p id="p-0219" num="0216">Each of the components (for example, a module or a program) according to embodiments may include one or a plurality of objects, and some subcomponents of the subcomponents described above may be omitted, or other subcomponents may be further included in the embodiments. Alternatively or additionally, some components (e.g., modules or programs) may be integrated into one entity to perform the same or similar functions performed by each respective component prior to integration. Operations performed by a module, program, or other component, in accordance with the embodiments of the disclosure, may be performed sequentially, in a parallel, repetitive, or heuristic manner, or at least some operations may be performed in a different order, omitted, or other operations may be added.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An electronic apparatus providing augmented reality (AR) content, comprising:<claim-text>a display;</claim-text><claim-text>a camera; and</claim-text><claim-text>a processor configured to:</claim-text><claim-text>display augmented reality (AR) content through the display,</claim-text><claim-text>detect a hand of a user based on an image obtained through the camera, and</claim-text><claim-text>identify a first interaction of the hand with the AR content based on a size of the hand,</claim-text><claim-text>wherein the size of the hand is obtained based on the information about an object provided through the display.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The electronic apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor is further configured to:<claim-text>set the size of the detected hand to a preset value,</claim-text><claim-text>identify whether the second interaction of the hand occurs for the object provided through the display based on the set size,</claim-text><claim-text>based on identifying that the second interaction occurs, identify the size of the hand based on the information on the object, and</claim-text><claim-text>identify the first interaction of the hand for the AR content based on the identified size of the hand.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The electronic apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the object comprises at least one of a first type object included in the image obtained through the camera and a second type object included in the AR content displayed on the display.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The electronic apparatus of <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising:<claim-text>a memory storing feature information and size information of a reference object,</claim-text><claim-text>wherein the processor is further configured to:</claim-text><claim-text>based on the first type object and the hand of the user being detected from image obtained through the camera, identify whether the first type object is the reference object based on the feature information stored in the memory and feature information of the detected first type object, and</claim-text><claim-text>based on identifying that the first type object is the reference object, identify the size of the hand based on a size of the reference object.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The electronic apparatus of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the processor is further configured to:<claim-text>based on identifying that the first type object is not the reference object based on the feature information stored in the memory and the feature information of the detected first type object, identify the size of the first type object included in consecutive image frames by using the consecutive image frames obtained through the camera, and</claim-text><claim-text>identify the size of the hand based on the size of the first type object.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The electronic apparatus of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the processor is further configured to:<claim-text>set the size of the hand to a preset value,</claim-text><claim-text>identify whether the second interaction of the hand occurs with the second type object through the display based on the set size, and</claim-text><claim-text>identify the size of the hand based on a depth of the second type object in which interaction of the hand is identified.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A method of controlling an electronic apparatus providing augmented reality (AR) content, the method comprising:<claim-text>displaying augmented reality (AR) content on the display;</claim-text><claim-text>detecting a hand of a user based on image obtained through the camera; and</claim-text><claim-text>identifying a first interaction of the hand with the AR content based on a size of the hand,</claim-text><claim-text>wherein the size of the hand is obtained based on the information about an object provided through the display.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:<claim-text>setting the size of the detected hand to a preset value;</claim-text><claim-text>identifying whether the second interaction of the hand occurs for the object provided through the display based on the set size; and</claim-text><claim-text>based on identifying that the second interaction occurs, identifying the size of the hand based on the information on the object.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the object comprises at least one of a first type object included in the image obtained through the camera and a second type object included in the AR content displayed on the display.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>based on the first type object and the hand of the user being detected from image obtained through the camera, identifying whether the first type object is a reference object based on the feature information stored in the electronic apparatus and feature information of the detected first type object; and</claim-text><claim-text>based on identifying that the first type object is the reference object, identifying the size of the hand based on a size of the reference object.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising:<claim-text>based on identifying that the first type object is not the reference object based on the feature information stored in the electronic apparatus and the feature information of the detected first type object, identifying the size of the first type object included in consecutive image frames by using the consecutive image frames obtained through the camera; and</claim-text><claim-text>identifying the size of the hand based on the size of the first type object.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>setting the size of the hand to a preset value;</claim-text><claim-text>identifying whether the second interaction of the hand occurs with the second type object through the display based on the set size; and</claim-text><claim-text>identifying the size of the hand based on a depth of the second type object in which interaction of the hand is identified.</claim-text></claim-text></claim></claims></us-patent-application>