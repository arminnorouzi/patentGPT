<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005480A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005480</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17930822</doc-number><date>20220909</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>21</main-group><subgroup>0208</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Voice Filtering Other Speakers From Calls And Audio Messages</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17086296</doc-number><date>20201030</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11462219</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17930822</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Google LLC</orgname><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Sharifi</last-name><first-name>Matthew</first-name><address><city>Kilchberg</city><country>CH</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Carbune</last-name><first-name>Victor</first-name><address><city>Zurich</city><country>CH</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Google LLC</orgname><role>02</role><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method includes receiving a first instance of raw audio data corresponding to a voice-based command and receiving a second instance of the raw audio data corresponding to an utterance of audible contents for an audio-based communication spoken by a user. When a voice filtering recognition routine determines to activate voice filtering for at least the voice of the user, the method also includes obtaining a respective speaker embedding of the user and processing, using the respective speaker embedding, the second instance of the raw audio data to generate enhanced audio data for the audio-based communication that isolates the utterance of the audible contents spoken by the user and excludes at least a portion of the one or more additional sounds that are not spoken by the user The method also includes executing.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="68.92mm" wi="158.75mm" file="US20230005480A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="230.46mm" wi="164.42mm" orientation="landscape" file="US20230005480A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="228.52mm" wi="161.88mm" orientation="landscape" file="US20230005480A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="205.49mm" wi="92.29mm" orientation="landscape" file="US20230005480A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="216.58mm" wi="166.88mm" orientation="landscape" file="US20230005480A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="219.63mm" wi="142.07mm" file="US20230005480A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="230.21mm" wi="143.59mm" orientation="landscape" file="US20230005480A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This U.S. patent application is a continuation of, and claims priority under 35 U.S.C. &#xa7; 120 from, U.S. patent application Ser. No. 17/086,296, filed on Aug. 29, 2022. The disclosure of this prior application is considered part of the disclosure of this application and is hereby incorporated by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">This disclosure relates to voice filtering other speakers from calls and audio messages.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">A speech-enabled environment permits a user to only speak a query or command out loud and an automated assistant will field and answer the query and/or cause the command to be performed. A speech-enabled environment (e.g., home, workplace, school, etc.) can be implemented using a network of connected microphone devices distributed throughout various rooms and/or areas of the environment. As such, a connected microphone device may implement an automated assistant and a user may interact with the automated assistant by providing spoken utterances that the automated assistant may respond to by performing an action, controlling another device, and/or providing responsive content (e.g., visual and/or audible natural language output).</p><p id="p-0005" num="0004">An automated assistant can convert audio data, corresponding to a spoken utterance of a user, into corresponding text (or other semantic representation). For instance, the automated assistant can include a speech recognition engine that attempts to recognize various characteristics of the spoken utterance, such as sounds produced (e.g., phonemes), an order of the pronounced sounds, rhythm of speech, intonation, etc., and then identify text words or phrases represented by these characteristics. Automated assistants may employ voice filtering techniques as a pre-processing step performed on an utterance spoken by a user to help focus the speech recognition engine on the voice of the user that spoke the utterance.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">One aspect of the disclosure provides a method for activating voice filtering in an audio-based communication. The method includes receiving, at data processing hardware, a first instance of raw audio data corresponding to a voice-based command for an assistant-enabled device to facilitate an audio-based communication between a user of the assistant-enabled device and a recipient. The voice-based command is spoken by the user and captured by the assistant-enabled device. The method also includes receiving, at the data processing hardware, a second instance of the raw audio data corresponding to an utterance of audible contents for the audio-based communication spoken by the user and captured by the assistant-enabled device. The second instance of the raw audio data captures one or more additional sounds that are not spoken by the user. The method also includes executing, by the data processing hardware, a voice filtering recognition routine to determine whether to activate voice filtering for at least a voice of the user in the audio-based communication based on the first instance of the raw audio data. When the voice filtering recognition routine determines to activate voice filtering for at least the voice of the user, the method includes: obtaining, by the data processing hardware, a respective speaker embedding of the user that represents voice characteristics for the user; and processing, by the data processing hardware, using the respective speaker embedding of the user, the second instance of the raw audio data to generate enhanced audio data for the audio-based communication that isolates the utterance of the audible contents spoken by the user and excludes at least a portion of the one or more additional sounds that are not spoken by the user. The method also includes transmitting, by the data processing hardware, the enhanced audio data to a recipient device associated with the recipient. The enhanced audio data when received by the recipient device, causes the recipient device to audibly output the utterance of the audible contents spoken by the user.</p><p id="p-0007" num="0006">Another aspect of the disclosure provides a system for activating voice filtering in an audio-based communication. The system includes data processing hardware and memory hardware in communication with the data processing hardware. The memory hardware stores instructions that when executed on the data processing hardware cause the data processing hardware to perform operations that include receiving a first instance of raw audio data corresponding to a voice-based command for an assistant-enabled device to facilitate an audio-based communication between a user of the assistant-enabled device and a recipient. The voice-based command is spoken by the user and captured by the assistant-enabled device. The operations also include receiving a second instance of the raw audio data corresponding to an utterance of audible contents for the audio-based communication spoken by the user and captured by the assistant-enabled device. The second instance of the raw audio data captures one or more additional sounds that are not spoken by the user. The operations also include executing a voice filtering recognition routine to determine whether to activate voice filtering for at least a voice of the user in the audio-based communication based on the first instance of the raw audio data. When the voice filtering recognition routine determines to activate voice filtering for at least the voice of the user, the operations also include: obtaining a respective speaker embedding of the user that represents voice characteristics for the user; and processing, using the respective speaker embedding of the user, the second instance of the raw audio data to generate enhanced audio data for the audio-based communication that isolates the utterance of the audible contents spoken by the user and excludes at least a portion of the one or more additional sounds that are not spoken by the user. The operations also include transmitting the enhanced audio data to a recipient device associated with the recipient. The enhanced audio data when received by the recipient device, causes the recipient device to audibly output the utterance of the audible contents spoken by the user.</p><p id="p-0008" num="0007">The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">DESCRIPTION OF DRAWINGS</heading><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> are example systems for activating voice filtering to focus on one or more voices in an audio-based communication.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an example voice filtering recognition routine.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an example voice filtering engine that includes a voice filtering model for generating enhanced audio data.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of an example arrangement of operations for a method of activating voice filtering in audio-based communications.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic view of an example computing device that may be used to implement the systems and methods described herein.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0014" num="0013">Like reference symbols in the various drawings indicate like elements.</p><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0015" num="0014">A user may use an automated assistant for transmitting audio communications, such as sending/receiving audio messages and making phone calls (e.g., audio and/or visual), with a remote recipient. When the user is in a noisy environment (e.g., in a busy place, within an automobile, or in a noisy household), the recipient of the audio communication may have difficult hearing or understanding the content of the audio communication due to high background noise levels.</p><p id="p-0016" num="0015">Implementations herein are directed toward applying voice filtering to focus on one or more voices in an audio-based communication transmitted to (or received from) another user by removing unwanted background noise from the audio communication. When audio data captured by an assistant-enabled device includes an utterance spoken by a user that conveys the audible content of the audio-based communication as well as unwanted noise, applying voice filtering can generate an enhanced version of the audio data by removing the unwanted background noise so that the end recipient receives a clear and consistent audio-based communication. As used herein, an audio-based communication may refer to an audio message, a phone call, a video call (e.g., an audio-video call), or broadcasted audio. For instance, the assistant-enabled device could record the content of an audio message spoken by a user and then send the audio message via a messaging or email platform to the recipient. Voice filtering may be applied to remove unwanted background noise from the audio data conveying the audio message at the assistant-enabled device, at an intermediate cloud-based node while the audio message is in route to the recipient, or at a recipient client device once the audio message is received. As such, when the recipient wishes to playback the audio message, the recipient client device audibly outputs the enhanced version of the audio message that does not include the unwanted background noise that was initially captured when the user was speaking the utterance conveying the content of the audio message. Likewise, the assistant-enabled device could facilitate a phone call and apply voice filtering in real-time to remove unwanted background noise. As with the audio message, the voice filtering can be applied to remove the unwanted noise from the audio data of the phone call locally at the assistant-enabled device or at any point along the communication path to the recipient device.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> illustrate an example system <b>100</b> for voice filtering to focus on at least a voice of a user <b>102</b> in an audio-based communication <b>150</b> transmitted to (or received from) another user <b>103</b> by removing unwanted background noise from the audio-based communication <b>150</b>. The system <b>100</b> includes an assistant-enabled device (AED) <b>104</b> executing a digital assistant <b>109</b> that the user <b>102</b> may interact with through speech. In the example shown, the AED <b>104</b> corresponds to a smart speaker. However, the AED <b>104</b> can include other computing devices, such as, without limitation, a smart phone, tablet, smart display, desktop/laptop, smart watch, smart appliance, headphones, or vehicle infotainment device. The AED <b>104</b> includes data processing hardware <b>10</b> and memory hardware <b>12</b> storing instructions that when executed on the data processing hardware <b>10</b> cause the data processing hardware <b>10</b> to perform operations. The AED <b>104</b> includes an array of one or more microphones <b>16</b> configured to capture acoustic sounds such as speech directed toward the AED <b>104</b>. The AED <b>104</b> may also include, or be in communication with, an audio output device (e.g., speaker) <b>16</b> that may output audio such as audible content from an audio-based communication <b>150</b> received from the other user <b>103</b> and/or synthesized speech from the digital assistant <b>109</b>.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> shows the user <b>102</b> speaking a first utterance <b>106</b>, &#x201c;Ok Computer, send the following audio message to Bob&#x201d; in the vicinity of the AED <b>104</b>. The microphone <b>16</b> of the AED <b>104</b> receives the utterance <b>106</b> and processes the raw audio data that corresponds to the first utterance <b>106</b>. The initial processing of the audio data may involve filtering the audio data and converting the audio data from an analog signal to a digital signal. As the AED <b>104</b> processes the audio data, the AED may store the audio data in a buffer of the memory hardware <b>12</b> for additional processing. With the audio data in the buffer, the AED <b>104</b> may use a hotword detector <b>108</b> to detect whether the raw audio data includes a hotword <b>110</b>. The hotword detector <b>108</b> is configured to identify hotwords that are included in the audio data without performing speech recognition on the audio data. In the example shown, the hotword detector <b>108</b> may determine that the utterance <b>106</b> &#x201c;Ok computer, send the following audio message to Bob&#x201d; includes the hotword <b>110</b> &#x201c;ok computer&#x201d; if the hotword detector <b>108</b> detects acoustic features in the audio data that are characteristic of the hotword <b>110</b>. The acoustic features may be mel-frequency cepstral coefficients (MFCCs) that are representations of short-term power spectrums of the utterance <b>106</b> or may be mel-scale filterbank energies for the utterance <b>106</b>.</p><p id="p-0019" num="0018">When the hotword detector <b>108</b> determines that the audio data that corresponds to the utterance <b>106</b> includes the hotword <b>110</b>, the AED <b>104</b> may trigger a wake-up process to initiate speech recognition on the audio data that corresponds to the utterance <b>106</b>. For example, an automated speech recognizer (ASR) <b>116</b> running on the AED <b>104</b> may perform speech recognition and semantic interpretation on the audio data that corresponds to the utterance <b>106</b>. The ASR <b>116</b> may process at least the portion of the raw audio data that follows the hotword <b>110</b> to generate a speech recognition result for the received raw audio data and perform semantic interpretation on the speech recognition result to determine that the utterance <b>106</b> includes a voice-based command <b>118</b> to facilitate an audio-based communication <b>150</b> between the user <b>102</b> and the recipient <b>103</b>. In this example, the ASR <b>116</b> may process a first instance of raw audio data for &#x201c;send the following audio message to Bob&#x201d; and identify the voice-based command <b>118</b>.</p><p id="p-0020" num="0019">In some implementations, the ASR <b>116</b> is located on a server <b>120</b> in addition to, or in lieu, of the AED <b>104</b>. Upon the hotword detector <b>108</b> triggering the AED <b>104</b> to wake-up responsive to detecting the hotword <b>110</b> in the utterance <b>106</b>, the AED <b>104</b> may transmit the first instance of the raw audio data corresponding to the utterance <b>106</b> to the server <b>120</b> via a network <b>132</b>. The AED <b>104</b> may transmit the portion of the audio data that includes the hotword <b>110</b> for the server <b>120</b> to confirm the presence of the hotword <b>110</b>. Alternatively, the AED <b>104</b> may transmit only the portion of the audio data that corresponds to the portion of the utterance <b>106</b> after the hotword <b>110</b> to the server <b>120</b>. The server <b>120</b> executes the ASR <b>116</b> to perform speech recognition and returns a speech recognition result (e.g., transcription) of the audio data to the AED <b>104</b>. In turn, the AED <b>104</b> identifies the words in the utterance <b>106</b>, and the AED <b>104</b> performs semantic interpretation to identify the voice command <b>118</b>. The AED <b>104</b> (and/or the server <b>120</b>) may identify the voice-based command <b>118</b> for the digital assistant <b>109</b> to facilitate the audio-based communication <b>150</b> of an audible message through the network <b>132</b> from the AED <b>104</b> to a recipient device <b>105</b> associated with the recipient <b>103</b>. Thereafter, the AED <b>104</b> leaves the microphone <b>16</b> open and receives a second instance of the raw audio data corresponding to an utterance <b>124</b> of audible contents <b>126</b> for the audio message <b>150</b> spoken by the user and captured by the AED <b>104</b>. In the example shown, the utterance <b>124</b> of audible contents <b>126</b> includes &#x201c;Hi Bob, how are you?&#x201d; The second instance of the raw audio data also captures one or more additional sounds <b>128</b>, such as background noise, not spoken by the user <b>102</b>.</p><p id="p-0021" num="0020">Before or after receiving the second instance of the raw audio data corresponding to the utterance of the audible contents, the AED <b>104</b> executes a voice filtering recognition routine (&#x2018;routine&#x2019;) <b>200</b> to determine whether to activate voice filtering for at least a voice of the user <b>102</b> in the audio-based communication (e.g., the audio message) <b>150</b> based on the first instance of the raw audio data corresponding to the voice-based command <b>118</b>. When the routine <b>200</b> determines not to activate voice filtering, the AED <b>104</b> will simply transmit the second instance of the raw audio data corresponding to the utterance <b>124</b> of audible contents <b>126</b> of the audible message <b>155</b> to the recipient device <b>105</b>. Here, the recipient device <b>105</b> will simply playback the utterance <b>124</b> of audible contents <b>126</b> of the utterance <b>124</b> of audible contents <b>126</b> includes &#x201c;Hi Bob, how are you?&#x201d; as well as any background noise captured by the second instance of raw audio data to Bob the recipient <b>103</b>. When the routine <b>200</b> determines to activate voice filtering, the AED <b>104</b> uses a voice filter engine <b>300</b> to generate enhanced audio data <b>152</b> for the audio-based communication <b>150</b> that isolates the utterance <b>124</b> of the audible contents <b>126</b> spoken by the user and excludes at least a portion of the one or more additional sounds that are not spoken by the user <b>102</b>. That is, when routine <b>200</b> determines to activate voice filtering for other individuals in addition to the user <b>102</b> and when at least a portion of the one or more additional sounds include an additional utterance of audible contents spoken by another individual, then the voice filter engine <b>300</b> will generate enhanced audio data <b>152</b> that does not exclude the additional utterance of audible contents. Otherwise, if the routine <b>200</b> determines to activate voice filtering for only the user <b>102</b>, then the voice filter engine <b>300</b> will generate enhanced audio data <b>152</b> that isolates only the voice of the user <b>102</b> and excludes any other sounds not spoken by the user <b>102</b> that are captured by the second instance of raw audio data.</p><p id="p-0022" num="0021">While described in greater detail below with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, when the routine <b>200</b> determines to activate voice filtering for the voice of the user <b>102</b>, the AED <b>104</b> (or server <b>120</b>) instructs the voice filtering engine to obtain a respective speaker embedding <b>318</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>) of the user <b>102</b> that represents the voice characteristics for the user, and use the respective speaker embedding to process the second instance of the raw audio data corresponding to the utterance <b>124</b> of the audible contents <b>126</b> to generate the enhanced audio data <b>152</b> for the audio message <b>150</b> that isolates the utterance <b>124</b> of the audible contents <b>126</b> spoken by the user <b>102</b> and excludes the one or more additional sounds such as background noise <b>128</b> not spoken by the user <b>102</b>. <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> shows the AED <b>104</b> (or the server <b>120</b>) transmitting the enhanced audio data <b>152</b> for the audio message <b>150</b> to the recipient device <b>105</b> of the recipient <b>103</b>, whereby the recipient device <b>105</b> audibly outputs the enhanced audio data <b>152</b> to allow the recipient <b>103</b> to hear the utterance <b>124</b> of audible contents <b>126</b> &#x201c;Hi Bob, how are you?&#x201d; spoken by the user <b>102</b> without hearing the background noise <b>128</b> that was originally captured in the environment of the AED <b>104</b>.</p><p id="p-0023" num="0022">In some examples, the audio message <b>150</b> is not transmitted to the recipient device <b>105</b>, but instead stored on the AED <b>104</b> for the intended recipient to retrieve at a later time. In these examples, the recipient <b>103</b> may invoke the AED <b>104</b> to audibly playback the recorded audio message <b>150</b> with the enhanced audio data <b>152</b> generated by the voice filter engine <b>300</b> to isolate the voice of the user <b>102</b> conveying the audible contents of the audio message <b>150</b>. In other examples, the functionality of the routine <b>200</b> and voice filter engine <b>300</b> may execute on the recipient device <b>105</b> such that the recipient device <b>105</b> only receives raw audio data in the audio-based communication <b>150</b>. In these examples, the recipient device <b>105</b> may determine to activate voice filtering for at least the voice of the sender <b>102</b> and process the raw audio data to isolate the voice of the sender <b>102</b> conveying the audible contents of the audio-based communication <b>150</b>. In some additional examples, the AED <b>104</b> sends both raw audio data <b>301</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>) and the enhanced audio data <b>152</b> to the recipient device <b>105</b> to permit the recipient <b>103</b> to select either playback of the raw audio data to listen to audible contents of the audio-based communication without voice filtering or playback of the enhanced audio data <b>152</b> to listen to the audible contents of the audio-based communication <b>150</b> with voice filtering activated for at least the voice of the user <b>102</b>. Moreover, the AED <b>104</b> could send multiple versions of enhanced audio data <b>152</b> each associated with voice filtering applied to a different combination of voices. As such, the recipient <b>103</b> may toggle between playing back different versions of the enhanced audio data <b>152</b> to listen to different combinations of isolated voices.</p><p id="p-0024" num="0023">The recipient device <b>105</b> and/or the AED <b>104</b> may display in a graphical user interface (GUI) a graphical indicator indicating whether or not voice filtering is currently activated for at least the voice of the user <b>102</b>. The GUI may further render one or more controls for activating/deactivating voice filtering for at least the voice of the user. Here, the user may select the controls to select between playback of the raw audio data to listen to audible contents of the audio-based communication without voice filtering and playback of the enhanced audio data <b>152</b> to listen to the audible contents of the audio-based communication <b>150</b> with voice filtering activated for at least the voice of the user <b>102</b>. User input indications indicating selection of a control may be provided as user feedback <b>315</b> for training a classification model <b>210</b> of the voice filtering recognition routine <b>200</b> discussed below. The AED <b>104</b> may also include a physical button that may be selected to activate or deactivate voice filtering. The recipient device, however, would not be afforded these types of controls for activating or deactivating voice filtering.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows an example of the voice filtering recognition routine <b>200</b> executing on the AED <b>104</b> (or the server <b>120</b>) for determining whether to activate voice filtering for at least a voice of the user <b>102</b> in the audio-based communication <b>150</b>. Executing the voice filtering recognition routine <b>200</b> may include executing a classification model <b>210</b> configured to receive contextual inputs <b>202</b> associated with the audio-based communication <b>150</b>, and generate, as output, a classification result <b>212</b> that indicates one of: to activate voice filtering for one or more voices in the audio-based communication; or to not activate voice filtering on any voices. When the classification result <b>212</b> based on the contextual inputs <b>202</b> is to activate voice filtering on one or more voices, the result <b>212</b> may specify each of the one or more voices.</p><p id="p-0026" num="0025">In some examples, one or more of the contextual inputs <b>202</b> are derived from performing semantic interpretation on the speech recognition result for the first instance of the raw audio data corresponding to the voice-based command <b>118</b>. Here, the ASR <b>116</b> (<figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>) may generate, and perform semantic interpretation on, the speech recognition result for the first instance of the raw audio data corresponding to the voice-based command <b>118</b> to identify/determine one or more of the contextual inputs <b>202</b> such as the recipient <b>103</b> of the audio-based communication <b>150</b>. These contextual inputs <b>202</b> may include an identity of the recipient <b>103</b> of the audio-based communication <b>150</b> and/or an explicit instruction to activate the voice filtering for at least the voice of the user <b>102</b> in the audio-based communication. The classification model <b>210</b> may determine whether the identified recipient <b>103</b> includes a particular recipient type indicating that activating the voice filtering for voices in the audio-based communication is appropriate. For instance, when the identified recipient <b>103</b> includes a business, the classification model <b>210</b> may determine to activate voice filtering. On the other hand, when the identified recipient <b>103</b> includes a friend or family member of the user <b>102</b>, the classification model <b>210</b> may determine to not activate voice filtering. In additional examples, when the voice-based command <b>118</b> includes an explicit instruction to activate voice filtering, the classification model <b>210</b> determines to activate voice filtering for at least the voice of the user. For example, a voice-based command <b>118</b> that says &#x201c;call the plumber and cancel out the background noise&#x201d; includes the explicit command to activate voice filtering and identifies a recipient (e.g., plumber) that includes a particular recipient type where voice filtering may be appropriate In another example, the voice-based command <b>118</b> that says &#x201c;call mom so that the kids can speak with her&#x201d; identifies that the &#x201c;kids&#x201d; are also participants in the audio-based communication and identifies a recipient (e.g., mom) that includes a particular recipient type where voice filtering may not be appropriate. In this example, the classification result <b>212</b> may be to activate voice filtering for the voice of each kid of the user <b>102</b> during the ensuing audio-based communication (e.g., phone call) between the kids and mom.</p><p id="p-0027" num="0026">In additional examples, the AED <b>104</b> (or the server <b>120</b>) processes the first instance of the raw audio data (e.g., the utterance <b>106</b> in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> or the utterance <b>156</b> in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>) in other ways to derive contextual inputs <b>202</b> that may be meaningful for the routine <b>200</b> to determine whether or not activating voice filtering is appropriate. For example, when the first instance of raw audio data includes preamble audio and/or a hotword <b>110</b>, <b>160</b> preceding the voice command <b>118</b>, <b>168</b>, the routine <b>200</b> may extract audio features from the preamble audio and/or hotword to determine a background noise level of an environment of the AED <b>104</b> at the time the voice command was initiated. Here, the background noise level may serve as a contextual input <b>202</b> fed to the classification model <b>210</b> that indicates a likelihood that the ensuing second instance of raw audio data corresponding to the utterance of audible contents for the audio-based communication <b>150</b> will capture background noise. For example, higher background noise levels may indicate that activating voice filtering is more appropriate than if the background noise level was low.</p><p id="p-0028" num="0027">Similarly, a contextual input <b>202</b> could include a location of the AED <b>104</b>. In this instance, an AED <b>104</b> located in a home or office environment of the user <b>102</b> may be less likely to activate voice filtering than if the AED <b>104</b> were located at a public place such as a train station. The classification model <b>210</b> may also consider a type of the AED <b>104</b> as a contextual input when determining whether to activate voice filtering. Here, some particular types of AEDs may be more suitable for activating voice filtering than others. For instance, a shared AED <b>104</b> such as a smart speaker in a multi-user environment may be more suitable for activating voice filtering than a personal AED <b>104</b> such a phone since the shared AED <b>104</b> is more likely to capture background sounds than a phone held close to the mouth of the user <b>102</b>.</p><p id="p-0029" num="0028">Referring to <figref idref="DRAWINGS">FIGS. <b>1</b>B and <b>2</b></figref>, in some implementations, one of the contextual inputs <b>202</b> includes image data <b>20</b> (<figref idref="DRAWINGS">FIG. <b>1</b>B</figref>) captured by an image capture device <b>18</b> implemented at the AED <b>104</b> or otherwise in communication with the AED <b>104</b>. For example, <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> shows the AED <b>104</b> capturing a first instance of raw audio data for an utterance <b>156</b> spoken by the user that corresponds to a voice command <b>168</b> for the AED <b>104</b> (i.e., via the digital assistant <b>109</b> executing on the AED <b>104</b>) to facilitate a video call <b>150</b> as an audio-based communication between the user <b>102</b> and the recipient Bob <b>103</b>. The AED <b>104</b> may include a tablet or smart display configured for voice calls, and as such, the image capture device <b>18</b> may capture image data <b>20</b> indicating that at least the user <b>102</b> is in the image frame, and thus, participating in the video call. <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> shows the image data <b>20</b> capturing both the user <b>102</b> and another individual <b>107</b>, e.g., the user's <b>102</b> daughter. The AED <b>104</b> receives the first instance of raw audio data for the utterance <b>156</b> that captures the user <b>102</b> speaking &#x201c;Ok Computer, video call Bob&#x201d; in which a hotword <b>160</b>, &#x201c;Ok computer&#x201d;, precedes the voice command <b>168</b>, &#x201c;video call Bob&#x201d;. Up until this point, the contextual inputs <b>202</b> fed to the classification model <b>210</b> of the voice filtering recognition routine <b>200</b> may include the recipient &#x201c;Bob&#x201d; identified as a brother of the user, the type of AED <b>104</b> such as a shared smart display configured for video calls, an environment of the AED <b>104</b>, background noise levels derived from audio features extracted from the preamble and/or hotword <b>160</b>, and the image data <b>20</b> indicating that the user <b>102</b> and the other individual <b>107</b> are likely participants in the video call <b>150</b> that will ensue with the recipient <b>103</b>. A contextual input <b>202</b> may further indicate that semantic interpretation performed on a recognition result of the utterance <b>156</b> did not identify any explicit instruction to activate voice filtering.</p><p id="p-0030" num="0029">Based on the received voice command <b>168</b> instructing the AED <b>104</b> to facilitate the video call <b>150</b> with the recipient Bob <b>103</b>, the AED <b>104</b> may initiate the video call <b>150</b> by first establishing a connection via the network <b>132</b> with the recipient device <b>105</b> associated with the recipient <b>103</b>. Thereafter, the AED <b>104</b> leaves the microphone <b>16</b> open and receives a second instance of the raw audio data corresponding to an utterance <b>176</b> of audible contents <b>178</b> for the video call <b>150</b> spoken by the user and captured by the AED <b>104</b>. In the example shown, the utterance <b>176</b> of audible contents <b>178</b> includes &#x201c;Hi Uncle Bob.&#x201d; The second instance of the raw audio data also captures additional sounds not spoken by the user <b>102</b>, such as background noise <b>179</b> and an additional utterance <b>180</b> spoken by the other individual <b>107</b> that includes the audible contents &#x201c;We miss you&#x201d; following the audible contents <b>178</b> &#x201c;Hi Uncle Bob&#x201d;. While recognized as an additional sound not spoken by the user <b>102</b>, the additional utterance <b>180</b> is spoken by the other individual <b>107</b> who is indicated by the image data <b>20</b> as a likely participant of the voice call, and thus contains audible contents intended for the recipient <b>103</b> to hear. Accordingly, when executing the routine <b>200</b> results in the classification model <b>210</b> generating a classification result <b>212</b> that indicates to activate voice filtering for the voices of the user <b>102</b> and the other individual <b>107</b>, the voice filtering engine <b>300</b> will apply voice filtering to generate enhanced audio data <b>152</b> that excludes the background noise <b>179</b> and isolates the voices of the user <b>102</b> and the other individual <b>107</b> in the video call <b>150</b>.</p><p id="p-0031" num="0030">While described in greater detail below with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, when the routine <b>200</b> determines to activate voice filtering for the voices of the user <b>102</b> and the other individual <b>107</b>, the AED <b>104</b> (or server <b>120</b>) instructs the voice filtering engine <b>300</b> to obtain a respective speaker embedding <b>318</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>) for each of one of the user <b>102</b> and the other individual <b>107</b>. The respective speaker embedding <b>318</b> for the user <b>102</b> may be obtained by processing audio features of the first instance of the raw audio data (e.g., the hotword <b>160</b>) to generate a verification embedding and matching it to a stored speaker embedding <b>318</b>. If no stored speaker embedding <b>318</b> is available (e.g., the user <b>102</b> is not enrolled with the AED <b>104</b>), the respective speaker embedding <b>318</b> serving as the verification embedding can be used directly for applying voice filtering on the voice of the user <b>102</b> in subsequent speech. The respective speaker embedding <b>318</b> for the individual <b>107</b>, and optionally the user <b>102</b>, may be obtained by identifying the individual <b>107</b> based on the image data <b>20</b> through facial recognition when the individual is an enrolled user of the AED. Optionally, a facial image for the individual <b>107</b> may be extracted from the image data <b>20</b> and the speaker embedding <b>318</b> may be resolved by extracting audio features from audio synchronized with lips of the individual moving in the extracted facial image. The voice filtering engine <b>300</b> uses the respective speaker embeddings <b>318</b> to process the second instance of the raw audio data to generate the enhanced audio data <b>152</b> for the video call <b>150</b> that isolates the utterance <b>176</b> (spoken by the user <b>102</b>) and the additional utterance <b>180</b> (spoken by the other individual <b>107</b>) and excludes the background noise <b>179</b>. Accordingly, in combination with the image data <b>20</b>, the AED <b>104</b> (or the server <b>120</b>) may transmit the enhanced audio data <b>152</b> to the recipient device <b>105</b> of the recipient <b>103</b> during the video call <b>150</b>. The recipient device <b>105</b> may audibly output the enhanced audio data <b>152</b> to allow the recipient <b>103</b> to hear the utterance <b>178</b> &#x201c;Hi Uncle Bob&#x201d; spoken by the user <b>102</b> and the additional utterance <b>180</b> &#x201c;We miss you&#x201d; spoken by the other individual (e.g., the user's daughter) <b>107</b> without hearing the background noise <b>179</b> that was originally captured in the environment of the AED <b>104</b>.</p><p id="p-0032" num="0031">With continued reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the voice filtering recognition routine <b>200</b> may execute continuously such that the classification model <b>210</b> continuously updates the classification result <b>212</b> based on the contextual inputs <b>202</b>. In doing so, the routine <b>200</b> may dynamically adjust for which voices voice filtering is active during an on-going audio-based communication session between the AED <b>104</b> and the recipient device <b>105</b>. For example, the classification model <b>210</b> may initially generate a classification result <b>212</b> indicative of activating voice filtering for only the voice of the user <b>102</b>, such that the voice filter engine <b>300</b> generates enhanced audio data <b>152</b> that isolates only the voice of the user and excludes all other sounds not spoken by the user. However, upon receiving the second instance of the raw audio data conveying the audible contents of the audio message, the ASR <b>116</b> through speech recognition and semantic interpretation may indicate that a speech recognition result for the audible contents identify at least one other individual participating in the audio-based communication <b>150</b>. In one example, an utterance of audible contents may include the user <b>102</b> speaking &#x201c;Hi Bob, it's me and Alex&#x201d;, whereby recognition of the utterance and subsequent semantic interpretation can identify that, in addition to the user, Alex is also a participant of the audio-based communication <b>150</b>. Thus, the classification model <b>210</b> may receive a contextual input <b>202</b> that the user <b>102</b> and Alex are participants and generate an updated classification result <b>212</b> that activates voice filtering for the voices of the user <b>102</b> and Alex. Without this update based on the contextual input <b>202</b>, any utterances spoken by Alex would be excluded from the audio-based communication even though those utterances likely contain audible content intended for the recipient <b>103</b> to listen to. In some examples, during a current voice-based communication thread/session, the voice filtering recognition routine <b>200</b> simply determines to re-activate voice filtering on the same voice(s) in a current outgoing audio-based communication that was activated for a previous audio-based communication.</p><p id="p-0033" num="0032">Executing the voice filtering recognition routine <b>200</b> may include executing the classification model <b>210</b> as a heuristically-based model or a trained machine learning model. In some implementations, when the classification model <b>210</b> is a trained machine learning model, the trained machine learning model is re-trained/tuned to adaptively learn how to activate voice filtering for particular contextual inputs <b>202</b> based on user feedback <b>215</b> received after the voice filter engine <b>300</b> applies voice filtering on an audio-based communication based on the classification result <b>212</b> generated by the model <b>210</b> for the same particular contextual inputs <b>202</b>. Here, the user feedback <b>215</b> may indicate acceptance of the voices voice filtering was active for or may indicate a subsequent user input indication indicating an adjustment to which voices the voice filtering was active for. For example, if voice filtering was applied to isolate only the voice of the user the user may provide a user input indication indicating that the user does not want specific voices and/or other sounds not spoken by the user to be isolated from the audio-based communication. As such, the AED <b>104</b> may execute a training process that continuously retains the machine learning classification model <b>210</b> based on the contextual inputs <b>202</b>, associated classification results <b>212</b>, and the obtained user feedback <b>215</b> so that the classification model <b>210</b> adaptively learns to output voice filtering classification results <b>212</b> personalized for the user <b>102</b> based on past user behavior/reaction in similar contexts.</p><p id="p-0034" num="0033">Referring now to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, when the voice filtering recognition routine <b>200</b> determines to activate voice filtering for at least the voice of the user <b>102</b>, the voice filter engine <b>300</b> can use a frequency transformer <b>303</b> (which may be implemented at the ASR <b>116</b>) to generate a frequency representation <b>302</b> for the received raw audio data <b>301</b> captured by the AED <b>104</b>. Here, the raw audio data <b>301</b> may include one or more utterances of the audible content for an audio-based communication. The frequency representation <b>302</b> can be, for example, streaming audio data that is processed in an online manner (e.g., in real-time or in near real-time such as in a phone or video call) or non-streaming audio data that has been previously recorded (e.g., such as in an audio message) and provided to the voice filter engine. The voice filter engine also receives a speaker embedding <b>318</b> from a speaker embedding engine <b>317</b>.</p><p id="p-0035" num="0034">The speaker embedding <b>318</b> is an embedding for a given human speaker, and can be obtained based on processing one or more instances of audio data, from the given speaker, using a speaker embedding model. As described herein, in some implementations, the speaker embedding <b>318</b> is previously generated by the speaker embedding engine based on previous instance(s) of audio data from the given speaker. In some of those implementations, the speaker embedding <b>318</b> is associated with an account of the given speaker and/or a client device of the given speaker, and the speaker embedding <b>318</b> can be provided for utilization with the frequency representation <b>302</b> based on the frequency representation <b>302</b> coming from the AED <b>104</b> where the account has been authorized. The speaker embedding engine <b>317</b> can determine a respective speaker embedding <b>318</b> representing voice characteristics for each of one or more human speakers identified by the routine <b>200</b> for activating voice filtering. In some implementations, the speaker embedding engine <b>317</b> processes portion(s) of the captured raw audio data <b>301</b> using a speaker embedding model (not depicted) to generate the speaker embedding. Additionally or alternatively, speaker embedding engine <b>317</b> can select a pre-generated speaker embedding (e.g., a speaker embedding previously generated using an enrollment process) using voice fingerprinting, image recognition, a passcode, and/or other verification techniques to determine the human speaker currently active and, as a result, the speaker embedding for the currently active human speaker. In many implementations, a normalization engine <b>312</b> normalizes each of the one or more selected speaker embeddings <b>318</b>.</p><p id="p-0036" num="0035">The voice filter engine <b>300</b> may optionally process the frequency representation <b>302</b> using a power compression process to generate power compression <b>304</b>. In many implementations, the power compression process equalize (or partially equalize) the importance of quieter sounds relative to loud sounds in the audio data. Additionally or alternatively, the voice filter engine <b>300</b> may optionally process frequency representation <b>302</b> using a normalization process to generate normalization <b>306</b>, and may optionally process speaker embedding <b>318</b> using the normalization process to generate normalization <b>312</b>.</p><p id="p-0037" num="0036">The voice filter engine <b>300</b> may include a voice filter model <b>112</b> trained to process a frequency representation <b>302</b> of raw audio data <b>301</b> as well as a speaker embedding <b>318</b> corresponding to a human speaker to generate a predicted mask <b>322</b>, where the frequency representation can be processed with the predicted mask <b>322</b> to generate a revised frequency representation <b>310</b> isolating utterance(s) of the human speaker. In lieu of using the predicted mask <b>322</b>, other types of voice filtering models <b>112</b> are possible without departing from the scope of the present disclosure. For instance, an end-to-end voice filter model or a generative adversarial network (GAN)-based (model may directly produce the filtered spectrograms.</p><p id="p-0038" num="0037">More specifically, the frequency representation <b>302</b> can be applied as input to a convolutional neural network (CNN) portion <b>314</b> of the voice filter model <b>112</b>. In some implementations, the CNN portion <b>314</b> is a one-dimensional convolutional neural network. In many implementations, convolutional output generated by the CNN portion <b>314</b>, as well as speaker embedding <b>318</b>, is applied as input to a recurrent neural network (RNN) portion <b>316</b> of voice filter model <b>112</b>. Here, the RNN portion <b>316</b> can include uni-directional memory units (e.g., long short term memory units (LSTM), gated recurrent units (GRU), and/or additional memory unit(s)). Additionally or alternatively, RNN output generated by the RNN portion <b>316</b> can be applied as input to a fully connected feed-forward neural network portion <b>320</b> of voice filter model <b>112</b> to generate the predicted mask <b>322</b>. In some examples, the CNN portion <b>314</b> is omitted and both the frequency representation <b>302</b> and the speaker embedding <b>318</b> are applied as input to the RNN <b>316</b>.</p><p id="p-0039" num="0038">The engine <b>300</b> may process the frequency representation <b>302</b> with the predicted mask <b>322</b> to generate the revised frequency representation <b>310</b>. For example, frequency representation <b>302</b> can be convolved <b>308</b> with predicted mask <b>322</b> to generate the revised frequency representation <b>310</b>. A waveform synthesizer <b>324</b> may apply an inverse frequency transformation on the revised frequency representation <b>310</b> to generate the enhanced audio data <b>152</b> isolating the utterance(s) of the human speaker for playback. The enhanced audio data <b>152</b> can: be the same as the raw audio data <b>301</b> when the raw audio data <b>301</b> captures only utterance(s) from the speaker corresponding to the speaker embedding <b>318</b>; be null/zero when the raw audio data <b>301</b> lacks utterances from the speaker corresponding to the speaker embedding <b>318</b>; or exclude additional sound(s) while isolating utterance(s) from the speaker corresponding to the speaker embedding <b>318</b>, when the raw audio data <b>301</b> includes utterance(s) from the speaker and additional sound(s) (e.g., overlapping utterance(s) of other human speaker(s) and/or additional background noise).</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>4</b></figref> provides a flowchart of an example method <b>400</b> for activating voice filtering to focus on at least a voice of a user <b>102</b> in an audio-based communication <b>150</b>. At operation <b>402</b>, the method <b>400</b> includes receiving a first instance of raw audio data corresponding to a voice-based command <b>118</b> for an assistant-enabled device <b>104</b> to facilitate an audio-based communication <b>150</b> between a user <b>102</b> of the assistant-enabled device <b>104</b> and a recipient <b>103</b>. The voice-based command <b>118</b> is spoken by the user <b>102</b> and captured by the assistant-enabled device <b>104</b>.</p><p id="p-0041" num="0040">At operation <b>404</b>, the method <b>400</b> includes receiving a second instance of the raw audio data corresponding to an utterance <b>124</b> of audible contents <b>126</b> for the audio-based communication <b>150</b> spoken by the user <b>102</b> and captured by the assistant-enabled device <b>104</b>. The second instance of the raw audio data captures one or more additional sounds that are not spoken by the user <b>102</b>.</p><p id="p-0042" num="0041">At operation <b>406</b>, the method <b>400</b> includes executing a voice filtering recognition routine <b>200</b> to determine whether to activate voice filtering for at least a voice of the user <b>102</b> in the audio-based communication <b>150</b> based on the first instance of the raw audio data. At operation <b>408</b>, when the voice filtering recognition routine determines to activate voice filtering for at least the voice of the user, the method <b>400</b> also includes obtaining a respective speaker embedding <b>318</b> of the user <b>102</b> that represents voice characteristics for the user. At operation <b>410</b>, the method <b>400</b> includes processing, using the speaker embedding <b>318</b>, the second instance of the raw audio data to generate enhanced audio data <b>152</b> for the audio-based communication <b>150</b> that isolates the utterance of the audible contents spoken by the user <b>102</b> and excludes at least a portion of the one or more additional sounds that are not spoken by the user.</p><p id="p-0043" num="0042">At operation <b>412</b>, the method <b>400</b> includes transmitting the enhanced audio data <b>152</b> to a recipient device <b>105</b> associated with the recipient <b>103</b>. The enhanced audio data <b>152</b> when received by the recipient device <b>105</b>, causing the recipient device <b>105</b> to audibly output the utterance <b>124</b> of the audible contents <b>126</b> spoken by the user <b>102</b>.</p><p id="p-0044" num="0043">A software application (i.e., a software resource) may refer to computer software that causes a computing device to perform a task. In some examples, a software application may be referred to as an &#x201c;application,&#x201d; an &#x201c;app,&#x201d; or a &#x201c;program.&#x201d; Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.</p><p id="p-0045" num="0044">The non-transitory memory may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by a computing device. The non-transitory memory may be volatile and/or non-volatile addressable semiconductor memory. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is schematic view of an example computing device <b>500</b> that may be used to implement the systems and methods described in this document. The computing device <b>500</b> is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.</p><p id="p-0047" num="0046">The computing device <b>500</b> includes a processor <b>510</b>, memory <b>520</b>, a storage device <b>530</b>, a high-speed interface/controller <b>540</b> connecting to the memory <b>520</b> and high-speed expansion ports <b>550</b>, and a low speed interface/controller <b>560</b> connecting to a low speed bus <b>570</b> and a storage device <b>530</b>. Each of the components <b>510</b>, <b>520</b>, <b>530</b>, <b>540</b>, <b>550</b>, and <b>560</b>, are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate. The processor <b>510</b> can process instructions for execution within the computing device <b>500</b>, including instructions stored in the memory <b>520</b> or on the storage device <b>530</b> to display graphical information for a graphical user interface (GUI) on an external input/output device, such as display <b>580</b> coupled to high speed interface <b>540</b>. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also, multiple computing devices <b>500</b> may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).</p><p id="p-0048" num="0047">The memory <b>520</b> stores information non-transitorily within the computing device <b>500</b>. The memory <b>520</b> may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s). The non-transitory memory <b>520</b> may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by the computing device <b>500</b>. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read- only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.</p><p id="p-0049" num="0048">The storage device <b>530</b> is capable of providing mass storage for the computing device <b>500</b>. In some implementations, the storage device <b>530</b> is a computer-readable medium. In various different implementations, the storage device <b>530</b> may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional implementations, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as the memory <b>520</b>, the storage device <b>530</b>, or memory on processor <b>510</b>.</p><p id="p-0050" num="0049">The high speed controller <b>540</b> manages bandwidth-intensive operations for the computing device <b>500</b>, while the low speed controller <b>560</b> manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only. In some implementations, the high-speed controller <b>540</b> is coupled to the memory <b>520</b>, the display <b>580</b> (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports <b>550</b>, which may accept various expansion cards (not shown). In some implementations, the low-speed controller <b>560</b> is coupled to the storage device <b>530</b> and a low-speed expansion port <b>590</b>. The low-speed expansion port <b>590</b>, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.</p><p id="p-0051" num="0050">The computing device <b>500</b> may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server <b>500</b><i>a </i>or multiple times in a group of such servers <b>500</b><i>a</i>, as a laptop computer <b>500</b><i>b</i>, or as part of a rack server system <b>500</b><i>c. </i></p><p id="p-0052" num="0051">Various implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.</p><p id="p-0053" num="0052">These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms &#x201c;machine-readable medium&#x201d; and &#x201c;computer-readable medium&#x201d; refer to any computer program product, non-transitory computer readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term &#x201c;machine-readable signal&#x201d; refers to any signal used to provide machine instructions and/or data to a programmable processor.</p><p id="p-0054" num="0053">The processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.</p><p id="p-0055" num="0054">To provide for interaction with a user, one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.</p><p id="p-0056" num="0055">A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method executed on data processing hardware that causes the data processing hardware to perform operations comprising:<claim-text>receiving raw audio data corresponding to an utterance of audible contents spoken by a user and captured by an assistant-enabled device, the raw audio data capturing one or more additional sounds that are not spoken by the user;</claim-text><claim-text>receiving, from an image capture device in communication with the data processing hardware, image data capturing the user while speaking the utterance of the audible contents;</claim-text><claim-text>extracting, from the image data, a facial image for the user;</claim-text><claim-text>extracting, from the raw audio data, audio features synchronized with lips of the user moving in the extracted facial image; and</claim-text><claim-text>processing, using the extracted audio features, the raw audio data to generate enhanced audio data that isolates the utterance of the audible contents spoken by the user and excludes at least a portion of the one or more additional sounds that are not spoken by the user.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the operations further comprise transmitting the enhanced audio data in an audio-based communication to a recipient device associated with a recipient of the audio-based communication, the enhanced audio data when received by the recipient device, causing the recipient device to audibly output the utterance of the audible contents spoken by the user.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computer-implemented method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the operations further comprise:<claim-text>executing a voice filtering recognition routine to determine to activate voice filtering for a voice of the user in the audio-based communication based on the raw audio data,</claim-text><claim-text>wherein processing the raw audio data to generate the enhanced audio data is based on determining to activate voice filtering for the voice of the user.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computer-implemented method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein executing the voice filtering recognition routine to determine to activate the voice filtering for the voice of the user comprises<claim-text>identifying the recipient of the audio-based communication;</claim-text><claim-text>determining that the identified recipient of the audio-based communication comprises a particular recipient type indicating that activating the voice filtering for the voice of the user in the audio-based communication is appropriate; and</claim-text><claim-text>based on determining that the identified recipient of the audio-based communication comprises the particular recipient type, determining to activate voice filtering for at least the voice of the user.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computer-implemented method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the recipient type comprises a business.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the operations further comprise:<claim-text>receiving initial audio data corresponding to a voice-based command for the assistant-enabled device to facilitate an audio-based communication between the user of the assistant-enabled device and a recipient, the voice-based command spoken by the user and captured by the assistant-enabled device;</claim-text><claim-text>processing using a speech recognizer, the initial audio data to generate a speech recognition result; and</claim-text><claim-text>performing semantic interpretation on the speech recognition result for the initial audio data to determine that the initial audio data comprises the voice-based command to facilitate the audio-based communication between the user and the recipient,</claim-text><claim-text>wherein the utterance of the audible contents spoken by the user is for the audio-based communication spoken.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computer-implemented method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the operations further comprise:<claim-text>determining, based on the semantic interpretation performed on the speech recognition result for the initial audio data, that the voice-based command comprises an explicit instruction to activate voice filtering for the voice of the user,</claim-text><claim-text>wherein processing the raw audio data to generate the enhanced audio data is based on determining that the voice-based command comprises the explicit instruction to activate voice filtering.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the raw audio data comprises preamble audio and a hotword preceding the utterance of the audible contents.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computer-implemented method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the operations further comprise:<claim-text>extracting audio features from the preamble audio and/or the hotword to determine a background noise level of an environment of the assistant-enabled device,</claim-text><claim-text>wherein processing the raw audio data to generate the enhanced audio data is based on the background noise level of the environment of the assistant-enabled device.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the operations further comprise displaying, in a graphical user interface (GUI) displayed on a screen in communication with the data processing hardware:<claim-text>a graphical indicator indicating whether or not voice filtering is currently activated for the voice of the user; and</claim-text><claim-text>a control for activating/deactivating voice filtering for the voice of the user.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A system comprising:<claim-text>data processing hardware; and</claim-text><claim-text>memory hardware in communication with the data processing hardware, the memory hardware storing instructions that when executed on the data processing hardware cause the data processing hardware to perform operations comprising:<claim-text>receiving raw audio data corresponding to an utterance of audible contents spoken by a user and captured by an assistant-enabled device, the raw audio data capturing one or more additional sounds that are not spoken by the user;</claim-text><claim-text>receiving, from an image capture device in communication with the data processing hardware, image data capturing the user while speaking the utterance of the audible contents;</claim-text><claim-text>extracting, from the image data, a facial image for the user;</claim-text><claim-text>extracting, from the raw audio data, audio features synchronized with lips of the user moving in the extracted facial image; and</claim-text><claim-text>processing, using the extracted audio features, the raw audio data to generate enhanced audio data that isolates the utterance of the audible contents spoken by the user and excludes at least a portion of the one or more additional sounds that are not spoken by the user.</claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the operations further comprise transmitting the enhanced audio data in an audio-based communication to a recipient device associated with a recipient of the audio-based communication, the enhanced audio data when received by the recipient device, causing the recipient device to audibly output the utterance of the audible contents spoken by the user.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the operations further comprise:<claim-text>executing a voice filtering recognition routine to determine to activate voice filtering for a voice of the user in the audio-based communication based on the raw audio data,</claim-text><claim-text>wherein processing the raw audio data to generate the enhanced audio data is based on determining to activate voice filtering for the voice of the user.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein executing the voice filtering recognition routine to determine to activate the voice filtering for the voice of the user comprises<claim-text>identifying the recipient of the audio-based communication;</claim-text><claim-text>determining that the identified recipient of the audio-based communication comprises a particular recipient type indicating that activating the voice filtering for the voice of the user in the audio-based communication is appropriate; and</claim-text><claim-text>based on determining that the identified recipient of the audio-based communication comprises the particular recipient type, determining to activate voice filtering for at least the voice of the user.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the recipient type comprises a business.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the operations further comprise:<claim-text>receiving initial audio data corresponding to a voice-based command for the assistant-enabled device to facilitate an audio-based communication between the user of the assistant-enabled device and a recipient, the voice-based command spoken by the user and captured by the assistant-enabled device;</claim-text><claim-text>processing using a speech recognizer, the initial audio data to generate a speech recognition result; and</claim-text><claim-text>performing semantic interpretation on the speech recognition result for the initial audio data to determine that the initial audio data comprises the voice-based command to facilitate the audio-based communication between the user and the recipient,</claim-text><claim-text>wherein the utterance of the audible contents spoken by the user is for the audio-based communication spoken.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the operations further comprise:<claim-text>determining, based on the semantic interpretation performed on the speech recognition result for the initial audio data, that the voice-based command comprises an explicit instruction to activate voice filtering for the voice of the user,</claim-text><claim-text>wherein processing the raw audio data to generate the enhanced audio data is based on determining that the voice-based command comprises the explicit instruction to activate voice filtering.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the raw audio data comprises preamble audio and a hotword preceding the utterance of the audible contents.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the operations further comprise:<claim-text>extracting audio features from the preamble audio and/or the hotword to determine a background noise level of an environment of the assistant-enabled device,</claim-text><claim-text>wherein processing the raw audio data to generate the enhanced audio data is based on the background noise level of the environment of the assistant-enabled device.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the operations further comprise displaying, in a graphical user interface (GUI) displayed on a screen in communication with the data processing hardware:<claim-text>a graphical indicator indicating whether or not voice filtering is currently activated for the voice of the user; and</claim-text><claim-text>a control for activating/deactivating voice filtering for the voice of the user.</claim-text></claim-text></claim></claims></us-patent-application>