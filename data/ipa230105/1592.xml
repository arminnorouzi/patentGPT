<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230001593A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230001593</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17779182</doc-number><date>20201204</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="regional"><country>EP</country><doc-number>19215228.8</doc-number><date>20191211</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>26</class><subclass>B</subclass><main-group>19</main-group><subgroup>38</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>26</class><subclass>B</subclass><main-group>19</main-group><subgroup>388</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>45</class><subclass>D</subclass><main-group>2044</main-group><subgroup>007</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">HAIR REMOVAL INSTRUCTIONS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>KONINKLIJKE PHILIPS N.V.</orgname><address><city>EINDHOVEN</city><country>NL</country></address></addressbook><residence><country>NL</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>PALERO</last-name><first-name>Jonathan Alambra</first-name><address><city>WAALRE</city><country>NL</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>VARGHESE</last-name><first-name>Babu</first-name><address><city>EINDHOVEN</city><country>NL</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>BOURQUIN</last-name><first-name>Yannyk Parulian Julian</first-name><address><city>EINDHOVEN</city><country>NL</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>AKKERMANS</last-name><first-name>Steffie Petronella</first-name><address><city>VELDHOVEN</city><country>NL</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>BUIL</last-name><first-name>Vincentius Paulus</first-name><address><city>VELDHOVEN</city><country>NL</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>DURACHER</last-name><first-name>Lucie Tamara Christine</first-name><address><city>EINDHOVEN</city><country>NL</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/EP2020/084757</doc-number><date>20201204</date></document-id><us-371c12-date><date>20220524</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A computer-implemented method (<b>100</b>) is described. The method includes obtaining (<b>102</b>) an indication comprising a skin parameter of a user. The obtained indication further includes an interaction between the user's skin and a hair removal unit. The method further includes determining (<b>104</b>) a position of the hair removal unit relative to the user's skin. The method further includes determining (<b>106</b>) a hair removal instruction for the user based on the indication and the position such that a user interface is caused (<b>108</b>) to provide the hair removal instruction for the user.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="209.04mm" wi="112.10mm" file="US20230001593A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="205.49mm" wi="133.18mm" file="US20230001593A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="179.83mm" wi="131.15mm" file="US20230001593A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="107.36mm" wi="144.70mm" file="US20230001593A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD OF THE INVENTION</heading><p id="p-0002" num="0001">The invention relates to a method, apparatus and tangible machine-readable medium for providing hair removal instructions.</p><heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading><p id="p-0003" num="0002">Hair removal techniques such as shaving may cause skin irritation. However, a user may not be aware of the optimal hair removal technique for reducing skin irritation due to hair removal. A user may have difficulty establishing the optimal hair removal technique since they may find it difficult or be unable to determine whether or not their hair removal technique delivers optimal results and/or they may not be aware of any better techniques for delivering optimal results.</p><p id="p-0004" num="0003">Accordingly, an object is to provide user guidance to improve hair removal results. Another object is to reduce skin irritation due to hair removal.</p><heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0005" num="0004">Aspects or embodiments described herein relate to providing user guidance to improve hair removal results and/or reduce skin irritation due to hair removal. Aspects or embodiments described herein may obviate one or more problems associated with hair removal.</p><p id="p-0006" num="0005">In a first aspect, a computer-implemented method is described. The method comprises obtaining an indication comprising a skin parameter of a user. The indication further comprises an interaction between the user's skin and a hair removal unit. The method further comprises determining a position of the hair removal unit relative to the user's skin. The method further comprises determining a hair removal instruction for the user based on the indication and the position. The method further comprises causing a user interface to provide the hair removal instruction for the user.</p><p id="p-0007" num="0006">In some embodiments, obtaining the indication comprising the skin parameter of the user comprises accessing skin parameter data for the user determined based on imaging data of the user's skin.</p><p id="p-0008" num="0007">In some embodiments, the method comprises causing an imaging device to acquire the imaging data prior to a user hair removal session, to determine pre-hair removal skin parameter data.</p><p id="p-0009" num="0008">In some embodiments, the method comprises causing the imaging device to acquire the imaging data during and/or after the user hair removal session, to determine present and/or post-hair removal skin parameter data. The method may further comprise generating skin parameter data based on the imaging data. The method may further comprise determining a skin parameter map for the user based on a comparison between the pre-hair removal and present and/or post-hair removal skin parameter data.</p><p id="p-0010" num="0009">In some embodiments, the skin parameter comprises a visible skin irritation indicator. The skin parameter may be based on whether or not the comparison identifies any change in the visible skin irritation indicator between the pre-hair removal and present and/or post-hair removal skin parameter data.</p><p id="p-0011" num="0010">In some embodiments, the indication further comprises a hair parameter of the user. The method may further comprise determining the hair removal instruction taking into account the hair parameter.</p><p id="p-0012" num="0011">In some embodiments, determining the hair removal instruction comprises accessing an optimal hair removal map of the user's skin. A spatial location of the optimal hair removal map may be associated with an optimal hair removal technique. The optimal hair removal technique may be determined based on at least one of: pre-hair removal skin parameter data; historical data for the user; and predetermined knowledge regarding hair removal. The method may further comprise determining the hair removal instruction for the spatial location based on the optimal hair removal map.</p><p id="p-0013" num="0012">In some embodiments, the method comprises determining, in real-time, the position of the hair removal unit relative to the user's skin. In some embodiments, the method comprises determining, in real-time, the interaction between the user's skin and the hair removal unit. In some embodiments, the method comprises determining, in real-time, the skin parameter. The method may further comprise determining a real-time hair removal instruction for the user based on at least one of: the position; the interaction; the skin parameter; historical hair removal performance data for the user; and pre-determined hair removal performance data.</p><p id="p-0014" num="0013">In some embodiments, the method comprises causing the user interface to provide, in real-time, the hair removal instruction for the user.</p><p id="p-0015" num="0014">In some embodiments, the historical hair removal performance data for the user comprises at least one of: user skin type; user skin condition; pressure applied between the hair removal unit and the user's skin; user hair removal behavior; visible skin irritation; hair removal results; hair removal unit motion and hair removal unit operational performance. The historical hair removal performance data may be determined from at least one previous user hair removal session. The pre-determined hair removal performance data may comprise knowledge acquired from other users and/or clinical data regarding at least one of: skin type; skin condition; pressure applied between the hair removal unit and the other user's skin; hair removal behavior; visible skin irritation; hair removal results; hair removal unit motion and hair removal unit operational performance. A recommended hair removal instruction for the user can be determined in order to provide improved hair removal experience as compared to a previous user hair removal session. The recommended hair removal instruction may be based on at least one the historical hair removal performance data for the user and the pre-determined hair removal performance data.</p><p id="p-0016" num="0015">In some embodiments, the hair removal instruction is configured to provide a personalized recommendation for the user regarding at least one of: pressure to apply between the hair removal unit and the user's skin; hair removal unit positioning relative to the user's skin and hair removal unit motion. The method may further comprise causing the user interface to provide the hair removal instruction for the user based on whether or not the user has deviated from a previously-recommended hair removal instruction.</p><p id="p-0017" num="0016">In some embodiments, determining the position of the hair removal unit relative to the user's skin comprises acquiring at least one of: imaging data of the user's skin and the hair removal unit; and motion data from a sensor on-board the hair removal unit. The position of the hair removal unit relative to the user's skin may comprise at least one of: a position of a hair removal device of the hair removal unit on the user's skin; and an orientation of the hair removal device relative to the user's skin.</p><p id="p-0018" num="0017">In a second aspect, apparatus comprising processing circuitry is described. The processing circuitry comprises an obtaining module, a determining module and a user instruction module. The obtaining module is configured to obtain an indication comprising a skin parameter of a user. The indication further comprises an interaction between the user's skin and a hair removal unit. The determining module is configured to determine a position of the hair removal unit relative to the user's skin. The determining module is further configured to determine a hair removal instruction for the user based on the indication and the position. The user instruction module is configured to cause a user interface to provide the hair removal instruction for the user.</p><p id="p-0019" num="0018">In some embodiments, the apparatus further comprises at least one of: an imaging device and the user interface. The imaging device may be for acquiring imaging data of the user's skin and the hair removal unit.</p><p id="p-0020" num="0019">In a third aspect, a tangible machine-readable medium is described. The tangible machine-readable medium stores instructions which, when executed by at least one processor, cause the at least one processor to obtain an indication comprising a skin parameter of a user. The indication further comprises an interaction between the user's skin and a hair removal unit. The instructions further cause the at least one processor to determine a position of the hair removal unit relative to the user's skin. The instructions further cause the at least one processor to determine a hair removal instruction for the user based on the indication and the position. The instructions further cause the at least one processor to cause a user interface to provide the hair removal instruction for the user.</p><p id="p-0021" num="0020">These and other aspects of the invention will be apparent from and elucidated with reference to the embodiment(s) described hereinafter.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0022" num="0021">Exemplary embodiments of the invention will now be described, by way of embodiment only, with reference to the following drawings, in which:</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>1</b></figref> refers to a method of providing hair removal instructions for a user according to an embodiment;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic drawing of a system for providing hair removal instructions for a user according to an embodiment;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>3</b></figref> refers to a method of providing hair removal instructions for a user according to an embodiment;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic drawing of an apparatus for providing hair removal instructions according to an embodiment;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic drawing of an apparatus for providing hair removal instructions according to an embodiment; and</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic drawing of a machine-readable medium for providing hair removal instructions according to an embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a method <b>100</b> (e.g., a computer-implemented method) of providing hair removal instructions (e.g., guidance) for a user. As will be described in more detail herein, the method <b>100</b> may allow the user to improve and/or optimize hair removal results and/or reduce skin irritation due to hair removal.</p><p id="p-0030" num="0029">The method <b>100</b> comprises, at block <b>102</b>, obtaining an indication. The indication comprises a skin parameter of a user. The skin parameter may refer to a characteristic of the skin that may affect hair removal and/or be affected by hair removal. A spatial location on the user's skin may be associated with the skin parameter. Accordingly, a map of the user's skin may comprise a plurality of spatial locations where each spatial location has an associated skin parameter. A plurality of skin parameters may be associated with each spatial location.</p><p id="p-0031" num="0030">The skin parameter may refer to, for example, skin type, skin health status, skin moisture, skin roughness, after-hair removal irritation (e.g., skin redness) of the user and/or any skin characteristic associated with the user's skin. The skin parameter may be indicative of certain information regarding the user's skin. The information may comprise a calculated or estimated value indicating, for example, a skin irritation level where different values may indicative a different level of skin irritation. Where a plurality of skin parameters are associated with a spatial location on the skin, different skin parameters may be associated with that spatial location. At least one of the skin parameters may provide certain information to facilitate e.g., improved hair removal results and/or a reduction in skin irritation from hair removal.</p><p id="p-0032" num="0031">The obtained indication further comprises an interaction between the user's skin and a hair removal unit. For example, the interaction may comprise a pressure applied by the user on the user's skin by the hair removal unit.</p><p id="p-0033" num="0032">The method <b>100</b> further comprises, at block <b>104</b>, determining a position of the hair removal unit relative to the user's skin.</p><p id="p-0034" num="0033">The method <b>100</b> may allow the position of the hair removal unit to be tracked in relation to the user's skin. A more detailed description of the hair removal unit position determination is provided below.</p><p id="p-0035" num="0034">The method <b>100</b> further comprises, at block <b>106</b>, determining a hair removal instruction for the user based on the indication and the position.</p><p id="p-0036" num="0035">The method <b>100</b> may take into account certain information derived from the indication and/or the position of the hair removal unit to determine the hair removal instruction. As will be described in more detail below, the hair removal instruction may be used to provide guidance for the user in terms of how to remove hair from their skin using the hair removal unit. For example, the hair removal instruction may determine that the user should apply more or less pressure using the hair removal unit. Additionally or alternatively, the hair removal instruction may determine guidance in terms of the direction, hair removal unit orientation and/or speed with which the user is to use the hair removal unit. Additionally or alternatively, the method <b>100</b> may determine a hair removal instruction to provide to the user that is indicative of a skin and/or hair treatment regime, hair removal unit charge level and any other factor which may affect whether or not the hair removal session provides improved/optimal hair removal results and/or reduces skin irritation.</p><p id="p-0037" num="0036">Determining the position of the hair removal unit relative to the user's skin may provide certain information which may be used to determine the hair removal instruction. For example, a determination may be made that the hair removal unit is at or about to arrive at a certain spatial location on the user's skin. A determination may be made, based on the indication and/or the position, regarding a recommended hair removal unit action/technique for that spatial location on the skin and/or the next/predicted spatial location on the skin.</p><p id="p-0038" num="0037">The method <b>100</b> further comprises, at block <b>108</b>, causing a user interface to provide the hair removal instruction for the user.</p><p id="p-0039" num="0038">Once the hair removal instruction has been determined, the hair removal instruction may be provided in an appropriate format to enable the user to identity the hair removal instruction and attempt to follow the hair removal instruction. The hair removal instruction may be provided in any appropriate format for the user (e.g., via a visual and/or audible format). The user interface may comprise a device capable of providing the hair removal instruction for the user in a visual and/or audible manner. The user interface may be provided by a user equipment such as a mobile phone, tablet, mirror, smart device or any other device capably of conveying visual and/or audible instructions. For example, the user may possess a user equipment capable of providing a visualization of the user's skin (e.g., via a graphical user interface such as a screen) and corresponding hair removal instructions. For example, an arrow, moving indicator or other hair removal instruction may be visualized on the screen, which the user can interpret and follow. Additionally or alternatively, the user equipment may provide an audible hair removal instruction. For example, if too much or too little pressure is applied by the hair removal unit, an audible warning such as a beep or verbal instruction may be played for the user. Any combination of visual and/or audible hair removal instructions may be provided by the user interface.</p><p id="p-0040" num="0039">Accordingly, the method <b>100</b> may provide a user with hair removal guidance, which may help the user to achieve improved and/or optimal hair removal results and/or reduce skin irritation due to hair removal. Thus, a user may be made aware of the optimal hair removal technique for reducing skin irritation from hair removal. By obtaining the indication (e.g. at block <b>102</b>) and/or the position of the hair removal unit (e.g., at block <b>104</b>), the hair removal instruction may be tailored to the user's individual needs e.g., to provide improved hair removal results and/or reduce skin irritation.</p><p id="p-0041" num="0040">The method <b>100</b> may enable the user to be trained so that their future hair removal sessions provide improved/optimal results and/or reduce skin irritation. In some embodiments, data from different hair removal sessions may be compared to determine whether or not hair removal performance could be improved and/or whether or not skin irritation could be reduced. The hair removal instruction may take into account such a comparison e.g., to learn how the user could improve their hair removal results.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a system <b>200</b> for implementing certain methods described herein. In the system <b>200</b>, a user <b>202</b> removes hair from their skin (e.g., from their face or another part of their body) with a hair removal unit <b>204</b>. The hair removal unit <b>204</b> may comprise an electric shaver (e.g., a motorized rotary blade or a foil-based razor), an epilator, a manual razor, smart razor or indeed any type of hair removal unit capable of removing hair from skin whether by cutting, pulling or otherwise removing at least a portion of the hair from the skin. Any reference to shaving may refer to any form of hair removal e.g., by any type of hair removal unit.</p><p id="p-0043" num="0042">The hair removal unit <b>204</b> may comprise at least one sensor for determining the interaction between the user's skin and the hair removal unit <b>204</b>. For example, the hair removal unit <b>204</b> may comprise a pressure sensor for measuring contact pressure between the user's skin and the hair removal unit <b>204</b>.</p><p id="p-0044" num="0043">A user measurement unit <b>206</b> of the system <b>200</b> is configured to determine the indication of the skin parameter and/or an indication of any other parameters described herein (for example, a hair parameter). The user measurement unit <b>206</b> may acquire information relating to the skin parameter and/or any other parameters (e.g., from imaging data acquired by a user equipment) in order to determine the indication. For example, the imaging data may indicate that a hair removal session has caused some skin irritation, which may be apparent by comparing the redness in the skin between different images. Thus, the user measurement unit <b>206</b> may perform measurements which can be used to determine the indication of the skin parameter.</p><p id="p-0045" num="0044">The user measurement unit <b>206</b> may map the skin in order to determine the indication of the skin parameter (and/or any other parameters) for different spatial locations on the user's skin.</p><p id="p-0046" num="0045">The user measurement unit <b>206</b> may perform processing (e.g., using on-board processing circuitry of a user equipment) to determine the skin parameter and/or any other parameters described herein. Additionally or alternatively, the user measurement unit <b>206</b> may send its acquired data to an online service so that the online service may determine the skin parameter and/or any other parameters. Further the processing may facilitate mapping of the user's skin such that a spatial location is associated with a certain skin parameter and/or any other parameter described herein.</p><p id="p-0047" num="0046">In some embodiments, the user measurement unit <b>206</b> is configured to map certain relevant skin and/or hair parameters such as skin moisture, skin roughness, hair growth orientation, hair length, hair density, post-hair removal irritation/redness of the user.</p><p id="p-0048" num="0047">The user measurement unit <b>206</b> may be configured to cause an on-board sensor of the user equipment (e.g., an imaging device such as a camera) to acquire data such as imaging data to be used (e.g., by the user measurement unit <b>206</b> itself or an online service) to determine the skin parameter and/or any other parameter.</p><p id="p-0049" num="0048">The position of the hair removal unit <b>204</b> relative to the user's skin may be determined by a hair removal unit localization unit <b>208</b>. The position of the hair removal unit <b>204</b> may refer to or be indicative of a position of a hair removal device (e.g., a blade) of the hair removal unit <b>204</b> on the user's skin. Additionally or alternatively, the position of the hair removal unit <b>204</b> may refer to or be indicative of an orientation of the hair removal device relative to the user's skin. The hair removal unit <b>204</b> may comprise the hair removal device and other components such as a handle. While performing hair removal, the position of the hair removal device itself may be used to determine the hair removal instruction. Thus, the hair removal unit localization unit <b>208</b> may determine the position of the hair removal device itself, which may provide accurate information regarding the spatial location on the user's skin where the hair removal device (e.g., the blade) is located. Additionally or alternatively, the position of any other part of the hair removal unit <b>204</b> may be determined, which may infer or be used to determine the position of the hair removal device relative to the user's skin.</p><p id="p-0050" num="0049">The hair removal unit localization unit <b>208</b> may be implemented by any user equipment (e.g., a smart device) or other user device depending on the procedure used to determine the position of the hair removal unit <b>204</b>. For example, the hair removal unit <b>204</b> itself may comprise an on-board sensor such as a motion-sensitive detector (e.g., accelerometer) and/or an imaging device to determine its position relative to the user's skin.</p><p id="p-0051" num="0050">Additionally or alternatively, the user equipment (e.g., a smart device) may comprise or at least partially implement the hair removal localization unit <b>208</b>. For example, the user equipment may comprise an imaging device such as a camera for acquiring imaging data, which can be used to track the position of the hair removal unit <b>204</b> and/or the user's hand relative to the user's skin. The imaging data may be processed by on-board processing circuitry of the user equipment and/or may be communicated to an online service or other processing apparatus to be processed. The tracking of the position of the hair removal unit <b>204</b> relative to the user's skin may, for example, involve a machine vision-based tracking procedure. The tracking procedure may also take into account the user's skin using a skin recognition algorithm. For example, if tracking the hair removal unit <b>204</b> on the user's face, a facial recognition algorithm in combination with a tracking algorithm may be used to determine where, on the user's face, the hair removal unit <b>204</b> is positioned.</p><p id="p-0052" num="0051">The system <b>200</b> comprises a processing unit <b>210</b> for implementing certain methods described herein, such as the method <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The processing unit <b>210</b> comprises processing circuitry for implementing the method. In this embodiment, the processing unit <b>210</b> obtains the indication comprising the skin parameter and/or any other parameter of the user from the user measurement unit <b>206</b> (e.g., in accordance with block <b>102</b> of the method <b>100</b>). The processing unit <b>210</b> further obtains the indication comprising the interaction between the user's skin and the hair removal unit <b>204</b> from the hair removal unit <b>204</b> (e.g., in accordance with block <b>104</b> of the method <b>100</b>). The processing unit <b>210</b> determines the position of the hair removal unit <b>204</b> relative to the user's skin based on data provided by the hair removal unit localization unit <b>208</b> (e.g., in accordance with block <b>106</b> of the method <b>100</b>).</p><p id="p-0053" num="0052">The processing unit <b>210</b> determines a hair removal instruction for the user <b>202</b> based on the indication and the position (e.g., in accordance with block <b>106</b> of the method <b>100</b>). In some embodiments, the processing unit <b>210</b> may determine a map of optimal hair removal instructions for at least one spatial location on the user's skin. For example, the hair removal instruction may be indicative of at least one of: an applied pressure, hair removal unit <b>204</b> motion direction, hair removal unit <b>204</b> motion speed that is recommended for the user <b>202</b> based on at least one of: the skin parameter (e.g., as provided in a skin measurement map), any other parameters as described herein; and the position of the hair removal unit <b>204</b> relative to the user's skin.</p><p id="p-0054" num="0053">The processing unit <b>210</b> causes a user interface <b>212</b> of the system <b>200</b> to provide the hair removal instruction to the user (e.g., in accordance with block <b>108</b> of the method <b>100</b>). In some embodiments, the user interface <b>212</b> comprises a display for visualization of the recommended hair removal instruction mapped on to a skin and/or hair parameter map to provide real-time guidance (e.g., visual guidance) to the user for hair removal that may lead to improved and/or optimal hair removal results and/or reduced/minimized skin irritation.</p><p id="p-0055" num="0054">The user interface <b>212</b> may be provided by a user equipment that is the same as or different to the user equipment or user device providing the user measurement unit <b>206</b> and/or the hair removal unit localization unit <b>208</b>. For example, a smart phone or other smart device may perform imaging via its on-board camera to obtain the indication and the position. Further, a display screen and/or speaker of the smart device may be configured to provide the hair removal instruction (e.g., in a format appropriate for the user to interpret).</p><p id="p-0056" num="0055">In some embodiments, the processing unit <b>210</b> further comprises or can access a memory unit for storing at least one of: certain measurements (such as obtained by the hair removal unit <b>204</b>, user measurement unit <b>206</b> and/or hair removal unit localization unit <b>208</b>), a skin and/or hair parameter map (e.g., comprising a skin, hair and/or other parameter for a corresponding spatial location on the skin), a derived skin parameter and/or any other parameter, hair removal unit position relative to the user's skin, a map of hair removal unit <b>204</b> usage (e.g., previous usage) and a map of optimal hair removal unit <b>204</b> instructions.</p><p id="p-0057" num="0056">In some embodiments, the processing unit <b>210</b> may calculate an optimal hair removal unit <b>204</b> configuration. The hair removal unit <b>204</b> configuration may refer to, for example, a blade speed, blade rotation speed, cutting force and/or power for the hair removal unit <b>204</b>. The processing unit <b>210</b> may provide feedback to the hair removal unit <b>204</b> such that the hair removal unit <b>204</b> adapts in real-time for optimal hair removal. The calculation of the optimal hair removal unit <b>204</b> configuration may be provided in addition to certain methods described herein or may replace certain blocks of certain methods. For example, the calculation of the optimal hair removal unit <b>204</b> configuration may be provided in addition to the blocks of the method <b>100</b>. Alternatively, blocks <b>106</b> and <b>108</b> of the method <b>100</b> may be omitted and the calculation of the optimal hair removal unit <b>204</b> configuration may be implemented in combination with blocks <b>102</b> and <b>104</b> of the method <b>100</b>. Alternatively, blocks <b>104</b>, <b>106</b> and <b>108</b> of the method <b>100</b> may be omitted and the calculation of the optimal hair removal unit <b>204</b> configuration may be implemented in combination with block <b>102</b> of the method <b>100</b>.</p><p id="p-0058" num="0057">The processing unit <b>210</b> for implementing certain methods described herein may be provided by a user equipment such as described above. Alternatively, the processing unit <b>210</b> may be provided by an online service (e.g., at a server or cloud-based service).</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a method <b>300</b> (e.g., a computer-implemented method) of providing hair removal instructions for a user. The method <b>300</b> may be implemented by processing circuitry such as provided by the processing unit <b>210</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> or any other processing apparatus or circuitry described herein. As will be described in more detail herein, the method <b>300</b> may allow the user to improve and/or optimize hair removal results and/or reduce skin irritation due to hair removal. The method <b>300</b> may comprise certain blocks corresponding to certain blocks of the method <b>100</b>. Certain blocks of the method <b>300</b> may be omitted and/or modified.</p><p id="p-0060" num="0059">In some embodiments, a hair removal unit such as a shaver may be communicatively coupled to a smart device, e.g. smartphone loaded with a Real Time Shaving Guidance application, or &#x2018;app&#x2019;, to assist a user with hair removal. The hair removal unit may be used for shaving/removing facial hair and/or for removing hair from any other part of the body.</p><p id="p-0061" num="0060">The method <b>300</b> comprises, at block <b>302</b>, acquiring imaging data (e.g., at least one image) of the user's skin. For example, the block <b>302</b> may cause an imaging device (e.g., of a user equipment) to obtain the imaging data. In this block <b>302</b>, the imaging data is acquired prior to a user hair removal session.</p><p id="p-0062" num="0061">Based on the imaging data, the method <b>300</b> comprises, at block <b>304</b>, determining certain data regarding a skin and/or hair parameter associated with the user. The data regarding the skin and/or hair parameter may be referred to as pre-hair removal skin parameter data (i.e., the data may relate to the skin and/or hair parameter). In some embodiments, an algorithm may determine the skin and/or hair parameter based on a machine learning model which has been trained to identity and/or classify certain skin parameters (e.g., skin redness, for example) from the imaging data.</p><p id="p-0063" num="0062">A corresponding skin and/or hair parameter map may be generated by the processing circuitry based on the skin and/or hair parameter data. The skin and/or hair parameter map may comprise at least one skin and/or hair parameter (and/or any other parameter) associated with at least one spatial location of the user's skin.</p><p id="p-0064" num="0063">A user may initially capture an image of their skin (e.g., their face) using their smart device to obtain a baseline skin parameter map (e.g., a baseline facial skin map). This can be done via certain facial tracking techniques (e.g., based on machine learning or another algorithm). During this imaging data acquisition, the present skin and/or hair conditions (e.g., length and/or type of hair) for each spatial location may be determined and recorded in a memory for future use. This image capture and present skin/hair condition determination procedure may be performed before each hair removal session and/or may be performed before the first (i.e., first ever) hair removal session with the hair removal unit.</p><p id="p-0065" num="0064">In some embodiments, the baseline skin parameter map may comprise any relevant skin and/or hair parameters (e.g., skin moisture, skin roughness, hair growth orientation, hair length, hair density, post-hair removal irritation/redness of the user).</p><p id="p-0066" num="0065">The method <b>300</b> further comprises, at block <b>306</b>, providing or accessing historical data for the user (e.g., from a memory such as the memory unit referred to in relation to <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0067" num="0066">A previous hair removal session may have yielded certain data regarding hair removal performance and/or at least one skin and/or hair parameter associated with the user. For example, if the previous hair removal session caused skin irritation, this may be reflected by the corresponding skin and/or hair parameter for the spatial location(s) affected by the skin irritation. The historical data may comprise or be used to calculate a comparison of an outcome of a hair removal session (e.g., a comparison of the post-hair removal skin parameter data with the pre-hair removal skin parameter data).</p><p id="p-0068" num="0067">In some embodiments, the historical data may comprise or be referred to as a post-hair removal skin parameter map (i.e., the historical data may relate to the skin and/or hair parameter). The post-hair removal skin parameter map may have been obtained previously after a previous hair removal session. The post-hair removal skin parameter map may comprise the comparison of a skin and/or hair parameter map obtained before and after the hair removal session.</p><p id="p-0069" num="0068">The method comprises, at block <b>308</b>, providing or accessing predetermined knowledge regarding hair removal (which knowledge may be stored in a memory e.g., of an online service or of a user equipment). For example, the predetermined knowledge may comprise general (e.g., clinical) knowledge on hair removal techniques and/or the skin-hair interaction. Such knowledge may comprise, for example, at least one of: an optimal hair removal unit pressure to apply on the skin (e.g., for certain skin types and/or position of the hair removal unit); optimal hair removal unit speed across the user's skin; optimal hair removal unit direction and/or motion pattern for certain spatial locations on the skin and/or hair lengths/types.</p><p id="p-0070" num="0069">The predetermined knowledge may be used for providing an initial recommendation on the hair removal technique. In some embodiments, further recommendations may be personalized based on data obtained from subsequent hair removal sessions. Thus, the method <b>300</b> comprises, at block <b>310</b>, generating an optimal hair removal map of the user's skin. A spatial location of the optimal hair removal map may be associated with an optimal hair removal technique that is determined based on at least one of: historical data for the user (e.g., from block <b>304</b>); pre-hair removal skin parameter data (e.g., from block <b>306</b>) and predetermined knowledge regarding hair removal (e.g., from block <b>308</b>).</p><p id="p-0071" num="0070">In some embodiments, the optimal hair removal map may be stored in a memory (e.g., of a user equipment or an online service), for example, to allow the optimal hair removal map to be accessed during or after a hair removal session.</p><p id="p-0072" num="0071">The spatial location may be associated with at least one of: a skin parameter and a hair parameter for the user's skin at that spatial location. For example, the optimal hair removal map may provide an indication of the skin and/or hair parameter at the spatial location. The indication of the skin and/or hair parameter provided by the optimal hair removal map may be referred to as pre-hair removal skin and/or hair parameter data.</p><p id="p-0073" num="0072">In some embodiments, the method <b>300</b> comprises accessing the optimal hair removal map of the user's skin (e.g., as generated according to block <b>310</b>); and determining the hair removal instruction for the spatial location based on the optimal hair removal map. For example, the method <b>300</b> comprises, at block <b>312</b>, starting a hair removal session in which the optimal hair removal map is accessed.</p><p id="p-0074" num="0073">In some embodiments, obtaining the indication (e.g., in accordance with block <b>102</b> of the method <b>100</b>) comprising the skin parameter of the user comprises accessing skin parameter data for the user determined based on imaging data of the user's skin. For example, the imaging data of the user's skin may refer to the imaging data acquired at block <b>302</b> of the method <b>300</b>. As described above, the optimal hair removal map may provide the indication of the skin parameter. Thus, obtaining the indication comprising the skin parameter of the user may comprise accessing the optimal hair removal map (e.g., as described in relation to blocks <b>310</b>/<b>312</b> of the method <b>300</b>).</p><p id="p-0075" num="0074">Additionally or alternatively, the imaging data of the user's skin may refer to imaging data acquired at another block of the method <b>300</b>, which may be used to provide the indication comprising the skin parameter of the user (for example, in real-time, as will be described in more detail herein).</p><p id="p-0076" num="0075">Before or during a hair removal session, the optimal hair removal map may be accessed in order to allow determination of a hair removal instruction for the user based on the indication comprising the skin parameter of the user (e.g., as referred to in block <b>106</b> of the method <b>100</b>). As will be described in more detail below, the method <b>300</b> may use information derived from the optimal hair removal map in conjunction with other information obtained during the hair removal session in order to determine the hair removal instruction.</p><p id="p-0077" num="0076">The method <b>300</b> comprises, at block <b>314</b>, determining, in real-time, the position of the hair removal unit relative to the user's skin. The data for determining the position of the hair removal unit in block <b>314</b> may be obtained by the hair removal unit localization unit <b>208</b> described in relation to <figref idref="DRAWINGS">FIG. <b>2</b></figref>. This data may be obtained from at least one of: imaging data acquired from an imaging device for acquiring images of the skin; and an on-board sensor of the hair removal unit <b>304</b>. The determination of the position of the hair removal unit may be determined from this data (e.g., using the processing unit <b>210</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0078" num="0077">In some embodiments, determining the position of the hair removal unit relative to the user's skin comprises acquiring at least one of: imaging data of the user's skin and the hair removal unit; and motion data from a sensor on-board the hair removal unit.</p><p id="p-0079" num="0078">The position of the hair removal unit relative to the user's skin may comprise at least one of: a position of a hair removal device of the hair removal unit on the user's skin; and an orientation of the hair removal device relative to the user's skin.</p><p id="p-0080" num="0079">Positioning data may be obtained from the imaging data and/or the on-board sensor. This positioning data may be used to track the position of the hair removal unit relative to the user's skin as the user moves the hair removal unit across their skin.</p><p id="p-0081" num="0080">In the example of the app give above, during the hair removal session, the app may determine the real-time motion (e.g., position and/or orientation) of the hair removal unit relative to the user's skin, using a series of images captured by a camera of the user equipment. This determination of the real-time motion can be performed by tracking the hair removal unit and/or the user's hand within the image series, for example, using a computer vision algorithm. The determination may be supported by motion and/or orientation tracking within the hair removal unit itself (e.g., using an on-board sensor of the hair removal unit).</p><p id="p-0082" num="0081">The method <b>300</b> further comprises, at block <b>316</b>, determining, in real-time, the interaction between the user's skin and the hair removal unit. As mentioned previously, the hair removal unit <b>204</b> may comprise at least one sensor for determining the interaction (e.g., applied pressure) between the user's skin and the hair removal unit <b>204</b>. In some embodiments, data for determining the interaction may be obtained from the hair removal unit <b>204</b>.</p><p id="p-0083" num="0082">For example, during a hair removal session, the applied pressure may be recorded in real-time and may be linked to the position of the hair removal unit.</p><p id="p-0084" num="0083">In some embodiments, the amount of hair cut/removed may be recorded or inferred. For example, the amount of hair cut/removed may be determined from at least one of: a sound analysis (e.g., using a microphone of a user equipment or of the hair removal unit itself to determine how many hairs are being cut or removed); a motor resistance observed by a hair removal device (e.g., motorized blade) of the hair removal unit (i.e., the motor resistance may be affected by pressure on skin and/or number of hairs cut/removed); and a computer vision analysis (e.g., using imaging data obtained from a camera of the user equipment) of the results obtained during the hair removal session.</p><p id="p-0085" num="0084">The method <b>300</b> comprises, at block <b>318</b>, determining, in real-time, the skin parameter. As mentioned above, the indication comprising the skin parameter of the user may be determined from imaging data acquired before the hair removal session. In block <b>318</b>, the skin parameter may be determined from imaging data (e.g., at least one image) acquired during the hair removal session. For example, the imaging data may be acquired by an imaging device of a user equipment and this imaging data may be processed in order to provide the indication comprising the skin parameter of the user (e.g., in a similar manner to block <b>304</b> determining certain data regarding the skin parameter associated with the user).</p><p id="p-0086" num="0085">In some embodiments, block <b>318</b> may cause the imaging device to acquire the imaging data during the user hair removal session, to determine present skin parameter data. The skin parameter of the user may be based on a comparison between the pre-hair removal skin parameter data (e.g., as referred to in block <b>310</b>) and the present skin parameter data.</p><p id="p-0087" num="0086">The method <b>300</b> comprises, at block <b>320</b>, determining a real-time hair removal instruction for the user based on at least one of: the position; the interaction; the skin parameter; historical hair removal performance data for the user; and pre-determined hair removal performance data. Thus, at least one of blocks <b>314</b>, <b>316</b> and <b>318</b> may be implemented in order to determine the real-time hair removal instruction. For example, the real-time hair removal instruction may provide at least one of: a recommended pressure, hair removal unit motion direction and/or pattern and/or hair removal unit motion speed which has been calculated for each position on the skin and/or hair parameter map.</p><p id="p-0088" num="0087">The historical hair removal performance data for the user may comprise at least one of: user skin type; user skin condition; pressure applied between the hair removal unit and the user's skin; user hair removal behavior; visible skin irritation (e.g., skin redness); hair removal results (e.g., hair cutting effectiveness); hair removal unit motion (e.g., direction and type of movement) and hair removal unit operational performance (e.g., battery level, cutting speed). The historical hair removal performance data may be determined from at least one previous user hair removal session.</p><p id="p-0089" num="0088">This historical hair removal performance data may be an example of the historical data provided at block <b>306</b> of the method <b>300</b>. The term &#x2018;visible&#x2019; in relation to the skin irritation may refer to whether or not the skin irritation is visible to a machine vision system or any system capable of detecting skin irritation, whether visible to the human eye, or not.</p><p id="p-0090" num="0089">The pre-determined hair removal performance data may comprise knowledge acquired from other users and/or clinical data regarding at least one of: skin type; skin condition; pressure applied between the hair removal unit and the other user's skin; hair removal behavior; visible skin irritation; hair removal results; hair removal unit motion and hair removal unit operational performance.</p><p id="p-0091" num="0090">This knowledge may be used to determine a recommended hair removal instruction for the user in order to provide improved hair removal experience (e.g., more efficient cutting, less time, less skin irritation) as compared to a previous user hair removal session. This pre-determined hair removal performance data may be an example of the pre-determined knowledge regarding hair removal provided at block <b>308</b> of the method <b>300</b>.</p><p id="p-0092" num="0091">The method <b>300</b> further comprises, at block <b>322</b>, comprising causing a user interface (e.g., the user interface <b>212</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>) to provide, in real-time, the real-time hair removal instruction for the user.</p><p id="p-0093" num="0092">At block <b>324</b> of the method <b>300</b>, the user ends their hair removal session. Certain blocks described below may be used to evaluate the results of the hair removal session, which may provide certain information that can be used in a subsequent hair removal session (e.g., the information may be stored in a memory (e.g., of a user equipment or an online service) so as to be provided at block <b>306</b>).</p><p id="p-0094" num="0093">The method <b>300</b> comprises, at block <b>326</b>, causing the imaging device to acquire the imaging data (i.e., after the user hair removal session), to determine post-hair removal skin parameter data. At block <b>328</b> of the method <b>300</b>, skin and/or hair parameter data is generated (which may be referred to as post-hair removal skin parameter data). This skin and/or hair parameter data may be used to generate a skin and/or hair parameter map. The post-hair removal skin parameter data may relate to a skin and/or hair parameter and/or any other parameter.</p><p id="p-0095" num="0094">The method <b>300</b> further comprises, at block <b>330</b>, determining a skin and/or hair parameter map for the user based on a comparison between the pre-hair removal and present and/or post-hair removal skin parameter data. The comparison may be made between a map derived from the present and/or post-hair removal skin parameter data and the baseline skin parameter map.</p><p id="p-0096" num="0095">In some embodiments, the skin parameter comprises a visible skin irritation indicator (e.g., skin redness). The skin parameter may be based on whether or not the comparison (e.g., at block <b>330</b>) identifies any change in the visible skin irritation indicator between the pre-hair removal and present and/or post-hair removal skin parameter data.</p><p id="p-0097" num="0096">For example, after the first hair removal session, the skin redness is measured and a map of the redness is saved (e.g., the map may correspond to the skin parameter map). This measurement may be performed by analyzing data such as imaging data acquired from a camera of a user equipment and/or from a separate skin analysis device. The analysis may comprise a comparison with the image(s) captured before the hair removal session began and/or use data provided by the separate skin analysis device. The separate skin analysis device may refer to any other device capable of measuring a certain property of the skin such as hydration, gloss/oiliness, spots and redness, among other properties. Certain examples of such skin analysis devices may illuminate the skin with radiation (e.g., ultraviolet, visible and/or infrared) and detect characteristics (such as a change in spectral content and/or intensity) from the radiation reflected by the skin.</p><p id="p-0098" num="0097">Additionally or alternatively, certain data such as the recorded hair removal unit applied pressure, skin redness, hair removal results and/or the hair removal unit motion data may be processed to identify hair removal actions/techniques related to a position on the skin where those actions/techniques resulted in optimal, sub-optimal or poor hair removal results.</p><p id="p-0099" num="0098">For example, the data acquired at certain blocks (e.g., blocks <b>302</b>, <b>314</b>, <b>316</b>, <b>318</b>, <b>326</b>) and analyzed (e.g., at blocks <b>310</b>, <b>328</b>, <b>330</b>) may provide an indication that can be used to update the hair removal instruction. In an example, too much pressure applied by the hair removal unit on the skin may result in skin irritation and/or suboptimal hair cutting. In another example, insufficient skin contact (e.g., including suboptimal orientation of the hair removal unit in relation to the skin) may result in sub-optimal hair cutting and/or hair pulling. In another example, a suboptimal direction or motion pattern (e.g., straight vs circular) of the hair removal unit may result in suboptimal hair cutting. In another example, suboptimal hair removal unit motion speeds (e.g., too fast or too slow) may result in skin irritation, suboptimal hair cutting and/or shaving inefficiency. In another example, too many passes of the hair removal unit over a particular spatial location of the skin may be a result of suboptimal hair cutting, resulting in skin irritation and/or shaving inefficiency. In another example, other suboptimal hair removal techniques may be identified that can be improved by different user and/or hair removal unit behavior. Any combination of these and other examples may be identified from the acquired data and used to recommend a technique (e.g., hair removal instruction) to the user which may result in improved and/or optimal hair removal and/or reduced skin irritation.</p><p id="p-0100" num="0099">During a subsequent hair removal session (e.g., after the first or initial hair removal session), certain blocks of the method <b>300</b> may be implemented. For example, the real-time motion (e.g., position and/or orientation) of the hair removal unit relative to the skin may be determined using a series of images captured by the camera (e.g., in accordance with block <b>318</b>). In some embodiments, other parameters such as the skin and/or hair parameter may be determined and/or other sensors may be used as well to provide data which can be used to determine the recommendation for the user.</p><p id="p-0101" num="0100">For example, based on the real-time position of the hair removal unit, the recommended hair removal unit pressure, direction and speed may be visually shown to the user in real-time. In a further example, the recommended hair removal guidance may be shown (e.g., by a display of a user equipment) in relation to the actual shaving behavior of the user, which may provide direct cues to the user such as indicating: an increase or decrease of applied pressure, an increase or decrease in the motion direction and/or a certain motion pattern (e.g., straight or circular motion and/or a different diameter of circular motion) and/or an increase or decrease in motion speed.</p><p id="p-0102" num="0101">During the subsequent hair removal session, the applied hair removal pressure (and/or other parameters) may be recorded in real-time and linked to the position of the hair removal unit.</p><p id="p-0103" num="0102">After the subsequent hair removal session, the skin irritation (e.g., skin redness) may be measured again and a map of the skin irritation may be saved in a memory (and used to update the hair removal instruction).</p><p id="p-0104" num="0103">The recorded pressure, skin irritation, position data and/or any previous recommendations may be processed, to calculate personalized recommendations. These calculated personalized recommendations may be based on the level of adherence to the guidance indicated by the cues described above.</p><p id="p-0105" num="0104">Additionally or alternatively, these recommendations may be based on the results in terms of skin irritation and/or hair removal efficiency. In some cases, perfect adherence by the user can still lead to suboptimal results, which may indicate that the general knowledge may not apply to this user and may need to be personalized for the user by learning from data acquired from the user's hair removal session(s).</p><p id="p-0106" num="0105">Once the personalized recommendations have been calculated, the recommended pressure, motion direction and/or motion speed may be recalculated for each position in the skin and/or hair parameter map for the next hair removal session.</p><p id="p-0107" num="0106">In some embodiments, the hair removal instruction is configured to provide a personalized recommendation for the user regarding at least one of: pressure to apply between the hair removal unit and the user's skin; hair removal unit positioning (e.g., including orientation of the hair removal unit) relative to the user's skin and hair removal unit motion (e.g., including direction, speed and motion pattern of the hair removal unit). In other words, methods described herein may obtain information such as the skin and/or hair parameter for the user and thereby determine the personalized recommendation.</p><p id="p-0108" num="0107">Certain embodiments described herein refer to a user's skin parameter, an indication of which is obtained by certain methods described herein. In some embodiments, the indication further comprises a hair parameter of the user. Thus, certain methods described herein may further comprise determining the hair removal instruction taking into account the hair parameter. In other words, the hair removal instruction may be based on an analysis of the skin and/or hair parameter acquired from imaging data, which may have been obtained prior to a hair removal session (e.g., at block <b>302</b> or from a previous hair removal session's block <b>326</b>) or during a hair removal session (e.g., at block <b>318</b>).</p><p id="p-0109" num="0108">In some embodiments, the method <b>300</b> may further comprise causing the user interface to provide the hair removal instruction for the user based on whether or not the user has deviated from a previously-recommended hair removal instruction. For example, during a hair removal session, a user may deviate from the recommended hair removal instruction. The method <b>300</b> may then update the hair removal instruction to accommodate/correct for the user's deviation.</p><p id="p-0110" num="0109"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an apparatus <b>400</b>, which may be used for implementing certain methods described herein such as the methods <b>100</b>, <b>300</b>. The apparatus <b>400</b> comprises processing circuitry <b>402</b>. The processing circuitry <b>402</b> may correspond to the processing circuitry of the processing unit <b>210</b> described in relation to <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In this embodiment, the processing circuitry <b>402</b> comprises an obtaining module <b>404</b>. The obtaining module <b>404</b> is configured to obtain an indication comprising a skin parameter of a user; and an interaction between the user's skin and a hair removal unit (such as described in relation to block <b>102</b> of the method <b>100</b>).</p><p id="p-0111" num="0110">The processing circuitry <b>402</b> further comprises a determining module <b>406</b>. The determining module <b>406</b> is configured to determine a position of the hair removal unit relative to the user's skin (such as described in relation to block <b>104</b> of the method <b>100</b>). The determining module <b>406</b> is further configured to determine a hair removal instruction for the user based on the indication and the position (such as described in relation to block <b>106</b> of the method <b>100</b>).</p><p id="p-0112" num="0111">The processing circuitry <b>402</b> further comprises a user instruction module <b>408</b>. The user instruction module <b>408</b> is configured to cause a user interface to provide the hair removal instruction for the user (such as described in relation to block <b>108</b> of the method <b>100</b>).</p><p id="p-0113" num="0112"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an apparatus <b>500</b>, which may be used for implementing certain methods described herein such as the methods <b>100</b>, <b>300</b>. The apparatus <b>500</b> comprises processing circuitry <b>502</b>. The processing circuitry <b>502</b> comprises the processing circuitry <b>402</b> of the apparatus <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0114" num="0113">In some embodiments, the apparatus <b>500</b> further comprises an imaging device <b>504</b> such as a camera of a user equipment for acquiring imaging data of the user's skin and the hair removal unit (e.g., hair removal unit <b>204</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0115" num="0114">In some embodiments, the apparatus <b>500</b> further comprises the user interface <b>506</b> (e.g., as referred to in the user instruction module <b>408</b>) of a user equipment.</p><p id="p-0116" num="0115">In some embodiments, the apparatus <b>500</b> comprises both the imaging device <b>504</b> and the user interface <b>506</b>.</p><p id="p-0117" num="0116"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a tangible machine-readable medium <b>600</b> storing instructions <b>602</b> which, when executed by at least one processor <b>604</b>, cause the at least one processor <b>604</b> to implement certain methods described herein (such as the methods <b>100</b>, <b>300</b>).</p><p id="p-0118" num="0117">The instructions <b>602</b> comprise instructions <b>606</b> that cause the at least one processor <b>604</b> to obtain an indication comprising a skin parameter of a user; and an interaction between the user's skin and a hair removal unit (such as described in relation to block <b>102</b> of the method <b>100</b>).</p><p id="p-0119" num="0118">The instructions <b>602</b> further comprise instructions <b>608</b> that cause the at least one processor <b>604</b> to determine a position of the hair removal unit relative to the user's skin (such as described in relation to block <b>104</b> of the method <b>100</b>).</p><p id="p-0120" num="0119">The instructions <b>602</b> further comprise instructions <b>610</b> that cause the at least one processor <b>604</b> to determine a hair removal instruction for the user based on the indication and the position (such as described in relation to block <b>106</b> of the method <b>100</b>).</p><p id="p-0121" num="0120">The instructions <b>602</b> further comprise instructions <b>612</b> that cause the at least one processor <b>604</b> to cause a user interface to provide the hair removal instruction for the user (such as described in relation to block <b>108</b> of the method <b>100</b>).</p><p id="p-0122" num="0121">One or more features described in one embodiment may be combined with or replace features described in another embodiment. For example, the methods <b>100</b> and <b>300</b> of <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>3</b></figref> may be modified based on features described in relation to the system <b>200</b> and apparatus <b>400</b>, <b>500</b> of <figref idref="DRAWINGS">FIGS. <b>2</b>, <b>4</b> and <b>5</b></figref>, and vice versa.</p><p id="p-0123" num="0122">In some embodiments, certain methods described herein may be implemented by processing circuitry of a user equipment such as a mobile phone, tablet, mirror, smart device or any other device. In some embodiments, certain methods described herein may be implemented by processing circuitry of an online service such as provided by a server or cloud-based service. In some embodiments, the user equipment and the online service may exchange information as part of the implementation of certain methods described herein.</p><p id="p-0124" num="0123">Embodiments in the present disclosure can be provided as methods, systems or as a combination of machine readable instructions and processing circuitry. Such machine readable instructions may be included on a non-transitory machine (for example, computer) readable storage medium (including but not limited to disc storage, CD-ROM, optical storage, etc.) having computer readable program codes therein or thereon.</p><p id="p-0125" num="0124">The present disclosure is described with reference to flow charts and block diagrams of the method, devices and systems according to embodiments of the present disclosure. Although the flow charts described above show a specific order of execution, the order of execution may differ from that which is depicted. Blocks described in relation to one flow chart may be combined with those of another flow chart. It shall be understood that each block in the flow charts and/or block diagrams, as well as combinations of the blocks in the flow charts and/or block diagrams can be realized by machine readable instructions.</p><p id="p-0126" num="0125">The machine readable instructions may, for example, be executed by a general purpose computer, a special purpose computer, an embedded processor or processors of other programmable data processing devices to realize the functions described in the description and diagrams. In particular, a processor or processing circuitry, or a module thereof, may execute the machine readable instructions. Thus functional modules of the apparatus <b>400</b>, <b>500</b> (for example, the obtaining module <b>404</b>, determining module <b>406</b> and/or user instruction module <b>408</b>) and other devices described herein may be implemented by a processor executing machine readable instructions stored in a memory, or a processor operating in accordance with instructions embedded in logic circuitry. The term &#x2018;processor&#x2019; is to be interpreted broadly to include a CPU, processing unit, ASIC, logic unit, or programmable gate array etc. The methods and functional modules may all be performed by a single processor or divided amongst several processors.</p><p id="p-0127" num="0126">Such machine readable instructions may also be stored in a computer readable storage that can guide the computer or other programmable data processing devices to operate in a specific mode.</p><p id="p-0128" num="0127">Such machine readable instructions may also be loaded onto a computer or other programmable data processing devices, so that the computer or other programmable data processing devices perform a series of operations to produce computer-implemented processing, thus the instructions executed on the computer or other programmable devices realize functions specified by block(s) in the flow charts and/or in the block diagrams.</p><p id="p-0129" num="0128">Further, the teachings herein may be implemented in the form of a computer program product, the computer program product being stored in a storage medium and comprising a plurality of instructions for making a computer device implement the methods recited in the embodiments of the present disclosure.</p><p id="p-0130" num="0129">Elements or steps described in relation to one embodiment may be combined with or replaced by elements or steps described in relation to another embodiment. Other variations to the disclosed embodiments can be understood and effected by those skilled in the art in practicing the claimed invention, from a study of the drawings, the disclosure, and the appended claims. In the claims, the word &#x201c;comprising&#x201d; does not exclude other elements or steps, and the indefinite article &#x201c;a&#x201d; or &#x201c;an&#x201d; does not exclude a plurality. A single processor or other unit may fulfil the functions of several items recited in the claims. The mere fact that certain measures are recited in mutually different dependent claims does not indicate that a combination of these measures cannot be used to advantage. A computer program may be stored or distributed on a suitable medium, such as an optical storage medium or a solid-state medium supplied together with or as part of other hardware, but may also be distributed in other forms, such as via the Internet or other wired or wireless telecommunication systems. Any reference signs in the claims should not be construed as limiting the scope.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-01-15" num="01-15"><claim-text><b>1</b>-<b>15</b>. (canceled)</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A computer-implemented method comprising:<claim-text>obtaining an indication comprising:<claim-text>a skin parameter of a user, wherein the indication of the skin parameter comprises an indication of skin irritation determined by a measurement of imaging data, and</claim-text><claim-text>an interaction between skin of the user and a hair removal unit;</claim-text></claim-text><claim-text>determining a position of the hair removal unit relative to the user's skin;</claim-text><claim-text>determining a hair removal instruction for the user based on the indication and the position; and</claim-text><claim-text>causing a user interface to provide the hair removal instruction for the user.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the skin parameter comprises a user skin condition determined, using a machine-learning model trained to identify and/or classify skin parameters, from the imaging data of the user's skin.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the indication further comprises a hair parameter of the user, wherein the hair parameter comprises a user hair condition determined, using the machine-learning model, from the imaging data of the user's skin.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the interaction between the user's skin and the hair removal unit is pressure applied between the hair removal unit and the user's skin.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising causing an imaging device to acquire the imaging data prior to a user hair removal session, to determine pre-hair removal skin parameter data.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The method of <claim-ref idref="CLM-00020">claim 20</claim-ref>, further comprising:<claim-text>causing the imaging device to acquire the imaging data during and/or after the user hair removal session, to determine present and/or post-hair removal skin parameter data;</claim-text><claim-text>generating the skin parameter data based on the imaging data; and</claim-text><claim-text>determining a skin parameter map for the user based on a comparison between the pre-hair removal skin parameter data and the present and/or post-hair removal skin parameter data.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the skin parameter comprises a visible skin irritation indicator and is based on whether or not the comparison identifies any change in the visible skin irritation indicator between the pre-hair removal skin parameter data and the present and/or post-hair removal skin parameter data.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein determining the hair removal instruction comprises:<claim-text>accessing an optimal hair removal map of the user's skin, wherein a spatial location of the optimal hair removal map is associated with an optimal hair removal technique that is determined based on at least one of: pre-hair removal skin parameter data; historical data for the user; and predetermined knowledge regarding hair removal; and</claim-text><claim-text>determining the hair removal instruction for the spatial location based on the optimal hair removal map.</claim-text></claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising at least one of:<claim-text>determining, in real-time, the position of the hair removal unit relative to the user's skin;</claim-text><claim-text>determining, in real-time, the interaction between the user's skin and the hair removal unit;</claim-text><claim-text>determining, in real-time, the skin parameter; and</claim-text><claim-text>determining a real-time hair removal instruction for the user based on at least one of: the position; the interaction; the skin parameter; historical hair removal performance data for the user; and pre-determined hair removal performance data.</claim-text></claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The method of <claim-ref idref="CLM-00024">claim 24</claim-ref>, further comprising causing the user interface to provide, in real-time, the hair removal instruction for the user.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The method of <claim-ref idref="CLM-00024">claim 24</claim-ref>,<claim-text>wherein the historical hair removal performance data for the user comprises at least one of: user skin type; user skin condition; pressure applied between the hair removal unit and the user's skin; user hair removal behavior; visible skin irritation; hair removal results; hair removal unit motion and hair removal unit operational performance, as determined from at least one previous user hair removal session, and</claim-text><claim-text>wherein the pre-determined hair removal performance data comprises knowledge acquired from other users and/or clinical data regarding at least one of: skin type; skin condition; pressure applied between the hair removal unit and the other user's skin; hair removal behavior; visible skin irritation; hair removal results; hair removal unit motion and hair removal unit operational performance, from which a recommended hair removal instruction for the user can be determined in order to provide improved hair removal experience as compared to a previous user hair removal session.</claim-text></claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the hair removal instruction is configured to provide a personalized recommendation for the user regarding at least one of: pressure to apply between the hair removal unit and the user's skin; hair removal unit positioning relative to the user's skin; and hair removal unit motion,<claim-text>Wherein the method further comprises causing the user interface to provide the hair removal instruction for the user based on whether or not the user has deviated from a previously-recommended hair removal instruction.</claim-text></claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein determining the position of the hair removal unit relative to the user's skin comprises acquiring at least one of: imaging data of the user's skin and the hair removal unit; and motion data from a sensor on-board the hair removal unit, and wherein the position of the hair removal unit relative to the user's skin comprises at least one of: a position of a hair removal device of the hair removal unit on the user's skin; and an orientation of the hair removal device relative to the user's skin.</claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. An apparatus comprising processing circuitry, the processing circuitry comprising:<claim-text>an obtaining module configured to obtain an indication comprising:<claim-text>a skin parameter of a user, wherein the indication of the skin parameter comprises an indication of skin irritation determined by a measurement of imaging data; and</claim-text><claim-text>an interaction between skin of the user and a hair removal unit;</claim-text></claim-text><claim-text>a determining module configured to:<claim-text>determine a position of the hair removal unit relative to the user's skin;</claim-text></claim-text><claim-text>a hair removal instruction for the user based on the indication and the position; and</claim-text><claim-text>a user instruction module configured to cause a user interface to provide the hair removal instruction for the user.</claim-text></claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. A tangible machine-readable medium storing instructions which, when executed by at least one processor, cause the at least one processor to:<claim-text>obtain an indication comprising:<claim-text>a skin parameter of a user, wherein the indication of the skin parameter comprises an indication of skin irritation determined by a measurement of imaging data; and</claim-text><claim-text>an interaction between skin of the user and a hair removal unit;</claim-text></claim-text><claim-text>determine a position of the hair removal unit relative to the user's skin;</claim-text><claim-text>determine a hair removal instruction for the user based on the indication and the position; and</claim-text><claim-text>cause a user interface to provide the hair removal instruction for the user.</claim-text></claim-text></claim></claims></us-patent-application>