<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004255A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004255</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17852147</doc-number><date>20220628</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>AU</country><doc-number>2021902017</doc-number><date>20210702</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>042</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0421</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>013</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">SYSTEMS AND METHODS FOR INTERACTING WITH A USER INTERFACE OF AN ELECTRONIC DEVICE</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>NNAK Pty Ltd.</orgname><address><city>Doreen</city><country>AU</country></address></addressbook><residence><country>AU</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Tabrizi</last-name><first-name>Kaveh Moghaddam</first-name><address><city>Doreen</city><country>AU</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems and methods for interacting with a user interface of an electronic device are disclosed herein. A system according to one aspect comprises an image recording device fixed in relation to the electronic device and configured to generate image data of an image area spaced from the user interface; an infrared device fixed in relation to the electronic device and configured to generate infrared data of an infrared area spaced from the user interface; and a controller operatively connected to the infrared device and the image recording device.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="80.09mm" wi="126.24mm" file="US20230004255A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="95.50mm" wi="128.27mm" file="US20230004255A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="140.12mm" wi="107.53mm" file="US20230004255A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="151.64mm" wi="56.13mm" file="US20230004255A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="140.21mm" wi="131.23mm" file="US20230004255A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="143.93mm" wi="107.19mm" file="US20230004255A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="151.72mm" wi="56.22mm" file="US20230004255A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="159.43mm" wi="107.10mm" file="US20230004255A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="209.72mm" wi="138.94mm" file="US20230004255A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="160.10mm" wi="129.96mm" file="US20230004255A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="205.15mm" wi="112.95mm" file="US20230004255A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present disclosure relates to methods and systems for interacting with a user interface of an electronic device.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Touch user interfaces are used with a range of electronic devices, such as computers, for enhancing user interaction therewith. For example, in the services industry, touch user interfaces are used with self-service kiosks, point-of-sale devices, self-checkout devices, EFTPOS devices, vending machines and advertising displays. In the banking industry, touch user interfaces are widely used with automated teller machines (ATM). In the medical industry, touch user interfaces are also used with telehealth devices and medical devices or instruments, such as dialysis machines and medical imaging devices. Such devices typically operate in busy environments and are used by multiple users.</p><p id="p-0004" num="0003">A disadvantage of touch user interfaces is that germs and other causes of infection, such as bacteria, viruses or other harmful organisms, may be placed on the surface of the touch user interfaces. Subsequent users may inadvertently become contaminated with such germs and viruses when using the touch user interface. Contaminated users may then transmit the germs and viruses to other persons, thus causing harm to those persons. Similarly, user interfaces in the form of public control buttons, such as pedestrian crossing push buttons, elevator push buttons and keypads of non-touch interfacing ATMs, are used by multiple users and may also become contaminated by germs and viruses during user contact with the control buttons.</p><p id="p-0005" num="0004">Apart from the public health risks posed by the spread of germs and viruses, such user interfaces may also be vulnerable to security threats, particularly in circumstances where a user is required to input a security pin or password using the user interfaces. For example, users may leave marks or fingerprints on the surface of such user interfaces which may be exploited by would-be perpetrators.</p><p id="p-0006" num="0005">Object</p><p id="p-0007" num="0006">It is an object of the present disclosure to substantially overcome or ameliorate one or more of the above disadvantages, or at least provide a useful alternative.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0008" num="0007">In accordance with an aspect of the present disclosure, there is provided a system for interacting with a user interface of an electronic device, the system comprising:</p><p id="p-0009" num="0008">an image recording device fixed in relation to the electronic device and configured to generate image data of an image area spaced from the user interface;</p><p id="p-0010" num="0009">an infrared device fixed in relation to the electronic device and configured to generate infrared data of an infrared area spaced from the user interface; and</p><p id="p-0011" num="0010">a controller operatively connected to the infrared device and the image recording device, the controller configured to:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0011">detect at least a portion of a finger of a person in the infrared area based on the infrared data;</li>        <li id="ul0002-0002" num="0012">in response to the detection of the portion of the finger, determine the position of the portion of the finger in relation to the infrared device based on the infrared data;</li>        <li id="ul0002-0003" num="0013">detect at least an eye of the person in the image area based on the image data;</li>        <li id="ul0002-0004" num="0014">in response to the detection of the eye of the person, determine the position of the eye in relation to the image recording device based on the image data; and</li>        <li id="ul0002-0005" num="0015">determine a desired point on the user interface based on the position of the portion of the finger and the position of the eye; and</li>        <li id="ul0002-0006" num="0016">generate an instruction to interact with the user interface based on the desired point.</li>    </ul>    </li></ul></p><p id="p-0012" num="0017">The controller may be configured to determine the position of the portion of the finger by:</p><p id="p-0013" num="0018">calculating a pixel coordinate of a point of the portion of the finger based on the infrared data; and</p><p id="p-0014" num="0019">calculating the position of the portion of the finger based on the pixel coordinate of the point of the portion of the finger.</p><p id="p-0015" num="0020">The controller may be configured to determine the position of the one eye by:</p><p id="p-0016" num="0021">calculating a pixel coordinate of a central point of the one eye; and</p><p id="p-0017" num="0022">calculating the position of the one eye based on the pixel coordinate of the central point of the one eye.</p><p id="p-0018" num="0023">The controller may be configured to determine a desired point on the user interface by:</p><p id="p-0019" num="0024">calculating a position on the user interface based on the position of the portion of the finger and the position of the one eye; and</p><p id="p-0020" num="0025">calculating the desired point, in the form of a pixel coordinate, on the user interface based on the position on the user interface.</p><p id="p-0021" num="0026">The controller may be configured to determine the position of the two eyes by:</p><p id="p-0022" num="0027">calculating a pixel coordinate of a central point between the two eyes; and</p><p id="p-0023" num="0028">calculating the position of the two eyes based on the pixel coordinate of the central point of the two eyes.</p><p id="p-0024" num="0029">The controller may be configured to determine a desired point on the user interface by:</p><p id="p-0025" num="0030">calculating a position on the user interface based on the position of the portion of the finger and the position of the two eyes; and</p><p id="p-0026" num="0031">calculating the desired point, in the form of a pixel coordinate, on the user interface based on the position on the user interface.</p><p id="p-0027" num="0032">The image recording device may be a camera.</p><p id="p-0028" num="0033">The infrared device may be an infrared touch frame.</p><p id="p-0029" num="0034">In accordance with another aspect of the present disclosure, there is provided a method of interacting with a user interface of an electronic device, the method comprising:</p><p id="p-0030" num="0035">detecting at least a portion of a finger of a person in an infrared area spaced from the user interface, based on infrared data generated by an infrared device;</p><p id="p-0031" num="0036">in response to the detection of the portion of the finger, determining the position of the portion of the finger in relation to the infrared device based on the infrared data;</p><p id="p-0032" num="0037">detecting at least an eye of the person in an image area spaced from the user interface, based on image data generated by an image recording device;</p><p id="p-0033" num="0038">in response to the detection of the eye of the person, determining the position of the eye in relation to the image recording device based on the image data;</p><p id="p-0034" num="0039">determining a desired point on the user interface based on the position of the portion of the finger and the position of the eye; and</p><p id="p-0035" num="0040">generating an instruction to interact with the user interface based on the desired point.</p><p id="p-0036" num="0041">Determining the position of the portion of the finger may comprise:</p><p id="p-0037" num="0042">calculating a pixel coordinate of a point of the portion of the finger based on the infrared data; and</p><p id="p-0038" num="0043">calculating the position of the portion of the finger based on the pixel coordinate of the point of the portion of the finger.</p><p id="p-0039" num="0044">Determining the position of the one eye may comprise:</p><p id="p-0040" num="0045">calculating a pixel coordinate of a central point of the one eye; and</p><p id="p-0041" num="0046">calculating the position of the one eye based on the pixel coordinate of the central point of the one eye.</p><p id="p-0042" num="0047">Determining a desired point on the user interface may comprise:</p><p id="p-0043" num="0048">calculating a position on the user interface based on the position of the portion of the finger and the position of the one eye; and</p><p id="p-0044" num="0049">calculating the desired point, in the form of a pixel coordinate, on the user interface based on the position on the user interface.</p><p id="p-0045" num="0050">Determining the position of the two eyes may comprise:</p><p id="p-0046" num="0051">calculating a pixel coordinate of a central point between the two eyes; and</p><p id="p-0047" num="0052">calculating the position of the two eyes based on the pixel coordinate of the central point of the two eyes.</p><p id="p-0048" num="0053">Determining a desired point on the user interface may comprise:</p><p id="p-0049" num="0054">calculating a position on the user interface based on the position of the portion of the finger and the position of the two eyes; and</p><p id="p-0050" num="0055">calculating the desired point, in the form of a pixel coordinate, on the user interface based on the position on the user interface.</p><p id="p-0051" num="0056">In accordance with a further aspect of the present disclosure, there is provided a system for interacting with a user interface of an electronic device, the system comprising:</p><p id="p-0052" num="0057">a first infrared device fixed in relation to the electronic device and configured to generate first infrared data of a first infrared area spaced from the user interface;</p><p id="p-0053" num="0058">a second infrared device fixed in relation to the electronic device and configured to generate second infrared data of a second infrared area spaced from the user interface and the first infrared area; and</p><p id="p-0054" num="0059">a controller operatively connected to the first infrared device and the second infrared device, the controller configured to:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0060">detect a first portion of a finger of a person in the first infrared area based on the first infrared data;</li>        <li id="ul0004-0002" num="0061">in response to the detection of the first portion of the finger, determine the position of the first portion of the finger in relation to the first infrared device based on the first infrared data;</li>        <li id="ul0004-0003" num="0062">detect a second portion of the finger in the second infrared area based on the second infrared data;</li>        <li id="ul0004-0004" num="0063">in response to the detection of the second portion of the finger, determine the position of the second portion of the finger in relation to the second infrared device based on the second infrared data;</li>        <li id="ul0004-0005" num="0064">determine a desired point on the user interface based on the position of the first portion of the finger and the position of the second portion of the finger; and</li>        <li id="ul0004-0006" num="0065">generate an instruction to interact with the user interface based on the desired point.</li>    </ul>    </li></ul></p><p id="p-0055" num="0066">The controller may be configured to determine the position of the first portion of the finger by:</p><p id="p-0056" num="0067">calculating a pixel coordinate of a point of the first portion of the finger based on the infrared data; and</p><p id="p-0057" num="0068">calculating the position of the first portion of the finger based on the pixel coordinate of the point of the first portion of the finger.</p><p id="p-0058" num="0069">The controller may be configured to determine the position of the second portion of the finger by:</p><p id="p-0059" num="0070">calculating a pixel coordinate of a point of the second portion of the finger based on the infrared data; and</p><p id="p-0060" num="0071">calculating the position of the second portion of the finger based on the pixel coordinate of the point of the second portion of the finger.</p><p id="p-0061" num="0072">The controller may be configured to determine a desired point on the user interface by:</p><p id="p-0062" num="0073">calculating a position on the user interface based on the position of the first portion of the finger and the position of the second portion of the finger; and</p><p id="p-0063" num="0074">calculating the desired point, in the form of a pixel coordinate, on the user interface based on the position on the user interface.</p><p id="p-0064" num="0075">Each of the first and second infrared devices may be an infrared touch frame.</p><p id="p-0065" num="0076">In accordance with another aspect of the present disclosure, there is provided a method of interacting with a user interface of an electronic device, the method comprising:</p><p id="p-0066" num="0077">detecting a first portion of a finger of a person in a first infrared area spaced from the user interface, based on infrared data generated by a first infrared device;</p><p id="p-0067" num="0078">in response to the detection of the first portion of the finger, determining the position of the first portion of the finger in relation to the first infrared device based on the first infrared data;</p><p id="p-0068" num="0079">detecting a second portion of a finger of a person in a second infrared area spaced from the user interface and the first infrared area, based on second infrared data generated by a second infrared device;</p><p id="p-0069" num="0080">in response to the detection of the second portion of the finger, determining the position of the second portion of the finger in relation to the second infrared device based on the second infrared data;</p><p id="p-0070" num="0081">determining a desired point on the user interface based on the position of the first portion and the position of the second portion; and</p><p id="p-0071" num="0082">generating an instruction to interact with the user interface based on the desired point.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0072" num="0083">Embodiments of the present disclosure will now be described hereinafter, by way of examples only, with reference to the accompanying drawings, in which:</p><p id="p-0073" num="0084"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a perspective view of an embodiment of a system fixedly attached to an electronic device in the form of a computer;</p><p id="p-0074" num="0085"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic illustration of the system of <figref idref="DRAWINGS">FIG. <b>1</b></figref> operatively connected to the electronic device;</p><p id="p-0075" num="0086"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow diagram of an embodiment of a method of interacting with a user interface of the electronic device using the system of <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0076" num="0087"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a perspective view of another embodiment of a system fixedly attached to an electronic device in the form of a computer;</p><p id="p-0077" num="0088"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic illustration of the system of <figref idref="DRAWINGS">FIG. <b>4</b></figref> operatively connected to the electronic device;</p><p id="p-0078" num="0089"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flow diagram of an embodiment of a method of interacting with a user interface of the electronic device using the system of <figref idref="DRAWINGS">FIG. <b>4</b></figref>;</p><p id="p-0079" num="0090"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic illustration of another embodiment of a system operatively connected to an electronic device;</p><p id="p-0080" num="0091"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flow diagram of an embodiment of a method of interacting with a user interface of the electronic device using the system of <figref idref="DRAWINGS">FIG. <b>7</b></figref>;</p><p id="p-0081" num="0092"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a perspective view of another embodiment of a system connected to an elevator push button;</p><p id="p-0082" num="0093"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a perspective view of another embodiment of a system connected to an ATM keypad; and</p><p id="p-0083" num="0094"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a perspective view of the system of <figref idref="DRAWINGS">FIG. <b>10</b></figref>, without the infrared device.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0084" num="0095"><figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> show an embodiment of a system <b>10</b> for interacting with a user interface <b>22</b> of an electronic device <b>20</b>. In this embodiment, the electronic device <b>20</b> is in the form of a computer with a forwardly-facing touch screen display <b>24</b>. The display <b>24</b> is substantially planar and has a maximum width W<sub>M </sub>along the x-axis and a maximum height H<sub>M </sub>along the y-axis. Further, in this embodiment, the display <b>24</b> has a maximum resolution (X<sub>MAX</sub>, Y<sub>MAX</sub>) and the user interface <b>22</b> is the display <b>24</b> showing a graphical user interface thereon. It will be appreciated that a user will be able to interact with the graphical user interface by physically pressing on a desired point on the display <b>24</b>.</p><p id="p-0085" num="0096">The system <b>10</b> comprises an infrared (IR) device <b>100</b> in the form of an IR touch frame. The IR device <b>100</b> is configured to be fixedly attached to the electronic device <b>20</b> so as to be disposed in front of the display <b>24</b> and not obscure the display <b>24</b> when viewed from the front.</p><p id="p-0086" num="0097">The IR device <b>100</b> comprises an IR sensor <b>106</b> with a set of IR transmitters and receivers. The IR sensor <b>106</b> is configured to generate IR data of a pre-defined two-dimensional IR area <b>108</b> with a boundary defined by the IR device <b>100</b>. The IR area <b>108</b> has a maximum width W<sub>S1 </sub>along the x-axis and a maximum height H<sub>S1 </sub>along the y-axis. In this embodiment, the width W<sub>S1 </sub>and the height H<sub>S1 </sub>are substantially the same as the width W<sub>M </sub>and the height H<sub>M </sub>of the display <b>24</b>, respectively. The IR area <b>108</b> also has a maximum resolution (X<sub>S1,MAX</sub>, Y<sub>S1,MAX</sub>). The IR area <b>108</b> is spaced from the display <b>24</b> at a pre-defined distance D and is substantially parallel thereto when the IR device <b>100</b> is attached to the electronic device <b>20</b>.</p><p id="p-0087" num="0098">Further, the system <b>10</b> comprises an image recording device <b>110</b> attached to the IR device <b>100</b>. In this embodiment, the image recording device <b>110</b> is in the form of a camera. The image recording device <b>110</b> is configured to generate image data of a pre-defined image area spaced forwardly of the user interface <b>22</b> and forwardly of the IR area <b>108</b> such that the IR area <b>108</b> is disposed between the image area and the display <b>24</b>. The image recording device <b>110</b> has an angle of view that defines a maximum width W<sub>E </sub>along the x-axis and a maximum height H<sub>E </sub>along the y-axis. The image data generated by the image recording device <b>110</b> includes two-dimensional images. Each image has a maximum resolution (X<sub>E.MAX</sub>, Y<sub>E.MAX</sub>).</p><p id="p-0088" num="0099">The system <b>10</b> further comprises a controller <b>104</b> mounted to the IR device <b>100</b> and operatively connected to the IR device <b>100</b> and the image recording device <b>110</b>. The controller <b>104</b> is configured to control various functions of the system <b>10</b> and may be in the form of a microcontroller, for example, having a processor and a memory. The memory is configured to store information and/or instructions for directing the processor. The processor is configured to execute instructions, such as those stored in the memory. In this embodiment, the microcontroller is an ARM11 microcontroller.</p><p id="p-0089" num="0100">In other embodiments, the microcontroller may have a storage device, such as a Hard Disk Drive (HDD).</p><p id="p-0090" num="0101">The system <b>10</b> also comprises an interfacing circuitry <b>112</b> operatively connected to the controller <b>104</b> and is configured to allow communication between the electronic device <b>20</b> and the controller <b>104</b>. In this embodiment, the interfacing circuitry <b>112</b> includes a USB connector (not shown) configured to be connected to a USB port of the electronic device <b>20</b> for data transmission therebetween. In some embodiments, the interfacing circuitry <b>112</b> may also be configured to receive, via the USB connector, power from the electronic device <b>20</b> to power electronic components of the system <b>10</b>. Moreover, the interfacing circuitry <b>112</b> comprises a mouse emulator module <b>114</b> for routing instructions from the controller <b>104</b> to the electronic device <b>20</b>.</p><p id="p-0091" num="0102">In alternative embodiments, the interfacing circuitry <b>112</b> may be wirelessly connected to the electronic device <b>20</b> by any wireless technology such as, for example, Bluetooth, Near-Field Communication (NFC), Radio-Frequency Identification (RFID), or Wi-Fi.</p><p id="p-0092" num="0103">The system <b>10</b> also comprises one or more speakers <b>116</b> and one or more light sources <b>118</b> operatively connected to the controller <b>104</b>. In this embodiment, the one or more light sources <b>118</b> comprises an LED array.</p><p id="p-0093" num="0104">With reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the controller <b>104</b> of the system <b>10</b> is configured to execute instructions to carry out the method operations described hereinafter. The method operations are commenced only when the USB connector of the interfacing circuitry <b>112</b> is connected to the USB port of the electronic device <b>20</b>. It will also be appreciated that the IR device <b>100</b> and the image recording device <b>110</b> will be fixed in relation to the electronic device <b>20</b> when the IR device <b>100</b> is attached to the electronic device <b>20</b>.</p><p id="p-0094" num="0105">The method begins at step <b>200</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in which the controller <b>104</b> detects at least a portion <b>12</b> of a finger of a person in the IR area <b>108</b> (see also <figref idref="DRAWINGS">FIG. <b>1</b></figref>) based on the IR data. In this regard, the controller <b>104</b> periodically obtains IR data from the IR sensor <b>106</b> and the IR data is recorded and stored in the memory. Additionally or optionally, the controller <b>104</b> may control the speaker <b>116</b> to produce a sound and/or the light source <b>118</b> to illuminate upon detection of the first portion <b>32</b> of the finger in the IR area <b>108</b>. The controller <b>104</b> then determines a pixel coordinate (PosX<sub>S1</sub>, PosY<sub>S1</sub>) of the location of the portion <b>12</b> of the finger in the IR area <b>108</b> based on the IR data. In some embodiments, the pixel coordinate (PosX<sub>S1</sub>, PosY<sub>S1</sub>) may be indicative of a central point of the portion <b>12</b> of the finger in the IR area <b>108</b>. In other embodiments, the pixel coordinate (PosX<sub>S1</sub>, PosY<sub>S1</sub>) may be indicative of any point of the portion <b>12</b> of the finger in the IR area <b>108</b>.</p><p id="p-0095" num="0106">In response to the detection of the portion <b>12</b> of the finger in the IR area <b>108</b>, the controller <b>104</b> then determines at step <b>202</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> the position (X<sub>S1</sub>, Y<sub>S1</sub>) of the portion <b>12</b> of the finger in relation to the IR device <b>100</b>, based on the pixel coordinate (PosX<sub>S1</sub>, PosY<sub>S1</sub>) derived from the IR data. In this embodiment, the position (X<sub>S1</sub>, Y<sub>S1</sub>) of the portion <b>12</b> of the finger is calculated as follows:</p><p id="p-0096" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>X</mi>      <mrow>       <mi>s</mi>       <mo>&#x2062;</mo>       <mn>1</mn>      </mrow>     </msub>     <mo>=</mo>     <mrow>      <mfrac>       <msub>        <mi>W</mi>        <mrow>         <mi>s</mi>         <mo>&#x2062;</mo>         <mn>1</mn>        </mrow>       </msub>       <msub>        <mi>X</mi>        <mrow>         <mrow>          <mi>S</mi>          <mo>&#x2062;</mo>          <mn>1</mn>         </mrow>         <mo>.</mo>         <mi>MAX</mi>        </mrow>       </msub>      </mfrac>      <mo>&#x2062;</mo>      <mrow>       <mo>(</mo>       <mrow>        <msub>         <mi>X</mi>         <mrow>          <mrow>           <mi>S</mi>           <mo>&#x2062;</mo>           <mn>1</mn>          </mrow>          <mo>.</mo>          <mi>MAX</mi>         </mrow>        </msub>        <mo>-</mo>        <msub>         <mi>PosX</mi>         <mrow>          <mi>S</mi>          <mo>&#x2062;</mo>          <mn>1</mn>         </mrow>        </msub>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>1</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00001-2" num="00001.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>Y</mi>      <mrow>       <mi>s</mi>       <mo>&#x2062;</mo>       <mn>1</mn>      </mrow>     </msub>     <mo>=</mo>     <mrow>      <mfrac>       <msub>        <mi>H</mi>        <mrow>         <mi>s</mi>         <mo>&#x2062;</mo>         <mn>1</mn>        </mrow>       </msub>       <msub>        <mi>Y</mi>        <mrow>         <mrow>          <mi>S</mi>          <mo>&#x2062;</mo>          <mn>1</mn>         </mrow>         <mo>.</mo>         <mi>MAX</mi>        </mrow>       </msub>      </mfrac>      <mo>&#x2062;</mo>      <mrow>       <mo>(</mo>       <mrow>        <msub>         <mi>Y</mi>         <mrow>          <mrow>           <mi>S</mi>           <mo>&#x2062;</mo>           <mn>1</mn>          </mrow>          <mo>.</mo>          <mi>MAX</mi>         </mrow>        </msub>        <mo>-</mo>        <msub>         <mi>PosY</mi>         <mrow>          <mi>S</mi>          <mo>&#x2062;</mo>          <mn>1</mn>         </mrow>        </msub>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0097" num="0107">At step <b>204</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the controller <b>104</b> then detects at least an eye of the person in the image area (see also <figref idref="DRAWINGS">FIG. <b>1</b></figref>) based on the image data. In this regard, the controller <b>104</b> periodically obtains image data from the image recording device <b>110</b> and the image data is recorded and stored in the memory. The controller <b>104</b> identifies the eye of the person in the obtained image data through known methods including, for example, neural network image processing and fuzzy classification algorithms. The controller <b>104</b> determines a pixel coordinate (PosX<sub>E</sub>, PosY<sub>E</sub>) of the location of the eye in the image area based on the obtained image data. In some embodiments, the pixel coordinate (PosX<sub>E</sub>, PosY<sub>E</sub>) may be indicative of a central point of the eye in the image area. In other embodiments, the controller <b>104</b> may detect both eyes of the person in the image area and the pixel coordinate (PosX<sub>E</sub>, PosY<sub>E</sub>) may be indicative of a central point between the eyes of the person. In further embodiments, the controller <b>104</b> may determine an average position of the eyes using facial recognition algorithms.</p><p id="p-0098" num="0108">In response to the detection of the eye of the person, the controller <b>104</b> determines at step <b>206</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> the position of the eye in relation to the image recording device <b>110</b>, based on the pixel coordinate (PosX<sub>E</sub>, PosY<sub>E</sub>) derived from the image data. In this embodiment, the position (X<sub>E</sub>, Y<sub>E</sub>) of the eye is calculated as follows:</p><p id="p-0099" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>X</mi>      <mi>E</mi>     </msub>     <mo>=</mo>     <mrow>      <msub>       <mi>W</mi>       <mi>E</mi>      </msub>      <mo>(</mo>      <mrow>       <mn>1</mn>       <mo>-</mo>       <mfrac>        <msub>         <mi>PosX</mi>         <mi>E</mi>        </msub>        <msub>         <mi>X</mi>         <mrow>          <mi>E</mi>          <mo>.</mo>          <mi>MAX</mi>         </mrow>        </msub>       </mfrac>      </mrow>      <mo>)</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00002-2" num="00002.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>Y</mi>      <mi>E</mi>     </msub>     <mo>=</mo>     <mrow>      <msub>       <mi>H</mi>       <mi>E</mi>      </msub>      <mo>(</mo>      <mrow>       <mn>1</mn>       <mo>-</mo>       <mfrac>        <msub>         <mi>PosY</mi>         <mi>E</mi>        </msub>        <msub>         <mi>Y</mi>         <mrow>          <mi>E</mi>          <mo>.</mo>          <mi>MAX</mi>         </mrow>        </msub>       </mfrac>      </mrow>      <mo>)</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>4</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0100" num="0109">Subsequently, at step <b>208</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the controller <b>104</b> determines a desired point <b>14</b> on the graphical user interface shown on the display <b>24</b> (see also <figref idref="DRAWINGS">FIG. <b>1</b></figref>). The desired point <b>14</b> is the point on the graphical user interface of the display <b>24</b> that the user is pointing to. In this embodiment, the controller <b>104</b> determines the desired point <b>14</b> by first calculating a position (X, Y) on the display <b>24</b> based on the position (X<sub>S1</sub>, Y<sub>S1</sub>) of the portion <b>12</b> of the finger and the position (X<sub>E</sub>, Y<sub>E</sub>) of the eye, as follows:</p><p id="p-0101" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>X</mi>     <mo>=</mo>     <mfrac>      <mrow>       <mrow>        <msub>         <mi>X</mi>         <mrow>          <mi>S</mi>          <mo>&#x2062;</mo>          <mn>1</mn>         </mrow>        </msub>        <mo>(</mo>        <mrow>         <mi>D</mi>         <mo>+</mo>         <mi>Z</mi>        </mrow>        <mo>)</mo>       </mrow>       <mo>-</mo>       <msub>        <mi>DX</mi>        <mi>E</mi>       </msub>      </mrow>      <mi>Z</mi>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>5</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00003-2" num="00003.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>Y</mi>     <mo>=</mo>     <mfrac>      <mrow>       <mrow>        <msub>         <mi>Y</mi>         <mrow>          <mi>S</mi>          <mo>&#x2062;</mo>          <mn>1</mn>         </mrow>        </msub>        <mo>(</mo>        <mrow>         <mi>D</mi>         <mo>+</mo>         <mi>Z</mi>        </mrow>        <mo>)</mo>       </mrow>       <mo>-</mo>       <msub>        <mi>DY</mi>        <mi>E</mi>       </msub>      </mrow>      <mi>Z</mi>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>6</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0102" num="0000">where:</p><p id="p-0103" num="0110">Z is the distance between the eye of the person and the IR area <b>108</b> and is determined using known face detection algorithms, which estimates the apparent size of the eye of the person.</p><p id="p-0104" num="0111">The controller <b>104</b> then calculates a desired point position (PosX, PosY) of the desired point <b>14</b> on the graphical user interface of the display <b>24</b>, in the form of a pixel coordinate, based on the position (X, Y), as follows:</p><p id="p-0105" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>PosX</mi>     <mo>=</mo>     <mrow>      <msub>       <mi>X</mi>       <mi>MAX</mi>      </msub>      <mo>(</mo>      <mrow>       <mn>1</mn>       <mo>-</mo>       <mfrac>        <mi>X</mi>        <msub>         <mi>W</mi>         <mi>M</mi>        </msub>       </mfrac>      </mrow>      <mo>)</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>7</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00004-2" num="00004.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>PosY</mi>     <mo>=</mo>     <mrow>      <msub>       <mi>X</mi>       <mi>MAX</mi>      </msub>      <mo>(</mo>      <mrow>       <mn>1</mn>       <mo>-</mo>       <mfrac>        <mi>Y</mi>        <msub>         <mi>H</mi>         <mi>M</mi>        </msub>       </mfrac>      </mrow>      <mo>)</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>8</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0106" num="0112">At step <b>210</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the controller <b>104</b> then generates an instruction signal based on the desired point position (PosX, PosY) of the desired point <b>14</b> and transmits the instruction signal to the electronic device <b>20</b>, via the mouse emulator module <b>114</b> of the interfacing circuitry <b>112</b>, to interact with the user interface <b>22</b> such that the desired point <b>14</b> is emulated on the graphical user interface. It will be appreciated that the electronic device <b>20</b> will emulate the desired point <b>14</b> on the graphical user interface through known methods based on the instruction signal received from the controller <b>104</b>.</p><p id="p-0107" num="0113"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows another embodiment of a system <b>30</b> for interacting with a user interface <b>22</b> of an electronic device <b>20</b>. The system <b>30</b> is substantially similar to the system <b>10</b> and like features have been indicated with like reference numerals. However, in this embodiment, the system <b>30</b> comprises a further IR device <b>300</b> in place of an image recording device. The IR device <b>300</b> is in the form of an IR touch frame attached to the IR device <b>100</b> and disposed rearwardly thereof. The IR device <b>300</b> is configured to be fixedly attached to the electronic device <b>20</b> so as to not obscure the display <b>24</b> when viewed from the front. It will be appreciated that in other embodiments the IR device <b>300</b> may be disposed forwardly of the IR device <b>100</b> and the IR device <b>100</b> may be configured to be fixedly attached to the electronic device <b>20</b> so as to not obscure the display <b>24</b> when viewed from the front.</p><p id="p-0108" num="0114">The IR device <b>300</b> is operatively connected to the controller <b>104</b> and comprises an IR sensor <b>302</b> with a set of IR transmitters and receivers. The IR sensor <b>302</b> is configured to generate IR data of a pre-defined two-dimensional IR area <b>304</b> with a boundary defined by the IR device <b>300</b>. The IR area has a maximum width W<sub>S2 </sub>along the x-axis and a maximum height H<sub>S2 </sub>along the y-axis that are substantially the same as the width W<sub>M </sub>and the height H<sub>M </sub>of the display <b>24</b>, respectively. The IR area <b>304</b> also has a maximum resolution (X<sub>S2.MAX</sub>, Y<sub>S2.MAX</sub>) that is substantially the same as the maximum resolution (X<sub>S1.MAX</sub>, Y<sub>S1.MAX</sub>) of the IR area <b>108</b>. The IR area <b>304</b> is spaced from the display <b>24</b> at a pre-defined distance D<sub>m </sub>and is substantially parallel thereto when the IR device <b>300</b> is attached to the electronic device <b>20</b>. The IR area <b>304</b> is also spaced from the IR area <b>108</b> at a pre-defined distance D<sub>f</sub>.</p><p id="p-0109" num="0115">With reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the controller <b>104</b> of the system <b>30</b> is configured to execute instructions to carry out the method operations described hereinafter. The method operations are commenced only when the USB connector of the interfacing circuitry <b>112</b> is connected to the USB port of the electronic device <b>20</b>. It will also be appreciated that the IR devices <b>100</b>, <b>300</b> will be fixed in relation to the electronic device <b>20</b> when the IR devices <b>100</b>, <b>300</b> are attached to the electronic device <b>20</b>.</p><p id="p-0110" num="0116">The method begins at step <b>400</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, in which the controller <b>104</b> detects at least a first portion <b>32</b> of a finger of a person in the IR area <b>108</b> (see also <figref idref="DRAWINGS">FIG. <b>4</b></figref>) based on the IR data. In this regard, the controller <b>104</b> periodically obtains IR data from the IR sensor <b>106</b> and the IR data is recorded and stored in the memory. Additionally or optionally, the controller <b>104</b> may control the speaker <b>116</b> to produce a sound and/or the light source <b>118</b> to illuminate upon detection of the first portion <b>32</b> of the finger in the IR area <b>108</b>. The controller <b>104</b> then determines a pixel coordinate (PosX<sub>S1</sub>, PosY<sub>S1</sub>) of the location of the first portion <b>32</b> of the finger in the IR area <b>108</b> based on the IR data. In some embodiments, the pixel coordinate (PosX<sub>S1</sub>, PosY<sub>S1</sub>) may be indicative of a central point of the first portion <b>32</b> of the finger in the IR area <b>108</b>. In other embodiments, the pixel coordinate (PosX<sub>S1</sub>, PosY<sub>S1</sub>) may be indicative of any point of the first portion <b>32</b> of the finger in the IR area <b>108</b>.</p><p id="p-0111" num="0117">In response to the detection of the first portion <b>32</b> of the finger in the IR area <b>108</b>, the controller <b>104</b> then determines at step <b>402</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref> the position (X<sub>S1</sub>, Y<sub>S1</sub>) of the first portion <b>32</b> of the finger in relation to the IR device <b>100</b>, based on the pixel coordinate (PosX<sub>S1</sub>, PosY<sub>S1</sub>) derived from the IR data. The position (X<sub>S1</sub>, Y<sub>S1</sub>) of the first portion <b>32</b> of the finger may be calculated in a similar manner to that described above at step <b>202</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0112" num="0118">At step <b>404</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the controller <b>104</b> then detects at least a second portion <b>34</b> of the finger in the IR area <b>304</b> based on the IR data. In this regard, the controller <b>104</b> periodically obtains IR data from the IR sensor <b>302</b> and the IR data is recorded and stored in the memory. Additionally or optionally, the controller <b>104</b> may control the speaker <b>116</b> to produce a sound and/or the light source <b>118</b> to illuminate upon detection of the second portion <b>34</b> of the finger in the IR area <b>304</b>. The controller <b>104</b> then determines a pixel coordinate (PosX<sub>S2</sub>, PosY<sub>S2</sub>) of the location of the second portion <b>34</b> of the finger in the IR area <b>304</b> based on the IR data. In some embodiments, the pixel coordinate (PosX<sub>S2</sub>, PosY<sub>S2</sub>) may be indicative of a central point of the second portion <b>34</b> of the finger in the IR area <b>304</b>. In other embodiments, the pixel coordinate (PosX<sub>S2</sub>, PosY<sub>S2</sub>) may be indicative of any point of the second portion <b>34</b> of the finger in the IR area <b>304</b>.</p><p id="p-0113" num="0119">In response to the detection of the second portion <b>34</b> of the finger in the IR area <b>304</b>, the controller <b>104</b> then determines at step <b>406</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref> the position (X<sub>S2</sub>, Y<sub>S2</sub>) of the second portion <b>34</b> of the finger in relation to the IR device <b>300</b>, based on the pixel coordinate (PosX<sub>S2</sub>, PosY<sub>S2</sub>) derived from the IR data. The position (X<sub>S2</sub>, Y<sub>S2</sub>) of the second portion <b>34</b> of the finger may be calculated in a similar manner to that described above at step <b>202</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0114" num="0120">Subsequently, at step <b>408</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the controller <b>104</b> determines a desired point <b>36</b> on the graphical user interface shown on the display <b>24</b> (see also <figref idref="DRAWINGS">FIG. <b>4</b></figref>). In this embodiment, the controller <b>104</b> determines the desired point <b>36</b> by first calculating a position (X, Y) on the display <b>24</b> based on the position (X<sub>S1</sub>, Y<sub>S1</sub>) of the first portion <b>32</b> of the finger and the position (X<sub>S2</sub>, Y<sub>S2</sub>) of the second portion <b>34</b> of the finger, as follows:</p><p id="p-0115" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>X</mi>     <mo>=</mo>     <mrow>      <msub>       <mi>X</mi>       <mrow>        <mi>S</mi>        <mo>&#x2062;</mo>        <mn>2</mn>       </mrow>      </msub>      <mo>-</mo>      <mrow>       <mfrac>        <msub>         <mi>D</mi>         <mi>m</mi>        </msub>        <msub>         <mi>D</mi>         <mi>f</mi>        </msub>       </mfrac>       <mo>&#x2062;</mo>       <mrow>        <mo>(</mo>        <mrow>         <msub>          <mi>X</mi>          <mrow>           <mi>S</mi>           <mo>&#x2062;</mo>           <mn>1</mn>          </mrow>         </msub>         <mo>-</mo>         <msub>          <mi>X</mi>          <mrow>           <mi>S</mi>           <mo>&#x2062;</mo>           <mn>2</mn>          </mrow>         </msub>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>9</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00005-2" num="00005.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>Y</mi>     <mo>=</mo>     <mrow>      <msub>       <mi>Y</mi>       <mrow>        <mi>S</mi>        <mo>&#x2062;</mo>        <mn>2</mn>       </mrow>      </msub>      <mo>-</mo>      <mrow>       <mfrac>        <msub>         <mi>D</mi>         <mi>m</mi>        </msub>        <msub>         <mi>D</mi>         <mi>f</mi>        </msub>       </mfrac>       <mo>&#x2062;</mo>       <mrow>        <mo>(</mo>        <mrow>         <msub>          <mi>Y</mi>          <mrow>           <mi>S</mi>           <mo>&#x2062;</mo>           <mn>1</mn>          </mrow>         </msub>         <mo>-</mo>         <msub>          <mi>Y</mi>          <mrow>           <mi>S</mi>           <mo>&#x2062;</mo>           <mn>2</mn>          </mrow>         </msub>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>10</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0116" num="0121">The controller <b>104</b> then calculates the desired point position (PosX, PosY) of the desired point <b>36</b> on the graphical user interface of the display <b>24</b>, in the form of a pixel coordinate, based on the position (X, Y), in a similar manner described at step <b>208</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. At step <b>410</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the controller <b>104</b> then generates an instruction signal based on the desired point position (PosX, PosY) of the desired point <b>36</b> and transmits the instruction signal to the electronic device <b>20</b>, via the mouse emulator module <b>114</b> of the interfacing circuitry <b>112</b>, to interact with the user interface <b>22</b> such that the desired point <b>36</b> is emulated on the graphical user interface. It will be appreciated that the electronic device <b>20</b> will emulate the desired point <b>36</b> on the graphical user interface through known methods based on the instruction signal received from the controller <b>104</b>.</p><p id="p-0117" num="0122"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a further embodiment of a system <b>40</b> for interacting with a user interface <b>22</b> of an electronic device <b>20</b>. The system <b>40</b> is a combination of the systems <b>10</b>, <b>30</b> which includes the image recording device <b>110</b> and the IR devices <b>100</b>, <b>300</b>, and like features have been indicated with like reference numerals.</p><p id="p-0118" num="0123">With reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the controller <b>104</b> of the system <b>40</b> is configured to execute instructions to carry out the method operations described hereinafter. The method operations are commenced only when the USB connector of the interfacing circuitry <b>112</b> is connected to the USB port of the electronic device <b>20</b>. It will also be appreciated that the IR devices <b>100</b>, <b>300</b> and the image recording device <b>110</b> will be fixed in relation to the electronic device <b>20</b> when the IR devices <b>100</b>, <b>300</b> are attached to the electronic device <b>20</b>.</p><p id="p-0119" num="0124">The method begins at step <b>500</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, in which the controller <b>104</b> detects at least a first portion <b>32</b> of a finger of a person in the IR area <b>108</b> based on the IR data. In this regard, the controller <b>104</b> periodically obtains IR data from the IR sensor <b>106</b> and the IR data is recorded and stored in the memory. Additionally or optionally, the controller <b>104</b> may control the speaker <b>116</b> to produce a sound and/or the light source <b>118</b> to illuminate upon detection of the first portion <b>32</b> of the finger in the IR area <b>108</b>. The controller <b>104</b> then determines a pixel coordinate (PosX<sub>S1</sub>, PosY<sub>S1</sub>) of the location of the first portion <b>32</b> of the finger in the IR area <b>10</b> based on the IR data.</p><p id="p-0120" num="0125">In response to the detection of the first portion <b>32</b> of the finger in the IR area <b>108</b>, the controller <b>104</b> then determines at step <b>502</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref> the position (X<sub>S1</sub>, Y<sub>S1</sub>) of the first portion <b>32</b> of the finger in relation to the IR device <b>100</b>, based on the pixel coordinate (PosX<sub>S1</sub>, PosY<sub>S1</sub>) derived from the IR data. The position (X<sub>S1</sub>, Y<sub>S1</sub>) of the first portion <b>32</b> of the finger may be calculated in a similar manner to that described above at step <b>202</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0121" num="0126">In response to the determination of the position (X<sub>S1</sub>, Y<sub>S1</sub>) of the first portion <b>32</b> of the finger in relation to the IR device <b>100</b>, the controller <b>104</b> then detects at least an eye of the person in the image area based on the image data. In this regard, the controller <b>104</b> periodically obtains image data from the image recording device <b>110</b> and the image data is recorded and stored in the memory.</p><p id="p-0122" num="0127">If, at step <b>504</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the eye of the person is identified, the controller <b>104</b> then determines a pixel coordinate (PosX<sub>E</sub>, PosY<sub>E</sub>) of the location of the eye in the image area based on the obtained image data. In response to the detection of the eye of the person, the controller <b>104</b> determines at step <b>506</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref> the position of the eye in relation to the image recording device <b>110</b>, based on the pixel coordinate (PosX<sub>E</sub>, PosY<sub>E</sub>) derived from the image data. The position of the eye may be calculated in a similar manner described above at step <b>206</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0123" num="0128">In response to the determination of the position of the eye, the controller <b>104</b> then detects at least a second portion <b>34</b> of the finger in the IR area <b>304</b> based on the IR data. In this regard, the controller <b>104</b> periodically obtains IR data from the IR sensor <b>302</b> and the IR data is recorded and stored in the memory. Additionally or optionally, the controller <b>104</b> may control the speaker <b>116</b> to produce a sound and/or the light source <b>118</b> to illuminate upon detection of the second portion <b>34</b> of the finger in the IR area <b>304</b>.</p><p id="p-0124" num="0129">If, at step <b>508</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the second portion <b>34</b> of the finger in the IR area is detected, the controller <b>104</b> then determines a pixel coordinate (PosX<sub>S2</sub>, PosY<sub>S2</sub>) of the location of the second portion <b>34</b> of the finger in the IR area <b>304</b> based on the IR data. In response to the detection of the second portion <b>34</b> of the finger in the IR area <b>304</b>, the controller <b>104</b> then determines at step <b>510</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref> the position (X<sub>S2</sub>, Y<sub>S2</sub>) of the second portion <b>34</b> of the finger in relation to the IR device <b>300</b>, based on the pixel coordinate (PosX<sub>S2</sub>, PosY<sub>S2</sub>) derived from the IR data, in a similar manner to that described above.</p><p id="p-0125" num="0130">Subsequently, at step <b>512</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the controller <b>104</b> determines a desired point on the graphical user interface shown on the display <b>24</b> by calculating a position (X, Y) on the display <b>24</b> based on the position (X<sub>S1</sub>, Y<sub>S1</sub>) of the first portion <b>32</b> of the finger, the position (X<sub>S2</sub>, Y<sub>S2</sub>) of the second portion <b>34</b> of the finger, and the position (X<sub>E</sub>, Y<sub>E</sub>) of the eye, and then calculating the desired point position (PosX, PosY) of the desired point on the graphical user interface of the display, in the form of a pixel coordinate, based on the position (X, Y), in a similar manner described above. At step <b>514</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the controller <b>104</b> then generates an instruction signal based on the desired point position (PosX, PosY) of the desired point and transmits the instruction signal to the electronic device <b>20</b>, via the mouse emulator module <b>114</b> of the interfacing circuitry <b>112</b>, to interact with the user interface <b>22</b> such that the desired point is emulated on the graphical user interface. It will be appreciated that the electronic device <b>20</b> will emulate the desired point on the graphical user interface through known methods based on the instruction signal received from the controller <b>104</b>.</p><p id="p-0126" num="0131">If, at step <b>508</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the second portion <b>34</b> of the finger in the IR area <b>304</b> is not detected, the controller <b>104</b> then determines at step <b>512</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref> a desired point on the graphical user interface shown on the display <b>24</b> by calculating a position (X, Y) on the user interface <b>22</b> based on the position (X<sub>S1</sub>, Y<sub>S1</sub>) of the portion <b>12</b> of the finger and the position (X<sub>E</sub>, Y<sub>E</sub>) of the eye, and then calculating the desired point position (PosX, PosY) of the desired point on the graphical user interface, in the form of a pixel coordinate, based on the position (X, Y), in a similar manner described above. At step <b>514</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the controller <b>104</b> then generates an instruction signal based on the desired point position (PosX, PosY) of the desired point and transmits the instruction signal to the electronic device <b>20</b>, via the mouse emulator module <b>114</b> of the interfacing circuitry <b>112</b>, to interact with the user interface <b>22</b> such that the desired point is emulated on the graphical user interface. It will be appreciated that the electronic device <b>20</b> will emulate the desired point on the graphical user interface through known methods based on the instruction signal received from the controller <b>104</b>.</p><p id="p-0127" num="0132">If, at step <b>504</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the eye of the person is not identified, the controller <b>104</b> then detects at least a second portion <b>34</b> of the finger in the IR area <b>304</b> based on the IR data. In this regard, the controller <b>104</b> periodically obtains IR data from the IR sensor <b>302</b> and the IR data is recorded and stored in the memory. Additionally or optionally, the controller <b>104</b> may control the speaker <b>116</b> to produce a sound and/or the light source <b>118</b> to illuminate upon detection of the second portion <b>34</b> of the finger in the IR area <b>304</b>.</p><p id="p-0128" num="0133">If, at step <b>516</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the second portion of the finger in the IR area is detected, the controller <b>104</b> then determines a pixel coordinate (PosX<sub>S2</sub>, PosY<sub>S2</sub>) of the location of the second portion <b>34</b> of the finger in the IR area <b>304</b> based on the IR data. In response to the detection of the second portion <b>34</b> of the finger in the IR area <b>304</b>, the controller <b>104</b> then determines at step <b>510</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref> the position (X<sub>S2</sub>, Y<sub>S2</sub>) of the second portion <b>34</b> of the finger in relation to the IR device <b>300</b>, based on the pixel coordinate (PosX<sub>S2</sub>, PosY<sub>S2</sub>) derived from the IR data, in a similar manner to that described above.</p><p id="p-0129" num="0134">Subsequently, at step <b>512</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the controller <b>104</b> determines a desired point on the graphical user interface shown on the display <b>24</b> by calculating a position (X, Y) on the display <b>24</b> based on the position (X<sub>S1</sub>, Y<sub>S1</sub>) of the first portion <b>32</b> of the finger and the position (X<sub>S2</sub>, Y<sub>S2</sub>) of the second portion <b>34</b> of the finger, and then calculating the desired point position (PosX, PosY) of the desired point on the graphical user interface of the display <b>24</b>, in the form of a pixel coordinate, based on the position (X, Y), in a similar manner described above. At step <b>514</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the controller <b>104</b> then generates an instruction signal based on the desired point position (PosX, PosY) of the desired point and transmits the instruction signal to the electronic device <b>20</b>, via the mouse emulator module <b>114</b> of the interfacing circuitry <b>112</b>, to interact with the user interface <b>22</b> such that the desired point is emulated on the graphical user interface. It will be appreciated that the electronic device <b>20</b> will emulate the desired point on the graphical user interface through known methods based on the instruction signal received from the controller <b>104</b>.</p><p id="p-0130" num="0135">If, at step <b>516</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the second portion <b>34</b> of the finger in the IR area <b>304</b> is not detected, the controller <b>104</b> then reverts to step <b>504</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref> to detect at least an eye of the person in the image area based on the image data.</p><p id="p-0131" num="0136">In other embodiments, the electronic device may be in the form of a laptop computer, a tablet device, a point-of-sale device, a self-service kiosk, an automated teller machine (ATM), self-checkout device, a vending machine, an EFPOS device, or any other device of the type that is configured to allow a user to interact therewith through a touch screen user interface.</p><p id="p-0132" num="0137">In other embodiments, the electronic device may be in the form of a pedestrian crossing push button, an elevator push button, or any other electronic device of the type that is configured to allow a user to interact therewith through a push button user interface. <figref idref="DRAWINGS">FIG. <b>9</b></figref> shows an embodiment of a system <b>50</b> connected to an elevator push button. The system <b>50</b> may have similar components and functionality as the system <b>10</b> and like features have been indicated with like reference numerals. However, in this embodiment, the interfacing circuitry <b>112</b> of the system <b>50</b> is connect directly to the circuitry <b>62</b> of a push button panel <b>60</b> via a relay board <b>64</b> for routing instructions from the controller <b>104</b> to the push button panel <b>60</b>. It will be appreciated that the controller <b>104</b> of the system <b>50</b> may be configured to execute instructions to carry out any of the above method operations in a similar manner. In this regard, the controller <b>104</b> determines a desired point corresponding to a button <b>66</b> on the push button panel <b>60</b> that the user is pointing to, and generates and transmits an instruction, in the form of an electrical signal, to the push button panel <b>60</b> to interact therewith such that the desired point is registered on the push button panel <b>60</b>. In alternative embodiments, the system <b>50</b> may have similar components and functionality as the system <b>30</b> or the system <b>40</b> and the controller <b>104</b> of the system <b>50</b> may be configured to execute instructions to carry out any of the above method operations in a similar manner.</p><p id="p-0133" num="0138">In some embodiments, the electronic device may be of the type that does not allow communication with the controller of the system, either through a USB connection or via direct electrical connection as described above. However, the electronic device may be provided with a keypad for allowing user interaction therewith, such as a keypad of an ATM, for example.</p><p id="p-0134" num="0139"><figref idref="DRAWINGS">FIGS. <b>10</b> and <b>11</b></figref> show an embodiment of a system <b>70</b> for interacting with a keypad <b>80</b> of an ATM. The keypad <b>80</b> has a set of push buttons <b>82</b>. The system <b>70</b> may have similar components and functionality as the system <b>10</b> and like features have been indicated with like reference numerals. However, in this embodiment, the system <b>70</b> comprises an actuating interface <b>600</b> disposed rearwardly of the IR area <b>108</b> of the IR frame <b>100</b> and configured to be fixedly attached to the keypad <b>80</b>. The actuating interface <b>600</b> comprises a panel <b>602</b> having a front surface <b>604</b>, and a label <b>606</b> attached to the front surface <b>604</b>. The label <b>606</b> includes an image of a set of buttons <b>612</b> that substantially correspond to the appearance of the push buttons <b>82</b> of the keypad <b>80</b> and directly overlap with the push buttons <b>82</b> when the actuating interface <b>600</b> is attached to the keypad <b>80</b>. The actuating interface <b>600</b> also comprises a set of solenoid actuators <b>608</b> operatively connected to the controller <b>104</b> of the system <b>70</b> and is configured to interact with the push buttons <b>82</b> of the keypad <b>80</b>, as shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. Each of the solenoid actuators <b>608</b> is disposed rearwardly of a respective button <b>612</b> of the label <b>606</b> so as to be operatively associated therewith. Each of the solenoid actuators <b>608</b> has a moveable piston <b>610</b> that is configured to push against the push buttons <b>82</b> of the keypad <b>80</b> when the actuating interface <b>600</b> is attached to the keypad <b>80</b>.</p><p id="p-0135" num="0140">It will be appreciated that the controller <b>104</b> of the system <b>70</b> may be configured to execute instructions to carry out any of the above method operations in a similar manner. In this regard, the controller <b>104</b> determines a desired point corresponding to a button <b>612</b> on the label <b>606</b> and a respective push button <b>82</b> of the keypad <b>80</b>, and generates and transmits an instruction, in the form of an electrical signal, to the solenoid actuator <b>608</b> operatively associated with the button <b>612</b> to move the piston <b>610</b> and push against the corresponding push button <b>82</b> of the keypad <b>80</b>. In alternative embodiments, the system <b>70</b> may have similar components and functionality as the system <b>30</b> or the system <b>40</b> and the controller <b>104</b> of the system <b>70</b> may be configured to execute instructions to carry out any of the above method operations in a similar manner.</p><p id="p-0136" num="0141">The above described embodiments have numerous advantages. For example, the systems <b>10</b>, <b>30</b>, <b>40</b>, <b>50</b> <b>70</b> eliminate the need for a user to touch the display <b>24</b> of the electronic device <b>20</b> during use. This significantly reduces the risk of germs and other causes of infection being placed on the display <b>24</b> of the electronic device <b>20</b>, and the risk of those germs and other causes of infection being transmitted by subsequent users. Further, by eliminating the need for users to contact the display <b>24</b> of the electronic device <b>20</b>, users do not leave marks or fingerprints on the display <b>24</b>, which may otherwise be exploited by would-be perpetrators. The systems are also easy to use as a user simply points to a desired point on the user interface <b>22</b> without any significant change in the normal behaviour of the user. Moreover, the systems <b>10</b>, <b>30</b>, <b>40</b>, <b>50</b> <b>70</b> are configured to be easily connected to the electronic device <b>20</b> and may be adapted for a wide range of electronic devices <b>20</b>. The systems <b>10</b>, <b>30</b>, <b>40</b>, <b>50</b> <b>70</b> are also designed such that they occupy minimal real-estate when attached to the electronic device <b>20</b>, thus not affecting the user's normal view of the display <b>24</b> of the electronic device <b>20</b>.</p><p id="p-0137" num="0142">It will be appreciated by persons skilled in the art that numerous variations and/or modifications may be made to the above-described embodiments, without departing from the broad general scope of the present disclosure. The present embodiments are, therefore, to be considered in all respects as illustrative and not restrictive.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001 MATH-US-00001-2" nb-file="US20230004255A1-20230105-M00001.NB"><img id="EMI-M00001" he="13.04mm" wi="76.20mm" file="US20230004255A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002 MATH-US-00002-2" nb-file="US20230004255A1-20230105-M00002.NB"><img id="EMI-M00002" he="13.04mm" wi="76.20mm" file="US20230004255A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003 MATH-US-00003-2" nb-file="US20230004255A1-20230105-M00003.NB"><img id="EMI-M00003" he="12.02mm" wi="76.20mm" file="US20230004255A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004 MATH-US-00004-2" nb-file="US20230004255A1-20230105-M00004.NB"><img id="EMI-M00004" he="12.70mm" wi="76.20mm" file="US20230004255A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005 MATH-US-00005-2" nb-file="US20230004255A1-20230105-M00005.NB"><img id="EMI-M00005" he="13.72mm" wi="76.20mm" file="US20230004255A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system for interacting with a user interface of an electronic device, the system comprising:<claim-text>an image recording device fixed in relation to the electronic device and configured to generate image data of an image area spaced from the user interface;</claim-text><claim-text>an infrared device fixed in relation to the electronic device and configured to generate infrared data of an infrared area spaced from the user interface; and</claim-text><claim-text>a controller operatively connected to the infrared device and the image recording device, the controller configured to:<claim-text>detect at least a portion of a finger of a person in the infrared area based on the infrared data;</claim-text><claim-text>in response to the detection of the portion of the finger, determine the position of the portion of the finger in relation to the infrared device based on the infrared data;</claim-text><claim-text>detect one or two eyes of the person in the image area based on the image data;</claim-text><claim-text>in response to the detection of the one or two eyes of the person, determine the position of the one or two eyes in relation to the image recording device based on the image data; and</claim-text><claim-text>determine a desired point on the user interface based on the position of the portion of the finger and the position of the one or two eyes; and</claim-text><claim-text>generate an instruction to interact with the user interface based on the desired point.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the controller is configured to determine the position of the portion of the finger by:<claim-text>calculating a pixel coordinate of a point of the portion of the finger based on the infrared data; and</claim-text><claim-text>calculating the position of the portion of the finger based on the pixel coordinate of the point of the portion of the finger.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the controller is configured to determine the position of the one eye by:<claim-text>calculating a pixel coordinate of a central point of the one eye; and</claim-text><claim-text>calculating the position of the one eye based on the pixel coordinate of the central point of the one eye.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the controller is configured to determine a desired point on the user interface by:<claim-text>calculating a position on the user interface based on the position of the portion of the finger and the position of the one eye; and</claim-text><claim-text>calculating the desired point, in the form of a pixel coordinate, on the user interface based on the position on the user interface.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the controller is configured to determine the position of the two eyes by:<claim-text>calculating a pixel coordinate of a central point between the two eyes; and</claim-text><claim-text>calculating the position of the two eyes based on the pixel coordinate of the central point of the two eyes.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the controller is configured to determine a desired point on the user interface by:<claim-text>calculating a position on the user interface based on the position of the portion of the finger and the position of the two eyes; and</claim-text><claim-text>calculating the desired point, in the form of a pixel coordinate, on the user interface based on the position on the user interface.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the image recording device is a camera.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the infrared device is an infrared touch frame.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A method of interacting with a user interface of an electronic device, the method comprising:<claim-text>detecting at least a portion of a finger of a person in an infrared area spaced from the user interface, based on infrared data generated by an infrared device;</claim-text><claim-text>in response to the detection of the portion of the finger, determining the position of the portion of the finger in relation to the infrared device based on the infrared data;</claim-text><claim-text>detecting one or two eyes of the person in an image area spaced from the user interface, based on image data generated by an image recording device;</claim-text><claim-text>in response to the detection of the one or two eyes of the person, determining the position of the one or two eyes in relation to the image recording device based on the image data;</claim-text><claim-text>determining a desired point on the user interface based on the position of the portion of the finger and the position of the one or two eyes; and</claim-text><claim-text>generating an instruction to interact with the user interface based on the desired point.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein determining the position of the portion of the finger comprises:<claim-text>calculating a pixel coordinate of a point of the portion of the finger based on the infrared data; and</claim-text><claim-text>calculating the position of the portion of the finger based on the pixel coordinate of the point of the portion of the finger.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein determining the position of the one eye comprises:<claim-text>calculating a pixel coordinate of a central point of the one eye; and</claim-text><claim-text>calculating the position of the one eye based on the pixel coordinate of the central point of the one eye.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein determining a desired point on the user interface comprises:<claim-text>calculating a position on the user interface based on the position of the portion of the finger and the position of the one eye; and</claim-text><claim-text>calculating the desired point, in the form of a pixel coordinate, on the user interface based on the position on the user interface.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein determining the position of the two eyes comprises:<claim-text>calculating a pixel coordinate of a central point between the two eyes; and</claim-text><claim-text>calculating the position of the two eyes based on the pixel coordinate of the central point of the two eyes.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein determining a desired point on the user interface comprises:<claim-text>calculating a position on the user interface based on the position of the portion of the finger and the position of the two eyes; and</claim-text><claim-text>calculating the desired point, in the form of a pixel coordinate, on the user interface based on the position on the user interface.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A system for interacting with a user interface of an electronic device, the system comprising:<claim-text>a first infrared device fixed in relation to the electronic device and configured to generate first infrared data of a first infrared area spaced from the user interface;</claim-text><claim-text>a second infrared device fixed in relation to the electronic device and configured to generate second infrared data of a second infrared area spaced from the user interface and the first infrared area; and</claim-text><claim-text>a controller operatively connected to the first infrared device and the second infrared device, the controller configured to:<claim-text>detect a first portion of a finger of a person in the first infrared area based on the first infrared data;</claim-text><claim-text>in response to the detection of the first portion of the finger, determine the position of the first portion of the finger in relation to the first infrared device based on the first infrared data;</claim-text><claim-text>detect a second portion of the finger in the second infrared area based on the second infrared data;</claim-text><claim-text>in response to the detection of the second portion of the finger, determine the position of the second portion of the finger in relation to the second infrared device based on the second infrared data;</claim-text><claim-text>determine a desired point on the user interface based on the position of the first portion of the finger and the position of the second portion of the finger; and</claim-text><claim-text>generate an instruction to interact with the user interface based on the desired point.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the controller is configured to determine the position of the first portion of the finger by:<claim-text>calculating a pixel coordinate of a point of the first portion of the finger based on the infrared data; and</claim-text><claim-text>calculating the position of the first portion of the finger based on the pixel coordinate of the point of the first portion of the finger.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the controller is configured to determine the position of the second portion of the finger by:<claim-text>calculating a pixel coordinate of a point of the second portion of the finger based on the infrared data; and</claim-text><claim-text>calculating the position of the second portion of the finger based on the pixel coordinate of the point of the second portion of the finger.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the controller is configured to determine a desired point on the user interface by:<claim-text>calculating a position on the user interface based on the position of the first portion of the finger and the position of the second portion of the finger; and</claim-text><claim-text>calculating the desired point, in the form of a pixel coordinate, on the user interface based on the position on the user interface.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein each of the first and second infrared devices is an infrared touch frame.</claim-text></claim></claims></us-patent-application>