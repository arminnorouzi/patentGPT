<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005144A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005144</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17939040</doc-number><date>20220907</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>KR</country><doc-number>10-2021-0085542</doc-number><date>20210630</date></priority-claim><priority-claim sequence="02" kind="national"><country>KR</country><doc-number>10-2022-0079770</doc-number><date>20220629</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>14</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>16</class><subclass>H</subclass><main-group>50</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0012</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>4824</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>4227</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>4878</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>445</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>14</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180101</date></cpc-version-indicator><section>G</section><class>16</class><subclass>H</subclass><main-group>50</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30201</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30041</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20132</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20021</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e79">METHOD FOR HOSPITAL VISIT GUIDANCE FOR MEDICAL TREATMENT FOR ACTIVE THYROID EYE DISEASE, AND SYSTEM FOR PERFORMING SAME</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/KR2022/009356</doc-number><date>20220629</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17939040</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>THYROSCOPE INC.</orgname><address><city>Ulsan</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>SHIN</last-name><first-name>Kyubo</first-name><address><city>Ulsan</city><country>KR</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>PARK</last-name><first-name>Jaemin</first-name><address><city>Busan</city><country>KR</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>KIM</last-name><first-name>Jongchan</first-name><address><city>Ulsan</city><country>KR</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>THYROSCOPE INC.</orgname><role>03</role><address><city>Ulsan</city><country>KR</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">According to the present application, a computer-implemented method of predicting thyroid eye disease is disclosed. The method comprising: preparing a conjunctival hyperemia prediction model, a conjunctival edema prediction model, a lacrimal edema prediction model, an eyelid redness prediction model, and an eyelid edema prediction model, obtaining a facial image of an object, obtaining a first processed image and a second processed image from the facial image, wherein the first processed image is different from the second processed image, obtaining predicted values for each of a conjunctival hyperemia, a conjunctival edema and a lacrimal edema by applying the first processed image to the conjunctival hyperemia prediction model, the conjunctival edema prediction model, and the lacrimal edema prediction model, and obtaining predicted values for each of an eyelid redness and an eyelid edema by applying the second processed image to the eyelid redness prediction model and the eyelid edema prediction model.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="94.57mm" wi="84.67mm" file="US20230005144A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="127.51mm" wi="91.10mm" file="US20230005144A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="106.26mm" wi="57.57mm" file="US20230005144A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="84.58mm" wi="57.57mm" file="US20230005144A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="97.96mm" wi="114.81mm" file="US20230005144A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="61.72mm" wi="71.54mm" file="US20230005144A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="61.72mm" wi="71.54mm" file="US20230005144A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="61.72mm" wi="71.54mm" file="US20230005144A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="61.72mm" wi="71.54mm" file="US20230005144A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="183.98mm" wi="99.31mm" file="US20230005144A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="179.83mm" wi="98.04mm" file="US20230005144A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="103.97mm" wi="130.56mm" file="US20230005144A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="113.37mm" wi="130.22mm" file="US20230005144A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="45.30mm" wi="129.20mm" file="US20230005144A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="120.57mm" wi="143.00mm" file="US20230005144A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="128.61mm" wi="77.22mm" file="US20230005144A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="134.28mm" wi="77.30mm" file="US20230005144A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="132.67mm" wi="77.55mm" file="US20230005144A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="164.08mm" wi="124.04mm" file="US20230005144A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="73.07mm" wi="104.90mm" file="US20230005144A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="123.87mm" wi="118.79mm" file="US20230005144A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="115.15mm" wi="111.42mm" file="US20230005144A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="117.86mm" wi="116.92mm" file="US20230005144A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="109.22mm" wi="86.70mm" file="US20230005144A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="104.22mm" wi="91.95mm" file="US20230005144A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="104.22mm" wi="84.67mm" file="US20230005144A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="104.22mm" wi="83.82mm" file="US20230005144A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="104.22mm" wi="82.47mm" file="US20230005144A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="135.04mm" wi="119.63mm" file="US20230005144A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="166.54mm" wi="100.92mm" file="US20230005144A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of International Application No. PCT/KR2022/009356 filed on Jun. 29, 2022, which claims priority to Korean Patent Application No. 10-20201-0085542 filed on Jun. 30, 2021 and Korean Patent Application No. 10-2022-0079770 filed on Jun. 29, 2022, the entire contents of which are herein incorporated by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to a method for hospital visit guidance for medical treatment for active thyroid eye disease, and a system for performing the method.</p><heading id="h-0003" level="1">BACKGROUND ART</heading><p id="p-0004" num="0003">An eye disease is a disease that occurs in the eyeball and surrounding parts. Many people in the world have been affected with eye diseases, and in severe cases, eye diseases cause great inconvenience in life, such as damage to eyesight, so it is necessary to monitor the occurrence or extent of the eye diseases.</p><p id="p-0005" num="0004">In the meantime, an eye disease may be one of several complications caused by other diseases. For example, thyroid eye disease is a complication caused by thyroid dysfunction.</p><p id="p-0006" num="0005">When thyroid eye disease becomes worse, the eyeball protrudes and cannot be treated without surgery. Therefore, early diagnosis of thyroid eye disease is very important for the treatment of thyroid eye disease. However, it is difficult to diagnose thyroid eye disease early because the disease does not show clear prognostic symptoms. In the medical community, efforts have been made to diagnose thyroid eye disease early through an evaluation method with the clinical activity score (CAS), which was proposed in 1989.</p><p id="p-0007" num="0006">In determining the clinical activity score for thyroid eye disease, a total of seven items are considered, and the seven items are 1) spontaneous retrobulbar pain, 2) pain on an attempted upward or downward gaze, 3) redness of an eyelid, 4) redness of a conjunctiva, 5) swelling of an eyelid, 6) swelling of a conjunctiva, and 7) swelling of a lacrimal caruncle.</p><p id="p-0008" num="0007">In order to determine a clinical activity score, it is essential that an individual visits a hospital or clinic in person and the doctor performs a medical examination through interview and observation with the naked eye. For example, spontaneous retrobulbar pain and pain on an attempted upward or downward gaze can be checked through interview by a doctor, and redness of an eyelid, redness of a conjunctiva, swelling of an eyelid, swelling of a conjunctiva, and swelling of a lacrimal caruncle can be checked by a doctor's observation with the naked eye. The doctor's medical examination with the naked eye and interview method for determining a clinical activity score require a hospital visit by a patient in person for a diagnosis of thyroid eye disease, as a precondition, so it is difficult to diagnose thyroid eye disease early.</p><p id="p-0009" num="0008">Accordingly, it is desired to develop a method of enabling individuals to recognize a risk of eye disease more easily and quickly without a hospital visit in person so that continuous monitoring can be performed, and of informing a patient of a risk of eye disease to induce the patient to visit the hospital when necessary.</p><heading id="h-0004" level="1">DISCLOSURE</heading><heading id="h-0005" level="1">Technical Problem</heading><p id="p-0010" num="0009">The disclosure in the present application is directed to providing a learning model used in predicting a clinical activity score for thyroid eye disease by using an image that is obtained with a digital camera that ordinary people can use rather than a professional medical diagnostic device.</p><p id="p-0011" num="0010">In addition, the disclosure in the present application is directed to providing a method and a system for enabling ordinary people to continuously monitor a clinical activity score for thyroid eye disease without a doctor's help and a hospital visit in person.</p><p id="p-0012" num="0011">In addition, the disclosure in the present application is directed to providing a method of recommending a hospital visit for medical treatment for active thyroid eye disease according to a result of monitoring a clinical activity score, and a system for performing the method.</p><p id="p-0013" num="0012">Technical problems to be solved by the present application are not limited to the aforementioned technical problems and other technical problems which are not mentioned will be clearly understood by those skilled in the art from the present specification and the accompanying drawings.</p><heading id="h-0006" level="1">Technical Solution</heading><p id="p-0014" num="0013">According to one aspect of the present application, a computer-implemented method of predicting thyroid eye disease is disclosed. The method comprising: preparing a conjunctival hyperemia prediction model, a conjunctival edema prediction model, a lacrimal edema prediction model, an eyelid redness prediction model, and an eyelid edema prediction model, obtaining a facial image of an object, obtaining a first processed image and a second processed image from the facial image, wherein the first processed image is different from the second processed image, obtaining predicted values for each of a conjunctival hyperemia, a conjunctival edema and a lacrimal edema by applying the first processed image to the conjunctival hyperemia prediction model, the conjunctival edema prediction model, and the lacrimal edema prediction model, obtaining predicted values for each of an eyelid redness and an eyelid edema by applying the second processed image to the eyelid redness prediction model and the eyelid edema prediction model, and determining a possibility of the object having thyroid eye disease based on the predicted values for the conjunctival hyperemia, the conjunctival edema, the lacrimal edema, the eyelid redness, and the eyelid edema. Wherein the first processed image is an image masked a region corresponding an inner of an outline of an iris and a region corresponding an outer of an outline of an eye and cropped along a first region comprising the outline of the eye based on position information of pixels corresponding the outline of the iris comprised in the eye and position information of pixels corresponding the outline of the eye, and wherein the second processed image is an image cropped along a second region that is wider than the first region based on position information of pixels corresponding the outline of the iris comprised in the eye and position information of pixels corresponding the outline of the eye.</p><p id="p-0015" num="0014">In some embodiments, wherein the position information of pixels corresponding the outline of the iris comprised in the eye and the position information of pixels corresponding the outline of the eye are obtained by segmentation model.</p><p id="p-0016" num="0015">In some embodiments, wherein the first processed image comprises a first processed left eye image and a first processed right eye image, and wherein the second processed image comprises a second processed left eye image and a second processed right eye image.</p><p id="p-0017" num="0016">In some embodiments, wherein the conjunctival hyperemia prediction model comprises a left eye conjunctival hyperemia prediction model and a right eye conjunctival hyperemia prediction model, wherein the conjunctival edema prediction model comprises a left eye conjunctival edema prediction model and a right eye conjunctival edema prediction model, wherein the lacrimal edema prediction model comprises a left eye lacrimal edema prediction model and a right eye lacrimal edema prediction model, wherein the eyelid redness prediction model comprises a left eyelid redness prediction model and a right eye eyelid redness prediction model, and wherein the eyelid edema prediction model comprises a left eye eyelid edema prediction model and a right eye eyelid edema prediction model.</p><p id="p-0018" num="0017">In some embodiments, wherein the predicted value for the conjunctival hyperemia is determined based on a result obtained by inputting the first processed left eye image to the left eye conjunctival hyperemia prediction model and a result obtained by inputting the first processed right eye image to the right eye conjunctival hyperemia prediction model, wherein the predicted value for the conjunctival edema is determined based on a result obtained by inputting the first processed left eye image to the left eye conjunctival edema prediction model and a result obtained by inputting the first processed right eye image to the right eye conjunctival edema prediction model, wherein the predicted value for the lacrimal edema is determined based on a result obtained by inputting the first processed left eye image to the left eye lacrimal edema prediction model and a result obtained by inputting the first processed right eye image to the right eye lacrimal edema prediction model, wherein the predicted value for the eyelid redness is determined based on a result obtained by inputting the second processed left eye image to the left eye eyelid redness prediction model and a result obtained by inputting the second processed right eye image to the right eye eyelid redness prediction model, and wherein the predicted value for the eyelid edema is determined based on a result obtained by inputting the second processed left eye image to the left eye eyelid edema prediction model and a result obtained by inputting the second processed right eye image to the right eye eyelid edema prediction model.</p><p id="p-0019" num="0018">In some embodiments, the method further comprising: processing any one of the first processed left eye image and the first processed right eye image by inverting left and right, and processing any one of the second processed left eye image and the second processed right eye image by inverting left and right.</p><p id="p-0020" num="0019">In some embodiments, wherein the predicted value for the conjunctival hyperemia is determined based on results obtained by inputting the image laterally inverted and by inputting the image not laterally inverted to the conjunctival hyperemia prediction model, wherein the predicted value for the conjunctival edema is determined based on results obtained by inputting the image laterally inverted and by inputting the image not laterally inverted to the conjunctival edema prediction model, wherein the predicted value for the lacrimal edema is determined based on results obtained by inputting the image laterally inverted and by inputting the image not laterally inverted to the lacrimal edema prediction model, wherein the predicted value for the eyelid redness is determined based on results obtained by inputting the image laterally inverted and by inputting the image not laterally inverted to the eyelid redness prediction model, and wherein the predicted value for the eyelid edema is determined based on results obtained by inputting the image laterally inverted and by inputting the image not laterally inverted to the eyelid edema prediction model.</p><p id="p-0021" num="0020">In some embodiments, the method further comprising: resizing the first processed left eye image and the first processed right eye image, and resizing the second processed left eye image and the second processed right eye image.</p><heading id="h-0007" level="1">Advantageous Effects</heading><p id="p-0022" num="0021">According to the disclosure in the present application, a clinical activity score for thyroid eye disease can be predicted using images obtained through a digital camera that ordinary people can use, rather than a professional medical diagnostic device.</p><p id="p-0023" num="0022">In addition, according to the disclosure in the present application, ordinary people can continuously monitor a clinical activity score for thyroid eye disease without a doctor's help and a hospital visit in person, and a hospital visit is recommended when necessary.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0008" level="1">DESCRIPTION OF DRAWINGS</heading><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating a system for predicting a clinical activity score for thyroid eye disease according to an embodiment described in the present application.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a user terminal provided in the present application.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating a server described in the present application.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating an eye and the surrounding tissues that are exposed to the outside so that the eye and the surrounding tissues are captured by a camera when a picture of the face is taken using the camera.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating an eyeball exposed to the outside.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram illustrating an outline of an eye.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram illustrating a cornea exposed to the outside.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram illustrating a conjunctiva exposed to the outside.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>9</b>A</figref> is a diagram illustrating a facial image.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>9</b>B</figref> is a diagram illustrating a two-eye image.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>10</b>A</figref> is a diagram illustrating a two-eye image.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>10</b>B</figref> is a diagram illustrating a left eye image and a right eye image.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram illustrating X<sub>max</sub>, X<sub>min</sub>, Y<sub>max</sub>, and Y<sub>min </sub>of outline pixels.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating a second cropped region determined.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>13</b>A</figref> is a diagram illustrating an example of a second cropped image.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>13</b>B</figref> is a diagram illustrating an example of a second cropped image.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIGS. <b>14</b>A and <b>14</b>B</figref> are diagrams illustrating examples of third cropped images.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIGS. <b>15</b> and <b>16</b></figref> are diagrams illustrating iris segmentation.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a diagram illustrating eye outline segmentation.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>18</b>A</figref> is a diagram illustrating an example of a first masking image.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>18</b>B</figref> is a diagram illustrating an example of a first masking image.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a diagram illustrating an example of a second masking image.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIGS. <b>20</b> to <b>22</b></figref> are diagrams illustrating various examples of original images and laterally inverted images.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a flowchart illustrating a conjunctival hyperemia prediction method.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a flowchart illustrating a conjunctival edema prediction method.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a flowchart illustrating a lacrimal edema prediction method.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a flowchart illustrating an eyelid redness prediction method.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a flowchart illustrating an eyelid edema prediction method.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is a diagram illustrating a method of predicting a clinical activity score for thyroid eye disease.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a diagram illustrating a method of continuously monitoring a clinical activity score for thyroid eye disease, and a method of recommending a hospital visit on the basis of the former.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0009" level="1">DETAILED DESCRIPTION</heading><p id="p-0054" num="0053">The above-described objectives, features, and advantages of the present application will be more apparent from the following detailed description with reference to the accompanying drawings. In addition, various modifications may be made to the present application, and various embodiments of the present application may be practiced. Therefore, specific embodiments will be described in detail below with reference to the accompanying drawings.</p><p id="p-0055" num="0054">Throughout the specification, the same reference numerals denote the same elements in principle. In addition, elements having the same function within the same scope illustrated in the drawings of the embodiments are described using the same reference numerals, and a redundant description will be omitted.</p><p id="p-0056" num="0055">A detailed description of a well-known function or configuration relating to the present application is omitted when determined as obfuscating the nature and gist of the present application. In addition, throughout the present specification, the terms first, second, and so on are used only to distinguish from one element to another.</p><p id="p-0057" num="0056">In addition, the terms &#x201c;module&#x201d; and &#x201c;part&#x201d; that are used to name an element in the description below are used considering only the ease with which the present specification is written. The terms are not intended as having different special meanings or functions and thus may be used individually or interchangeably.</p><p id="p-0058" num="0057">In the following embodiments, an expression used in the singular encompasses the expression of the plural, unless it has a clearly different meaning in the context.</p><p id="p-0059" num="0058">In the following embodiments, it is to be understood that terms such as &#x201c;including&#x201d;, &#x201c;having&#x201d;, etc. are intended to indicate the existence of features or elements disclosed in the specification, and are not intended to preclude the possibility that one or more other features or elements may be added.</p><p id="p-0060" num="0059">Sizes of elements in the drawings may be exaggerated or reduced for convenience of description. For example, any size and thickness of each element shown in the drawings are shown for convenience of description, and the present disclosure is not limited thereto.</p><p id="p-0061" num="0060">In a case in which a particular embodiment is realized otherwise, a particular process may be performed out of the order described. For example, two processes described in succession may be performed substantially simultaneously, or may proceed in the order opposite to the order described.</p><p id="p-0062" num="0061">In the following embodiments, when elements are referred to as being connected to each other, the elements are directly connected to each other or the elements are indirectly connected to each other with intervening elements therebetween. For example, in the present specification, when elements are referred to as being electrically connected to each other, the elements are directly electrically connected to each other or the elements are indirectly electrically connected with intervening elements therebetween.</p><p id="p-0063" num="0062">According to one aspect of the present application, a computer-implemented method of predicting thyroid eye disease is disclosed. The method comprising: preparing a conjunctival hyperemia prediction model, a conjunctival edema prediction model, a lacrimal edema prediction model, an eyelid redness prediction model, and an eyelid edema prediction model, obtaining a facial image of an object, obtaining a first processed image and a second processed image from the facial image, wherein the first processed image is different from the second processed image, obtaining predicted values for each of a conjunctival hyperemia, a conjunctival edema and a lacrimal edema by applying the first processed image to the conjunctival hyperemia prediction model, the conjunctival edema prediction model, and the lacrimal edema prediction model, obtaining predicted values for each of an eyelid redness and an eyelid edema by applying the second processed image to the eyelid redness prediction model and the eyelid edema prediction model, and determining a possibility of the object having thyroid eye disease based on the predicted values for the conjunctival hyperemia, the conjunctival edema, the lacrimal edema, the eyelid redness, and the eyelid edema. Wherein the first processed image is an image masked a region corresponding an inner of an outline of an iris and a region corresponding an outer of an outline of an eye and cropped along a first region comprising the outline of the eye based on position information of pixels corresponding the outline of the iris comprised in the eye and position information of pixels corresponding the outline of the eye, and wherein the second processed image is an image cropped along a second region that is wider than the first region based on position information of pixels corresponding the outline of the iris comprised in the eye and position information of pixels corresponding the outline of the eye.</p><p id="p-0064" num="0063">In some embodiments, wherein the position information of pixels corresponding the outline of the iris comprised in the eye and the position information of pixels corresponding the outline of the eye are obtained by segmentation model.</p><p id="p-0065" num="0064">In some embodiments, wherein the first processed image comprises a first processed left eye image and a first processed right eye image, and wherein the second processed image comprises a second processed left eye image and a second processed right eye image.</p><p id="p-0066" num="0065">In some embodiments, wherein the conjunctival hyperemia prediction model comprises a left eye conjunctival hyperemia prediction model and a right eye conjunctival hyperemia prediction model, wherein the conjunctival edema prediction model comprises a left eye conjunctival edema prediction model and a right eye conjunctival edema prediction model, wherein the lacrimal edema prediction model comprises a left eye lacrimal edema prediction model and a right eye lacrimal edema prediction model, wherein the eyelid redness prediction model comprises a left eyelid redness prediction model and a right eye eyelid redness prediction model, and wherein the eyelid edema prediction model comprises a left eye eyelid edema prediction model and a right eye eyelid edema prediction model.</p><p id="p-0067" num="0066">In some embodiments, wherein the predicted value for the conjunctival hyperemia is determined based on a result obtained by inputting the first processed left eye image to the left eye conjunctival hyperemia prediction model and a result obtained by inputting the first processed right eye image to the right eye conjunctival hyperemia prediction model, wherein the predicted value for the conjunctival edema is determined based on a result obtained by inputting the first processed left eye image to the left eye conjunctival edema prediction model and a result obtained by inputting the first processed right eye image to the right eye conjunctival edema prediction model, wherein the predicted value for the lacrimal edema is determined based on a result obtained by inputting the first processed left eye image to the left eye lacrimal edema prediction model and a result obtained by inputting the first processed right eye image to the right eye lacrimal edema prediction model, wherein the predicted value for the eyelid redness is determined based on a result obtained by inputting the second processed left eye image to the left eye eyelid redness prediction model and a result obtained by inputting the second processed right eye image to the right eye eyelid redness prediction model, and wherein the predicted value for the eyelid edema is determined based on a result obtained by inputting the second processed left eye image to the left eye eyelid edema prediction model and a result obtained by inputting the second processed right eye image to the right eye eyelid edema prediction model.</p><p id="p-0068" num="0067">In some embodiments, the method further comprising: processing any one of the first processed left eye image and the first processed right eye image by inverting left and right, and processing any one of the second processed left eye image and the second processed right eye image by inverting left and right.</p><p id="p-0069" num="0068">In some embodiments, wherein the predicted value for the conjunctival hyperemia is determined based on results obtained by inputting the image laterally inverted and by inputting the image not laterally inverted to the conjunctival hyperemia prediction model, wherein the predicted value for the conjunctival edema is determined based on results obtained by inputting the image laterally inverted and by inputting the image not laterally inverted to the conjunctival edema prediction model, wherein the predicted value for the lacrimal edema is determined based on results obtained by inputting the image laterally inverted and by inputting the image not laterally inverted to the lacrimal edema prediction model, wherein the predicted value for the eyelid redness is determined based on results obtained by inputting the image laterally inverted and by inputting the image not laterally inverted to the eyelid redness prediction model, and wherein the predicted value for the eyelid edema is determined based on results obtained by inputting the image laterally inverted and by inputting the image not laterally inverted to the eyelid edema prediction model.</p><p id="p-0070" num="0069">In some embodiments, the method further comprising: resizing the first processed left eye image and the first processed right eye image, and resizing the second processed left eye image and the second processed right eye image.</p><p id="p-0071" num="0070">According to the present application, disclosed is a system for predicting a clinical activity score (CAS) for a user's thyroid eye disease, and for providing, on the basis of the CAS, guidance about the necessity for the user to visit the hospital.</p><p id="p-0072" num="0071">1. Whole System</p><p id="p-0073" num="0072">(1) Hardware Construction of System</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating a system for predicting a clinical activity score for thyroid eye disease according to an embodiment described in the present application.</p><p id="p-0075" num="0074">Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the system <b>1</b> includes a plurality of user terminals <b>10</b> and a server <b>20</b>.</p><p id="p-0076" num="0075">Hereinafter, the plurality of user terminals <b>10</b> and the server <b>20</b> will be described in detail.</p><p id="p-0077" num="0076">(2) Functions of User Terminals</p><p id="p-0078" num="0077">The plurality of user terminals <b>10</b> transmit information to the server <b>20</b> over various networks, and receive information from the server <b>20</b>.</p><p id="p-0079" num="0078">The plurality of user terminals <b>10</b> obtain images of users' upper eyelids, lower eyelids, and eyeballs exposed to the outside by the upper eyelids and the lower eyelids(hereinafter, referred to as eye images). The plurality of user terminals <b>10</b> may perform necessary processing on the obtained eye images, or may transmit the obtained eye images or the processed eye images to the server <b>20</b>.</p><p id="p-0080" num="0079">The plurality of user terminals <b>10</b> may receive, from the server <b>20</b>, prediction results about clinical activity scores processed by the server <b>20</b>.</p><p id="p-0081" num="0080">(3) Functions of Server</p><p id="p-0082" num="0081">The server <b>20</b> transmits information to the plurality of user terminals <b>10</b> over various networks, and receive information from the plurality of user terminals <b>10</b>.</p><p id="p-0083" num="0082">The server <b>20</b> may receive the eye images from the plurality of user terminals <b>10</b>. Herein, the server <b>20</b> may process the eye images. Alternatively, the server <b>20</b> may receive the processed eye images.</p><p id="p-0084" num="0083">The server <b>20</b> may obtain, on the basis of the processed eye images, prediction results about clinical activity scores for users' thyroid eye diseases.</p><p id="p-0085" num="0084">The server <b>20</b> may transmit the prediction results about the clinical activity scores to the plurality of user terminals <b>10</b>.</p><p id="p-0086" num="0085">(4) Software Construction of System</p><p id="p-0087" num="0086">In order for the system <b>1</b> to operate, several software constructions are required.</p><p id="p-0088" num="0087">To perform communication between the user terminals <b>10</b> and the server <b>20</b>, terminal software needs to be installed on the plurality of user terminals <b>10</b>, and server software needs to be installed on the server <b>20</b>.</p><p id="p-0089" num="0088">In order to perform pre-processing necessary for the eye images, various preprocessing algorithms may be used.</p><p id="p-0090" num="0089">A plurality of learning models for predicting clinical activity scores on the basis of the preprocessed eye images may be used.</p><p id="p-0091" num="0090">The plurality of preprocessing algorithms may be run by the terminal software installed on the user terminals <b>10</b>, or may be run by the software installed on the server <b>20</b>. Alternatively, some of the plurality of preprocessing algorithms may be executed by the user terminals <b>10</b>, and the others may be executed by the server <b>20</b>.</p><p id="p-0092" num="0091">The plurality of learning models may be run by the software installed on the server <b>20</b>. Alternatively, the plurality of learning models may be run by the terminal software installed on the user terminals <b>10</b>. Alternatively, some of the plurality of learning models may be executed by the user terminals <b>10</b>, and the others may be executed by the server <b>20</b>.</p><p id="p-0093" num="0092">(5) Elements of User Terminal</p><p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a user terminal described in the present application.</p><p id="p-0095" num="0094">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a user terminal <b>10</b> described in the present application includes an output part <b>110</b>, a communication part <b>120</b>, a memory <b>130</b>, a camera <b>140</b>, and a controller <b>150</b>.</p><p id="p-0096" num="0095">The output part <b>110</b> outputs various types of information according to control commands of the controller <b>150</b>. According to an embodiment, the output part <b>110</b> may include a display <b>112</b> for outputting information visually to a user. Alternatively, although not shown in the drawings, a speaker for outputting information audibly to a user, and a vibration motor for outputting information tactually to a user may be included.</p><p id="p-0097" num="0096">The communication part <b>120</b> may include a wireless communication module and/or a wired communication module. Herein, examples of the wireless communication module may include a Wi-Fi communication module, a cellular communication module, etc.</p><p id="p-0098" num="0097">The memory <b>130</b> stores therein executable code readable by the controller <b>150</b>, processed result values, necessary data, etc. Examples of the memory <b>130</b> may include a hard disk drive (HDD), a solid state disk (SSD), a silicon disk drive (SDD), ROM, RAM, etc. The memory <b>130</b> may store therein the above-described terminal software, and may store therein executable codes for realizing the above-described various preprocessing algorithms and/or learning models. Furthermore, the memory <b>130</b> may store therein an eye image obtained through the camera <b>140</b>, the preprocessed eye images, etc.</p><p id="p-0099" num="0098">The camera <b>140</b> is a digital camera, and may include an image sensor and an image processor. The image sensor is a device for converting an optical image into electrical signals, and may be provided as a chip in which multiple photodiodes are integrated. Examples of the image sensor may include a charge-coupled device (CCD), a complementary metal-oxide-semiconductor (CMOS), etc. In the meantime, the image processor may perform image processing on captured results, and may generate image information.</p><p id="p-0100" num="0099">The controller <b>150</b> may include at least one processor. Herein, each of the processors may perform a predetermined operation by executing at least one instruction stored in the memory <b>130</b>. Specifically, the controller <b>150</b> may process information according to the terminal software, the preprocessing algorithms, and/or the learning models running on the user terminal <b>10</b>. In the meantime, the controller <b>150</b> controls the overall operation of the user terminal <b>10</b>.</p><p id="p-0101" num="0100">Although not shown in the drawings, the user terminal <b>10</b> may include a user input part. The user terminal <b>10</b> may receive, from a user, various types of information required for the operation of the user terminal <b>10</b> through the user input part.</p><p id="p-0102" num="0101">(6) Elements of Server</p><p id="p-0103" num="0102"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating a server described in the present application.</p><p id="p-0104" num="0103">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the server <b>20</b> described in the present application includes a communication part <b>210</b>, a memory <b>220</b>, and a controller <b>230</b>.</p><p id="p-0105" num="0104">The communication part <b>210</b> may include a wireless communication module and/or a wired communication module. Herein, examples of the wireless communication module may include a Wi-Fi communication module, a cellular communication module, etc.</p><p id="p-0106" num="0105">The memory <b>220</b> stores therein executable code readable by the controller <b>230</b>, processed result values, necessary data, etc. Examples of the memory <b>220</b> may include a hard disk drive (HDD), a solid state disk (SSD), a silicon disk drive (SDD), ROM, RAM, etc. The memory <b>220</b> may store therein the above-described server software, and may store therein executable codes for realizing the above-described various preprocessing algorithms and/or learning models. Furthermore, the memory <b>220</b> may store therein an eye image received from the user terminal <b>10</b>, the preprocessed eye images, etc.</p><p id="p-0107" num="0106">The controller <b>230</b> may include at least one processor. Herein, each of the processors may perform a predetermined operation by executing at least one instruction stored in the memory <b>220</b>. Specifically, the controller <b>230</b> may process information according to the server software, the preprocessing algorithms, and/or the learning models running on the server <b>20</b>. In the meantime, the controller <b>230</b> controls the overall operation of the server <b>20</b>.</p><p id="p-0108" num="0107">Hereinafter, in order to more clearly and easily understand the technology described in the present application, an eye, an eyeball, and the tissues near the eyeball including an upper eyelid, a lower eyelid, and a lacrimal caruncle will be briefly described, and the terms related to an eye and the surroundings used in the present specification will be defined.</p><p id="p-0109" num="0108">2. Construction of Eye and Definition of Terms</p><p id="p-0110" num="0109">(1) Eyeball and Surrounding Tissues</p><p id="p-0111" num="0110"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating an eye and the surrounding tissues that are exposed to the outside so that the eye and the surrounding tissues are captured by a camera when a picture of the face is taken using the camera.</p><p id="p-0112" num="0111"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows eyelids(the upper eyelid and the lower eyelid), a lacrimal caruncle, and a conjunctiva and a cornea partially exposed and partially covered by the upper eyelid, the lower eyelid, and the lacrimal caruncle.</p><p id="p-0113" num="0112">In general, an eye or eyeball is larger than that shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. However, an eyeball is protected from the outside by tissues, such as the upper eyelid, and the lower eyelid, and thus, only part of the eyeball is exposed to the outside even when the person has his or her eye open.</p><p id="p-0114" num="0113">(2) Definition of Terms</p><p id="p-0115" num="0114">Conjunctiva, White of Eye</p><p id="p-0116" num="0115">Hereinafter, a conjunctiva generally corresponds to the position of the white of an eye, so the terms the conjunctiva and the white of the eye may be used interchangeably.</p><p id="p-0117" num="0116">Cornea, Iris</p><p id="p-0118" num="0117">Hereinafter, a cornea generally corresponds to the position of the iris of an eye, so the terms cornea and iris may be used interchangeably. In the meantime, in the present specification, the term &#x2018;iris&#x2019; is used in a sense including a pupil region.</p><p id="p-0119" num="0118">Eyelids</p><p id="p-0120" num="0119">Eyelids are two, upper and lower folds of skin covering the front part of an eyeball. Eyelids are also called palpebrae. The eyelid above an eyeball is called the upper eyelid, and the eyelid below the eyeball is called the lower eyelid. The outer surface is skin and the inner surface is a conjunctiva, and therebetween, there are muscles moving the eyelids, and tarsal plates containing meibomian glands, which are sebaceous glands, thus maintaining the shape of the eyelids. The eyelids protect the eyeball, and simultaneously, make the eyeball clean with tears by blinking the eye or make the cornea shiny and transparent.</p><p id="p-0121" num="0120">Eyebrow</p><p id="p-0122" num="0121">An eyebrow refers to hairs grown in an arc along the bony ridge above an eye.</p><p id="p-0123" num="0122">Eyelashes</p><p id="p-0124" num="0123">Eyelashes refer to hairs about 10 mm in length on the edge of upper and lower eyelids.</p><p id="p-0125" num="0124">Eyeball Exposed to Outside</p><p id="p-0126" num="0125">Hereinafter, &#x201c;the eyeball exposed to the outside&#x201d; means the part not covered by the upper eyelid, the lower eyelid, and the lacrimal caruncle, that is, the part exposed to the outside by the upper eyelid, the lower eyelid, and the lacrimal caruncle when a person has his or her eye open. For example, the inside of the dotted line shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> is called &#x201c;the eyeball exposed to the outside&#x201d;.</p><p id="p-0127" num="0126">Outline of Eye</p><p id="p-0128" num="0127">Hereinafter, &#x201c;the outline of the eye&#x201d; means the outline of the part including both the eyeball exposed to the outside and the lacrimal caruncle region when a person has his or her eye open. That is, the outline of the region that is a combination of the eyeball exposed to the outside and the lacrimal caruncle is called &#x201c;the outline of the eye&#x201d;. For example, the dotted line shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is called &#x201c;the outline of the eye&#x201d;.</p><p id="p-0129" num="0128">Cornea Exposed to Outside (Iris Exposed to Outside)</p><p id="p-0130" num="0129">Hereinafter, &#x201c;the cornea exposed to the outside&#x201d; means the cornea part not covered by the upper eyelid and the lower eyelid, that is, the cornea part exposed to the outside by the upper eyelid and the lower eyelid, when a person has his or her eye open. For example, the inside of the dotted line shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref> is called &#x201c;the cornea exposed to the outside&#x201d;.</p><p id="p-0131" num="0130">Conjunctiva Exposed to Outside (White of Eye Exposed to Outside)</p><p id="p-0132" num="0131">Hereinafter, &#x201c;the conjunctiva exposed to the outside&#x201d; means the conjunctiva part not covered by the upper eyelid, the lower eyelid, and the lacrimal caruncle, that is, the conjunctiva part exposed to the outside by the upper eyelid, the lower eyelid, and the lacrimal caruncle, when a person has his or her eye open. For example, the inside of the dotted line shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> is called &#x201c;the conjunctiva exposed to the outside&#x201d;.</p><p id="p-0133" num="0132">Hereinafter, various image preprocessing algorithms for performing image preprocessing described in the present application will be described.</p><p id="p-0134" num="0133">3. Image Preprocessing Algorithms</p><p id="p-0135" num="0134">(1) Necessity of Image Preprocessing</p><p id="p-0136" num="0135">The present application is directed to providing a learning model for predicting a clinical activity score for thyroid eye disease by using an image obtained by a digital camera that ordinary people can use, rather than a professional medical diagnostic device.</p><p id="p-0137" num="0136">To this end, in predicting a clinical activity score for thyroid eye disease, images that can be easily obtained by ordinary people for eyeballs and the tissues near the eyeballs need to be used. For example, an image analysis uses a digital image obtained by a digital camera or a camera built in a smartphone that can be easily used by ordinary people rather than a digital image obtained by a specialized medical device used in a medical institution.</p><p id="p-0138" num="0137">Under this environment, a digital image obtained by a user is difficult to be standardized, and in order to more accurately and quickly recognize a digital image obtained by a user, various types of preprocessing of the obtained image are required.</p><p id="p-0139" num="0138">(2) First Cropping (Two-Eye Image Cropping)</p><p id="p-0140" num="0139">An image used in predicting a clinical activity score for thyroid eye disease needs to include a left eye, a right eye, and the surrounding regions.</p><p id="p-0141" num="0140">However, for a more quick and accurate analysis, it is more efficient to use, in an image analysis, an image of only two eyes and the surrounding regions not including several unnecessary regions (for example, the regions corresponding to the nose, the mouth, the forehead, etc.) (hereinafter, referred to as a two-eye image), rather than an image of the entire face (hereinafter, referred to as a facial image).</p><p id="p-0142" num="0141">Therefore, it is necessary to cut out the image including two eyes (left eye/right eye) (hereinafter, referred to as a two-eye image) from the image of the entire face(hereinafter, referred to as a facial image) obtained by the user.</p><p id="p-0143" num="0142">For example, from the facial image obtained by a user shown in <figref idref="DRAWINGS">FIG. <b>9</b>(<i>a</i>)</figref>, a two-eye image (the inner region of the quadrangle marked with the dotted line) as shown in <figref idref="DRAWINGS">FIG. <b>9</b>(<i>b</i>)</figref> may be obtained. Hereinafter, obtaining a two-eye image from a facial image acquired by a user in this way is called two-eye image cropping or first cropping.</p><p id="p-0144" num="0143">(3) Necessity of Applying Additional Cropping Methods</p><p id="p-0145" num="0144">The inventors of the present application built a system for predicting scores for five items related to thyroid eye disease through prediction models, which will be described later, using a first cropped image (two-eye image) describe above, but it was found that the accuracy of prediction was low.</p><p id="p-0146" num="0145">The inventors of the present application determined that the accuracy of prediction was low because the two-eye image included many regions unnecessary for an analysis, and determined that it is necessary to obtain a more elaborate cropped image. That is, it was determined that it is more efficient to separately obtain and use the left eye image and the right eye image as shown in <figref idref="DRAWINGS">FIG. <b>10</b>(<i>b</i>)</figref> than to use the two-eye image as shown in <figref idref="DRAWINGS">FIG. <b>10</b>(<i>a</i>)</figref> in which the left eye and the right eye are included in one image.</p><p id="p-0147" num="0146">(4) Necessity of Applying Different Cropping Methods</p><p id="p-0148" num="0147">It has been described that five of the seven items for evaluating a clinical activity score for thyroid eye disease are the items evaluated according to a doctor's observation with the naked eye, of user's eyeballs and the surrounding regions. The five items are as follows.</p><p id="p-0149" num="0148">1) Conjunctival hyperemia (redness of conjunctiva),</p><p id="p-0150" num="0149">2) Conjunctival edema (swelling of conjunctiva),</p><p id="p-0151" num="0150">3) Lacrimal edema (swelling of lacrimal caruncle),</p><p id="p-0152" num="0151">4) Redness of eyelid, and</p><p id="p-0153" num="0152">5) Eyelid edema (swelling of eyelid).</p><p id="p-0154" num="0153">Hereinafter, as will be described later, in order to evaluate a clinical activity score for thyroid eye disease, independent prediction models provided in the present application have been applied to the five symptoms.</p><p id="p-0155" num="0154">There may be a method of using an image to which different cropping methods are applied through five independent prediction models, but the inventors of the present application determined that sufficient prediction accuracy could be obtained by applying an image cropping method for analyzing a conjunctiva and a lacrimal caruncle and an image cropping method for analyzing an eyelid.</p><p id="p-0156" num="0155">(5) Second Cropping (Eye-Outline-Based Cropping)</p><p id="p-0157" num="0156">Hereinafter, eye-outline-based cropping (second cropping) will be described. Second cropping may be applied to both a right eye image and a left eye image, but a description will be given based on the case of obtaining a right eye cropped image for convenience.</p><p id="p-0158" num="0157">Purpose of Second Cropping</p><p id="p-0159" num="0158">Second cropping is to generate an image to be used as an input image of a model for predicting whether there is redness of a conjunctiva, a model for predicting whether there is swelling of a conjunctiva, and a model for predicting whether there is swelling of a lacrimal caruncle among the prediction models to be described later. Second cropping is to generate an image in which information on the cornea and the lacrimal caruncle is maximized and information on the other regions is minimized.</p><p id="p-0160" num="0159">Input Image</p><p id="p-0161" num="0160">Second cropping may be applied to a facial image or a two-eye image (first cropped image).</p><p id="p-0162" num="0161">Detection of Outline of Eye</p><p id="p-0163" num="0162">According to an embodiment, in order to detect the outline of the right eye, the pixels corresponding to the boundary between the upper eyelid and the eyeball and to the boundary between the lower eyelid and the eyeball may be detected. In addition, in order to detect the outline of the right eye, the pixels corresponding to the points at which the upper eyelid and the lower eyelid meet may be detected. Furthermore, in order to detect the outline of the right eye, the pixels corresponding to the lacrimal caruncle may be detected.</p><p id="p-0164" num="0163">According to another embodiment, the outline pixels corresponding to the outermost part of the outline of the eye may be detected using an eye outline segmentation model, which will be described later.</p><p id="p-0165" num="0164">Determination of Maximum Values and Minimum Values of X and Y Coordinates of Outline Pixels</p><p id="p-0166" num="0165">Determining the detected pixels, the maximum value X<sub>max </sub>of the X coordinate values, the minimum value X<sub>min </sub>of the X coordinate values, the maximum value Y<sub>max </sub>of the Y coordinate values, and the minimum value Y<sub>min </sub>of the Y coordinate values are determined.</p><p id="p-0167" num="0166"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram illustrating X<sub>max</sub>, X<sub>min</sub>, Y<sub>max</sub>, and Y<sub>min </sub>of outline pixels.</p><p id="p-0168" num="0167">Cropped Region Determination</p><p id="p-0169" num="0168">On the basis of the determined X<sub>max</sub>, X<sub>min</sub>, Y<sub>max</sub>, and Y<sub>min </sub>of the outline pixels, a quadrangle having the following four points as vertexes is generated, and the region included inside the quadrangle is determined as a cropped region.</p><p id="p-0170" num="0169">(X<sub>min</sub>, Y<sub>max</sub>),</p><p id="p-0171" num="0170">(X<sub>max</sub>, Y<sub>max</sub>),</p><p id="p-0172" num="0171">(X<sub>max</sub>, Y<sub>min</sub>), and</p><p id="p-0173" num="0172">(X<sub>min</sub>, Y<sub>min</sub>)</p><p id="p-0174" num="0173"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating a second cropped region determined.</p><p id="p-0175" num="0174">As described above, the second cropped region may be determined in the same manner for the left eye.</p><p id="p-0176" num="0175">Generation of Second Cropped Images</p><p id="p-0177" num="0176">The second cropped regions are determined, and on the basis of the determined second cropped regions, as shown in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, second cropped images (a second right eye cropped image and a second left eye cropped image) may be generated from the facial image or the two-eye image by using the pixels included inside the second cropped regions determined as described above.</p><p id="p-0178" num="0177">Hereinafter, the term &#x201c;second right eye cropped image&#x201d; and the term &#x201c;right eye outline cropped image&#x201d; may be used interchangeably, and the term &#x201c;second left eye cropped image&#x201d; and the term &#x201c;left eye outline cropped image&#x201d; may be used interchangeably.</p><p id="p-0179" num="0178">In addition, without specific mention hereinbelow, the term &#x201c;second cropped image (or outline cropped image)&#x201d; may mean either a second right eye cropped image or a second left eye cropped image, or may mean both depending on the context.</p><p id="p-0180" num="0179">A second cropped image means an image cropped with respect to &#x2018;the outline of the eye&#x2019;. A cropped image generated in a method different from the above-described method is referred to as a second cropped image (outline cropped image) if the cropped image is generated such that the top, bottom, rightmost, and leftmost pixels of &#x2018;the outline of the eye&#x2019; are included in the cropped region.</p><p id="p-0181" num="0180">In the meantime, X coordinate values and Y coordinate values in the present application have different sizes and directions depending on a relative position with respect to a reference point, so the terms maximum value and minimum value should be understood in a relative sense, but not in an absolute sense. That is, as the position of the origin of the coordinate system is changed, the maximum value of the above-described X coordinate value may be the minimum value of the X coordinate value in the coordinate system of which the origin is changed, and the minimum value of the X coordinate value may be the maximum value of the X coordinate value in the coordinate system of which the origin is changed. This may be equally applied to the Y coordinate value.</p><p id="p-0182" num="0181">(6) Third Cropping (Eyelid-Included Cropping)</p><p id="p-0183" num="0182">Hereinafter, eyelid-included cropping (third cropping) will be described. Third cropping may be applied to both a right eye image and a left eye image, but a description will be given based on the case of obtaining a right eye cropped image for convenience.</p><p id="p-0184" num="0183">Purpose of Third Cropping</p><p id="p-0185" num="0184">Third cropping is to generate an image to be used as an input image of a model for predicting whether there is redness of eyelids and a model for predicting whether there is swelling of eyelids among the prediction models to be described later. Third cropping is to include information on eyelids in the image. Herein, rather than cropping with only the pixels corresponding to eyelids, it may be better to generate a cropped image such that all the pixels included in the outline of the eye are included. This is because inference and determination are required for color values in order to predict whether there is eyelid redness, and the color values of the pixels corresponding to the iris and/or the white of the eye may be used.</p><p id="p-0186" num="0185">Input Image</p><p id="p-0187" num="0186">Third cropping may be applied to a facial image or a two-eye image (first cropped image).</p><p id="p-0188" num="0187">Detection of Outline of Eye, and Determination of Maximum Values and Minimum Values of X and Y Coordinates of Outline Pixels</p><p id="p-0189" num="0188">According to an embodiment, the eye outline detection method described in second cropping may be applied as it is, or the outline pixels corresponding to the outermost part of the outline of the eye may be detected. Determining the detected pixels, the maximum value X<sub>max </sub>of the X coordinate values, the minimum value X<sub>min </sub>of the X coordinate values, the maximum value Y<sub>max </sub>of the Y coordinate values, and the minimum value Y<sub>min </sub>of the Y coordinate values may be determined.</p><p id="p-0190" num="0189">Cropped Region Determination #1</p><p id="p-0191" num="0190">On the basis of the determined Y<sub>max </sub>value and Y<sub>min </sub>value, a first expansion value Y<sub>e </sub>determined according to a predetermined criterion is added to the Y<sub>max </sub>value, and a second expansion value Y<sub>e</sub>&#x2032; determined according to a predetermined criterion is subtracted from the Y<sub>min </sub>value, and a third cropped region may be determined similarly to the above-described method of determining the second cropped region.</p><p id="p-0192" num="0191">That is, a quadrangle having the following four points as vertexes is generated, and the region included inside the quadrangle is determined as a third cropped region.</p><p id="p-0193" num="0192">(X<sub>min</sub>, Y<sub>max</sub>+Y<sub>e</sub>),</p><p id="p-0194" num="0193">(X<sub>max</sub>, Y<sub>max</sub>+Y<sub>e</sub>),</p><p id="p-0195" num="0194">(X<sub>max</sub>, Y<sub>min</sub>&#x2212;Y<sub>e</sub>&#x2032;) and</p><p id="p-0196" num="0195">(X<sub>min</sub>, Y<sub>min</sub>&#x2212;Y<sub>e</sub>&#x2032;)</p><p id="p-0197" num="0196">When the third cropped region is determined in this way, more pixels corresponding to the upper eyelid and the lower eyelid may be included in the image, compared to the second cropped region.</p><p id="p-0198" num="0197">Herein, the first expansion value and the second expansion value may be the same, but are not necessarily the same.</p><p id="p-0199" num="0198">In the meantime, the criterion for determining the first expansion value and the criterion for determining the second expansion value may be the same, but are not necessarily the same.</p><p id="p-0200" num="0199">The first expansion value and the second expansion value may be determined on the basis of the size of the second cropped region. For example, the first expansion value and the second expansion value may be determined using the number of pixels corresponding to the length calculated by multiplying an expansion percentage by the horizontal length of the second cropped region. As another example, the first expansion value and the second expansion value may be determined using the number of pixels corresponding to the length calculated by multiplying an expansion percentage by the vertical length of the second cropped region.</p><p id="p-0201" num="0200">Herein, the specific percentage may be any one of the following: 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, and 60%.</p><p id="p-0202" num="0201">The expansion percentage used in determining the first expansion value and the expansion percentage used in determining the second expansion value may be the same, but are not necessarily the same and may be different from each other.</p><p id="p-0203" num="0202">When the horizontal length of the second cropped region is used in determining the first expansion value, the horizontal length may also be used in determining the second expansion value, but no limitation thereto is imposed and the vertical length may be used in determining the second expansion value.</p><p id="p-0204" num="0203">Cropped Region Determination #2</p><p id="p-0205" num="0204">On the basis of the determined X<sub>max </sub>value and X<sub>min </sub>value, a first width expansion value X<sub>we </sub>is determined according to a predetermined criterion, and a second width expansion value X<sub>we</sub>&#x2032; is determined according to a predetermined criterion.</p><p id="p-0206" num="0205">On the basis of the determined Y<sub>max </sub>value and Y<sub>min </sub>value, a first height expansion value Y<sub>he </sub>is determined according to a predetermined criterion, and a second height expansion value Y<sub>he</sub>&#x2032; is determined according to a predetermined criterion.</p><p id="p-0207" num="0206">On the basis of the value obtained by adding the first width expansion value X<sub>we </sub>to the X<sub>max </sub>value, the value obtained by subtracting the second width expansion value X<sub>we</sub>&#x2032; from the X<sub>min </sub>value, the value obtained by adding the first height expansion value Y<sub>he </sub>to the Y<sub>max </sub>value, and the value obtained by subtracting the second height expansion value Y<sub>he</sub>&#x2032; from the Y<sub>min </sub>value, a third cropped region may be determined similarly to the above-described method of determining the second cropped region.</p><p id="p-0208" num="0207">That is, a quadrangle having the following four points as vertexes is generated, and the region included inside the quadrangle is determined as a third cropped region.</p><p id="p-0209" num="0208">(X<sub>min</sub>&#x2212;X<sub>we</sub>&#x2032;, Y<sub>max</sub>+Y<sub>he</sub>),</p><p id="p-0210" num="0209">(X<sub>max</sub>+X<sub>we</sub>, Y<sub>max</sub>+Y<sub>he</sub>),</p><p id="p-0211" num="0210">(X<sub>max</sub>+X<sub>we</sub>, Y<sub>min</sub>&#x2212;Y<sub>he</sub>&#x2032;) and</p><p id="p-0212" num="0211">(X<sub>min</sub>&#x2212;X<sub>we</sub>&#x2032;, Y<sub>min</sub>&#x2212;Y<sub>he</sub>&#x2032;)</p><p id="p-0213" num="0212">When the third cropped region is determined in this way, more pixels corresponding to the upper eyelid and the lower eyelid may be included in the image, compared to the second cropped region.</p><p id="p-0214" num="0213">Furthermore, the cropped image includes more pixels in a left-right direction than the image cropped by the method &#x201c;cropped region determination #1&#x201d;. As a result, the cropped image includes more information on the upper eyelid and the lower eyelid. Because the width of the upper eyelid and the lower eyelid is generally wider than the width of the eyeball exposed to the outside, more pixels corresponding to the upper eyelid and the lower eyelid are included through vertical expansion as well as horizontal expansion.</p><p id="p-0215" num="0214">Herein, the first width expansion value and the second width expansion value may be the same, but are not necessarily the same.</p><p id="p-0216" num="0215">In the meantime, the criterion for determining the first height expansion value and the criterion for determining the second height expansion value may be the same, but are not necessarily the same.</p><p id="p-0217" num="0216">The first width expansion value and the second width expansion value may be determined on the basis of the size of the second cropped region. For example, the first width expansion value and the second width expansion value may be determined using the number of pixels corresponding to the length calculated by multiplying an expansion percentage by the horizontal length of the second cropped region. As another example, the first width expansion value and the second width expansion value may be determined using the number of pixels corresponding to the length calculated by multiplying an expansion percentage by the vertical length of the second cropped region.</p><p id="p-0218" num="0217">Herein, the specific percentage may be any one of the following: 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, and 60%.</p><p id="p-0219" num="0218">The expansion percentage used in determining the first width expansion value and the expansion percentage used in determining the second width expansion value may be the same, but are not necessarily the same and may be different from each other.</p><p id="p-0220" num="0219">When the horizontal length of the second cropped region is used in determining the first width expansion value, the horizontal length may also be used in determining the second width expansion value, but no limitation thereto is imposed and the vertical length may be used in determining the second expansion value.</p><p id="p-0221" num="0220">In the meantime, the method of determining the first height expansion value and the second height expansion value is the same as the above-described method of determining the first expansion value and the second expansion value, so a detailed description will be omitted.</p><p id="p-0222" num="0221">Generation of Third Cropped Images</p><p id="p-0223" num="0222">The third cropped regions are determined, and on the basis of the determined third cropped regions, as shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, third cropped images (a third right eye cropped image and a third left eye cropped image) may be generated from the facial image or the two-eye image by using the pixels included inside the second cropped regions determined as described above.</p><p id="p-0224" num="0223">For reference, <figref idref="DRAWINGS">FIG. <b>14</b>(<i>a</i>)</figref> shows the third cropped images cropped by the above-described method &#x2018;cropped region determination #1&#x2019;, and <figref idref="DRAWINGS">FIG. <b>14</b>(<i>b</i>)</figref> shows the third cropped images cropped by the above-described method &#x2018;cropped region determination #2&#x2019;.</p><p id="p-0225" num="0224">Hereinafter, the term &#x201c;third right eye cropped image&#x201d; and the term &#x201c;right eyelid-included-cropped image&#x201d; may be used interchangeably, and the term &#x201c;third left eye cropped image&#x201d; and the term &#x201c;left eyelid-included-cropped image&#x201d; may be used interchangeably.</p><p id="p-0226" num="0225">In addition, without specific mention hereinbelow, the term &#x201c;third cropped image (or eyelid-included cropped image)&#x201d; may mean either a third right eye cropped image or a third left eye cropped image, or may mean both depending on the context.</p><p id="p-0227" num="0226">A third cropped image means an image that is generated such that the image includes information on eyelids. A cropped image generated in a method different from the above-described method is referred to as a third cropped image (eyelid-included cropped image) if the boundary of the cropped region is determined such that the pixels corresponding to eyelids are additionally included.</p><p id="p-0228" num="0227">(7) Iris Segmentation</p><p id="p-0229" num="0228">Hereinafter, iris segmentation will be described.</p><p id="p-0230" num="0229">Iris segmentation may be performed by a model that distinguishes a region corresponding to the iris or cornea from an image of the eyeball and the surroundings.</p><p id="p-0231" num="0230">By using iris segmentation, the pixels corresponding to the iris within the image may be inferred as shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref>.</p><p id="p-0232" num="0231">By using iris segmentation, the pixels corresponding to the iris exposed to the outside within the image may be inferred as shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref>.</p><p id="p-0233" num="0232">The model for iris segmentation may receive a facial image as input data, may output &#x2018;1&#x2019; for the pixels inferred to be the pixels corresponding to the iris within the facial image, and may output &#x2018;0&#x2019; for the other pixels.</p><p id="p-0234" num="0233">The iris segmentation model may be trained using training data that includes a facial image and an image in which the pixel values of the pixels corresponding to the iris within a facial image are &#x2018;1&#x2019; and the pixel values of the remaining pixels are &#x2018;0&#x2019;.</p><p id="p-0235" num="0234">Although it has been described that a facial image is received as input data and is subjected to iris segmentation, iris segmentation may also be performed using the above-described two-eye image as input data.</p><p id="p-0236" num="0235">(8) Eye Outline Segmentation</p><p id="p-0237" num="0236">Hereinafter, eye outline segmentation will be described.</p><p id="p-0238" num="0237">Eye outline segmentation may be performed by a model that distinguishes a region corresponding to the inside of the outline of the eye from an image of the eyeball and the surroundings.</p><p id="p-0239" num="0238">By using eye outline segmentation, the pixels corresponding to the inside of the outline of the eye within the image may be inferred as shown in <figref idref="DRAWINGS">FIG. <b>17</b></figref>.</p><p id="p-0240" num="0239">The model for eye outline segmentation may receive a facial image as input data, may output &#x2018;1&#x2019; for the pixels inferred to be the pixels corresponding to the inside of the outline of the eye within the facial image, and may output &#x2018;0&#x2019; for the other pixels.</p><p id="p-0241" num="0240">The eye outline segmentation model may be trained using training data that includes a facial image and an image in which the pixel values of the pixels corresponding to the inside of the outline of the eye within the facial image are &#x2018;1&#x2019; and the pixel values of the remaining pixels are &#x2018;0&#x2019;.</p><p id="p-0242" num="0241">(9) Masking</p><p id="p-0243" num="0242">First Masking</p><p id="p-0244" num="0243">In the present application, first masking means that in an image, removed is information reflected in the pixel values corresponding to the regions excluding the pixels corresponding to the conjunctiva and the lacrimal caruncle.</p><p id="p-0245" num="0244">Removing information reflected in pixel values means changing the pixel values of the pixels of which information is to be removed into a predetermined particular value. For example, all the pixel values of the pixels of which information is to be removed may be changed to 0.</p><p id="p-0246" num="0245">First masking may be performed before an image is input to the models for predicting symptoms related to the conjunctiva and the lacrimal caruncle among the above-described prediction models for predicting a clinical activity score for thyroid eye disease.</p><p id="p-0247" num="0246">Second Masking</p><p id="p-0248" num="0247">In the present application, second masking means that in an image, removed is information reflected in the pixel values corresponding to the regions corresponding to the cornea exposed to the outside (the iris exposed to the outside).</p><p id="p-0249" num="0248">Second masking may be performed before an image is input to the models for predicting symptoms related to eyelids (upper eyelid and lower eyelid) among the above-described prediction models for predicting a clinical activity score for thyroid eye disease.</p><p id="p-0250" num="0249">Method of First Masking</p><p id="p-0251" num="0250">First masking may be performed on a first masking target image that is one selected from the group of a facial image, a first cropped image (two-eye image), and a second cropped image (outline cropped image).</p><p id="p-0252" num="0251">On the basis of the first masking target image, the eye outline segmentation result, and the iris segmentation result, a first masking image may be generated. For example, from the first masking target image, the values of the pixels excluding the pixels corresponding to the inside of the outside of the eye, and the pixels values of the pixels corresponding to the iris (or the iris exposed to the outside) may be removed.</p><p id="p-0253" num="0252"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram illustrating examples of a first masking image.</p><p id="p-0254" num="0253">Method of Second Masking</p><p id="p-0255" num="0254">Second masking may be performed on a second masking target image that is one selected from the group of a facial image, a first cropped image (two-eye image), and a third cropped image (eyelid-included cropped image).</p><p id="p-0256" num="0255">On the basis of the second masking target image, the eye outline segmentation result, and the iris segmentation result, a second masking image may be generated. For example, from the second masking target image, the pixels values of the pixels corresponding to the inside of the outline of the eye and simultaneously corresponding to the iris (or the iris exposed to the outside) may be removed.</p><p id="p-0257" num="0256"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a diagram illustrating an example of a second masking image.</p><p id="p-0258" num="0257">Another Embodiment of First Masking</p><p id="p-0259" num="0258">According to the above description, it has been described that first masking removes all the pixel values of the pixels corresponding to the regions excluding the pixels corresponding to the conjunctiva and the lacrimal caruncle, but first masking may not remove the pixel values of the pixels corresponding to the cornea (iris) when necessary.</p><p id="p-0260" num="0259">However, since colors of irises may vary according to race, removing the pixel values of the pixels corresponding to the iris is advantageous for quicker learning and higher accuracy.</p><p id="p-0261" num="0260">Option of Second Masking</p><p id="p-0262" num="0261">According to the above description, it has been described that second masking removes all the pixel values of the pixels corresponding to the iris, but it is allowed not to perform second masking at all.</p><p id="p-0263" num="0262">However, since colors of irises may vary according to race, removing the pixel values of the pixels corresponding to the iris by performing second masking is advantageous for quicker learning and higher accuracy.</p><p id="p-0264" num="0263">(10) Lateral Inversion</p><p id="p-0265" num="0264">Necessity of Lateral Inversion</p><p id="p-0266" num="0265">According to a method, which is provided in the present application, of predicting a clinical activity score for thyroid eye disease, cropped images of the left eye and the right eye are used instead of using a two-eye image.</p><p id="p-0267" num="0266">In the meantime, the outline of an eye is asymmetric. For example, with respect to the right eye, the lacrimal caruncle is at the left end of the right eye, but the point at which the upper eyelid and the lower eyelid meet naturally is at the right end of the right eye.</p><p id="p-0268" num="0267">Accordingly, for quicker learning and more accurate prediction, it is more effective to distinguish and use a learning model trained with respect to a right eye and a learning model trained with respect to a left eye.</p><p id="p-0269" num="0268">However, when the left eye is turned over to be the right eye on the basis of the line of symmetry between the left eye and the right eye, the shape features of the right eye and the left eye are similar to each other.</p><p id="p-0270" num="0269">Accordingly, according to the present application, either the right eye or the left eye is used without lateral inversion, the other eye is used with lateral inversion, so that only one learning model can be used.</p><p id="p-0271" num="0270">Lateral Inversion Method</p><p id="p-0272" num="0271">Laterally inverting an image (converting the left and the right of the image) means that with a left and right reference line (X=a) vertically crossing the image to be inverted and dividing the image in half left and right, when a first pixel value corresponds to the pixel (a+&#x394;, Y) in the image and a second pixel value corresponds to the pixel (a&#x2212;&#x394;, Y), the pixel value of (a+&#x394;, Y) is changed from the first pixel value to the second pixel value and the pixel value of (a&#x2212;&#x394;, Y) is changed from the second pixel value to the first pixel value.</p><p id="p-0273" num="0272">Lateral Inversion Target Image</p><p id="p-0274" num="0273">Laterally inverting either the image of the left eye or the image of the right eye is sufficient. Which one of the left eye image and the right eye image is subjected to lateral inversion is determined according to which one of the left eye image and the right eye image is based when the prediction models, which will be described later, are trained.</p><p id="p-0275" num="0274">In the meantime, lateral inversion may be performed on the image on which both masking and cropping (second cropping or third cropping) have been performed, or lateral inversion may be performed on the image on which only cropping has been performed, but masking has not been performed.</p><p id="p-0276" num="0275"><figref idref="DRAWINGS">FIGS. <b>20</b> to <b>22</b></figref> are diagrams illustrating various examples of original images and laterally inverted images.</p><p id="p-0277" num="0276">Option of Lateral Inversion</p><p id="p-0278" num="0277">However, as described above, lateral inversion is applied to unify a prediction model for the left eye and a prediction model for the right eye. Therefore, if the prediction model for the left eye and the prediction model for the right eye are realized as different models, lateral inversion preprocessing may be omitted.</p><p id="p-0279" num="0278">(11) Resizing</p><p id="p-0280" num="0279">Necessity of Resizing</p><p id="p-0281" num="0280">As described above, when an image is cropped with respect to the outline of the eye and the cropped image is used, sizes of eyes vary from person to person and cropped images vary in size from person to person.</p><p id="p-0282" num="0281">In the meantime, when a left eye image and a right eye image are independently cropped and obtained, the left eye cropped image and the right eye cropped image of the same person are different from each other because of the difference in size between the left eye and the right eye.</p><p id="p-0283" num="0282">For this reason, before an eye image is input to the prediction models, which will be described later, it is necessary to resize the eye image to standard sizes corresponding to the respective prediction models.</p><p id="p-0284" num="0283">Standard Size for each Prediction Model</p><p id="p-0285" num="0284">The standard sizes corresponding to a first prediction model to a fifth prediction model, respectively, may be different from each other.</p><p id="p-0286" num="0285">The standard sizes corresponding to the prediction models using a second cropped image as an input image may be the same.</p><p id="p-0287" num="0286">The standard sizes corresponding to the prediction models using a third cropped image as an input image may be the same.</p><p id="p-0288" num="0287">The standard size corresponding to the prediction models using a second cropped image as an input image may be different from the standard size corresponding to the prediction models using a third cropped image as an input image.</p><p id="p-0289" num="0288">Alternatively, the standard sizes corresponding to the first prediction model to the fifth prediction model, respectively, may be the same.</p><p id="p-0290" num="0289">Resizing Method</p><p id="p-0291" num="0290">The size of a resizing target image is adjusted to a standard size.</p><p id="p-0292" num="0291">When the width or height of the resizing target image is greater than the width or height of the standard size, the width or height of the resizing target image may be decreased.</p><p id="p-0293" num="0292">When the width or height of the resizing target image is less than the width or height of the standard size, the width or height of the resizing target image may be increased.</p><p id="p-0294" num="0293">In resizing, the aspect ratio of the image before resizing may be different from the aspect ratio of the image after resizing.</p><p id="p-0295" num="0294">4. Prediction Models</p><p id="p-0296" num="0295">(1) First Prediction Model</p><p id="p-0297" num="0296">Purpose and Operation of First Prediction Model</p><p id="p-0298" num="0297">The first prediction model is a model for predicting whether there is conjunctival hyperemia.</p><p id="p-0299" num="0298">The first prediction model may receive an eye image as input data and may output a probability value that the conjunctiva captured in the input eye image is hyperemic.</p><p id="p-0300" num="0299">When the first prediction model includes a first left eye prediction model and a first right eye prediction model, the first left eye prediction model may receive a left eye image and output a probability value that the conjunctiva captured in the left eye image is hyperemic, and the first right eye prediction model may receive a right eye image and may output a probability value that the conjunctiva captured in the right eye image is hyperemic.</p><p id="p-0301" num="0300">When the first prediction model is not dualized and is realized as one model, the first prediction model may receive either a right eye image or a left eye image to output a probability value that the conjunctiva captured in the input image is hyperemic, and may receive the other image to output a probability value that the conjunctiva captured in the input image is hyperemic.</p><p id="p-0302" num="0301">The eye image may be an image preprocessed by the above-described preprocessing algorithms.</p><p id="p-0303" num="0302">For example, the eye image may be an image on which preprocessing according to second cropping is performed.</p><p id="p-0304" num="0303">As another example, the eye image may be an image on which preprocessing including second cropping and resizing is performed.</p><p id="p-0305" num="0304">As still another example, the eye image may be an image on which preprocessing including second cropping, first masking, and resizing is performed.</p><p id="p-0306" num="0305">As yet still another example, the eye image may be an image on which preprocessing including second cropping, first masking, lateral inversion, and resizing is performed.</p><p id="p-0307" num="0306">In the present specification, the first prediction model may be called a conjunctival hyperemia prediction model.</p><p id="p-0308" num="0307">Training of First Prediction Model</p><p id="p-0309" num="0308">To train the first prediction model, a plurality of training data sets may be prepared. A training data set may include an eye image and an evaluation value for conjunctival hyperemia captured in the eye image. The eye image may be an image preprocessed by the above-described preprocessing algorithms. For example, the eye image may be an image on which preprocessing including second cropping, first masking, and resizing is performed.</p><p id="p-0310" num="0309">To train the first prediction model, an artificial intelligence model may be prepared.</p><p id="p-0311" num="0310">Examples of the artificial intelligence model may be a support-vector machine (SVM), Random Forest, Gradient Boosting Algorithm, ResNet, VGG, GoogLeNet, MobileNet, and Vision Transformer.</p><p id="p-0312" num="0311">Next, the eye images included in the prepared plurality of training data sets are input to the artificial intelligence model, and training is performed using the evaluation value corresponding to each of the input eye images and an output value output from the artificial intelligence model.</p><p id="p-0313" num="0312">When the first prediction model includes the first left eye prediction model and the first right eye prediction model, the plurality of training data sets for training the first left eye prediction model may include left eye images and evaluation values for conjunctival hyperemia captured in the left eye images, and the plurality of training data sets for training a first right eye prediction model may include right eye images and evaluation values for conjunctival hyperemia captured in the right eye images. In the meantime, in order to increase the number of training data sets, the plurality of training data sets for training the first left eye prediction model may include right eye images on which lateral inversion is processed and evaluation values for conjunctival hyperemia captured in the right eye images, and the plurality of training data sets for training the first right eye prediction model may include left eye images on which lateral inversion is processed and evaluation values for conjunctival hyperemia captured in the left eye images.</p><p id="p-0314" num="0313">When it is intended not to dualize the first prediction model, but to realize the first prediction model as one model, the plurality of training data sets may include right eye images and evaluation values for conjunctival hyperemia captured in the right eye images, or may include left eye images on which lateral inversion is performed and evaluation values for conjunctival hyperemia captured in the left eye images. Alternatively, the plurality of training data sets may include left eye images and evaluation values for conjunctival hyperemia captured in the left eye images, or may include right eye images on which lateral inversion is performed and evaluation values for conjunctival hyperemia captured in the right eye images.</p><p id="p-0315" num="0314">In the meantime, in training the first prediction model, in order to predict whether there is conjunctival hyperemia without distinguishing between right eye images and left eye images, all right eye images, right eye images on which lateral inversion is performed, left eye images, and left eye images on which lateral inversion is performed are used as training data for training one model.</p><p id="p-0316" num="0315">For example, when the first prediction model includes the first left eye prediction model and the first right eye prediction model, the plurality of training data sets for training the first left eye prediction model may include: left eye images and evaluation values for conjunctival hyperemia captured in the left eye images; and right eye images on which lateral inversion is performed and evaluation values for conjunctival hyperemia captured in the right eye images, and the plurality of training data sets for training the first right eye prediction model may include: right eye images and evaluation values for conjunctival hyperemia captured in the right eye images; and left eye images on which lateral inversion is performed and evaluation values for conjunctival hyperemia captured in the left eye images.</p><p id="p-0317" num="0316">When it is intended not to dualize the first prediction model, but to realize the first prediction model as one model, the plurality of training data sets may include: right eye images and evaluation values for conjunctival hyperemia captured in the right eye images; right eye images on which lateral inversion is performed and evaluation values for conjunctival hyperemia captured in the right eye images; left eye images and evaluation values for conjunctival hyperemia captured in the left eye images; and left eye images on which lateral inversion is performed and evaluation values for conjunctival hyperemia captured in the left eye images.</p><p id="p-0318" num="0317">(2) Second Prediction Model</p><p id="p-0319" num="0318">Purpose and Operation of Second Prediction Model</p><p id="p-0320" num="0319">The second prediction model is a model for predicting whether there is conjunctival edema.</p><p id="p-0321" num="0320">The second prediction model may receive an eye image as input data and may output a probability value of the presence of conjunctival edema captured in the input eye image.</p><p id="p-0322" num="0321">When the second prediction model includes a second left eye prediction model and a second right eye prediction model, the second left eye prediction model may receive a left eye image and output a probability value of the presence of conjunctival edema captured in the left eye image, and the second right eye prediction model may receive a right eye image and output a probability value of the presence of conjunctival edema captured in the right eye image.</p><p id="p-0323" num="0322">When the second prediction model is not dualized and is realized as one model, the second prediction model may receive either a right eye image or a left eye image to output a probability value of the presence of conjunctival edema captured in the input image, and may receive the other image to output a probability value of the presence of conjunctival edema captured in the input image.</p><p id="p-0324" num="0323">The eye image may be an image preprocessed by the above-described preprocessing algorithms.</p><p id="p-0325" num="0324">For example, the eye image may be an image on which preprocessing according to second cropping is performed.</p><p id="p-0326" num="0325">As another example, the eye image may be an image on which preprocessing including second cropping and resizing is performed.</p><p id="p-0327" num="0326">As still another example, the eye image may be an image on which preprocessing including second cropping, first masking, and resizing is performed.</p><p id="p-0328" num="0327">As yet still another example, the eye image may be an image on which preprocessing including second cropping, first masking, lateral inversion, and resizing is performed.</p><p id="p-0329" num="0328">In the present specification, the second prediction model may be called a conjunctival edema prediction model.</p><p id="p-0330" num="0329">Training of Second Prediction Model</p><p id="p-0331" num="0330">To train the second prediction model, a plurality of training data sets may be prepared. A training data set may include an eye image and an evaluation value for the presence of conjunctival edema captured in the eye image. The eye image may be an image preprocessed by the above-described preprocessing algorithms. For example, the eye image may be an image on which preprocessing including second cropping, first masking, and resizing is performed.</p><p id="p-0332" num="0331">To train the second prediction model, an artificial intelligence model may be prepared.</p><p id="p-0333" num="0332">Examples of the artificial intelligence model may be a support-vector machine (SVM), Random Forest, Gradient Boosting Algorithm, ResNet, VGG, GoogLeNet, MobileNet, and Vision Transformer.</p><p id="p-0334" num="0333">Next, the eye images included in the prepared plurality of training data sets are input to the artificial intelligence model, and training is performed using the evaluation value corresponding to each of the input eye images and an output value output from the artificial intelligence model.</p><p id="p-0335" num="0334">When the second prediction model includes the second left eye prediction model and the second right eye prediction model, the plurality of training data sets for training the second left eye prediction model may include left eye images and evaluation values for the presence of conjunctival edema captured in the left eye images, and the plurality of training data sets for training the second right eye prediction model may include right eye images and evaluation values for the presence of conjunctival edema captured in the right eye images. In the meantime, in order to increase the number of training data sets, the plurality of training data sets for training the second left eye prediction model may include right eye images on which lateral inversion is processed and evaluation values for the presence of conjunctival edema captured in the right eye images, and the plurality of training data sets for training the second right eye prediction model may include left eye images on which lateral inversion is processed and evaluation values for the presence of conjunctival edema captured in the left eye images.</p><p id="p-0336" num="0335">When it is intended not to dualize the second prediction model, but to realize the second prediction model as one model, the plurality of training data sets may include right eye images and evaluation values for the presence of conjunctival edema captured in the right eye images, or may include left eye images on which lateral inversion is performed and evaluation values for the presence of conjunctival edema captured in the left eye images. Alternatively, the plurality of training data sets may include left eye images and evaluation values for the presence of conjunctival edema captured in the left eye images, or may include right eye images on which lateral inversion is performed and evaluation values for the presence of conjunctival edema captured in the right eye images.</p><p id="p-0337" num="0336">In the meantime, in training the second prediction model, in order to predict whether there is conjunctival edema without distinguishing between right eye images and left eye images, all right eye images, right eye images on which lateral inversion is performed, left eye images, and left eye images on which lateral inversion is performed are used as training data for training one model.</p><p id="p-0338" num="0337">For example, when the second prediction model includes the second left eye prediction model and the second right eye prediction model, the plurality of training data sets for training the second left eye prediction model may include: left eye images and evaluation values for conjunctival edema captured in the left eye images; and right eye images on which lateral inversion is performed and evaluation values for conjunctival edema captured in the right eye images, and the plurality of training data sets for training the second right eye prediction model may include: right eye images and evaluation values for conjunctival edema captured in the right eye images; and left eye images on which lateral inversion is performed and evaluation values for conjunctival edema captured in the left eye images.</p><p id="p-0339" num="0338">When it is intended not to dualize the second prediction model, but to realize the second prediction model as one model, the plurality of training data sets may include: right eye images and evaluation values for conjunctival edema captured in the right eye images; right eye images on which lateral inversion is performed and evaluation values for conjunctival edema captured in the right eye images; left eye images and evaluation values for conjunctival edema captured in the left eye images; and left eye images on which lateral inversion is performed and evaluation values for conjunctival edema captured in the left eye images.</p><p id="p-0340" num="0339">(3) Third Prediction Model</p><p id="p-0341" num="0340">Purpose and Operation of Third Prediction Model</p><p id="p-0342" num="0341">The third prediction model is a model for predicting whether there is lacrimal edema.</p><p id="p-0343" num="0342">The third prediction model may receive an eye image as input data and may output a probability value of the presence of lacrimal edema captured in the input eye image.</p><p id="p-0344" num="0343">When the third prediction model includes a third left eye prediction model and a third right eye prediction model, the third left eye prediction model may receive a left eye image and output a probability value of the presence of lacrimal edema captured in the left eye image, and the third right eye prediction model may receive a right eye image and output a probability value of the presence of lacrimal edema captured in the right eye image.</p><p id="p-0345" num="0344">When the third prediction model is not dualized and is realized as one model, the third prediction model may receive either a right eye image or a left eye image to output a probability value of the presence of lacrimal edema captured in the input image, and may receive the other image to output a probability value of the presence of lacrimal edema captured in the input image.</p><p id="p-0346" num="0345">The eye image may be an image preprocessed by the above-described preprocessing algorithms.</p><p id="p-0347" num="0346">For example, the eye image may be an image on which preprocessing according to second cropping is performed.</p><p id="p-0348" num="0347">As another example, the eye image may be an image on which preprocessing including second cropping and resizing is performed.</p><p id="p-0349" num="0348">As still another example, the eye image may be an image on which preprocessing including second cropping, first masking, and resizing is performed.</p><p id="p-0350" num="0349">As yet still another example, the eye image may be an image on which preprocessing including second cropping, first masking, lateral inversion, and resizing is performed.</p><p id="p-0351" num="0350">In the present specification, the third prediction model may be called a lacrimal edema prediction model.</p><p id="p-0352" num="0351">Training of Third Prediction Model</p><p id="p-0353" num="0352">To train the third prediction model, a plurality of training data sets may be prepared. A training data set may include an eye image and an evaluation value for the presence of lacrimal edema captured in the eye image. The eye image may be an image preprocessed by the above-described preprocessing algorithms. For example, the eye image may be an image on which preprocessing including second cropping, first masking, and resizing is performed.</p><p id="p-0354" num="0353">To train the third prediction model, an artificial intelligence model may be prepared.</p><p id="p-0355" num="0354">Examples of the artificial intelligence model may be a support-vector machine (SVM), Random Forest, Gradient Boosting Algorithm, ResNet, VGG, GoogLeNet, MobileNet, and Vision Transformer.</p><p id="p-0356" num="0355">Next, the eye images included in the prepared plurality of training data sets are input to the artificial intelligence model, and training is performed using the evaluation value corresponding to each of the input eye images and an output value output from the artificial intelligence model.</p><p id="p-0357" num="0356">When the third prediction model includes the third left eye prediction model and the third right eye prediction model, the plurality of training data sets for training the third left eye prediction model may include left eye images and evaluation values for the presence of lacrimal edema captured in the left eye images, and the plurality of training data sets for training the third right eye prediction model may include right eye images and evaluation values for the presence of lacrimal edema captured in the right eye images. In the meantime, in order to increase the number of training data sets, the plurality of training data sets for training the third left eye prediction model may include right eye images on which lateral inversion is processed and evaluation values for the presence of lacrimal edema captured in the right eye images, and the plurality of training data sets for training the third right eye prediction model may include left eye images on which lateral inversion is processed and evaluation values for the presence of lacrimal edema captured in the left eye images.</p><p id="p-0358" num="0357">When it is intended not to dualize the third prediction model, but to realize the third prediction model as one model, the plurality of training data sets may include right eye images and evaluation values for the presence of lacrimal edema captured in the right eye images, or may include left eye images on which lateral inversion is performed and evaluation values for the presence of lacrimal edema captured in the left eye images. Alternatively, the plurality of training data sets may include left eye images and evaluation values for the presence of lacrimal edema captured in the left eye images, or may include right eye images on which lateral inversion is performed and evaluation values for the presence of lacrimal edema captured in the right eye images.</p><p id="p-0359" num="0358">In the meantime, in training the third prediction model, in order to predict whether there is lacrimal edema without distinguishing between right eye images and left eye images, all right eye images, right eye images on which lateral inversion is performed, left eye images, and left eye images on which lateral inversion is performed are used as training data for training one model.</p><p id="p-0360" num="0359">For example, when the third prediction model includes the third left eye prediction model and the third right eye prediction model, the plurality of training data sets for training the third left eye prediction model may include: left eye images and evaluation values for lacrimal edema captured in the left eye images; and right eye images on which lateral inversion is performed and evaluation values for lacrimal edema captured in the right eye images, and the plurality of training data sets for training the third right eye prediction model may include: right eye images and evaluation values for lacrimal edema captured in the right eye images; and left eye images on which lateral inversion is performed and evaluation values for lacrimal edema captured in the left eye images.</p><p id="p-0361" num="0360">When it is intended not to dualize the third prediction model, but to realize the third prediction model as one model, the plurality of training data sets may include: right eye images and evaluation values for lacrimal edema captured in the right eye images; right eye images on which lateral inversion is performed and evaluation values for lacrimal edema captured in the right eye images; left eye images and evaluation values for lacrimal edema captured in the left eye images; and left eye images on which lateral inversion is performed, and evaluation values for lacrimal edema captured in the left eye images.</p><p id="p-0362" num="0361">(4) Fourth Prediction Model</p><p id="p-0363" num="0362">Purpose and Operation of Fourth Prediction Model</p><p id="p-0364" num="0363">The fourth prediction model is a model for predicting whether there is eyelid redness.</p><p id="p-0365" num="0364">The fourth prediction model may receive an eye image as input data and may output a probability value of the presence of eyelid redness captured in the input eye image.</p><p id="p-0366" num="0365">When the fourth prediction model includes a fourth left eye prediction model and a fourth right eye prediction model, the fourth left eye prediction model may receive a left eye image and output a probability value of the presence of eyelid redness captured in the left eye image, and the fourth right eye prediction model may receive a right eye image and output a probability value of the presence of eyelid redness captured in the right eye image.</p><p id="p-0367" num="0366">When the fourth prediction model is not dualized and is realized as one model, the fourth prediction model may receive either a right eye image or a left eye image to output a probability value of the presence of eyelid redness captured in the input image, and may receive the other image to output a probability value of the presence of eyelid redness captured in the input image.</p><p id="p-0368" num="0367">The eye image may be an image preprocessed by the above-described preprocessing algorithms.</p><p id="p-0369" num="0368">For example, the eye image may be an image on which preprocessing according to third cropping is performed.</p><p id="p-0370" num="0369">As another example, the eye image may be an image on which preprocessing including third cropping and resizing is performed.</p><p id="p-0371" num="0370">As still another example, the eye image may be an image on which preprocessing including third cropping, lateral inversion, and resizing is performed.</p><p id="p-0372" num="0371">As still another example, the eye image may be an image on which preprocessing including third cropping, second masking, and resizing is performed.</p><p id="p-0373" num="0372">As yet still another example, the eye image may be an image on which preprocessing including third cropping, second masking, lateral inversion, and resizing is performed.</p><p id="p-0374" num="0373">In the present specification, the fourth prediction model may be called an eyelid redness prediction model.</p><p id="p-0375" num="0374">Training of Fourth Prediction Model</p><p id="p-0376" num="0375">To train the fourth prediction model, a plurality of training data sets may be prepared. A training data set may include an eye image and an evaluation value for eyelid redness captured in the eye image. The eye image may be an image preprocessed by the above-described preprocessing algorithms. For example, the eye image may be an image on which preprocessing including second cropping, first masking, and resizing is performed.</p><p id="p-0377" num="0376">To train the fourth prediction model, an artificial intelligence model may be prepared.</p><p id="p-0378" num="0377">Examples of the artificial intelligence model may be a support-vector machine (SVM), Random Forest, Gradient Boosting Algorithm, ResNet, VGG, GoogLeNet, MobileNet, and Vision Transformer.</p><p id="p-0379" num="0378">Next, the eye images included in the prepared plurality of training data sets are input to the artificial intelligence model, and training is performed using the evaluation value corresponding to each of the input eye images and an output value output from the artificial intelligence model.</p><p id="p-0380" num="0379">When the fourth prediction model include the fourth left eye prediction model and the fourth right eye prediction model, the plurality of training data sets for training the fourth left eye prediction model may include left eye images and evaluation values for eyelid redness captured in the left eye images, and the plurality of training data sets for training the fourth right eye prediction model may include right eye images and evaluation values for eyelid redness captured in the right eye images. In the meantime, in order to increase the number of training data sets, the plurality of training data sets for training the fourth left eye prediction model may include right eye images on which lateral inversion is processed and evaluation values for eyelid redness captured in the right eye images, and the plurality of training data sets for training the fourth right eye prediction model may include left eye images on which lateral inversion is processed and evaluation values for eyelid redness captured in the left eye images.</p><p id="p-0381" num="0380">When it is intended not to dualize the fourth prediction model, but to realize the fourth prediction model as one model, the plurality of training data sets may include right eye images and evaluation values for eyelid redness captured in the right eye images, or may include left eye images on which lateral inversion is performed and evaluation values for eyelid redness captured in the left eye images. Alternatively, the plurality of training data sets may include left eye images and evaluation values for eyelid redness captured in the left eye images, or may include right eye images on which lateral inversion is performed and evaluation values for eyelid redness captured in the right eye images.</p><p id="p-0382" num="0381">In the meantime, in training the fourth prediction model, in order to predict whether there is eyelid redness without distinguishing between right eye images and left eye images, all right eye images, right eye images on which lateral inversion is performed, left eye images, and left eye images on which lateral inversion is performed are used as training data for training one model.</p><p id="p-0383" num="0382">For example, when the fourth prediction model includes the fourth left eye prediction model and the fourth right eye prediction model, the plurality of training data sets for training the fourth left eye prediction model may include: left eye images and evaluation values for eyelid redness captured in the left eye images; and right eye images on which lateral inversion is performed and evaluation values for eyelid redness captured in the right eye images, and the plurality of training data sets for training the fourth right eye prediction model may include: right eye images and evaluation values for eyelid redness captured in the right eye images; and left eye images on which lateral inversion is performed and evaluation values for eyelid redness captured in the left eye images.</p><p id="p-0384" num="0383">When it is intended not to dualize the fourth prediction model, but to realize the fourth prediction model as one model, the plurality of training data sets may include: right eye images and evaluation values for eyelid redness captured in the right eye images; right eye images on which lateral inversion is performed and evaluation values for eyelid redness captured in the right eye images; left eye images and evaluation values for eyelid redness captured in the left eye images; and left eye images on which lateral inversion is performed and evaluation values for eyelid redness captured in the left eye images.</p><p id="p-0385" num="0384">(5) Fifth Prediction Model</p><p id="p-0386" num="0385">Purpose and Operation of Fifth Prediction Model</p><p id="p-0387" num="0386">The fifth prediction model is a model for predicting whether there is eyelid edema.</p><p id="p-0388" num="0387">The fifth prediction model may receive an eye image as input data and may output a probability value of the presence of eyelid edema captured in the input eye image.</p><p id="p-0389" num="0388">When the fifth prediction model includes a fifth left eye prediction model and a fifth right eye prediction model, the fifth left eye prediction model may receive a left eye image and output a probability value of the presence of eyelid edema captured in the left eye image, and the fifth right eye prediction model may receive a right eye image and output a probability value of the presence of eyelid edema captured in the right eye image.</p><p id="p-0390" num="0389">When the fifth prediction model is not dualized and is realized as one model, the fifth prediction model may receive either a right eye image or a left eye image to output a probability value of the presence of eyelid edema captured in the input image, and may receive the other image to output a probability value of the presence of eyelid edema captured in the input image.</p><p id="p-0391" num="0390">The eye image may be an image preprocessed by the above-described preprocessing algorithms.</p><p id="p-0392" num="0391">For example, the eye image may be an image on which preprocessing according to third cropping is performed.</p><p id="p-0393" num="0392">As another example, the eye image may be an image on which preprocessing including third cropping and resizing is performed.</p><p id="p-0394" num="0393">As still another example, the eye image may be an image on which preprocessing including third cropping, lateral inversion, and resizing is performed.</p><p id="p-0395" num="0394">As still another example, the eye image may be an image on which preprocessing including third cropping, second masking, and resizing is performed.</p><p id="p-0396" num="0395">As yet still another example, the eye image may be an image on which preprocessing including third cropping, second masking, lateral inversion, and resizing is performed.</p><p id="p-0397" num="0396">In the present specification, the fifth prediction model may be called an eyelid edema prediction model.</p><p id="p-0398" num="0397">Training of Fifth Prediction Model</p><p id="p-0399" num="0398">To train the fifth prediction model, a plurality of training data sets may be prepared. A training data set may include an eye image and an evaluation value for the presence of eyelid edema captured in the eye image. The eye image may be an image preprocessed by the above-described preprocessing algorithms. For example, the eye image may be an image on which preprocessing including third cropping, second masking, and resizing is performed.</p><p id="p-0400" num="0399">To train the fifth prediction model, an artificial intelligence model may be prepared.</p><p id="p-0401" num="0400">Examples of the artificial intelligence model may be a support-vector machine (SVM), Random Forest, Gradient Boosting Algorithm, ResNet, VGG, GoogLeNet, MobileNet, and Vision Transformer.</p><p id="p-0402" num="0401">Next, the eye images included in the prepared plurality of training data sets are input to the artificial intelligence model, and training is performed using the evaluation value corresponding to each of the input eye images and an output value output from the artificial intelligence model.</p><p id="p-0403" num="0402">When the fifth prediction model includes the fifth left eye prediction model and the fifth right eye prediction model, the plurality of training data sets for training the fifth left eye prediction model may include left eye images and evaluation values for the presence of eyelid edema captured in the left eye images, and the plurality of training data sets for training the fifth right eye prediction model may include right eye images and evaluation values for the presence of eyelid edema captured in the right eye images. In the meantime, in order to increase the number of training data sets, the plurality of training data sets for training the fifth left eye prediction model may include right eye images on which lateral inversion is processed and evaluation values for the presence of eyelid edema captured in the right eye images, and the plurality of training data sets for training the fifth right eye prediction model may include left eye images on which lateral inversion is processed and evaluation values for the presence of eyelid edema captured in the left eye images.</p><p id="p-0404" num="0403">When it is intended not to dualize the fifth prediction model, but to realize the fifth prediction model as one model, the plurality of training data sets may include right eye images and evaluation values for the presence of eyelid edema captured in the right eye images, or may include left eye images on which lateral inversion is performed and evaluation values for the presence of eyelid edema captured in the left eye images. Alternatively, the plurality of training data sets may include left eye images and evaluation values for the presence of eyelid edema captured in the left eye images, or may include right eye images on which lateral inversion is performed and evaluation values for the presence of eyelid edema captured in the right eye images.</p><p id="p-0405" num="0404">In the meantime, in training the fifth prediction model, in order to predict whether there is eyelid edema without distinguishing between right eye images and left eye images, all right eye images, right eye images on which lateral inversion is performed, left eye images, and left eye images on which lateral inversion is performed are used as training data for training one model.</p><p id="p-0406" num="0405">For example, when the fifth prediction model includes the fifth left eye prediction model and the fifth right eye prediction model, the plurality of training data sets for training the fifth left eye prediction model may include: left eye images and evaluation values for eyelid edema captured in the left eye images; and right eye images on which lateral inversion is performed and evaluation values for eyelid edema captured in the right eye images, and the plurality of training data sets for training the fifth right eye prediction model may include: right eye images and evaluation values for eyelid edema captured in the right eye images; and left eye images on which lateral inversion is performed and evaluation values for eyelid edema captured in the left eye images.</p><p id="p-0407" num="0406">When it is intended not to dualize the fifth prediction model, but to realize the fifth prediction model as one model, the plurality of training data sets may include: right eye images and evaluation values for eyelid edema captured in the right eye images; right eye images on which lateral inversion is performed and evaluation values for eyelid edema captured in the right eye images; left eye images and evaluation values for eyelid edema captured in the left eye images; and left eye images on which lateral inversion is performed and evaluation values for eyelid edema captured in the left eye images.</p><p id="p-0408" num="0407">The training of the prediction models may be performed by an electronic device, and in particular, may be performed by the server <b>20</b> described above. Furthermore, the training of the prediction models by the electronic device or the server <b>20</b> means a series of processes for enabling output values of the prediction models for input data to be values similar to output values labelled with the input data. To this end, the electronic device or the server <b>20</b> may use the differences between the output values of the prediction models and the labelled values to change a weight value of each of the nodes included in the prediction models. Herein, the electronic device or the server <b>20</b> may determine the amount of change in the weight value of each of the nodes by using various feedback functions.</p><p id="p-0409" num="0408">Hereinafter, the following methods through the above-described system <b>1</b> will be described: a method of predicting each symptom related to a clinical activity score for thyroid eye disease by preprocessing an eye image and inputting the preprocessed eye image to the above-described prediction models; a method of predicting a clinical activity score on the basis of a prediction result for each symptom; and a method of monitoring a prediction result of a clinical activity score and giving guidance or recommendation according to the monitored result so that a user visits the hospital and has a medical examination.</p><p id="p-0410" num="0409">5. Conjunctival Hyperemia Prediction Method</p><p id="p-0411" num="0410">The conjunctival hyperemia prediction method described in the present application may be performed by the server <b>20</b>.</p><p id="p-0412" num="0411"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a flowchart illustrating a conjunctival hyperemia prediction method.</p><p id="p-0413" num="0412">Referring to <figref idref="DRAWINGS">FIG. <b>23</b></figref>, the server <b>20</b> acquires a facial image in step S<b>100</b>, preprocesses the acquired facial image in step S<b>110</b>, inputs the preprocessed image to the above-described first prediction model (conjunctival hyperemia prediction model) in step S<b>120</b>, and acquires an output value of the first prediction model in step S<b>130</b>.</p><p id="p-0414" num="0413">Acquisition of Facial Image</p><p id="p-0415" num="0414">The server <b>20</b> acquires a facial image in step S<b>100</b>. The server <b>20</b> may acquire the facial image from the user terminal <b>10</b>.</p><p id="p-0416" num="0415">Preprocessing of Facial Image</p><p id="p-0417" num="0416">The server <b>20</b> may preprocess the acquired facial image in step S<b>110</b>. The server <b>20</b> may perform, on the acquired facial image, iris segmentation, eye outline segmentation, masking, cropping, and resizing, which are described above.</p><p id="p-0418" num="0417">Segmentation Processing</p><p id="p-0419" num="0418">The server <b>20</b> may perform the iris segmentation and the eye outline segmentation, and the server <b>20</b> may thus determine the pixels corresponding to the iris and the pixels corresponding to the inside of the outline of the eye within the acquired facial image. The server <b>20</b> may determine the coordinate values of the pixels corresponding to the iris and the coordinate values of the pixels corresponding to the inside of the outline of the eye.</p><p id="p-0420" num="0419">Masking Processing</p><p id="p-0421" num="0420">The server <b>20</b> may perform the first masking on the facial image on the basis of information on the determined pixels. Through the first masking processing, the server <b>20</b> may remove the pixel values of the pixels excluding the pixels corresponding to the conjunctiva exposed to the outside and the lacrimal caruncle among the pixels included in the facial image. Accordingly, the pixel values of the pixels corresponding to the conjunctiva and lacrimal caruncle of the left eye and to the conjunctiva and lacrimal caruncle of the right eye may remain to be the original pixel values, but the pixel values of the pixels corresponding to the iris (or cornea) of the left eye, to the iris (or cornea) of the right eye, to the outside of the outline of the left eye, and to the outside of the outline of the right eye may be removed or changed to other values.</p><p id="p-0422" num="0421">Cropping Processing</p><p id="p-0423" num="0422">The server <b>20</b> may crop the masked facial image. The server <b>20</b> may crop the masked facial image to generate a left eye cropped image and a right eye cropped image. In performing the conjunctival hyperemia prediction method, the server <b>20</b> may use the second cropping (eye outline cropping) method among the cropping methods described above. Since the second cropping method has already been described in detail, a detailed description thereof will be omitted here.</p><p id="p-0424" num="0423">Resizing Processing and Lateral Inversion Processing</p><p id="p-0425" num="0424">The server <b>20</b> may resize the size of the left eye cropped image and the size of the right eye cropped image to a predetermined size.</p><p id="p-0426" num="0425">In the meantime, when the first prediction model is not dualized, but realized as one model, the server <b>20</b> may laterally invert either the left eye cropped image or the right eye cropped image as described above. The server <b>20</b> does not laterally invert the other among the left eye cropped image and the right eye cropped image. Herein, it is determined that the criterion for determining which one of the left eye image and the right eye image is subjected to lateral inversion is the same as the criterion applied when the first prediction model is trained. That is, in training the first prediction model, when the left eye image is inverted and the right eye image is not inverted, the server <b>20</b> inverts the left eye image and does not invert the right eye image, similarly.</p><p id="p-0427" num="0426">As described above, in realizing the first prediction model, when the first prediction model is dualized to the first left eye prediction model and the first right eye prediction model, the server <b>20</b> may not perform lateral inversion processing.</p><p id="p-0428" num="0427">In the meantime, it has been described that when preprocessing is performed, segmentation, masking processing, cropping processing, resizing processing, and lateral inversion processing are performed, but the sequence of these types of preprocessing may be changed within a range capable of achieving the purpose of the conjunctival hyperemia prediction method disclosed in the present application.</p><p id="p-0429" num="0428">Input of Preprocessed Image</p><p id="p-0430" num="0429">The server <b>20</b> may input the preprocessed image to the first prediction model in step S<b>120</b>.</p><p id="p-0431" num="0430">When the first prediction model is not dualized and is realized as one model, the server <b>20</b> inputs the right eye preprocessed image and the laterally inverted left eye preprocessed image to the first prediction model in order.</p><p id="p-0432" num="0431">In realizing the first prediction model, when the first prediction model is dualized to the first left eye prediction model and the first right eye prediction model, the server <b>20</b> inputs the left eye preprocessed image to the first left eye prediction model and inputs the right eye preprocessed image to the first right eye prediction model. Alternatively, the server <b>20</b> may input the left eye preprocessed image to the first left eye prediction model, may input the laterally inverted left eye preprocessed image to the first right eye prediction model, may input the right eye preprocessed image to the first right eye prediction model, and may input the laterally inverted right eye preprocessed image to the first left eye prediction model.</p><p id="p-0433" num="0432">In realizing the first prediction model, when the first prediction model is not dualized and is realized as one model and, simultaneously, is trained to be capable of determining whether there is conjunctival hyperemia without distinguishing between a left eye image and a right eye image, the server <b>20</b> may input the left eye preprocessed image and the right eye preprocessed image to the first prediction model without lateral inversion. Alternatively, the server <b>20</b> may input the left eye preprocessed image, the laterally inverted left eye preprocessed image, the right eye preprocessed image, and the laterally inverted right eye preprocessed image to the first prediction model.</p><p id="p-0434" num="0433">Conjunctival Hyperemia Prediction Result</p><p id="p-0435" num="0434">The server <b>20</b> may output a result value output from the first prediction model in step S<b>130</b>. The result value may be a probability value that is predicted with respect to conjunctival hyperemia captured in an image. On the basis of a predetermined threshold value, when the predicted probability value is equal to or greater than the threshold value, the server <b>20</b> determines that the conjunctiva is hyperemic, or when the predicted probability value is less than the threshold value, the server <b>20</b> determines that the conjunctiva is not hyperemic.</p><p id="p-0436" num="0435">The server <b>20</b> may acquire both a prediction result for the left eye and a prediction result for the right eye.</p><p id="p-0437" num="0436">When the server <b>20</b> inputs the left eye preprocessed image to the first left eye prediction model, inputs the laterally inverted left eye preprocessed image to the first right eye prediction model, inputs the right eye preprocessed image to the first right eye prediction model, and inputs the laterally inverted right eye preprocessed image to the first left eye prediction model, the server <b>20</b> may obtain a prediction result for the left eye considering both a result obtained by inputting the left eye preprocessed image to the first left eye prediction model and a result obtained by inputting the laterally inverted left eye preprocessed image to the first right eye prediction model. Herein, the server <b>20</b> may obtain a prediction result for the right eye considering both a result obtained by inputting the right eye preprocessed image to the first right eye prediction model and a result obtained by inputting the laterally inverted right eye preprocessed image to the first left eye prediction model.</p><p id="p-0438" num="0437">For example, the server <b>20</b> may obtain a prediction result for the left eye on the basis of whether an average value of the result obtained by inputting the left eye preprocessed image to the first left eye prediction model and the result obtained by inputting the laterally inverted left eye preprocessed image to the first right eye prediction model is equal to or greater than the threshold value.</p><p id="p-0439" num="0438">As another example, when a value of either the result obtained by inputting the left eye preprocessed image to the first left eye prediction model or the result obtained by inputting the laterally inverted left eye preprocessed image to the first right eye prediction model is equal to or greater than the above-described threshold value, the server <b>20</b> may predict that the conjunctiva of the left eye is hyperemic.</p><p id="p-0440" num="0439">As still another example, when both the result obtained by inputting the left eye preprocessed image to the first left eye prediction model and the result obtained by inputting the laterally inverted left eye preprocessed image to the first right eye prediction model are equal to or greater than the above-described threshold value, the server <b>20</b> may predict that the conjunctiva of the left eye is hyperemic.</p><p id="p-0441" num="0440">When the server <b>20</b> inputs the left eye preprocessed image, the laterally inverted left eye preprocessed image, the right eye preprocessed image, and the laterally inverted right eye preprocessed image to the first prediction model that is not dualized, the server <b>20</b> may obtain a prediction result for the left eye considering both a result obtained by inputting the left eye preprocessed image to the first prediction model and a result obtained by inputting the laterally inverted left eye preprocessed image to the first prediction model. Herein, the server <b>20</b> may obtain a prediction result for the right eye considering both a result obtained by inputting the right eye preprocessed image to the first prediction model and a result obtained by inputting the laterally inverted right eye preprocessed image to the first prediction model.</p><p id="p-0442" num="0441">For example, the server <b>20</b> may obtain a prediction result for the left eye on the basis of whether an average value of the result obtained by inputting the left eye preprocessed image to the first prediction model and the result obtained by inputting the laterally inverted left eye preprocessed image to the first prediction model is equal to or greater than the threshold value.</p><p id="p-0443" num="0442">As another example, when a value of either the result obtained by inputting the left eye preprocessed image to the first prediction model or the result obtained by inputting the laterally inverted left eye preprocessed image to the first prediction model is equal to or greater than the above-described threshold value, the server <b>20</b> may predict that the conjunctiva of the left eye is hyperemic.</p><p id="p-0444" num="0443">As still another example, when both the result obtained by inputting the left eye preprocessed image to the first prediction model and the result obtained by inputting the laterally inverted left eye preprocessed image to the first prediction model are equal to or greater than the above-described threshold value, the server <b>20</b> may predict that the conjunctiva of the left eye is hyperemic.</p><p id="p-0445" num="0444">The above-described method may be similarly applied in determining whether there is conjunctival hyperemia of the right eye.</p><p id="p-0446" num="0445">6. Conjunctival Edema Prediction Method</p><p id="p-0447" num="0446">The conjunctival edema prediction method described in the present application may be performed by the server <b>20</b>.</p><p id="p-0448" num="0447"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a flowchart illustrating a conjunctival edema prediction method.</p><p id="p-0449" num="0448">Referring to <figref idref="DRAWINGS">FIG. <b>24</b></figref>, the server <b>20</b> acquires a facial image in step S<b>200</b>, preprocesses the acquired facial image in step S<b>210</b>, inputs the preprocessed image to the above-describe second prediction model (conjunctival edema prediction model) in step S<b>220</b>, and acquires an output value of the second prediction model in step S<b>230</b>.</p><p id="p-0450" num="0449">The conjunctival edema prediction method is the same as or very similar to the conjunctival hyperemia prediction method except that the second prediction model is used instead of the first prediction model and a finally acquired result value is a predicted value for whether there is conjunctival edema, so a detailed description of the conjunctival edema prediction method will be omitted.</p><p id="p-0451" num="0450">7. Lacrimal Edema Prediction Method</p><p id="p-0452" num="0451">The lacrimal edema prediction method described in the present application may be performed by the server <b>20</b>.</p><p id="p-0453" num="0452"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a flowchart illustrating a lacrimal edema prediction method.</p><p id="p-0454" num="0453">Referring to <figref idref="DRAWINGS">FIG. <b>25</b></figref>, the server <b>20</b> acquires a facial image in step S<b>300</b>, preprocesses the acquired facial image in step S<b>310</b>, inputs the preprocessed image to the above-described third prediction model (lacrimal edema prediction model) in step S<b>320</b>, and acquires an output value of the third prediction model in step S<b>330</b>.</p><p id="p-0455" num="0454">The lacrimal edema prediction method is the same as or very similar to the conjunctival hyperemia prediction method except that the third prediction model is used instead of the first prediction model, so a detailed description of the conjunctival edema prediction method will be omitted.</p><p id="p-0456" num="0455">As described above, the conjunctival hyperemia prediction method, the conjunctival edema prediction method, and the lacrimal edema prediction method use the same image preprocessing methods, but only the prediction models to which the preprocessed images are input are different from each other. Therefore, after image preprocessing as described above, the images may be input to different prediction models.</p><p id="p-0457" num="0456">However, it has been described that the lacrimal edema prediction method uses the same image preprocessing methods as the conjunctival hyperemia and conjunctival edema prediction methods, but in some cases, the lacrimal edema prediction method may use the images preprocessed in a different way. For example, a preprocessed image may be used that is cropped such that the image includes a lacrimal caruncle and part of an iris. Alternatively, a preprocessed image may be used that is cropped such that the image does not include an iris, but includes a lacrimal caruncle.</p><p id="p-0458" num="0457">8. Eyelid Redness Prediction Method</p><p id="p-0459" num="0458">The eyelid redness prediction method described in the present application may be performed by the server <b>20</b>.</p><p id="p-0460" num="0459"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a flowchart illustrating an eyelid redness prediction method.</p><p id="p-0461" num="0460">Referring to <figref idref="DRAWINGS">FIG. <b>26</b></figref>, the server <b>20</b> acquires a facial image in step S<b>400</b>, preprocesses the acquired facial image in step S<b>410</b>, inputs the preprocessed image to the above-described fourth prediction model (eyelid redness prediction model) in step S<b>420</b>, and acquires an output value of the fourth prediction model in step S<b>430</b>.</p><p id="p-0462" num="0461">Acquisition of Facial Image</p><p id="p-0463" num="0462">The server <b>20</b> acquires a facial image in step S<b>400</b>. The server <b>20</b> may acquire the facial image from the user terminal <b>10</b>.</p><p id="p-0464" num="0463">Preprocessing of Facial Image</p><p id="p-0465" num="0464">The server <b>20</b> may preprocess the acquired facial image in step S<b>410</b>. The server <b>20</b> may perform, on the acquired facial image, iris segmentation, eye outline segmentation, masking, cropping, and resizing, which are described above.</p><p id="p-0466" num="0465">Segmentation Processing</p><p id="p-0467" num="0466">The server <b>20</b> may perform the iris segmentation and the eye outline segmentation, and the server <b>20</b> may thus determine the pixels corresponding to the iris and the pixels corresponding to the inside of the outline of the eye within the acquired facial image. The server <b>20</b> may determine the coordinate values of the pixels corresponding to the iris and the coordinate values of the pixels corresponding to the inside of the outline of the eye.</p><p id="p-0468" num="0467">However, in performing the eyelid redness prediction method, as described later, iris segmentation needs to be performed when masking processing is performed, but it is allowed not to perform iris segmentation when masking processing is not performed.</p><p id="p-0469" num="0468">Masking Processing</p><p id="p-0470" num="0469">The server <b>20</b> may perform the second masking on the facial image on the basis of information on the determined pixels. Through the second masking processing, the server <b>20</b> may remove the pixel values of the pixels corresponding to the iris (cornea) exposed to the outside among the pixels included in the facial image. Accordingly, the pixel values of the pixels corresponding to the region excluding the iris (cornea) of the left eye and the iris (cornea) of the right eye may remain to be the original pixel values, but the pixel values of the pixels corresponding to the iris (or cornea) of the left eye and the iris (or cornea) of the right eye may be removed or changed to other values.</p><p id="p-0471" num="0470">However, in performing the eyelid redness prediction method, performing preprocessing of masking an iris (cornea) is advantageous in several aspects, but it is allowed not to perform masking on an iris.</p><p id="p-0472" num="0471">Cropping Processing</p><p id="p-0473" num="0472">The server <b>20</b> may crop the masked facial image. The server <b>20</b> may crop the masked facial image to generate a left eye cropped image and a right eye cropped image. In performing the eyelid redness prediction method, the server <b>20</b> may use the third cropping (eyelid-included cropping) method among the cropping methods described above. Since the third cropping method has already been described in detail, a detailed description thereof will be omitted here.</p><p id="p-0474" num="0473">Resizing Processing and Lateral Inversion Processing</p><p id="p-0475" num="0474">The server <b>20</b> may resize the size of the left eye cropped image and size of the right eye cropped image to a predetermined size.</p><p id="p-0476" num="0475">In the meantime, when the fourth prediction model is not dualized, but realized as one model, the server <b>20</b> may laterally invert either the left eye cropped image or the right eye cropped image as described above. The server <b>20</b> does not laterally invert the other among the left eye cropped image and the right eye cropped image. Herein, it is determined that the criterion for determining which one of the left eye image and the right eye image is subjected to lateral inversion is the same as the criterion applied when the fourth prediction model is trained. That is, in training the fourth prediction model, when the left eye image is inverted and the right eye image is not inverted, the server <b>20</b> inverts the left eye image and does not invert the right eye image, similarly.</p><p id="p-0477" num="0476">As described above, in realizing the fourth prediction model, when the fourth prediction model is dualized to the fourth left eye prediction model and the fourth right eye prediction model, the server <b>20</b> may not perform lateral inversion processing.</p><p id="p-0478" num="0477">In the meantime, it has been described that when preprocessing is performed, segmentation, masking processing, cropping processing, resizing processing, and lateral inversion processing are performed, but the sequence of these types of preprocessing may be changed within a range capable of achieving the purpose of the eyelid redness prediction method disclosed in the present application.</p><p id="p-0479" num="0478">Input of Preprocessed Image</p><p id="p-0480" num="0479">The server <b>20</b> may input the preprocessed image to the fourth prediction model in step S<b>420</b>.</p><p id="p-0481" num="0480">When the fourth prediction model is not dualized and is realized as one model, the server <b>20</b> inputs the right eye preprocessed image and the laterally inverted left eye preprocessed image to the fourth prediction model in order.</p><p id="p-0482" num="0481">In realizing the fourth prediction model, when the fourth prediction model is dualized to the fourth left eye prediction model and the fourth right eye prediction model, the server <b>20</b> inputs the left eye preprocessed image to the fourth left eye prediction model and inputs the right eye preprocessed image to the fourth right eye prediction model. Alternatively, the server <b>20</b> may input the left eye preprocessed image to the fourth left eye prediction model, may input the laterally inverted left eye preprocessed image to the fourth right eye prediction model, may input the right eye preprocessed image to the fourth right eye prediction model, and may input the laterally inverted right eye preprocessed image to the fourth left eye prediction model.</p><p id="p-0483" num="0482">In realizing the fourth prediction model, when the fourth prediction model is not dualized and is realized as one model and, simultaneously, is trained to be capable of determining whether there is redness of eyelid without distinguishing between a left eye image and a right eye image, the server <b>20</b> may input the left eye preprocessed image and the right eye preprocessed image to the fourth prediction model without lateral inversion. Alternatively, the server <b>20</b> may input the left eye preprocessed image, the laterally inverted left eye preprocessed image, the right eye preprocessed image, and the laterally inverted right eye preprocessed image to the fourth prediction model.</p><p id="p-0484" num="0483">Eyelid Redness Prediction Result</p><p id="p-0485" num="0484">The server <b>20</b> may output a result value output from the fourth prediction model in step S<b>430</b>. The result value may be a probability value that is predicted with respect to eyelid redness captured in an image. On the basis of a predetermined threshold value, when the predicted probability value is equal to or greater than the threshold value, the server <b>20</b> determines that there is eyelid redness, or when the predicted probability value is less than the threshold value, the server <b>20</b> determines that there is no eyelid redness.</p><p id="p-0486" num="0485">The server <b>20</b> may acquire both a prediction result for the left eye and a prediction result for the right eye.</p><p id="p-0487" num="0486">When the server <b>20</b> inputs the left eye preprocessed image to the fourth left eye prediction model, inputs the laterally inverted left eye preprocessed image to the fourth right eye prediction model, inputs the right eye preprocessed image to the fourth right eye prediction model, and inputs the laterally inverted right eye preprocessed image to the fourth left eye prediction model, the server <b>20</b> may obtain a prediction result for the left eye considering both a result obtained by inputting the left eye preprocessed image to the fourth left eye prediction model and a result obtained by inputting the laterally inverted left eye preprocessed image to the fourth right eye prediction model. Herein, the server <b>20</b> may obtain a prediction result for the right eye considering both a result obtained by inputting the right eye preprocessed image to the fourth right eye prediction model and a result obtained by inputting the laterally inverted right eye preprocessed image to the fourth left eye prediction model.</p><p id="p-0488" num="0487">For example, the server <b>20</b> may obtain a prediction result for the left eye on the basis of whether an average value of the result obtained by inputting the left eye preprocessed image to the fourth left eye prediction model and the result obtained by inputting the laterally inverted left eye preprocessed image to the fourth right eye prediction model is equal to or greater than the threshold value.</p><p id="p-0489" num="0488">As another example, when a value of either the result obtained by inputting the left eye preprocessed image to the fourth left eye prediction model or the result obtained by inputting the laterally inverted left eye preprocessed image to the fourth right eye prediction model is equal to or greater than the above-described threshold value, the server <b>20</b> may predict that the eyelid redness is present in the left eye.</p><p id="p-0490" num="0489">As still another example, when both the result obtained by inputting the left eye preprocessed image to the fourth left eye prediction model and the result obtained by inputting the laterally inverted left eye preprocessed image to the fourth right eye prediction model are equal to or greater than the above-described threshold value, the server <b>20</b> may predict that the conjunctiva of the left eye is hyperemic.</p><p id="p-0491" num="0490">When the server <b>20</b> inputs the left eye preprocessed image, the laterally inverted left eye preprocessed image, the right eye preprocessed image, and the laterally inverted right eye preprocessed image to the fourth prediction model that is not dualized, the server <b>20</b> may obtain a prediction result for the left eye considering both a result obtained by inputting the left eye preprocessed image to the fourth prediction model and a result obtained by inputting the laterally inverted left eye preprocessed image to the fourth prediction model. Herein, the server <b>20</b> may obtain a prediction result for the right eye considering both a result obtained by inputting the right eye preprocessed image to the fourth prediction model and a result obtained by inputting the laterally inverted right eye preprocessed image to the fourth prediction model.</p><p id="p-0492" num="0491">For example, the server <b>20</b> may obtain a prediction result for the left eye on the basis of whether an average value of the result obtained by inputting the left eye preprocessed image to the fourth prediction model and the result obtained by inputting the laterally inverted left eye preprocessed image to the fourth prediction model is equal to or greater than the threshold value.</p><p id="p-0493" num="0492">As another example, when a value of either the result obtained by inputting the left eye preprocessed image to the fourth prediction model or the result obtained by inputting the laterally inverted left eye preprocessed image to the fourth prediction model is equal to or greater than the above-described threshold value, the server <b>20</b> may predict that there is eyelid redness of the left eye.</p><p id="p-0494" num="0493">As still another example, when both the result obtained by inputting the left eye preprocessed image to the fourth prediction model and the result obtained by inputting the laterally inverted left eye preprocessed image to the fourth prediction model are equal to or greater than the above-described threshold value, the server <b>20</b> may predict that there is eyelid redness of the left eye.</p><p id="p-0495" num="0494">The above-described method may be similarly applied in determining whether there is eyelid redness of the right eye.</p><p id="p-0496" num="0495">9. Eyelid Edema Prediction Method</p><p id="p-0497" num="0496">The eyelid edema prediction method described in the present application may be performed by the server <b>20</b>.</p><p id="p-0498" num="0497"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a flowchart illustrating an eyelid edema prediction method.</p><p id="p-0499" num="0498">Referring to <figref idref="DRAWINGS">FIG. <b>27</b></figref>, the server <b>20</b> acquires a facial image in step S<b>500</b>, preprocesses the acquired facial image in step S<b>510</b>, inputs the preprocessed image to the above-described fifth prediction model (eyelid edema prediction model) in step S<b>520</b>, and acquires an output value of the fifth prediction model in step S<b>530</b>.</p><p id="p-0500" num="0499">The eyelid edema prediction method is the same as or very similar to the eyelid redness prediction method except that the fifth prediction model is used instead of the fourth prediction model and a finally acquired result value is a predicted value for whether there is eyelid edema, so a detailed description of the eyelid edema prediction method will be omitted.</p><p id="p-0501" num="0500">As described above, the eyelid redness prediction method and the eyelid edema prediction method use the same image preprocessing methods, but only the prediction models to which the preprocessed images are input are different from each other. Therefore, after image preprocessing as described above, the images may be input to different prediction models.</p><p id="p-0502" num="0501">10. Method of Predicting Clinical Activity Score for Thyroid Eye Disease</p><p id="p-0503" num="0502">Hereinafter, described will be a method, which is described in the present application, of predicting a clinical activity score for thyroid eye disease.</p><p id="p-0504" num="0503"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is a diagram illustrating a method of predicting a clinical activity score for thyroid eye disease.</p><p id="p-0505" num="0504">The server <b>20</b> may acquire a facial image.</p><p id="p-0506" num="0505">The server <b>20</b> performs two different types of preprocessing on one facial image. First preprocessing (hereinafter, first preprocessing) includes iris segmentation, eye outline segmentation, first masking, second cropping (eye outline cropping), resizing, and lateral inversion, and second preprocessing (hereinafter, second preprocessing) includes iris segmentation, eye outline segmentation, second masking, third cropping (eyelid-included cropping), resizing, and lateral inversion. However, as described in the eyelid redness prediction method, iris segmentation and second masking may be omitted.</p><p id="p-0507" num="0506">The server <b>20</b> acquires a first preprocessed image by performing first preprocessing on the acquired facial image, and the first preprocessed image includes a first left eye preprocessed image and a first right eye preprocessed image. Herein, either the first left eye preprocessed image or the first right eye preprocessed image is an image on which lateral inversion is processed. Furthermore, as already described in detail, since the first preprocessed image is an image obtained using second cropping, the number of pixels corresponding to the eyelids within the first preprocessed image is minimized and the pixels corresponding to the conjunctivas exposed to the outside and the lacrimal caruncles are included. Furthermore, the first preprocessed image is an image obtained using first masking, the pixel values of pixels corresponding to irises (or corneas) and eyelids (upper eyelid and lower eyelid) are removed, but the pixel values of pixels corresponding to the conjunctivas exposed to the outside and the lacrimal caruncles remain.</p><p id="p-0508" num="0507">In addition, the server <b>20</b> acquires a second preprocessed image by performing second preprocessing on the acquired facial image, and the second preprocessed image includes a second left eye preprocessed image and a second right eye preprocessed image. Herein, either the second left eye preprocessed image or the second right eye preprocessed image is an image on which lateral inversion is performed. Furthermore, as already described in detail, since the second preprocessed image is an image obtained by using third cropping, the second preprocessed image includes sufficient pixels corresponding to eyelids. Furthermore, when the second masking method is used to obtain the second preprocessed image, the pixel values of the pixels corresponding to the irises (or corneas) and eyelids (upper eyelid and lower eyelid) may be removed.</p><p id="p-0509" num="0508">The server <b>20</b> inputs the first preprocessed image (the first left eye preprocessed image and the first right eye preprocessed image) to the first prediction model in order. The server <b>20</b> obtains a result value (probability value) of the first prediction model for the first left eye preprocessed image, and determines, on the basis of the result value, whether there is conjunctival hyperemia of the left eye. In addition, the server <b>20</b> obtains a result value (probability value) of the first prediction model for the first right eye preprocessed image, and determines, on the basis of the result value, whether there is conjunctival hyperemia of the right eye.</p><p id="p-0510" num="0509">The server <b>20</b> synthesizes a determination result for the left eye and a determination result for the right eye, and finally determines whether there is conjunctival hyperemia of both eyes. For example, when determining that there is conjunctival hyperemia of either the left eye or the right eye or both, the server <b>20</b> finally determines that there is conjunctival hyperemia.</p><p id="p-0511" num="0510">Next, the server <b>20</b> inputs the first preprocessed image (the first left eye preprocessed image and the first right eye preprocessed image) to the second prediction model in order. The server <b>20</b> obtains a result value (probability value) of the second prediction model for the first left eye preprocessed image, and determines, on the basis of the result value, whether there is conjunctival edema of the left eye. In addition, the server <b>20</b> obtains a result value (probability value) of the second prediction model for the first right eye preprocessed image, and determines, on the basis of the result value, whether there is conjunctival edema of the right eye.</p><p id="p-0512" num="0511">The server <b>20</b> synthesizes a determination result for the left eye and a determination result for the right eye, and finally determines whether there is conjunctival edema of both eyes. For example, when determining that there is conjunctival edema of either the left eye or the right eye or both, the server <b>20</b> finally determines that there is conjunctival edema.</p><p id="p-0513" num="0512">Next, the server <b>20</b> inputs the first preprocessed image (the first left eye preprocessed image and the first right eye preprocessed image) to the third prediction model in order. The server <b>20</b> obtains a result value (probability value) of the third prediction model for the first left eye preprocessed image, and determines, on the basis of the result value, whether there is lacrimal edema of the left eye. In addition, the server <b>20</b> obtains a result value (probability value) of the third prediction model for the first right eye preprocessed image, and determines, on the basis of the result value, whether there is lacrimal edema of the right eye.</p><p id="p-0514" num="0513">The server <b>20</b> synthesizes a determination result for the left eye and a determination result for the right eye, and finally determines whether there is lacrimal edema of both eyes. For example, when determining that there is lacrimal edema of either the left eye or the right eye or both, the server <b>20</b> finally determines that there is lacrimal edema.</p><p id="p-0515" num="0514">The server <b>20</b> inputs the second preprocessed image (the second left eye preprocessed image and the second right eye preprocessed image) to the fourth prediction model in order. The server <b>20</b> obtains a result value (probability value) of the fourth prediction model for the second left eye preprocessed image, and determines, on the basis of the result value, whether there is eyelid redness of the left eye. In addition, the server <b>20</b> obtains a result value (probability value) of the fourth prediction model for the second right eye preprocessed image, and determines, on the basis of the result value, whether there is eyelid redness of the right eye.</p><p id="p-0516" num="0515">The server <b>20</b> synthesizes a determination result for the left eye and a determination result for the right eye, and finally determines whether there is eyelid redness of both eyes. For example, when determining that there is eyelid redness of either the left eye or the right eye or both, the server <b>20</b> finally determines that there is eyelid redness.</p><p id="p-0517" num="0516">The server <b>20</b> inputs the second preprocessed image (the second left eye preprocessed image and the second right eye preprocessed image) to the fifth prediction model in order. The server <b>20</b> obtains a result value (probability value) of the fifth prediction model for the second left eye preprocessed image, and determines, on the basis of the result value, whether there is eyelid edema of the left eye. In addition, the server <b>20</b> obtains a result value (probability value) of the fifth prediction model for the second right eye preprocessed image, and determines, on the basis of the result value, whether there is eyelid edema of the right eye.</p><p id="p-0518" num="0517">The server <b>20</b> synthesizes a determination result for the left eye and a determination result for the right eye, and finally determines whether there is eyelid edema of both eyes. For example, when determining that there is eyelid edema of either the left eye or the right eye or both, the server <b>20</b> finally determines that there is eyelid edema.</p><p id="p-0519" num="0518">When it is determined that there is a symptom through a prediction model, the server <b>20</b> may give a predetermined score (for example, a score of 1) for the symptom. The server may give scores for five respective symptoms according to determination results for the five prediction models, and may also obtain a value obtained by adding all the scores.</p><p id="p-0520" num="0519">It has been described that the above-described method, which is described in the present application, of predicting a clinical activity score for thyroid eye disease is performed by the server <b>20</b>. However, the above-described method may be performed by a user terminal <b>10</b>. Alternatively, preprocessing of the above-described methods may be performed by the user terminal <b>10</b>, and determination for each of the symptoms may be performed by the server. That is, the above-described steps may be appropriately distributed to the user terminal <b>10</b> and the server <b>20</b> and performed.</p><p id="p-0521" num="0520">11. Hospital Visit Recommendation Method Based on Continuous Monitoring of Clinical Activity Score for Thyroid Eye Disease</p><p id="p-0522" num="0521">Hereinafter, described will be a method of continuously monitoring a clinical activity score for thyroid eye disease, and a method of recommending a hospital visit on the basis of the monitoring method described in the present application.</p><p id="p-0523" num="0522"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a diagram illustrating a method of continuously monitoring a clinical activity score for thyroid eye disease, and a method of recommending a hospital visit on the basis of the monitoring method described in the present application.</p><p id="p-0524" num="0523">A user terminal <b>10</b> may output a guide for acquiring a facial image, through a display <b>112</b> in step S<b>600</b>.</p><p id="p-0525" num="0524">The user terminal <b>10</b> may output, through the display <b>112</b>, an image (for example, an image of a user's face) captured in real time through a camera <b>140</b>. Herein, the guide may be output together.</p><p id="p-0526" num="0525">The user terminal <b>10</b> may acquire a facial image of the user's face through the camera <b>140</b> in step S<b>610</b>.</p><p id="p-0527" num="0526">The user terminal <b>10</b> may transmit the acquired facial image to the server <b>20</b> in step S<b>620</b>.</p><p id="p-0528" num="0527">The user terminal <b>10</b> may output, through the display, a graphical user interface (GUI) for receiving a user input with respect to spontaneous retrobulbar pain and pain on an attempted upward or downward gaze among a total of seven items considered in determining a clinical activity score for thyroid eye disease. Next, the user terminal <b>10</b> may receive the user's response to the two items in step S<b>630</b>. The user terminal <b>10</b> may give, on the basis of the input response of the user, a predetermined score (for example, a score of 1) for each of the items. For example, when the user provides an input that the user has spontaneous retrobulbar pain, the user terminal <b>10</b> may give a score of 1 for the item. In addition, when the user provides an input that the user has pain on an attempted upward or downward gaze, the user terminal <b>10</b> may give a score of 1 for the item.</p><p id="p-0529" num="0528">The user terminal <b>10</b> may receive, from the server <b>20</b> in step S<b>640</b>, determination results or a total score thereof for redness of a conjunctiva, swelling of a conjunctiva, swelling of a lacrimal caruncle, redness of an eyelid, and swelling of an eyelid among the total of seven items considered in determining a clinical activity score for thyroid eye disease on the basis of the acquired facial image.</p><p id="p-0530" num="0529">The user terminal <b>10</b> may calculate a final clinical activity score for thyroid eye disease in step S<b>650</b> on the basis of the scores determined by the user input and the scores received from the server <b>20</b> or the scores determined on the basis of the determination results received from the server.</p><p id="p-0531" num="0530">The user terminal <b>10</b> may store the time at which the facial image of the user is acquired, or the time at which a calculation value of the final clinical activity score for thyroid eye disease is acquired, or the corresponding time (hereinafter, referred to as measurement time, yy/mm/dd, hh:mm) in a memory <b>130</b> together with the calculated clinical activity score. Alternatively, the user terminal <b>10</b> may transmit the above-described measurement time and a clinical activity score corresponding thereto to the server <b>20</b>. Herein, the server <b>20</b> may store the measurement time and the clinical activity score in association with the user terminal <b>10</b> or the user in step S<b>660</b>.</p><p id="p-0532" num="0531">In the meantime, the measurement time includes information on the date. The measurement time may include both information on the date and information on the hour and/or minute. Alternatively, the measurement time may include only the information on the date, and may not include the information on the hour or minute.</p><p id="p-0533" num="0532">The user terminal <b>10</b> may output, on the basis of the calculated clinical activity score, information for recommending that the user visit the hospital and have a detailed medical examination, through the display <b>112</b> in step S<b>670</b>.</p><p id="p-0534" num="0533">When the calculated clinical activity score is less than a score of 3, the user terminal <b>10</b> may output information indicating that there is no risk of thyroid eye disease, through the display <b>112</b>.</p><p id="p-0535" num="0534">When the calculated clinical activity score is a score of 3 or 4, the user terminal <b>10</b> may output information indicating that there is no risk of thyroid eye disease, through the display <b>112</b>, or may output information for recommending that the user visit the hospital and have a detailed medical examination, through the display <b>112</b>, by choice.</p><p id="p-0536" num="0535">When the calculated clinical activity score is equal to or greater than a score of 5, the user terminal <b>10</b> may output information for recommending that the user visit the hospital and have a detailed medical examination, through the display <b>112</b>.</p><p id="p-0537" num="0536">When the calculated clinical activity score is a score of 3 or 4, the clinical activity score that was measured a predetermined period of time (for example, one week) before the corresponding time point is determined, and it may be determined whether the clinical activity score was a score of 3 or 4 during the corresponding time interval (hereinafter, the monitoring time interval). Herein, when a score of 3 or 4 occurred one or more times during the monitoring time interval, the user terminal <b>10</b> outputs information for recommending that the user visit the hospital and have a detailed medical examination, through the display <b>112</b>. When a score of 3 or 4 never occurred during the monitoring time interval, the user terminal <b>10</b> outputs information indicating that there is no risk of thyroid eye disease, through the display <b>112</b>.</p><p id="p-0538" num="0537">When the calculated clinical activity score is equal to or greater than a score of 3, the user terminal <b>10</b> may output information for recommending that the user visit the hospital and have a detailed medical examination, through the display <b>112</b> without additional determination for past records.</p><p id="p-0539" num="0538">According to the above description, in outputting information to the user through the user terminal <b>10</b>, outputting the information visually through the display <b>112</b> has been described as an example, but in some cases, the information may be audibly output through a speaker.</p><p id="p-0540" num="0539">In addition, it has been described that the method of continuously monitoring a clinical activity score for thyroid eye disease, and the method of recommending a hospital visit on the basis of the monitoring method described in the present application are performed by the user terminal <b>10</b>. However, the steps of the above-described methods may be appropriately distributed to the user terminal <b>10</b> and the server <b>20</b> and performed. For example, when the measurement time and the clinical activity score are transmitted to the server <b>20</b> and stored, whether a score of 3 or 4 occurred during the monitoring time interval may be determined by the server <b>20</b>.</p><p id="p-0541" num="0540">12. Experimental Example #1</p><p id="p-0542" num="0541">(1) Preparation of Facial Images</p><p id="p-0543" num="0542">1,020 facial images were prepared. Each of the facial images was an image including both a left eye and a right eye, and was an image obtained according to a predetermined photographing structure.</p><p id="p-0544" num="0543">(2) Securing Labeling Information of Facial Images</p><p id="p-0545" num="0544">For each of the 1,020 facial images, information on conjunctival hyperemia, conjunctival edema, lacrimal edema, eyelid redness, and eyelid edema for the left eye, and information on conjunctival hyperemia, conjunctival edema, lacrimal edema, eyelid redness, and eyelid edema for the right eye were secured, and the data were used as labeling data.</p><p id="p-0546" num="0545">Among 1,020 data sets, 714 data sets were used as a training data set (training set), 102 data sets were used as a validation sets, and 204 data sets were used as a test set.</p><p id="p-0547" num="0546">Furthermore, dividing the 1,020 data sets into a training data set, a validation set, and a test set was randomly performed 30 times, and a first training data set group to a 30th training data set group were created accordingly.</p><p id="p-0548" num="0547">(3) Securing First Preprocessed Images and Second Preprocessed Images of Facial Images</p><p id="p-0549" num="0548">For each of the 1,020 facial images, second cropping processing (eye outline cropping) was performed on each of the left eye and the right eye in the above-described manner to secure a first left eye preprocessed image and a first right eye preprocessed image. Herein, a laterally inverted image was used as the first right eye preprocessed image, and a non-laterally inverted image was used as the first left eye preprocessed image. In the meantime, both the first left eye preprocessed image and the first right eye preprocessed image were images on which the above-described first masking processing was performed.</p><p id="p-0550" num="0549">For each of the 1,020 facial images, third cropping processing (eyelid-included cropping) was performed on each of the left eye and the right eye in the above-described manner to secure a second left eye preprocessed image and a second right eye preprocessed image. Herein, a laterally inverted image was used as the second right eye preprocessed image, and a non-laterally inverted image was used as the second left eye preprocessed image. In the meantime, both the second left eye preprocessed image and the second right eye preprocessed image were images on which masking processing was not performed.</p><p id="p-0551" num="0550">(4) Training of First to Fifth Prediction Models according to Experimental Example #1</p><p id="p-0552" num="0551">The secured first preprocessed images and the secured pieces of labeling information thereof, and the secured second preprocessed images and the secured pieces of labeling information thereof were used in training the first to fifth prediction models.</p><p id="p-0553" num="0552">As the prediction models, the models using the above-described ViT as the backbone architecture were used, and each of the prediction models was trained with unification as one model without dividing into a left eye prediction model and a right eye prediction model.</p><p id="p-0554" num="0553">(5) Acquisition of Prediction result for each Symptom by using Prediction Models</p><p id="p-0555" num="0554">Prediction results were acquired using the test data sets for the trained first to fifth prediction models. Herein, a laterally inverted preprocessed image was used as a right eye image, and a non-laterally inverted preprocessed image was as a left eye image.</p><p id="p-0556" num="0555">(6) Accuracy, Sensitivity, Specificity, Positive Predictive Value (PPV), and Negative Predictive Value (NPV) of Eyelid Redness Prediction Model according to Experimental Example #1</p><p id="p-0557" num="0556">The values shown in [Table 1] are average values of accuracy, sensitivity, specificity, PPV, and NPV measured for the first to fifth prediction models that were trained for each of the 30 data set groups according to the above-described experimental example #1.</p><p id="p-0558" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="6"><colspec colname="1" colwidth="42pt" align="center"/><colspec colname="2" colwidth="49pt" align="center"/><colspec colname="3" colwidth="35pt" align="center"/><colspec colname="4" colwidth="49pt" align="center"/><colspec colname="5" colwidth="21pt" align="center"/><colspec colname="6" colwidth="21pt" align="center"/><thead><row><entry namest="1" nameend="6" rowsep="1">TABLE 1</entry></row><row><entry namest="1" nameend="6" align="center" rowsep="1"/></row><row><entry/><entry>Accuracy</entry><entry>Sensitivity</entry><entry>Specificity</entry><entry>PPV</entry><entry>NPV </entry></row><row><entry/><entry>(%)</entry><entry>(%)</entry><entry>(%)</entry><entry>(%)</entry><entry>(%)</entry></row><row><entry namest="1" nameend="6" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="6"><colspec colname="1" colwidth="42pt" align="left"/><colspec colname="2" colwidth="49pt" align="center"/><colspec colname="3" colwidth="35pt" align="center"/><colspec colname="4" colwidth="49pt" align="center"/><colspec colname="5" colwidth="21pt" align="center"/><colspec colname="6" colwidth="21pt" align="center"/><tbody valign="top"><row><entry>Conjunctival</entry><entry>80.80</entry><entry>86.84</entry><entry>76.40</entry><entry>73.58</entry><entry>89.10</entry></row><row><entry>hyperemia</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>(first</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>prediction</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>model)</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>Conjunctival</entry><entry>89.12</entry><entry>42.18</entry><entry>94.82</entry><entry>50.39</entry><entry>93.15</entry></row><row><entry>edema</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>(second</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>prediction</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>model)</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>Lacrimal</entry><entry>88.14</entry><entry>55.54</entry><entry>92.52</entry><entry>53.14</entry><entry>93.96</entry></row><row><entry>edema</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>(third</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>prediction</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>model)</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>Eyelid</entry><entry>72.42</entry><entry>73.94</entry><entry>71.55</entry><entry>59.10</entry><entry>84.17</entry></row><row><entry>redness</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>(fourth</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>prediction</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>model)</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>Eyelid</entry><entry>81.29</entry><entry>86.38</entry><entry>53.52</entry><entry>91.16</entry><entry>43.59</entry></row><row><entry>edema</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>(fifth</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>prediction</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>model)</entry></row><row><entry namest="1" nameend="6" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0559" num="0557">13. Experimental Example #2</p><p id="p-0560" num="0558">(1) Preparation of Facial Images</p><p id="p-0561" num="0559">The facial images used in experimental example #1 were used as they were.</p><p id="p-0562" num="0560">(2) Securing Labeling Information of Facial Images</p><p id="p-0563" num="0561">The pieces of labeling information of the facial images used in experimental example #1 were used as they were.</p><p id="p-0564" num="0562">(3) Securing First Preprocessed Images and Second Preprocessed Images of Facial Images</p><p id="p-0565" num="0563">For each of the 1,020 facial images, second cropping processing (eye outline cropping) is performed on each of the left eye and the right eye in the above-described manner to secure first preprocessed images. Unlike experimental example #1, a first left eye preprocessed image not subjected to lateral inversion, a first left eye preprocessed image subjected to lateral inversion, a first right eye preprocessed image not subjected to lateral inversion, and a first right eye preprocessed image subjected to lateral inversion were secured, and were used in training. Herein, both the first left eye preprocessed image and the first right eye preprocessed image were images on which the above-described first masking processing was performed.</p><p id="p-0566" num="0564">For each of the 1,020 facial images, third cropping processing (eyelid-included cropping) was performed on each of the left eye and the right eye in the above-described manner to secure second preprocessed images. Unlike experimental example #1, a second left eye preprocessed image not subjected to lateral inversion, a second left eye preprocessed image subjected to lateral inversion, a second right eye preprocessed image not subjected to lateral inversion, and a second right eye preprocessed image subjected to lateral inversion were secured, and were used in training. Herein, both the second left eye preprocessed image and the second right eye preprocessed image were images on which masking processing was not performed.</p><p id="p-0567" num="0565">(4) Training of First to Fifth Prediction Models according to Experimental Example #2</p><p id="p-0568" num="0566">The secured first preprocessed images and the secured pieces of labeling information thereof, and the secured second preprocessed images and the secured pieces of labeling information thereof were used in training the first to fifth prediction models.</p><p id="p-0569" num="0567">As the prediction models, the models using the above-described ViT as the backbone architecture were used, and each of the prediction models was trained being dualized into a left eye prediction model and a right eye prediction model. In particular, when the left eye prediction models were trained, left eye preprocessed images not subjected to lateral inversion and right eye preprocessed images subjected to lateral inversion were used, and when the right eye prediction models were trained, right eye preprocessed images not subjected to lateral inversion and left eye preprocessed images subjected to lateral inversion were used.</p><p id="p-0570" num="0568">(5) Acquisition of Prediction result for each Symptom by using Prediction Models</p><p id="p-0571" num="0569">Prediction results were acquired using the test data sets for the trained first to fifth prediction models. Herein, the prediction results for the right eye were acquired by inputting a right eye preprocessed image not subjected to lateral inversion to each of the right eye prediction models, and the prediction results for the left eye were acquired by inputting a left eye preprocessed image not subjected to lateral inversion to each of the left eye prediction models.</p><p id="p-0572" num="0570">(6) Accuracy, Sensitivity, Specificity, Positive Predictive Value (PPV), and Negative Predictive Value (NPV) of Eyelid Redness Prediction Model according to Experimental Example #2</p><p id="p-0573" num="0571">The values shown in [Table 2] are average values of accuracy, sensitivity, specificity, PPV, and NPV measured for the first to fifth prediction models that were trained for each of the 30 data set groups according to the above-described experimental example #2.</p><p id="p-0574" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="6"><colspec colname="1" colwidth="42pt" align="left"/><colspec colname="2" colwidth="49pt" align="center"/><colspec colname="3" colwidth="35pt" align="center"/><colspec colname="4" colwidth="49pt" align="center"/><colspec colname="5" colwidth="21pt" align="center"/><colspec colname="6" colwidth="21pt" align="center"/><thead><row><entry namest="1" nameend="6" rowsep="1">TABLE 2</entry></row><row><entry namest="1" nameend="6" align="center" rowsep="1"/></row><row><entry/><entry>Accuracy</entry><entry>Sensitivity</entry><entry>Specificity</entry><entry>PPV</entry><entry>NPV </entry></row><row><entry/><entry>(%)</entry><entry>(%)</entry><entry>(%)</entry><entry>(%)</entry><entry>(%)</entry></row><row><entry namest="1" nameend="6" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>Conjunctival</entry><entry>79.54</entry><entry>87.26</entry><entry>73.99</entry><entry>71.84</entry><entry>89.19</entry></row><row><entry>hyperemia</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>(first</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>prediction</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>model)</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>Conjunctival</entry><entry>89.22</entry><entry>45.04</entry><entry>94.61</entry><entry>52.53</entry><entry>93.48</entry></row><row><entry>edema</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>(second</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>prediction</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>model)</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>Lacrimal</entry><entry>88.35</entry><entry>49.51</entry><entry>93.70</entry><entry>55.48</entry><entry>93.20</entry></row><row><entry>edema</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>(third</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>prediction</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>model)</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>Eyelid</entry><entry>76.13</entry><entry>70.65</entry><entry>79.08</entry><entry>64.63</entry><entry>83.86</entry></row><row><entry>redness</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>(fourth</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>prediction</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>model)</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>Eyelid</entry><entry>81.47</entry><entry>86.51</entry><entry>54.44</entry><entry>91.22</entry><entry>44.62</entry></row><row><entry>edema</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>(fifth</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>prediction</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>model)</entry></row><row><entry namest="1" nameend="6" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0575" num="0572"><b>1</b>: System</p><p id="p-0576" num="0573"><b>10</b>: User terminal</p><p id="p-0577" num="0574"><b>20</b>: Server</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method of predicting thyroid eye disease, comprising:<claim-text>preparing a conjunctival hyperemia prediction model, a conjunctival edema prediction model, a lacrimal edema prediction model, an eyelid redness prediction model, and an eyelid edema prediction model,</claim-text><claim-text>obtaining a facial image of an object,</claim-text><claim-text>obtaining a first processed image and a second processed image from the facial image, wherein the first processed image is different from the second processed image,</claim-text><claim-text>obtaining predicted values for each of a conjunctival hyperemia, a conjunctival edema and a lacrimal edema by applying the first processed image to the conjunctival hyperemia prediction model, the conjunctival edema prediction model, and the lacrimal edema prediction model,</claim-text><claim-text>obtaining predicted values for each of an eyelid redness and an eyelid edema by applying the second processed image to the eyelid redness prediction model and the eyelid edema prediction model, and</claim-text><claim-text>determining a possibility of the object having thyroid eye disease based on the predicted values for the conjunctival hyperemia, the conjunctival edema, the lacrimal edema, the eyelid redness, and the eyelid edema,</claim-text><claim-text>wherein the first processed image is an image masked a region corresponding an inner of an outline of an iris and a region corresponding an outer of an outline of an eye and cropped along a first region comprising the outline of the eye based on position information of pixels corresponding the outline of the iris comprised in the eye and position information of pixels corresponding the outline of the eye, and</claim-text><claim-text>wherein the second processed image is an image cropped along a second region that is wider than the first region based on position information of pixels corresponding the outline of the iris comprised in the eye and position information of pixels corresponding the outline of the eye.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computer-implemented method of predicting thyroid eye disease of <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the position information of pixels corresponding the outline of the iris comprised in the eye and the position information of pixels corresponding the outline of the eye are obtained by segmentation model.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computer-implemented method of predicting thyroid eye disease of <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the first processed image comprises a first processed left eye image and a first processed right eye image, and</claim-text><claim-text>wherein the second processed image comprises a second processed left eye image and a second processed right eye image.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computer-implemented method of predicting thyroid eye disease of <claim-ref idref="CLM-00003">claim 3</claim-ref>,<claim-text>wherein the conjunctival hyperemia prediction model comprises a left eye conjunctival hyperemia prediction model and a right eye conjunctival hyperemia prediction model,</claim-text><claim-text>wherein the conjunctival edema prediction model comprises a left eye conjunctival edema prediction model and a right eye conjunctival edema prediction model,</claim-text><claim-text>wherein the lacrimal edema prediction model comprises a left eye lacrimal edema prediction model and a right eye lacrimal edema prediction model,</claim-text><claim-text>wherein the eyelid redness prediction model comprises a left eye eyelid redness prediction model and a right eye eyelid redness prediction model, and</claim-text><claim-text>wherein the eyelid edema prediction model comprises a left eye eyelid edema prediction model and a right eye eyelid edema prediction model.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computer-implemented method of predicting thyroid eye disease of <claim-ref idref="CLM-00004">claim 4</claim-ref>,<claim-text>wherein the predicted value for the conjunctival hyperemia is determined based on a result obtained by inputting the first processed left eye image to the left eye conjunctival hyperemia prediction model and a result obtained by inputting the first processed right eye image to the right eye conjunctival hyperemia prediction model,</claim-text><claim-text>wherein the predicted value for the conjunctival edema is determined based on a result obtained by inputting the first processed left eye image to the left eye conjunctival edema prediction model and a result obtained by inputting the first processed right eye image to the right eye conjunctival edema prediction model,</claim-text><claim-text>wherein the predicted value for the lacrimal edema is determined based on a result obtained by inputting the first processed left eye image to the left eye lacrimal edema prediction model and a result obtained by inputting the first processed right eye image to the right eye lacrimal edema prediction model,</claim-text><claim-text>wherein the predicted value for the eyelid redness is determined based on a result obtained by inputting the second processed left eye image to the left eye eyelid redness prediction model and a result obtained by inputting the second processed right eye image to the right eye eyelid redness prediction model, and</claim-text><claim-text>wherein the predicted value for the eyelid edema is determined based on a result obtained by inputting the second processed left eye image to the left eye eyelid edema prediction model and a result obtained by inputting the second processed right eye image to the right eye eyelid edema prediction model.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computer-implemented method of predicting thyroid eye disease of <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprises:<claim-text>processing any one of the first processed left eye image and the first processed right eye image by inverting left and right, and</claim-text><claim-text>processing any one of the second processed left eye image and the second processed right eye image by inverting left and right.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computer-implemented method of predicting thyroid eye disease of <claim-ref idref="CLM-00006">claim 6</claim-ref>,<claim-text>wherein the predicted value for the conjunctival hyperemia is determined based on results obtained by inputting the image laterally inverted and by inputting the image not laterally inverted to the conjunctival hyperemia prediction model,</claim-text><claim-text>wherein the predicted value for the conjunctival edema is determined based on results obtained by inputting the image laterally inverted and by inputting the image not laterally inverted to the conjunctival edema prediction model,</claim-text><claim-text>wherein the predicted value for the lacrimal edema is determined based on results obtained by inputting the image laterally inverted and by inputting the image not laterally inverted to the lacrimal edema prediction model,</claim-text><claim-text>wherein the predicted value for the eyelid redness is determined based on results obtained by inputting the image laterally inverted and by inputting the image not laterally inverted to the eyelid redness prediction model, and</claim-text><claim-text>wherein the predicted value for the eyelid edema is determined based on results obtained by inputting the image laterally inverted and by inputting the image not laterally inverted to the eyelid edema prediction model.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The computer-implemented method of predicting thyroid eye disease of <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprises:<claim-text>resizing the first processed left eye image and the first processed right eye image, and</claim-text><claim-text>resizing the second processed left eye image and the second processed right eye image.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computer-implemented method of predicting thyroid eye disease of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprises obtaining user responses with respect to a spontaneous retrobulbar pain and a pain on an attempted upward or downward gaze, and,<claim-text>wherein the determining the possibility comprises calculating a clinical activity score based on the predicted values for the conjunctival hyperemia, the conjunctival edema, the lacrimal edema, the eyelid redness, and the eyelid edema, and the obtained responses for the spontaneous retrobulbar pain and the pain on an attempted upward or downward gaze.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The computer-implemented method of predicting thyroid eye disease of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprises outputting information for recommending that a user visit a hospital in response to determining a clinical activity score is equal to or greater than a score of 3,<claim-text>wherein a score assigned to each symptom is a score of 1.</claim-text></claim-text></claim></claims></us-patent-application>