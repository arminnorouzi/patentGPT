<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004365A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004365</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17684871</doc-number><date>20220302</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>8</main-group><subgroup>41</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>8</main-group><subgroup>44</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>8</main-group><subgroup>425</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">MULTISTAGE COMPILER ARCHITECTURE</invention-title><us-related-documents><continuation-in-part><relation><parent-doc><document-id><country>US</country><doc-number>17390143</doc-number><date>20210730</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11467811</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17684871</doc-number></document-id></child-doc></relation></continuation-in-part><us-provisional-application><document-id><country>US</country><doc-number>63230598</doc-number><date>20210806</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>63214651</doc-number><date>20210624</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Marvell Asia Pte Ltd</orgname><address><city>Singapore</city><country>SG</country></address></addressbook><residence><country>SG</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Hanebutte</last-name><first-name>Ulf</first-name><address><city>Gig Harbor</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Durakovic</last-name><first-name>Senad</first-name><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Chou</last-name><first-name>Chien-Chun</first-name><address><city>Morgan Hill</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Wang</last-name><first-name>Fu-Hwa</first-name><address><city>Saratoga</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Tandyala</last-name><first-name>Mohana</first-name><address><city>Fremont</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A system includes a compiler including a plurality of compiler blocks. The compiler blocks of the plurality of compiler blocks are compossible. The compiler is configured to identify one or more resources in a hardware to execute a set of low-level instructions that is generated from a high-level function in a high-level code. The compiler is further configured to determine one or more processing operations to be performed that is associated with the high-level function in the high-level code. The determining of the one or more processing operations occurs based on architecture of the hardware. The compiler is configured to compile the high-level function in the high-level code of the application into the set of low-level instructions to be executed on the hardware.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="83.57mm" wi="152.82mm" file="US20230004365A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="154.86mm" wi="125.22mm" orientation="landscape" file="US20230004365A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="222.25mm" wi="137.58mm" orientation="landscape" file="US20230004365A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="244.01mm" wi="122.09mm" orientation="landscape" file="US20230004365A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="155.36mm" wi="121.33mm" file="US20230004365A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="237.32mm" wi="148.42mm" orientation="landscape" file="US20230004365A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="248.84mm" wi="157.73mm" orientation="landscape" file="US20230004365A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="239.44mm" wi="148.25mm" orientation="landscape" file="US20230004365A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="202.27mm" wi="123.44mm" orientation="landscape" file="US20230004365A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="182.12mm" wi="130.22mm" orientation="landscape" file="US20230004365A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="128.78mm" wi="140.72mm" orientation="landscape" file="US20230004365A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="225.89mm" wi="155.02mm" orientation="landscape" file="US20230004365A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="227.16mm" wi="160.36mm" file="US20230004365A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is a nonprovisional application and claims the benefit and priority to a provisional application No. 63/230,598 filed on Aug. 6, 2021, which is incorporated herein by reference in its entirety.</p><p id="p-0003" num="0002">This application is a continuation-in-part application that claims the benefit and priority to the U.S. patent application Ser. No. 17/390,143 that was filed on Jul. 30, 2021, which further claims the benefit and priority to a provisional application No. 63/214,651 that was filed on Jun. 24, 2021, which is incorporated herein by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Use and implementations of machine learning (ML) and artificial intelligence (AI) methods on electronic devices has become ubiquitous. The design of a hardware architecture of the electronic devices, whether a processor, a programmable logic, a dedicated hardware such as application specific integrated circuit (ASIC), or a dedicated ML hardware, often goes through various optimization and compilation processes.</p><p id="p-0005" num="0004">A compilation process or a compiler generates low-level executable instructions (in binary) from one or more high-level code and identifies hardware resources to execute the low-level executable instructions. The compilation process may include quantization, reduction in mathematical precision, mapping of the application (e.g., a neural network) to a specific number of processing tiles of the hardware. In general, the compiler maps data, e.g., the network tensor weight, the network tensor bias constants, the network tensor input and output for each network layer, etc., to particular memories and generates the executable code associated therewith. For example, the compiler decides on which processing tile and which processing unit (e.g., POD and/or PE) of the tile of a multi-core system will be processing certain data. As another example, the compiler may decide that certain data is to be processed by a central processing unit as opposed to a tile within a ML hardware.</p><p id="p-0006" num="0005">Electronic devices have become more complex and may include multiple memory systems, as an example. As one nonlimiting example, a dedicated ML hardware may include multiple memory systems. During the execution of the compiled instructions on the ML hardware, data, e.g., tensor data, may reside on multiple different memory blocks within the hierarchy. Moreover, the data may be represented by different precisions, orientation, or split across distributed blocks based on the system requirement, e.g., channel/height/width as opposed to height/width/channel and number of bytes needs due to alignment needed in hardware. Unfortunately, none of this information is automatically available for debugging, verification, and/or optimization purposes. Conventionally, for smaller networks, the memory allocation and access may be spot check using a manual and time consuming process which is not scalable to the entire network or the memory space.</p><p id="p-0007" num="0006">Conventionally, a compiler that has been used has been static and incapable of adapting to rapidly changing environment and technologies. For example, conventionally a compilation block has been incapable of being replaced with a different compilation block, thereby requiring a completely new compiler to be developed which has been a time consuming process. Furthermore, compilation has been traditionally a black box without much feedback or suggestion to the user to debug, optimize, correct error, proper use, etc.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007">Aspects of the present disclosure are best understood from the following detailed description when read with the accompanying figures. It is noted that, in accordance with the standard practice in the industry, various features are not drawn to scale. In fact, the dimensions of the various features may be arbitrarily increased or reduced for clarity of discussion.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> depicts an example of a diagram of a system to generate a multi-level structured metadata when the high-level code is being compiled into low-level instructions of an application for running on ML hardware according to one aspect of the present embodiments.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIGS. <b>1</b>B-<b>1</b>G</figref> depict examples of data formats and memory layout according to one aspect of the present embodiments.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>B</figref> depict an example of the set of low-level instructions compiled from a high-level code for an operation using a compiler according to one aspect of the present embodiments.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIGS. <b>2</b>C-<b>2</b>G</figref> depict examples of decisions/operations being performed by the compiler to convert a high-level code to a set of low-level instructions according to one aspect of the present embodiments.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts another example of decisions/operations being performed by the compiler to convert a high-level code to a set of low-level instructions according to one aspect of the present embodiments.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts an example of an inference engine that includes a plurality of processing tiles arranged in a two-dimensional array of a plurality of rows and columns according to one aspect of the present embodiments.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts a flowchart of an example of a process to support generating a multi-level structured metadata when the high-level code is being compiled into low-level instructions of an application for running on ML hardware according to one aspect of the present embodiments.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0016" num="0015">The following disclosure provides many different embodiments, or examples, for implementing different features of the subject matter. Specific examples of components and arrangements are described below to simplify the present disclosure. These are, of course, merely examples and are not intended to be limiting. In addition, the present disclosure may repeat reference numerals and/or letters in the various examples. This repetition is for the purpose of simplicity and clarity and does not in itself dictate a relationship between the various embodiments and/or configurations discussed.</p><p id="p-0017" num="0016">Before various embodiments are described in greater detail, it should be understood that the embodiments are not limiting, as elements in such embodiments may vary. It should likewise be understood that a particular embodiment described and/or illustrated herein has elements which may be readily separated from the particular embodiment and optionally combined with any of several other embodiments or substituted for elements in any of several other embodiments described herein. It should also be understood that the terminology used herein is for the purpose of describing the certain concepts, and the terminology is not intended to be limiting. Unless defined otherwise, all technical and scientific terms used herein have the same meaning as commonly understood in the art to which the embodiments pertain.</p><p id="p-0018" num="0017">A new approach is proposed that contemplates systems and methods to support a multi-leveled compiler-generated metadata that may be utilized by a software or a person for code verification, debugging, and/or optimization purposes. In general, a compiler is configured to go through multiple levels or stages during compilation of high-level code into low-level executable instructions on a hardware. At each level (i.e. stage), the compiler needs to make one or more decisions on compilation, e.g., how to map the data to be processed and to which memory blocks, decision on a particular processing tile to execute the executable code for a particular data, etc. It is appreciated that references to level of backend compiler (discussed later in the application) refers to stages of compilation by the backend compiler. At each level, the compiler in addition to generating the low-level executable code also generates the multi-layered structured metadata for that stage that reflects the action(s)/decision(s) being made by the compiler, e.g., mapping of data to memory blocks, precision, quantization, processing tile to perform a particular task/instruction, dimension reordering, copying across processing tiles, etc. It is appreciated that the compiler action(s)/decision(s) occur first in order for the high-level code to be compiled into low-level executable instructions. In some embodiments, the multi-layered structured metadata may include comments in a generated code that is human readable. It is appreciated that the multi-layered structured metadata may be readable or executable by the compiler or another software in some embodiments. In some embodiments, the multi-layered structured metadata may be stored in one or more files or it may be included as part of the assembly code.</p><p id="p-0019" num="0018">It is appreciated that according to some embodiments, the stages of compilation may be compossible. In other words, different compiler blocks associated with each stage may be swapped and replaced with a different compiler block, as needed. For example, a first compiler block that is different from a second compiler block may be used at different stages of compilation. As such, different compiler blocks may be adapted and/or optimized depending on the use case, which has become even more important in rapidly emerging technology such as ML. Examples of the use cases may include but is not limited to support of customer-specific deep-learning networks, novel deep-learning networks with new operators, compilation to different ML hardware (i.e., accelerators architecture), etc.</p><p id="p-0020" num="0019">According to some embodiments, a compiler block may be replaced with a block that includes experimental algorithms and/or implementations, thereby enabling further optimization and debugging capabilities that were previously not available. Experimental compiler block may include a different mapping strategy and/or memory allocation strategy. In yet another embodiment, the compiler block may be replaced with a debug version of the same block. It is appreciated that a debug version of the compiler block may track and store additional information regarding the compilation and/or modification to the internal representation and/or meta data that results in a debug binary. Performance of the original version of the compiler block and the debug version may be compared and necessary optimization may be performed.</p><p id="p-0021" num="0020">It is appreciated that compossibility of the compiler block aids in development of the overall compiler and provides flexibility to adapt to rapidly changing technologies, e.g., evolving ML models, evolving ML hardware, etc.</p><p id="p-0022" num="0021">In some ML applications, the multi-layered structured metadata may be generated by the compiler automatically and it may include information such as location of data, e.g., tensor, which is a nested data structure widely used for ML applications, in various memory blocks within the layer. It is appreciated that the multi-layered structured metadata may also provide information regarding the memory location (e.g., host memory, device memory, chip memory, etc.) for each tensor at any given stage in the network execution. Accordingly, expected memory dumps may be generated based on the original tensor that can be used for comparison to memory dumps of the actual hardware, software emulator or hardware emulator runs. As such, the low-level code/instructions can be verified and debugged based on the metadata generated by the compiler.</p><p id="p-0023" num="0022">The multi-layered structured metadata at each layer may also include information regarding certain actions (i.e. decisions) by the compiler, e.g., precision, orientation, split across distributed blocks, quantization, processing tile to perform a certain operation, etc. In some embodiments, the multi-layered structured metadata may describe transformation associated with data being processed, e.g., transformation associated with tensors such as quantization, reducing precision, dimension reordering (e.g., conversion to/from width/height/channel (WHC) from/to channel/height/width (CHW)), splitting or copying across processing tiles, or other compile time optimizations that may result in reduced execution time of the compiled code. It is appreciated that references to tensors are provided for illustrative purposes throughout the application and should not be construed as limiting the scope of the embodiments.</p><p id="p-0024" num="0023">In some embodiments, the multi-layered structured metadata at each layer may be used for optimization purposes, e.g., reducing data movement, reducing storage, reducing duplicate computations, reducing communication by duplicating computing if beneficial, reducing data conversions, etc. In some embodiments, the multi-layered structured metadata generated from one layer may be input into a subsequent layer and it may be relied upon by the compiler itself in order to optimize the compilation and decisions on how to process data and perform operations at the subsequent layer in an optimized fashion, e.g., by reducing data movement, reducing storage, reducing duplicate computations, reducing communications, reducing data conversions, etc.</p><p id="p-0025" num="0024">It is appreciated that the compiler automatically generates the multi-layered structured metadata because the compiler is aware of the system requirements, e.g., channel/height/width as opposed to height/width/channel and number of bytes needs due to alignment needed in hardware. Moreover, the compiler is aware of the hardware architecture, e.g., ML hardware (number of processing tiles, etc.), and as a result automatically generates the multi-layered structured metadata for each layer and decisions that the compiler is making with respect to how to process/map processing and data to the hardware. As such, the multi-layered structured metadata once generated can be used for debugging, verification, or optimization purposes.</p><p id="p-0026" num="0025">Since the overall number of low-level instructions to be executed on the ML hardware remains the same and no additional instructions are introduced because the multi-layered structured metadata is generated as comments that are not executed or stored in one or more files, the instruction flow and the executables of the application are not adversely affected or disturbed for performance profiling purposes. As a result, accurate performance profiling and debugging of the application can be achieved as well as optimization if desired.</p><p id="p-0027" num="0026">Although an instruction set architecture (ISA) is used as a non-limiting example of the low-level instruction format to illustrate the proposed approach in the embodiments described below, it is appreciated that the same or similar approach is equally applicable to other types of low-level instructions. It is also appreciated that an ML hardware (e.g., inference engine) is used as a non-limiting example of the hardware where the low-level instructions are executed to illustrate the proposed approach in the embodiments described below, it is appreciated that the same or similar approach is equally applicable to other types of hardware or hardware simulator to support generating a metadata using a compiler that can ultimately be used for verification, debugging, and optimization purposes. Moreover, it is appreciated that an ML-related operation or function is used as a non-limiting example of the application of the high-level code to illustrate the proposed approach in the embodiments described below, it is appreciated that the same or similar approach is equally applicable to other types of software applications including but not limited to firmware, hardware simulation software, or register transfer level (RTL) simulation software, to support the compiler generating a metadata.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> depicts an example of a diagram of a system to support generating a multi-level structured metadata when the high-level code is being compiled into low-level instructions of an application for running on ML hardware. Although the diagrams depict components as functionally separate, such depiction is merely for illustrative purposes. It will be apparent that the components portrayed in this figure can be arbitrarily combined or divided into separate software, firmware and/or hardware components. Furthermore, it will also be apparent that such components, regardless of how they are combined or divided, can execute on the same host or multiple hosts, and wherein the multiple hosts can be connected by one or more networks.</p><p id="p-0029" num="0028">In the example of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, the system includes a host <b>110</b>, a compiler (compiling engine) <b>120</b>, optionally an ML library <b>180</b>, and an ML hardware <b>160</b>. It is appreciated that one or more components of the system may run on one or more computing units or devices (not shown) each with software instructions stored in a storage unit such as a non-volatile memory of the computing unit for practicing one or more processes. When the software instructions are executed, at least a subset of the software instructions is loaded into memory by one of the computing units, which becomes a special purposed one for practicing the processes. The processes may also be at least partially embodied in the computing units into which computer program code is loaded and/or executed, such that, the computing units become special purpose computing units for practicing the processes.</p><p id="p-0030" num="0029">In the example of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, the compiler <b>120</b> coupled to a host <b>110</b> is configured to accept a high-level code of an application (e.g., an ML operation) from the host <b>110</b>, wherein the high-level code includes a plurality of high-level functions/operators each called at one or more lines in the high-level code. The compiler <b>120</b> is then configured to compile each high-level function/operator in the high-level code into a set of low-level instructions to be executed on the ML hardware <b>160</b>, wherein each set of the low-level instructions is uniquely identified and associated with the high-level function. It is appreciated that the ML hardware <b>160</b> is provided for illustrative purposes and should not be construed as limiting the scope of the embodiments. For example, any type of hardware based system configured to execute low-level instructions may be used.</p><p id="p-0031" num="0030">Here, the high-level code is a software code written through a commonly-used high-level programming language. For a non-limiting example, the high-level functions of the application or ML operation can be a dense and/or regular operation, e.g., a matrix operation such as multiplication, matrix manipulation, tanh, sigmoid, etc. For another non-limiting example, the high-level functions of the application or ML operation can be a sparse or irregular operation, e.g., memory transpose, addition operation, operations on irregular data structures (such as trees, graphs, and priority queues), etc. In some embodiments, the high-level code of the application may include one or more library function calls to an ML library <b>180</b>. For a non-limiting example, the compiler <b>120</b> may call a library function to perform a matrix-matrix-multiplication of two matrices of given sizes and the ML library <b>180</b> returns the set of low-level instructions that are needed to perform this library function, wherein the set of low-level instructions includes one or more of loading data from a memory (e.g., OCM) into registers, executing dot-product, and storing the data back into the memory.</p><p id="p-0032" num="0031">In some embodiments, the set of low-level instructions are in the format of ISA designed for efficient data processing covering, for non-limiting examples, one or more of different addressing modes, native data types, registers, memory architectures, and interrupts. In some embodiments, the ISA is a predominantly asynchronous instruction set, wherein each instruction in the ISA format programs a state-machine, which then runs asynchronously with respect to other state machines. It is appreciated that a series of instructions in the ISA format do not necessarily imply sequential execution. In some embodiments, the ISA provides separate synchronizing instructions to ensure order between instructions where needed. In some embodiments, when being executed on the ML hardware <b>160</b>, the set of low-level instructions in the ISA format program the ML hardware <b>160</b> by one or more of: (i) programming one or more input data streams to the ML hardware <b>160</b>; (ii) programming one or more operations to be performed on the input data streams; and (iii) programming one or more output data streams from the ML hardware <b>160</b>.</p><p id="p-0033" num="0032">In some embodiments, the compiler <b>120</b> is configured to generate additional information to further correlate the high-level function to one or more layers of a neural network used for machine learning applications. For non-limiting examples, the neural network can be but is not limited to one of a convolution neural network (CNN), a recurrent neural network (RNN), a gradient boosting machine (GBM), and a generative adversarial neural network. For non-limiting examples, the additional information includes but is not limited to which tasks of the high-level function belong to a specific neural network layer as well as which neural network layer the high-level function belongs to.</p><p id="p-0034" num="0033">Once the set of low-level instructions has been compiled from each high-level function, the compiler <b>120</b> is configured to stream the set of low-level instructions as well as data received from the host for the application to the ML hardware <b>160</b> for execution. In the example of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, the ML hardware <b>160</b> is a dedicated hardware block/component including one or more microprocessors and/or on-chip memory (OCM) units storing the data and/or the set of low-level instructions compiled from the high-level code performing one or more ML operations. For a non-limiting example, the ML hardware <b>160</b> can be but is not limited to an inference engine, which is configured to infer and identify a subject for the application via inference from trained data. At runtime, the ML hardware <b>160</b> is configured to retrieve the set of low-level instructions and/or data received from the compiler <b>120</b> and execute the set of low-level instructions to perform the high-level application/ML operation according to the set of low-level instructions. <figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts a non-limiting example of an inference engine <b>160</b> that includes a plurality of processing tiles, e.g., tiles 0, . . . , 63, arranged in a two-dimensional array of a plurality of rows and columns, e.g., 8 row by 8 columns. Each processing tile (e.g., tile 0) includes at least one OCM, a first type of processing unit (POD), and a second type of processing unit (PE). Both types of processing units can execute and be programmed by some of the plurality of low-level instructions received from the compiler <b>120</b>. In some embodiments, a plurality of processing tiles forms a processing block, e.g., tiles 0-3 forms processing block 1 and the processing tiles within each processing block are coupled to one another via a routing element, e.g., tiles 0-3 are coupled to one another via routing element R to form processing block 1.</p><p id="p-0035" num="0034">In order to generate the low-level instructions from high-level functions/code, the compiler <b>120</b> having knowledge of the ML hardware <b>160</b> architecture and software/system requirements makes certain decisions and performs certain operations in order to generate low-level instructions that are as efficient and as optimized as possible (e.g., from hardware perspective and/or software perspective). For example, the compiler <b>120</b> may take certain actions and make certain decisions to reduce data movement, to reduce data conversions, to reduce storage usage, to reduce computation (or duplication of computation), to reduce communication (by duplicating compute if beneficial), etc. A nonlimiting and non-exhaustive list of decisions being made by the compiler <b>120</b> in addition to the above includes but is not limited to:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0035">identifying and associating certain sub-graphs of a layer to be processed by ML hardware <b>160</b> but other sub-graphs to other processing components (e.g., a central processing unit),</li>        <li id="ul0002-0002" num="0036">fusing operators into composite to map to hardware ISA task (i.e. maps optimally to hardware architecture capabilities),</li>        <li id="ul0002-0003" num="0037">splitting input/output tensors of an operation into N parts where N may be the maximum number of tiles or smaller and distributing the parts across the N tiles. The parts may be of unequal sizes and the split input/output may duplicate the associated weights and bias tensors across all N tiles,</li>        <li id="ul0002-0004" num="0038">splitting weights/bias (similar to splitting input/output but applied to weights/bias),</li>        <li id="ul0002-0005" num="0039">SAMM/LAMM (different mappings of two matrices onto the POD registers based on the shape of the matrices and where SAMM indicates one dimension of the input being short whereas LAMM indicates one dimension of the input being long),</li>        <li id="ul0002-0006" num="0040">direct convolution (i.e. performing a convolution by directly applying the kernel to the input tensor in contrast to converting a convolution into a matrix-matrix-multiply that is executed after the input tensor is transformed by the flattening stage which results in an increased data movement and data duplication),</li>        <li id="ul0002-0007" num="0041">serializing in time (i.e. mapping an operation into a sequence of steps that are executed sequentially in time),</li>        <li id="ul0002-0008" num="0042">number of tiles to use for certain processing/tasks,</li>        <li id="ul0002-0009" num="0043">dividing tensors and duplicating on tiles (i.e. manner by which to map data to local tiles either distribute or copy or both, where a set of tiles may be grouped together and within the group the data may be split after the original data is duplicated or copied to each group),</li>        <li id="ul0002-0010" num="0044">number of halo cells (i.e. also referred to as ghost cells or rows that are added to distribute data on a tile which contains copies of rows or cells belonging to its neighboring tiles) that allows calculations on a tile be done locally without requiring data to be obtained from neighboring tiles even though it may need the halo cells/rows to be filled via communication prior to executing the calculations,</li>        <li id="ul0002-0011" num="0045">data movement,</li>        <li id="ul0002-0012" num="0046">rebalancing processing on different tiles,</li>        <li id="ul0002-0013" num="0047">memory hierarchy mapping,</li>        <li id="ul0002-0014" num="0048">determining tensor life-cycle (i.e. the amount of time that the tensor data is required to be in memory (mapped to local OCM) to ensure that the last task/instruction that needs to have access to the tensor data has access to the tensor data) in order to perform memory management and to free up unused memory,</li>        <li id="ul0002-0015" num="0049">quantization scaling values (i.e. the output of a certain layer in a quantized network may be rescaled to stay within a particular data range),</li>        <li id="ul0002-0016" num="0050">quantization data types (e.g., signed versus unsigned such as int8 and uint8),</li>        <li id="ul0002-0017" num="0051">rescaling,</li>        <li id="ul0002-0018" num="0052">determining which primitive to use for a given operator (e.g., direct convolution as opposed to flattening plus compute pipeline, complete fully connected (FC) layer (i.e. a matrix-matrix-multiply that might be performed as one distributed matrix-matrix-multiply (performed as single computation block followed by a single communication block) as opposed to being broken up into a pipeline sequence distributed matrix-matrix-multiplies which allows overlapping of communication and computation),</li>        <li id="ul0002-0019" num="0053">input to pipeline decisions (i.e. decision whether to apply a pipeline strategy, e.g., based on matrix sizes the optimal strategy may not be pipelined),</li>        <li id="ul0002-0020" num="0054">overlapping different hardware components, e.g., processing elements, direct memory access (DMA), etc., on ML hardware <b>160</b> to increase parallelism,</li>        <li id="ul0002-0021" num="0055">optimizing use of synchronization primitives</li>        <li id="ul0002-0022" num="0056">exposing and utilizing the ML hardware <b>160</b> capabilities for diverse set of workloads, e.g., ML workloads,</li>        <li id="ul0002-0023" num="0057">memory layout and conversion, as described in more detail in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, (e.g., in channel/height/width or height/width/channel format, etc.).<br/>In some embodiments, the compiler <b>120</b> in addition to generating low-level instructions is also configured to generate multi-level structured metadata <b>122</b> that can be used to debug the code, to verify the code, and/or to optimize the code and compilation. The metadata <b>122</b> encapsulates the decisions (as described above) that have been made by the compiler <b>120</b> in order to generate the low-level instructions from the high-level code.</li>    </ul>    </li></ul></p><p id="p-0036" num="0058">Referring now to <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, memory layout for channel, height, and width (CHW) according to some embodiments is shown. In this nonlimiting example, for a quantized int8 network, each element of the weight matrix is an int8 value that is represented by 1 byte, however, in an fp16 network, 2 bytes per weight elements may be needed, as 2 bytes are needed to represent an fp16 value. In this nonlimiting example, the input of the OCM layout for layer 2 tensor is in CHW format. According to this nonlimiting example, there are 2 channels and the height and width are 5 bytes each. Accordingly, there are 2 blocks of 5&#xd7;5 data. In this example, the system may require 8 bytes internally for alignment needed by the hardware. Accordingly, the memory layout needed is 5&#xd7;5 bytes for one channel and another 5&#xd7;5 bytes for the second channel, as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>. In the nonlimiting example of <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>, unique names are given for each tensor element (i.e. 1, 2, 11, a1, a11) that is different from the hex values such as a45 to be 2626 in decimal, a number much larger than the range of int8 (i.e. &#x2212;128 to 127), the data (2 dimensional matrices that is looked at as a single 3 dimensional tensor where the first is representing channel=1 and the second is representing channel=2) may be a matrix</p><p id="p-0037" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mo>[</mo>  <mtable>   <mtr>    <mtd>     <mn>1</mn>    </mtd>    <mtd>     <mn>2</mn>    </mtd>    <mtd>     <mn>3</mn>    </mtd>    <mtd>     <mn>4</mn>    </mtd>    <mtd>     <mn>5</mn>    </mtd>   </mtr>   <mtr>    <mtd>     <mrow>      <mn>1</mn>      <mo>&#x2062;</mo>      <mn>1</mn>     </mrow>    </mtd>    <mtd>     <mrow>      <mn>1</mn>      <mo>&#x2062;</mo>      <mn>2</mn>     </mrow>    </mtd>    <mtd>     <mrow>      <mn>1</mn>      <mo>&#x2062;</mo>      <mn>3</mn>     </mrow>    </mtd>    <mtd>     <mrow>      <mn>1</mn>      <mo>&#x2062;</mo>      <mn>4</mn>     </mrow>    </mtd>    <mtd>     <mrow>      <mn>1</mn>      <mo>&#x2062;</mo>      <mn>5</mn>     </mrow>    </mtd>   </mtr>   <mtr>    <mtd>     <mrow>      <mn>2</mn>      <mo>&#x2062;</mo>      <mn>1</mn>     </mrow>    </mtd>    <mtd>     <mrow>      <mn>2</mn>      <mo>&#x2062;</mo>      <mn>2</mn>     </mrow>    </mtd>    <mtd>     <mrow>      <mn>2</mn>      <mo>&#x2062;</mo>      <mn>3</mn>     </mrow>    </mtd>    <mtd>     <mrow>      <mn>2</mn>      <mo>&#x2062;</mo>      <mn>4</mn>     </mrow>    </mtd>    <mtd>     <mrow>      <mn>2</mn>      <mo>&#x2062;</mo>      <mn>5</mn>     </mrow>    </mtd>   </mtr>   <mtr>    <mtd>     <mrow>      <mn>3</mn>      <mo>&#x2062;</mo>      <mn>1</mn>     </mrow>    </mtd>    <mtd>     <mrow>      <mn>3</mn>      <mo>&#x2062;</mo>      <mn>2</mn>     </mrow>    </mtd>    <mtd>     <mrow>      <mn>3</mn>      <mo>&#x2062;</mo>      <mn>3</mn>     </mrow>    </mtd>    <mtd>     <mrow>      <mn>3</mn>      <mo>&#x2062;</mo>      <mn>4</mn>     </mrow>    </mtd>    <mtd>     <mrow>      <mn>3</mn>      <mo>&#x2062;</mo>      <mn>5</mn>     </mrow>    </mtd>   </mtr>   <mtr>    <mtd>     <mrow>      <mn>4</mn>      <mo>&#x2062;</mo>      <mn>1</mn>     </mrow>    </mtd>    <mtd>     <mrow>      <mn>4</mn>      <mo>&#x2062;</mo>      <mn>2</mn>     </mrow>    </mtd>    <mtd>     <mrow>      <mn>4</mn>      <mo>&#x2062;</mo>      <mn>3</mn>     </mrow>    </mtd>    <mtd>     <mrow>      <mn>4</mn>      <mo>&#x2062;</mo>      <mn>4</mn>     </mrow>    </mtd>    <mtd>     <mrow>      <mn>4</mn>      <mo>&#x2062;</mo>      <mn>5</mn>     </mrow>    </mtd>   </mtr>  </mtable>  <mo>]</mo> </mrow></math></maths></p><p id="p-0038" num="0000">while the data (channel=2 data of the weight tensor) may be a matrix</p><p id="p-0039" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mrow>   <mo>[</mo>   <mtable>    <mtr>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>1</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>2</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>3</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>4</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>5</mn>      </mrow>     </mtd>    </mtr>    <mtr>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>1</mn>       <mo>&#x2062;</mo>       <mn>1</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>1</mn>       <mo>&#x2062;</mo>       <mn>2</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>1</mn>       <mo>&#x2062;</mo>       <mn>3</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>1</mn>       <mo>&#x2062;</mo>       <mn>4</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>1</mn>       <mo>&#x2062;</mo>       <mn>5</mn>      </mrow>     </mtd>    </mtr>    <mtr>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>2</mn>       <mo>&#x2062;</mo>       <mn>1</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>2</mn>       <mo>&#x2062;</mo>       <mn>2</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>2</mn>       <mo>&#x2062;</mo>       <mn>3</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>2</mn>       <mo>&#x2062;</mo>       <mn>4</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>2</mn>       <mo>&#x2062;</mo>       <mn>5</mn>      </mrow>     </mtd>    </mtr>    <mtr>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>3</mn>       <mo>&#x2062;</mo>       <mn>1</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>3</mn>       <mo>&#x2062;</mo>       <mn>2</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>3</mn>       <mo>&#x2062;</mo>       <mn>3</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>3</mn>       <mo>&#x2062;</mo>       <mn>4</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>3</mn>       <mo>&#x2062;</mo>       <mn>5</mn>      </mrow>     </mtd>    </mtr>    <mtr>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>4</mn>       <mo>&#x2062;</mo>       <mn>1</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>4</mn>       <mo>&#x2062;</mo>       <mn>2</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>4</mn>       <mo>&#x2062;</mo>       <mn>3</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>4</mn>       <mo>&#x2062;</mo>       <mn>4</mn>      </mrow>     </mtd>     <mtd>      <mrow>       <mi>a</mi>       <mo>&#x2062;</mo>       <mn>4</mn>       <mo>&#x2062;</mo>       <mn>5</mn>      </mrow>     </mtd>    </mtr>   </mtable>   <mo>]</mo>  </mrow>  <mo>.</mo> </mrow></math></maths></p><p id="p-0040" num="0000">The memory layout when stored is illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>, in this nonlimiting example, the system requires 8 bytes internally and since the data is 5 bytes the remainder 3 bytes are illustrated as &#x201c;x&#x201d; and used by the system for internal alignment.</p><p id="p-0041" num="0059">It is appreciated that, in some embodiments, the compiler <b>120</b> has knowledge of the architecture of the ML hardware <b>160</b> and its requirements, e.g., determining that conversion to HWC format is needed. Referring now to <figref idref="DRAWINGS">FIG. <b>1</b>E</figref>, the memory layout reflecting the conversion from CHW to HWC format is shown. In this example, since the height is 5 then it is determined that there are 5 blocks of 5&#xd7;2 since the width is 5 bytes and the channel is 2. <figref idref="DRAWINGS">FIG. <b>1</b>F</figref> illustrates the blocks of data for the example shown in <figref idref="DRAWINGS">FIGS. <b>1</b>B-<b>1</b>D</figref>. <figref idref="DRAWINGS">FIG. <b>1</b>G</figref> illustrates the data once it is stored in the OCM in HWC format according to some embodiments. Here similar to <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>, since the system requires 8 internal bytes for alignment, the first two bytes are the data and the remainder 6 bytes for each row is illustrated as &#x201c;x&#x201d; and used for internal alignment.</p><p id="p-0042" num="0060">It is appreciated that the conversion and the information regarding the memory layout for example is encapsulated within the multi-level structured metadata <b>122</b> being generated by the compiler <b>120</b>. It is similarly appreciated that other decisions or operations performed by the compiler <b>120</b> is captured within the multi-level structured metadata <b>122</b> that can be used to optimize the operation of the compiler, debug the code, and/or verify the code.</p><p id="p-0043" num="0061">Referring now to <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, a compiler <b>120</b> according to some embodiments is shown. In this nonlimiting example, the compiler <b>120</b> may include a frontend compiler <b>210</b> and a backend compiler <b>220</b>. It is appreciated that the frontend compiler <b>210</b> designation and the backend compiler <b>220</b> designation is for illustration purposes only and should not be construed as limiting the scope of the embodiments. For example, a single compiler may be used. The frontend compiler <b>210</b> may perform the analysis phase of the compilation by reading the source code, dividing the code into core parts and checking for lexical, grammar, and syntax. In some embodiments, the frontend compiler <b>210</b> may include lexical analysis, syntax analysis, a semantic analysis, etc., and generates an intermediate data <b>212</b> (also known as intermediate representation). The intermediate data <b>212</b> is input into the backend compiler <b>220</b> in order to perform specific optimization and to generate the low-level instructions. It is appreciated that for ML compilers, the frontend compiler <b>210</b> may include transformation from representation in one ML-framework (such as Keras) into another representation (such as ONNX standard).</p><p id="p-0044" num="0062">It is appreciated that the backend compiler <b>220</b> may include multiple levels according to some embodiments. For example, the backend compiler <b>220</b> may include a first level backend compiler <b>222</b>, a second level backend compiler <b>224</b>, a third level backend compiler <b>226</b>, and Nth level backend compiler <b>228</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>. It is appreciated that any number of levels for the backend compiler may be used and that the number of levels shown is for illustrative purposes and should not be construed as limiting the scope of the embodiments. It is appreciated that the output from each level backend compiler is input to its subsequent level backend compiler. It is also appreciated that one or more of the level backend compilers may receive additional data from a source other than other level backend compilers.</p><p id="p-0045" num="0063">It is appreciated that according to some embodiments, the first level backend compiler <b>222</b> and/or the second level backend compiler <b>224</b> and/or the third level backend compiler <b>226</b> and/or the Nth level backend compiler <b>228</b> may be compossible. In other words, the compiler block at each level may be swapped and replaced with a different compiler block, as needed. For example, a first level backend compiler <b>222</b> block may be swapped out with a block that includes experimental algorithm (i.e., provides a different mapping strategy and/or memory allocation, etc.), debug version that tracks and stores additional information regarding the compilation and/or modification to the internal representation and/or meta data, etc. It is appreciated that the second level backend compiler <b>224</b> may or may not be the same as the first level compiler <b>222</b> and that the second level backend compiler <b>224</b> may similarly be compossible. Other compiler levels may also be compossible. As such, different compiler blocks may be adapted and/or optimized depending on the use case, which has become even more important in rapidly emerging technology such as ML. Examples of the use cases may include but is not limited to support of customer-specific deep-learning networks, novel deep-learning networks with new operators, compilation to different ML hardware (i.e., accelerators architecture), etc.</p><p id="p-0046" num="0064">Accordingly, the compiler becomes more flexible, adaptable, debugable, and optimizable that were previously not available. It is appreciated that compossibility of the compiler block aids in development of the overall compiler and provides flexibility to adapt to rapidly changing technologies, e.g., evolving ML models, evolving ML hardware, etc.</p><p id="p-0047" num="0065">It is appreciated that at each level backend compiler one or more structure metadata is generated in addition to the specific tasks/operations being performed by the backend compiler. For example, the first level backend compiler <b>222</b> receives the intermediate data <b>212</b> and performs transformation/optimization, e.g., target specific fusing/composition, specific data/weigh/output layout format adjustment (an example of the data/weight/output layout format adjustment is illustrated in <figref idref="DRAWINGS">FIGS. <b>1</b>B-<b>1</b>G</figref>), target specific drop no operations, auto-layer identification in a subgraph (discussed in more detail with respect to the second level backend compiler <b>224</b> and in <figref idref="DRAWINGS">FIG. <b>2</b>C</figref>). It is appreciated that the first level backend compiler <b>222</b> also generates a structured metadata <b>223</b> that provides information regarding the operations/decisions performed/made by the first level backend compiler <b>222</b>. It is appreciated that the output of the first level backend compiler <b>222</b> is input to the second level backend compiler <b>224</b>.</p><p id="p-0048" num="0066">In some embodiments, the second level backend compiler <b>224</b> in some nonlimiting examples performs a specific multi-layer based optimization (as an example and described in greater detail in <figref idref="DRAWINGS">FIG. <b>2</b>C</figref>). It is appreciated that in some embodiments the second level backend compiler <b>224</b> may receive data from a source other than other backend compilers. For example, the second level backend compiler <b>224</b> may also receive the target configuration for code generation in addition to receiving the output from the first level backend compiler <b>222</b>. It is appreciated that the target configuration received during inference part of the ML operation can be used to determine the number of tiles to use, OCM base address and size, determining whether to pin all memory usages in OCM or not, determining whether to use special starting memory addresses, user received input on the strategy, determining whether to use int8 of fp16 or pre-quantized flow, etc. An example of the target configuration is provided below for illustration purposes and should not be construed as limiting the scope of the embodiments. It is appreciated that the target configuration describes both the hardware architecture specifics, e.g., arch type (M1K in this example), OCM memory size (0x100000), etc., as well as specific compilation instructions, e.g., number of tiles to use such as 26 and the type of quantized network such as int8.<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0067">max_layer=100000</li>        <li id="ul0004-0002" num="0068">quantize=int8</li>        <li id="ul0004-0003" num="0069">arch=m1k</li>        <li id="ul0004-0004" num="0070">inp_quantized_to=uint8</li>        <li id="ul0004-0005" num="0071">out_dequantized_from=uint8</li>        <li id="ul0004-0006" num="0072">dram_addr_relocatable=1</li>        <li id="ul0004-0007" num="0073">ocm_base=0x0</li>        <li id="ul0004-0008" num="0074">ocm_size=0x100000</li>        <li id="ul0004-0009" num="0075">num_tiles=26</li>        <li id="ul0004-0010" num="0076">b=1</li>        <li id="ul0004-0011" num="0077">future-be</li>        <li id="ul0004-0012" num="0078">wb_pin_ocm=0</li>        <li id="ul0004-0013" num="0079">dump_wb</li>        <li id="ul0004-0014" num="0080">new_metadata</li>        <li id="ul0004-0015" num="0081">ext_strategy_file=&#x3c;name&#x3e;</li>    </ul>    </li></ul></p><p id="p-0049" num="0082">In some nonlimiting examples, the computation and data are moved by the compiler <b>120</b> from inference time to compiler time once in compilation in order to reduce computations and data movements at inference runtime. It is appreciated that the second level backend compiler <b>224</b> may use a model, e.g., roofline model, given the target hardware configuration (i.e. ML hardware <b>160</b>) and data layouts, at compilation time to estimate specific runtime performance. It is appreciated that the second level backend compiler <b>224</b> similar to the first level backend compiler <b>222</b> also generates a structured metadata <b>225</b>. The structured metadata <b>225</b> provides information regarding the operations/decisions performed/made by the second level backend compiler <b>224</b>. It is appreciated that the output of the second level backend compiler <b>224</b> is input to the third level backend compiler <b>226</b>.</p><p id="p-0050" num="0083">In some embodiments, the third level backend compiler <b>226</b> may transform the layer subgraph from the structured metadata <b>225</b> to primitive subgraph where each of the primitives may describe a certain algorithmic procedures. In some embodiments, the primitives may perform only computational tasks, only communication tasks between tiles or between tiles and double data rate (DDR), only synchronization tasks, or any combination thereof. For example, the matrix-matrix-multiply primitives LAMM and SAMM are two different computational primitives that are optimized for different matrix shapes. While &#x201c;all to all&#x201d; is a communication primitive, as are halo, rebalance and forward gather which are primitives that perform data movements on distributed tensor data. An example of a combined communication and computation primitive is the flattening overlap. Examples of other algorithmic procedures may include MAXPOOL, direct convolution, padding, scratch, etc. The third level backend compiler <b>226</b> determines mapping, resource allocation, and parallelism that may be applied on a layer by layer case. For example, the third level backend compiler <b>226</b> may determine whether to split input/output on tiles, split weight/bias on tiles, combination of split input/output and weight/bias and serialization on tiles, overlap primitives on tiles, use LAMM as opposed to SAMM1/SAMM2 (described in <figref idref="DRAWINGS">FIG. <b>2</b>D</figref>) based on the manner in which the register files are used, apply direct convolution or flatten math multiplication (flattening followed by matrix-matrix multiply) or flattening matrix-matrix-multiply overlap based on layer configurations and layer format (described in <figref idref="DRAWINGS">FIG. <b>2</b>E</figref>). In some nonlimiting examples, the third level backend compiler <b>226</b> may also determine the number of tiles to use for a layer and the way to split data tensors and their computations among the tiles for that layer. The third level backend compiler <b>226</b> may also determine whether to glue or rebalance and halo tensors or partial tensors and if so the manner of which to do so between different tiles of previous layer and tiles of the next layer. In some nonlimiting examples, the third level backend compiler <b>226</b> may determine the manner by which to sync the rebalance tasks among the tiles, e.g., by applying local sync within a tile, global sync among tiles, barrier for all tiles, sync up between specific producer to specific consumer, etc. As synchronization steps are generally costly operations, different levels of synchronizations are supported by hardware that are often inserted judiciously by the compiler. For example, the PE and POD within a tile can be synchronized using a &#x201c;local sync&#x201d;, which is very light weight as opposed to a global sync among a group of tiles or all tiles that is much more costly. Additionally, synchronization primitives are provided that are optimized as they are limited to specific consumer/producer tiles of a given communication pattern. It is appreciated that in some embodiments, the third level backend compiler <b>226</b> may determine the manner of which to reserve DDR and/or OCM memory regions for full or partial tensors to avoid read write data hazards (i.e. data corruption due to unintentional address reuse for optimization that has reused addresses), manner by which perform serialization, and manner by which to reduce data movement, etc. It is also appreciated that in some embodiments, the third level backend compiler <b>226</b> may determine the manner of which to reserve DDR and/or OCM memory regions for full or partial tensors, to perform serialization and to reduce data movement. In some nonlimiting examples, the third level backend compiler <b>226</b> may pipeline ISA tasks running on the same tile but different processing elements (i.e. PE versus POD) or on different tiles as determined from space-time analysis based on data allocations. Moreover, the third level backend compiler <b>226</b> may generate primitive graphs for representing initial job, per-inference runtime job, and per-inference finishing job based on performance needs. Additionally, the third level backend compiler <b>226</b> may use a primitive roofline model (e.g., given target hardware configuration (i.e., ML hardware <b>160</b>)) at compilation time to estimate the ML hardware <b>160</b> specific runtime performance and once the final runtime performance statistics are collected the primitives may be calibrated and optimized.</p><p id="p-0051" num="0084">It is appreciated that in some embodiments the third level backend compiler <b>226</b> may receive data from a source other than other backend compilers. For example, the third level backend compiler <b>226</b> may also receive a strategy indicated by a user (i.e. user strategy) in addition to receiving the output from the second level backend compiler <b>224</b>, as illustrated below. It is appreciated that the strategy may be an external strategy generated by an analysis/profiling tool which is run external to the compiler flow. It is appreciated that in the following strategy, information for each layer of the fused graph is give. Details such as the type of operation, e.g., convolution or maxpool, the corresponding first and last ONNX operator of the original ONNX graph, the selected strategy and the externally provided strategy hints are given. For the first layer, in this example, the strategy of splitting the input and output among the tiles is applied while the weights and bias tensors are being duplicated. For this example, the hints are matching the applied strategy, but it does not need to be.</p><p id="p-0052" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="left"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>{ &#x201c;file_type&#x201d;: &#x201c;ExtStrategy&#x201d;,</entry></row><row><entry>&#x2002;&#x201c;layers&#x201d;: [</entry></row><row><entry>&#x2003;&#x2002;{ &#x201c;id&#x201d;: 1, &#x201c;op&#x201d;: &#x201c;CONV&#x201d;, &#x201c;to_layer_ids&#x201d;: [ 2 ], &#x201c;first_onnx_op&#x201d;:</entry></row><row><entry>&#x201c;resnetv17_conv0_fwd_transpose&#x201d;, &#x201c;last_onnx_op&#x201d;: &#x201c;resnetv17_relu0_fwd<sub>&#x2014;&#x2014;</sub>1&#x201d;,</entry></row><row><entry>&#x2003;&#x2003;&#x201c;strategy_applied&#x201d;: [ &#x201c;split_io&#x201d;, &#x201c;dupl_wb&#x201d; ],</entry></row><row><entry>&#x2003;&#x2003;&#x201c;external_strategy_hints&#x201d;: [ &#x201c;split_io&#x201d;, &#x201c;dupl_wb&#x201d; ] }</entry></row><row><entry>&#x2003;,{ &#x201c;id&#x201d;: 2, &#x201c;op&#x201d;: &#x201c;MAXPOOL&#x201d;, &#x201c;to_layer_ids&#x201d;: [ 3, 4 ], &#x201c;first_onnx_op&#x201d;:</entry></row><row><entry>&#x201c;resnetv17_pool0_fwd<sub>&#x2014;&#x2014;</sub>1&#x201d;, &#x201c;last_onnx_op&#x201d;: &#x201c;resnetv17_pool0_fwd<sub>&#x2014;&#x2014;</sub>1&#x201d;,</entry></row><row><entry>&#x2003;&#x2003;&#x201c;strategy_applied&#x201d;: [ &#x201c;split_io&#x201d; ],</entry></row><row><entry>&#x2003;&#x2003;&#x201c;external_strategy_hints&#x201d;: [ &#x201c;split_io&#x201d; ] }</entry></row><row><entry>&#x2003;,{ &#x201c;id&#x201d;: 3, &#x201c;op&#x201d;: &#x201c;CONV&#x201d;, &#x201c;to_layer_ids&#x201d;: [ 7 ], &#x201c;first_onnx_op&#x201d;:</entry></row><row><entry>&#x201c;resnetv17_stage1_conv3_fwd&#x201d;, &#x201c;last_onnx_op&#x201d;:</entry></row><row><entry>&#x201c;resnetv17_stage1_batchnorm3_fwd<sub>&#x2014;&#x2014;</sub>1&#x201d;,</entry></row><row><entry>&#x2003;&#x2003;&#x201c;strategy_applied&#x201d;: [ &#x201c;split_io&#x201d;, &#x201c;dupl_wb&#x201d; ],</entry></row><row><entry>&#x2003;&#x2003;&#x201c;external_strategy_hints&#x201d;: [ &#x201c;split_io&#x201d;, &#x201c;dupl_wb&#x201d; ] }</entry></row><row><entry>&#x2003;,{ &#x201c;id&#x201d;: 4, &#x201c;op&#x201d;: &#x201c;CONV&#x201d;, &#x201c;to_layer_ids&#x201d;: [ 5 ], &#x201c;first_onnx_op&#x201d;:</entry></row><row><entry>&#x201c;resnetv17_stage1_conv0_fwd&#x201d;, &#x201c;last_onnx_op&#x201d;: &#x201c;resnetv17_stage1_relu0_fwd<sub>&#x2014;&#x2014;</sub>1&#x201d;,</entry></row><row><entry>&#x2003;&#x2003;&#x201c;strategy_applied&#x201d;: [ &#x201c;split_io&#x201d;, &#x201c;dupl_wb&#x201d; ],</entry></row><row><entry>&#x2003;&#x2003;&#x201c;external_strategy_hints&#x201d;: [ &#x201c;split_io&#x201d;, &#x201c;dupl_wb&#x201d; ] }</entry></row><row><entry>&#x2003;,{ &#x201c;id&#x201d;: 5, &#x201c;op&#x201d;: &#x201c;CONV&#x201d;, &#x201c;to_layer_ids&#x201d;: [ 6 ], &#x201c;first_onnx_op&#x201d;:</entry></row><row><entry>&#x201c;resnetv17_stage1_conv1_fwd&#x201d;, &#x201c;last_onnx_op&#x201d;: &#x201c;resnetv17_stage1_relu1_fwd<sub>&#x2014;&#x2014;</sub>1&#x201d;,</entry></row><row><entry>&#x2003;&#x2003;&#x201c;strategy_applied&#x201d;: [ &#x201c;split_io&#x201d;, &#x201c;dupl_wb&#x201d;, &#x201c;DIRECTCONV&#x201d; ],</entry></row><row><entry>&#x2003;&#x2003;&#x201c;external_strategy_hints&#x201d;: [ &#x201c;split_io&#x201d;, &#x201c;dupl_wb&#x201d; ] }</entry></row><row><entry>&#x2003;,{ &#x201c;id&#x201d;: 6, &#x201c;op&#x201d;: &#x201c;CONV&#x201d;, &#x201c;to_layer_ids&#x201d;: [ 7 ], &#x201c;first_onnx_op&#x201d;:</entry></row><row><entry>&#x201c;resnetv17_stage1_conv2_fwd&#x201d;, &#x201c;last_onnx_op&#x201d;:</entry></row><row><entry>&#x201c;resnetv17_stage1_batchnorm2_fwd<sub>&#x2014;&#x2014;</sub>1&#x201d;,</entry></row><row><entry>&#x2003;&#x2003;&#x201c;strategy_applied&#x201d;: [ &#x201c;split_io&#x201d;, &#x201c;dupl_wb&#x201d; ],</entry></row><row><entry>&#x2003;&#x2003;&#x201c;external_strategy_hints&#x201d;: [ &#x201c;split_io&#x201d;, &#x201c;dupl_wb&#x201d; ] }</entry></row><row><entry>&#x2003;,{ &#x201c;id&#x201d;: 7, &#x201c;op&#x201d;: &#x201c;SUM&#x201d;, &#x201c;to_layer_ids&#x201d;: [ 8, 11 ], &#x201c;first_onnx_op&#x201d;:</entry></row><row><entry>&#x201c;resnetv17_stage1<sub>&#x2014;&#x2014;</sub>plus0<sub>&#x2014;&#x2014;</sub>1&#x201d;, &#x201c;last_onnx_op&#x201d;: &#x201c;resnetv17_stage1_activation0<sub>&#x2014;&#x2014;</sub>1&#x201d;,</entry></row><row><entry>&#x2003;&#x2003;&#x201c;strategy_applied&#x201d;: [ &#x201c;split_io&#x201d; ],</entry></row><row><entry>&#x2003;&#x2003;&#x201c;external_strategy_hints&#x201d;: [ &#x201c;split_io&#x201d; ] }</entry></row><row><entry>&#x2003;,{ &#x201c;id&#x201d;: 8, &#x201c;op&#x201d;: &#x201c;CONV&#x201d;, &#x201c;to_layer_ids&#x201d;: [ 9 ], &#x201c;first_onnx_op&#x201d;:</entry></row><row><entry>&#x201c;resnetv17_stage1_conv4_fwd&#x201d;, &#x201c;last_onnx_op&#x201d;: &#x201c;resnetv17_stage1_relu2_fwd<sub>&#x2014;&#x2014;</sub>1&#x201d;,</entry></row><row><entry>&#x2003;&#x2003;&#x201c;strategy_applied&#x201d;: [ &#x201c;split_io&#x201d;, &#x201c;dupl_wb&#x201d; ],</entry></row><row><entry>&#x2003;&#x2003;&#x201c;external_strategy_hints&#x201d;: [ &#x201c;split_io&#x201d;, &#x201c;dupl_wb&#x201d; ] }</entry></row><row><entry>&#x2003;,{ &#x201c;id&#x201d;: 9, &#x201c;op&#x201d;: &#x201c;CONV&#x201d;, &#x201c;to_layer_ids&#x201d;: [ 10 ], &#x201c;first_onnx_op&#x201d;:</entry></row><row><entry>&#x201c;resnetv17_stage1_conv5_fwd&#x201d;, &#x201c;last_onnx_op&#x201d;: &#x201c;resnetv17_stage1_relu3_fwd<sub>&#x2014;&#x2014;</sub>1&#x201d;,</entry></row><row><entry>&#x2003;&#x2003;&#x201c;strategy_applied&#x201d;: [ &#x201c;split_io&#x201d;, &#x201c;dupl_wb&#x201d;, &#x201c;DIRECTCONV&#x201d; ],</entry></row><row><entry>&#x2003;&#x2003;&#x201c;external_strategy_hints&#x201d;: [ &#x201c;split_io&#x201d;, &#x201c;dupl_wb&#x201d; ] }</entry></row><row><entry>... ] }</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0053" num="0085">It is appreciated that the third level backend compiler <b>226</b> similar to the first and second level backend compilers <b>222</b> and <b>224</b> also generates a structured metadata <b>227</b>. The structured metadata <b>227</b> provides information regarding the operations/decisions performed/made by the third level backend compiler <b>226</b>. It is appreciated that the output of the third level backend compiler <b>226</b> is input to the subsequent level backend compiler(s). It is appreciated that the structured metadata <b>227</b> generated by the third level backend compiler <b>226</b> may be fed back into the third level backend compiler <b>226</b> in order to reduce the number of primitives (an example is described in <figref idref="DRAWINGS">FIG. <b>2</b>F</figref>).</p><p id="p-0054" num="0086">A nonlimiting example of the structured metadata <b>227</b> is shown below for illustration purposes. Below the structured metadata for DDR or OCM regions and inputs, flattening addresses, weights and bias outputs, etc., is shown. The following is a nonlimiting example of a structured metadata for convolution layer. The first part of the structured metadata provides information regarding the input, weight, bias, and outputs tensors regarding shape, format, name in original network graph and local memory allocation. Moreover, pertinent information regarding the convolution operation is given such as stride in this example. The second section of the structure metadata here provides the sequential call list of the calls to the ML-library and the specific arguments.</p><p id="p-0055" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="259pt" align="left"/><thead><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>//</entry><entry>inputs: [</entry></row><row><entry>//</entry><entry>&#x2002;{ N: 1, inH: 32, inW: 32, inC: 1, inCStride: 1, dataFormat: NHWC, name:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="left"/><tbody valign="top"><row><entry>permute_input_0, ddr_addr: 0x36780 }</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>],</entry></row><row><entry>//</entry><entry>stride: [ 1, 1 ],</entry></row><row><entry>//</entry><entry>flattening: { ocm_addr_start: 0xfd5c0, ocm_addr_end: 0xff400 }</entry></row><row><entry>//</entry><entry>weight: { outC: 64, kH: 3, kW: 3, inC: 1, name: mrvl_1_const_0, ddr_addr: 0x180,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="left"/><tbody valign="top"><row><entry>ocm_addr_start: 0x0, ocm_addr_end: 0x7ff },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>bias: { kind: FP32, outC: 64, name: mrvl_1_const_1, ddr_addr: 0x0, ocm_addr_start:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="left"/><tbody valign="top"><row><entry>0x800, ocm_addr_end: 0x8ff },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>outputs: [</entry></row><row><entry>//</entry><entry>&#x2002;{ N: 1, outH: 30, outW: 30, outC: 64, outCStride: 64, name: mrvl_1_Conv2D,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="left"/><tbody valign="top"><row><entry>ocm_addr_start: 0xe11c0, ocm_addr_end: 0xfd400 }</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>],</entry></row><row><entry>//</entry><entry>mllib_call_list: [</entry></row><row><entry>//</entry><entry>&#x2003;{ layer_id: 2, inst_id: 1, name: Input, type: F_ACTIV, instrType:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="left"/><tbody valign="top"><row><entry>DDRTOLOCALCOPY_LINEAR_SEGMENTED_PERTILE,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;tilemask: 0x1, ddrStartAddr: 0x36780, localStartAddr: 0xff7c0, length: 2048</entry></row><row><entry>//</entry><entry>&#x2003;, tileOffset: 2048, signExt: 0 },</entry></row><row><entry>//</entry><entry>&#x2003;{ layer_id: 2, inst_id: 4, name: Flatten_Lamm_Overlap, type:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="left"/><tbody valign="top"><row><entry>F_FLATTEN_OVERLAP, instrType: OVERLAP_FLAT1,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;tilemask: 0x1, localStartAddr: 0xff7c0, localOutputAddr: 0xfd5c0</entry></row><row><entry>//</entry><entry>&#x2003;, inH: 32, inW: 32, inC: 1, kH: 3, kW: 3, kC: 1, lineStride: 1, tileOffset:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="left"/><tbody valign="top"><row><entry>4294967295 },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;{ layer_id: 2, inst_id: 10, name: Flatten_Lamm_Overlap, type: F_LAMM,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="left"/><tbody valign="top"><row><entry>instrType: OVERLAP_MULT1,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;tilemask: 0x1, numRow: 64, arowBcol: 9, numCol: 60, amtxAddr: 0x0, bmtxAddr:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="left"/><tbody valign="top"><row><entry>0xfd5c0, cmtxAddr: 0xe11c0, biasAddr: 0x800</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;, rscale: 1, rshift: 0, dscale: 1, qscale: 1, doRelu: 1, doTanhSigmoid: 0, tileOffset:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="left"/><tbody valign="top"><row><entry>4294967295 },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>{ layer_id: 2, inst_id: 19, name: Flatten_Lamm_Overlap, type:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="left"/><tbody valign="top"><row><entry>F_FLATTEN_OVERLAP, instrType: OVERLAP_FLAT1,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;tilemask: 0x1, localStartAddr: 0xff7c0, localOutputAddr: 0xfd5c0</entry></row><row><entry>//</entry><entry>&#x2003;, inH: 32, inW: 32, inC: 1, kH: 3, kW: 3, kC: 1, lineStride: 1, tileOffset:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="left"/><tbody valign="top"><row><entry>4294967295 },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;{ layer_id: 2, inst_id: 25, name: Flatten_Lamm_Overlap, type: F_LAMM,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="left"/><tbody valign="top"><row><entry>instrType: OVERLAP_MULT1,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;tilemask: 0x1, numRow: 64, arowBcol: 9, numCol: 60, amtxAddr: 0x0, bmtxAddr:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="left"/><tbody valign="top"><row><entry>0xfd5c0, cmtxAddr: 0xe11c0, biasAddr: 0x800</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;, rscale: 1, rshift: 0, dscale: 1, qscale: 1, doRelu: 1, doTanhSigmoid: 0, tileOffset:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="left"/><tbody valign="top"><row><entry>4294967295 },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;{ layer_id: 2, inst_id: 34, name: Flatten_Lamm_Overlap, type:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="left"/><tbody valign="top"><row><entry>F_FLATTEN_OVERLAP, instrType: OVERLAP_FLAT1,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;tilemask: 0x1, localStartAddr: 0xff7c0, localOutputAddr: 0xfd5c0</entry></row><row><entry>//</entry><entry>&#x2003;, inH: 32, inW: 32, inC: 1, kH: 3, kW: 3, kC: 1, lineStride: 1, tileOffset:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="left"/><tbody valign="top"><row><entry>4294967295 },</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0056" num="0087">Another example of a structured metadata <b>227</b> is shown below for illustration purposes. In the example below the strategy to map to 8 tiles is illustrated and illustrates how the input tensor is split among the tiles, rebalanced, haloed, and how the output tensors are split after computation. In this nonlimiting example, the maxpool layer is executed in parallel on 8 tiles. Here, the structured metadata provides information regarding the applied strategy and the mapping information of the data across 8 tiles when a row-wise split is applied. Moreover, the information includes the number of rows including padded rows as well as the number of halo rows on each tile.</p><p id="p-0057" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><thead><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>//</entry><entry>Layer 3: MAXPOOL-INT8</entry></row><row><entry>//</entry><entry>**********</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>//INSTRUMENTATION_BEGIN</entry></row><row><entry>//json_annotation: {</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2002;strategy_plan: { split_io: yes, split_wb: no, split_in_time: no, overlap: no },</entry></row><row><entry>//</entry><entry>&#x2002;code_gen: { io: split_io, wb: unknown },</entry></row><row><entry>//</entry><entry>&#x2002;mapping_info: {</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;batch_size: 1,</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;per_batch_num_tiles: 8,</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;per_batch_mapping_list: [</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;{ tid: 0, input: 14, padded: 15, to_nxt: &#x2212;1, halo: 1, image: 15, y_steps: 7, x_steps:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>56, output: 7 },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;{ tid: 1, input: 14, padded: 14, to_nxt: &#x2212;1, halo: 1, image: 15, y_steps: 7, x_steps:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>56, output: 7 },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;{ tid: 2, input: 14, padded: 14, to_nxt: &#x2212;1, halo: 1, image: 15, y_steps: 7, x_steps:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>56, output: 7 },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;{ tid: 3, input: 14, padded: 14, to_nxt: &#x2212;1, halo: 1, image: 15, y_steps: 7, x_steps:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>56, output: 7 },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;{ tid: 4, input: 14, padded: 14, to_nxt: &#x2212;1, halo: 1, image: 15, y_steps: 7, x_steps:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>56, output: 7 },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;{ tid: 5, input: 14, padded: 14, to_nxt: &#x2212;1, halo: 1, image: 15, y_steps: 7, x_steps:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>56, output: 7 },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;{ tid: 6, input: 14, padded: 14, to_nxt: &#x2212;1, halo: 1, image: 15, y_steps: 7, x_steps:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>56, output: 7 },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;{ tid: 7, input: 14, padded: 15, to_nxt: 0, halo: 0, image: 16, y_steps: 7, x_steps: 56,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>output: 7 },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;],</entry></row><row><entry>//</entry><entry>&#x2002;},</entry></row><row><entry>//</entry><entry>&#x2002;inputs: [</entry></row><row><entry>//</entry><entry>&#x2003;{ N: 1, inH: 112, inW: 112, inC: 64, inCStride: 64, dataFormat: NHWC, name:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>resnetv17_relu0_fwd<sub>&#x2014;&#x2014;</sub>1, ocm_addr_start: 0xe3fc0, ocm_addr_end: 0x100000 }</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2002;],</entry></row><row><entry>//</entry><entry>&#x2002;pad: { top: 1, left: 1, bottom: 1, right: 1, ocm_addr_start: 0xc9340, ocm_addr_end:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>0xe3f3f },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2002;stride: [ 2, 2 ],</entry></row><row><entry>//</entry><entry>&#x2002;kernel: { kH: 3, kW: 3, name: data },</entry></row><row><entry>//</entry><entry>&#x2002;outputs: [</entry></row><row><entry>//</entry><entry>&#x2003;{ N: 1, outH: 56, outW: 56, outC: 64, outCStride: 64, name:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>resnetv17_pool0_fwd<sub>&#x2014;&#x2014;</sub>1, ocm_addr_start: 0xf9dc0, ocm_addr_end: 0x100000 }</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0058" num="0088">Other level backend compilers may perform other operations and make other decisions. For example, other backend level compilers may perform functions based on specified attributes for the primitives, e.g., forming a set of common ML library and application programming interface (APIs), in order to generate ISA tasks codes to fulfill the need for all primitives for the ML hardware <b>160</b>. In some nonlimiting examples, based on specified ML library APIs with their arguments, the particular level backend compiler may generate the appropriate ISA task codes to utilize the ML hardware <b>160</b> in a streaming fashion, as an example. It is appreciated that for each ML library API with its arguments, a per ML library API roofline model is used, at the time that the code is being generated, to estimate the target specific runtime performance and to monitor and check performance with respect to each ISA instruction, and/or to determine boundary violations (attributes that lead to memory wrap around or data hazard ISA instructions being produced due to memory address reuse). It is appreciated that at the time that the compiler calls the ML library API, the arguments to the library call have all the pertinent information regarding tensors and the arithmetical operations to be performed. Thus, a roofline model can be computed for this specific API call which will provide an estimate target specific runtime of these arithmetical operations. Accordingly, the compiler can iteratively decide on which API to call in cases where multiple different APIs are available to perform the same arithmetical operations. In some nonlimiting examples, other operations/decisions may include a model binary analyzer subcomponent that performs an overall analysis to identify potential problems in the low-level instructions (i.e. generate model binary), e.g., ill-formed OCM memory overlapping between ISA tasks/instructions, data hazard between consumer-producer tasks, etc. It is appreciated that these other level backend compilers may also generate their respective structured metadata that provide information regarding the operations/decisions performed/made by their respective level backend compiler. It is appreciated that the generated structured metadata and other output from other backend compilers are input to the Nth level backend compiler <b>228</b> as an input.</p><p id="p-0059" num="0089">The Nth level backend compiler <b>228</b> in some nonlimiting examples performs ahead of time (AOT) compilation while inference on the ML hardware <b>160</b> accelerators and/or other processing units (e.g., CPU) run in real time. In some examples, the Nth level backend compiler <b>228</b> generates performance statistics for the inference run associated with the ML hardware <b>160</b>. The Nth level backend compiler <b>228</b> may decide on whether to run the compiled code on the ML hardware <b>160</b>, on its software emulator, or on a full machine emulator with the ML hardware <b>160</b> submodules. Based on the performance statistics, certain aspects of the system may be optimized, e.g., calibrate and optimize the generated code, the primitives, etc. It is appreciated that the Nth level backend compiler <b>228</b> also generates the low-level instructions for execution by the ML hardware <b>160</b>. The Nth level backend compiler <b>228</b> also generates a structured metadata <b>229</b>. The structured metadata <b>229</b> provides information regarding the operations/decisions performed/made by the Nth level backend compiler <b>228</b>.</p><p id="p-0060" num="0090">It is appreciated that even though not shown, one or more outputs of a given level backend compiler may be fed as a feedback loop into itself or to a preceding level backend compiler. For example, in some nonlimiting examples the output of the third level backend compiler <b>226</b> may be fed back into itself for optimization while the output of the Nth level backend compiler <b>228</b> may be fed back into the first level backend compiler <b>222</b>, the third level backend compiler <b>226</b>, etc. It is also appreciated that one or more of the level backend compilers may receive additional data from a source other than other level backend compilers.</p><p id="p-0061" num="0091">Referring now to <figref idref="DRAWINGS">FIG. <b>2</b>C</figref>, auto-layer identification in a subgraph according to some embodiments is shown. In this nonlimiting example nodes 1-9 and a-p are executable nodes of an ML layer. The graph shows executable nodes where nodes (i.e. e, j, k, n, o) that are not supported or unsuited for execution on the ML hardware <b>160</b> are shaded for illustration purposes. As described above, the first level backend compiler <b>222</b> may make a determination to split the graph of nodes <b>252</b> to two subgraph nodes <b>254</b> and <b>256</b>, as illustrated on the right hand side, where the subgraph nodes <b>254</b> will be executed by the ML hardware <b>160</b> and the subgraph nodes <b>256</b> will be executed by a processing component other than the ML hardware <b>160</b>, e.g., a CPU.</p><p id="p-0062" num="0092">The first subgraph <b>254</b> is created based on nodes that are suited for execution on the ML hardware <b>160</b> layer (e.g., not only supported but also efficient to be executed by the ML hardware <b>160</b>). The second subgraph <b>256</b> is created based on nodes that contains nodes that are better suited for execution on a processor other than the ML hardware <b>160</b>. In this example, the first level backend compiler <b>222</b> has determined that even though nodes d, f, h, m, l, and p are executable on the ML hardware <b>160</b> (e.g., native to the ML hardware <b>160</b>), it is more efficient for them to be executed on a non-ML hardware component, e.g., CPU, along with other nodes, e.g., e, j, k, n, o. It is appreciated that the nodes e, j, k, n, and o may be nonnative to the ML hardware <b>160</b> and as such better suited to be executed by a different component. Also, since nodes f, h, l, m, and p are intertwined in such a way with nodes e, j, k, n, and o that increases the overhead (e.g., storage, data movement, processing, etc.) the first level compiler <b>222</b> may determine that it is more efficient for those node layers to be executed with nodes e, j, k, n, and o. As such, nodes f, l, h, m, and p are defused back to original intermediate data (i.e. intermediate representation) operations. The defused nodes are illustrated as f&#x2033;, l&#x2033;, h&#x2033;, m&#x2033;, and p&#x2033;. Moreover, it is appreciated that the output from nodes c, i, and d from subgraph <b>254</b> is input to subgraph <b>256</b>.</p><p id="p-0063" num="0093">Referring now to <figref idref="DRAWINGS">FIG. <b>2</b>D</figref>, a nonlimiting example of a decision/operation by the third level backend compiler is shown. In this example, the third level backend compiler <b>226</b> determines whether to use LAMM or SAMM and whether to split input/output on tiles, split weight/bias on tiles, combination of split input/output and weight/bias and serialization on tiles, overlap primitives on tiles, for a multiplication. It is appreciated that in some nonlimiting examples, the third level backend compiler <b>226</b> may determine that LAMM is better suited as opposed to SAMM when a batch size is 8 because it requires no transpose but SAMM does, which requires copying and as such increases the execution time. In some embodiments, the weight and the input size is also factored into the decision being made of the third level backend compiler <b>226</b>. The table of <figref idref="DRAWINGS">FIG. <b>2</b>D</figref> is provided as an example. Areg and Breg are registers within the POD of the ML hardware <b>160</b>. Moreover, the decision may be based on whether to split I/O or to split weight.</p><p id="p-0064" num="0094">Referring now to <figref idref="DRAWINGS">FIG. <b>2</b>E</figref>, another example of a determination by the third level backend compiler according to some embodiments is shown. <figref idref="DRAWINGS">FIG. <b>2</b>E</figref> illustrates strategy and mapping A and strategy and mapping B based on the configuration using different primitives for performing a convolution followed by MaxPool layer. It is appreciated that strategy A uses a flattening and Lamm overlap followed by maxpool. For illustration purposes, the PE and POD on each tile work concurrently to execute the flattening and Lamm overlap which is subsequently followed by the PE performing the maxpool operation. Strategy B uses a different approach for the convolution (DirectConv) which is executed on POD while the MaxPool is executed on PE concurrently using pipelining.</p><p id="p-0065" num="0095">Referring now to <figref idref="DRAWINGS">FIG. <b>2</b>F</figref>, a nonlimiting example of using a generated structured metadata by a level backend compiler to reduce the number of primitives is shown. In this example, layer A <b>292</b> may call into primitives A1 and A2 and it may fork into layer B <b>293</b> and layer C <b>294</b>. In this nonlimiting example, the layer B calls into layer D and primitives B2 and B3 whereas layer C <b>294</b> calls into layer D and primitive C2. A backend compiler, e.g., the third level backend compiler <b>226</b>, may optimize to reduce the number of primitives using the generated structure metadata <b>227</b>. For example, the third level backend compiler <b>226</b> may determine that the layer D occurs twice and as such one can be reduced. Accordingly, the optimization may form layer A <b>295</b> calling into primitives A1 and A2 and layer D that forks into layer B <b>296</b> calling into primitives B2 and B3 and layer C <b>297</b> that calls into primitive C2. As illustrated, the call into layer D is moved into the layer A rather than being called separately by layer B and C each. As such, the structured metadata can be used by the backend compiler itself for optimization purposes.</p><p id="p-0066" num="0096">Referring now to <figref idref="DRAWINGS">FIG. <b>2</b>G</figref>, a nonlimiting example of a backend compiler that receives a first layer in CHW format and how it maps it to the tiles and performs the required padding according to some embodiments is shown. In some examples, the first layer is received as an input in CHW format and it may be transposed to HWC format (as described above) as part of the flattening process in order to nicely map the convolution into a standard matrix-matrix multiply based on the POD architecture. In this example, the size of the padding is 3 and the input is in CHW form for a batch size of 3&#xd7;224&#xd7;224. It is appreciated that in some embodiments, no flattening may be needed and as such the transpose might be needed as part of the output of the previous layer or as a separate step in the input layer. In this nonlimiting example, the slicing to map to the tiles is a batch of 8 across 64 tiles, each input is split across 8 tiles and is row-wise (i.e., &#x3c;35, 35, 35, 35, 35, 35, 35, 19&#x3e; for tiles &#x3c;7, . . . , 0&#x3e;.</p><p id="p-0067" num="0097">As described above, the backend compiler may generate one or more structure metadata. Below is another example of a code followed by the backend compiler generated metadata that illustrates the input, the weight, and the bias constants and output for a fp16 network for illustration purposes. In this nonlimiting example, a convolution layer in a network that is reduced to fp16 precision is illustrated. The structured metadata first describes the tensors involved in this operation in addition to operator specific arguments such as padding and stride information. The total number of multiply and accumulate (MAC) instructions are given. The second part of the structured metadata describes the memory layout of the tensors.</p><heading id="h-0005" level="2">Layer 1: Conb</heading><p id="p-0068" num="0000"><ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0098">Input[1]: float16, [batch, inC, H, W]=[1, 1, 32, 32]</li>        <li id="ul0006-0002" num="0099">Weight: float16, [outC, inC, H, W]=[64, 1, 3, 3]</li>        <li id="ul0006-0003" num="0100">Bias: float, [64]</li>        <li id="ul0006-0004" num="0101">Padding: [top, left, bottom, right]=0, 0, 0, 0</li>        <li id="ul0006-0005" num="0102">Stride: [h, w]=[1, 1]</li>        <li id="ul0006-0006" num="0103">Activation: relu</li>        <li id="ul0006-0007" num="0104">output: float16, [batch, H, W, outC]=[1, 30, 30, 64]</li>        <li id="ul0006-0008" num="0105"># of MACS: 1036800</li>        <li id="ul0006-0009" num="0106"># of Parameters: 640</li>    </ul>    </li></ul></p><p id="p-0069" num="0000"><tables id="TABLE-US-00004" num="00004"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="left"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>//json_annotation:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="21pt" align="left"/><colspec colname="2" colwidth="238pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>code_gen: { io: split_io, wb: dupl_wb },</entry></row><row><entry>//</entry><entry>mapping_info: {</entry></row><row><entry>//</entry><entry>&#x2003;batch_size: 1,</entry></row><row><entry>//</entry><entry>&#x2003;per_batch_num_tiles: 8,</entry></row><row><entry>//</entry><entry>&#x2002;},</entry></row><row><entry>//</entry><entry>inputs: [</entry></row><row><entry>//</entry><entry>&#x2002;{ N: 1, inH: 32, inW: 32, inC: 1, inCStride: 1, dataFormat: NCHW, name:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>permute_input_0, ddr_addr: 0x36780 }</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="21pt" align="left"/><colspec colname="2" colwidth="238pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>],</entry></row><row><entry>//</entry><entry>weight: { outC: 64, kH: 3, kW: 3, inC: 1, name: conv2d_kernel_01, ddr_addr:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>0x32300, ocm_addr_start: 0x0, ocm_addr_end: 0x7ff },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="21pt" align="left"/><colspec colname="2" colwidth="238pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>bias: { kind: FP32, outC: 64, name: conv2d_bias_0, ddr_addr: 0x32080,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>ocm_addr_start: 0x800, ocm_addr_end: 0x8ff },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="21pt" align="left"/><colspec colname="2" colwidth="238pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>outputs: [</entry></row><row><entry>//</entry><entry>&#x2002;{ N: 1, outH: 30, outW: 30, outC: 64, outCStride: 64, name: conv2d_Relu1,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>ocm_addr_start: 0xfc380, ocm_addr_end: 0xfffbf }</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="21pt" align="left"/><colspec colname="2" colwidth="238pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2002;]</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0070" num="0107">Below is yet another example of a code followed by the backend compiler generated metadata that illustrates quantized network for illustration purposes. In this nonlimiting example, the same convolution layer as in the previous example is shown except that in this example a network is quantized to int8. The structured metadata first describes the tensors involved in this operation in addition to operator specific arguments such as padding and stride information. The total number of MAC instructions are given. The second part of the structured metadata describes the memory layout of the tensors.</p><heading id="h-0006" level="2">Layer 1: Conv</heading><p id="p-0071" num="0000"><ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0108">Input[1]: uint8, [batch, inC, H, W]=[1, 1, 32, 32]</li>        <li id="ul0008-0002" num="0109">Weight: int8, [outC, inC, H, W]=[64, 1, 3, 3]</li>        <li id="ul0008-0003" num="0110">Bias: int32, [64]</li>        <li id="ul0008-0004" num="0111">Padding: [top, left, bottom, right]=0, 0, 0, 0</li>        <li id="ul0008-0005" num="0112">Stride: [h, w]=[1, 1]</li>        <li id="ul0008-0006" num="0113">Activation: relu</li>        <li id="ul0008-0007" num="0114">output: uint8, [batch, H, W, outC]=[1, 30, 30, 64]</li>        <li id="ul0008-0008" num="0115"># of MACS: 1036800</li>        <li id="ul0008-0009" num="0116"># of Parameters: 640</li>    </ul>    </li></ul></p><p id="p-0072" num="0000"><tables id="TABLE-US-00005" num="00005"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="left"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>//INSTRUMENTATION_BEGIN</entry></row><row><entry>//json_annotation: {</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="21pt" align="left"/><colspec colname="2" colwidth="238pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>code_gen: { io: split_io, wb: dupl_wb },</entry></row><row><entry>//</entry><entry>mapping_info: {</entry></row><row><entry>//</entry><entry>&#x2003;batch_size: 1,</entry></row><row><entry>//</entry><entry>&#x2003;per_batch_num_tiles: 8,</entry></row><row><entry>//</entry><entry>},</entry></row><row><entry>//</entry><entry>inputs: [</entry></row><row><entry>//</entry><entry>&#x2002;{ N: 1, inH: 32, inW: 32, inC: 1, inCStride: 1, dataFormat: NCHW, name:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>permute_input_0, ddr_addr: 0x1b540 }</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="21pt" align="left"/><colspec colname="2" colwidth="238pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>],</entry></row><row><entry>//</entry><entry>weight: { outC: 64, kH: 3, kW: 3, inC: 1, name: conv2d_kernel_01, ddr_addr:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>0x19300, ocm_addr_start: 0x0, ocm_addr_end: 0xfff },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="21pt" align="left"/><colspec colname="2" colwidth="238pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>bias: { kind: INT32, outC: 64, name: conv2d_bias_0, ddr_addr: 0x19080,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>ocm_addr_start: 0x1000, ocm_addr_end: 0x10ff },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="21pt" align="left"/><colspec colname="2" colwidth="238pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>outputs: [</entry></row><row><entry>//</entry><entry>&#x2002;{ N: 1, outH: 30, outW: 30, outC: 64, outCStride: 64, name: conv2d_Relu1,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>ocm_addr_start: 0xfe180, ocm_addr_end: 0xfffbf }</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="21pt" align="left"/><colspec colname="2" colwidth="238pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>]</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0073" num="0117">As yet another example, the code as generated by the backend compiler and its structured metadata is shown below for illustration purposes. In this nonlimiting example a low-level ISA instructions to be executed on ML hardware <b>160</b> is shown which is augmented with the structured metadata that are provided as comments that are excluded at runtime by the hardware.</p><p id="p-0074" num="0000"><tables id="TABLE-US-00006" num="00006"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><thead><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>//</entry><entry>**********</entry></row><row><entry>//</entry><entry>Layer 2: CONV-INT8</entry></row><row><entry>//</entry><entry>**********</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>//INSTRUMENTATION_BEGIN</entry></row><row><entry>//json_annotation: {</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2002;strategy_plan: { split_io: yes, split_wb: no, split_in_time: no, overlap: no, matmul:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>DIRECTCONV },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2002;code_gen: { io: split_io, wb: dupl_wb },</entry></row><row><entry>//</entry><entry>&#x2002;mapping_info: {</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;batch_size: 8,</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;per_batch_num_tiles: 1,</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;per_batch_mapping_list: [</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;{ tid: 0, input: 16, padded: 20, to_nxt: 0, halo: 0, image: 20, y_steps: 16, x_steps:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>16, output: 16 },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;],</entry></row><row><entry>//</entry><entry>&#x2002;},</entry></row><row><entry>//</entry><entry>&#x2002;inputs: [</entry></row><row><entry>//</entry><entry>&#x2003;{ N: 8, inH: 16, inW: 16, inC: 128, inCStride: 128, dataFormat: NHWC, name:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>input, ddr_addr: 0x32100 }</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2002;],</entry></row><row><entry>//</entry><entry>&#x2002;pad: { top: 2, left: 2, bottom: 2, right: 2, ocm_addr_start: 0xe93c0, ocm_addr_end:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>0xf5c00 },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2002;stride: [ 1, 1 ],</entry></row><row><entry>//</entry><entry>&#x2002;weight: { outC: 64, kH: 5, kW: 5, inC: 128, name: conv2d_kernel_0_0, ddr_addr:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>0x100, ocm_addr_start: 0x0, ocm_addr_end: 0x31fff },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2002;bias: { kind: INT32, outC: 64, name: conv2d_bias_0, ddr_addr: 0x0, ocm_addr_start:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>0x32000, ocm_addr_end: 0x320ff },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2002;FC-fwd-gather: { ocm_addr_start: 0xc8fc0, ocm_addr_end: 0xe9000 },</entry></row><row><entry>//</entry><entry>&#x2002;outputs: [</entry></row><row><entry>//</entry><entry>&#x2003;{ N: 8, outH: 16, outW: 16, outC: 64, outCStride: 64, name: conv2d,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>ocm_addr_start: 0xc8fc0, ocm_addr_end: 0xe9000, ddr_addr: 0x72100 }</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2002;],</entry></row><row><entry>//</entry><entry>&#x2002;mllib_call_list: [</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;{ layer_id: 2, inst_id: 1, name: Input, type: F_ACTIV, instrType:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>DDRTOLOCALCOPY_LINEAR_SEGMENTED_PERTILE,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;tilemask: 0xff, ddrStartAddr: 0x32100, localStartAddr: 0xf5fc0, length: 32768</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;, tileOffset: 32768, signExt: 1 },</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;{ layer_id: 2, inst_id: 4, name: --Padding, type: F_PADDING, instrType:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>INT8_MEMSET,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;tilemask: 0xff, localOutputAddr: 0xe93c0, length: 128</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;, lineStride: 128, numLines: 40, numBlks: 0, tileOffset: 4294967295 },</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;{ layer_id: 2, inst_id: 8, name: --Padding, type: F_PADDING, instrType:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>INT8_MEMSET,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;tilemask: 0xff, localOutputAddr: 0xea7c0, length: 256</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;, lineStride: 2560, numLines: 18, numBlks: 0, tileOffset: 4294967295 },</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;{ layer_id: 2, inst_id: 12, name: --Padding, type: F_PADDING, instrType:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>LOCAL_FWD_SCATTER,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;tilemask: 0xff, localStartAddr: 0xf5fc0, localOutputAddr: 0xea8c0, length: 128</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;, lineStride: 128, numLines: 16, blkStride: 2560, numBlks: 16, tileOffset:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>4294967295 },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;{ layer_id: 2, inst_id: 16, name: --Padding, type: F_PADDING, instrType:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>INT8_MEMSET,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;tilemask: 0xff, localOutputAddr: 0xeb0c0, length: 256</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;, lineStride: 2560, numLines: 18, numBlks: 0, tileOffset: 4294967295 },</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;{ layer_id: 2, inst_id: 20, name: --Padding, type: F_PADDING, instrType:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>INT8_MEMSET,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;tilemask: 0xff, localOutputAddr: 0xf47c0, length: 128</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;, lineStride: 128, numLines: 40, numBlks: 0, tileOffset: 4294967295 },</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;{ layer_id: 2, inst_id: 24, name: Direct_Conv, type: F_DIRECT_CONV, instrType:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>UNKNOWN,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;tilemask: 0xff, localStartAddr: 0x0, localOutputAddr: 0xc4bc0, srcAddr: 0xe93c0,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>biasAddr: 0x32000</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;, inH: 20, inW: 20, inC: 128, outC: 64, kH: 5, kW: 5, kC: 128, strideH: 1, strideW:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>1, dstStride: 64, rscale: 1347632000, rshift: 42, dscale: 1, qscale: 1, doRelu: 0,</entry></row><row><entry>doTanhSigmoid: 0, tileOffset: 4294967295 },</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;{ layer_id: 2, inst_id: 35, name: all2all, type: F_ALL_TO_ALL, instrType:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>COPY_REMOTE_ABS,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;tilemask: 0xff, srcAddr: 0xc4bc0, destAddr: 0xc8fc0, length: 64</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;, numLines: 256, numBlks: 1, stTile: 0, endTile: 0, tileOffset: 1, groupOfTile: 8 },</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;{ layer_id: 2, inst_id: 41, name: ALL2ALL Global Sync, type: F_GLOB_SYNC,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>instrType: COPY_REMOTE_ABS,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;tilemask: 0xff</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;, tileOffset: 4294967295 },</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;{ layer_id: 2, inst_id: 43, name: output, type: F_OUTPUT, instrType: UNKNOWN,</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;tilemask: 0x1, ddrOutputAddr: 0x72100, localOutputAddr: 0xc8fc0, length:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>131072</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;, tileOffset: 4294967295 },</entry></row><row><entry>//</entry><entry>&#x2002;],</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>//}</entry></row><row><entry>//INSTRUMENTATION_END</entry></row><row><entry>//{ layer_id: 2, inst_id: 1, name: Input, type: F_ACTIV, instrType:</entry></row><row><entry>DDRTOLOCALCOPY_LINEAR_SEGMENTED_PERTILE,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;tilemask: 0xff, ddrStartAddr: 0x32100, localStartAddr: 0xf5fc0, length: 32768</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;, tileOffset: 32768, signExt: 1 }</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>//1</entry></row><row><entry>DMATaskBcst 2 0xff 0 0 0 0 0 0 0</entry></row><row><entry>DMAUnrollTaskLoopPerMaskBit 1 1 1 32768 0 0 0 0 0 0</entry></row><row><entry>DMA_DDR_to_OCM 0x32100 0xf5fc0 256 256 128 32768 1 1</entry></row><row><entry>//{ layer_id: 2, inst_id: 4, name: --Padding, type: F_PADDING, instrType:</entry></row><row><entry>INT8_MEMSET,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;tilemask: 0xff, localOutputAddr: 0xe93c0, length: 128</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;, lineStride: 128, numLines: 40, numBlks: 0, tileOffset: 4294967295 }</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>//2</entry></row><row><entry>PETaskBcst 3 0xff 1 0 0 0 0 0 0</entry></row><row><entry>PELoadRegImm 0x1 0 16 0 1 1 1</entry></row><row><entry>PEMove 0x1f 0x1 16 0 0 1 1 320</entry></row><row><entry>PEStoreStream 0xe93c0 16 16 8 128 40 0 0 0</entry></row><row><entry>//{ layer_id: 2, inst_id: 8, name: --Padding, type: F_PADDING, instrType:</entry></row><row><entry>INT8_MEMSET,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;tilemask: 0xff, localOutputAddr: 0xea7c0, length: 256</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;, lineStride: 2560, numLines: 18, numBlks: 0, tileOffset: 4294967295 }</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>//3</entry></row><row><entry>PETaskBcst 3 0xff 1 0 0 0 0 0 0</entry></row><row><entry>PELoadRegImm 0x1 0 16 0 1 1 1</entry></row><row><entry>PEMove 0x1f 0x1 16 0 0 1 1 288</entry></row><row><entry>PEStoreStream 0xea7c0 16 16 16 2560 18 0 0 0</entry></row><row><entry>//{ layer_id: 2, inst_id: 12, name: --Padding, type: F_PADDING, instrType:</entry></row><row><entry>LOCAL_FWD_SCATTER,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>tilemask: 0xff, localStartAddr: 0xf5fc0, localOutputAddr: 0xea8c0, length: 128</entry></row><row><entry>//</entry><entry>, lineStride: 128, numLines: 16, blkStride: 2560, numBlks: 16, tileOffset:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>4294967295 }</entry></row><row><entry>//4</entry></row><row><entry>PETaskBcst 3 0xff 1 0 0 0 0 0 0</entry></row><row><entry>Loop 2 1 2 2048 2 2 2560 0 0 0 16</entry></row><row><entry>PELoadStream1 0xf5fc0 64 64 2 128 16</entry></row><row><entry>PEStoreStream 0xea8c0 64 64 2 128 16 0 0 0</entry></row><row><entry>//{ layer_id: 2, inst_id: 16, name: --Padding, type: F_PADDING, instrType:</entry></row><row><entry>INT8_MEMSET,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;tilemask: 0xff, localOutputAddr: 0xeb0c0, length: 256</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;, lineStride: 2560, numLines: 18, numBlks: 0, tileOffset: 4294967295 }</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>//5</entry></row><row><entry>PETaskBcst 3 0xff 1 0 0 0 0 0 0</entry></row><row><entry>PELoadRegImm 0x1 0 16 0 1 1 1</entry></row><row><entry>PEMove 0x1f 0x1 16 0 0 1 1 288</entry></row><row><entry>PEStoreStream 0xeb0c0 16 16 16 2560 18 0 0 0</entry></row><row><entry>//{ layer_id: 2, inst_id: 20, name: --Padding, type: F_PADDING, instrType:</entry></row><row><entry>INT8_MEMSET,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;tilemask: 0xff, localOutputAddr: 0xf47c0, length: 128</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;, lineStride: 128, numLines: 40, numBlks: 0, tileOffset: 4294967295 }</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>//6</entry></row><row><entry>PETaskBcst 3 0xff 1 0 0 0 0 0 0</entry></row><row><entry>PELoadRegImm 0x1 0 16 0 1 1 1</entry></row><row><entry>PEMove 0x1f 0x1 16 0 0 1 1 320</entry></row><row><entry>PEStoreStream 0xf47c0 16 16 8 128 40 0 0 0</entry></row><row><entry>//{ layer_id: 2, inst_id: 24, name: Direct_Conv, type: F_DIRECT_CONV, instrType:</entry></row><row><entry>UNKNOWN,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;tilemask: 0xff, localStartAddr: 0x0, localOutputAddr: 0xc4bc0, srcAddr: 0xe93c0,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>biasAddr: 0x32000</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;, inH: 20, inW: 20, inC: 128, outC: 64, kH: 5, kW: 5, kC: 128, strideH: 1, strideW:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>1, dstStride: 64, rscale: 1347632000, rshift: 42, dscale: 1, qscale: 1, doRelu: 0,</entry></row><row><entry>doTanhSigmoid: 0, tileOffset: 4294967295 }</entry></row><row><entry>//7</entry></row><row><entry>PodTaskBcst 10 0 0xff 1 0 0 0 0 0 0</entry></row><row><entry>Loop 9 2 2 32 6 2 25600 9 2 8 8</entry></row><row><entry>TileLoadQuantScaleConsts 1347632000 42 1 1</entry></row><row><entry>PDLoadBiasMM 0x32000 8 0</entry></row><row><entry>Loop 6 4 2 2560 6 2 1024 0 0 0 16</entry></row><row><entry>PDBcstBiastoCregMM 8 16 0</entry></row><row><entry>Loop 3 1 2 640 2 2 2560 0 0 0 5</entry></row><row><entry>PDLoadAregMM 0x0 128 128 3200 8 128 5</entry></row><row><entry>PDLoadBregMM 0xe93c0 128 128 128 16 128 5</entry></row><row><entry>PDDotProductMM 128 128 8 16 5</entry></row><row><entry>PDStoreCregMM 0xc4bc0 8 64 16 0 2 0 0</entry></row><row><entry>//{ layer_id: 2, inst_ id: 35, name: all2all, type: F_ALL_TO_ALL, instrType:</entry></row><row><entry>COPY_REMOTE_ABS,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;tilemask: 0xff, srcAddr: 0xc4bc0, destAddr: 0xc8fc0, length: 64</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;, numLines: 256, numBlks: 1, stTile: 0, endTile: 0, tileOffset: 1, groupOfTile: 8 }</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>//8</entry></row><row><entry>PETaskBcst 5 0xff 1 0 0 0 0 0 0</entry></row><row><entry>TileTaskParamUpdUsingTileID 4 3 2 1 0 0 0 0 0 0 8</entry></row><row><entry>PELoadStream1 0xc4bc0 16 16 4 64 256</entry></row><row><entry>PEMove 0x1f 0x1e 16 0 0 1 1 1024</entry></row><row><entry>PEStoreStream 0xc8fc0 16 16 4 64 256 2 0 0</entry></row><row><entry>PESync</entry></row><row><entry>//{ layer_id: 2, inst_id: 41, name: ALL2ALL Global Sync, type: F_GLOB_SYNC,</entry></row><row><entry>instrType: COPY_REMOTE_ABS,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="266pt" align="left"/><tbody valign="top"><row><entry>//</entry><entry>&#x2003;&#x2002;tilemask: 0xff</entry></row><row><entry>//</entry><entry>&#x2003;&#x2002;, tileOffset: 4294967295 }</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="left"/><tbody valign="top"><row><entry>//9</entry></row><row><entry>PETaskBcst 1 0xff 2 0 0 0 0 0 0</entry></row><row><entry>PESync</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0075" num="0118"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts another example of decisions/operations being performed by the compiler to convert a high-level code to a set of low-level instructions according to one aspect of the present embodiments. <figref idref="DRAWINGS">FIG. <b>3</b></figref> is similar to that of <figref idref="DRAWINGS">FIG. <b>2</b>A</figref> except that in this embodiment, the frontend compiler <b>210</b> is coupled to the output module <b>310</b> and the backend compiler <b>220</b> is coupled to the output module <b>320</b>. In some embodiments, the output module <b>310</b> may be a user guidance and error messaging module that renders one or more messages on a display or outputs into a log file for the user. Similarly, the output module <b>320</b> may be a user guidance and error messaging module that renders one or more messages on a display to the user. The display associated with the output modules <b>310</b> and <b>320</b> may or may not be the same. It is appreciated in some embodiments, the output modules <b>310</b> and <b>320</b> may be communicatively coupled to one another (not shown here). According to some embodiments, the output modules <b>310</b> and/or <b>320</b> render various informative information to the user, e.g., incorrect command line, command line options, guidance on correction of incorrect command line, etc. In some nonlimiting examples, the output modules <b>310</b> and <b>320</b> may be used to analyze the compiled deep-learning network and to identify unsupported operators and/or ill-formed networks or to identify other limitations and to provide guidance and suggestions to the user. As such, the output modules <b>310</b> and <b>320</b> may be used for debugging purposes, to provide suggestions on fixes to errors, to provide suggestions on optimization, etc.</p><p id="p-0076" num="0119"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts a flowchart <b>500</b> of an example of a process to support generating a multi-level structured metadata when the high-level code is being compiled into low-level instructions of an application for running on ML hardware according to one aspect of the present embodiments. Although the figure depicts functional steps in a particular order for purposes of illustration, the processes are not limited to any particular order or arrangement of steps. One skilled in the relevant art will appreciate that the various steps portrayed in this figure could be omitted, rearranged, combined and/or adapted in various ways.</p><p id="p-0077" num="0120">In the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the flowchart <b>500</b> starts at block <b>502</b>, where a high-level function in a high-level code of an application is received. At step <b>504</b>, resources in a hardware to execute a set of low-level instructions that is generated from the high-level function in the high-level code is identified. At step <b>506</b>, one or more processing operations to be performed that is associated with the high-level function in the high-level code is determined. The determining of the one or more processing operations occurs based on architecture of the hardware. At step <b>508</b>, the high-level function in the high-level code of the application is compiled into the set of low-level instructions to be executed on the hardware. At step <b>510</b>, a plurality of structured metadata is generated. The plurality of structured metadata includes information associated with the determining resources in the hardware and further includes information associated with the determining one or more processing operations.</p><p id="p-0078" num="0121">It is appreciated that in some embodiments a structured metadata may be selected and fed back into one of the backend compilers in order to optimize its operation. It is further appreciated that in some embodiments the one or more processing operations is one of changing precision, quantization, dimension reordering, or splitting or copying data across one or more processing tiles of the hardware. In one nonlimiting example, the method may further include reducing data movement by using the one or more processing operations. According to some embodiments, the hardware may be a dedicated hardware block including one or more microprocessors and/or OCM units storing the data and/or the set of low-level instructions compiled from the high-level function. According to some embodiments, the method further includes reducing storage by using the one or more processing operations. In one alternative example, the method may further include reducing computations by using the one or more processing operations or reducing data conversion by using the one or more processing operations. It is appreciated that as described above, the method may further include comparing data stored in a memory location as identified by a structured metadata of the plurality of structured metadata to expected data for verification of the high-level function. In some nonlimiting examples, the method may further include comparing data stored in a memory location as identified by a structured metadata of the plurality of structured metadata to expected data and debugging of the high-level function based on the comparison. According to some embodiments, the method may further include determining resources in the hardware and mapping operations and data to one or more tiles of the hardware to execute the set of low-level instructions. It is appreciated that the method may further include optimizing the high-level function of the high-level code based on the plurality of structured metadata.</p><p id="p-0079" num="0122">The foregoing description of various embodiments of the claimed subject matter has been provided for the purposes of illustration and description. It is not intended to be exhaustive or to limit the claimed subject matter to the precise forms disclosed. Many modifications and variations will be apparent to the practitioner skilled in the art. Embodiments were chosen and described in order to best describe the principles of the invention and its practical application, thereby enabling others skilled in the relevant art to understand the claimed subject matter, the various embodiments and the various modifications that are suited to the particular use contemplated.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230004365A1-20230105-M00001.NB"><img id="EMI-M00001" he="15.49mm" wi="76.20mm" file="US20230004365A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230004365A1-20230105-M00002.NB"><img id="EMI-M00002" he="15.49mm" wi="76.20mm" file="US20230004365A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system, comprising:<claim-text>a compiler including a plurality of compiler blocks, wherein compiler blocks of the plurality of compiler blocks are compossible, and wherein the compiler is configured to<claim-text>identify one or more resources in a hardware to execute a set of low-level instructions that is generated from a high-level function in a high-level code;</claim-text><claim-text>determine one or more processing operations to be performed that is associated with the high-level function in the high-level code, and wherein the determining of the one or more processing operations occurs based on architecture of the hardware; and</claim-text><claim-text>compile the high-level function in the high-level code of the application into the set of low-level instructions to be executed on the hardware.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the compiler comprises a frontend compiler and a backend compiler, and wherein the backend compiler comprises the plurality of compiler blocks, and wherein the plurality of compiler blocks is configured to generate a plurality of structured metadata that includes information associated with the determining resources in the hardware and one or more processing operations, and wherein a structured metadata from the plurality of structured metadata is selected and fed back into one backend compiler of the plurality of backend compilers, and wherein the selected structured metadata is used by the one backend compiler to optimize its operation.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the plurality of structured metadata provides information associated with memory location storing data, and wherein the memory location storing the data is compared to expected data for verification of the high-level function.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the plurality of structured metadata provides information associated with memory location storing data, and wherein the memory location storing the data is compared to expected data for debugging of the high-level function.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a compiler block of the plurality of compiler blocks is replaceable with a different compiler block.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a compiler block of the plurality of compiler blocks is replaceable with a compiler block that includes experimental algorithms for a different mapping strategy or memory allocation.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a compiler block of the plurality of compiler blocks is replaceable with a compiler block that includes a debug version of the compiler block being replaces.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the debug version compiler block is configured to store data associated with compilation and modify internal representation and metadata that results in debug binary.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processing operations is one of changing precision, quantization, dimension reordering, or splitting or copying data across one or more processing tiles of the hardware.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processing operations reduces data movement.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processing operations reduces storage.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processing operations reduces computations.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processing operations reduces data conversion.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining resources in the hardware includes mapping of operations and data to one or more tiles of the hardware to execute the set of low-level instructions.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the hardware is a machine learning (ML) hardware, and wherein the application is an ML operation.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the hardware is a dedicated hardware block including one or more microprocessors and/or on-chip memory (OCM) units storing the data and/or the set of low-level instructions compiled from the high-level function.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the compiler is further configured to:<claim-text>receive a high-level function in a high-level code of an application;</claim-text><claim-text>divide the high-level function into core parts; and</claim-text><claim-text>check for lexical, grammar, and syntax.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. A system, comprising:<claim-text>a compiler comprising a frontend compiler and a backend compiler;</claim-text><claim-text>a first output module coupled to the frontend compiler, wherein the first output module is configured to output data renderable to a user to provide information associated with incorrect command line, guidance on correct usage, identification of unsupported operators, or ill-formed networks; and</claim-text><claim-text>a second output module coupled to the backend compiler, wherein the second output module is configured to output data renderable to a user to provide information associated with incorrect command line, guidance on correct usage, identification of unsupported operators, or ill-formed networks,</claim-text><claim-text>wherein the compiler is configured to:<claim-text>identify one or more resources in a hardware to execute a set of low-level instructions that is generated from a high-level function in a high-level code;</claim-text><claim-text>determine one or more processing operations to be performed that is associated with the high-level function in the high-level code, and wherein the determining of the one or more processing operations occurs based on architecture of the hardware; and</claim-text><claim-text>compile the high-level function in the high-level code of the application into the set of low-level instructions to be executed on the hardware.</claim-text></claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the backend compiler comprises:<claim-text>a plurality of compiler blocks, wherein compiler blocks of the plurality of compiler blocks are compossible.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the plurality of compiler blocks is configured to generate a plurality of structured metadata that includes information associated with the determining resources in the hardware and one or more processing operations, and wherein a structured metadata from the plurality of structured metadata is selected and fed back into one backend compiler of the plurality of backend compilers, and wherein the selected structured metadata is used by the one backend compiler to optimize its operation.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The system of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the plurality of structured metadata provides information associated with memory location storing data, and wherein the memory location storing the data is compared to expected data for verification of the high-level function.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The system of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the plurality of structured metadata provides information associated with memory location storing data, and wherein the memory location storing the data is compared to expected data for debugging of the high-level function.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein a compiler block of the plurality of compiler blocks is replaceable with a different compiler block.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein a compiler block of the plurality of compiler blocks is replaceable with a compiler block that includes experimental algorithms for a different mapping strategy or memory allocation.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein a compiler block of the plurality of compiler blocks is replaceable with a compiler block that includes a debug version of the compiler block being replaces.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The system of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein the debug version compiler block is configured to store data associated with compilation and modify internal representation and metadata that results in debug binary.</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the one or more processing operations is one of changing precision, quantization, dimension reordering, or splitting or copying data across one or more processing tiles of the hardware.</claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the one or more processing operations reduces data movement.</claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the one or more processing operations reduces storage.</claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the one or more processing operations reduces computations.</claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the one or more processing operations reduces data conversion.</claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein determining resources in the hardware includes mapping of operations and data to one or more tiles of the hardware to execute the set of low-level instructions.</claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the hardware is a machine learning (ML) hardware, and wherein the application is an ML operation.</claim-text></claim><claim id="CLM-00034" num="00034"><claim-text><b>34</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the hardware is a dedicated hardware block including one or more microprocessors and/or on-chip memory (OCM) units storing the data and/or the set of low-level instructions compiled from the high-level function.</claim-text></claim><claim id="CLM-00035" num="00035"><claim-text><b>35</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the compiler is further configured to:<claim-text>receive a high-level function in a high-level code of an application;</claim-text><claim-text>divide the high-level function into core parts; and</claim-text><claim-text>check for lexical, grammar, and syntax.</claim-text></claim-text></claim><claim id="CLM-00036" num="00036"><claim-text><b>36</b>. A method, comprising:<claim-text>identifying one or more resources in a hardware to execute a set of low-level instructions that is generated from a high-level function in a high-level code;</claim-text><claim-text>determining one or more processing operations to be performed that is associated with the high-level function in the high-level code, and wherein the determining of the one or more processing operations occurs based on architecture of the hardware;</claim-text><claim-text>compiling the high-level function in the high-level code of the application into the set of low-level instructions to be executed on the hardware,</claim-text><claim-text>wherein a plurality of blocks of a compiler performing the steps above are compossible.</claim-text></claim-text></claim><claim id="CLM-00037" num="00037"><claim-text><b>37</b>. A method, comprising:<claim-text>identifying one or more resources in a hardware to execute a set of low-level instructions that is generated from a high-level function in a high-level code;</claim-text><claim-text>determining one or more processing operations to be performed that is associated with the high-level function in the high-level code, and wherein the determining of the one or more processing operations occurs based on architecture of the hardware;</claim-text><claim-text>compiling the high-level function in the high-level code of the application into the set of low-level instructions to be executed on the hardware; and</claim-text><claim-text>outputting data renderable to a user to provide information associated with incorrect command line, guidance on correct usage, identification of unsupported operators, or ill-formed networks.</claim-text></claim-text></claim><claim id="CLM-00038" num="00038"><claim-text><b>38</b>. A system, comprising:<claim-text>a means for identifying one or more resources in a hardware to execute a set of low-level instructions that is generated from a high-level function in a high-level code;</claim-text><claim-text>a means for determining one or more processing operations to be performed that is associated with the high-level function in the high-level code, and wherein the determining of the one or more processing operations occurs based on architecture of the hardware;</claim-text><claim-text>a means for compiling the high-level function in the high-level code of the application into the set of low-level instructions to be executed on the hardware,</claim-text><claim-text>wherein the means performing the steps above are compossible.</claim-text></claim-text></claim><claim id="CLM-00039" num="00039"><claim-text><b>39</b>. A system, comprising:<claim-text>a means comprising a frontend compiler and a backend compiler, wherein the means is configured to:<claim-text>identify one or more resources in a hardware to execute a set of low-level instructions that is generated from a high-level function in a high-level code;</claim-text><claim-text>determine one or more processing operations to be performed that is associated with the high-level function in the high-level code, and wherein the determining of the one or more processing operations occurs based on architecture of the hardware; and</claim-text><claim-text>compile the high-level function in the high-level code of the application into the set of low-level instructions to be executed on the hardware;</claim-text></claim-text><claim-text>a means coupled to the frontend compiler for outputting data renderable to a user to provide information associated with incorrect command line, guidance on correct usage, identification of unsupported operators, or ill-formed networks; and</claim-text><claim-text>a means coupled to the backend compiler for outputting data renderable to a user to provide information associated with incorrect command line, guidance on correct usage, identification of unsupported operators, or ill-formed networks.</claim-text></claim-text></claim></claims></us-patent-application>