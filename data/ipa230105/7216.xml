<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007217A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007217</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17851780</doc-number><date>20220628</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2021-110102</doc-number><date>20210701</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>9</main-group><subgroup>31</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>217</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>9</main-group><subgroup>3155</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>217</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232121</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">IMAGE DISPLAY SYSTEM AND IMAGE DISPLAY METHOD</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SEIKO EPSON CORPORATION</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>YONENO</last-name><first-name>Kunio</first-name><address><city>Shiojiri-Shi</city><country>JP</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An image display system includes a projector projecting image light onto a projection surface, at least one camera picking up at least one image of the projection surface and thus acquiring at least one picked-up image, at least one microphone detecting a sound generated in an image pickup range of the camera, and a control device controlling a position or an orientation of an image displayed on the projection surface by the image light, based on the at least one picked-up image, when a target sound is detected based on the sound.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="132.33mm" wi="144.36mm" file="US20230007217A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="144.36mm" wi="146.39mm" file="US20230007217A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="203.12mm" wi="143.59mm" orientation="landscape" file="US20230007217A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="191.09mm" wi="147.32mm" orientation="landscape" file="US20230007217A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="218.27mm" wi="137.24mm" file="US20230007217A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="139.19mm" wi="144.44mm" file="US20230007217A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="203.28mm" wi="156.04mm" orientation="landscape" file="US20230007217A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><p id="p-0002" num="0001">The present application is based on, and claims priority from JP Application Serial Number 2021-110102, filed Jul. 1, 2021, the disclosure of which is hereby incorporated by reference herein in its entirety.</p><heading id="h-0001" level="1">BACKGROUND</heading><heading id="h-0002" level="1">1. Technical Field</heading><p id="p-0003" num="0002">The present disclosure relates to an image display system and an image display method.</p><heading id="h-0003" level="1">2. Related Art</heading><p id="p-0004" num="0003">JP-A-2011-227683 discloses a technique of operating a projector, based on a tap sound generated by a user tapping a table or the like. JP-A-2021-51760 discloses a technique of using a sound generated by a user tapping a table or the like, as an audio input for rotating a projection image projected on the table.</p><p id="p-0005" num="0004">In the techniques of JP-A-2011-227683 and JP-A-2021-51760, the timing when the table is tapped can be detected by detecting the tap sound, but it is difficult to detect the position where the tap sound is generated on the table. Therefore, it is difficult to control the position or the orientation of a display image projected on the table, solely based on the tap sound.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">An image display system according to an aspect of the present disclosure includes a projector projecting image light, an image pickup element picking up an image of a projection surface, a detector detecting a target sound generated in an image pickup range of the image pickup element, and a controller controlling a position or an orientation of a display image displayed on the projection surface by the image light. The controller performs an adjustment of the position or the orientation of the display image, based on the picked-up image of the projection surface acquired from the image pickup element, when the target sound is detected by the detector.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically shows an overview of an image display system according to a first embodiment.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a functional block diagram showing the configuration of the image display system according to the first embodiment.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an explanatory view for explaining the functions of a binarizer and a time filter.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an example of a difference image detected by a difference detector.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an explanatory view for explaining a process in which a display parameter calculator calculates a display position and an orientation of a display image, based on a difference image.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>6</b></figref> schematically, shows an overview of an image display system according to a second embodiment.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a functional block diagram showing the configuration of the image display system according to the second embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF EXEMPLARY EMBODIMENTS</heading><p id="p-0014" num="0013">An embodiment of the present disclosure will now be described with reference to the drawings.</p><p id="p-0015" num="0014">In the drawings described below, in order to make each component easier to view, the dimensions may not be to scale depending on the component.</p><heading id="h-0007" level="1">First Embodiment</heading><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically shows an overview of an image display system <b>1</b> according to a first embodiment of the present disclosure. <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a functional block diagram showing the configuration of the image display system <b>1</b> according to the first embodiment. As shown in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, the image display system <b>1</b> has a projector <b>2</b>, a microphone <b>3</b>, a camera <b>4</b>, and a control device <b>5</b>. The projector <b>2</b> is an example of a projector. The microphone <b>3</b> is an example of a detector. The camera <b>4</b> is an example of an image pickup element. The control device <b>5</b> is an example of a controller. While the control device <b>5</b> is not illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the control device <b>5</b> may be built in the casing of the projector <b>2</b> or may be provided outside the projector <b>2</b>.</p><p id="p-0017" num="0016">The projector <b>2</b> is a projection-type display device that projects image light L based on an image control signal SC<b>1</b> outputted from the control device <b>5</b>. A projection surface <b>200</b>, which is a surface where the image light L is projected, is the top surface of a flat top board of a desk or a table, for example. In an example, the projection surface <b>200</b> is a rectangular surface having two long sides and two short sides. The projector <b>2</b> is arranged directly above the center of the projection surface <b>200</b> and projects the image light L toward the projection surface <b>200</b> from. the position of arrangement.</p><p id="p-0018" num="0017">Although not illustrated, the projector <b>2</b> has an electro-optical device that emits the image light L based on the image control signal SC<b>1</b>, and a projection system that projects the image light L emitted from the electro-optical device, in an enlarged form. The electro-optical device is, for example, a self-emitting electro-optical device such as an organic EL (electroluminescence) panel. The self-emitting electro-optical device is a device that generates light by itself, using electrical energy provided from outside, without needing a light source such as a backlight. The electro-optical device may be a non-self-emitting electro-optical device such as a liquid crystal panel, which needs a light source. The projection system includes optical elements such as a lens, a mirror, and a prism.</p><p id="p-0019" num="0018">Based on the image control signal SC<b>1</b> outputted from the control device <b>5</b>, the timing of light emission and the luminance of light emission of all the pixels provided in the electro-optical device are controlled. For example, the image control signal SC<b>1</b> includes a horizontal synchronization signal, a vertical synchronization signal, and an RGB signal or the like. The RGB signal includes a signal setting the luminance of light emission of a red pixel, a signal setting the luminance of light emission of a green pixel, and a signal setting the luminance of light emission of a blue pixel. The image light L emitted from the electro-optical device includes color light emitted from the pixels emitting the light with the luminance of light emission set by the RGB signal.</p><p id="p-0020" num="0019">As the image light L, generated by the electro-optical device is projected in an enlarged form on the projection surface <b>200</b> by the projection system as described above, a display image <b>100</b> visible to a user A is displayed on the projection surface <b>200</b>. The size of a projection area <b>300</b>, which is a rectangular area where the image light L is projected, in an in-plane area of the projection surface <b>200</b>, varies depending on the optical characteristics of the projection system and the distance from the projection surface <b>200</b> to the projector <b>2</b>, or the like. In an example, the size of the projection area <b>300</b> is set in such a way that the four sides of the projection area <b>300</b> are located about several centimeters inward from the four sides of the projection surface <b>200</b>.</p><p id="p-0021" num="0020">Controlling the position and number of pixels to emit light in the electro-optical device enables the display image <b>100</b> to be displayed in a desired size at a desired position in the projection area <b>300</b>. In an example, the rectangular display image <b>100</b> approximately in A4 size or B4 size is displayed near the hand of the user A seated facing one of the two long sides of the projection surface <b>200</b>, as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. For example, when all the pixels in the electro-optical device emit light, the display image <b>100</b> is displayed over the entire projection area <b>300</b>. That is, when all the pixels in the electro-optical device emit light, the display image <b>100</b> in the same size as the projection area <b>300</b> is displayed on the projection surface <b>200</b>.</p><p id="p-0022" num="0021">The microphone <b>3</b> detects a sound generated on the projection surface <b>200</b>, where the image light L is projected. For example, when the projection surface <b>200</b> is the top surface of the top board of a desk or a table, the microphone <b>3</b> is attached in tight contact with the back surface of the top board. As the microphone <b>3</b> is attached to the back surface of the top board, when the user A taps the projection surface <b>200</b>, for example, with the second joint of the middle finger of the right hand, the microphone <b>3</b> can detect a vibration sound generated by the tap. The microphone <b>3</b> outputs a sound wave signal SA<b>1</b> representing the result of detecting the sound generated on the projection surface <b>200</b> to the control device <b>5</b> via a signal cable, not illustrated.</p><p id="p-0023" num="0022">The position where the microphone <b>3</b> is attached is not limited to the back surface of the top board. For example, the microphone <b>3</b> may be attached to a structural member at the back side of the top board or may be attached at a position that does not overlap the projection area <b>300</b> at an edge of the projection surface <b>200</b>. The number of microphones <b>3</b> may be one or plural.</p><p id="p-0024" num="0023">The camera <b>4</b> is, for example, a digital camera such as a CCD (charge-coupled device) camera and picks up an image of the projection surface <b>200</b>. In an example, the camera <b>4</b> is attached to the casing of the projector <b>2</b>. That is, the camera <b>4</b> is arranged directly above the center of the projection surface <b>200</b> and picks up an image of the projection surface <b>200</b> from the position of arrangement. The camera <b>4</b> outputs picked-up image data SB<b>1</b> representing the picked-up image by the projection surface <b>200</b> to the control device <b>5</b> via a signal cable, not illustrated. When the entirety of the projection surface <b>200</b> cannot fit within the angle of view of one camera <b>4</b>, a plurality of cameras <b>4</b> may be used to pick up an image of the projection surface <b>200</b>. In this embodiment, the image pickup range of the camera <b>4</b> is the entirety of the projection surface <b>200</b>. The image pickup range of the camera <b>4</b> may be the projection area <b>300</b>.</p><p id="p-0025" num="0024">The control device <b>5</b> controls the display state of the display image <b>100</b> displayed on the projection surface <b>200</b> by the image light L, based on the sound wave signal SA<b>1</b> outputted from the microphone <b>3</b> and the picked-up image data SB<b>1</b> outputted from the camera <b>4</b>. The display state of the display image <b>100</b> includes not only the content of the display image <b>100</b> but also the size, position, and orientation or the like of the display image <b>100</b>. In this embodiment, the size of the display image <b>100</b> is fixed to about A4 size or B4 size, as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, and the position and orientation of the display image <b>100</b> in the projection area <b>300</b> are controlled based on the sound wave signal SA<b>1</b> and the picked-up image data SB<b>1</b>. While, in this embodiment, both the position and orientation of the display image <b>100</b> are controlled based on the sound wave signal SA<b>1</b> and the picked-up image data SB<b>1</b>, this is not limiting. Only the position of the display image <b>100</b> or only the orientation of the display image <b>100</b> may be controlled based on the sound wave signal SA<b>1</b> and the picked-up image data SB<b>1</b>.</p><p id="p-0026" num="0025">Although not described in detail in this embodiment, the user A not only can view the content of the display image <b>100</b> displayed near the user A's own hand but also can hold the hand over the display image <b>100</b> to execute a predetermined operation such as a page turning operation or a writing operation and thus can turn a page of the display image <b>100</b> or write something on the display image <b>100</b>. That is, the user A can switch the display content or write something by a hand gesture. However, when the user A moves to another position along the edge or the side of the projection surface <b>200</b> after the display image <b>100</b> is displayed near the hand of the user A, it is difficult for the user A to not only view the content of the display image <b>100</b> but also execute a predetermined operation such as a page turning operation or a writing operation on the display image <b>100</b>.</p><p id="p-0027" num="0026">To cope with this, in this embodiment, the system is configured in such a way that, when the user A taps the projection surface <b>200</b> according to a predetermined rule, the position and orientation of the display image <b>100</b> displayed on the projection surface <b>200</b> are controlled to be an appropriate display position and orientation corresponding to the position of the user A. In order to implement adaptive control on the display image <b>100</b> as described above, the control device <b>5</b> in this embodiment has a function of adjusting the position and orientation of the display image <b>100</b>, based on the picked-up image of the projection surface <b>200</b> acquired from the camera <b>4</b>, when a target sound is detected by the microphone <b>3</b>.</p><p id="p-0028" num="0027">The target sound in this embodiment is a sound generated by the user A to control the operation of the projector <b>2</b> and is a sound generated by the user A within the image pickup range of the camera <b>4</b>. In an example, the target sound is a sound generated when the user A taps the projection surface <b>200</b> with, for example, the second joint of the middle finger of the right hand. The control device acquires, as the target sound, a frequency component included in a predetermined frequency band, of the frequency components included in the sound detected by the microphone <b>3</b>, that is, the sound wave signal SA<b>1</b>. In the description below, the target sound may be referred to as a tap sound. The control device <b>5</b> recognizes a time point when the volume of the tap sound exceeds a predetermined threshold, as a time point when the tap sound is detected.</p><p id="p-0029" num="0028">In this embodiment, when the tap sound is detected a predetermined number of times within a first predetermined time period TA, the control device <b>5</b> adjusts the position and orientation of the display image <b>100</b>, based on the picked-up image, that is, the picked-up image data SB<b>1</b>. More specifically, when the second tap sound is detected within the first predetermined time period TA from the detection of the first tap sound, the control device <b>5</b> adjusts the position and orientation of the display image <b>100</b>, based on the picked-up image. When the third tap sound is detected within a second predetermined time period TB from the detection of the second tap sound, the control device <b>5</b> does not adjust the position and orientation of the display image <b>100</b>.</p><p id="p-0030" num="0029">In this embodiment, the control device <b>5</b> adjusts the position and orientation of the display image <b>100</b>, based on a difference image acquired from the picked-up image. Specifically, the control device <b>5</b> calculates a centroid position of the difference image and adjusts the position of the display image <b>100</b>, based on the centroid position. The control device <b>5</b> also determines the position of the user A, based on the centroid position of the difference image, and adjusts the orientation of the display image <b>100</b>, based on the result of determining the position of the user A.</p><p id="p-0031" num="0030">The specific configuration and operation of the control device <b>5</b> having the above functions will now be described.</p><p id="p-0032" num="0031">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the control device <b>5</b> has an amplifier <b>11</b>, a frequency filter <b>12</b>, a binarizer <b>13</b>, a time filter <b>14</b>, a frame memory <b>15</b>, a difference detector <b>16</b>, a display parameter calculator <b>17</b>, a display image reproducer <b>18</b>, and a display controller <b>19</b>. The sound wave signal SA<b>1</b> outputted from the microphone <b>3</b> is inputted to the amplifier <b>11</b>. The picked-up image data SB<b>1</b> outputted from the camera <b>4</b> is inputted to the frame memory <b>15</b> and the difference detector <b>16</b>.</p><p id="p-0033" num="0032">The amplifier <b>11</b> is, for example, an amplifying circuit such as an ampler and amplifies the sound wave signal SA<b>1</b> inputted from the microphone <b>3</b>. The amplifier <b>11</b> outputs an amplified sound wave signal SA<b>2</b> acquired by amplifying the sound wave signal SA<b>1</b>, to the frequency filter <b>12</b>.</p><p id="p-0034" num="0033">The frequency filter <b>12</b> is a filter that passes a frequency component included in a predetermined frequency band, of the frequency components included in the amplified sound wave signal SA<b>2</b> inputted from the amplifier <b>11</b>, as a frequency component equivalent to the tap sound (target sound). For example, when a low-pass filter is used as the frequency filter <b>12</b>, a cutoff frequency may be set in such a way as to pass a frequency component lower than 350 Hz, which is a formant frequency of male voice. As the cutoff frequency is set in this way, the frequency filter <b>12</b> can damp a relatively high frequency component such as a voice of conversation by the user A, a sound generated by metal legs of chairs hitting each other, or a sound generated by an ID card suspended from the neck of the user A hitting a button on the clothing, and can selectively pass the frequency component equivalent to the tap sound on the projection surface <b>200</b>. A signal including the frequency component passed through the frequency filter <b>12</b> is outputted as tap sound signal SA<b>3</b> to the binarizer <b>13</b>. Also, a band-pass filter that can pass the frequency component equivalent to the tap sound on the projection surface <b>200</b> can be used as the frequency filter <b>12</b>.</p><p id="p-0035" num="0034">The binarizer <b>13</b> converts the tap sound signal SA<b>3</b> inputted from the frequency filter <b>12</b> into a binary signal SA<b>4</b>, which is a signal that can have binary values of high level and low level, and outputs the binary signal SA<b>4</b> to the time filter <b>14</b> and the difference detector <b>16</b>. Specifically, the binarizer <b>13</b> compares the tap sound signal SA<b>3</b> with a predetermined threshold level Vh. When the tap sound signal SA<b>3</b> exceeds the threshold level Vh, the binarizer <b>13</b> sets the level of the binary signal SA<b>4</b> to high level for a duration tw. That is, the binarizer <b>13</b> outputs a one-shot pulse having the predetermined duration tw when the volume of the tap sound exceeds the predetermined threshold.</p><p id="p-0036" num="0035">In an example, the binarizer <b>13</b> having the functions as described above can be implemented by a comparator and a one-shot multivibrator. The comparator compares the tap sound signal SA<b>3</b> with the threshold level Vh and outputs a signal representing the result of the comparison to the one-shot multivibrator. The comparator outputs a high-level signal when the tap sound signal SA<b>3</b> exceeds the threshold level Vh. The one-shot multivibrator outputs a one-shot pulse having the duration tw decided by an externally attached circuit element such as a resistance element, when the signal inputted from the comparator changes from low level to high level. The output signal from the one-shot multivibrator is the binary signal SA<b>4</b>.</p><p id="p-0037" num="0036">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, for example, when the user A taps the projection surface <b>200</b> twice successively, a part with an increased amplitude (volume of the tap sound) is generated twice successively in the tap sound signal SA<b>3</b>, due to the occurrence of the two successive tap sounds. In <figref idref="DRAWINGS">FIG. <b>3</b></figref>, at a time point t<b>1</b>, the tap sound signal SA<b>3</b> exceeds the threshold level Vh due to the first tap sound. At a time point t<b>2</b>, the tap sound signal SA<b>3</b> exceeds the threshold level Vh due to the second tap sound. As described above, the control device <b>5</b> recognizes the time point when the volume of the tap sound exceeds the predetermined threshold, as the time point when the tap sound is detected.</p><p id="p-0038" num="0037">In the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the binarizer <b>13</b> sets the level of the binary signal SA<b>4</b> to high level for the duration tw from the time point t<b>1</b> and sets the level of the binary signal SA<b>4</b> to high level for the duration tw from the time point t<b>2</b>. In other words, the binarizer <b>13</b> outputs a first one-shot pulse P<b>1</b> having the duration tw at the time point t<b>1</b>, when the tap sound signal SA<b>3</b> exceeds the threshold level Vh due to the first tap sound, and outputs a second one-shot pulse P<b>2</b> having the duration tw at the time point t<b>2</b>, when the tap sound signal SA<b>3</b> exceeds the threshold level Vh due to the second tap sound. Because of the characteristics of the one-shot multivibrator, the output level of the one-shot multivibrator, that is, the level of the binary signal SA<b>4</b>, does not change even when the tap sound signal SA<b>3</b> exceeds the threshold level Vh successively within the duration tw.</p><p id="p-0039" num="0038">The time filter <b>14</b> determines whether the binary signal SA<b>4</b> inputted from the binarizer <b>13</b> satisfies a predetermined condition or not, and outputs a condition determination signal SA<b>5</b> representing the result of the determination to the display parameter calculator <b>17</b>. Specifically, as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the time filter <b>14</b> outputs an active-level signal, for example, a high-level signal, as the condition determination signal SA<b>5</b>, when the second one-shot pulse P<b>2</b> is generated within the first predetermined time period TA from the generation of the first one-shot pulse P<b>1</b> and a third one-shot pulse P<b>3</b> is not generated within the second predetermined time period TB from the generation of the second one-shot pulse P<b>2</b>. Meanwhile, the time filter <b>14</b> outputs an inactive-level signal, for example, a low-level signal, as the condition determination signal SA<b>5</b>, when the second one-shot pulse P<b>2</b> is generated within the first predetermined time period TA from the generation of the first one-shot pulse P<b>1</b> and the third one-shot pulse P<b>3</b> is generated within the second predetermined time period TB from the generation of the second one-shot pulse P<b>2</b>.</p><p id="p-0040" num="0039">The time filter <b>14</b> measures a first time period T<b>1</b> from the rising edge of the first one-shot pulse P<b>1</b> to the rising edge of the second one-shot pulse P<b>2</b>. In the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the time filter <b>14</b> measures the time period from the time point t<b>1</b> to the time point t<b>2</b> as the first time period T<b>1</b>. The time filter <b>14</b> starts measuring a second time period T<b>2</b>, triggered by the rising edge of the second one-shot pulse P<b>2</b>, and monitors whether or not a rising edge, that is, the third one-shot pulse P<b>3</b>, is generated in the binary signal SA<b>4</b> before the second time period T<b>2</b> reaches the second predetermined time period TB.</p><p id="p-0041" num="0040">The time filter <b>14</b> outputs an active-level signal as the condition determination signal SA<b>5</b>, when the first time period T<b>1</b> from the rising edge of the first one-shot pulse P<b>1</b> to the rising edge of the second one-shot pulse P<b>2</b> is shorter than the first predetermined time period TA and the third one-shot pulse P<b>3</b> is not generated in the binary signal SA<b>4</b> before the second time period T<b>2</b> reaches the second predetermined time period TB. Meanwhile, the time filter <b>14</b> outputs an inactive-level signal as the condition determination signal SA<b>5</b>, when the first time period T<b>1</b> from the rising edge of the first one-shot pulse P<b>1</b> to the rising edge of the second one-shot pulse P<b>2</b> is shorter than the first predetermined time period TA and the third one-shot pulse P<b>3</b> is generated in the binary signal SA<b>4</b> before the second time period T<b>2</b> reaches the second predetermined time period TB.</p><p id="p-0042" num="0041">As described above, the time filter <b>14</b> outputs an active-level signal as the condition determination signal SA<b>5</b>, when the second tap sound is detected within the first predetermined time period TA from the detection of the first tap sound and the third tap sound is not detected within the second predetermined time period TB from the detection of the second tap sound. Otherwise, the time filter <b>14</b> outputs an inactive-level signal as the condition determination signal SA<b>5</b>.</p><p id="p-0043" num="0042">In an example, it is assumed that the first predetermined time period TA is set to 1.2 seconds, that the second predetermined time period TB is set to 1 second, and that the duration tw is set to 0.2 seconds. In this case, when the user A taps the projection surface <b>200</b> to generate the first tap sound and then generates the second tap sound within a period of 0.2seconds or longer and shorter than 1.2 seconds and 1 second passes without the user A generating the third tap sound, an active-level signal is outputted as the condition determination signal SA<b>5</b> from the time filter <b>14</b>. In such a configuration, an active-level signal is not outputted from the time filter <b>14</b> when the tap sound is generated in a pattern deviating from a predetermined rule.</p><p id="p-0044" num="0043">That is, in this embodiment, the predetermined rule for the user A is &#x201c;to generate the second tap sound within the first predetermined time period TA from the generation of the first tap sound and not to generate the third tap sound within the second predetermined time period TB from the generation of tie second tap sound&#x201d;.</p><p id="p-0045" num="0044">The frequency filter <b>12</b>, the binarizer <b>13</b>, and the time filter <b>14</b> may be formed by hardware including an analog circuit and a digital circuit or may be formed by software operating on a processor such as a CPU. In an example, the frequency filter <b>12</b> and the binarizer <b>13</b> may be formed by hardware based on an analog circuit and the time filter <b>14</b> may be formed by software.</p><p id="p-0046" num="0045">The picked-up image data SB<b>1</b> outputted from the camera <b>4</b> is inputted directly to the difference detector <b>16</b> and is also inputted to the difference detector <b>16</b> via the frame memory <b>15</b>. In the description below, the picked-up image data SB<b>1</b> inputted directly to the difference detector <b>16</b> from the camera <b>4</b> is referred to as first picked-up image data SB<b>1</b>, and the picked-up image data SB<b>1</b> inputted to the difference detector <b>16</b> via the frame memory <b>15</b> is referred to as second picked-up image data SB<b>2</b>. Also, the binary signal SA<b>4</b> outputted from the binarizer <b>13</b> is inputted to the difference detector <b>16</b>.</p><p id="p-0047" num="0046">The difference detector <b>16</b> detects a difference image based on the first picked-up image data SB<b>1</b> and the second picked-up image data SB<b>2</b> during the time period between the first one-shot pulse P<b>1</b> and the second one-shot pulse P<b>2</b> appearing in the binary signal SA<b>4</b>. <figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an example of a difference image <b>400</b> detected by the difference detector <b>16</b>. When the user A taps a desk twice with the right hand, the difference image <b>400</b> appears around the hand due to the movement of the hand. The difference detector <b>16</b> outputs difference image data SB<b>3</b> representing the detected difference image <b>400</b> to the display parameter calculator <b>17</b>. When a plurality of successive frames of difference images <b>400</b> are detected during the period between the first one-shot pulse P<b>1</b> and the second one-shot pulse P<b>2</b>, for example, the difference image <b>400</b> having the maximum number of pixels where difference data is detected may be employed.</p><p id="p-0048" num="0047">The difference image data SB<b>3</b> outputted from the difference detector <b>16</b> and the condition determination signal SA<b>5</b> outputted from the time filter <b>14</b> are inputted to the display parameter calculator <b>17</b>. When the condition determination signal SA<b>5</b> changes from low level to high level, the display parameter calculator <b>17</b> calculates the display, position and orientation of the display image <b>100</b> as a display parameter, based on the difference image <b>400</b> represented by the difference image data SB<b>3</b>. In other words, when the second tap sound is detected within the first predetermined time period TA from the detection of the first tap sound and the third tap sound is not detected within the second predetermined time period TB from the detection of the second cap sound, the display parameter calculator <b>17</b> calculates the display position and orientation of the display image <b>100</b>, based on the difference image <b>400</b>.</p><p id="p-0049" num="0048">The process in which the dis lay parameter calculator <b>17</b> calculates the display position and orientation of the display image <b>100</b>, based on the difference image <b>400</b>, will now be specifically described, referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0050" num="0049">As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the display parameter calculator <b>17</b> first calculates a centroid position G from the difference image <b>400</b>, then specifies the nearest side to the centroid position G of the four sides of the projection area <b>300</b>, and draws a perpendicular line VL to the specified side from the centroid position G. In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the nearest side to the centroid position G is the lower long side.</p><p id="p-0051" num="0050">The display parameter calculator <b>17</b> decides the display, position of the display image <b>100</b> in such a way that the center the horizontal orientation of the display image <b>100</b> coincides with the perpendicular line VL and that the lower end in the vertical orientation of the display image <b>100</b> coincides with the specified side of the projection area <b>300</b>, that is, the nearest side to the centroid position G. Also, the display parameter calculator determines which position the user A is located in relation to a center line CL in the projection area <b>300</b>, based on the positional relationship between the center line CL and the centroid position G, and decides the orientation of the display image <b>100</b>, based on the result of determining the position of the user A. In the example shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the display parameter calculator decides the orientation of the display image <b>100</b> in such a way that the user A can view the display image <b>100</b> from the lower end side of the projection area <b>300</b>.</p><p id="p-0052" num="0051">The display parameter calculator <b>17</b> outputs the result of calculating the display parameter to the display controller <b>19</b>. The display parameter includes the display position and orientation of the display image <b>100</b>. In the description below, the display position of the display image <b>100</b> calculated by the display parameter calculator <b>17</b> may be referred to as a target display position and the orientation of the display image <b>100</b> calculated by the display, parameter calculator <b>17</b> may be referred to as a target display orientation.</p><p id="p-0053" num="0052">The display controller <b>19</b> generates an Image control signal SC<b>1</b> and outputs the image control signal SC<b>1</b> to the projector <b>2</b> in such a way that an image reproduced by the display image reproducer <b>18</b> is displayed as the display image <b>100</b> at the target display position in the projection area <b>300</b> and in the state of facing the target display orientation, based on the result of calculating the display parameter acquired from the display parameter calculator <b>17</b>. The display image reproducer <b>18</b> may reproduce an image, for example, based on image data stored in a recording medium such as a DVD (digital versatile disc), or may reproduce an image based on image data downloaded from a communication network such as the internet.</p><p id="p-0054" num="0053">By the functions of the control device <b>5</b> as described above, the position and orientation of the display image <b>100</b> displayed on the projection surface <b>200</b> are adjusted to be an appropriate display position and orientation corresponding to the position of the user A when the user A taps the projection surface <b>200</b> according to a predetermined rule. More specifically, by the functions of the control device <b>5</b> as described above, the position and orientation of the display image <b>100</b> displayed on the projection surface <b>200</b> are adjusted to be an appropriate display position and orientation corresponding to the position of the user A, when the second tap sound is detected within the first predetermined time period TA from the detection of the first tap sound and the third tap sound is not detected within the second predetermined time period TB from the detection of the second tap sound.</p><p id="p-0055" num="0054">When the display image <b>100</b> sticks out of the projection area <b>300</b> as a result of adjusting the position and orientation of the display image <b>100</b>, the amount of sticking out may be found in advance, based on the size of the display image <b>100</b> and the size of the projection area <b>300</b>, and the display position of the display image <b>100</b> may be corrected, based on the amount of sticking out.</p><p id="p-0056" num="0055">As described above, in this embodiment, when the tap sound (target sound) is detected by the microphone <b>3</b>, the control device <b>5</b> adjusts the position and orientation of the display image <b>100</b>, based on the picked-up image of the projection surface <b>200</b> acquired from the camera <b>4</b>.</p><p id="p-0057" num="0056">According to this embodiment as described above, the position and orientation of the display image <b>100</b> displayed on the projection surface <b>200</b> are adjusted to be an appropriate display position and orientation corresponding to the position of the user A when the user A taps the projection surface <b>200</b> according to a predetermined rule. Thus, even when the user A moves to another position along the edge or the side of the projection surface <b>200</b> after the display image <b>100</b> is displayed near the hand of the user A, the user A not only can easily continue to view the content of the display image <b>100</b> but also can easily execute a predetermined operation such as a page turning operation or a writing operation to the display image <b>100</b>.</p><p id="p-0058" num="0057">In this embodiment, the control device <b>5</b> adjusts the position and orientation of the display image <b>100</b>, based on the picked-up image, when the tap sound is detected a predetermined number of times within the first predetermined time period TA.</p><p id="p-0059" num="0058">Thus, when the tap sound is generated in a pattern deviating from the rule that the tap sound is detected the predetermined number of times within the first predetermined time period TA, that is, when the user A generates an unintended tap sound, the position and orientation of the display, image <b>100</b> are not adjusted and therefore the display image <b>100</b> can be prevented from moving at a timing that is not intended by the user A.</p><p id="p-0060" num="0059">In this embodiment, the control device <b>5</b> adjusts the position and orientation of the display image <b>100</b>, based on the picked-up image, when the second tap sound is detected within the first predetermined time period TA from the detection of the first tap sound.</p><p id="p-0061" num="0060">Thus, when the tap sound is generated in a pattern deviating from the rule that the second tap sound is detected within the first predetermined time period TA from the detection of the first tap sound, the position and orientation of the display image <b>100</b> are not adjusted and therefore the display image <b>100</b> can be prevented from moving at a timing that is not intended by the user A.</p><p id="p-0062" num="0061">In this embodiment, the control device <b>5</b> does not adjust the position and orientation of the display image <b>100</b> when the third tap sound is detected within the second predetermined time period TR from the detection of the second tap sound.</p><p id="p-0063" num="0062">Thus, when the tap sound is generated in a pattern deviating from the rule that the second tap sound is detected within the first predetermined time period TA from the detection of the first tap sound and that the third tap sound is not detected within the second predetermined time period TB from the detection of the second tap sound, the position and orientation of the display image <b>100</b> are not adjusted and therefore the display image <b>100</b> can be more securely prevented from moving at a timing that is not intended by the user A.</p><p id="p-0064" num="0063">In this embodiment, the control device <b>5</b> acquires a frequency component included in a predetermined frequency band, of the frequency components included in the sound detected by the microphone <b>3</b>, as the tap sound.</p><p id="p-0065" num="0064">Thus, even when a noise sound included in a frequency band that does not correspond to the tap sound, which is the target sound, is detected by the microphone <b>3</b>, the position and orientation of the display image <b>100</b> are not adjusted and therefore the display image <b>100</b> can be prevented from moving at a timing that is not intended by the user A.</p><p id="p-0066" num="0065">In this embodiment, the control device <b>5</b> recognizes the time point when the volume of the tap sound exceeds a predetermined threshold, as the time point when the tap sound is detected.</p><p id="p-0067" num="0066">Thus, when the volume of the tap sound does not exceed the predetermined threshold, the control device <b>5</b> does not recognize that the tap sound is detected, and the position and orientation of the display image <b>100</b> are not adjusted. Therefore, the display image <b>100</b> can be prevented from moving at a timing that is not intended by the user A.</p><p id="p-0068" num="0067">In this embodiment, the control device <b>5</b> adjusts the position and orientation of the display image <b>100</b>, based on the difference image acquired from the picked-up image.</p><p id="p-0069" num="0068">Thus, the position of the user A that is necessary for deciding the position and orientation of the display image <b>100</b> displayed on the projection surface <b>200</b> can be acquired accurately and easily.</p><p id="p-0070" num="0069">In this embodiment, the control device <b>5</b> calculates the centroid position of the difference image and adjusts the position of the display image <b>100</b>, based on the centroid position.</p><p id="p-0071" num="0070">In this way, since the centroid position of the difference image, which can be regarded as the position of the fingertip of the user A tapping the projection surface <b>200</b>, is calculated, the position of the display image <b>100</b> displayed on the projection surface <b>200</b> can be adjusted to be an appropriate display position corresponding to the position of the user A, particularly to the position of the fingertip.</p><p id="p-0072" num="0071">In this embodiment, the control device <b>5</b> determines the position of the user A, based on the centroid position of the difference image, and adjusts the orientation of the display image <b>100</b>, based on the result of determining the position of the user A.</p><p id="p-0073" num="0072">In this way, since the position of the user A is determined based on the centroid position of the difference image and the orientation of the display image <b>100</b> is adjusted based on the result of determining the position of the user A, the orientation of the display image <b>100</b> displayed on the projection surface <b>200</b> can be adjusted to be an appropriate orientation corresponding to the position of the user A, particularly to the position along the edge of the projection surface <b>200</b>.</p><heading id="h-0008" level="1">Second Embodiment</heading><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>6</b></figref> schematically shows an overview of an image display system <b>1</b>A according to a second embodiment of the present disclosure. <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a functional block diagram showing the configuration of the image display system. <b>1</b>A according to the second embodiment. In <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref>, the same components as the components of the image display system <b>1</b> according to the first embodiment, of the components of the image display system <b>1</b>A according to the second embodiment, are denoted by the same reference signs. In the description below, different components from the components of the image display system <b>1</b> according to the first embodiment, of the components of the image display system <b>1</b>A according to the second embodiment, are described and the description of the same components is omitted.</p><p id="p-0075" num="0074">As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, in the image display system <b>1</b>A according to the second embodiment, a stand <b>50</b> is provided via a frame or the like, not illustrated, above the center of the projection surface <b>200</b>, and the projector <b>2</b> is arranged at the stand <b>50</b> via a swing mechanism <b>60</b> and is thus configured to be able to project the image light L at a free position on the entirety of the projection surface <b>200</b> as a screen in the second embodiment, the camera <b>4</b> is attached to the stand <b>50</b> as an example.</p><p id="p-0076" num="0075">In this way, the projector <b>2</b> in the second embodiment has the swing mechanism <b>60</b> changing the direction of projection of the image light L. The swing mechanism <b>60</b> is an example of a projection direction changer. The control device <b>5</b> in the second embodiment controls the direction of projection of the image light via the swing mechanism <b>60</b> and thus adjusts the position of the display image <b>100</b>.</p><p id="p-0077" num="0076">As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, in the control device <b>5</b> in the second embodiment, a swing mechanism driver <b>20</b> is provided as a new component and a display direction controller <b>21</b> is provided instead of the display controller <b>19</b>. The output from the display parameter calculator <b>17</b> is coupled to the swing mechanism driver <b>20</b>, the display direction controller <b>21</b>, and the projector <b>2</b>.</p><p id="p-0078" num="0077">The swing mechanism driver <b>20</b> drives the swing mechanism <b>60</b>, based on the target display position calculated by the display parameter calculator <b>17</b>, and thus controls the position of projection of the image light L, by the projector <b>2</b>. The display direction controller <b>21</b> generates the image control signal SC<b>1</b> and outputs the image control signal SC<b>1</b> to the projector <b>2</b> in such a way that the image reproduced by the display image reproducer <b>18</b> is displayed as the display image <b>100</b> on the projection surface <b>200</b> in the state of facing the target display orientation, based on the target display orientation calculated by the display parameter calculator <b>17</b>.</p><p id="p-0079" num="0078">In the second embodiment, due to the face that the angle of incidence of the optical axis of the image light L incident on the projection surface <b>200</b> varies depending on the position on the projection surface <b>200</b>, a keystone distortion may occur in the display image <b>100</b>, depending on the display position of the display image <b>100</b>. In order to restrain the occurrence of such a keystone distortion, the projector <b>2</b> in the second embodiment performs keystone distortion correction based on the target display position calculated by the display parameter calculator <b>17</b> and a preset correction coefficient. Also, due to the face that the projection distance differs between the center and the peripheries of the projection surface <b>200</b>, the display image <b>100</b> may be blurred, depending on the position of projection. In order to restrain such blurring, the projector <b>2</b> may adjust the focus, based on the target display position calculated by the display parameter calculator <b>17</b> and a preset correction coefficient.</p><p id="p-0080" num="0079">According to the second embodiment as described above, as in the first embodiment, the position and orientation of the display image <b>100</b> displayed on the projection surface <b>200</b> are adjusted to be an appropriate display position and orientation corresponding to the position of the user A when the user A taps the projection surface <b>200</b> according to a predetermined rule. Thus, even when the user A moves to another position along the edge or the side of the projection surface <b>200</b> after the display image <b>100</b> is displayed near the hand of the user A, the user A not only can easily continue to view the content of the display image <b>100</b> but also can easily execute a predetermined operation such as a page turning operation or a writing operation to the display image <b>100</b>.</p><p id="p-0081" num="0080">The technical scope of the present disclosure is not limited to the embodiments. Various modifications can be made without departing from the spirit and scope of the present disclosure.</p><p id="p-0082" num="0081">For example, while an example where the position and orientation of the display image <b>100</b> are adjusted based on the picked-up image when the tap sound is detected twice within the first predetermined time period TA is described in the embodiments, the present disclosure is not limited to this example. The system may be configured in such a way that the position and orientation of the display image <b>100</b> are adjusted based on the picked-up image when the tap sound is detected once, or three times or more, within the first predetermined time period TA.</p><p id="p-0083" num="0082">For example, while an example where the position and orientation of the display image <b>100</b> are not adjusted when the third tap sound is detected within the second predetermined time period TB from the detection of the second tap sound is described in the embodiments, the present disclosure is not limited to this example. The system may be configured in such a way that the position and orientation of the display image <b>100</b> are adjusted when the tap sound is detected twice within the first predetermined time period TA, regardless of whether the third tap sound is detected or not.</p><p id="p-0084" num="0083">An image display system according to one aspect of the present disclosure may have the following configurations.</p><p id="p-0085" num="0084">According to one aspect of the present disclosure, an image display system includes: a projector projecting image light; an image pickup element picking up an image of a projection surface; a detector detecting a target sound generated in an image pickup range of the image pickup element; and a controller controlling position or a orientation of a display image displayed on the projection surface by the image light. The controller performs an adjustment of the position or the orientation of the display image, based on the picked-up image of the projection surface acquired from the image pickup element, when the target sound is detected by the detector.</p><p id="p-0086" num="0085">In the image display system according to the one aspect of the present disclosure, the controller may perform the adjustment based on the picked-up image when the target sound is detected a predetermined number of times within a first predetermined time period.</p><p id="p-0087" num="0086">In the image display system according to the one aspect of the present disclosure, the controller may perform the adjustment based on the picked-up image when the target sound of a second time is detected within the first predetermined time period from the detection of the target sound of a first time.</p><p id="p-0088" num="0087">In the image display system according to the one aspect of the present disclosure, the controller may not perform the adjustment when the target sound of a third time is detected within a second predetermined time period from the detection of the target sound of the second time.</p><p id="p-0089" num="0088">In the image display system according to the one aspect of the present disclosure, the controller may acquire a frequency component included in a predetermined frequency band, of frequency components included in the sound detected by the detector, as the target sound.</p><p id="p-0090" num="0089">In the image display system according to the one aspect of the present disclosure, the controller may recognize a time point when a volume of the target sound exceeds a predetermined threshold, as a time point when the target sound is detected.</p><p id="p-0091" num="0090">In the image display system according to the one aspect of the present disclosure, the controller may perform the adjustment based on a difference image acquired from the picked-up image.</p><p id="p-0092" num="0091">In the image display system according to the one aspect of the present disclosure, the controller may calculate a centroid position of the difference image and perform the adjustment based on the centroid position.</p><p id="p-0093" num="0092">In the image display system according to the one aspect of the present disclosure, the controller may determine a position of a user based on a centroid position of the difference image and perform the adjustment based on a result of determining the position of the user.</p><p id="p-0094" num="0093">In the image display system according to the one aspect of the present disclosure, the projector may have a projection direction changer changing a direction of projection of the image light, and the controller may control the direction of projection of the image light via the projection direction changer and thus perform the adjustment.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An image display system comprising:<claim-text>a projector which projects image light onto projection surface;</claim-text><claim-text>at least one camera which acquires at least one picked-up image by picking up at least one image of the projection surface;</claim-text><claim-text>at least one microphone which detects a sound generated in an image pickup range of the camera; and</claim-text><claim-text>a control device which controls a position or an orientation of an image displayed on the projection surface by the image light, based on the at least one picked-up image, when a target sound is detected based on the sound.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The image display system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the control device controls the position or the orientation of the image when the target sound is detected a number of times within a first time period.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The image display system according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the control device controls the position or the orientation of the image when the target sound of a second time is detected within the first time period from the detection of the target sound of a first time.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The image display system according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the control device does not control the position or the orientation of the image when the target sound of a third time is detected within a second time period from the detection of the target sound of the second time.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The image display system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the control device acquires a frequency component included in a frequency band, of frequency components included in the sound, as the target sound.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The image display system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the at least one picked-up image includes a first picked-up image and a second picked-up image, and</claim-text><claim-text>the control device controls the position or the orientation of the image, based on a difference image representing a difference between the first picked-up image and the second picked-up image.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The image display system according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein<claim-text>the control device controls the position or the orientation of the image, based on a centroid position of the difference image.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The image display system according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein<claim-text>the control device controls the position or the orientation of the image, based on a position of a user determined based on a centroid position of the difference image.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The image display system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the projector has a swing mechanism which changes a direction of projection of the image light, and</claim-text><claim-text>the control device controls the position or the orientation of the image by controlling the direction of projection of the image light using the swing mechanism.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. An image display method comprising:<claim-text>acquiring a picked-up image by picking up an image of a projection surface where image light is projected;</claim-text><claim-text>detecting a sound generated in an image pickup range of the picked-up image; and</claim-text><claim-text>controlling a position or an orientation of an image displayed on the projection surface by the image light, based on the picked-up image, when a target sound is detected based on the sound.</claim-text></claim-text></claim></claims></us-patent-application>