<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007240A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007240</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17940690</doc-number><date>20220908</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>105</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>182</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>172</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>105</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>0454</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>182</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>172</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">TECHNIQUES FOR DEEP NEURAL NETWORK BASED INTER-FRAME PREDICTION IN VIDEO CODING</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17476928</doc-number><date>20210916</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11490078</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17940690</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>63131625</doc-number><date>20201229</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>TENCENT AMERICA LLC</orgname><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>LI</last-name><first-name>Zeqiang</first-name><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Choi</last-name><first-name>Byeongdoo</first-name><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Wang</last-name><first-name>Wei</first-name><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Jiang</last-name><first-name>Wei</first-name><address><city>Sunnyvale</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Xu</last-name><first-name>Xiaozhong</first-name><address><city>State College</city><state>PA</state><country>US</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Liu</last-name><first-name>Shan</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>TENCENT AMERICA LLC</orgname><role>02</role><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Video coding using neural network based inter-frame prediction is performed by generating a current reference frame by generating intermediate flows based on two input frames, performing backward warping of the input frames to generate reconstruction frames, and generating a fusion map and a residual map based on the input frames, the intermediate flows and the reconstruction frames. The video coding method further includes outputting an enhanced frame or a virtual reference picture by generating a feature map with different levels, based on the current reference frame, a first reference frame and a second reference frame, generating a predicted frame based on aligned features from the generated feature map by refining the current reference frame, the first reference frame, and the second reference frame, generating a final residual based on the predicted frame, and computing the enhanced frame as an output by adding the final residual to the current reference frame.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="129.37mm" wi="158.75mm" file="US20230007240A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="186.18mm" wi="152.15mm" orientation="landscape" file="US20230007240A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="223.10mm" wi="159.43mm" orientation="landscape" file="US20230007240A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="169.25mm" wi="172.55mm" orientation="landscape" file="US20230007240A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="209.13mm" wi="126.49mm" orientation="landscape" file="US20230007240A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="187.96mm" wi="139.53mm" orientation="landscape" file="US20230007240A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="211.24mm" wi="132.76mm" orientation="landscape" file="US20230007240A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="226.06mm" wi="174.41mm" orientation="landscape" file="US20230007240A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="215.48mm" wi="131.49mm" orientation="landscape" file="US20230007240A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="223.94mm" wi="156.80mm" orientation="landscape" file="US20230007240A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="192.70mm" wi="172.55mm" orientation="landscape" file="US20230007240A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="120.40mm" wi="164.85mm" orientation="landscape" file="US20230007240A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This is a Continuation of U.S. application Ser. No. 17/476,928, filed on Sep. 16, 2021, which claims the benefit of priority to U.S. Provisional Patent Application No. 63/131,625, filed on Dec. 29, 2020, the disclosures of each of which being incorporated by reference herein in their entireties.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Uncompressed digital video can consist of a series of pictures, each picture having a spatial dimension of, for example, 1920&#xd7;1080 luminance samples and associated chrominance samples. The series of pictures can have a fixed or variable picture rate (informally also known as frame rate), of, for example 60 pictures per second or 60 Hz. Uncompressed video has significant bitrate requirements. For example, 1080p60 4:2:0 video at 8 bit per sample (1920&#xd7;1080 luminance sample resolution at 60 Hz frame rate) requires close to 1.5 Gbit/s bandwidth. An hour of such video requires more than 600 GByte of storage space.</p><p id="p-0004" num="0003">Traditional video coding standards, such as the H.264/Advanced Video Coding (H.264/AVC), High-Efficiency Video Coding (HEVC) and Versatile Video Coding (VVC) share a similar (recursive) block-based hybrid prediction/transform framework where individual coding tools like the intra/inter prediction, integer transforms, and context-adaptive entropy coding, are intensively handcrafted to optimize the overall efficiency.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">According to embodiments, a method of video coding using neural network based inter-frame prediction is performed by at least one processor and includes generating intermediate flows based on input frames, generating reconstruction frames by performing backward warping of the input frames with the intermediate flows, generating a fusion map and a residual map, based on the input frames, the intermediate flows, and the reconstruction frames, generating a feature map with a plurality of levels using a first neural network, based on a current reference frame, a first reference frame and a second reference frame, generating a predicted frame based on aligned features from the generated feature map by refining the current reference frame, the first reference frame and the second reference frame, generating a final residual based on the predicted frame, and computing an enhanced frame as an output by adding the final residual to the current reference frame.</p><p id="p-0006" num="0005">According to embodiments, an apparatus for video coding using neural network based inter-frame prediction including at least one memory configured to store program code and at least one processor configured to read the program code and operate as instructed by the program code. The program code including first generating code configured to cause the at least one processor to generate intermediate flows based on input frames, second generating code configured to cause the at least one processor to perform backward warping of the input frames with the intermediate flows to generate reconstruction frames, fusion code configured to cause the at least one processor to generate a fusion map and a residual map, based on the input frames, the intermediate flows and the reconstruction frames, third generating code configured to cause the at least one processor to generate a feature map with a plurality of levels using a first neural network, based on a current reference frame, a first reference frame and a second reference frame, predicting code configured to cause the at least one processor to predict a frame based on aligned features from the generated feature map by refining the current reference frame, the first reference frame and the second reference frame, residual code configured to cause the at least one processor to generate a final residual based on the predicted frame, and fourth generating code configured to cause the at least one processor to generate an enhanced frame as an output by adding the final residual to the current reference frame.</p><p id="p-0007" num="0006">According to embodiments, a non-transitory computer-readable medium storing instructions that, when executed by at least one processor for video coding using neural network based inter-frame prediction, cause the at least one processor to generate intermediate flows based on input frames, perform backward warping of the input frames with the intermediate flows to generate reconstruction frames, generate a fusion map and a residual map based on the input frames, the intermediate flows and the reconstruction frames, generate a feature map with a plurality of levels using a first neural network, based on a current reference frame, a first reference frame and a second reference frame, predict a frame based on aligned features from the generated feature map by refining the current reference frame, the first reference frame and the second reference frame, generate a final residual based on the predicted frame, and generate an enhanced frame as an output by adding the final residual to the current reference frame.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram of an environment in which methods, apparatuses and systems described herein may be implemented, according to embodiments.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of example components of one or more devices of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic of an example of virtual reference picture generation and insertion into a reference picture list.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of a test apparatus of a virtual reference generation process, during a test stage, according to embodiments.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a detailed block diagram of an Optical Flow Estimation and Intermediate frame synthesizing module from the test apparatus of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, during a test stage, according to embodiments.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a detailed block diagram of a Detail Enhancement module from the test apparatus of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, during a test stage, according to embodiments.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a detailed block diagram of a PCD Alignment module, during a test stage, according to embodiments.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a detailed block diagram of a TSA Fusion module, during a test stage, according to embodiments.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a detailed block diagram of the TSA Fusion module, during a test stage, according to another embodiment.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart of a method of video coding using neural network based inter-frame prediction, according to embodiments.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a block diagram of an apparatus of video coding using neural network based inter-frame prediction, according to embodiments.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0019" num="0018">This disclosure describes a Deep Neural Network (DNN)-based model relating to video coding and decoding. More specifically, a (DNN)-based model that uses and generates virtual reference data from adjacent reference frames for inter-frame prediction as a Video Frame Interpolation (VFI) task and a detail enhancement module to further improve the frame's quality and reduce artifacts (such as noises, blur, blocky effects, etc.) on motion boundaries.</p><p id="p-0020" num="0019">One purpose of video coding and decoding can be to reduce redundancy in the input video signal, through compression. Compression can help reducing aforementioned bandwidth or storage space requirements, in some cases by two orders of magnitude or more. Both lossless and lossy compression, as well as a combination thereof can be employed. Lossless compression refers to techniques in which an exact copy of the original signal can be reconstructed from the compressed original signal. When using lossy compression, the reconstructed signal may not be identical to the original signal, but the distortion between original and reconstructed signal is small enough to make the reconstructed signal useful for the intended application. In the case of video, lossy compression is widely employed. The amount of distortion tolerated depends on the application; for example, users of certain consumer streaming applications may tolerate higher distortion than users of television contribution applications. The compression ratio achievable can reflect that: higher allowable/tolerable distortion can yield higher compression ratios.</p><p id="p-0021" num="0020">Basically, the Spatiotemporal pixel neighborhoods are leveraged for predictive signal construction, to obtain corresponding residuals for subsequent transform, quantization, and entropy coding. On the other hand, the nature of Deep Neural Networks (DNN) is to extract different levels of spatiotemporal stimuli by analyzing spatiotemporal information from the receptive field of neighboring pixels. The capability of exploring highly nonlinearity and nonlocal spatiotemporal correlations provide promising opportunity for largely improved compression quality.</p><p id="p-0022" num="0021">One caveat of leveraging information from multiple neighboring video frames is the complex motion caused by moving camera and dynamic scenes. Traditional block-based motion vectors cannot work well for non-translational motions. Learning based optical flow methods can provide accurate motion information at pixel-level, which is, unfortunately prone to error, especially along the boundary of moving objects. This disclosure proposes to using a DNN-based model to implicitly handle arbitrary complex motion in a data-driven fashion, without explicit motion estimation.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram of an environment <b>100</b> in which methods, apparatuses and systems described herein may be implemented, according to embodiments.</p><p id="p-0024" num="0023">As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the environment <b>100</b> may include a user device <b>110</b>, a platform <b>120</b>, and a network <b>130</b>. Devices of the environment <b>100</b> may interconnect via wired connections, wireless connections, or a combination of wired and wireless connections.</p><p id="p-0025" num="0024">The user device <b>110</b> includes one or more devices capable of receiving, generating, storing, processing, and/or providing information associated with platform <b>120</b>. For example, the user device <b>110</b> may include a computing device (e.g., a desktop computer, a laptop computer, a tablet computer, a handheld computer, a smart speaker, a server, etc.), a mobile phone (e.g., a smart phone, a radiotelephone, etc.), a wearable device (e.g., a pair of smart glasses or a smart watch), or a similar device. In some implementations, the user device <b>110</b> may receive information from and/or transmit information to the platform <b>120</b>.</p><p id="p-0026" num="0025">The platform <b>120</b> includes one or more devices as described elsewhere herein. In some implementations, the platform <b>120</b> may include a cloud server or a group of cloud servers. In some implementations, the platform <b>120</b> may be designed to be modular such that software components may be swapped in or out. As such, the platform <b>120</b> may be easily and/or quickly reconfigured for different uses.</p><p id="p-0027" num="0026">In some implementations, as shown, the platform <b>120</b> may be hosted in a cloud computing environment <b>122</b>. Notably, while implementations described herein describe the platform <b>120</b> as being hosted in the cloud computing environment <b>122</b>, in some implementations, the platform <b>120</b> may not be cloud-based (i.e., may be implemented outside of a cloud computing environment) or may be partially cloud-based.</p><p id="p-0028" num="0027">The cloud computing environment <b>122</b> includes an environment that hosts the platform <b>120</b>. The cloud computing environment <b>122</b> may provide computation, software, data access, storage, etc. services that do not require end-user (e.g., the user device <b>110</b>) knowledge of a physical location and configuration of system(s) and/or device(s) that hosts the platform <b>120</b>. As shown, the cloud computing environment <b>122</b> may include a group of computing resources <b>124</b> (referred to collectively as &#x201c;computing resources <b>124</b>&#x201d; and individually as &#x201c;computing resource <b>124</b>&#x201d;).</p><p id="p-0029" num="0028">The computing resource <b>124</b> includes one or more personal computers, workstation computers, server devices, or other types of computation and/or communication devices. In some implementations, the computing resource <b>124</b> may host the platform <b>120</b>. The cloud resources may include compute instances executing in the computing resource <b>124</b>, storage devices provided in the computing resource <b>124</b>, data transfer devices provided by the computing resource <b>124</b>, etc. In some implementations, the computing resource <b>124</b> may communicate with other computing resources <b>124</b> via wired connections, wireless connections, or a combination of wired and wireless connections.</p><p id="p-0030" num="0029">As further shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the computing resource <b>124</b> includes a group of cloud resources, such as one or more applications (&#x201c;APPs&#x201d;) <b>124</b>-<b>1</b>, one or more virtual machines (&#x201c;VMs&#x201d;) <b>124</b>-<b>2</b>, virtualized storage (&#x201c;VSs&#x201d;) <b>124</b>-<b>3</b>, one or more hypervisors (&#x201c;HYPs&#x201d;) <b>124</b>-<b>4</b>, or the like.</p><p id="p-0031" num="0030">The application <b>124</b>-<b>1</b> includes one or more software applications that may be provided to or accessed by the user device <b>110</b> and/or the platform <b>120</b>. The application <b>124</b>-<b>1</b> may eliminate a need to install and execute the software applications on the user device <b>110</b>. For example, the application <b>124</b>-<b>1</b> may include software associated with the platform <b>120</b> and/or any other software capable of being provided via the cloud computing environment <b>122</b>. In some implementations, one application <b>124</b>-<b>1</b> may send/receive information to/from one or more other applications <b>124</b>-<b>1</b>, via the virtual machine <b>124</b>-<b>2</b>.</p><p id="p-0032" num="0031">The virtual machine <b>124</b>-<b>2</b> includes a software implementation of a machine (e.g., a computer) that executes programs like a physical machine. The virtual machine <b>124</b>-<b>2</b> may be either a system virtual machine or a process virtual machine, depending upon use and degree of correspondence to any real machine by the virtual machine <b>124</b>-<b>2</b>. A system virtual machine may provide a complete system platform that supports execution of a complete operating system (&#x201c;OS&#x201d;). A process virtual machine may execute a single program, and may support a single process. In some implementations, the virtual machine <b>124</b>-<b>2</b> may execute on behalf of a user (e.g., the user device <b>110</b>), and may manage infrastructure of the cloud computing environment <b>122</b>, such as data management, synchronization, or long-duration data transfers.</p><p id="p-0033" num="0032">The virtualized storage <b>124</b>-<b>3</b> includes one or more storage systems and/or one or more devices that use virtualization techniques within the storage systems or devices of the computing resource <b>124</b>. In some implementations, within the context of a storage system, types of virtualizations may include block virtualization and file virtualization. Block virtualization may refer to abstraction (or separation) of logical storage from physical storage so that the storage system may be accessed without regard to physical storage or heterogeneous structure. The separation may permit administrators of the storage system flexibility in how the administrators manage storage for end users. File virtualization may eliminate dependencies between data accessed at a file level and a location where files are physically stored. This may enable optimization of storage use, server consolidation, and/or performance of non-disruptive file migrations.</p><p id="p-0034" num="0033">The hypervisor <b>124</b>-<b>4</b> may provide hardware virtualization techniques that allow multiple operating systems (e.g., &#x201c;guest operating systems&#x201d;) to execute concurrently on a host computer, such as the computing resource <b>124</b>. The hypervisor <b>124</b>-<b>4</b> may present a virtual operating platform to the guest operating systems, and may manage the execution of the guest operating systems. Multiple instances of a variety of operating systems may share virtualized hardware resources.</p><p id="p-0035" num="0034">The network <b>130</b> includes one or more wired and/or wireless networks. For example, the network <b>130</b> may include a cellular network (e.g., a fifth generation (5G) network, a long-term evolution (LTE) network, a third generation (3G) network, a code division multiple access (CDMA) network, etc.), a public land mobile network (PLMN), a local area network (LAN), a wide area network (WAN), a metropolitan area network (MAN), a telephone network (e.g., the Public Switched Telephone Network (PSTN)), a private network, an ad hoc network, an intranet, the Internet, a fiber optic-based network, or the like, and/or a combination of these or other types of networks.</p><p id="p-0036" num="0035">The number and arrangement of devices and networks shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> are provided as an example. In practice, there may be additional devices and/or networks, fewer devices and/or networks, different devices and/or networks, or differently arranged devices and/or networks than those shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Furthermore, two or more devices shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> may be implemented within a single device, or a single device shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> may be implemented as multiple, distributed devices. Additionally, or alternatively, a set of devices (e.g., one or more devices) of the environment <b>100</b> may perform one or more functions described as being performed by another set of devices of the environment <b>100</b>.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of example components of one or more devices of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0038" num="0037">A device <b>200</b> may correspond to the user device <b>110</b> and/or the platform <b>120</b>. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the device <b>200</b> may include a bus <b>210</b>, a processor <b>220</b>, a memory <b>230</b>, a storage component <b>240</b>, an input component <b>250</b>, an output component <b>260</b>, and a communication interface <b>270</b>.</p><p id="p-0039" num="0038">The bus <b>210</b> includes a component that permits communication among the components of the device <b>200</b>. The processor <b>220</b> is implemented in hardware, firmware, or a combination of hardware and software. The processor <b>220</b> is a central processing unit (CPU), a graphics processing unit (GPU), an accelerated processing unit (APU), a microprocessor, a microcontroller, a digital signal processor (DSP), a field-programmable gate array (FPGA), an application-specific integrated circuit (ASIC), or another type of processing component. In some implementations, the processor <b>220</b> includes one or more processors capable of being programmed to perform a function. The memory <b>230</b> includes a random access memory (RAM), a read only memory (ROM), and/or another type of dynamic or static storage device (e.g., a flash memory, a magnetic memory, and/or an optical memory) that stores information and/or instructions for use by the processor <b>220</b>.</p><p id="p-0040" num="0039">The storage component <b>240</b> stores information and/or software related to the operation and use of the device <b>200</b>. For example, the storage component <b>240</b> may include a hard disk (e.g., a magnetic disk, an optical disk, a magneto-optic disk, and/or a solid state disk), a compact disc (CD), a digital versatile disc (DVD), a floppy disk, a cartridge, a magnetic tape, and/or another type of non-transitory computer-readable medium, along with a corresponding drive.</p><p id="p-0041" num="0040">The input component <b>250</b> includes a component that permits the device <b>200</b> to receive information, such as via user input (e.g., a touch screen display, a keyboard, a keypad, a mouse, a button, a switch, and/or a microphone). Additionally, or alternatively, the input component <b>250</b> may include a sensor for sensing information (e.g., a global positioning system (GPS) component, an accelerometer, a gyroscope, and/or an actuator). The output component <b>260</b> includes a component that provides output information from the device <b>200</b> (e.g., a display, a speaker, and/or one or more light-emitting diodes (LEDs)).</p><p id="p-0042" num="0041">The communication interface <b>270</b> includes a transceiver-like component (e.g., a transceiver and/or a separate receiver and transmitter) that enables the device <b>200</b> to communicate with other devices, such as via a wired connection, a wireless connection, or a combination of wired and wireless connections. The communication interface <b>270</b> may permit the device <b>200</b> to receive information from another device and/or provide information to another device. For example, the communication interface <b>270</b> may include an Ethernet interface, an optical interface, a coaxial interface, an infrared interface, a radio frequency (RF) interface, a universal serial bus (USB) interface, a Wi-Fi interface, a cellular network interface, or the like.</p><p id="p-0043" num="0042">The device <b>200</b> may perform one or more processes described herein. The device <b>200</b> may perform these processes in response to the processor <b>220</b> executing software instructions stored by a non-transitory computer-readable medium, such as the memory <b>230</b> and/or the storage component <b>240</b>. A computer-readable medium is defined herein as a non-transitory memory device. A memory device includes memory space within a single physical storage device or memory space spread across multiple physical storage devices.</p><p id="p-0044" num="0043">Software instructions may be read into the memory <b>230</b> and/or the storage component <b>240</b> from another computer-readable medium or from another device via the communication interface <b>270</b>. When executed, software instructions stored in the memory <b>230</b> and/or the storage component <b>240</b> may cause the processor <b>220</b> to perform one or more processes described herein. Additionally, or alternatively, hardwired circuitry may be used in place of or in combination with software instructions to perform one or more processes described herein. Thus, implementations described herein are not limited to any specific combination of hardware circuitry and software.</p><p id="p-0045" num="0044">The number and arrangement of components shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> are provided as an example. In practice, the device <b>200</b> may include additional components, fewer components, different components, or differently arranged components than those shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Additionally, or alternatively, a set of components (e.g., one or more components) of the device <b>200</b> may perform one or more functions described as being performed by another set of components of the device <b>200</b>.</p><p id="p-0046" num="0045">A typical video compression framework can be described as follows. Given an input video x including a plurality of image frames x<sub>1</sub>, . . . , x<sub>T</sub>, in the first motion estimation step, the frames are partitioned into spatial blocks. Each block can be partitioned into smaller blocks iteratively and a set of motion vectors m<sub>t </sub>between a current frame x<sub>t </sub>and a set of previous reconstructed frames {{circumflex over (x)}<sub>j</sub>}<sub>t&#x2212;1 </sub>is computed for each block. Note that the subscript t denotes the current t-th encoding cycle, which may not match the time stamp of the image frame. Also, the set of previous reconstructed frames {{circumflex over (x)}<sub>j</sub>}<sub>t&#x2212;1 </sub>contain frames from multiple previous encoding cycles. Then, in the second motion compensation step, a predicted frame x<sub>t </sub>is obtained by copying the corresponding pixels of the set of previous reconstructed frames {{circumflex over (x)}<sub>j</sub>}<sub>t&#x2212;1 </sub>based on the motion vectors m<sub>t</sub>, and a residual r<sub>t </sub>between the original frame x<sub>t </sub>and the predicted frame {tilde over (x)}<sub>t </sub>can be obtained (i.e. r<sub>t</sub>=x<sub>t</sub>&#x2212;{tilde over (x)}<sub>t</sub>). In the third motion compensation step, the residual r<sub>t </sub>is quantized. The quantization step gives a quantized &#x177;<sub>t</sub>. Both the motion vectors m<sub>t </sub>and the quantized &#x177;<sub>t </sub>are encoded into bit steams by entropy coding, which are sent to decoders. Then, on the decoder side, the quantized &#x177;<sub>t </sub>is dequantized (typically through inverse transformation like IDCT with the dequantized coefficients) to obtain a recovered residual {circumflex over (r)}<sub>t</sub>. Then the recovered residual &#x177;<sub>t </sub>is added back to the predicted frame {tilde over (x)}<sub>t </sub>to obtain a reconstructed frame {circumflex over (x)}<sub>t </sub>(i.e. {circumflex over (x)}<sub>t</sub>={tilde over (x)}<sub>t</sub>+{circumflex over (r)}<sub>t</sub>). Additional components are further used to improve the visual quality of the reconstructed frame {circumflex over (x)}<sub>t</sub>. Typically, one or multiple of the following enhancement modules can be selected to process the reconstructed frame {circumflex over (x)}<sub>t</sub>, including Deblocking Filter (DF), Sample-Adaptive Offset (SAO), Adaptive Loop Filter (ALF), etc.</p><p id="p-0047" num="0046">In HEVC, VVC or other video coding frameworks or standards, the decoded pictures may be included in the reference picture list (RPL) and may be used for motion-compensated prediction as a reference picture and other parameter prediction for coding the following picture(s) in the encoding or decoding order or may be used for intra-prediction or intra block copy for coding different regions or blocks of the current picture.</p><p id="p-0048" num="0047">In embodiments, one or more virtual references may be generated and included in the RPL in both the encoder and decoder, or only in the decoder. The virtual reference picture may be generated by one or more processes including signal-processing, spatial or temporal filtering, scaling, weighted averaging, up-/down-sampling, pooling, recursive processing with memory, linear system processing, non-linear system processing, neural-network processing, deep-learning based processing, AI-processing, pre-trained network processing, machine-learning based processing, on-line training network processing or their combinations. For the processing to generate the virtual reference(s), zero or more forward reference pictures, which precede the current picture in both output/display order and encoding/decoding order, and zero or more backward reference pictures, which follow the current picture both in output/display order but precede the current picture in encoding/decoding order are used as input data. The output of the processing is the virtual/generated picture to be used as a new reference picture.</p><p id="p-0049" num="0048">A DNN pre-trained network processing for virtual reference picture generation is described according to embodiments. <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example of virtual reference picture generation and insertion into a reference picture list including a hierarchical GOP structure <b>300</b>, a reference picture list <b>310</b>, and a virtual reference generation process <b>320</b>.</p><p id="p-0050" num="0049">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, given the hierarchical GOP structure <b>300</b>, when the current picture has a picture order count (POC) equal to 3, usually, the decoded picture with POC equal to 0, 2, 4 or 8 may be stored in a decoded picture buffer and some of them are included in the reference picture list for decoding the current picture. As an example, the nearest decoded pictures with POC equal to 2 or 4 may be fed into the virtual reference generation process <b>320</b> as input data. The virtual reference picture may be generated through one or multiple processes. The generated virtual reference picture may be stored in the decoded picture buffer and included into the reference picture list <b>310</b> of the current picture or one or more future pictures in decoding order. If the virtual reference picture is included into the reference picture list <b>310</b> of the current picture, the generated pixel data of the virtual reference picture may be used for motion compensated prediction as reference data, when it is indicated by a reference index that the virtual reference picture is used.</p><p id="p-0051" num="0050">In the same or another embodiment, the entire virtual reference generation process may include one of more signaling processing modules with one or more pre-trained neural network model or any pre-defined parameters. For example, the entire virtual reference generation process <b>320</b> may be composed of a flow estimation module <b>330</b>, a flow compensation module <b>340</b>, and a detail enhancement module <b>350</b>, as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The flow estimation module <b>330</b> may be an optical feature flow estimation process with DNN modeling. The flow compensation module <b>340</b> may be an optical flow compensation and coarse intermediate frame synthesizing process with DNN modeling. The detail enhancement module <b>350</b> may be an enhancing process with DNN modeling.</p><p id="p-0052" num="0051">A method and an apparatus of a DNN model for Video Frame Interpolation (VFI) according to embodiments will now be described in detail.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of a test apparatus for a virtual reference generation process <b>400</b>, during a test stage, according to embodiments.</p><p id="p-0054" num="0053">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the virtual reference generation process <b>400</b> includes an Optical Flow Estimation and Intermediate Frame synthesizing module <b>410</b> and a Detail Enhancement module <b>420</b>.</p><p id="p-0055" num="0054">In a random access configuration of a VVC decoder, two reference frames are fed into the Optical Flow Estimation and Intermediate Frame synthesizing module <b>410</b> to generate flow maps and then generate a coarse intermediate frame to be fed into the detail enhancement module <b>420</b> along with the forward/backward reference frame to further improve the frame's quality. A more detailed description of the DNN modules, the Optical Flow Estimation and Intermediate Frame Synthesizing module <b>410</b> and the Detail Enhancement module <b>420</b>, used to perform the reference frame generation process will be detailed later with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref> and <figref idref="DRAWINGS">FIG. <b>6</b></figref>, respectively.</p><p id="p-0056" num="0055">Depending on the prediction structure and/or encoding configuration, two or more different models may be trained and used, selectively. In the random access configuration, which may use hierarchical prediction structure with both forward and backward prediction reference pictures for motion compensated prediction, one or more forward reference picture that precedes the current picture in output (or display) order and one or more backward reference picture that follows the current picture in output (or display) order are the inputs of the network. In the low delay configuration, which may have forward prediction reference pictures only for motion compensated prediction, two or more forward reference pictures are the inputs of the network. For each configuration, one or more different network models may be selected and used for inter-prediction.</p><p id="p-0057" num="0056">Once one or more reference picture lists (RPLs) are constructed before the network inference is processed, the presence of the appropriate reference pictures that may be used as inputs of the network inference is checked in both encoder and decoder. If a backward reference picture that has the same POC distance from the current picture as the POC distance of a forward reference picture is present, the trained models for random access configuration may be selected and used in network inference to generate the intermediate (virtual) reference picture. If an appropriate backward reference picture is not present, two forward reference pictures, one of which has a POC distance that is double the other, may be selected and used in network inference.</p><p id="p-0058" num="0057">When the network model for inter-prediction explicitly uses an encoding configuration, one or more syntax elements in a high level syntax structure, such as parameter sets or headers, may indicate which models are used for the current sequence, picture, or slice. In an embodiment, all available network models in the current coded video sequence are listed in a sequence parameter set (SPS), and the selected model for each coded picture or slice is indicated by one or more syntax elements in a picture parameter set (PPS) or picture/slice header.</p><p id="p-0059" num="0058">The network topology and parameters may be explicitly specified in the specification document of video coding standards, such as VVC, HEVC or AV1. In this case, the predefined network model may be used for the entire coded video sequence. If two or more network models are defined in the specification, one or more of them may be selected for each coded video sequence, coded picture or slice.</p><p id="p-0060" num="0059">In a case where the customized network is used for each coded video sequence, picture or slice, the network topology and parameters may be explicitly signaled in the high level syntax structure, such as a parameter set or SEI message in an elementary bitstream, or in a metadata track in the file format. In an embodiment, one or more network models with network topologies and parameters are specified in one or more SEI messages, where those SEI messages may be inserted into the coded video sequence with different activation scopes. Further, the following SEI message in the coded video bitstream may update the previously activated SEI message.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a detailed block diagram of the Optical Flow Estimation and Intermediate Frame Synthesizing module <b>410</b>, during a test stage, according to embodiments.</p><p id="p-0062" num="0061">As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the Optical Flow Estimation and Intermediate Frame Synthesizing module <b>410</b> includes a Flow Estimation module <b>510</b>, a Backward Warping module <b>520</b>, and a Fusion Process module <b>530</b>.</p><p id="p-0063" num="0062">Two reference frames {I<sub>0</sub>, I<sub>1</sub>} are used as input to the Flow Estimation module <b>510</b> which approximates the intermediate flows {F<sub>0-&#x3e;t</sub>, F<sub>1-&#x3e;t</sub>} from the perspective of the frame I<sub>t </sub>that is expected to be synthesized. The Flow Estimation module <b>510</b> adopts a coarse-to-fine strategy with progressively increased resolutions: it iteratively updates a flow field. Conceptually, according to the iteratively updated flow fields, corresponding pixels are moved from two input frames to the same location in a latent intermediate frame.</p><p id="p-0064" num="0063">With the estimated intermediate flows {F<sub>0-&#x3e;t</sub>, F<sub>1-&#x3e;t</sub>}, the Backward Warping module <b>520</b> generates coarse reconstruction frames or warped frames {I<sub>0-&#x3e;t</sub>, I<sub>1-&#x3e;t</sub>} by performing backward warping on the input frames {I<sub>0</sub>, I<sub>1</sub>}. Backward warping of the frames may be performed by inversely mapping and sampling pixels of the latent intermediate frame to the input frames {I<sub>0</sub>, I<sub>1</sub>} to generate the warped frames {I<sub>0-&#x3e;t</sub>, I<sub>1-&#x3e;t</sub>}. The Fusion Process module <b>530</b> takes the input frames {I<sub>0</sub>, I<sub>1</sub>}, the warped frames {I<sub>0-&#x3e;t</sub>, I<sub>1-&#x3e;t</sub>}, and the estimated intermediate flows {F<sub>0-&#x3e;t</sub>, F<sub>1-&#x3e;t</sub>} as inputs. The Fusion Process module <b>530</b> estimates a fusion map along with another residual map. The fusion map and residual map are estimated maps that map feature variations in the input. Then, the warped frames are linearly combined according to the fusion result and added with the residual map to do reconstruction to get an intermediate frame (referred to as Reference Frame I<sub>t </sub>in <figref idref="DRAWINGS">FIG. <b>5</b></figref>).</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a detailed block diagram of the Detail Enhancement module <b>420</b>, during a test stage, according to embodiments.</p><p id="p-0066" num="0065">As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the Detailed Enhancement module <b>420</b> includes a PCD Alignment module <b>610</b>, a TSA Fusion module <b>620</b>, and a Reconstruction module <b>630</b>.</p><p id="p-0067" num="0066">Assuming that reference frame I<sub>t </sub>is a reference that all other frames will be aligned to using and two reference frames {I<sub>t&#x2212;1</sub>, I<sub>t+1</sub>} together as input, the PCD (Pyramid, Cascading and Deformable convolution) alignment module <b>610</b> refines the reference frame's (I<sub>t&#x2212;1</sub>, I<sub>t</sub>, I<sub>t+1</sub>) features as an aligned feature F<sub>t-aligned</sub>. Using the aligned feature F<sub>t-aligned</sub>, the TSA (Temporal and Spatial Attention) Fusion module <b>620</b> provides the weight of the feature map to apply attention to emphasize important features for subsequent restoration and outputs a predicted frame I<sub>p</sub>. A more detailed description of the PCD Alignment module <b>610</b> and the TSA Fusion module <b>620</b> will be detailed later with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref> and <figref idref="DRAWINGS">FIG. <b>8</b></figref>, respectively.</p><p id="p-0068" num="0067">Then, the Reconstruction module formats a final residual of the reference frame I<sub>t </sub>based on the predicted frame I<sub>p</sub>. Finally, the Add module <b>640</b> adds the formatted final residual to the reference frame I<sub>t </sub>to get an enhanced frame I<sub>t-enhanced </sub>as a final output of the Detailed Enhancement module <b>420</b>.</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a detailed block diagram of the PCD Alignment module <b>610</b> inside the Detailed Enhancement module <b>420</b>, during a test stage, according to embodiments. Modules in <figref idref="DRAWINGS">FIG. <b>7</b></figref> having the same name and numbering convention may be the same or one of a plurality of modules performing the described functions (e.g. Deformable Convolution module <b>730</b>).</p><p id="p-0070" num="0069">As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the PCD Alignment module <b>610</b> includes a Feature Extraction module <b>710</b>, an Offset Generation module <b>720</b>, and a Deformable Convolution module <b>730</b>. The PCD Alignment module <b>610</b> computes the aligned features F<sub>t-aligned </sub>of the inter-frame.</p><p id="p-0071" num="0070">First, using each reference frame in {I<sub>0</sub>, I<sub>1 </sub>. . . I<sub>t</sub>, I<sub>t+1 </sub>. . . } as input, the Feature Extraction module <b>710</b> computes a feature map {F<sub>0</sub>, F<sub>1 </sub>. . . F<sub>t</sub>, F<sub>t+1 </sub>. . . } with three different levels by using a Feature Extraction DNN through forward inference. The different levels have different resolutions for feature compensation to capture different levels of spatial/temporal information. For example, with larger motion in the sequence, the smaller feature map will be able to have a larger respective field and be able to handle a variety of object offset.</p><p id="p-0072" num="0071">With different level feature maps, an Offset Generation module <b>720</b> computes an offset map &#x394;P by concatenating the feature maps {F<sub>t</sub>, F<sub>t+1</sub>} and then passing the concatenated feature maps through an offset generation DNN. Then, the Deformable Convolution module <b>730</b> calculates a new position of a DNN convolution kernel P<sub>new </sub>by adding the offset map &#x394;P to an original position P<sub>original </sub>(i.e. &#x394;P+P<sub>original</sub>) using a Temporal Deformable Convolution (TDC) operation. Note that the reference frame I<sub>t </sub>can be any frame within {I<sub>0</sub>, I<sub>1 </sub>. . . I<sub>t</sub>, I<sub>t+1 </sub>. . . }. Without loss of generality, frames may be ranked based on their time stamp in accenting order. In one embodiment, when the current target is to enhance a current reconstructed frame I<sub>t</sub>, the reference frame I<sub>t </sub>is used as the intermediate frame.</p><p id="p-0073" num="0072">Since the new position P<sub>new </sub>may be an irregular position and may not be an integer, the TDC operation can be conducted by using interpolations (e.g., bilinear interpolation). By applying deformable convolution kernel in the Deformable Convolution module <b>730</b>, compensation features from different levels can be generated based on the feature map of the level and the corresponding generated offsets. Deformable convolution kernel may then be applied to obtain the top-level aligned feature F<sub>t-aligned </sub>based on the offset map &#x394;P and one or more of the compensation features, by up-sampling and adding to the upper-level compensation feature.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a detailed block diagram of the TSA Fusion module <b>620</b> inside the Detailed Enhancement module <b>420</b>, during a test stage, according to embodiments.</p><p id="p-0075" num="0074">As shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the TSA Fusion module <b>620</b> includes an Activation module <b>810</b>, an Element-wise Multiplication module <b>820</b>, a Fusion Convolution module <b>830</b>, a Frame Reconstruction module <b>840</b>, and a Frame Synthesis module <b>850</b>. The TSA Fusion module <b>620</b> uses temporal and spatial attention. The goal of temporal attention is to compute frame similarity in an embedding space. Intuitively, in an embedding space, a neighboring frame that is more similar to the reference frame should be paid more attention to. Given the feature maps {F<sub>t&#x2212;1</sub>, F<sub>t+1</sub>} and aligned feature F<sub>t-aligned </sub>of the center frame as input, the Activation module <b>810</b> uses a sigmoid activation function to restrict the inputs to [0, 1] as a simple convolution filters to get the temporal attention maps {F&#x2032;<sub>t&#x2212;1</sub>, F&#x2032;<sub>t</sub>, F&#x2032;<sub>t+1</sub>} as an output of the Activation module <b>810</b>. Note that for each spatial location, the temporal attention is spatial-specific.</p><p id="p-0076" num="0075">Then, the Element-wise Multiplication module <b>820</b> multiplies the temporal attention maps {F&#x2032;<sub>t&#x2212;1</sub>, F&#x2032;<sub>t</sub>, F&#x2032;<sub>t+1</sub>} in a pixel-wise manner with the aligned feature F<sub>t-aligned</sub>. An extra fusion convolution layer is adopted in the Fusion Convolution module <b>830</b> to obtain aggregated attention-modulated features F<sub>t-aligned-TSA</sub>. Using the temporal attention maps {F&#x2032;<sub>t&#x2212;1</sub>, F&#x2032;<sub>t</sub>, F&#x2032;<sub>t+1</sub>} and the attention-modulated features F<sub>t-aligned-TSA</sub>, the Frame Reconstruction module <b>840</b> uses a Frame Reconstruction DNN through feed forward inference computation to generate aligned frames I<sub>t-aligned</sub>. Then, the aligned frames I<sub>t-aligned </sub>are passed through a Frame Synthesis module <b>850</b> to generate the synthesized predicted frame I<sub>p </sub>as a final output.</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a detailed block diagram of the TSA Fusion module <b>620</b> inside the Detailed Enhancement module <b>420</b>, during a test stage, according to another example embodiment. Modules in <figref idref="DRAWINGS">FIG. <b>9</b></figref> having the same name and numbering convention may be the same or one of a plurality of modules performing the described functions.</p><p id="p-0078" num="0077">As shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the TSA Fusion module <b>620</b> according to this embodiment includes an Activation module <b>910</b>, an Element-wise Multiplication module <b>920</b>, a Fusion Convolution module <b>930</b>, a Down-sampled Convolution (DSC) module <b>940</b>, an Up-sampling and Add module <b>950</b>, a Frame Reconstruction module <b>960</b>, and a Frame Synthesis module <b>970</b>. Similar to embodiments of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, TSA Fusion module <b>620</b> uses temporal and spatial attention. Given the feature maps {F<sub>t&#x2212;1</sub>, F<sub>t+1</sub>} and aligned feature F<sub>t-aligned </sub>of the center frame as input, the Activation module <b>910</b> uses a sigmoid activation function to restrict the inputs to [0, 1] as a simple convolution filters to get the temporal attention maps {M<sub>t&#x2212;1</sub>, M<sub>t</sub>, M<sub>t+1</sub>} as an output of the Activation module <b>910</b>.</p><p id="p-0079" num="0078">Then, the Element-wise Multiplication module <b>920</b> multiplies the temporal attention maps {M<sub>t&#x2212;1</sub>, M<sub>t</sub>, M<sub>t+1</sub>} in a pixel-wise manner with the aligned feature F<sub>t-aligned</sub>. Fusion convolution is performed on the product, in the Fusion Convolution module <b>930</b>, to generate fused features F<sub>fused</sub>. The fused features F<sub>fused </sub>are passed down sampled and convolved by the DSC module <b>940</b>. Another convolution layer may be adopted and processed by the DSC module <b>940</b>, as shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. The output from each layer of the DSC module <b>940</b> is input to the Up-sampling and Add module <b>950</b>. An extra layer of the up sampling and adding is be applied along with the fused features F<sub>fused </sub>as input to the extra layer, as shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. The Up-sampling and Add modules <b>950</b> generate fused attention maps M<sub>t-fused</sub>. The Element-wise Multiplication module <b>920</b> multiplies the fused attention maps M<sub>t-fused </sub>in a pixel-wise manner with the fused features F<sub>fused </sub>to generate a TSA aligned feature F<sub>t-TSA</sub>. Using the temporal attention maps {M<sub>t&#x2212;1</sub>, M<sub>t</sub>, M<sub>t+1</sub>} and the TSA aligned feature F<sub>t-TSA</sub>, the Frame Reconstruction module <b>960</b> uses a Frame Reconstruction DNN through feed forward inference computation to generate TSA aligned frames I<sub>t-TSA</sub>. Then, the aligned frames I<sub>t-TSA </sub>are passed through a Frame Synthesis module <b>970</b> to generate the synthesized predicted frame I<sub>p </sub>as a final output.</p><p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart of a method <b>1000</b> of video coding using neural network based inter-frame prediction, according to embodiments.</p><p id="p-0081" num="0080">In some implementations, one or more process blocks of <figref idref="DRAWINGS">FIG. <b>10</b></figref> may be performed by the platform <b>120</b>. In some implementations, one or more process blocks of <figref idref="DRAWINGS">FIG. <b>10</b></figref> may be performed by another device or a group of devices separate from or including the platform <b>120</b>, such as the user device <b>110</b>.</p><p id="p-0082" num="0081">As shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, in operation <b>1001</b>, the method <b>1000</b> includes generating intermediate flows based on input frames. The intermediate flows may further be iteratively updated and corresponding pixels moved from two input frames to the same location in a latent intermediate frame.</p><p id="p-0083" num="0082">In operation <b>1002</b>, the method <b>1000</b> includes generating reconstruction frames by performing backward warping of the input frames with the intermediate flows.</p><p id="p-0084" num="0083">In operation <b>1003</b>, the method <b>1000</b> includes generating a fusion map and a residual map, based on the input frames, the intermediate flows, and the reconstruction frames.</p><p id="p-0085" num="0084">In operation <b>1004</b>, the method <b>1000</b> includes generating a feature map with a plurality of levels using a first neural network, based on a current reference frame, a first reference frame, and a second reference frame. The current reference frame may be generated by linearly combining the reconstruction frames according to the fusion map and adding the combined reconstruction frames with the residual map. Further, the first reference frame may be a reference frame that precedes the current reference frame in an output order and the second reference frame may be a reference frame that follows the current reference frame in the output order.</p><p id="p-0086" num="0085">In operation <b>1005</b>, the method <b>1000</b> includes generating a predicted frame based on aligned features from the generated feature map by refining the current reference frame, the first reference frame, and the second reference frame. Specifically, the predicted frame may be generated by first performing convolution to obtain attention maps, generating attention features, based on the attention maps and the aligned features, generating an aligned frame, based on the attention maps and the attention features, and then synthesizing the aligned frame to obtain the predicted frame.</p><p id="p-0087" num="0086">The aligned features may be generated by computing an offset for the plurality of levels of the feature map generated in operation <b>1004</b> and performing deformable convolution to generate compensation features for the plurality of levels. The aligned features may then be generated based on the offset and at least one of the generated compensation features.</p><p id="p-0088" num="0087">In operation <b>1006</b>, the method <b>1000</b> includes generating a final residual based on the predicted frame. A weight of the feature map, generated in operation <b>1004</b>, may also be generated, emphasizing important features for generation of subsequent final residuals.</p><p id="p-0089" num="0088">In operation <b>1007</b>, the method <b>1000</b> includes computing an enhanced frame as an output by adding the final residual to the current reference frame.</p><p id="p-0090" num="0089">Although <figref idref="DRAWINGS">FIG. <b>10</b></figref> shows example blocks of the method, in some implementations, the method may include additional blocks, fewer blocks, different blocks, or differently arranged blocks than those depicted in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. Additionally, or alternatively, two or more of the blocks of the method may performed in parallel.</p><p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a block diagram of an apparatus <b>1100</b> of video coding using neural network based inter-frame prediction, according to embodiments.</p><p id="p-0092" num="0091">As shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the apparatus includes first generating code <b>1101</b>, second generating code <b>1102</b>, fusion code configured <b>1103</b>, third generating code <b>1104</b>, predicting code <b>1105</b>, residual code <b>1106</b>, and fourth generating code <b>1107</b>.</p><p id="p-0093" num="0092">The first generating code <b>1101</b> is configured to cause the at least one processor to generate intermediate flows based on input frames.</p><p id="p-0094" num="0093">The second generating code <b>1102</b> is configured to cause the at least one processor to perform backward warping of the input frames with the intermediate flows to generate reconstruction frames.</p><p id="p-0095" num="0094">The fusion code configured <b>1103</b> is configured to cause the at least one processor to generate a fusion map and a residual map, based on the input frames, the intermediate flows, and the reconstruction frames.</p><p id="p-0096" num="0095">The third generating code <b>1104</b> is configured to cause the at least one processor to generate a feature map with a plurality of levels using a first neural network, based on a current reference frame, a first reference frame, and a second reference frame.</p><p id="p-0097" num="0096">The predicting code <b>1105</b> is configured to cause the at least one processor to predict a frame based on aligned features from the generated feature map by refining the current reference frame, the first reference frame, and the second reference frame.</p><p id="p-0098" num="0097">The residual code <b>1106</b> is configured to cause the at least one processor to generate a final residual based on the predicted frame.</p><p id="p-0099" num="0098">The fourth generating code <b>1107</b> is configured to cause the at least one processor to generate an enhanced frame as an output by adding the final residual to the current reference frame.</p><p id="p-0100" num="0099">Although <figref idref="DRAWINGS">FIG. <b>11</b></figref> shows example blocks of the apparatus, in some implementations, the apparatus may include additional blocks, fewer blocks, different blocks, or differently arranged blocks than those depicted in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. Additionally, or alternatively, two or more of the blocks of the apparatus may be combined.</p><p id="p-0101" num="0100">The apparatus may further include updating code configured to cause the at least one processor to iteratively update the intermediate flows and move corresponding pixels from two input frames to the same location in a latent intermediate frame, reference frame code configured to cause the at least one processor to generate the current reference frame by linearly combining the reconstruction frames according to the fusion map and adding the combined reconstruction frames with the residual map, determining code configured to cause the at least one processor to determine a weight of the feature map, wherein the weight emphasizes important features for generating subsequent final residuals, computing code configured to cause the at least one processor to compute an offset for the plurality of levels, compensation feature generating code configured to cause the at least one processor to perform deformable convolution to generate compensation features for the plurality of levels, aligned feature generating code configured to cause the at least one processor to generate the aligned features based on the offset and at least one of the generated compensation features, performing code configured to cause the at least one processor to perform convolution to obtain attention maps, attention feature generating code configured to cause the at least one processor to generate attention features based on the attention maps and the aligned features, aligned frame generating code configured to cause the at least one processor to generate an aligned frame, based on the attention maps and the attention features, using a second neural network, and synthesizing code configured to cause the at least one processor to synthesize the aligned frame to obtain the predicted frame.</p><p id="p-0102" num="0101">Comparing with traditional inter-frame generation approaches, the proposed method performs DNN-based network in video coding. Instead of computing an explicit motion vector or motion flow that either cannot handle complex motion or is prone to error, the proposed methods directly take data from a Reference Picture List (RPL) to generate a virtual reference frame. And then the enhanced Deformable Convolution (DCN) is applied to capture the offset of pixels and implicitly compensate large complex motion for further detailed enhancement. Finally, a high-quality enhanced frame is reconstructed by a DNN model.</p><p id="p-0103" num="0102">The proposed methods may be used separately or combined in any order. Further, each of the methods (or embodiments) may be implemented by processing circuitry (e.g., one or more processors or one or more integrated circuits). In one example, the one or more processors execute a program that is stored in a non-transitory computer-readable medium.</p><p id="p-0104" num="0103">The present disclosure provides illustration and description, but is not intended to be exhaustive or to limit the implementations to the precise form disclosed. Modifications and variations are possible in light of the present disclosure or may be acquired from practice of the implementations.</p><p id="p-0105" num="0104">As used herein, the term component is intended to be broadly construed as hardware, firmware, or a combination of hardware and software.</p><p id="p-0106" num="0105">It will be apparent that systems and/or methods, described herein, may be implemented in different forms of hardware, firmware, or a combination of hardware and software. The actual specialized control hardware or software code used to implement these systems and/or methods is not limiting of the implementations. Thus, the operation and behavior of the systems and/or methods were described herein without reference to specific software code&#x2014;it being understood that software and hardware may be designed to implement the systems and/or methods based on the description herein.</p><p id="p-0107" num="0106">Even though combinations of features are recited in the claims and/or disclosed in the specification, these combinations are not intended to limit the disclosure of possible implementations. In fact, many of these features may be combined in ways not specifically recited in the claims and/or disclosed in the specification. Although each dependent claim listed below may directly depend on only one claim, the disclosure of possible implementations includes each dependent claim in combination with every other claim in the claim set.</p><p id="p-0108" num="0107">No element, act, or instruction used herein may be construed as critical or essential unless explicitly described as such. Also, as used herein, the articles &#x201c;a&#x201d; and &#x201c;an&#x201d; are intended to include one or more items, and may be used interchangeably with &#x201c;one or more.&#x201d; Furthermore, as used herein, the term &#x201c;set&#x201d; is intended to include one or more items (e.g., related items, unrelated items, a combination of related and unrelated items, etc.), and may be used interchangeably with &#x201c;one or more.&#x201d; Where only one item is intended, the term &#x201c;one&#x201d; or similar language is used. Also, as used herein, the terms &#x201c;has,&#x201d; &#x201c;have,&#x201d; &#x201c;having,&#x201d; or the like are intended to be open-ended terms. Further, the phrase &#x201c;based on&#x201d; is intended to mean &#x201c;based, at least in part, on&#x201d; unless explicitly stated otherwise.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of video coding using neural network based inter-frame prediction, the method being performed by at least one processor, and the method comprising:<claim-text>generating reference picture lists including a plurality of reference pictures;</claim-text><claim-text>determining a presence of first reference pictures included in the reference picture lists based on a picture order count (POC) distance;</claim-text><claim-text>selecting one or more different network models, based on the determination of the presence of the first reference pictures;</claim-text><claim-text>generating intermediate flows from a perspective of the first reference pictures and iteratively updating flow fields of the intermediate flows based on the first reference pictures;</claim-text><claim-text>generating an intermediate reference picture based on the selected one or more different neural network models and the intermediate flows;</claim-text><claim-text>predicted a picture based on a weight of a feature map by refine the intermediate reference picture, the feature map including a plurality of levels; and</claim-text><claim-text>computing an enhanced picture as an output by adding a final residual, based on the predicted picture, to the first reference pictures.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein flow fields of the intermediate flows are iteratively updated by moving corresponding pixels from two input reference pictures from the plurality of reference pictures to the same location in a latent intermediate picture.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>generating reconstruction pictures by performing backward warping of input reference pictures with the intermediate flows;</claim-text><claim-text>generating a fusion map and a residual map based on the reconstruction pictures;</claim-text><claim-text>generating a current reference picture by linearly combining the reconstruction pictures according to the fusion map and adding the combined reconstruction pictures with the residual map; and</claim-text><claim-text>generating the feature map using a first neural network based on the current reference picture and one or more of the plurality of reference pictures.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the first reference pictures include a first picture and a second picture, the first picture being a reference picture that precedes the current reference picture in an output order and the second picture being a reference picture that follows the current reference picture in the output order.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the weight of the feature map emphasizes a subset of features in the feature map for generation of subsequent final residuals.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>computing an offset for the plurality of levels;</claim-text><claim-text>performing deformable convolution to generate compensation features for the plurality of levels; and</claim-text><claim-text>generating aligned features of the feature map based on the offset and at least one of the generated compensation features.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:<claim-text>performing convolution to obtain fused attention maps;</claim-text><claim-text>generating attention features based on the fused attention maps and the aligned features;</claim-text><claim-text>generating an aligned frame, based on the fused attention maps and the attention features, using a second neural network; and</claim-text><claim-text>synthesizing the aligned frame to obtain the predicted picture.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. An apparatus for video coding using neural network based inter-frame prediction, the apparatus comprising:<claim-text>at least one memory configured to store program code; and</claim-text><claim-text>at least one processor configured to read the program code and operate as instructed by the program code, the program code comprising:</claim-text><claim-text>first generating code configured to cause the at least one processor to generate reference picture lists including a plurality of reference pictures;</claim-text><claim-text>determining code configured to cause the at least one processor to determine a presence of first reference pictures included in the reference picture lists based on a picture order count (POC) distance;</claim-text><claim-text>selecting code configured to cause the at least one processor to select one or more different neural network models, based on the determination of the presence of the first reference pictures;</claim-text><claim-text>second generating code configured to cause the at least one processor to generate intermediate flows from a perspective of the first reference pictures and iteratively updating flow fields of the intermediate flows based on the first reference pictures;</claim-text><claim-text>third generating code configured to cause the at least one processor to generate an intermediate reference picture based on the selected one or more different neural network models and the intermediate flows;</claim-text><claim-text>predicting code configured to cause the at least one processor to predict a picture based on a weight of a feature map by refining the intermediate reference picture, the feature map including a plurality of levels;</claim-text><claim-text>computing code configured to cause the at least one processor to compute an enhanced picture as an output by adding a final residual, based on the predicted picture, to the first reference pictures.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The apparatus of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein flow fields of the intermediate flows are iteratively updated by moving corresponding pixels from two input reference pictures from the plurality of reference pictures to the same location in a latent intermediate picture.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The apparatus of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:<claim-text>fourth generating code configured to cause the at least one processor to generate reconstruction pictures by performing backward warping of input reference pictures with the intermediate flows;</claim-text><claim-text>fusion code configured to cause the at least one processor to generate a fusion map and a residual map based on the reconstruction pictures;</claim-text><claim-text>reference picture code configured to cause the at least one processor to generate a current reference picture by linearly combining the reconstruction pictures according to the fusion map and adding the combined reconstruction pictures with the residual map; and</claim-text><claim-text>feature map generating code configured to cause the at least one processor to generate the feature map using a first neural network based on the current reference picture and one or more of the plurality of reference pictures.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the first reference pictures include a first picture and a second picture, the first picture being a reference picture that precedes the current reference picture in an output order and the second picture being a reference picture that follows the current reference picture in the output order.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The apparatus of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the weight of the feature map emphasizes a subset of features for generating subsequent final residuals.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The apparatus of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:<claim-text>offset computing code configured to cause the at least one processor to compute an offset for the plurality of levels;</claim-text><claim-text>compensation feature generating code configured to cause the at least one processor to perform deformable convolution to generate compensation features for the plurality of levels; and</claim-text><claim-text>aligned feature generating code configured to cause the at least one processor to generate aligned features of the feature map based on the offset and at least one of the generated compensation features.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:<claim-text>performing code configured to cause the at least one processor to perform convolution to obtain fused attention maps;</claim-text><claim-text>attention feature generating code configured to cause the at least one processor to generate attention features based on the fused attention maps and the aligned features;</claim-text><claim-text>aligned frame generating code configured to cause the at least one processor to generate an aligned frame, based on the fused attention maps and the attention features, using a second neural network; and</claim-text><claim-text>synthesizing code configured to cause the at least one processor to synthesize the aligned frame to obtain the predicted picture.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory computer-readable medium storing instructions that, when executed by at least one processor for video coding using neural network based inter-frame prediction, cause the at least one processor to:<claim-text>generate reference picture lists including a plurality of reference pictures;</claim-text><claim-text>determine a presence of first reference pictures included in the reference picture lists based on a picture order count (POC) distance;</claim-text><claim-text>select one or more different neural network models, based on the determination of the presence of the first reference pictures;</claim-text><claim-text>generate intermediate flows from a perspective of the first reference pictures and iteratively updating flow fields of the intermediate flows based on the first reference pictures;</claim-text><claim-text>generate an intermediate reference picture based on the selected one or more different neural network models and the intermediate flows;</claim-text><claim-text>predict a picture based on a weight of a feature map by refining the intermediate reference picture, the feature map including a plurality of levels; and</claim-text><claim-text>compute an enhanced picture as an output by adding a final residual, based on the predicted picture, to the first reference pictures.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions, when executed by the at least one processor, further cause the at least one processor to iteratively update flow fields of the intermediate flows and by moving corresponding pixels from two input reference pictures from the plurality of reference pictures to the same location in a latent intermediate picture.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions, when executed by the at least one processor, further cause the at least one processor to:<claim-text>generate reconstruction pictures by performing backward warping of input reference pictures with the intermediate flows;</claim-text><claim-text>generate a fusion map and a residual map based on the reconstruction pictures;</claim-text><claim-text>generate a current reference picture by linearly combining the reconstruction pictures according to the fusion map and adding the combined reconstruction pictures with the residual map, wherein the first reference pictures include a first picture and a second picture, the first picture being a reference picture that precedes the current reference picture in an output order and the second picture being a reference picture that follows the current reference picture in the output order; and</claim-text><claim-text>generate the feature map using a first neural network based on the current reference picture and one or more of the plurality of reference pictures.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the weight of the feature map emphasizes a subset of features in the feature map for generating subsequent final residuals.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions, when executed by the at least one processor, further cause the at least one processor to:<claim-text>compute an offset for the plurality of levels;</claim-text><claim-text>perform deformable convolution to generate compensation features for the plurality of levels; and</claim-text><claim-text>generate aligned features of the feature map based on the offset and at least one of the generated compensation features.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the instructions, when executed by the at least one processor, further cause the at least one processor to:<claim-text>perform convolution to obtain fused attention maps;</claim-text><claim-text>generate attention features based on the fused attention maps and the aligned features;</claim-text><claim-text>generate an aligned frame, based on the fused attention maps and the attention features, using a second neural network; and</claim-text><claim-text>synthesize the aligned frame to obtain the predicted picture.</claim-text></claim-text></claim></claims></us-patent-application>