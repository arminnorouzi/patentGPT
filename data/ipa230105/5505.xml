<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005506A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005506</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17940057</doc-number><date>20220908</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202010167788.0</doc-number><date>20200311</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>11</class><subclass>B</subclass><main-group>27</main-group><subgroup>02</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>57</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>166</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>11</class><subclass>B</subclass><main-group>27</main-group><subgroup>02</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>57</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>165</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>166</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">AUDIO PROCESSING METHOD AND ELECTRONIC DEVICE</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/CN2021/079144</doc-number><date>20210304</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17940057</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>VIVO MOBILE COMMUNICATION CO., LTD.</orgname><address><city>Chang'an Dongguan</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>HU</last-name><first-name>Jixiang</first-name><address><city>Chang'an Dongguan</city><country>CN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Embodiments of the present disclosure provide an audio processing method and an electronic device. The method includes: first obtaining text information corresponding to a to-be-processed audio, where the text information includes a to-be-processed text and a playback period corresponding to each field in the to-be-processed text; then receiving a first input on the to-be-processed text; in response to the first input, determining, as a to-be-processed field, a field indicated by the first input in the to-be-processed text; then receiving a second input on the to-be-processed field; obtaining a target audio segment in response to the second input; and finally modifying an audio segment at a playback period corresponding to the to-be-processed field according to the target audio segment, to obtain a target audio.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="147.74mm" wi="141.73mm" file="US20230005506A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="159.43mm" wi="143.76mm" file="US20230005506A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="191.18mm" wi="144.27mm" file="US20230005506A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="203.88mm" wi="128.35mm" file="US20230005506A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="164.68mm" wi="128.35mm" file="US20230005506A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="99.82mm" wi="159.77mm" file="US20230005506A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="193.21mm" wi="144.27mm" file="US20230005506A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="143.26mm" wi="86.53mm" file="US20230005506A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="147.74mm" wi="160.36mm" file="US20230005506A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of International Application No. PCT/CN2021/079144, filed on Mar. 4, 2021, which claims priority to Chinese Patent Application No. 202010167788.0, filed on Mar. 11, 2020 in China, which are incorporated herein by reference in their entireties.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to the field of communications technologies, and in particular, to an audio processing method and an electronic device.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">In life, people often record audio, but there are problems such as a slip of the tongue and a pet phrase during the recording. In this case, audio content needs to be modified, and an unwanted segment needs to be deleted.</p><p id="p-0005" num="0004">In a conventional technology, a user usually manually adjusts a progress bar of the audio to find a playback period of an audio segment that needs to be modified, and then modifies the audio segment at the playback period. In an operation process, the user often needs to repeatedly adjust the progress bar to accurately locate the playback period of the audio segment that needs to be modified. Consequently, the entire operation process is relatively cumbersome, and audio processing efficiency is relatively low.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">Embodiments of the present disclosure provide an audio processing method and an electronic device.</p><p id="p-0007" num="0006">According to a first aspect of the present disclosure, an audio processing method. The method is applied to an electronic device is provided, which includes:</p><p id="p-0008" num="0007">obtaining text information corresponding to a to-be-processed audio, where the text information includes a to-be-processed text and a playback period corresponding to each field in the to-be-processed text;</p><p id="p-0009" num="0008">receiving a first input on the to-be-processed text;</p><p id="p-0010" num="0009">in response to the first input, determining a to-be-processed field in the to-be-processed text according to a field indicated by the first input;</p><p id="p-0011" num="0010">receiving a second input on the to-be-processed field;</p><p id="p-0012" num="0011">obtaining a target audio segment according to the second input; and</p><p id="p-0013" num="0012">modifying an audio segment at a playback period corresponding to the to-be-processed field according to the target audio segment, to obtain a target audio.</p><p id="p-0014" num="0013">According to a second aspect of the present disclosure, an electronic device is provided, which includes:</p><p id="p-0015" num="0014">a first obtaining module, configured to obtain text information corresponding to a to-be-processed audio, where the text information includes a to-be-processed text and a playback period corresponding to each field in the to-be-processed text;</p><p id="p-0016" num="0015">a first receiving module, configured to receive a first input on the to-be-processed text;</p><p id="p-0017" num="0016">a first determining module, configured to: in response to the first input, determine a to-be-processed field in the to-be-processed text according to a field indicated by the first input;</p><p id="p-0018" num="0017">a second receiving module, configured to receive a second input on the to-be-processed field;</p><p id="p-0019" num="0018">a second obtaining module, configured to obtain a target audio segment according to the second input; and</p><p id="p-0020" num="0019">a second determining module, configured to modify an audio segment at a playback period corresponding to the to-be-processed field according to the target audio segment, to obtain a target audio.</p><p id="p-0021" num="0020">According to a third aspect of the present disclosure, an electronic device is provided, which includes a processor, a memory, and an audio processing program that is stored in the memory and that can be run on the processor, and when the audio processing program is executed by the processor, the steps of the audio processing method in the first aspect are implemented.</p><p id="p-0022" num="0021">According to a fourth aspect of the present disclosure, a computer-readable storage medium is provided, where the computer-readable storage medium stores an audio processing program, and when the audio processing program is executed by a processor, the steps of the audio processing method in the first aspect are implemented.</p><p id="p-0023" num="0022">In view of the above, according to the audio processing method and the electronic device provided in the embodiments of the present disclosure, text information corresponding to a to-be-processed audio is obtained, where the text information includes a to-be-processed text and a playback period corresponding to each field in the to-be-processed text; then a first input on the to-be-processed text is received; in response to the first input, a field indicated by the first input in the to-be-processed text is determined as a to-be-processed field; then a second input on the to-be-processed field is received; a target audio segment is obtained in response to the second input; and finally an audio segment at a playback period corresponding to the to-be-processed field is modified according to the target audio segment, to obtain a target audio.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0024" num="0023">To describe the technical solutions in the embodiments of the present disclosure more clearly, the following briefly introduces the accompanying drawings required for describing the embodiments of the present disclosure. Apparently, the accompanying drawings in the following description show merely some embodiments of the present disclosure, and a person of ordinary skill in the art may obtain other accompanying drawings from these accompanying drawings without creative efforts.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a step flowchart of an embodiment of an audio processing method according to the present disclosure;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>2</b>-<b>1</b></figref> is a step flowchart of another embodiment of an audio processing method according to the present disclosure;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>2</b>-<b>2</b></figref> is a schematic diagram of an example of displaying a to-be-processed text according to an embodiment of the present disclosure;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>2</b>-<b>3</b></figref> is a schematic diagram of another example of displaying a to-be-processed text according to an embodiment of the present disclosure;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>2</b>-<b>4</b></figref> is a schematic diagram of an example of editing a to-be-processed text according to an embodiment of the present disclosure;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>2</b>-<b>5</b></figref> is a schematic diagram of another example of editing a to-be-processed text according to an embodiment of the present disclosure;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>2</b>-<b>6</b></figref> is a schematic diagram of still another example of editing a to-be-processed text according to an embodiment of the present disclosure;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a step flowchart of still another embodiment of an audio processing method according to the present disclosure;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a structural block diagram of an embodiment of an electronic device according to the present disclosure; and</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of a hardware structure of an embodiment of an electronic device according to the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0035" num="0034">The following clearly describes the technical solutions in the embodiments of the present disclosure with reference to the accompanying drawings in the embodiments of the present disclosure. Apparently, the described embodiments are some rather than all of the embodiments of the present disclosure. All other embodiments obtained by a person of ordinary skill in the art based on the embodiments of the present disclosure without creative efforts shall fall within the protection scope of the present disclosure.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a step flowchart of an embodiment of an audio processing method according to the present disclosure. The method may be applied to an electronic device. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the method may include step <b>101</b> to step <b>106</b>.</p><p id="p-0037" num="0036">Step <b>101</b>: Obtain text information corresponding to a to-be-processed audio, where the text information includes a to-be-processed text and a playback period corresponding to each field in the to-be-processed text.</p><p id="p-0038" num="0037">In this embodiment of the present disclosure, the to-be-processed audio may be locally stored audio, or may be audio that needs to be modified and that is downloaded from the Internet. The to-be-processed audio may be directly obtained by recording audio, or may be recorded in a video recording process, that is, the to-be-processed audio may be audio extracted from a video. Further, the to-be-processed text may be a text corresponding to the to-be-processed audio, and the corresponding text may be obtained by converting the to-be-processed audio by using a voice-to-text method. The playback period corresponding to each field in the to-be-processed text may be a corresponding playback period of audio corresponding to the field in the to-be-processed audio. For example, assuming that a corresponding playback period of audio corresponding to a field &#x201c;good mood&#x201d; in the to-be-processed text in the to-be-processed audio is the 5.1<sup>th </sup>second to the 5.9<sup>th </sup>second, the playback period &#x201c;the 5.1<sup>th </sup>second to the 5.9<sup>th </sup>second&#x201d; may be determined as the playback period corresponding to the field &#x201c;good mood&#x201d;.</p><p id="p-0039" num="0038">Step <b>102</b>: Receive a first input on the to-be-processed text.</p><p id="p-0040" num="0039">In this embodiment of the present disclosure, the first input on the to-be-processed text may be an operation of selecting a field that needs to be modified in the to-be-processed text on an interface on which the to-be-processed text is displayed. This operation may be a single tap or a double-tap.</p><p id="p-0041" num="0040">Step <b>103</b>: In response to the first input, determine a to-be-processed field in the to-be-processed text according to a field indicated by the first input.</p><p id="p-0042" num="0041">In this embodiment of the present disclosure, the field indicated by the first input is a field selected by a user through the first input, that is, a field corresponding to audio that needs to be modified by the user. Therefore, the to-be-processed field may be determined according to the field indicated by the first input. When the to-be-processed field in the to-be-processed text is determined according to the field indicated by the first input, the field indicated by the first input in the to-be-processed text may be used as the to-be-processed field.</p><p id="p-0043" num="0042">Step <b>104</b>: Receive a second input on the to-be-processed field.</p><p id="p-0044" num="0043">In this embodiment of the present disclosure, the second input on the to-be-processed field may be performed on the interface on which the to-be-processed text is displayed, and the second input may be performed by the user according to a modification requirement for an audio segment corresponding to the to-be-processed field. For example, the second input may be a delete operation on the to-be-processed field, or an input operation for replacing the to-be-processed field, or an operation used to input a field that needs to be added, or an input operation for replacing the audio segment corresponding to the to-be-processed field.</p><p id="p-0045" num="0044">Step <b>105</b>: Obtain a target audio segment in response to the second input.</p><p id="p-0046" num="0045">In this embodiment of the present disclosure, the target audio segment may be an audio segment that the user finally wants. The target audio segment may be directly input by the user, or may be obtained by an electronic device by editing the to-be-processed field. A specific manner of editing the to-be-processed field may be determined according to the second input. For example, when the second input is an operation of inputting a field that needs to be added, a new field may be added to the to-be-processed field. When the second input is a delete operation on the to-be-processed field, the to-be-processed field is deleted, and the like. The second input is performed by the user according to the modification requirement for the audio segment corresponding to the to-be-processed field. Therefore, the to-be-processed field is edited, so that it can be ensured that the obtained target audio segment is a field corresponding to audio that the user finally wants.</p><p id="p-0047" num="0046">Step <b>106</b>: Modify an audio segment at a playback period corresponding to the to-be-processed field according to the target audio segment, to obtain a target audio.</p><p id="p-0048" num="0047">In this embodiment of the present disclosure, when modification is performed according to the target audio segment, the playback period corresponding to the to-be-processed field may be read from the playback period corresponding to each field included in the text information, and then the audio segment at the playback period corresponding to the to-be-processed field is modified into the target audio segment, thereby implementing modification of the to-be-processed audio.</p><p id="p-0049" num="0048">In view of the above, according to the audio processing method provided in this embodiment of the present disclosure, text information corresponding to a to-be-processed audio is obtained, where the text information includes a to-be-processed text and a playback period corresponding to each field in the to-be-processed text; then a first input on the to-be-processed text is received; in response to the first input, a to-be-processed field in the to-be-processed text is determined according to a field indicated by the first input; then a second input on the to-be-processed field is received; a target audio segment is obtained in response to the second input; and finally an audio segment at a playback period corresponding to the to-be-processed field is modified according to the target audio segment, to obtain a target audio. In this way, audio can be modified without manually adjusting a progress bar, thereby improving audio processing efficiency.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>2</b>-<b>1</b></figref> is a step flowchart of another embodiment of an audio processing method according to the present disclosure. The method may be applied to an electronic device. As shown in <figref idref="DRAWINGS">FIG. <b>2</b>-<b>1</b></figref>, the method may include step <b>201</b> to step <b>207</b>.</p><p id="p-0051" num="0050">Step <b>201</b>: Obtain text information corresponding to a to-be-processed audio, where the text information includes a to-be-processed text and a playback period corresponding to each field in the to-be-processed text.</p><p id="p-0052" num="0051">In this embodiment of the present disclosure, the electronic device may obtain the text information corresponding to the to-be-processed audio by performing the following step <b>2011</b> to step <b>2013</b>.</p><p id="p-0053" num="0052">Step <b>2011</b>: Detect whether a subtitle file matching the to-be-processed audio exists, where the subtitle file includes a subtitle text and a playback period corresponding to each field in the subtitle text.</p><p id="p-0054" num="0053">In this embodiment of the present disclosure, the to-be-processed audio may be audio in a video, and correspondingly, the subtitle file may be a subtitle file matching the video. The to-be-processed audio may also be independent audio such as a song, and correspondingly, the subtitle file may be a lyric file matching the song. The detecting whether a subtitle file matching the to-be-processed audio exists may be searching for whether the subtitle file matching the to-be-processed audio online exists, or locally searching for whether the matched subtitle file exists.</p><p id="p-0055" num="0054">Step <b>2012</b>: If the subtitle file matching the to-be-processed audio exists, use the subtitle file as the text information corresponding to the to-be-processed audio.</p><p id="p-0056" num="0055">In this embodiment of the present disclosure, the using the subtitle file as the text information corresponding to the to-be-processed audio may be using a subtitle text included in the subtitle file as the to-be-processed text corresponding to the to-be-processed audio, and using a playback period corresponding to each field in the subtitle text as a playback period of the field in the to-be-processed audio. It is detected whether the matched subtitle file exists, and in a case that the subtitle file exists, the subtitle file is used as the text information corresponding to the to-be-processed audio. In this way, a step of generating a text according to audio may be omitted, and an audio processing time is further saved to some extent.</p><p id="p-0057" num="0056">Step <b>2013</b>: If the subtitle file matching the to-be-processed audio does not exist, convert an audio included in the to-be-processed audio into a text, generate a playback period corresponding to each field in the text according to playback time information of an audio segment in the to-be-processed audio, and use the text and the playback period corresponding to each field in the text as the text information corresponding to the to-be-processed audio.</p><p id="p-0058" num="0057">In this embodiment of the present disclosure, the to-be-processed audio may be converted into a text by using a voice-to-text method. Alternatively, the audio may be processed first to remove a noise in the audio, thereby avoid causing interference to the conversion process. Then, a feature value in the audio is extracted, and the audio is divided into a smaller audio segment, so that the audio segment includes one or more feature values. Matching is performed on a model feature value in an audio model library according to the feature value in the audio segment, and a text corresponding to the model feature value obtained through the matching is determined as a text corresponding to the audio segment. The generating a playback period corresponding to each field in the text may be reading a playback period corresponding to a to-be-divided audio segment in a process of converting audio into a text, and then using the playback period corresponding to the to-be-divided audio segment as a playback period corresponding to the field. In this way, when the subtitle file does not exist, the corresponding text is generated by using the audio, to obtain text content matching the to-be-processed audio, thereby ensuring that accurate text information can be subsequently provided.</p><p id="p-0059" num="0058">Step <b>202</b>: Receive a first input on the to-be-processed text.</p><p id="p-0060" num="0059">In this embodiment of the present disclosure, before the first input on the to-be-processed text is received, the to-be-processed text may be displayed by performing the following step:</p><p id="p-0061" num="0060">displaying a preset picture, and displaying all to-be-processed texts in the preset picture; or displaying each video picture of the to-be-processed video, and displaying a to-be-processed text corresponding to the video picture in the video picture.</p><p id="p-0062" num="0061">Alternatively, the preset picture may be preset according to an actual situation. For example, the preset picture may be a picture associated with the to-be-processed audio. For example, the preset picture may be a video cover of a video to which the to-be-processed audio belongs, or a cover of an audio album to which the to-be-processed audio belongs, or a photo of a singer of the to-be-processed audio. This is not limited in this embodiment of the present disclosure. Further, all to-be-processed texts are displayed in the preset picture, so that the user can visually view all the to-be-processed texts. Meanwhile, a preset picture related to the to-be-processed text is used, so that viewing experience of the user can be improved. For example, <figref idref="DRAWINGS">FIG. <b>2</b>-<b>2</b></figref> is a schematic diagram of an example of displaying a to-be-processed text according to an embodiment of the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>2</b>-<b>2</b></figref>, all to-be-processed texts are displayed in a related picture of a singer that sings a to-be-processed audio. It should be noted that in an actual application scenario, there may be many to-be-processed texts, and due to a limitation of a screen size of the electronic device, all to-be-processed texts may not be displayed at once. Therefore, scrolling display may be performed on the to-be-processed texts to ensure complete display.</p><p id="p-0063" num="0062">Further, the video picture may be displayed, and the corresponding to-be-processed text is displayed in the video picture. The to-be-processed text corresponding to the video picture may be a text whose playback period is the same as a playback period of the video picture. Content of the video picture is usually strongly associated with the to-be-processed text corresponding to the video picture. Therefore, in separate display in the video picture, it is convenient for the user to observe both content and text content of the video picture, thereby facilitating the user to select. Alternatively, a text display box may be generated on the video picture, and the to-be-processed text is displayed in the text display box. A specific form of the display box may be preset according to an actual situation. For example, <figref idref="DRAWINGS">FIG. <b>2</b>-<b>3</b></figref> is a schematic diagram of another example of displaying a to-be-processed text according to an embodiment of the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>2</b>-<b>3</b></figref>, the corresponding to-be-processed text is displayed in the video picture, that is, &#x201c;It was more than the beer last night that made me shed tears&#x201d;.</p><p id="p-0064" num="0063">Correspondingly, the electronic device may receive the first input by receiving a select input on the displayed to-be-processed text. In this way, the to-be-processed text is displayed in the preset picture or the video picture, so that the user can be provided with a visual selection scenario and rich information, and the user can conveniently select the to-be-processed text, thereby improving selection efficiency.</p><p id="p-0065" num="0064">Step <b>203</b>: In response to the first input, determine a to-be-processed field in the to-be-processed text according to a field indicated by the first input.</p><p id="p-0066" num="0065">Alternatively, when the field indicated by the first input in the to-be-processed text is determined as the to-be-processed field, all to-be-processed texts may be searched for fields indicated by all included first inputs, and then a searched field is determined as the to-be-processed field. The field indicated by the first input may be a field selected by the user for the select input on the displayed to-be-processed text. Alternatively, the first input may be performed by using a preset search area, and the field indicated by the first input may be input by using the search area. Correspondingly, the electronic device may display the search area before this step, and then receive the first input performed by the user by using the search area. In this way, the user needs to select only once to control the electronic device to modify all same fields, thereby improving selection efficiency.</p><p id="p-0067" num="0066">Further, after step <b>203</b> is performed, an audio volume may be adjusted by performing the following step A to step C.</p><p id="p-0068" num="0067">Step A: Receive a third input on the to-be-processed text.</p><p id="p-0069" num="0068">In this embodiment of the present disclosure, the third input on the to-be-processed text may be performed on the interface on which the to-be-processed text is displayed, and the third input may be an adjustment operation on a font of the to-be-processed text. The user may perform the third input when the font of the to-be-processed text needs to be adjusted, and correspondingly, the electronic device may receive the third input.</p><p id="p-0070" num="0069">Step B: In response to the third input, adjust a font size of a to-be-adjusted field indicated by the third input, to obtain an adjusted to-be-adjusted field.</p><p id="p-0071" num="0070">In this embodiment of the present disclosure, the adjusting a font size of a to-be-adjusted field indicated by the third input may be enlarging or narrowing the font size of the to-be-adjusted field according to the adjustment operation indicated by the third input, to obtain the adjusted to-be-adjusted field.</p><p id="p-0072" num="0071">Step C: Adjust a volume level of an audio corresponding to the to-be-adjusted field according to a font size of the adjusted to-be-adjusted field, where a larger font of the adjusted to-be-adjusted field indicates a larger volume of the audio corresponding to the to-be-adjusted field.</p><p id="p-0073" num="0072">In this embodiment of the present disclosure, the adjusting a volume level of an audio corresponding to the to-be-adjusted field according to a font size of the adjusted to-be-adjusted field may be first determining the font size of the adjusted to-be-adjusted field, then determining a volume corresponding to the font size of the adjusted to-be-adjusted field according to a preset correspondence between a font size and a volume level, and finally setting the volume level of the audio corresponding to the to-be-adjusted field as a volume of the audio corresponding to the to-be-adjusted field, thereby implementing volume adjustment. In the preset correspondence between a font size and a volume level, a larger font indicates a larger volume.</p><p id="p-0074" num="0073">For example, it is assumed that the font size of the adjusted to-be-adjusted field is 4 pt, and a volume corresponding to the font size 4 pt is 60 decibels. Correspondingly, the volume level of the audio corresponding to the to-be-adjusted field may be set to 60 decibels. In this way, the user only needs to adjust a font size of a text to correspondingly control adjustment of a volume level of corresponding audio, so that an audio volume adjustment process is simpler, thereby improving adjustment efficiency.</p><p id="p-0075" num="0074">Further, in this embodiment of the present disclosure, a curve used to adjust a font size may be preset. Correspondingly, the user may select, from the to-be-processed text, the to-be-adjusted field whose font size needs to be adjusted, and then adjust a shape of the curve, thereby implementing the second input. Further, when the font size is adjusted, a size of each word included in the to-be-adjusted field may be successively adjusted according to a height of each segment of an adjusted curve. The height of the segment may be directly proportional or inversely proportional to the size of the word. In this way, the user only needs to adjust the shape of the curve to adjust a volume level of a corresponding audio segment. In addition, due to a variety of shapes of the curve, the volume level of the audio segment corresponding to the to-be-processed field may vary. For example, the user may adjust the curve to a wave shape to control a volume corresponding to the to-be-adjusted field to increase or decrease, thereby improving fun of the audio.</p><p id="p-0076" num="0075">Step <b>204</b>: Receive a second input on the to-be-processed field.</p><p id="p-0077" num="0076">Alternatively, for an implementation of this step, refer to the foregoing step <b>104</b>. This is not limited in this embodiment of the present disclosure.</p><p id="p-0078" num="0077">Step <b>205</b>: Edit the to-be-processed field according to the second input to obtain a target field.</p><p id="p-0079" num="0078">In this embodiment of the present disclosure, if the second input is a delete input, it may be considered that the user needs to delete the to-be-processed field. Therefore, the to-be-processed field may be deleted, and a blank field obtained after the deletion is determined as the target field.</p><p id="p-0080" num="0079">Further, if the second input is a replace input, it may be considered that the user needs to replace the to-be-processed field. Therefore, a to-be-replaced field corresponding to the second input may be obtained, the to-be-processed field is deleted, and the to-be-replaced field is added at a location of the to-be-processed field to obtain the target field. The obtaining a to-be-replaced field corresponding to the second input may be extracting a field included in the second input, and using the field as the to-be-replaced field, or extracting a voice included in the second input, obtaining a text corresponding to the voice by using a voice-to-text method, and using the obtained text as the to-be-replaced field.</p><p id="p-0081" num="0080">Further, if the second input is an add input, it may be considered that the user needs to add a new filed to the to-be-processed field. Therefore, a to-be-added field corresponding to the second input may be obtained, and the to-be-added field is added at a location of the to-be-processed field to obtain the target field. In this embodiment of the present disclosure, the obtaining a to-be-added field corresponding to the second input may be extracting a field included in the second input, and using the field as the to-be-added field, or extracting a voice included in the second input, obtaining a text corresponding to the voice by using a voice-to-text method, and using the obtained text as the to-be-added field. In this embodiment of the present disclosure, corresponding edit operations can be performed according to different second inputs, thereby satisfying a plurality of modification requirements of the user and improving an audio modification effect.</p><p id="p-0082" num="0081">It should be noted that a preset mark may be further added to the displayed to-be-processed field, and the to-be-replaced field or the to-be-added field is displayed according to a display location corresponding to the to-be-processed field. Alternatively, the preset mark may be a mark that reflects a specific edit operation performed on the to-be-processed field, and preset marks corresponding to different edit operations are different. For example, if the edit operation is to delete the to-be-processed field, the preset mark may be a delete line added to the to-be-processed field, or may be a text mark added to the to-be-processed field to indicate that the field is deleted. If the edit operation is to replace the to-be-processed field, the preset mark may be an underline added to the to-be-processed field, or may be a text mark added to the to-be-processed field to indicate that the field is replaced, and the to-be-replaced field is displayed next to the to-be-processed field. A specific display location may be set according to an actual situation. If the edit operation is to add a field at the location of the to-be-processed field, the preset mark may be to add a field mark, such as an arrow, at the location corresponding to the to-be-processed field, to instruct to add a field at the location. In addition, the added to-be-added field may be displayed, so that the user learns a specific added field. There may be a plurality of specific mark manners. This is not limited in this embodiment of the present disclosure. The preset mark is added to the to-be-processed field, so that the user can learn more clearly the location of the to-be-processed field that is modified and a specific edit operation performed on the to-be-processed field.</p><p id="p-0083" num="0082">Further, the display location may be preset according to an actual requirement. For example, the display location may be below the to-be-deleted field. In this way, the to-be-replaced field or the to-be-added field is displayed at the display location corresponding to the to-be-deleted field, so that the user can quickly learn specific modified content, and it is convenient for the user to check later.</p><p id="p-0084" num="0083">For example, <figref idref="DRAWINGS">FIG. <b>2</b>-<b>4</b></figref> is a schematic diagram of an example of editing a to-be-processed text according to an embodiment of the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>2</b>-<b>4</b></figref>, if the to-be-processed field is &#x201c;made me shed tears&#x201d; and the second input is a delete input, the to-be-processed field is deleted, that is, a delete line is added to &#x201c;made me shed tears&#x201d; for deletion.</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>2</b>-<b>5</b></figref> is a schematic diagram of another example of editing a to-be-processed text according to an embodiment of the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>2</b>-<b>5</b></figref>, if the to-be-processed field is &#x201c;tears&#x201d;, and the second input is a replace input, the to-be-processed field is deleted and a to-be-replaced field is displayed, that is, a delete line is added to &#x201c;tears&#x201d;, and &#x201c;saliva&#x201d; under the to-be-processed field is the to-be-replaced field.</p><p id="p-0086" num="0085"><figref idref="DRAWINGS">FIG. <b>2</b>-<b>6</b></figref> is a schematic diagram of still another example of editing a to-be-processed text according to an embodiment of the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>2</b>-<b>6</b></figref>, if a location indicated by the to-be-processed field is between &#x201c;me&#x201d; and &#x201c;shed&#x201d;, and the second input is an add input, an arrow is used to indicate the location of the to-be-processed field, and &#x201c;today&#x201d; under the arrow is a to-be-added field.</p><p id="p-0087" num="0086">Step <b>206</b>: Determine an audio corresponding to the target field as the target audio segment</p><p id="p-0088" num="0087">Alternatively, a linguistic analysis may be first performed on a text to divide the target field into words, and then an audio waveform segment corresponding to a matched field is extracted from a voice synthesis library according to the words obtained through division, and an audio waveform segment corresponding to each word is synthesized to obtain an audio segment corresponding to the text. Alternatively, the to-be-processed audio may be searched to determine whether a field that is the same as the target field exits. If yes, an audio segment corresponding to the same field is extracted as the audio corresponding to the target field, to obtain the target audio segment.</p><p id="p-0089" num="0088">Step <b>207</b>: Modify an audio segment at a playback period corresponding to the to-be-processed field according to the target audio segment, to obtain a target audio.</p><p id="p-0090" num="0089">Alternatively, in this step, the playback period corresponding to a to-be-processed field may be first obtained from the playback period corresponding to each field, then an audio waveform graph corresponding to the to-be-processed audio is obtained, and finally a corresponding wave band of the playback period corresponding to the to-be-processed field in the audio waveform graph is modified into an audio wave band corresponding to the target audio segment to obtain the target audio.</p><p id="p-0091" num="0090">When the playback period corresponding to the to-be-processed field is obtained, each field may be searched for the to-be-processed field, and then the playback period corresponding to the to-be-processed field is read. When the audio waveform graph corresponding to the to-be-processed audio is obtained, a feature included in the audio may be extracted, for example, a vibration frequency, to process the feature, for example, normalized processing, to obtain a waveform graph for displaying an audio feature according a playback time.</p><p id="p-0092" num="0091">Further, when the corresponding wave band of the playback period corresponding to the to-be-processed field in the audio waveform graph is modified into a blank wave band corresponding to the target audio segment, the blank wave band may be used to replace the corresponding wave band to implement modification. Alternatively, the corresponding wave band may be directly deleted to implement modification. It should be noted that, waveform display of the corresponding wave band may be removed during the deletion, and changed to straight line display, to indicate that a sound is deleted.</p><p id="p-0093" num="0092">If the target field is a to-be-replaced field, the corresponding wave band may be directly replaced with the audio wave band corresponding to the target audio segment, or the corresponding wave band may be first deleted, and then an audio wave band corresponding to the to-be-replaced field in the audio wave band corresponding to the target audio segment is added at a deletion location. If the target field is a to-be-added field, the corresponding wave band may be directly replaced with the audio wave band corresponding to the target audio segment, or an audio wave band corresponding to the to-be-added field in the audio wave band corresponding to the target audio segment may be added according to a location of the playback period corresponding to the to-be-processed field in the corresponding wave band in the audio waveform graph, and an audio wave band obtained through synthesis is used as the target audio. In this way, the wave band of the to-be-processed audio is correspondingly modified in the audio waveform graph, to implement modification of the to-be-processed audio, so that a modification process is more accurate, thereby improving modification accuracy.</p><p id="p-0094" num="0093">It should be noted that the electronic device may further perform the following operations after obtaining the audio waveform graph:</p><p id="p-0095" num="0094">displaying the audio waveform graph corresponding to the to-be-processed audio; and marking the corresponding wave band of the playback period corresponding to the to-be-processed field in the audio waveform graph. The marking may be filling the corresponding wave band with different colors, or may be adding a mark at a location of the corresponding wave band. A specific marking form is not limited in this embodiment of the present disclosure. In this way, the audio waveform graph corresponding to the to-be-processed audio is displayed, and the wave band corresponding to the to-be-processed field is marked in the audio waveform graph, so that the user can conveniently view a modified audio wave band.</p><p id="p-0096" num="0095">It should be noted that before the audio wave band is modified, the to-be-processed audio may be further processed to separate a human voice from a background voice in the to-be-processed audio, and then the human voice in the to-be-processed audio is extracted, and an audio wave band corresponding to the human voice is correspondingly modified. Finally, the modified human voice and the background voice are synthesized to obtain a target audio. In this way, the background voice in the audio can be reserved only for modification of the human voice, thereby greatly reducing a degree of modification of the audio. In this way, the modified audio is more natural and coherent.</p><p id="p-0097" num="0096">In view of the above, according to the audio processing method provided in this embodiment of the present disclosure, text information corresponding to a to-be-processed audio is obtained, where the text information includes a to-be-processed text and a playback period corresponding to each field in the to-be-processed text; then a first input on the to-be-processed text is received; in response to the first input, a to-be-processed field in the to-be-processed text is determined according to a field indicated by the first input; then a second input on the to-be-processed field is received; the to-be-processed field is edited according to the second input to obtain a target field, and an audio corresponding to the target field is determined as a target audio segment; and finally an audio segment at a playback period corresponding to the to-be-processed field is modified according to the target audio segment, to obtain a target audio. In this way, corresponding edit operations can be performed according to different second inputs, thereby satisfying a plurality of modification requirements of a user and improving an audio modification effect. In addition, audio can be modified without manually adjusting a progress bar by the user, thereby improving audio processing efficiency.</p><p id="p-0098" num="0097"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a step flowchart of still another embodiment of an audio processing method according to the present disclosure. The method may be applied to an electronic device. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the method may include step <b>301</b> to step <b>307</b>.</p><p id="p-0099" num="0098">Step <b>301</b>: Obtain text information corresponding to a to-be-processed audio, where the text information includes a to-be-processed text and a playback period corresponding to each field in the to-be-processed text.</p><p id="p-0100" num="0099">Alternatively, for an implementation of this step, refer to the foregoing step <b>201</b>. This is not limited in this embodiment of the present disclosure.</p><p id="p-0101" num="0100">Step <b>302</b>: Receive a first input on the to-be-processed text.</p><p id="p-0102" num="0101">Alternatively, for an implementation of this step, refer to the foregoing step <b>202</b>. This is not limited in this embodiment of the present disclosure.</p><p id="p-0103" num="0102">Step <b>303</b>: In response to the first input, determine a to-be-processed field in the to-be-processed text according to a field indicated by the first input.</p><p id="p-0104" num="0103">Alternatively, for an implementation of this step, refer to the foregoing step <b>203</b>. This is not limited in this embodiment of the present disclosure.</p><p id="p-0105" num="0104">Step <b>304</b>: Receive a second input on the to-be-processed field.</p><p id="p-0106" num="0105">Alternatively, for an implementation of this step, refer to the foregoing step <b>104</b>. This is not limited in this embodiment of the present disclosure.</p><p id="p-0107" num="0106">Step <b>305</b>: Extract an audio segment carried in the second input.</p><p id="p-0108" num="0107">In this embodiment of the present disclosure, the second input may be an audio record operation, and correspondingly, the audio segment carried in the second input may be a voice segment recorded by the user. The second input may also be an audio upload operation, and correspondingly, the audio segment carried in the second input may also be an audio segment selected by the user to be uploaded.</p><p id="p-0109" num="0108">Step <b>306</b>: Determine the audio segment as the target audio segment.</p><p id="p-0110" num="0109">In this embodiment of the present disclosure, the second input is usually performed according to a modification requirement of the user for an audio segment corresponding to the to-be-processed field, that is, the audio segment carried in the second input is an audio segment that the user finally wants. Therefore, the audio segment may be directly determined as the target audio segment.</p><p id="p-0111" num="0110">It should be noted that, in this embodiment of the present disclosure, before the input audio segment is determined as the target audio segment, the user may be prompted whether to process the input audio segment. If yes, the input audio segment is truncated according to a user operation, and an audio segment obtained through the truncation is used as the target audio segment. In this way, quality of the target audio segment can be further improved by prompting the user whether to process the input audio segment.</p><p id="p-0112" num="0111">Step <b>307</b>: Modify an audio segment at a playback period corresponding to the to-be-processed field according to the target audio segment, to obtain a target audio.</p><p id="p-0113" num="0112">Alternatively, for an implementation of this step, refer to the foregoing step <b>207</b>. This is not limited in this embodiment of the present disclosure.</p><p id="p-0114" num="0113">In view of the above, according to the audio processing method provided in this embodiment of the present disclosure, text information corresponding to a to-be-processed audio is obtained, where the text information includes a to-be-processed text and a playback period corresponding to each field in the to-be-processed text; then a first input on the to-be-processed text is received; in response to the first input, a to-be-processed field in the to-be-processed text is determined according to a field indicated by the first input; then a second input on the to-be-processed field is received; an audio segment carried in the second input is extracted, and the audio segment is determined as a target audio segment; and finally an audio segment at a playback period corresponding to the to-be-processed field is modified according to the target audio segment, to obtain a target audio. In this way, the audio segment carried in the second input is directly extracted, so that the target audio segment can be conveniently obtained. Therefore, processing efficiency can be improved. In addition, audio can be modified without manually adjusting a progress bar by the user, thereby further improving audio processing efficiency.</p><p id="p-0115" num="0114">The foregoing describes the audio processing method provided in the embodiments of the present disclosure, and the following describes the electronic device provided in the embodiments of the present disclosure with reference to the accompanying drawings.</p><p id="p-0116" num="0115">Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, an embodiment of the present disclosure further provides an electronic device. As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the electronic device <b>40</b> may include:</p><p id="p-0117" num="0116">a first obtaining module <b>401</b>, configured to obtain text information corresponding to a to-be-processed audio, where the text information includes a to-be-processed text and a playback period corresponding to each field in the to-be-processed text;</p><p id="p-0118" num="0117">a first receiving module <b>402</b>, configured to receive a first input on the to-be-processed text;</p><p id="p-0119" num="0118">a first determining module <b>403</b>, configured to: in response to the first input, determine a to-be-processed field in the to-be-processed text according to a field indicated by the first input;</p><p id="p-0120" num="0119">a second receiving module <b>404</b>, configured to receive a second input on the to-be-processed field;</p><p id="p-0121" num="0120">a second obtaining module <b>405</b>, configured to obtain a target audio segment according to the second input; and</p><p id="p-0122" num="0121">a second determining module <b>406</b>, configured to modify an audio segment at a playback period corresponding to the to-be-processed field according to the target audio segment, to obtain a target audio.</p><p id="p-0123" num="0122">In view of the above, according to the electronic device provided in this embodiment of the present disclosure, text information corresponding to a to-be-processed audio is obtained, where the text information includes a to-be-processed text and a playback period corresponding to each field in the to-be-processed text; then a first input on the to-be-processed text is received; in response to the first input, a to-be-processed field in the to-be-processed text is determined according to a field indicated by the first input; then a second input on the to-be-processed field is received; a target audio segment is obtained in response to the second input; and finally an audio segment at a playback period corresponding to the to-be-processed field is modified according to the target audio segment, to obtain a target audio. In this way, audio can be modified without manually adjusting a progress bar, thereby improving audio processing efficiency.</p><p id="p-0124" num="0123">Optionally, the second obtaining module <b>405</b> is configured to:</p><p id="p-0125" num="0124">edit the to-be-processed field according to the second input to obtain a target field, and determine an audio corresponding to the target field as the target audio segment; or extract an audio segment carried in the second input, and determine the audio segment as the target audio segment.</p><p id="p-0126" num="0125">Optionally, the second obtaining module <b>405</b> is further configured to:</p><p id="p-0127" num="0126">if the second input is a delete input, delete the to-be-processed field, and determine a blank field obtained after the deletion as the target field;</p><p id="p-0128" num="0127">if the second input is a replace input, obtain a to-be-replaced field corresponding to the second input, deleting the to-be-processed field, and add the to-be-replaced field at a location of the to-be-processed field to obtain the target field; or</p><p id="p-0129" num="0128">if the second input is an add input, obtain a to-be-added field corresponding to the second input, and add the to-be-added field at a location of the to-be-processed field to obtain the target field.</p><p id="p-0130" num="0129">Optionally, the electronic device <b>40</b> further includes:</p><p id="p-0131" num="0130">a first display module, configured to: display a preset picture, and display all to-be-processed texts in the preset picture; or display each video picture of the to-be-processed video, and display a to-be-processed text corresponding to the video picture in the video picture.</p><p id="p-0132" num="0131">The first receiving module <b>402</b> is further configured to:</p><p id="p-0133" num="0132">receive a select input on the displayed to-be-processed text.</p><p id="p-0134" num="0133">Optionally, the electronic device <b>40</b> further includes:</p><p id="p-0135" num="0134">a second display module, configured to: add a preset mark to the displayed to-be-processed field, and display the to-be-replaced field or the to-be-added field according to a display location corresponding to the to-be-processed field.</p><p id="p-0136" num="0135">Optionally, the second determining module <b>406</b> is configured to:</p><p id="p-0137" num="0136">obtain the playback period corresponding to the to-be-processed field from the playback period corresponding to each field;</p><p id="p-0138" num="0137">obtain an audio waveform graph corresponding to the to-be-processed audio; and</p><p id="p-0139" num="0138">modify a corresponding wave band of the playback period corresponding to the to-be-processed field in the audio waveform graph into an audio wave band corresponding to the target audio segment, to obtain the target audio.</p><p id="p-0140" num="0139">Optionally, the electronic device <b>40</b> further includes:</p><p id="p-0141" num="0140">a third receiving module, configured to receive a third input on the to-be-processed text;</p><p id="p-0142" num="0141">a first adjusting module, configured to: in response to the third input, adjust a font size of a to-be-adjusted field indicated by the third input, to obtain an adjusted to-be-adjusted field; and</p><p id="p-0143" num="0142">a second adjusting module, configured to adjust a volume level of an audio corresponding to the to-be-adjusted field according to a font size of the adjusted to-be-adjusted field, where a larger font of the adjusted to-be-adjusted field indicates a larger volume of the audio corresponding to the to-be-adjusted field.</p><p id="p-0144" num="0143">Optionally, the first obtaining module <b>401</b> is configured to:</p><p id="p-0145" num="0144">detect whether a subtitle file matching the to-be-processed audio exists, where the subtitle file includes a subtitle text and a playback period corresponding to each field in the subtitle text; and</p><p id="p-0146" num="0145">if the subtitle file matching the to-be-processed audio exists, use the subtitle file as the text information corresponding to the to-be-processed audio; or</p><p id="p-0147" num="0146">if the subtitle file matching the to-be-processed audio does not exist, convert an audio included in the to-be-processed audio into a text, generate a playback period corresponding to each field in the text according to playback time information of an audio segment in the to-be-processed audio, and use the text and the playback period corresponding to each field in the text as the text information corresponding to the to-be-processed audio.</p><p id="p-0148" num="0147">In view of the above, according to the electronic device provided in this embodiment of the present disclosure, text information corresponding to a to-be-processed audio is obtained, where the text information includes a to-be-processed text and a playback period corresponding to each field in the to-be-processed text; then a first input on the to-be-processed text is received; in response to the first input, a to-be-processed field in the to-be-processed text is determined according to a field indicated by the first input; then a second input on the to-be-processed field is received; the to-be-processed field is edited according to the second input to obtain a target field, and an audio corresponding to the target field is determined as a target audio segment; and finally an audio segment at a playback period corresponding to the to-be-processed field is modified according to the target audio segment, to obtain a target audio. In this way, corresponding edit operations can be performed according to different second inputs, thereby satisfying a plurality of modification requirements of a user and improving an audio modification effect. In addition, audio can be modified without manually adjusting a progress bar by the user, thereby improving audio processing efficiency.</p><p id="p-0149" num="0148"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of a hardware structure of an embodiment of an electronic device according to the present disclosure.</p><p id="p-0150" num="0149">An electronic device <b>500</b> includes but is not limited to components such as a radio frequency unit <b>501</b>, a network module <b>502</b>, an audio output unit <b>503</b>, an input unit <b>504</b>, a sensor <b>505</b>, a display unit <b>506</b>, a user input unit <b>507</b>, an interface unit <b>508</b>, a memory <b>509</b>, a processor <b>510</b>, and a power supply <b>511</b>. A person skilled in the art may understand that a structure of the electronic device shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> constitutes no limitation on the electronic device, and the electronic device may include more or fewer components than those shown in the figure, or have a combination of some components, or have a different component arrangement. In this embodiment of the present disclosure, the electronic device includes but is not limited to a mobile phone, a tablet computer, a notebook computer, a palmtop computer, an in-vehicle terminal, a wearable device, a pedometer, and the like.</p><p id="p-0151" num="0150">The processor <b>510</b> is configured to obtain text information corresponding to a to-be-processed audio, where the text information includes a to-be-processed text and a playback period corresponding to each field in the to-be-processed text.</p><p id="p-0152" num="0151">The processor <b>510</b> is configured to receive a first input on the to-be-processed text.</p><p id="p-0153" num="0152">The processor <b>510</b> is configured to: in response to the first input, determine a to-be-processed field in the to-be-processed text according to a field indicated by the first input.</p><p id="p-0154" num="0153">The processor <b>510</b> is configured to receive a second input on the to-be-processed field.</p><p id="p-0155" num="0154">The processor <b>510</b> is configured to obtain a target audio segment in response to the second input.</p><p id="p-0156" num="0155">The processor <b>510</b> is configured to modify an audio segment at a playback period corresponding to the to-be-processed field according to the target audio segment, to obtain a target audio.</p><p id="p-0157" num="0156">In view of the above, according to the electronic device provided in this embodiment of the present disclosure, text information corresponding to a to-be-processed audio is obtained, where the text information includes a to-be-processed text and a playback period corresponding to each field in the to-be-processed text; then a first input on the to-be-processed text is received; in response to the first input, a to-be-processed field in the to-be-processed text is determined according to a field indicated by the first input; then a second input on the to-be-processed field is received; a target audio segment is obtained in response to the second input; and finally an audio segment at a playback period corresponding to the to-be-processed field is modified according to the target audio segment, to obtain a target audio. In this way, audio can be modified without manually adjusting a progress bar, thereby improving audio processing efficiency.</p><p id="p-0158" num="0157">Optionally, the processor <b>510</b> is configured to:</p><p id="p-0159" num="0158">edit the to-be-processed field according to the second input to obtain a target field, and determine an audio corresponding to the target field as the target audio segment; or</p><p id="p-0160" num="0159">extract an audio segment carried in the second input, and determine the audio segment as the target audio segment.</p><p id="p-0161" num="0160">Optionally, the processor <b>510</b> is further configured to:</p><p id="p-0162" num="0161">if the second input is a delete input, delete the to-be-processed field, and determine a blank field obtained after the deletion as the target field;</p><p id="p-0163" num="0162">if the second input is a replace input, obtain a to-be-replaced field corresponding to the second input, deleting the to-be-processed field, and add the to-be-replaced field at a location of the to-be-processed field to obtain the target field; or</p><p id="p-0164" num="0163">if the second input is an add input, obtain a to-be-added field corresponding to the second input, and add the to-be-added field at a location of the to-be-processed field to obtain the target field.</p><p id="p-0165" num="0164">Optionally, the display unit <b>506</b> is configured to:</p><p id="p-0166" num="0165">display a preset picture, and display all to-be-processed texts in the preset picture; or display each video picture of the to-be-processed video, and display a to-be-processed text corresponding to the video picture in the video picture.</p><p id="p-0167" num="0166">Correspondingly, the user input unit <b>507</b> is configured to receive a select input on the displayed to-be-processed text.</p><p id="p-0168" num="0167">Optionally, the processor <b>510</b> is configured to:</p><p id="p-0169" num="0168">obtain the playback period corresponding to the to-be-processed field from the playback period corresponding to each field;</p><p id="p-0170" num="0169">obtain an audio waveform graph corresponding to the to-be-processed audio; and</p><p id="p-0171" num="0170">modify a corresponding wave band of the playback period corresponding to the to-be-processed field in the audio waveform graph into an audio wave band corresponding to the target audio segment, to obtain the target audio.</p><p id="p-0172" num="0171">Optionally, the user input unit <b>507</b> is configured to:</p><p id="p-0173" num="0172">receive a third input on the to-be-processed text.</p><p id="p-0174" num="0173">The processor <b>510</b> is configured to:</p><p id="p-0175" num="0174">in response to the third input, adjust a font size of a to-be-adjusted field indicated by the third input, to obtain an adjusted to-be-adjusted field; and</p><p id="p-0176" num="0175">adjust a volume level of an audio corresponding to the to-be-adjusted field according to a font size of the adjusted to-be-adjusted field, where a larger font of the adjusted to-be-adjusted field indicates a larger volume of the audio corresponding to the to-be-adjusted field.</p><p id="p-0177" num="0176">Optionally, the processor <b>510</b> is configured to:</p><p id="p-0178" num="0177">detect whether a subtitle file matching the to-be-processed audio exists, where the subtitle file includes a subtitle text and a playback period corresponding to each field in the subtitle text; and</p><p id="p-0179" num="0178">if the subtitle file matching the to-be-processed audio exists, use the subtitle file as the text information corresponding to the to-be-processed audio; or</p><p id="p-0180" num="0179">if the subtitle file matching the to-be-processed audio does not exist, convert an audio included in the to-be-processed audio into a text, generate a playback period corresponding to each field in the text according to playback time information of an audio segment in the to-be-processed audio, and use the text and the playback period corresponding to each field in the text as the text information corresponding to the to-be-processed audio.</p><p id="p-0181" num="0180">It should be understood that, in this embodiment of the present disclosure, the radio frequency unit <b>501</b> may be configured to receive and send information or a signal in a call process. Alternatively, after receiving downlink data from a base station, the radio frequency unit <b>501</b> sends the downlink data to the processor <b>510</b> for processing. In addition, the radio frequency unit <b>501</b> sends uplink data to the base station. Usually, the radio frequency unit <b>501</b> includes but is not limited to an antenna, at least one amplifier, a transceiver, a coupler, a low noise amplifier, a duplexer, and the like. In addition, the radio frequency unit <b>501</b> may communicate with a network and another device through a wireless communication system.</p><p id="p-0182" num="0181">The electronic device provides wireless broadband Internet access for the user by using the network module <b>502</b>, for example, helping the user to send and receive an e-mail, brows a web page, and access streaming media.</p><p id="p-0183" num="0182">The audio output unit <b>503</b> may convert audio data received by the radio frequency unit <b>501</b> or the network module <b>502</b> or stored in the memory <b>509</b> into an audio signal and output the audio signal as a sound. In addition, the audio output unit <b>503</b> may further provide an audio output (for example, a call signal received voice, or a message received voice) related to a specific function implemented by the electronic device <b>500</b>. The audio output unit <b>503</b> includes a speaker, a buzzer, a telephone receiver, and the like.</p><p id="p-0184" num="0183">The input unit <b>504</b> is configured to receive an audio signal or a video signal. The input unit <b>504</b> may include a graphics processing unit (Graphics Processing Unit, GPU) <b>5041</b> and a microphone <b>5042</b>, and the graphics processing unit <b>5041</b> processes image data of a still picture or video obtained by an image capture apparatus (such as a camera) in a video capture mode or an image capture mode. A processed image frame may be displayed on the display unit <b>506</b>. The image frame processed by the graphics processing unit <b>5041</b> may be stored in the memory <b>509</b> (or another storage medium) or sent by using the radio frequency unit <b>501</b> or the network module <b>502</b>. The microphone <b>5042</b> may receive a sound and can process such sound into audio data. Processed audio data may be converted, in a call mode, into a format that can be sent to a mobile communication base station by using the radio frequency unit <b>501</b> for output.</p><p id="p-0185" num="0184">The electronic device <b>500</b> further includes at least one sensor <b>505</b> such as a light sensor, a motion sensor, and another sensor. Alternatively, the light sensor includes an ambient light sensor and a proximity sensor. The ambient light sensor may adjust luminance of the display panel <b>5061</b> based on brightness of ambient light. The proximity sensor may turn off the display panel <b>5061</b> and/or backlight when the electronic device <b>500</b> moves to an ear. As a type of the motion sensor, an accelerometer sensor may detect an acceleration value in each direction (generally, three axes), and detect a value and a direction of gravity when the accelerometer sensor is static, and may be used for recognizing a posture of the electronic device (such as screen switching between landscape and portrait modes, a related game, or magnetometer posture calibration), a function related to vibration recognition (such as a pedometer or a knock), and the like. The sensor <b>505</b> may further include a fingerprint sensor, a pressure sensor, an iris sensor, a molecular sensor, a gyroscope, a barometer, a hygrometer, a thermometer, an infrared sensor, and the like. Details are not described herein.</p><p id="p-0186" num="0185">The display unit <b>506</b> is configured to display information entered by a user or information provided for a user. The display unit <b>606</b> may include a display panel <b>5061</b>. The display panel <b>5061</b> may be configured in a form of a liquid crystal display (Liquid Crystal Display, LCD), an organic light-emitting diode (Organic Light-Emitting Diode, OLED), or the like.</p><p id="p-0187" num="0186">The user input unit <b>507</b> may be configured to: receive entered digital or character information, and generate key signal input related to a user setting and function control of the electronic device. Alternatively, the user input unit <b>507</b> includes a touch panel <b>5071</b> and another input device <b>5072</b>. The touch panel <b>5071</b> is also referred to as a touchscreen, and may collect a touch operation performed by a user on or near the touch panel <b>5071</b> (such as an operation performed by a user on the touch panel <b>5071</b> or near the touch panel <b>5071</b> by using any proper object or accessory, such as a finger or a stylus). The touch panel <b>5071</b> may include two parts: a touch detection apparatus and a touch controller. The touch detection apparatus detects a touch location of the user, detects a signal brought by the touch operation, and sends the signal to the touch controller. The touch controller receives touch information from the touch detection apparatus, converts the touch information into touch point coordinates, and sends the touch point coordinates to the processor <b>510</b>, and can receive and execute a command sent by the processor <b>510</b>. In addition, the touch panel <b>5071</b> may be of a resistive type, a capacitive type, an infrared type, a surface acoustic wave type, or the like. The user input unit <b>507</b> may include another input device <b>5072</b> in addition to the touch panel <b>5071</b>. Alternatively, the another input device <b>5072</b> may include but is not limited to a physical keyboard, a functional button (such as a volume control button or a power on/off button), a trackball, a mouse, and a joystick. Details are not described herein.</p><p id="p-0188" num="0187">Further, the touch panel <b>5071</b> may cover the display panel <b>5061</b>. When detecting the touch operation on or near the touch panel <b>5071</b>, the touch panel <b>5071</b> transmits the touch operation to the processor <b>510</b> to determine a type of a touch event, and then the processor <b>510</b> provides corresponding visual output on the display panel <b>5061</b> based on the type of the touch event. In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, although the touch panel <b>5071</b> and the display panel <b>5061</b> are used as two independent parts to implement input and output functions of the electronic device, in some embodiments, the touch panel <b>5071</b> and the display panel <b>5061</b> may be integrated to implement the input and output functions of the electronic device. This is not specifically limited herein.</p><p id="p-0189" num="0188">The interface unit <b>508</b> is an interface for connecting an external apparatus with the electronic device <b>500</b>. For example, the external apparatus may include a wired or wireless headphone port, an external power supply (or a battery charger) port, a wired or wireless data port, a storage card port, a port used to connect to an apparatus having an identity module, an audio input/output (I/O) port, a video I/O port, a headset port, and the like. The interface unit <b>508</b> may be configured to receive input (for example, data information and power) from an external apparatus and transmit the received input to one or more elements in the electronic device <b>500</b> or may be configured to transmit data between the electronic device <b>500</b> and an external apparatus.</p><p id="p-0190" num="0189">The memory <b>509</b> may be configured to store a software program and various data. The memory <b>509</b> may mainly include a program storage area and a data storage area. The program storage area may store an operating system, an application required by at least one function (such as a sound play function or an image play function), and the like. The data storage area may store data (such as audio data or an address book) created based on use of the mobile phone, and the like. In addition, the memory <b>509</b> may include a high-speed random access memory, and may further include a nonvolatile memory, for example, at least one magnetic disk storage device, a flash storage device, or another volatile solid-state storage device.</p><p id="p-0191" num="0190">The processor <b>510</b> is a control center of the electronic device, connects all parts of the entire electronic device by using various interfaces and lines, and performs various functions of the electronic device and data processing by running or executing a software program and/or a module that are/is stored in the memory <b>509</b> and by invoking data stored in the memory <b>509</b>, to overall monitor the electronic device. The processor <b>510</b> may include one or more processing units. Preferably, an application processor and a modem processor may be integrated into the processor <b>510</b>. The application processor mainly processes an operating system, a user interface, an application, and the like. The modem processor mainly processes wireless communications. It can be understood that, alternatively, the modem processor may not be integrated into the processor <b>510</b>.</p><p id="p-0192" num="0191">The electronic device <b>500</b> may further include the power supply <b>511</b> (such as a battery) that supplies power to each component. Preferably, the power supply <b>511</b> may be logically connected to the processor <b>510</b> by using a power supply management system, so as to implement functions such as charging management, discharging management, and power consumption management by using the power supply management system.</p><p id="p-0193" num="0192">In addition, the electronic device <b>500</b> includes some function modules not shown, and details are not described herein.</p><p id="p-0194" num="0193">Preferably, an embodiment of the present disclosure further provides an electronic device, including a processor <b>510</b>, a memory <b>509</b>, and an audio processing program that is stored in the memory <b>509</b> and that can be run on the processor <b>510</b>. When the audio processing program is executed by the processor <b>510</b>, the processes of the foregoing audio processing method embodiment are implemented and a same technical effect can be achieved. To avoid repetition, details are not described herein again.</p><p id="p-0195" num="0194">An embodiment of the present disclosure further provides a computer-readable storage medium. The computer-readable storage medium stores an audio processing program, and when the audio processing program is executed by a processor, the processes of the foregoing audio processing method embodiment are implemented and a same technical effect can be achieved. To avoid repetition, details are not described herein again. An example of the computer-readable storage medium includes a non-transitory computer-readable storage medium, such as a read-only memory (Read-only Memory, ROM for short), a random access memory (Random Access Memory, RAM for short), a magnetic disk, or an optical disc.</p><p id="p-0196" num="0195">The foregoing describes the aspects of the present disclosure with reference to flowcharts and/or block diagrams of the method, the apparatus (system), and the computer program product according to the embodiments of the present disclosure. It should be understood that each block in the flowchart and/or block diagram and a combination of blocks in the flowchart and/or block diagram may be implemented by a computer program instruction. These computer program instructions may be provided for a general-purpose computer, a dedicated computer, or a processor of another programmable data processing apparatus to generate a machine, so that when these instructions are executed by the computer or the processor of the another programmable data processing apparatus, specific functions/actions in one or more blocks in the flowcharts and/or in the block diagrams are implemented. The processor may be but is not limited to a general purpose processor, a dedicated processor, a special application processor, or a field programmable logic circuit. It should be further understood that each block in the block diagram or the flowchart and a combination of blocks in the block diagram or the flowchart may be implemented by using dedicated hardware that performs a specified function or operation, or may be implemented by using a combination of dedicated hardware and a computer instruction.</p><p id="p-0197" num="0196">It should be noted that, in this specification, the terms &#x201c;include&#x201d;, &#x201c;comprise&#x201d;, or their any other variant is intended to cover a non-exclusive inclusion, so that a process, a method, an article, or an apparatus that includes a list of elements not only includes those elements but also includes other elements which are not expressly listed, or further includes elements inherent to such process, method, article, or apparatus. An element limited by &#x201c;including a . . . &#x201d; does not, without more constraints, preclude the presence of additional identical elements in the process, method, article, or apparatus that includes the element.</p><p id="p-0198" num="0197">Based on the descriptions of the foregoing implementations, a person skilled in the art may clearly understand that the method in the foregoing embodiment may be implemented by software in addition to a necessary universal hardware platform or by hardware only. In most circumstances, the former is a preferred implementation. Based on such an understanding, the technical solutions of the present disclosure essentially or the part contributing to the prior art may be implemented in a form of a software product. The computer software product is stored in a storage medium (such as a ROM/RAM, a hard disk, or an optical disc), and includes several instructions for instructing a terminal (which may be mobile phone, a computer, a server, an air conditioner, a network device, or the like) to perform the methods described in the embodiments of the present disclosure.</p><p id="p-0199" num="0198">The embodiments of the present disclosure are described above with reference to the accompanying drawings, but the present disclosure is not limited to the above specific implementations, and the above specific implementations are only illustrative and not restrictive. Under the enlightenment of the present disclosure, those of ordinary skill in the art can make many forms without departing from the purpose of the present disclosure and the protection scope of the claims, all of which fall within the protection of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An audio processing method performed by an electronic device, comprising:<claim-text>obtaining text information corresponding to a to-be-processed audio, wherein the text information comprises a to-be-processed text and a playback period corresponding to each field in the to-be-processed text;</claim-text><claim-text>receiving a first input on the to-be-processed text;</claim-text><claim-text>in response to the first input, determining a to-be-processed field in the to-be-processed text according to a field indicated by the first input;</claim-text><claim-text>receiving a second input on the to-be-processed field;</claim-text><claim-text>obtaining a target audio segment according to the second input; and</claim-text><claim-text>modifying an audio segment at a playback period corresponding to the to-be-processed field according to the target audio segment, to obtain a target audio.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the obtaining a target audio segment according to the second input comprises:<claim-text>editing the to-be-processed field according to the second input to obtain a target field, and determining an audio corresponding to the target field as the target audio segment; or</claim-text><claim-text>extracting an audio segment carried in the second input, and determining the audio segment as the target audio segment.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the editing the to-be-processed field according to the second input to obtain a target field comprises:<claim-text>in a case that the second input is a delete input, deleting the to-be-processed field, and determining a blank field obtained after the deletion as the target field;</claim-text><claim-text>in a case that the second input is a replace input, obtaining a to-be-replaced field corresponding to the second input, deleting the to-be-processed field, and adding the to-be-replaced field at a location of the to-be-processed field to obtain the target field; or</claim-text><claim-text>in a case that the second input is an add input, obtaining a to-be-added field corresponding to the second input, and adding the to-be-added field at a location of the to-be-processed field to obtain the target field.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the to-be-processed audio is an audio comprised in a to-be-processed video; and<claim-text>before the receiving a first input on the to-be-processed text, the method further comprises:</claim-text><claim-text>displaying a preset picture, and displaying all to-be-processed texts in the preset picture; or displaying each video picture of the to-be-processed video, and displaying a to-be-processed text corresponding to the video picture in the video picture; and</claim-text><claim-text>the receiving a first input on the to-be-processed text comprises:</claim-text><claim-text>receiving a select input on the displayed to-be-processed text.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the modifying an audio segment at a playback period corresponding to the to-be-processed field according to the target audio segment, to obtain a target audio comprises:<claim-text>obtaining the playback period corresponding to the to-be-processed field from the playback period corresponding to each field;</claim-text><claim-text>obtaining an audio waveform graph corresponding to the to-be-processed audio; and</claim-text><claim-text>modifying a corresponding wave band of the playback period corresponding to the to-be-processed field in the audio waveform graph into an audio wave band corresponding to the target audio segment, to obtain the target audio.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein after the determining a to-be-processed field in the to-be-processed text according to a field indicated by the first input, the method further comprises:<claim-text>receiving a third input on the to-be-processed text;</claim-text><claim-text>in response to the third input, adjusting a font size of a to-be-adjusted field indicated by the third input, to obtain an adjusted to-be-adjusted field; and</claim-text><claim-text>adjusting a volume level of an audio corresponding to the to-be-adjusted field according to a font size of the adjusted to-be-adjusted field, wherein a larger font of the adjusted to-be-adjusted field indicates a larger volume of the audio corresponding to the to-be-adjusted field.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the obtaining text information corresponding to a to-be-processed audio comprises:<claim-text>detecting whether a subtitle file matching the to-be-processed audio exists, wherein the subtitle file comprises a subtitle text and a playback period corresponding to each field in the subtitle text; and</claim-text><claim-text>in a case that the subtitle file matching the to-be-processed audio exists, using the subtitle file as the text information corresponding to the to-be-processed audio; or</claim-text><claim-text>in a case that the subtitle file matching the to-be-processed audio does not exist, converting an audio comprised in the to-be-processed audio into a text, generating a playback period corresponding to each field in the text according to playback time information of an audio segment in the to-be-processed audio, and using the text and the playback period corresponding to each field in the text as the text information corresponding to the to-be-processed audio.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. An electronic device, comprising:<claim-text>a processor; and</claim-text><claim-text>a memory storing an audio processing program that is executable by the processor, wherein the audio processing program when executed by the processor, causes the electronic device to perform the following steps:</claim-text><claim-text>obtaining text information corresponding to a to-be-processed audio, wherein the text information comprises a to-be-processed text and a playback period corresponding to each field in the to-be-processed text;</claim-text><claim-text>receiving a first input on the to-be-processed text;</claim-text><claim-text>in response to the first input, determining a to-be-processed field in the to-be-processed text according to a field indicated by the first input;</claim-text><claim-text>receiving a second input on the to-be-processed field;</claim-text><claim-text>obtaining a target audio segment according to the second input; and</claim-text><claim-text>modifying an audio segment at a playback period corresponding to the to-be-processed field according to the target audio segment, to obtain a target audio.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The electronic device according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the obtaining a target audio segment according to the second input comprises:<claim-text>editing the to-be-processed field according to the second input to obtain a target field, and determining an audio corresponding to the target field as the target audio segment; or</claim-text><claim-text>extracting an audio segment carried in the second input, and determining the audio segment as the target audio segment.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The electronic device according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the editing the to-be-processed field according to the second input to obtain a target field comprises:<claim-text>in a case that the second input is a delete input, deleting the to-be-processed field, and determining a blank field obtained after the deletion as the target field;</claim-text><claim-text>in a case that the second input is a replace input, obtaining a to-be-replaced field corresponding to the second input, deleting the to-be-processed field, and adding the to-be-replaced field at a location of the to-be-processed field to obtain the target field; or</claim-text><claim-text>in a case that the second input is an add input, obtaining a to-be-added field corresponding to the second input, and adding the to-be-added field at a location of the to-be-processed field to obtain the target field.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The electronic device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the to-be-processed audio is an audio comprised in a to-be-processed video; and<claim-text>before the receiving a first input on the to-be-processed text, the method further comprises:</claim-text><claim-text>displaying a preset picture, and displaying all to-be-processed texts in the preset picture; or displaying each video picture of the to-be-processed video, and displaying a to-be-processed text corresponding to the video picture in the video picture; and</claim-text><claim-text>the receiving a first input on the to-be-processed text comprises:</claim-text><claim-text>receiving a select input on the displayed to-be-processed text.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The electronic device according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the modifying an audio segment at a playback period corresponding to the to-be-processed field according to the target audio segment, to obtain a target audio comprises:<claim-text>obtaining the playback period corresponding to the to-be-processed field from the playback period corresponding to each field;</claim-text><claim-text>obtaining an audio waveform graph corresponding to the to-be-processed audio; and</claim-text><claim-text>modifying a corresponding wave band of the playback period corresponding to the to-be-processed field in the audio waveform graph into an audio wave band corresponding to the target audio segment, to obtain the target audio.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The electronic device according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein after the determining a to-be-processed field in the to-be-processed text according to a field indicated by the first input, the method further comprises:<claim-text>receiving a third input on the to-be-processed text;</claim-text><claim-text>in response to the third input, adjusting a font size of a to-be-adjusted field indicated by the third input, to obtain an adjusted to-be-adjusted field; and</claim-text><claim-text>adjusting a volume level of an audio corresponding to the to-be-adjusted field according to a font size of the adjusted to-be-adjusted field, wherein a larger font of the adjusted to-be-adjusted field indicates a larger volume of the audio corresponding to the to-be-adjusted field.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The electronic device according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the obtaining text information corresponding to a to-be-processed audio comprises:<claim-text>detecting whether a subtitle file matching the to-be-processed audio exists, wherein the subtitle file comprises a subtitle text and a playback period corresponding to each field in the subtitle text; and</claim-text><claim-text>in a case that the subtitle file matching the to-be-processed audio exists, using the subtitle file as the text information corresponding to the to-be-processed audio; or</claim-text><claim-text>in a case that the subtitle file matching the to-be-processed audio does not exist, converting an audio comprised in the to-be-processed audio into a text, generating a playback period corresponding to each field in the text according to playback time information of an audio segment in the to-be-processed audio, and using the text and the playback period corresponding to each field in the text as the text information corresponding to the to-be-processed audio.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory computer-readable storage medium storing an audio processing program, wherein the audio processing program, when executed by a processor, performs the following steps:<claim-text>obtaining text information corresponding to a to-be-processed audio, wherein the text information comprises a to-be-processed text and a playback period corresponding to each field in the to-be-processed text;</claim-text><claim-text>receiving a first input on the to-be-processed text;</claim-text><claim-text>in response to the first input, determining a to-be-processed field in the to-be-processed text according to a field indicated by the first input;</claim-text><claim-text>receiving a second input on the to-be-processed field;</claim-text><claim-text>obtaining a target audio segment according to the second input; and</claim-text><claim-text>modifying an audio segment at a playback period corresponding to the to-be-processed field according to the target audio segment, to obtain a target audio.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the obtaining a target audio segment according to the second input comprises:<claim-text>editing the to-be-processed field according to the second input to obtain a target field, and determining an audio corresponding to the target field as the target audio segment; or</claim-text><claim-text>extracting an audio segment carried in the second input, and determining the audio segment as the target audio segment.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the editing the to-be-processed field according to the second input to obtain a target field comprises:<claim-text>in a case that the second input is a delete input, deleting the to-be-processed field, and determining a blank field obtained after the deletion as the target field;</claim-text><claim-text>in a case that the second input is a replace input, obtaining a to-be-replaced field corresponding to the second input, deleting the to-be-processed field, and adding the to-be-replaced field at a location of the to-be-processed field to obtain the target field; or</claim-text><claim-text>in a case that the second input is an add input, obtaining a to-be-added field corresponding to the second input, and adding the to-be-added field at a location of the to-be-processed field to obtain the target field.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the to-be-processed audio is an audio comprised in a to-be-processed video; and<claim-text>before the receiving a first input on the to-be-processed text, the method further comprises:</claim-text><claim-text>displaying a preset picture, and displaying all to-be-processed texts in the preset picture; or displaying each video picture of the to-be-processed video, and displaying a to-be-processed text corresponding to the video picture in the video picture; and</claim-text><claim-text>the receiving a first input on the to-be-processed text comprises:</claim-text><claim-text>receiving a select input on the displayed to-be-processed text.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the modifying an audio segment at a playback period corresponding to the to-be-processed field according to the target audio segment, to obtain a target audio comprises:<claim-text>obtaining the playback period corresponding to the to-be-processed field from the playback period corresponding to each field;</claim-text><claim-text>obtaining an audio waveform graph corresponding to the to-be-processed audio; and</claim-text><claim-text>modifying a corresponding wave band of the playback period corresponding to the to-be-processed field in the audio waveform graph into an audio wave band corresponding to the target audio segment, to obtain the target audio.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein after the determining a to-be-processed field in the to-be-processed text according to a field indicated by the first input, the method further comprises:<claim-text>receiving a third input on the to-be-processed text;</claim-text><claim-text>in response to the third input, adjusting a font size of a to-be-adjusted field indicated by the third input, to obtain an adjusted to-be-adjusted field; and</claim-text><claim-text>adjusting a volume level of an audio corresponding to the to-be-adjusted field according to a font size of the adjusted to-be-adjusted field, wherein a larger font of the adjusted to-be-adjusted field indicates a larger volume of the audio corresponding to the to-be-adjusted field.</claim-text></claim-text></claim></claims></us-patent-application>