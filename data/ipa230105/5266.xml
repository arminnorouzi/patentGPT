<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005267A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005267</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17744727</doc-number><date>20220516</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2021-108655</doc-number><date>20210630</date></priority-claim><priority-claim sequence="02" kind="national"><country>JP</country><doc-number>2022-025140</doc-number><date>20220221</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>52</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>Q</subclass><main-group>20</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>7</main-group><subgroup>14</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>52</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>Q</subclass><main-group>20</main-group><subgroup>4016</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>7</main-group><subgroup>1413</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30196</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10016</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>08</class><subclass>B</subclass><main-group>13</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e79">COMPUTER-READABLE RECORDING MEDIUM, FRAUD DETECTION METHOD, AND FRAUD DETECTION APPARATUS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>FUJITSU LIMITED</orgname><address><city>Kawasaki-shi</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>SUZUKI</last-name><first-name>Genta</first-name><address><city>Kawasaki</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>IWASAKI</last-name><first-name>Sho</first-name><address><city>Yokohama</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>SAITO</last-name><first-name>Takahiro</first-name><address><city>Asaka</city><country>JP</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>JO</last-name><first-name>Yuka</first-name><address><city>Kawasaki</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>FUJITSU LIMITED</orgname><role>03</role><address><city>Kawasaki-shi</city><country>JP</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An information processing program causes a computer to execute a process including: specifying, from an image that is captured by a camera, a person and a plurality of objects, generating, by inputting the image of the person into a machine learning model, skeleton information on the person, identifying, based on the plurality of objects and the skeleton information, a first feature value associated with one or more first motions of the person who retrieves an object from among the plurality of objects, identifying a second feature value associated with one or more objects registered to a first terminal by the person from among the plurality of object, and generating, based on a difference between the first feature value and the second feature value, an alert indicates that an object retrieved by the person is not registered in the first terminal.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="54.36mm" wi="158.75mm" file="US20230005267A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="133.18mm" wi="154.60mm" file="US20230005267A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="239.27mm" wi="83.06mm" orientation="landscape" file="US20230005267A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="238.93mm" wi="83.06mm" orientation="landscape" file="US20230005267A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="146.90mm" wi="160.19mm" file="US20230005267A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="149.94mm" wi="120.90mm" file="US20230005267A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="81.70mm" wi="143.93mm" file="US20230005267A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="250.70mm" wi="100.75mm" orientation="landscape" file="US20230005267A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="166.88mm" wi="165.78mm" file="US20230005267A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="237.83mm" wi="95.33mm" orientation="landscape" file="US20230005267A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="251.04mm" wi="151.13mm" file="US20230005267A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="180.34mm" wi="164.93mm" file="US20230005267A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="88.73mm" wi="113.79mm" file="US20230005267A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="133.35mm" wi="154.60mm" file="US20230005267A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="254.51mm" wi="166.37mm" orientation="landscape" file="US20230005267A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="180.59mm" wi="120.90mm" file="US20230005267A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="210.65mm" wi="166.12mm" file="US20230005267A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="244.77mm" wi="138.26mm" orientation="landscape" file="US20230005267A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="252.90mm" wi="144.95mm" orientation="landscape" file="US20230005267A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="133.52mm" wi="120.06mm" file="US20230005267A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="246.30mm" wi="128.86mm" file="US20230005267A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="215.31mm" wi="161.12mm" file="US20230005267A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is based upon and claims the benefit of priority of the prior Japanese Patent Application No. 2021-108655, filed on Jun. 30, 2021, and Japanese Patent Application No. 2022-025140, filed on Feb. 21, 2022, the entire contents of which are incorporated herein by reference.</p><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">The embodiments discussed herein are related to a computer-readable recording medium, a fraud detection method, and a fraud detection apparatus.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">In retail stores, in order to prevent cash registers from being crowded, introduction of a system in which customers scan and register commodity products and perform checkout themselves is being facilitated. Furthermore, in recent years, introduction of a system in which customers scan commodity products at a place other than cash registers, for example, at a place of a sales floor where each of the commodity products is picked up by the customers by using an application installed in a terminal lent inside stores of the retail stores has been started. In such a system for customers to scan commodity products themselves, there is a need to detect a scan omission of a commodity product caused by a fraudulent act, such as shoplifting, or an erroneous operation.</p><p id="p-0005" num="0004">In contrast, as a system for detecting a fraudulent behavior of a customer at retail stores, a system for detecting a suspicious behavior or a fraudulent act, such as shoplifting, of a customer by using, for example, a monitoring camera placed in a store has been developed.</p><p id="p-0006" num="0005">Patent Document 1: Japanese Laid-open Patent Publication No. 2019-193089</p><p id="p-0007" num="0006">Patent Document 2: U.S. Patent Application Publication No. 2017/0046707</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0008" num="0007">According to an aspect of an embodiment, a non-transitory computer-readable recording medium stores therein a fraud detection program that causes a computer to execute a process including specifying, from an image that is captured by a camera, a person and a plurality of objects, generating, by inputting the image of the person into a machine learning model, skeleton information on the person, identifying, based on the plurality of objects and the skeleton information, a first feature value associated with one or more first motions of the person who retrieves an object from among the plurality of objects, identifying a second feature value associated with one or more objects registered to a first terminal by the person from among the plurality of object, and generating, based on a difference between the first feature value and the second feature value, an alert indicates that an object retrieved by the person is not registered in the first terminal.</p><p id="p-0009" num="0008">The object and advantages of the invention will be realized and attained by means of the elements and combinations particularly pointed out in the claims.</p><p id="p-0010" num="0009">It is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory and are not restrictive of the invention, as claimed.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating a configuration example of a fraud detection system <b>1</b> according to a first embodiment;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating an example of a purchase of commodity products made by performing self scanning according to the first embodiment;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating another example of the purchase of commodity products made by performing self scanning according to the first embodiment;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating an example of scan omission detection obtained from a captured image according to the first embodiment;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a configuration example of a fraud detection apparatus <b>10</b> according to the first embodiment;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram illustrating an example of data stored in a motion history <b>33</b> according to the first embodiment;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram illustrating an example of a scan omission detection process according to the first embodiment;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram illustrating an example of object detection and skeleton detection according to the first embodiment;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram illustrating an example of specifying a motion according to the first embodiment;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart illustrating the flow of the scan omission detection process according to the first embodiment;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart illustrating the flow of a target person determination process according to the first embodiment; and</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating a hardware configuration example of the fraud detection apparatus <b>10</b>.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram illustrating a configuration example of a fraud detection system <b>2</b> according to a second embodiment;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram illustrating an example of scan omission detection according to the second embodiment;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a diagram illustrating a configuration example of the fraud detection apparatus <b>10</b> according to the second embodiment;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a diagram illustrating an example of data stored in the motion history <b>33</b> according to the second embodiment;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a diagram illustrating an example of data stored in commodity product registration information <b>34</b> according to the second embodiment;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram illustrating an example of scan omission detection according to the second embodiment;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a diagram illustrating an example of specifying a commodity product acquisition/return motion according to the second embodiment;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a flowchart illustrating the flow of a commodity product acquisition/return motion specifying process according to the second embodiment;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a flowchart illustrating the flow of a scan omission detection process according to the second embodiment;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a diagram illustrating an example of a hardware configuration of a user terminal <b>100</b>; and</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a diagram illustrating an example of a hardware configuration according to a self-service checkout terminal <b>400</b>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0034" num="0033">However, in a system for customers to scan commodity products themselves, a commodity product scan need not always be performed at sales floor of each of the commodity products, but may be performed at any location before calculation of a payment amount of commodity products to be purchased is performed. Accordingly, in some cases, it is not possible to or it is difficult to detect a scan omission of a commodity product under the condition in which the commodity product scan is allowed to be performed at any location.</p><p id="p-0035" num="0034">Preferred embodiments of the present invention will be explained with reference to accompanying drawings. Furthermore, the present embodiment is not limited by the embodiments. In addition, each of the embodiments can be used in any appropriate combination as long as processes do not conflict with each other.</p><heading id="h-0007" level="1">[a] First Embodiment</heading><p id="p-0036" num="0035">First, a fraud detection system for implementing the present embodiment will be described. <figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating a configuration example of the fraud detection system according to the first embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a fraud detection system <b>1</b> is a system in which the fraud detection apparatus <b>10</b>, and user terminals <b>100</b>-<b>1</b> to <b>100</b>-<i>n </i>(n is any integer. Hereinafter, collectively referred to as a &#x201c;user terminal <b>100</b>&#x201d;) are connected via a network <b>50</b> so as to be communicated with each other.</p><p id="p-0037" num="0036">Furthermore, the fraud detection apparatus <b>10</b> is also connected to camera devices <b>200</b>-<b>1</b> to <b>200</b>-<i>m </i>(m is any integer. Hereinafter, collectively referred to as a &#x201c;camera device <b>200</b>&#x201d;) and a store clerk terminal <b>300</b> via the network <b>50</b> so as to be communicated with each other.</p><p id="p-0038" num="0037">Various kinds of communication network, such as an intranet, that is used inside, for example, a store of a retail store may be used for the network <b>50</b> irrespective of a wired or wireless manner. Furthermore, instead of a single network, the network <b>50</b> may be constituted of, for example, an intranet and the Internet by way of a network device, such as a gateway, or another device (not illustrated).</p><p id="p-0039" num="0038">The fraud detection apparatus <b>10</b> is an information processing apparatus, such as a desktop personal computer (PC), a notebook PC, or a server computer, that is installed, for example, inside the store of the retail store and that is used by store staff, an administrator, or the like.</p><p id="p-0040" num="0039">The fraud detection apparatus <b>10</b> receives, from the camera device <b>200</b>, a plurality of images obtained by capturing, by the camera device <b>200</b>, a predetermined image capturing range, such as the inside of the store or the site of the retail store. Furthermore, the plurality of images mentioned here are, in a precise sense, video images captured by the camera device <b>200</b>, that is, a series of frames of a moving image.</p><p id="p-0041" num="0040">Furthermore, the fraud detection apparatus <b>10</b> specifies, from a captured image by using an existing object detecting technique, a customer who visits the store (hereinafter, sometimes simply referred to as a &#x201c;person&#x201d;), a shopping basket carried by the person (hereinafter, sometimes simply referred to as a &#x201c;basket&#x201d;), or the user terminal <b>100</b>. Furthermore, the fraud detection apparatus <b>10</b> generates, from the captured image by using an existing skeleton detection technique, skeleton information on the specified person, estimates a pose of the person, and specifies a motion of putting a commodity product into the basket or a motion of registering a commodity product to the user terminal <b>100</b>.</p><p id="p-0042" num="0041">Furthermore, the fraud detection apparatus <b>10</b> counts the number of motions of putting a commodity product into a basket and the number of motions of registering a commodity product to the user terminal <b>100</b>, and then, evaluates a behavior of the person exhibiting with respect to a purchase of the commodity product. Then, if the fraud detection apparatus <b>10</b> detects a fraudulent act, such as a scan omission of a commodity product, performed by the person, the fraud detection apparatus <b>10</b> notifies the store clerk terminal <b>300</b> of an alert.</p><p id="p-0043" num="0042">Furthermore, <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates the fraud detection apparatus <b>10</b> as a single computer; however, the fraud detection apparatus <b>10</b> may be a distributed computing system constituted by a plurality of computers.</p><p id="p-0044" num="0043">Furthermore, the fraud detection apparatus <b>10</b> may be a cloud computer device managed by a service provider that provides a cloud computing service.</p><p id="p-0045" num="0044">The user terminal <b>100</b> is an information processing terminal that is used by each of the customers to scan a barcode of a commodity product by themselves in order to purchase the commodity product and register the purchased commodity product. The user terminal <b>100</b> may be a mobile terminal, such as a smartphone or a tablet personal computer (PC), owned by the customer, or a dedicated terminal that is lent inside the store. The user terminal <b>100</b> has, installed therein in advance, an application for, for example, scanning and registering commodity products.</p><p id="p-0046" num="0045">The camera device <b>200</b> is a monitoring camera installed, for example, the inside of the store or the site of the retail store. Furthermore, <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a plurality of the camera devices <b>200</b>; however, for example, the number of the camera devices <b>200</b> may be one in a case of a small store or the like. A video image captured by the camera device <b>200</b> is transmitted to the fraud detection apparatus <b>10</b>.</p><p id="p-0047" num="0046">The store clerk terminal <b>300</b> may be a mobile terminal, such as a smartphone or a tablet PC, carried by a store clerk of the retail store, or may be an information processing apparatus, such as a desktop PC or a notebook PC, installed at a predetermined position disposed inside the store. The store clerk terminal <b>300</b> receives an alert from the fraud detection apparatus <b>10</b> in the case where a fraudulent act or an erroneous operation, such as a scan omission of a commodity product, performed by a person is detected. Furthermore, a plurality of number of the store clerk terminal <b>300</b> may be present for each store, but the terminal that receives a notification of that alert may be limited to the terminal that is carried by, for example, a store clerk responsible for security positioned in the vicinity of an exit.</p><p id="p-0048" num="0047">In the following, a method in which a customer purchases commodity products by scanning and registering commodity products by himself or herself (hereinafter, sometimes referred to as &#x201c;self scanning&#x201d;) will be described with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref> and <figref idref="DRAWINGS">FIG. <b>3</b></figref>. <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating an example of a purchase of commodity products made by performing self scanning according to the first embodiment.</p><p id="p-0049" num="0048">As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, first, a customer selects a visited store through an application displayed on the user terminal <b>100</b>. Then, the customer picks up commodity products to be purchased and reads, for example, a barcode of each of the commodity products, a barcode attached to a commodity product shelf for each of the commodity products, or the like by using the user terminal <b>100</b> (hereinafter, sometimes referred to as a &#x201c;commodity product scan&#x201d;). As a result, the commodity products to be purchased are registered in the application. Furthermore, the commodity product scan need not always be performed on each of the sales floors, but may be performed at any location and any timing.</p><p id="p-0050" num="0049">Then, the customer scans a payment code displayed on a display unit of, for example, the self-service checkout terminal <b>400</b>. Then, by making payment of an amount displayed on a payment screen of the self-service checkout terminal <b>400</b>, the purchase of the commodity products has been completed. Furthermore, the customer is able to exit the store by causing a gate reader <b>500</b> or the like installed at the exit of the store or the like to read a payment completion code displayed on the user terminal <b>100</b>. Furthermore, although not illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the self-service checkout terminal <b>400</b> and the gate reader <b>500</b> are connected to the fraud detection apparatus <b>10</b> via the network <b>50</b> so as to be able to communicate with each other.</p><p id="p-0051" num="0050">In the following, another example of a purchase of commodity products made by performing self scanning will be described. <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating another example of a purchase commodity products made by performing self scanning according to the first embodiment.</p><p id="p-0052" num="0051">As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, first, a customer logs in via the application displayed on the user terminal <b>100</b> and selects the visited store. Then, the customer picks up commodity products to be purchased and reads, for example, a barcode of each of the commodity products to be purchased, a barcode attached to a commodity product shelf for each of the commodity products, or the like by using the user terminal <b>100</b>. Furthermore, in also the case illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, similarly to the case illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the commodity product scan may be performed at any location and any timing.</p><p id="p-0053" num="0052">Then, the customer places a basket containing therein the commodity products to be purchased at a check point disposed inside the store, and pays a bill of the commodity products to be purchased by pressing an &#x201c;payment button&#x201d;, a &#x201c;purchase button&#x201d;, or the like displayed on the user terminal <b>100</b>. Furthermore, the payment of the commodity products to be purchased is able to be performed by using electronic money, a credit card, or the like through the application displayed on the user terminal <b>100</b>. Then, the customer is able to exit the store by causing the gate reader <b>500</b> or the like installed at the exit of the store or the like to read a settlement completion code displayed on the user terminal <b>100</b>.</p><p id="p-0054" num="0053">In the above, a purchase of commodity products made by performing self scanning has been described with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref> and <figref idref="DRAWINGS">FIG. <b>3</b></figref>; however, in self scanning, a customer is able to perform a commodity product scan at any location. Accordingly, in self scanning, for example, a customer is able to put commodity products into a basket without performing a commodity product scan and is able to avoid a payment without passing through a self-service checkout counter. Alternatively, by scanning only some of the commodity products to be purchased and paying the bill of only the scanned commodity products at the self-service checkout counter, a customer is able to avoid a payment of the other commodity products. In particular, a fraudulent behavior tends to be easily found by a store clerk or the like in the case where the number of items of the commodity products is small; however, for example, it is difficult to find a fraudulent act by a store clerk or the like in the case where the number of items of the commodity products is large and some of the commodity products are not scanned.</p><p id="p-0055" num="0054">Furthermore, in the case where a fraudulent act is detected, the following problem may further occur. For example, not all customers perform a commodity product scan by using the user terminal <b>100</b>. That is, in some cases, some customers collectively pay a bill at a self-service checkout counter as in the past or some customers pay a bill assisted by a store clerk using a cash register. In other words, a person who has not performed commodity product scan is not always a person who exhibits a fraudulent act. Accordingly, for example, just detecting a person who has not performed a commodity product scan from a video image captured by the camera device <b>200</b> will result in detection of a customer who makes a payment at an ordinary cash register, so that it is difficult to accurately specify a person who exhibits a fraudulent act.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating an example of scan omission detection from a captured image according to the first embodiment. <figref idref="DRAWINGS">FIG. <b>4</b></figref> is an example of an image of the inside of the store captured by the camera device <b>200</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, target persons for self scanning <b>150</b>-<b>1</b> to <b>150</b>-<b>3</b> who make a purchase of commodity products by performing self scanning using their user terminals <b>100</b> and target persons for normal cash register <b>160</b>-<b>1</b> to <b>160</b>-<b>3</b> who make conventional purchase of commodity products without performing self scanning may be present inside the store.</p><p id="p-0057" num="0056">For example, in the example illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, it is assumed that only the target person for self scanning <b>150</b>-<b>1</b> is a target person for a scan omission who has not intentionally performed self scanning. Therefore, as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, detecting only the target person for self scanning <b>150</b>-<b>1</b> as a target person for a scan omission is the correct answer for the scan omission detection.</p><p id="p-0058" num="0057">However, in the case where the fraud detection apparatus <b>10</b> determines that a person who does not made the motion of registering a commodity product to the user terminal <b>100</b> is a target person for a scan omission, the fraud detection apparatus <b>10</b> simply and erroneously detects the target person for normal cash register <b>160</b>-<b>1</b> to <b>160</b>-<b>3</b> who do not need to make this motion as target persons for the scan omission.</p><p id="p-0059" num="0058">Furthermore, in the case where a person who has not performed a commodity product scan is detected from a video image obtained by the camera device <b>200</b>, an amount of information transmitted to the fraud detection apparatus <b>10</b> or an amount of information to be processed is increased, so that a problem that the processing load will be increased may possibly occur. Thus, one of objects of the present embodiment is to solve this problem and to detect a scan omission caused by self scanning performed at the time of purchase of commodity products.</p><p id="p-0060" num="0059">Functional Configuration of Fraud Detection Apparatus <b>10</b></p><p id="p-0061" num="0060">In the following, a functional configuration of the fraud detection apparatus <b>10</b> will be described. <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating a configuration example of the fraud detection apparatus <b>10</b> according to the first embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the fraud detection apparatus <b>10</b> includes a communication unit <b>20</b>, a storage unit <b>30</b>, and a control unit <b>40</b>.</p><p id="p-0062" num="0061">The communication unit <b>20</b> is a processing unit that controls communication with another device, such as the user terminal <b>100</b> or the camera device <b>200</b>, and is, for example, a communication interface, such as a universal serial bus (USB) interface or a network interface card.</p><p id="p-0063" num="0062">The storage unit <b>30</b> has a function for storing various kinds of data and a program executed by the control unit <b>40</b> and is implemented by, for example, a storage device, such as a memory or a hard disk. The storage unit <b>30</b> stores therein an image DB <b>31</b>, skeleton information <b>32</b>, the motion history <b>33</b>, and the like. Furthermore, the DB is an abbreviation of a database.</p><p id="p-0064" num="0063">The image DB <b>31</b> stores therein a plurality of captured images that are a series of frames captured by the camera device <b>200</b>. Furthermore, the image DB <b>31</b> is able to store therein positional information on a person or an object that is included in an image and that is specified with respect to the subject captured image.</p><p id="p-0065" num="0064">The skeleton information <b>32</b> stores therein skeleton information on the person specified from the captured image that is captured by the camera device <b>200</b>. A process of generating the skeleton information will be described later.</p><p id="p-0066" num="0065">The motion history <b>33</b> stores therein the number of motions of putting a commodity product into a basket or the number of motions of registering a commodity product into the user terminal <b>100</b> specified on the basis of skeleton information or the like. <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram illustrating an example of data stored in the motion history <b>33</b> according to the first embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the motion history <b>33</b> stores therein, in an associated manner, &#x201c;personal ID&#x201d; that is an identifier for uniquely identifying a person, &#x201c;put-into-basket motion count&#x201d; that is the number of motions of putting a commodity product into a basket, and &#x201c;commodity product registration motion count&#x201d; that is the number of motions of registering a commodity product into the user terminal <b>100</b>.</p><p id="p-0067" num="0066">Furthermore, the above described information stored in the storage unit <b>30</b> is only an example, and the storage unit <b>30</b> is able to store various kinds of information other than the information described above.</p><p id="p-0068" num="0067">The control unit <b>40</b> is a processing unit that manages the entirety of the fraud detection apparatus <b>10</b> and is, for example, a processor. The control unit <b>40</b> includes a specifying unit <b>41</b>, a generating unit <b>42</b>, an evaluating unit <b>43</b>, and a notifying unit <b>44</b>. Moreover, each of the processing units is an example of an electronic circuit included by the processor or an example of a process executed by the processor.</p><p id="p-0069" num="0068">The specifying unit <b>41</b> specifies, from the captured image captured by the camera device <b>200</b>, a person visiting a store and an object that is being used by the subject person. Moreover, a process of specifying the person may include a process of tracking, from the captured images that are captured at different time, the same person at different time on the basis of the appearance and an amount of movement of the person.</p><p id="p-0070" num="0069">Furthermore, the specifying unit <b>41</b> specifies, on the basis of the specified object and the skeleton information that is generated by the generating unit <b>42</b>, a first motion of a person putting a commodity product sold at a store into a basket and a second motion of the person registering the commodity product targeted for the purchase to the user terminal <b>100</b>.</p><p id="p-0071" num="0070">A process of specifying the first motion of putting a commodity product into a basket may include a process of specifying, on the basis of the specified object and the skeleton information, the first motion in the case where, for example, the fingers of the person have come out from the region of the specified basket after the fingers entered the region for a predetermined time period. Furthermore, a process of specifying the second motion of registering the commodity product targeted for the purchase may include a process of specifying, on the basis of the specified object and the skeleton information, the second motion in the case where both elbows of the person have not moved for a predetermined period of time while being bent forward within a predetermined range of the region of the specified basket.</p><p id="p-0072" num="0071">The generating unit <b>42</b> generates skeleton information on the person specified by the specifying unit <b>41</b> from the captured image that is captured by the camera device <b>200</b>.</p><p id="p-0073" num="0072">The evaluating unit <b>43</b> counts a first count that is the number of the first motions of the person putting the commodity product sold at the store into the basket. Furthermore, the evaluating unit <b>43</b> counts a second count that is the number of the second motions of the person registering the commodity product targeted for the purchase to the user terminal <b>100</b>. Furthermore, the evaluating unit <b>43</b> evaluates, on the basis of the first count and the second count, a behavior of the person exhibiting with respect to the purchase of a commodity product. Here, a process of evaluating the behavior may include a process of determining that the person has behaved fraudulently or performed an erroneous operation in the case where a difference between the first count and the second count is greater than or equal to the first threshold and in the case where the first count is greater than or equal to the second threshold.</p><p id="p-0074" num="0073">Here, the state in which the difference between the first count and the second count is greater than or equal to the first threshold indicates that there is a comparatively large discrepancy between the number of motions of putting the commodity product into the basket and the number of motions of registering the commodity product. Accordingly, the evaluating unit <b>43</b> is able to determine that a scan omission of the commodity product occurs. Furthermore, the determination that the first count is greater than or equal to the second threshold is performed in order to determine whether or not the number of items of the commodity products that have been put into the basket is greater than or equal to a certain number. This is because, as described above, a fraudulent act performed in the case where the number of items of the commodity products is small is easily found by a store clerk but is difficult to be found by a store clerk or the like in the case where the number of items of the commodity products is large.</p><p id="p-0075" num="0074">Furthermore, even when the difference between the first count and the second count is less than the first threshold, if a person has been specified from an image having captured therein a sales floor of a high-priced commodity product and if the difference is greater than or equal to a third threshold, a process of determining that the person has behaved fraudulently or performed an erroneous operation may be included. Here, the third threshold may be greater than zero and less than the first threshold. This determination is performed to more reliably detect a scan omission of the high-priced commodity product even when the discrepancy between the number of motions of putting the commodity product into the basket and the number of motions of registering the commodity product is small.</p><p id="p-0076" num="0075">Furthermore, the process of counting the first count and the second count performed by the evaluating unit <b>43</b> and the process of evaluating a behavior may be performed in the case where the second motion is not specified after the first motion has been specified. Alternatively, each of the processes performed by the evaluating unit <b>43</b> may be performed at the time of calculation of a payment amount of the commodity products. Here, at the time of calculation of the payment amount of the commodity products mentioned here may be, for example, as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, at the time of a scan of a payment code displayed on a display unit, such as the self-service checkout terminal <b>400</b>. Alternatively, at the time of calculation of a payment amount of the commodity products may be, for example, as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, at the time of payment of a bill of the purchased commodity products.</p><p id="p-0077" num="0076">Furthermore, the evaluating unit <b>43</b> is able to check information on the commodity product targeted for a purchase registered by the first terminal against the specified second motion, and is able to associate the information with the second motion. As a result, it is possible to detect a scan omission of a commodity product in more detail. Furthermore, the information on the commodity product targeted for the purchase may be transmitted from the first terminal to the fraud detection apparatus <b>10</b> for, for example, each set of registered commodity product or transmitted at a predetermined timing. Furthermore, associating the information on the commodity product targeted for the purchase with the specified second motion may be performed by associating, for example, the time or the location, at which the information on the commodity product targeted for the purchase has been registered, that is matched with the time or the location, at which the second motion has been specified, with an item originated from a motion made by the same person.</p><p id="p-0078" num="0077">If it is determined that a person has behaved fraudulently or performed an erroneous operation on the basis of the evaluation of a behavior obtained by the evaluating unit <b>43</b>, the notifying unit <b>44</b> notifies the store clerk terminal <b>300</b> of an alert. Furthermore, the notification of the alert may be an output of a message, a sound, or the like. In addition, the notifying unit <b>44</b> is able to transmit, together with the notification of the alert, information for specifying a person who has behaved fraudulently or performed an erroneous operation, such as a captured image of, for example, the captured person who has behaved fraudulently or performed an erroneous operation, to the store clerk terminal <b>300</b>. Furthermore, in the case where a plurality of persons are captured in the captured image, the notifying unit <b>44</b> may process the captured image by, for example, surrounding the person who has behaved fraudulently or performed an erroneous operation with a frame such that the person who has behaved fraudulently or performed an erroneous operation is easily specified.</p><p id="p-0079" num="0078">Details of Functions</p><p id="p-0080" num="0079">In the following, a scan omission detection process according to the first embodiment performed by the fraud detection apparatus <b>10</b> functioning as an actor will be described with reference to <figref idref="DRAWINGS">FIGS. <b>7</b> to <b>9</b></figref>. <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram illustrating an example of a scan omission detection process according to the first embodiment. A captured image <b>250</b> illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref> is an example of a captured image having captured therein the inside of a store of a retail store captured by the camera device <b>200</b>. The fraud detection apparatus <b>10</b> specifies a person or an object from the captured image <b>250</b>. Specifying an object from the captured image <b>250</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram illustrating an example of object detection and skeleton detection according to the first embodiment. As illustrated on the upper right part of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the fraud detection apparatus <b>10</b> detects and specifies persons and baskets from a captured image <b>251</b> by using, for example, an existing object detection algorithm. The existing object detection algorithm mentioned here is, for example, an object detection algorithm using deep training, such as Faster Convolutional Neural Network (R-CNN). Furthermore, the existing object detection algorithm may be an object detection algorithm, such as You Only Look Once (YOLO) or Single Shot Multibox Detector (SSD).</p><p id="p-0082" num="0081">Furthermore, other than the persons or the baskets, for example, commodity products, the user terminal <b>100</b>, or clothes of a person may be detected from the captured image. As a result, the fraud detection apparatus <b>10</b> is able to detect a state in which, for example, a person does not perform self scanning even though the person has the user terminal <b>100</b>. Furthermore, the fraud detection apparatus <b>10</b> is able to exclude, for example, a person who wears a uniform of a store clerk from the target of the scan omission detection process.</p><p id="p-0083" num="0082">Furthermore, the fraud detection apparatus <b>10</b> is able to determine an age of the person specified from the captured image by using, for example, an existing algorithm, and specify a group relationship, such as a parent and child relationship, between the persons. As a result, for example, in a case of the parent and child relationship, it may be determined that a scan omission of a commodity product does not occur as long as a commodity product scan is performed on one of the targeted persons.</p><p id="p-0084" num="0083">Furthermore, as illustrated on the lower right part of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the fraud detection apparatus <b>10</b> detects skeletons of persons specified by the captured image <b>250</b> by using, for example, existing pose estimation and an existing skeleton estimation algorithm. The existing pose estimation and the existing skeleton estimation algorithm mentioned here are a pose estimation algorithm using deep training, such as DeepPose or OpenPose, or a skeleton estimation algorithm using deep training, such as Human Pose estimation. Then, the fraud detection apparatus <b>10</b> specifies the motion of the person on the basis of the detected skeleton information.</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram illustrating an example of specifying a motion according to the first embodiment. As illustrated on the left side of <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the fraud detection apparatus <b>10</b> specifies a motion of a person putting a commodity product into a basket in the case where, for example, the skeleton of the fingers of the person has come out from the region of the basket after the fingers entered the region of the basket for a predetermined period of time. Here, the motions made during the predetermined time period can be determined from consecutive motions that are specified from each of the captured images that are consecutively captured. Furthermore, the fraud detection apparatus <b>10</b> is able to determine a movement of each of the commodity products and a movement of the skeleton of the fingers specified from, for example, the captured image and is able to specify a more detailed motion, for example, is able to specify which one of the commodity products is put into the basket.</p><p id="p-0086" num="0085">Furthermore, as illustrated on the right side of <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the fraud detection apparatus <b>10</b> specifies a motion of a person registering a commodity product targeted for a purchase to the user terminal <b>100</b> in the case where, for example, the both elbows of the person have not moved for a predetermined time period while being bent forward within a predetermined range of a the region of the specified basket. Furthermore, the fraud detection apparatus <b>10</b> is able to detect, for example, a surface of a barcode of each of the commodity products and specify a more detailed motion, such as a motion indicating which commodity product has been registered.</p><p id="p-0087" num="0086">Furthermore, when the fraud detection apparatus <b>10</b> specifies each of the motions, it is possible to reduce a processing load by generating and processing the skeleton information having a smaller amount of information from the captured images, instead of directly processing the captured image.</p><p id="p-0088" num="0087">A description will be given here by referring back to <figref idref="DRAWINGS">FIG. <b>7</b></figref>. The fraud detection apparatus <b>10</b> specifies a person or an object from the captured image <b>250</b>, and specifies a motion of the specified person putting the commodity product into the basket and a motion of registering the commodity product. Then, the fraud detection apparatus <b>10</b> is able to detect, as a target person for a scan omission, a person who no longer performs any commodity product scan even though a person has performed a commodity product scan and registered the commodity product by using the user terminal <b>100</b> with respect to, for example, a motion of putting a commodity product into the basket. Furthermore, the fraud detection apparatus <b>10</b> is able to additionally perform a process on the captured image <b>250</b> by, for example, enclosing a target person for self scanning <b>150</b>-<b>1</b> who is a target person for a scan omission with a frame such that the target person for a scan omission is easily specified. Then, the fraud detection apparatus <b>10</b> notifies the store clerk terminal <b>300</b> of the captured image <b>250</b> from which the target person for a scan omission is easily specified and information indicating that a scan omission has been detected as an alert.</p><p id="p-0089" num="0088">Flow of Process</p><p id="p-0090" num="0089">In the following, the flow of a scan omission detection process performed by the fraud detection apparatus <b>10</b> will be described. <figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart illustrating the flow of the scan omission detection process according to the first embodiment. The scan omission detection process illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref> may be performed, for example, at fixed intervals, or every time a captured image is received from the camera device <b>200</b>.</p><p id="p-0091" num="0090">First, as illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the fraud detection apparatus <b>10</b> acquires, from the image DB <b>31</b>, a captured image of a predetermined image capturing range, such as the inside or the site of the store of the retail store, captured by the camera device <b>200</b> (Step S<b>101</b>). Furthermore, in the scan omission detection process illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, in order to process the captured image captured by the camera device <b>200</b>, in a precise sense, a monitoring video image in real time, the captured image is transmitted from the camera device <b>200</b> as needed, and is stored in the image DB <b>31</b>.</p><p id="p-0092" num="0091">Then, the fraud detection apparatus <b>10</b> detects, by using the existing object detection algorithm, a shopping basket from the captured image acquired at Step S<b>101</b> (Step S<b>102</b>).</p><p id="p-0093" num="0092">Then, the fraud detection apparatus <b>10</b> detects, by using the existing object detection algorithm, a person from the captured image acquired at Step S<b>101</b>, and furthermore, detects the skeleton of the detected person by using the existing pose estimation and skeleton estimation algorithms (Step S<b>103</b>). Furthermore, the order of the processes performed at Steps S<b>102</b> and S<b>103</b> may be performed in reverse order, or the processes at Steps S<b>102</b> and S<b>103</b> may be performed in parallel.</p><p id="p-0094" num="0093">Then, the fraud detection apparatus <b>10</b> detects the motion of the person on the basis of the basket detected at Step S<b>102</b> and the skeleton detected at Step S<b>103</b> (Step S<b>104</b>). The motion detected at Step S<b>104</b> is the motion of the person putting the commodity product into the basket and the motion of the person scanning and registering the commodity product targeted for the purchase to the user terminal <b>100</b>.</p><p id="p-0095" num="0094">Furthermore, the motion of putting the commodity product into the basket is specified in the case where, for example, the skeleton of the fingers of the person has come out from the region of the basket after the fingers entered the region for a predetermined period of time. The motion of registering the commodity is specified in the case where, for example, both elbows of the person have not moved for a predetermined period of time while being bent forward within a predetermined range of the region of the basket. Furthermore, in order to specify the motion of the person, the captured images that are consecutively captured and the basket and the skeleton information that are detected from these captured images are needed. Accordingly, at the time of process performed at Step S<b>104</b>, the processes at Steps S<b>101</b> to S<b>103</b> may be repeatedly performed a predetermined number of times by using different captured images.</p><p id="p-0096" num="0095">If the put-into-basket motion has been detected at the motion detection at Step S<b>104</b> (Yes at Step S<b>105</b>), the fraud detection apparatus <b>10</b> registers the put-into-basket motion to the motion history <b>33</b> (Step S<b>106</b>). This indicates that, for example, a put-into-basket motion count Nk stored in the motion history <b>33</b> is incremented by one.</p><p id="p-0097" num="0096">In contrast, if the put-into-basket motion is not detected (No at Step S<b>105</b>), or after having registered the put-into-basket motion (Step S<b>106</b>), the process at Step S<b>107</b> is performed. As a result, if a commodity product registration motion has been detected at the motion detection performed at Step S<b>104</b> (Yes at Step S<b>107</b>), the fraud detection apparatus <b>10</b> registers the commodity product registration motion to the motion history <b>33</b> (Step S<b>108</b>). This indicates that, for example, a commodity product registration motion count Ns stored in the motion history <b>33</b> is incremented by one. After having performed the process at Step S<b>108</b>, the scan omission detection process illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref> is ended.</p><p id="p-0098" num="0097">In contrast, if the commodity product registration motion is not detected (No at Step S<b>107</b>), the fraud detection apparatus <b>10</b> performs a target person determination process that will be described later by using <figref idref="DRAWINGS">FIG. <b>11</b></figref> (Step S<b>109</b>). If it is determined, on the basis of the target person determination process performed at Step S<b>109</b>, that the person for which no commodity product registration motion has been detected at Step S<b>107</b> is a target person for a scan omission (Yes at Step S<b>110</b>), the fraud detection apparatus <b>10</b> notifies the store clerk terminal <b>300</b> of an alert (Step S<b>111</b>). If it is determined that the person for which no commodity product registration motion has been detected after having performed the process at Step S<b>111</b> or at the process at Step S<b>107</b> is not a target person for a scan omission (No at Step S<b>110</b>), the scan omission detection process illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref> is ended.</p><p id="p-0099" num="0098">In the following, the target person determination process performed at Step S<b>109</b> illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref> will be described. <figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart illustrating the flow of the target person determination process according to the first embodiment.</p><p id="p-0100" num="0099">First, as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the fraud detection apparatus <b>10</b> determines whether or not the put-into-basket motion count Nk of the person targeted for the determination is greater than a first threshold (Step S<b>201</b>). The determination is, as described above, a determination of whether or not the number of items of the commodity products that have been put into the basket is greater than or equal to a certain number.</p><p id="p-0101" num="0100">If the put-into-basket motion count Nk is greater than the first threshold (Yes at Step S<b>201</b>), the fraud detection apparatus <b>10</b> determines whether or not a difference between the put-into-basket motion count Nk and the commodity product registration motion count Ns is greater than zero and is equal to or less than the second threshold (Step S<b>202</b>). The determination is, as described above, a determination of whether or not a difference between the number of motions of putting the commodity product into the basket and the number of motions of registering the commodity product is greater than the second threshold and whether or not there is a comparatively large discrepancy.</p><p id="p-0102" num="0101">If the difference between the put-into-basket motion count Nk and the commodity product registration motion count Ns is greater than the second threshold (No at Step S<b>202</b>), the fraud detection apparatus <b>10</b> determines that the person targeted for the determination is a target person for a scan omission.</p><p id="p-0103" num="0102">In contrast, if the difference between the put-into-basket motion count Nk and the commodity product registration motion count Ns is less than or equal to the second threshold (Yes at Step S<b>202</b>), the fraud detection apparatus <b>10</b> determines whether or not the person targeted for the determination attempts to purchase a high-priced commodity product (Step S<b>203</b>). Regarding this determination, for example, in the case where the person targeted for the determination has been detected from an image having captured therein a sales floor of the high-priced commodity product or in the case where a motion of putting the high-priced commodity product into a basket, it is determined that the person targeted for the determination attempts to purchase the high-priced commodity product.</p><p id="p-0104" num="0103">If the person targeted for the determination does not attempt to purchase the high-priced commodity product (No at Step S<b>203</b>), the fraud detection apparatus <b>10</b> determines that the person targeted for the determination is a non-target person for a scan omission.</p><p id="p-0105" num="0104">In contrast, if the person targeted for the determination attempts to purchase the high-priced commodity product (Yes at Step S<b>203</b>), the fraud detection apparatus <b>10</b> determines that the person targeted for the determination is a target person for a scan omission.</p><p id="p-0106" num="0105">Furthermore, if the put-into-basket motion count Nk is less than or equal to the first threshold (No at Step S<b>201</b>), the fraud detection apparatus <b>10</b> determines whether or not the person targeted for the determination carries a scan terminal, i.e., the user terminal <b>100</b> (Step S<b>204</b>). This determination may be determined in accordance with whether or not the user terminal <b>100</b> has been detected from the captured image by using, for example, the existing object detection algorithm.</p><p id="p-0107" num="0106">If the person targeted for the determination does not carry the user terminal <b>100</b> (No at Step S<b>204</b>), the fraud detection apparatus <b>10</b> determines that the person targeted for the determination is a non-target person for a scan omission.</p><p id="p-0108" num="0107">In contrast, if the person targeted for the determination carries the user terminal <b>100</b> (Yes at Step S<b>204</b>), the fraud detection apparatus <b>10</b> determines whether or not the commodity product registration motion count Ns is zero (Step S<b>205</b>). If the commodity product registration motion count Ns is zero (Yes at Step S<b>205</b>), the fraud detection apparatus <b>10</b> determines that the person targeted for the determination is a target person for a scan omission. The reason is that, even if the number of items of the commodity products that are put into the basket is small, if registration of the commodity products is not performed at all, this state is detected as a scan omission of the commodity products.</p><p id="p-0109" num="0108">In contrast, if the commodity product registration motion count Ns is not zero (No at Step S<b>205</b>), the fraud detection apparatus <b>10</b> determines that the person targeted for the determination is a non-target person for a scan omission. The reason is that, because the number of items of the commodity products that are put into the basket is small, it is determined that the discrepancy is small even if there is a difference between the number of motions of putting a commodity product into the basket and the number of motions of registering the commodity product as long as registration of the commodity products has been performed, and this state is not detected as a scan omission. In this case, if the number of items of the commodity products that are put into the basket is increased on the basis of a future behavior of the person targeted for the determination, this state is stopped at Step S<b>202</b>, and it may possibly be determined that the person targeted for the determination is a target person for a scan omission.</p><heading id="h-0008" level="1">[b] Second Embodiment</heading><p id="p-0110" num="0109">However, in some cases, a motion of scanning and registering a commodity product to the user terminal <b>100</b>, that is, a commodity product registration motion, is performed outside the image capturing range of the camera device <b>200</b>. For example, this case may occur in the case where a person collectively scans commodity products after moving to an unoccupied location that is outside the image capturing range of the camera device <b>200</b> due to congestion of a sales floor on which the camera device <b>200</b> is installed. In this case, the commodity product registration motion is not reflected in a video image captured by the camera device <b>200</b>, and it is thus not possible to detect the commodity product registration motion, so that the commodity product registration motion count corresponding to that amount is not counted. Accordingly, there may be some cases where a person who collectively performs commodity product scans is erroneously determined to be a target person for scan omission. To solve this type of problem, it is conceivable to increase the number of the camera devices <b>200</b>; however, this may possibly result in an increase in cost.</p><p id="p-0111" num="0110">Thus, as a second embodiment, in a determination process for determining a target person for scan omission, the number of scanned registration entries is used instead of the commodity product registration motion count. The number of scanned registration entries is the number of commodity products that are scanned by the customer by himself or herself by using the user terminal <b>100</b> and that are registered, as the commodity products to be purchased, to the user terminal <b>100</b> or the self-service checkout terminal <b>400</b>. Furthermore, the determination process of a target person for a scan omission described in the first embodiment and the second embodiment may be used in combination.</p><p id="p-0112" num="0111"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram illustrating a configuration example of the fraud detection system <b>2</b> according to the second embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the fraud detection system <b>2</b> is a system in which the fraud detection apparatus <b>10</b> is connected to the user terminals <b>100</b>, the camera devices <b>200</b>, and the store clerk terminal <b>300</b> via the network <b>50</b> so as to be communicated with each other. Furthermore, in the fraud detection system <b>2</b>, the fraud detection apparatus <b>10</b> is also connected to self-service checkout terminals <b>400</b>-<b>1</b> to <b>400</b>-<i>s </i>(s is any integer. Hereinafter, collectively referred to as a &#x201c;self-service checkout terminal <b>400</b>&#x201d;) via the network <b>50</b> so as to be communicated with each other.</p><p id="p-0113" num="0112">The fraud detection apparatus <b>10</b>, the network <b>50</b>, the user terminals <b>100</b>, the camera devices <b>200</b>, and the store clerk terminal <b>300</b> included in the fraud detection system <b>2</b> are the same as those included in the fraud detection system <b>1</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0114" num="0113">The self-service checkout terminal <b>400</b> is a terminal that is used by a customer by himself or herself to pay a bill of the commodity product. The self-service checkout terminal <b>400</b> receives information on the commodity products to be purchased via the user terminal <b>100</b> at the time of calculation of a payment amount of the commodity products. Then, the self-service checkout terminal <b>400</b> accepts, from the customer, the payment of the commodity products to be purchased made by cash, a credit card, electronic money, or the like.</p><p id="p-0115" num="0114"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram illustrating an example of scan omission detection according to the second embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the fraud detection apparatus <b>10</b> used in the fraud detection system <b>2</b> specifies, from the captured images of each of the sales floors captured by the camera devices <b>200</b>, the first motion of a person who is visiting the store acquiring a commodity product sold in the store, and counts the number of motions thereof as a commodity product acquisition motion count. The example illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref> indicates the state in which a motion of acquiring a commodity product of a HBC item is counted one time from the captured image of the sales floor of daily consumable products captured by a camera device <b>200</b>-<i>x</i>, and a motion of acquiring a commodity product of a beer item is counted one time from the captured image of a liquor sales floor captured by a camera device <b>200</b>-<i>y. </i></p><p id="p-0116" num="0115">Furthermore, the fraud detection apparatus <b>10</b> counts, as the number of scanned registration entries, the second count that indicates the number of times a commodity product targeted for a purchase has been registered to the first terminal by a person. Here, the first terminal corresponds to the user terminal <b>100</b> or the self-service checkout terminal <b>400</b>. The example illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref> indicates the state in which the number of commodity products that are registered by the person by scanning the respective bar codes or the respective QR codes (registered trademark) attached to the commodity products via the user terminal <b>100</b> is transmitted to and displayed on a self-service checkout terminal <b>400</b>-<i>x </i>at the time of calculation of a payment amount of the commodity products.</p><p id="p-0117" num="0116">Then, the fraud detection apparatus <b>10</b> checks the commodity product acquisition motion count that is the first count of the specified first motion against the number of scanned registration entries that is the counted second count, and evaluates, on the basis of the checking result, the behavior of the person exhibiting with respect to a purchase of the commodity product. More specifically, for example, if a difference between the commodity product acquisition motion count and the number of scanned registration entries is greater than or equal to a first threshold and if the commodity product acquisition motion count is greater than or equal to a second threshold, the fraud detection apparatus <b>10</b> determines that the target person has behaved fraudulently or performed an erroneous operation. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the commodity product acquisition motion count of the beer item is one time, whereas the number of scanned registration entries is zero, so that the fraud detection apparatus <b>10</b> is able to determine that the target person has behaved fraudulently or performed an erroneous operation.</p><p id="p-0118" num="0117">Functional Configuration of Fraud Detection Apparatus <b>10</b></p><p id="p-0119" num="0118">In the following, a functional configuration of the fraud detection apparatus <b>10</b> will be described. <figref idref="DRAWINGS">FIG. <b>15</b></figref> is a diagram illustrating a configuration example of the fraud detection apparatus <b>10</b> according to the second embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>15</b></figref>, the fraud detection apparatus <b>10</b> includes the communication unit <b>20</b>, the storage unit <b>30</b>, and the control unit <b>40</b>.</p><p id="p-0120" num="0119">Similarly to the fraud detection apparatus <b>10</b> according to the first embodiment, the communication unit <b>20</b> is a processing unit that controls communication with another device, such as the user terminal <b>100</b>, the camera device <b>200</b>, the store clerk terminal <b>300</b>, or the self-service checkout terminal <b>400</b>.</p><p id="p-0121" num="0120">Similarly to the fraud detection apparatus <b>10</b> according to the first embodiment, the storage unit <b>30</b> has a function for storing various kinds of data and the program executed by the control unit <b>40</b> and is implemented by, for example, a storage device, such as a memory or a hard disk. The storage unit <b>30</b> stores therein the image DB <b>31</b>, the skeleton information <b>32</b>, the motion history <b>33</b>, the commodity product registration information <b>34</b>, and the like.</p><p id="p-0122" num="0121">Similarly to the fraud detection apparatus <b>10</b> according to the first embodiment, the image DB <b>31</b> is able to store therein a plurality of captured images that are a series of frames captured by the camera device <b>200</b> and is able to store therein position information on a person or an object that is included in an image and that is specified with respect to the subject captured image.</p><p id="p-0123" num="0122">Similarly to the fraud detection apparatus <b>10</b> according to the first embodiment, the skeleton information <b>32</b> stores therein skeleton information on the person specified from the captured image that is captured by the camera device <b>200</b>.</p><p id="p-0124" num="0123">The motion history <b>33</b> stores therein the number of motions of a person specified on the basis of the skeleton information or the like. <figref idref="DRAWINGS">FIG. <b>16</b></figref> is an example of data stored in the motion history <b>33</b> according to the second embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>16</b></figref>, the motion history <b>33</b> stores therein, in an associated manner, for example, &#x201c;person ID&#x201d; that is an identifier for uniquely identifying a person who has performed a motion, &#x201c;time&#x201d; that is the time at which the motion has been specified by the fraud detection apparatus <b>10</b>, &#x201c;motion&#x201d; that indicates the type of the specified motion, and &#x201c;commodity product category&#x201d; that indicates a commodity product item of the acquired commodity product. Here, for example, the &#x201c;commodity product acquisition&#x201d; indicated by the &#x201c;motion&#x201d; illustrated in <figref idref="DRAWINGS">FIG. <b>16</b></figref> may be set in the case where the motion of the target person acquiring a commodity product from a sales floor has been specified, and the &#x201c;commodity product return&#x201d; may be set in the case where the motion of the target person returning the commodity product to the sales floor. Furthermore, although not illustrated, the motion history <b>33</b> may store feature information on the person extracted from the captured image. Furthermore, the feature information on the person may be, for example, an image feature of the body of a person, such as clothes, an image feature amount obtained by using Re-Identification (Re-ID), an image feature obtained by specifying the position of the body of the person after performing skeleton estimation, or the like.</p><p id="p-0125" num="0124">The commodity product registration information <b>34</b> stores therein the number of commodity products that have been registered as the commodity products to be purchased after the customer scanned the commodity products by himself or herself by using the user terminal <b>100</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref>, the commodity product registration information <b>34</b> stores therein, in an associated manner, &#x201c;person ID&#x201d; that is an identifier for uniquely identifying a person who has scanned a commodity product and registered the commodity product, &#x201c;the number of entries&#x201d; that indicates the number of entries of the commodity products scanned and registered, and &#x201c;commodity product category&#x201d; that indicates the commodity product item of the scanned commodity products.</p><p id="p-0126" num="0125">Furthermore, the above described information stored in the storage unit <b>30</b> is only an example, and the storage unit <b>30</b> is able to store various kinds of information other than the information described above.</p><p id="p-0127" num="0126">Similarly to the fraud detection apparatus <b>10</b> according to the first embodiment, the control unit <b>40</b> is a processing unit that manages the entirety of the fraud detection apparatus <b>10</b>. The control unit <b>40</b> includes the specifying unit <b>41</b>, the generating unit <b>42</b>, the evaluating unit <b>43</b>, and the notifying unit <b>44</b>.</p><p id="p-0128" num="0127">Similarly to the fraud detection apparatus <b>10</b> according to the first embodiment, the specifying unit <b>41</b> specifies, from the captured image captured by the camera device <b>200</b>, a person visiting a store and an object, such as, a commodity product or a basket, that is being used by the subject person. Furthermore, on the basis of the specified object, the skeleton information generated by the generating unit <b>42</b>, or the like, the specifying unit <b>41</b> specifies the first motion of a person acquiring a commodity product or specifies a motion of a person returning the commodity product to the sales floor. Furthermore, a process of specifying the motion of the person acquiring the commodity product may be a process of specifying a motion of a person acquiring a commodity product from a commodity product shelf, or may be a process of specifying a motion of a person taking out a commodity product from a basket. Similarly, the process of specifying the motion of a person returning the commodity product to the sales floor may be a process of specifying a motion of a person returning the commodity product shelf to the commodity product, or may be a process of specifying a motion of a person putting a commodity product into a basket.</p><p id="p-0129" num="0128">Similarly to the fraud detection apparatus <b>10</b> according to the first embodiment, the generating unit <b>42</b> generates skeleton information on the person specified by the specifying unit <b>41</b> from the captured image that has been captured by the camera device <b>200</b>.</p><p id="p-0130" num="0129">The evaluating unit <b>43</b> counts the first count of the first motion of a person acquiring a commodity product. Furthermore, the evaluating unit <b>43</b> counts the second count that indicates the number of times the commodity product targeted for a purchase has been registered, by a person, to the first terminal that is the user terminal <b>100</b> or the self-service checkout terminal <b>400</b>. Then, the evaluating unit <b>43</b> evaluates, on the basis of the first count and the second count, a behavior of the person exhibiting with respect to the purchase of the commodity product. Here, a process of evaluating the behavior may include a process of determining that the person has behaved fraudulently or performed an erroneous operation in the case where a difference between the first count and the second count is greater than or equal to the first threshold, and the first count is greater than or equal to the second threshold.</p><p id="p-0131" num="0130">Furthermore, a process of counting the first count and the second count performed by the evaluating unit <b>43</b> and the process of evaluating the behavior may be performed in the case where the second motion of the person registering commodity product targeted for a purchase to the first terminal is not specified after the first motion has been specified. Here, the first terminal is the user terminal <b>100</b> or the self-service checkout terminal <b>400</b>.</p><p id="p-0132" num="0131">Similarly to the fraud detection apparatus <b>10</b> according to the first embodiment, if it is determined, by the evaluation of the behavior obtained by the evaluating unit <b>43</b>, that the person has behaved fraudulently or performed an erroneous operation, the notifying unit <b>44</b> notifies the store clerk terminal <b>300</b> of an alert.</p><p id="p-0133" num="0132">In the following, a scan omission detection process according to the second embodiment performed by the fraud detection apparatus <b>10</b> functioning as an actor will be described in detail with reference to <figref idref="DRAWINGS">FIG. <b>18</b></figref>. <figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram illustrating an example of scan omission detection according to the second embodiment.</p><p id="p-0134" num="0133">As illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref>, the fraud detection apparatus <b>10</b> specifies, from the captured image of a sales floor A captured by the camera device <b>200</b>-<i>y</i>, a motion of a person P acquiring a commodity product from the commodity product shelf of alcoholic beverages, associates the person information with the specified motion, and stores, in the motion history <b>33</b>, the associated information for each commodity product category that is a commodity product item. Furthermore, regarding the commodity product item, for example, by previously designating a region, i.e., a region of interest (ROI), of each of the commodity product shelfs or the sales floor areas included in the image capturing region of the camera device <b>200</b>, the fraud detection apparatus <b>10</b> is able to specify which commodity product item associated with the commodity product has been acquired by the person. Alternatively, on the basis of the installation area of the camera device <b>200</b>, the fraud detection apparatus <b>10</b> may specify which commodity product item associated with the commodity product has been acquired by the person.</p><p id="p-0135" num="0134">Then, the fraud detection apparatus <b>10</b> acquires the data on the person P from the motion history <b>33</b> illustrated in <figref idref="DRAWINGS">FIG. <b>16</b></figref>, and counts for each commodity product category, if the &#x201c;motion&#x201d; is &#x201c;commodity product acquisition&#x201d;, a commodity product acquisition count by incrementing the commodity product acquisition count by one (+1). In contrast, if the &#x201c;motion&#x201d; is &#x201c;commodity product return&#x201d;, the fraud detection apparatus <b>10</b> counts the commodity product acquisition count by decrementing the commodity product acquisition count by one (&#x2212;1). By doing so, it is assumed that the commodity product acquisition count of the alcoholic beverages indicates three.</p><p id="p-0136" num="0135">Here, a method for acquiring or returning a commodity product will be described. <figref idref="DRAWINGS">FIG. <b>19</b></figref> is a diagram illustrating an example of specifying a commodity product acquisition/return motion according to the second embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>19</b></figref>, the fraud detection apparatus <b>10</b> specifies the hand and the commodity product shelf from the captured image captured by the camera device <b>200</b>, and specifies, from the positional relationship between a bounding box (BBOX) of the hand and the ROI of the commodity product shelf, a motion of extending the hand to the commodity product shelf, or a motion of returning the hand from the commodity product shelf.</p><p id="p-0137" num="0136">Then, the fraud detection apparatus <b>10</b> determines whether or not the person holds an object, that is, a commodity product, at the time of the specified motion of extending the hand or returning the hand by using a machine training model generated by being trained to identify presence or absence of an object from, for example, a part of the image of the hand included in the BBOX.</p><p id="p-0138" num="0137">Furthermore, the fraud detection apparatus <b>10</b> determines more detailed motion of the person on the basis of a combination of a determination result indicating whether or not the person holds the commodity product associated with the motion of extending the hand and the motion of returning the hand. More specifically, for example, in the case where the person does not have the commodity product at the time of the motion of extending the hand and the person has the commodity product at the time of the motion of returning the hand, the fraud detection apparatus <b>10</b> is able to determine that the target person has acquired the commodity product from the commodity product shelf.</p><p id="p-0139" num="0138">Furthermore, in the case where, for example, the target person does not have the commodity product at the time of both of the motion of extending the hand and the motion of returning the hand, the fraud detection apparatus <b>10</b> determines that the target person did not consequently acquire the commodity product even though the target person touched the commodity product shelf by the hand.</p><p id="p-0140" num="0139">Furthermore, in the case where, for example, the target person has the commodity product at the time of the motion of extending the hand but does not have the commodity product at the time of the motion of returning the hand, the fraud detection apparatus <b>10</b> is able to determine that the target person has returned the commodity product to the commodity product shelf. Furthermore, in the case where, for example, the target person has the commodity product at the time of both of the motion of extending the hand and the motion of returning the hand, the fraud detection apparatus <b>10</b> is able to determine that the target person returned the commodity product to the commodity product shelf and has acquired another commodity product.</p><p id="p-0141" num="0140">A description will be given here by referring back to <figref idref="DRAWINGS">FIG. <b>18</b></figref>. The fraud detection apparatus <b>10</b> specifies each of the persons who performs a procedure of calculation of a payment amount at the self-service checkout terminal <b>400</b> from the captured image having captured therein the cash register area, in which the self-service checkout terminal <b>400</b> is installed, captured by a camera device <b>200</b>-<i>z</i>. Moreover, at this time, the fraud detection apparatus <b>10</b> may specify the self-service checkout terminal <b>400</b> that is used by the specified person from the plurality of the self-service checkout terminals <b>400</b>, and may store the specified person in an associated manner with the specified self-service checkout terminal <b>400</b>. Then, by using an existing technology, such as the Re-ID, the fraud detection apparatus <b>10</b> performs person re-identification from the image feature of the person specified from the cash register area, and then, determines whether the person specified from the captured image of each of the sales floors is the same person. The example illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref> indicates a state in which the fraud detection apparatus <b>10</b> specifies, from the captured image of the cash register area, the person P who performs a procedure of calculation of a payment amount at a self-service checkout terminal <b>400</b>-<i>y</i>, and determines, on the basis of the captured image of the sales floor A, that the specified person P is the same as the person P who has been specified and who made a motion of acquiring the alcoholic beverages three times. Furthermore, in the example illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref>, it is determined that a person Q is a person who is different from the person P; however, it is, of course, conceivable to be determined that the person Q is the same person who has been specified from another captured image.</p><p id="p-0142" num="0141">Then, the fraud detection apparatus <b>10</b> compares the number of commodity products registered to the self-service checkout terminal <b>400</b>-<i>y </i>or registered to the user terminal <b>100</b> that is used by the person P to the commodity product acquisition motion count of the person P. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref>, an acquisition motion count of alcoholic beverages obtained by the person P is three times, whereas the number of commodity products registered by a commodity product scan is two, so that the fraud detection apparatus <b>10</b> determines that a scan omission has occurred. Furthermore, as illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref>, if the fraud detection apparatus <b>10</b> determines that a scan omission has occurred, the fraud detection apparatus <b>10</b> notifies the store clerk terminal <b>300</b> of an alert.</p><p id="p-0143" num="0142">In the following, the flow of a commodity product acquisition/return motion specifying process performed by the fraud detection apparatus <b>10</b> will be described. <figref idref="DRAWINGS">FIG. <b>20</b></figref> is a flowchart illustrating the flow of the commodity product acquisition/return motion specifying process according to the second embodiment. The commodity product acquisition/return motion specifying process illustrated in <figref idref="DRAWINGS">FIG. <b>20</b></figref> may be performed at, for example, regular intervals or every time a captured image is received from the camera device <b>200</b>.</p><p id="p-0144" num="0143">First, as illustrated in <figref idref="DRAWINGS">FIG. <b>20</b></figref>, the fraud detection apparatus <b>10</b> acquires, from the image DB <b>31</b>, the captured image having captured therein a predetermined sales floor captured by the camera device <b>200</b> (Step S<b>301</b>). Furthermore, in the commodity product acquisition/return motion specifying process illustrated in <figref idref="DRAWINGS">FIG. <b>20</b></figref>, the captured image captured by the camera device <b>200</b>, in a precise sense, a monitoring video image, is processed in real time, so that the captured image is transmitted from the camera device <b>200</b> as needed, and is stored in the image DB <b>31</b>.</p><p id="p-0145" num="0144">Then, by using, for example, the existing technology, the fraud detection apparatus <b>10</b> specifies a person or an object from the captured image acquired at Step S<b>301</b>, detects skeletons of the person, and specifies a motion of the person acquiring the commodity product, or a motion of returning the commodity product (Step S<b>302</b>). Moreover, the object that is specified here may be a commodity product, a commodity product shelf, or the like.</p><p id="p-0146" num="0145">Then, on the basis of, for example, the ROI of the commodity product shelf specified from the captured image acquired at Step S<b>301</b>, an installation area of the camera device <b>200</b>, or the like, the fraud detection apparatus <b>10</b> acquires the commodity product item information on the commodity product that has been acquired or returned at Step S<b>302</b> (Step S<b>303</b>).</p><p id="p-0147" num="0146">Then, the fraud detection apparatus <b>10</b> uses, for example, the existing technology, and extract the feature information on the person specified at Step S<b>302</b> from the captured image acquired at Step S<b>301</b> (Step S<b>304</b>). Moreover, the feature information on the person extracted here may be, as described above, for example, the image feature of the body of the person, the image feature amount based on the Re-ID, an image feature related to the skeletons obtained from the skeleton estimation of the person, or the like.</p><p id="p-0148" num="0147">In addition, the fraud detection apparatus <b>10</b> registers and stores, in a DB, for example, in the motion history <b>33</b> in an associated manner, the motion specified at Step S<b>302</b>, the commodity product item information acquired at Step S<b>303</b>, the feature information on the person extracted at Step S<b>304</b>, the time information, and the like (Step S<b>305</b>). After having performed the process at Step S<b>305</b>, the commodity product acquisition/return motion specifying process illustrated in <figref idref="DRAWINGS">FIG. <b>20</b></figref> is ended.</p><p id="p-0149" num="0148">In the following, the flow of a scan omission detection process performed by the fraud detection apparatus <b>10</b> will be described. <figref idref="DRAWINGS">FIG. <b>21</b></figref> is a flowchart illustrating the flow of the scan omission detection process according to the second embodiment.</p><p id="p-0150" num="0000">The scan omission detection process illustrated in <figref idref="DRAWINGS">FIG. <b>21</b></figref> may be performed at, for example, regular intervals or every time a signal indicating, for example, the start of a process of calculation of a payment amount has been received from the self-service checkout terminal <b>400</b> or every time a captured image is received from the camera device <b>200</b>.</p><p id="p-0151" num="0149">First, as illustrated in <figref idref="DRAWINGS">FIG. <b>21</b></figref>, the fraud detection apparatus <b>10</b> acquires, from the commodity product registration information <b>34</b>, a scan record of the commodity product targeted for a purchase that has been registered by scanning each of the commodity products by using the user terminal <b>100</b>, such as the number of scanned registration entries for each commodity product item (Step S<b>401</b>). Moreover, in the scan omission detection process illustrated in <figref idref="DRAWINGS">FIG. <b>21</b></figref>, the scan record of the commodity product targeted for a purchase is received from the user terminal <b>100</b> by the self-service checkout terminal <b>400</b>, is further transmitted to the fraud detection apparatus <b>10</b>, and is stored in the commodity product registration information <b>34</b>.</p><p id="p-0152" num="0150">Then, the fraud detection apparatus <b>10</b> acquires, from the image DB <b>31</b>, the captured image that has been captured by the camera device <b>200</b> and that has therein the cash register area in which the target self-service checkout terminal <b>400</b> is installed (Step S<b>402</b>). Moreover, in the scan omission detection process illustrated in <figref idref="DRAWINGS">FIG. <b>21</b></figref>, the captured image captured by the camera device <b>200</b>, in a precise sense, a monitoring video image, is processed in real time, so that the captured image is transmitted from the camera device <b>200</b> as needed, and is stored in the image DB <b>31</b>.</p><p id="p-0153" num="0151">Then, the fraud detection apparatus <b>10</b> specifies, by using, for example, the existing technology, the person from the captured image that is acquired at Step S<b>402</b> (Step S<b>403</b>).</p><p id="p-0154" num="0152">Then, the fraud detection apparatus <b>10</b> uses, for example, the existing technology, and extracts, from the captured image acquired at Step S<b>402</b>, the feature information on the person specified at Step S<b>403</b> (Step S<b>404</b>). Moreover, the feature information on the person extracted here may be, as described above, for example, the image feature of the body of the person, the image feature amount based on the Re-ID, an image feature related to the skeletons obtained from the skeleton estimation of the person, or the like.</p><p id="p-0155" num="0153">Then, the fraud detection apparatus <b>10</b> searches, on the basis of the feature information on the person extracted at Step S<b>404</b> or the like, the DB, for example, the motion history <b>33</b> (Step S<b>405</b>). If the result of the search of the DB performed at Step S<b>405</b> indicates that the data on the target person is not stored (No at Step S<b>406</b>), the scan omission detection process indicated in <figref idref="DRAWINGS">FIG. <b>21</b></figref> is ended.</p><p id="p-0156" num="0154">In contrast, if the data on the target person is stored in the motion history <b>33</b> (Yes at Step S<b>406</b>), the fraud detection apparatus <b>10</b> acquires and accumulates the pieces of data on the target person from the motion history <b>33</b>, and then, counts and acquires the commodity product acquisition motion count (Step S<b>407</b>).</p><p id="p-0157" num="0155">Then, the fraud detection apparatus <b>10</b> compares the commodity product acquisition motion count acquired at Step S<b>407</b> to the number of scanned registration entries acquired at Step S<b>401</b> (Step S<b>408</b>). If the difference between the commodity product acquisition motion count and the number of scanned registration entries is less than a predetermined threshold (No at Step S<b>409</b>), the scan omission detection process illustrated in <figref idref="DRAWINGS">FIG. <b>21</b></figref> is ended.</p><p id="p-0158" num="0156">In contrast, if the difference between the commodity product acquisition motion count and the number of scanned registration entries is greater than or equal to the predetermined threshold (Yes at Step S<b>409</b>), the fraud detection apparatus <b>10</b> determines that the target person has behaved fraudulently or performed an erroneous operation and notifies the store clerk terminal <b>300</b> that is used by an employee who is in charge of, for example, a cash register area of an alert (Step S<b>410</b>). After having performed the process at Step S<b>410</b>, the scan omission detection process illustrated in <figref idref="DRAWINGS">FIG. <b>21</b></figref> is ended.</p><p id="p-0159" num="0157">Effects</p><p id="p-0160" num="0158">As described above, the fraud detection apparatus <b>10</b> specifies, from the captured image, a person who is visiting the store and an object that is used by the person; generates the skeleton information <b>32</b> on the person specified from the image; specifies, on the basis of the specified object and the skeleton information <b>32</b>, the first motion of the person acquiring the commodity product sold in the store; counts the second count that indicates the number of times the commodity product targeted for a purchase has been registered to the first terminal by the person; and evaluates, on the basis of the first count of the specified first motion and the counted second count, a behavior of the person exhibiting with respect to the purchase of the commodity product.</p><p id="p-0161" num="0159">As a result, the fraud detection apparatus <b>10</b> evaluates the behavior of the customer on the basis of the number of motions of acquiring the commodity products and the number of scanned registration entries, so that the fraud detection apparatus <b>10</b> is able to detect a scan omission of a commodity product in a system in which customers perform commodity product scans by themselves.</p><p id="p-0162" num="0160">Furthermore, the process of evaluating the behavior performed by the fraud detection apparatus <b>10</b> includes a process of determining that the person has behaved fraudulently or performed an erroneous operation when a difference between the first count and the second count is greater than or equal to the first threshold and when the first count is greater than or equal to the second threshold, and, if it is determined that the person has behaved fraudulently or performed an erroneous operation, the fraud detection apparatus <b>10</b> notifies the store clerk terminal <b>300</b> that is the second terminal of an alert.</p><p id="p-0163" num="0161">As a result, in the system in which the customers perform commodity product scans by themselves, the fraud detection apparatus <b>10</b> is able to notify the store clerk when a scan omission of the commodity product is detected.</p><p id="p-0164" num="0162">Furthermore, the fraud detection apparatus <b>10</b> specifies the second motion of the person registering the commodity product targeted for a purchase to the first terminal on the basis of the specified object and the skeleton information; the process of specifying the first motion performed by the fraud detection apparatus <b>10</b> includes a process of specifying, on the basis of the specified object and the skeleton information <b>32</b>, the first motion in the case where the fingers of the person have come out from the region of the basket after the fingers entered the region of the basket for a predetermined period of time; and</p><p id="p-0165" num="0000">the process of specifying the second motion performed by the fraud detection apparatus <b>10</b> includes a process of specifying, on the basis of the specified object and the skeleton information <b>32</b>, the second motion in the case where the both elbows of the person have not moved for a predetermined time period while being bent forward within a predetermined range of a the region of the specified basket.</p><p id="p-0166" num="0163">As a result, in the system in which the customers perform commodity product scans by themselves, the fraud detection apparatus <b>10</b> is able to more accurately specify each of the motions that have been made by the persons with respect to the commodity products and is able to the registration motion of the commodity product and detect the scan omission of the commodity product.</p><p id="p-0167" num="0164">Furthermore, the process of specifying the person performed by the fraud detection apparatus <b>10</b> includes a process of tracking, from captured images that are captured at different time, the same person at different time on the basis of an appearance and an amount of movement of the person.</p><p id="p-0168" num="0165">As a result, in the system in which the customers perform commodity product scans by themselves, the fraud detection apparatus <b>10</b> is able to track and detect the target person for a scan omission of the commodity product.</p><p id="p-0169" num="0166">Furthermore, the process of counting the first count and the second count and the process of evaluating the behavior performed by the fraud detection apparatus <b>10</b> are executed when the second motion of the person registering the commodity product targeted for the purchase to the first terminal is not specified after the first motion is specified.</p><p id="p-0170" num="0167">As a result, in the system in which the customers perform commodity product scans by themselves, the fraud detection apparatus <b>10</b> is able to detect a scan omission of a commodity product of a target person for self scanning without erroneously detecting a target person for normal cash register.</p><p id="p-0171" num="0168">Furthermore, the process of evaluating the behavior performed by the fraud detection apparatus <b>10</b> includes a process of determining that the person has behaved fraudulently in the case where the person has been specified from the image having captured therein a sales floor of a high-priced commodity product and in the case where the difference is greater than or equal to the third threshold even when the difference between the first count and the second count is less than the first threshold.</p><p id="p-0172" num="0169">As a result, in the system in which the customers perform commodity product scans by themselves, the fraud detection apparatus <b>10</b> is able to more accurately detect the scan omission of the high-priced commodity product.</p><p id="p-0173" num="0170">Furthermore, the fraud detection apparatus <b>10</b> specifies, from a first image having captured therein an area that includes a shelf on which the commodity products in the store are accommodated, a motion of the first person acquiring a commodity product from the shelf; specifies the first person and the self-service checkout terminal <b>400</b> from a second image having captured therein the area that includes the self-service checkout terminal <b>400</b>; stores the first person and the self-service checkout terminal <b>400</b> in an associated manner; receives a purchase history from the self-service checkout terminal <b>400</b> that is associated with the first person; and specifies the second count on the basis of the purchase history.</p><p id="p-0174" num="0171">As a result, in the system in which the customers perform commodity product scans by themselves, the fraud detection apparatus <b>10</b> is able to more accurately detect a scan omission of a commodity product.</p><p id="p-0175" num="0172">Furthermore, the fraud detection apparatus <b>10</b> specifies an area in which the first person is located in the store when the fraud detection apparatus <b>10</b> specifies a third motion of the first person putting a commodity product into a basket; specifies, on the basis of a type of the commodity product associated with the specified area, a first category item indicating the type of the commodity product that has been put into the basket out of the category items indicating the types of the plurality of commodity products; counts a third count of the third motion that has been made with respect to the specified first category item; counts, on the basis of the received purchase history, the second count performed with respect to the first category item; and evaluates the behavior of the first person exhibiting with respect to a purchase of the commodity product on the basis of the number of counts of the third motion that has been made with respect to the first category item and the second count performed with respect to the first category item.</p><p id="p-0176" num="0173">As a result, in the system in which the customers perform commodity product scans by themselves, the fraud detection apparatus <b>10</b> is able to detect, for each commodity product item, a scan omission of the commodity product.</p><p id="p-0177" num="0174">Furthermore, the first terminal is a terminal that stores therein information that is related to the commodity product targeted for the purchase and that is obtained by scanning a bar code or a QR code attached to the commodity product.</p><p id="p-0178" num="0175">As a result, in the system in which the customers perform commodity product scans by themselves, the fraud detection apparatus <b>10</b> is able to detect a scan omission of a commodity product.</p><p id="p-0179" num="0176">Furthermore, the first terminal is the self-service checkout terminal <b>400</b> and specifies the second count on the basis of the information transmitted from the terminal that stores therein the information that is related to the commodity product targeted for the purchase and that is obtained by scanning a bar code or a QR code attached to the commodity product.</p><p id="p-0180" num="0177">As a result, in the system in which the customers perform commodity product scans by themselves, the fraud detection apparatus <b>10</b> is able to detect a scan omission of a commodity product.</p><p id="p-0181" num="0178">System</p><p id="p-0182" num="0179">The flow of the processes, the control procedures, the specific names, and the information containing various kinds of data or parameters indicated in the above specification and drawings can be arbitrarily changed unless otherwise stated. Furthermore, specific examples, distributions, numerical values, and the like described in the embodiment are only examples and can be arbitrarily changed.</p><p id="p-0183" num="0180">Furthermore, the specific shape of a separate or integrated device is not limited to the drawings. In other words, all or part of the device can be configured by functionally or physically separating or integrating any of the units in accordance with various loads or use conditions. In addition, all or any part of each of the processing functions performed by the each of the devices can be implemented by a CPU and by programs analyzed and executed by the CPU or implemented as hardware by wired logic.</p><p id="p-0184" num="0181">Hardware</p><p id="p-0185" num="0182">In the following, a hardware configuration of each of the devices used in each of the first embodiment and the second embodiment will be described. <figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating a hardware configuration example of the fraud detection apparatus <b>10</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the fraud detection apparatus <b>10</b> includes a communication interface <b>10</b><i>a</i>, a hard disk drive (HDD) <b>10</b><i>b</i>, a memory <b>10</b><i>c</i>, and a processor <b>10</b><i>d</i>. Furthermore, each of the units illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref> is connected each other via a bus or the like. Furthermore, <figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates the hardware configuration of the fraud detection apparatus <b>10</b>; however, the store clerk terminal <b>300</b> may also have the same configuration as that of the fraud detection apparatus <b>10</b>.</p><p id="p-0186" num="0183">The communication interface <b>10</b><i>a </i>is a network interface card or the like and communicates with another server. The HDD <b>10</b><i>b </i>stores therein programs or the DB that operates the function illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0187" num="0184">The processor <b>10</b><i>d </i>is a hardware circuit that operates the process that executes each of the functions described above in <figref idref="DRAWINGS">FIG. <b>5</b></figref> or the like by reading the programs that execute the same process as that performed by each of the processing units illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref> from the HDD <b>10</b><i>b </i>or the like and loading the read programs in the memory <b>10</b><i>c</i>. In other words, the process executes the same function as that performed by each of the processing units included in the fraud detection apparatus <b>10</b>. Specifically, the processor <b>10</b><i>d </i>reads, from the HDD <b>10</b><i>b </i>or the like, the programs having the same function as that performed by the specifying unit <b>41</b>, the generating unit <b>42</b>, the evaluating unit <b>43</b>, the notifying unit <b>44</b>, and the like. Then, the processor <b>10</b><i>d </i>executes the process for executing the same processes as those performed by the specifying unit <b>41</b> and the like.</p><p id="p-0188" num="0185">In this way, the fraud detection apparatus <b>10</b> is operated as an information processing apparatus that executes a motion control process by reading and executing the programs that execute the same process as those performed by each of the processing units illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. Furthermore, the fraud detection apparatus <b>10</b> can also implement the same function as that described above in the embodiment by reading the programs from a recording medium by a medium recording device and executing the read programs. Furthermore, the programs described in another embodiment are not limited to be executed by the fraud detection apparatus <b>10</b>. For example, the present embodiment may also be similarly used in a case in which another computer or a server executes a program or in a case in which another computer and a server cooperatively execute the program with each other.</p><p id="p-0189" num="0186">The programs that execute the same process as those performed by each of the processing units illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref> can be distributed via a network, such as the Internet. Furthermore, the programs can be executed by storing the programs in a recording medium that can be read by a computer readable medium, such as a hard disk, a flexible disk (FD), a CD-ROM, a magneto-optical disk (MO), a digital versatile disk (DVD), or the like, and read the programs from the recording medium by the computer.</p><p id="p-0190" num="0187"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a diagram illustrating hardware configuration example of the user terminal <b>100</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>22</b></figref>, the user terminal <b>100</b> includes a communication interface <b>100</b><i>a</i>, an HDD <b>100</b><i>b</i>, a memory <b>100</b><i>c</i>, a processor <b>100</b><i>d</i>, an input device <b>100</b><i>e</i>, and a display device <b>100</b><i>f</i>. Furthermore, each of the units illustrated in FIG. <b>22</b> is connected each other via a bus or the like.</p><p id="p-0191" num="0188">The communication interface <b>100</b><i>a </i>is a network interface card or the like and communicates with another information processing apparatus. The HDD <b>100</b><i>b </i>stores therein programs and data that operate each of the functions of the user terminal <b>100</b>.</p><p id="p-0192" num="0189">The processor <b>100</b><i>d </i>is a hardware circuit that operates the process that executes each of the functions of the user terminal <b>100</b> by reading the program that executes each of the functions of the user terminal <b>100</b> from the HDD <b>100</b><i>b </i>or the like and loading the read program in the memory <b>100</b><i>c</i>. In other words, the process executes the same function as that performed by each of the processing units included in the user terminal <b>100</b>.</p><p id="p-0193" num="0190">In this way, the user terminal <b>100</b> is operated as an information processing apparatus that executes a motion control process by reading and executing the programs that execute a process performed by each of the functions of the user terminal <b>100</b>. Furthermore, the user terminal <b>100</b> is also able to implement each of the functions of the user terminal <b>100</b> by reading the programs from a recording medium by a medium recording device and executing the read programs. Furthermore, the programs described in another embodiment are not limited to be executed by the user terminal <b>100</b>. For example, the present embodiment may also be similarly used in a case in which another computer or a server executes a program or in a case in which another computer and a server cooperatively execute the program with each other.</p><p id="p-0194" num="0191">Furthermore, the programs that execute the process of each of the functions of the user terminal <b>100</b> is able to be distributed via a network, such as the Internet. Furthermore, the programs are able to be executed by storing the programs in a recording medium that can be read by a computer readable medium, such as a hard disk, a flexible disk (FD), a CD-ROM, a magneto-optical disk (MO), a digital versatile disk (DVD), or the like, and read the programs from the recording medium by the computer.</p><p id="p-0195" num="0192">The input device <b>100</b><i>e </i>detects various input operations performed by a user, such as an input operation performed with respect to the programs executed by the processor <b>100</b><i>d</i>. Examples of the input operation include a touch operation or an operation of inserting an earphone terminal into the user terminal <b>100</b>. The touch operation mentioned here indicates various motions of touching the display device <b>100</b><i>f</i>, such as a tap, a double tap, a swipe, or a pinch. Furthermore, the touch operation includes, for example, a motion of an object, such as a finger, approaching the display device <b>100</b><i>f</i>. The input device <b>100</b><i>e </i>may be, for example, a button, a touch panel, a proximity sensor, or the like.</p><p id="p-0196" num="0193">The display device <b>100</b><i>f </i>displays various kinds of visual information based on the control performed by the processor <b>100</b><i>d</i>. The display device <b>100</b><i>f </i>may be a liquid crystal display (LCD), an organic light emitting diode (OLED), such as an organic electro luminescence (EL) display.</p><p id="p-0197" num="0194"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a diagram illustrating a hardware configuration example of the self-service checkout terminal <b>400</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>23</b></figref>, the self-service checkout terminal <b>400</b> includes a communication interface <b>400</b><i>a</i>, an HDD <b>400</b><i>b</i>, a memory <b>400</b><i>c</i>, a processor <b>400</b><i>d</i>, an input device <b>400</b><i>e</i>, and an output unit <b>400</b><i>f</i>. Furthermore, each of the units illustrated in <figref idref="DRAWINGS">FIG. <b>23</b></figref> is connected each other via a bus or the like.</p><p id="p-0198" num="0195">The communication interface <b>400</b><i>a </i>is a network interface card or the like and communicates with another information processing apparatus. The HDD <b>400</b><i>b </i>stores therein programs and data that operate each of the functions of the self-service checkout terminal <b>400</b>.</p><p id="p-0199" num="0196">The processor <b>400</b><i>d </i>is a hardware circuit that operates the process that executes each of the functions of the self-service checkout terminal <b>400</b> by reading the program from the HDD <b>400</b><i>b </i>or the like and loading the read program in the memory <b>400</b><i>c</i>. In other words, the process executes the same function as that performed by each of the processing units included in the self-service checkout terminal <b>400</b>.</p><p id="p-0200" num="0197">In this way, the self-service checkout terminal <b>400</b> is operated as an information processing apparatus that executes a motion control process by reading and executing the programs that execute a process performed by each of the self-service checkout terminal <b>400</b>. Furthermore, the self-service checkout terminal <b>400</b> is also able to implement each of the functions of the self-service checkout terminal <b>400</b> by reading the programs from a recording medium by a medium recording device and executing the read programs. Furthermore, the programs described in another embodiment are not limited to be executed by the self-service checkout terminal <b>400</b>. For example, the present embodiment may also be similarly used in a case in which another computer or a server executes a program or in a case in which another computer and a server cooperatively execute the program with each other.</p><p id="p-0201" num="0198">Furthermore, the programs that execute the process of each of the functions of the self-service checkout terminal <b>400</b> is able to be distributed via a network, such as the Internet. Furthermore, the programs are able to be executed by storing the programs in a recording medium that can be read by a computer readable medium, such as a hard disk, a flexible disk (FD), a CD-ROM, a magneto-optical disk (MO), a digital versatile disk (DVD), or the like, and read the programs from the recording medium by the computer.</p><p id="p-0202" num="0199">The input device <b>400</b><i>e </i>detects various input operations performed by a user, such as an input operation performed with respect to the programs executed by the processor <b>400</b><i>d</i>. An example of the input operation includes a touch operation. In a case of the touch operation, the self-service checkout terminal <b>400</b> further includes a display device, and an input operation detected by the input device <b>400</b><i>e </i>may be a touch operation performed on the display device. The input device <b>400</b><i>e </i>may be, for example, a button, a touch panel, a proximity sensor, or the like.</p><p id="p-0203" num="0200">The output unit <b>400</b><i>f </i>outputs data that is output from the program executed by the processor <b>400</b><i>d </i>via an external device connected to the self-service checkout terminal <b>400</b>, such as, an external display device. Furthermore, in the case where the self-service checkout terminal <b>400</b> includes a display device, the self-service checkout terminal <b>400</b> need not include the output unit <b>400</b><i>f. </i></p><p id="p-0204" num="0201">According to an aspect of an embodiment, it is possible to detect a scan omission of a commodity product in the system in which the customers perform commodity product scans by themselves.</p><p id="p-0205" num="0202">All examples and conditional language recited herein are intended for pedagogical purposes of aiding the reader in understanding the invention and the concepts contributed by the inventor to further the art, and are not to be construed as limitations to such specifically recited examples and conditions, nor does the organization of such examples in the specification relate to a showing of the superiority and inferiority of the invention. Although the embodiments of the present invention have been described in detail, it should be understood that the various changes, substitutions, and alterations could be made hereto without departing from the spirit and scope of the invention.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A non-transitory computer-readable recording medium having stored therein an information processing program that causes a computer to execute a process comprising:<claim-text>specifying, from an image that is captured by a camera, a person and a plurality of objects;</claim-text><claim-text>generating, by inputting the image of the person into a machine learning model, skeleton information on the person;</claim-text><claim-text>identifying, based on the plurality of objects and the skeleton information, a first feature value associated with one or more first motions of the person who retrieves an object from among the plurality of objects;</claim-text><claim-text>identifying a second feature value associated with one or more objects registered to a first terminal by the person from among the plurality of object; and</claim-text><claim-text>generating, based on a difference between the first feature value and the second feature value, an alert indicates that an object retrieved by the person is not registered in the first terminal.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The non-transitory computer-readable recording medium according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the process further comprises:<claim-text>specifying, based on the skeleton information and an object that is used by the person, a first motion of the person acquiring a commodity product sold in a store;</claim-text><claim-text>counting a second count that indicates the number of times the commodity product targeted for a purchase registered to the first terminal by the person; and</claim-text><claim-text>evaluating, based on a first count of the first motion and the counted second count, a behavior of the person exhibiting with respect to the purchase of the commodity product.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The non-transitory computer-readable recording medium according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the evaluating the behavior includes determining that the person behaves fraudulently or performs an erroneous operation when a difference between the first count and the second count is greater than or equal to a first threshold and when the first count is greater than or equal to a second threshold, wherein the process further comprises</claim-text><claim-text>notifying, when it is determined that the person behaves fraudulently or performs the erroneous operation, a second terminal of an alert.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The non-transitory computer-readable recording medium according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the process further comprises,<claim-text>specifying, based on the specified object and the skeleton information, a second motion of the person registering the commodity product targeted for the purchase to the first terminal, wherein</claim-text><claim-text>the specifying the first motion includes specifying, based on the specified object and the skeleton information, the first motion when fingers of the person have come out from a region of a specified basket after the fingers entered the region of the specified basket for a predetermined time period, and</claim-text><claim-text>the specifying the second motion includes specifying, based on the specified object and the skeleton information, the second motion when both elbows of the person have not moved for a predetermined time period while being bent forward within a predetermined range of the region of the specified basket.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The non-transitory computer-readable recording medium according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the specifying the person includes tracking, from a plurality of images captured at different time, based on an appearance and an amount of movement of the person, the same person at the different time.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The non-transitory computer-readable recording medium according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the counting the first count and the second count, and the evaluating the behavior are executed when a second motion of the person registering the commodity product targeted for the purchase to the first terminal is not specified after the first motion is specified.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The non-transitory computer-readable recording medium according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the evaluating the behavior includes determining that the person behaves fraudulently or performs the erroneous operation when the person is specified from the image captured therein a sales floor of a high-priced commodity product and when the difference is greater than or equal to a third threshold even when the difference is less than the first threshold.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The non-transitory computer-readable recording medium according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the process further comprises:<claim-text>specifying, from a first image captured therein an area that includes a shelf on which the commodity products in the store are accommodated, a motion of a first person acquiring the commodity product from the shelf;</claim-text><claim-text>specifying, from a second image captured therein an area that includes a self-service checkout terminal, the first person and the self-service checkout terminal;</claim-text><claim-text>storing, in an associated manner, the first person and the self-service checkout terminal;</claim-text><claim-text>receiving a purchase history from the self-service checkout terminal that is associated with the first person; and</claim-text><claim-text>specifying, based on the purchase history, the second count.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The non-transitory computer-readable recording medium according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the process further comprises:<claim-text>specifying an area in which the first person is located in the store when a third motion of the first person putting the commodity product into a basket;</claim-text><claim-text>specifying, based on a type of the commodity product associated with the specified area, a first category item that indicates the type of the commodity product put into the basket out of category items indicating types of a plurality of commodity products;</claim-text><claim-text>counting a third count of the third motion for the specified first category item;</claim-text><claim-text>counting, based on the purchase history, the second count performed with respect to the first category item; and</claim-text><claim-text>evaluating, based on the number of counts of the third motion for the first category item and the second count performed with respect to the first category item, the behavior of the first person exhibiting with respect to the purchase of the commodity product.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The non-transitory computer-readable recording medium according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the first terminal is a terminal that stores therein information on the commodity product targeted for the purchase by scanning a bar code or a QR code attached to the commodity product.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The non-transitory computer-readable recording medium according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the first terminal is a self-service checkout terminal, and</claim-text><claim-text>the first terminal specifies the second count based on information transmitted from a terminal that stores therein information on the commodity product targeted for the purchase by scanning a bar code or a QR code attached to the commodity product.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. An information processing method comprising:<claim-text>specifying, by a computer, from an image that is captured by a camera, a person and a plurality of objects;</claim-text><claim-text>generating, by a computer, by inputting the image of the person into a machine learning model, skeleton information on the person;</claim-text><claim-text>identifying, by a computer, based on the plurality of objects and the skeleton information, a first feature value associated with one or more first motions of the person who retrieves an object from among the plurality of objects;</claim-text><claim-text>identifying, by a computer, a second feature value associated with one or more objects registered to a first terminal by the person from among the plurality of object; and</claim-text><claim-text>generating, by a computer, based on a difference between the first feature value and the second feature value, an alert indicates that an object retrieved by the person is not registered in the first terminal.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. An information processing apparatus, comprising:<claim-text>a memory; and</claim-text><claim-text>a processor coupled to the memory and the processor configured to:<claim-text>specify, from an image that is captured by a camera, a person and a plurality of objects,</claim-text><claim-text>generate, by inputting the image of the person into a machine learning model, skeleton information on the person,</claim-text><claim-text>identify, based on the plurality of objects and the skeleton information, a first feature value associated with one or more first motions of the person who retrieves an object from among the plurality of objects,</claim-text><claim-text>identify a second feature value associated with one or more objects registered to a first terminal by the person from among the plurality of object, and</claim-text><claim-text>generating, based on a difference between the first feature value and the second feature value, an alert indicates that an object retrieved by the person is not registered in the first terminal.</claim-text></claim-text></claim-text></claim></claims></us-patent-application>