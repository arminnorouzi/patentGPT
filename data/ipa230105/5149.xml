<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005150A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005150</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17941053</doc-number><date>20220909</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>204</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>1</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0014</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>204</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>1</main-group><subgroup>00009</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>1</main-group><subgroup>00193</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20160201</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>34</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEMS, METHODS, AND COMPUTER-READABLE MEDIA FOR DETECTING IMAGE DEGRADATION DURING SURGICAL PROCEDURES</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17256422</doc-number><date>20201228</date></document-id><parent-status>PENDING</parent-status><parent-pct-document><document-id><country>WO</country><doc-number>PCT/US2019/038869</doc-number><date>20190625</date></document-id></parent-pct-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17941053</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62693530</doc-number><date>20180703</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Covidien LP</orgname><address><city>Mansfield</city><state>MA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Meglan</last-name><first-name>Dwight</first-name><address><city>Westwood</city><state>MA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Methods, systems, and computer-readable media for detecting image degradation during a surgical procedure are provided. A method includes receiving images of a surgical instrument; obtaining baseline images of an edge of the surgical instrument; comparing a characteristic of the images of the surgical instrument to a characteristic of the baseline images of the edge of the surgical instrument, the images of the surgical instrument being received subsequent to obtaining the baseline images of the edge of the surgical instrument and being received while the surgical instrument is disposed at a surgical site in a patient; determining whether the images of the surgical instrument are degraded, based on the comparing of the characteristic of the images of the surgical instrument and the characteristic of the baseline images of the surgical instrument; and generating an image degradation notification, in response to a determination that the images of the surgical instrument are degraded.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="234.95mm" wi="151.89mm" file="US20230005150A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="212.26mm" wi="156.46mm" orientation="landscape" file="US20230005150A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="201.25mm" wi="141.73mm" file="US20230005150A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="235.12mm" wi="143.59mm" file="US20230005150A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="156.38mm" wi="146.64mm" file="US20230005150A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="258.23mm" wi="167.64mm" file="US20230005150A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The present application is a Continuation of U.S. patent application Ser. No. 17/256,422, filed Dec. 28, 2020, which is a U.S. National Stage Application filed under 35 U.S.C. &#xa7; 371(a) of International Patent Application Serial No. PCT/US2019/038869, filed Jun. 25, 2019, which claims the benefit of and priority to U.S. Provisional Patent Application Ser. No. 62/693,530, filed Jul. 3, 2018, the entire disclosure of each of which is incorporated by reference herein.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Robotic surgical systems may be used in minimally invasive surgical procedures. During a robotic surgical procedure, a surgeon controls a robotic surgical arm with a user interface at a remote surgeon console. The user interface allows the surgeon to manipulate a surgical instrument coupled to the robotic arm and to control a camera to receive images of a surgical site within a patient.</p><p id="p-0004" num="0003">The surgeon console may include a stereoscopic display, sometimes referred to as a three-dimensional (3D) display. In this regard, sometimes in conjunction with a corresponding pair of stereoscopic eyeglasses worn by the surgeon, such displays facilitate depth perception from an image by presenting the image to the surgeon as a pair of distinct images separately provided to the left and right eyes, respectively. The stereoscopic display may display images provided by a stereoscopic endoscope. Stereoscopic endoscopes employ two signal paths, usually a left-eye view and a right-eye view, which are matched and interdigitated to generate a stereoscopic image. As does typically occur during surgical procedures, biological material or other procedure-related material may occlude one of the lenses of the stereoscopic endoscope, thereby degrading the images provided to the display. In the case of a stereoscopic endoscope, this degradation has potential side effects upon the surgeon through the now mismatched stereoscopic image pairs which can cause perception issues that tax the surgeon's visual and cognitive pathways without the surgeon's awareness. This can result in degraded performance of the surgeon in perceiving and responding to the observed stereoscopic information. Thus, it is useful to be able to detect these mismatch situations and inform the surgeon of the need to correct the situation.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">Disclosed according to embodiments of the present disclosure are methods for detecting image degradation during a surgical procedure. In an aspect of the present disclosure, an illustrative method includes receiving images of a surgical instrument, obtaining baseline images of an edge of the surgical instrument, comparing a characteristic of the images of the surgical instrument to a characteristic of the baseline images of the edge of the surgical instrument, the images of the surgical instrument being received subsequent to the obtaining of the baseline images of the edge of the surgical instrument and being received while the surgical instrument is disposed at a surgical site in a patient, determining whether the images of the surgical instrument are degraded, based on the comparing of the characteristic of the images of the surgical instrument and the characteristic of the baseline images of the edge of the surgical instrument, and generating an image degradation notification, in response to a determination that the images of the surgical instrument are degraded.</p><p id="p-0006" num="0005">In a further aspect of the present disclosure, the images of the surgical instrument are received by an image capture device.</p><p id="p-0007" num="0006">In another aspect of the present disclosure, the image capture device is a stereoscopic endoscope including a left-eye lens and a right-eye lens.</p><p id="p-0008" num="0007">In a further aspect of the present disclosure, the characteristic of the baseline images of the edge of the surgical instrument is obtained during an initial image capture device calibration.</p><p id="p-0009" num="0008">In a further aspect of the present disclosure, the method further includes periodically receiving images of the edge of the surgical instrument at a predetermined interval.</p><p id="p-0010" num="0009">In another aspect of the present disclosure, the determination that the images of the surgical instrument are degraded is based at least in part on a difference between the characteristic of the received images of the surgical instrument and the characteristic of the baseline images of the edge of the surgical instrument being greater than a threshold value.</p><p id="p-0011" num="0010">In yet another aspect, the method further includes determining the characteristic of the images of the surgical instrument by computing a modulation transfer function derived from the received images of the surgical instrument.</p><p id="p-0012" num="0011">Disclosed according to embodiments of the present disclosure are systems for detecting image degradation during a surgical procedure. In an aspect of the present disclosure, an illustrative system includes a surgical instrument including at least one edge, an image capture device configured to capture images of the surgical instrument, the images of the surgical instrument including a characteristic, a display device, at least one processor coupled to the image capture device and the display device, and a memory coupled to the at least one processor and having stored thereon a characteristic of baseline images of the edge of the surgical instrument, and instructions which, when executed by the at least one processor, cause the at least one processor to obtain the characteristic of the baseline images of the edge of the surgical instrument, receive the images of the surgical instrument, compare a characteristic of the images of the surgical instrument to the characteristic of the baseline images of the edge of the surgical instrument, the images of the surgical instrument being received subsequent to the obtaining of the characteristic of the baseline images of the edge of the surgical instrument and being received while the surgical instrument is disposed at a surgical site in a patient, determine whether the images of the surgical instrument are degraded, based on the comparing of the characteristic of the images of the surgical instrument and the characteristic of the baseline images of the edge of the surgical instrument, and generate an image degradation notification, in response to a determination that the images of the surgical instrument are degraded.</p><p id="p-0013" num="0012">Disclosed according to embodiments of the present disclosure are non-transitory computer-readable media storing instructions for detecting image degradation during a surgical procedure. In an aspect of the present disclosure, an illustrative non-transitory computer-readable medium stores instructions which, when executed by a processor, cause the processor to receive images of a surgical instrument, obtain baseline images of an edge of the surgical instrument, compare a characteristic of the images of the surgical instrument to a characteristic of the baseline images of the edge of the surgical instrument, the images of the surgical instrument being received subsequent to obtaining the baseline images of the edge of the surgical instrument and being received while the surgical instrument is disposed at a surgical site in a patient, determine whether the images of the surgical instrument are degraded, based on the comparison of the characteristic of the images of the surgical instrument and the characteristic of the baseline images of the edge of the surgical instrument, and generate an image degradation notification, in response to a determination that the images of the surgical instrument are degraded.</p><p id="p-0014" num="0013">Any of the above aspects and embodiments of the present disclosure may be combined without departing from the scope of the present disclosure.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0015" num="0014">Objects and features of the presently disclosed systems, methods, and computer-readable media will become apparent to those of ordinary skill in the art when descriptions of various embodiments thereof are read with reference to the accompanying drawings, wherein:</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram of a robotic surgical system, in accordance with embodiments of the present disclosure;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a simplified perspective view of an image capture device and a surgical instrument, in accordance with embodiments of the present disclosure;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an image of a surgical site received from an image capture device, in accordance with the present disclosure;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a test pattern formed by a difference in luminance between a surgical instrument and surrounding anatomical material, in accordance with an embodiment of the present disclosure;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows images of a test patterns which may be disposed on a surgical instrument, in accordance with another embodiment of the present disclosure;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a modulation transfer function graph for the test pattern of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, in accordance with the present disclosure; and</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating an example method of detecting image degradation, in accordance with the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0023" num="0022">The present disclosure generally relates to dynamic detection of image degradation, and providing associated notifications, during a surgical procedure. In order to determine an amount of degradation occurring during a surgical procedure, endoscopic calibration techniques may be used. Prior to its use, and/or at the start of a surgical procedure, an endoscopic imaging system may be calibrated. Calibration, prior to use, includes the process of determining and recording base parameters, at peak or near-peak operating conditions for the imaging system by using a calibration target. Calibration prior to the use of the endoscopic system thus provides a baseline metric of the endoscopic system before the occurrence of degradation. During a surgical procedure, a similar technique as that employed during calibration may be used to determine current parameters of the endoscopic imaging system. By automatically and dynamically comparing the current parameters with those of the base parameters, endoscopic image degradation can be determined.</p><p id="p-0024" num="0023">To that end, the present disclosure relates to systems, methods, and computer-readable media for enabling dynamic detection of image degradation of images of a surgical site during a surgical procedure, and for generating and displaying image degradation notifications during the surgical procedure. In this regard, during calibration of the image capture device, one or more baseline parameters of the image capture device, based on calibration targets such as test patterns and/or edges of tools within the image capture device's field of view, are determined and recorded. During the surgical procedure, images of the surgical site are captured by the image capture device and provided to a computing device, such as a control console, for processing, using a similar technique as that employed during calibration, to determine one or more current parameters of the image capture device. By dynamically comparing the current parameter(s) with the baseline parameter(s) of the image capture device, a determination regarding image degradation can be made.</p><p id="p-0025" num="0024">As used herein, the terms &#x201c;clinician,&#x201d; &#x201c;surgeon,&#x201d; &#x201c;observer,&#x201d; and/or &#x201c;viewer&#x201d; generally refer to a user of a stereoscopic display device described herein. Additionally, although the terms &#x201c;first eye&#x201d; and &#x201c;second eye&#x201d; are used herein to refer to a left eye and a right eye, respectively, of a user, this use is provided by way of example and should not be construed as limiting. Throughout this description, the term &#x201c;proximal&#x201d; refers to the portion of the device or component thereof that is farthest away from the patient (and thus closest to the clinician and/or surgical robot) and the term &#x201c;distal&#x201d; refers to the portion of the device or component thereof that is closest to the patient (and thus furthest away from the clinician and/or surgical robot). Further, as referred herein, the term &#x201c;signal path&#x201d; (whether right-eye or left-eye) refers to an optical-electrical-optical signal path whereby images are captured optically, converted to an electrical/digital signal to be transmitted, and again converted back to an optical image when received by a computing or display device. While the illustrative embodiments below describe a robotic surgical system, those skilled in the art will recognize that the systems, methods, and computer-readable media described herein may also be used in other surgical procedures, for example minimally-invasive surgical procedures, where a patient image capture device is used to capture images of a surgical site. Thus, the present disclosure is not intended to be limited to the exemplary embodiments using a robotic surgical system, as described hereinbelow.</p><p id="p-0026" num="0025">With reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a robotic surgical system <b>1</b> is illustrated, and generally includes a surgical robot <b>10</b>, a controller <b>30</b>, at least one processor <b>32</b>, at least one memory <b>35</b>, and a user interface console <b>40</b>. Surgical robot <b>10</b> generally includes one or more robotic arms <b>12</b> and a base <b>18</b>. Robotic arms <b>12</b> may be in the form of arms or linkages each having an end <b>14</b> that supports a surgical instrument <b>250</b>. Surgical instrument <b>250</b> may be any type of instrument usable with robotic arm <b>12</b>, such as a grasper, a knife, scissors, staplers, and/or the like. One or more of robotic arms <b>12</b> may include an imaging device <b>16</b> for imaging a surgical site &#x201c;S,&#x201d; as also shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0027" num="0026">Controller <b>30</b> includes, and/or is communicatively coupled to, the at least one processor <b>32</b> and memory <b>35</b>, and may be integrated with user interface <b>40</b> or provided as a standalone device within the operating theater. As described in further detail below, processor <b>32</b> executes instructions (not shown) stored in memory <b>35</b> to perform steps and/or procedures of the various embodiments described herein. As will be appreciated, the implementation of processor <b>32</b> and memory <b>35</b> is provided by way of example only and should not be construed as limiting. For instance, steps and/or procedures of any of the embodiments of the present disclosure may be implemented by hardware components, firmware components, software components, and/or any combination thereof.</p><p id="p-0028" num="0027">User interface <b>40</b> communicates with base <b>18</b> through controller <b>30</b> and includes a display device <b>44</b> which is configured to display stereoscopic images of the surgical site &#x201c;S.&#x201d; The images are captured by an imaging device (also referred to as &#x201c;image capture device&#x201d;) <b>16</b> and/or captured by imaging devices that are positioned about the surgical theater (e.g., an imaging device positioned adjacent patient &#x201c;P,&#x201d; and/or an imaging device <b>56</b> positioned at a distal end of an imaging arm <b>52</b>). Imaging devices (e.g., imaging devices <b>16</b>, <b>56</b>) may capture optical images, infra-red images, ultrasound images, X-ray images, thermal images, and/or any other known real-time images of surgical site &#x201c;S.&#x201d; Imaging devices <b>16</b>, <b>56</b> transmit captured images to controller <b>30</b> for processing, such as by processor <b>32</b>, and transmits the captured and/or processed images to display device <b>44</b> for display. In one embodiment, one or both of imaging devices <b>16</b>, <b>56</b> are stereoscopic endoscopes capable of capturing images of surgical site &#x201c;S&#x201d; via a right-eye lens <b>210</b> and a left-eye lens <b>220</b>, as further described in the description of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0029" num="0028">In further embodiments, user interface <b>40</b> may include or be associated with a portable display device <b>45</b>, which, similar to display device <b>44</b>, is configured to permit the user to view the stereoscopic images in a manner that the user perceives a three-dimensional and/or depth effect from the stereoscopic images. Portable display device <b>45</b> may be goggles, glasses, or any other portable or semi-portable display device, which may be used to allow the user to view stereoscopic images.</p><p id="p-0030" num="0029">User interface <b>40</b> further includes input handles attached to gimbals <b>70</b> which allow a clinician to manipulate surgical robot <b>10</b> (e.g., move robotic arms <b>12</b>, ends <b>14</b> of robotic arms <b>12</b>, and/or surgical instrument <b>250</b>). Each of gimbals <b>70</b> is in communication with controller <b>30</b> and processor <b>32</b> to transmit control signals thereto and to receive feedback signals therefrom. Additionally or alternatively, each of gimbals <b>70</b> may include control interfaces or input devices (not shown) which allow the surgeon to manipulate (e.g., clamp, grasp, fire, open, close, rotate, thrust, slice, etc.) surgical instrument <b>250</b> supported at ends <b>14</b> of robotic arms <b>12</b>.</p><p id="p-0031" num="0030">Each of gimbals <b>70</b> is moveable to move ends <b>14</b> of robotic arms <b>12</b> within surgical site &#x201c;S.&#x201d; The stereoscopic images displayed on display device <b>44</b> are oriented such that movement of gimbals <b>70</b> moves ends <b>14</b> of robotic arms <b>12</b> as viewed on display device <b>44</b>. It will be appreciated that the orientation of the stereoscopic images on display device <b>44</b> may be mirrored or rotated relative to a view from above patient &#x201c;P.&#x201d; In addition, it will be appreciated that the size of the stereoscopic images displayed on display device <b>44</b> may be scaled to be larger or smaller than the actual structures of surgical site &#x201c;S&#x201d; permitting the surgeon to have a better view of structures within surgical site &#x201c;S.&#x201d; As gimbal <b>70</b> is moved, surgical instrument <b>250</b> are moved within surgical site &#x201c;S.&#x201d; Movement of surgical instrument <b>250</b> may also include movement of ends <b>14</b> of robotic arms <b>12</b> which support surgical instrument <b>250</b>. In addition to gimbals <b>70</b>, one or more additional input devices may be included as part of user interface <b>40</b>, such as a handle including a clutch switch, a touchpad, joystick, keyboard, mouse, or other computer accessory, and/or a foot switch, pedal, trackball, or other actuatable device configured to translate physical movement from the clinician into signals sent to processor <b>32</b>.</p><p id="p-0032" num="0031">As noted briefly above, to provide the user with a view of surgical site &#x201c;S&#x201d; during a surgical procedure, one or more of imaging devices <b>16</b>, <b>56</b> may be a stereoscopic endoscope disposed about surgical site &#x201c;S,&#x201d; such as adjacent to surgical instrument <b>250</b>, and configured to capture images of surgical site &#x201c;S&#x201d; to be displayed as stereoscopic images on display device <b>44</b>.</p><p id="p-0033" num="0032">Turning now to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a simplified, perspective view of a distal end of image capture device <b>200</b>, such as imaging devices <b>16</b>, <b>56</b>, and a distal end of surgical instrument <b>250</b> are provided, in accordance with an embodiment of the present disclosure. Image capture device <b>200</b> captures images of surgical site &#x201c;S&#x201d; via right-eye lens <b>210</b> and left-eye lens <b>220</b> to provide two distinct view point images that are transmitted to processor <b>32</b> for processing, and to display device <b>44</b> for display. Image capture device <b>200</b> includes a body <b>202</b>, which includes, at its distal end, a lens assembly including right-eye lens <b>210</b> and left-eye lens <b>220</b>. Right-eye lens <b>210</b> and left-eye lens <b>220</b> are each associated with a respective right-eye signal path and a left-eye signal path to provide the captured images of surgical site &#x201c;S&#x201d; to processor <b>32</b> and display device <b>44</b>.</p><p id="p-0034" num="0033">Surgical instrument <b>250</b> is illustrated as a vessel sealing device, which includes a body <b>242</b> having a surface <b>252</b>. Those skilled in the art will recognize that this illustrative surgical instrument <b>250</b> is provided merely as an example, and that any other surgical tool or device may be substituted for the illustrated vessel sealing device without departing from the scope of the present disclosure. In some embodiments, one or more test patterns (for example, test pattern <b>255</b><i>a</i>) are included on surgical instrument <b>250</b>. The test pattern is an identifier, for example a unique identifier, that can be used to distinguish surgical instrument <b>250</b> from a background during image processing. For example, as depicted in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, test pattern <b>255</b><i>a </i>is disposed on surface <b>252</b> of surgical instrument <b>250</b> and may be black and white alternating solid bars. In other embodiments, the test pattern may be any pattern, shapes, colors, or design providing contrast to be used in the detection of degradation of lenses <b>210</b>, <b>220</b> of image capture device <b>200</b> and/or of images captured by image capture device <b>200</b>.</p><p id="p-0035" num="0034">In another embodiment, as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, test pattern <b>255</b><i>b </i>may be made up of increasingly thinner alternating solid black and white bars. Using such a pattern permits image capture device <b>200</b> to distinguish between where a black bar ends and a white bar begins. For example, and as described in greater detail in the description of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, as the bars become thinner from one end of test pattern <b>255</b><i>b </i>to the other, the distinction between the bars becomes increasingly more difficult to detect. In still another embodiment, the test pattern is a passive test pattern, which is not detectable by the user during a surgical procedure. For example, the passive test pattern may be disposed on surface <b>252</b> of surgical instrument <b>250</b> in a manner that reflects light in an infrared or ultraviolet range.</p><p id="p-0036" num="0035">In still another embodiment, test patterns <b>255</b><i>a</i>, <b>255</b><i>b </i>extend along an entire outer area of surface <b>252</b> of surgical instrument <b>250</b>. Alternatively, it is contemplated that test patterns <b>255</b><i>a</i>, <b>255</b><i>b </i>may be located at discrete locations on surface <b>252</b> of surgical instrument <b>250</b>. In a further embodiment, it is contemplated that any of test patterns <b>255</b><i>a</i>, <b>255</b><i>b </i>may include specific markers, such as specialized shapes, which enhance the ability of image capture device <b>200</b> to determine that test patterns <b>255</b><i>a</i>, <b>255</b><i>b </i>are present within the received image of surgical site &#x201c;S.&#x201d; In embodiments, it is contemplated that test patterns <b>255</b><i>a</i>, <b>255</b><i>b </i>correspond to a type of surgical instrument so that each different surgical instrument <b>250</b> or type of surgical instrument <b>250</b> (for example, ablation device, dissection device, stapler, vessel sealing device, etc.) has a unique test pattern, which can be used to, where necessary, identify surgical instrument <b>250</b>.</p><p id="p-0037" num="0036">In still another embodiment, a pseudo test pattern is generated using a contrast between surgical instrument <b>250</b> and the background of an image of surgical instrument <b>250</b>. The pseudo test pattern may be a proxy or substitute for an actual test pattern, such as test pattern <b>255</b><i>a</i>, <b>255</b><i>b</i>. In some embodiments, one or more geometric characteristics of surgical instrument <b>250</b> may be used as a pseudo test pattern and/or may be used to define a region of an image that acts as a pseudo test pattern. In one illustrative embodiment, one or more edges of surgical tool <b>250</b> are used to define a pseudo test pattern, e.g., the regions of the image on both sides of an edge of surgical tool <b>250</b> define a pseudo test pattern. For example, referring now to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, image capture device <b>200</b> (not shown), using lenses <b>210</b>, <b>220</b> continually captures images of surgical site &#x201c;S.&#x201d; Surgical site &#x201c;S&#x201d; includes anatomical material <b>230</b>, which may include tissue, bone, blood vessels, and/or other biological material, and surgical instrument <b>250</b>, which includes edges <b>251</b>, is positioned within surgical site &#x201c;S.&#x201d; Although shown as a single image, each of right-eye lens <b>210</b> and left-eye lens <b>220</b> captures a different image of surgical site &#x201c;S,&#x201d; which is displayed by display device <b>44</b> as a stereoscopic image of surgical site &#x201c;S.&#x201d; As briefly alluded to above, test patterns need not be disposed on surface <b>252</b> of surgical instrument <b>250</b>. Rather, a pseudo test pattern <b>255</b><i>c </i>may be created by the differences in luminance between surgical instrument <b>250</b> extending to edges <b>251</b> and surrounding anatomical matter <b>230</b> of surgical site &#x201c;S.&#x201d; In particular, the edges <b>251</b> of surgical tool <b>250</b> as captured in images by image capture device <b>200</b>, may be substituted for a test pattern <b>255</b><i>a</i>, <b>255</b><i>b</i>, and, as further described below, the edges <b>251</b> of surgical instrument <b>250</b>, and particularly the contrast/difference between the luminance of surgical instrument <b>250</b> proximate edges <b>251</b> and the luminance of the background, e.g. surgical site &#x201c;S,&#x201d; may be used as a pseudo test pattern <b>225</b><i>c </i>instead of actual test patterns <b>225</b><i>a</i>, <b>225</b><i>b</i>. While differences in contrast between any edges of surgical instrument <b>250</b> may be used to define a pseudo test pattern, in some embodiments slanted edges, e.g. straight edges that are at non-orthogonal angles as viewed by image capture device <b>200</b>, are used. For example, analysis software may be used to look for and identify edges that are not substantially distant from, or off of, horizontal or vertical, e.g., between about 5&#xb0; and 10&#xb0; away.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates another view of surgical site &#x201c;S,&#x201d; showing surgical instrument <b>250</b> placed within surgical site &#x201c;S&#x201d; and the surrounding anatomical material <b>230</b>. A pseudo test pattern <b>255</b><i>c </i>is created from a section of the image of surgical site &#x201c;S&#x201d; including at least one edge <b>251</b> of surgical instrument <b>250</b>. For example, a section an image of surgical site &#x201c;S&#x201d; including surgical instrument <b>250</b> and surrounding anatomical material <b>230</b> may be used to generate pseudo test pattern <b>255</b><i>c</i>, which may be based on the differences in luminance between surgical instrument <b>250</b>, extending to edges <b>251</b>, and surrounding anatomical material <b>230</b>. For example, pseudo test pattern <b>255</b><i>c </i>may include a darker section contrasted with a brighter section on opposing side of edge <b>251</b> of surgical tool <b>250</b>, thereby forming two zones of different and contrasted luminance created by surface <b>252</b> of surgical instrument <b>250</b> leading up to edges <b>251</b> of surgical instrument <b>250</b>, and surrounding anatomical material <b>230</b>. As will be appreciated by those skilled in the art, the brighter section and the darker section may respectively correspond to the surface <b>252</b> of surgical tool <b>250</b> and the surrounding anatomical material <b>230</b>, and, depending on the type of surgical instrument <b>250</b> and/or image capture device <b>200</b>, may be reversed in terms of which correspond to the brighter and darker sections.</p><p id="p-0039" num="0038">Referring now to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, an illustrative comparison is provided of an image of a test pattern captured during calibration of image capture device <b>200</b>, and an image of the test pattern captured during a surgical procedure. For purpose of example, the illustrative comparison described below will refer to an image of test pattern <b>255</b><i>b </i>as disposed on surgical instrument <b>250</b> captured during calibration of image capture device <b>200</b>, and an image of test pattern <b>255</b><i>b</i>&#x2032; captured during the surgical procedure. However, those skilled in the art will recognize that other test patterns, such as described above, may also be used and/or substituted for test pattern <b>255</b><i>b </i>without departing from the scope of the present disclosure. The image of test pattern <b>255</b><i>b </i>is an image captured during calibration of image capture device <b>200</b>, while the image of test pattern <b>255</b><i>b</i>&#x2032; is the image received by image capture device <b>200</b> at some point during a surgical procedure. Due to occlusion of and/or anatomical material <b>230</b> coming into contact with lenses <b>210</b>, <b>220</b>, the image of test pattern <b>255</b><i>b</i>&#x2032; is slightly blurred, or degraded, as compared to the image of test pattern <b>255</b><i>b</i>. As test patterns <b>255</b><i>b</i>, <b>255</b><i>b</i>&#x2032; are viewed from left to right, each can be broken into groups of increasingly thinner width black bars and white bars. The contrast between black bars and white bars of test patterns <b>255</b><i>b</i>, <b>255</b><i>b</i>&#x2032; is easily distinguished for all of area &#x201c;X.&#x201d; However, within area &#x201c;Y,&#x201d; it becomes increasingly difficult to distinguish the locations of black bars, white bars, and areas of transition of each.</p><p id="p-0040" num="0039">In an embodiment, the image of test pattern <b>255</b><i>b</i>&#x2032; is used to determine a value of a modulation transfer function (&#x201c;MTF&#x201d;). For example, the MTF is used to provide a measure of the transfer of contrast of test pattern <b>255</b><i>b</i>&#x2032; and how well lenses <b>210</b>, <b>220</b> of image capture device <b>200</b> reproduce (or transfer) the detail of test pattern <b>255</b><i>b </i>in a captured image. By obtaining an image of test pattern <b>255</b><i>b </i>or other test patterns (for example, test pattern <b>255</b><i>a</i>, pseudo test pattern <b>255</b><i>c</i>, and/or the like) during calibration of image capture device <b>200</b>, baseline characteristics of the image of test pattern <b>255</b><i>b </i>or other test patterns can be determined. During use of image capture device <b>200</b>, the ability of lenses <b>210</b>, <b>220</b> of image capture device <b>200</b> to continue to reproduce (or transfer) the detail of the test pattern is determined by applying the MTF to the received image of the test pattern to yield one or more determined characteristics of the image of the test pattern. Over time, the determined characteristic(s) of the image of the test pattern may change due to degradation and/or occlusion of lenses <b>210</b>, <b>220</b> of image capture device <b>200</b>, for example, as shown in the image of test pattern <b>255</b><i>b</i>&#x2032;. As described below with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the baseline characteristics of the test pattern can be compared to the determined characteristic(s) of test pattern <b>255</b><i>b</i>&#x2032; as image capture device <b>200</b> is used during a surgical procedure in order to determine when image degradation has occurred.</p><p id="p-0041" num="0040">In order to calculate the MTF for both the baseline characteristics of the test pattern and determined characteristic(s) of the test pattern, processor <b>32</b> is used to differentiate the black bars and white bars of test pattern <b>255</b><i>b</i>, or in the case of pseudo test pattern <b>255</b><i>c</i>, the darker sections and the brighter sections of the captured image proximate edges <b>251</b> of surgical instrument <b>250</b>. Based on the differences between the black bars and white bars, or darker sections and brighter sections, a sinusoidal graph can be plotted. Turning now to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a graph of the contrast between black bars and white bars of test pattern <b>255</b><i>b</i>&#x2032; of <figref idref="DRAWINGS">FIG. <b>5</b></figref> is illustrated. The graph of <figref idref="DRAWINGS">FIG. <b>6</b></figref> is used to calculate the MTF of test patterns <b>255</b><i>b</i>&#x2032;. In some embodiments, such as depicted in the graph of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, an amplitude of 100 is assigned to areas containing black bars and an amplitude of &#x2212;100 is assigned for areas containing white bars. As test pattern <b>255</b><i>b </i>transitions between black bars and white bars the plot of the graph increases or decreases between the amplitude of &#x2212;100 and 100.</p><p id="p-0042" num="0041">Additionally, as the widths of the black bars and white bars of test pattern <b>255</b><i>b</i>&#x2032; decrease, the ability of processor <b>32</b> to continue to differentiate between black bars and white bars may likewise decrease. As such, the peak amplitudes of the graph may no longer approach peak amplitudes of &#x2212;100 and 100 such that, eventually, the width of the black bars and white bars becomes so thin that processor <b>32</b> can no longer distinguish the black bars from white bars of test pattern <b>225</b><i>b</i>&#x2032; and the peak amplitude settles at 0. In some embodiments, a value of 100% may be assigned where the peak amplitude is between &#x2212;100 and 100 and a value of 0% may be assigned where the peak amplitudes settles at 0. For peak amplitudes between &#x2212;100 and 100, a corresponding percentage between 100% and 0% can be assigned. The MTF is typically expressed as a percentage of the distinguishable contrast between black bars and white bars based on the line widths per picture height (LW/PH). Thus, as the line widths (width of black bars and white bars) of test pattern <b>255</b><i>b</i>&#x2032; become increasingly thinner, the percentage representing the contrast between the black and white bars decreases.</p><p id="p-0043" num="0042">By assigning a value to the groups of increasingly thinner widths of black bars and white bars in the form of the LW/PH, the MTF percentage may correspond to the LW/PH. For example, if a portion of the graph of <figref idref="DRAWINGS">FIG. <b>6</b></figref> ranges between peak amplitudes of 100 and &#x2212;100 for a LW/PH as small as 100 mm, the MTF is 100% at 100 mm. If another portion of the graph of <figref idref="DRAWINGS">FIG. <b>6</b></figref> ranges between peak amplitudes of 50 and &#x2212;50 for the LW/PH at 50 mm, the MTF is 50% at 50 mm. These values of the MTF are determined by processor <b>32</b> and stored in memory <b>35</b>.</p><p id="p-0044" num="0043">In further embodiments, the MTF percentages may be converted from percentages ranging between 0% to 100% to corresponding values ranging between 0 and 1, wherein 0 corresponds to processor <b>32</b> being incapable of distinguishing black bars from white bars, and 1 corresponds to processor <b>32</b> being able to completely distinguish the black bars from white bars. It is further contemplated that processor <b>32</b> is capable of determining the MTF of test pattern <b>255</b><i>b</i>&#x2032; as it is received by way of each of right-eye lens <b>210</b> and left-eye lens <b>220</b>, independently. Thus, image degradation for images captured by way of each of right-eye lens <b>210</b> and left-eye lens <b>220</b> may be detected.</p><p id="p-0045" num="0044">While the above description of <figref idref="DRAWINGS">FIGS. <b>5</b> and <b>6</b></figref> refer to test pattern <b>255</b><i>b </i>as an example, similar determinations and calculations may be performed when pseudo test pattern <b>255</b><i>c </i>is substituted for test pattern <b>255</b><i>b</i>. For example, in embodiments, one or more slanted edges <b>251</b> of surgical instrument <b>250</b> may be used to calculate the MTF. In such embodiments, there may not be a test pattern <b>255</b><i>a</i>, <b>255</b><i>b </i>disposed on surgical instrument <b>250</b>. Instead, the contrast between brighter sections and darker sections, corresponding to either the surface <b>252</b> proximate edge <b>251</b> of surgical instrument <b>250</b> or the surrounding anatomical material <b>230</b>, respectively. Further information regarding computing the MTF based on a slanted edge <b>251</b> of surgical instrument <b>250</b> is described in <i>Slanted</i>-<i>Edge MTF for Digital Camera and Scanner Analysis</i>, by Peter D. Burns, Proc. IS&#x26;T 2000 PICS Conference, pg. 135-138 (2000), and Sharpness: What is it and how is it measured?, http://www.imatest.com/docs/sharpness (last visited Sep. 11, 2017), the entire contents of each of which are incorporated herein by reference.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is flowchart of a method <b>700</b> of detecting image degradation of images captured by image capture device <b>200</b> during a surgical procedure, in accordance with embodiments of the present disclosure. Method <b>700</b> may be implemented, at least in part, by processor <b>32</b> executing instructions stored in memory <b>35</b>. Additionally, the particular sequence of steps shown in method <b>700</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref> is provided by way of example and not limitation. Thus, the steps of method <b>700</b> may be executed in sequences other than the sequence shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref> without departing from the scope of the present disclosure. Further, some steps shown in method <b>700</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref> may be concurrently executed with respect to one another instead of sequentially executed with respect to one another, and/or may be repeated or omitted without departing from the scope of the present disclosure.</p><p id="p-0047" num="0046">Generally, prior to the execution of method <b>700</b>, calibration of image capture device <b>200</b> will have been performed. For example, during a factory calibration process, image capture device <b>200</b> may receive, or the memory <b>32</b> may have stored therein, left-eye and right-eye images of one or more test patterns. The test patterns may be similar to test patterns <b>255</b><i>a</i>, <b>255</b><i>b </i>disposed on surface <b>252</b> of surgical device <b>250</b>, or may be data related to the contrast between the edges of surgical device <b>250</b> and a surrounding environment, thereby generating a pseudo test pattern <b>255</b><i>c</i>. In another example, the calibration process may be performed at the start of a surgical procedure. In an embodiment in which images of the test patterns (&#x201c;baseline images&#x201d;) are received by image capture device <b>200</b> during calibration, a pattern analysis function, such as the modulation transfer function (MTF), is applied to the test pattern to calculate output values. The output values may represent the sharpness or clarity of a transition across features making up the test pattern and may be expressed as line widths per picture height (LW/PH). The calculated output values may be included as one or more characteristics of the baseline images captured by the right-eye lens <b>210</b> and the left-eye lens <b>220</b> of the image capture device <b>200</b> (&#x201c;baseline characteristics&#x201d;), and can be stored in the memory <b>32</b> for later use, as will be described in detail below. Alternatively, these baseline characteristics may be known values that are stored in the memory <b>32</b>. Additionally, during a calibration process, image capture device <b>200</b> may store the baseline characteristics of the test pattern in memory.</p><p id="p-0048" num="0047">In any case, surgical system <b>10</b> is configured to permit the user to begin the surgical procedure within surgical site &#x201c;S,&#x201d; at step <b>705</b>. For example, in the case of a robotic surgical procedure, the user moves the gimbals <b>70</b> to thereby position image capture device <b>200</b> and surgical instrument <b>250</b> about surgical site &#x201c;S&#x201d;. In some embodiments, the field of view of image capture device <b>200</b> may initially be aligned with surgical instrument <b>250</b> to enable image capture device <b>200</b> to capture images of surgical instrument <b>250</b>, and particularly test patterns <b>255</b><i>a</i>, <b>255</b><i>b</i>, via right-eye lens <b>210</b> and left-eye lens <b>220</b>, respectively. Alternatively, as will be appreciated by those skilled in the art, in non-robotic minimally-invasive surgical procedures, image capture device <b>200</b> and surgical instrument <b>250</b> may be positioned manually about surgical site &#x201c;S.&#x201d;</p><p id="p-0049" num="0048">Once suitably positioned, image capture device <b>200</b> captures images of surgical site &#x201c;S,&#x201d; and transmits the captured images to controller <b>30</b> at step <b>710</b>. In addition to tissue and surrounding anatomical material <b>230</b> on which the surgical procedure is being performed, the captured images show surgical instrument <b>250</b> as it is being manipulated by the user. The captured images may be stereoscopic images, that is, left-eye images and right-eye images.</p><p id="p-0050" num="0049">After receiving the captured images from image capture device <b>200</b>, processor <b>32</b> processes the captured images to identify a test pattern, at step <b>715</b>. As described herein, it is contemplated that the test pattern may be a pattern disposed on surface <b>252</b> of surgical instrument <b>250</b> or a test pattern formed by the contrast been surgical instrument <b>250</b> and surrounding anatomical material <b>230</b> about a slanted edge <b>251</b> of surgical instrument <b>250</b>. As noted above, examples of test patterns include but are not limited to test patterns <b>255</b><i>a</i>, <b>255</b><i>b </i>and pseudo test pattern <b>255</b><i>c</i>, and/or the like. In an embodiment, a suitable algorithm is applied to the left-eye images and the right-eye images of surgical site &#x201c;S&#x201d; to output a result, which is analyzed by processor <b>32</b> to detect a presence of the test pattern.</p><p id="p-0051" num="0050">Optionally, in an embodiment where test pattern <b>255</b><i>b </i>is disposed on surgical instrument <b>250</b>, at step <b>720</b>, after identifying test pattern <b>255</b><i>b</i>&#x2032;, a determination is made as to whether test pattern <b>255</b><i>b</i>&#x2032; matches a known test pattern. For example, a database of known test patterns may be stored in memory <b>35</b>, for example, in a look-up table. The images of test pattern <b>255</b><i>b</i>&#x2032; captured by image capture device <b>200</b> is compared with the known test pattern images stored in memory <b>35</b>. In an embodiment, each of the known test patterns is associated with a different surgical instrument. As such, matching test pattern <b>255</b><i>b</i>&#x2032; with the known test pattern further includes identifying the surgical instrument corresponding to the known test pattern. In an embodiment, the identification of the surgical instrument is provided to the user via display device <b>44</b> or through an audio device. For illustrative purposes and provided by way of example, pseudo test pattern <b>255</b><i>c</i>, that is, the slanted edges <b>251</b> of surgical instrument <b>250</b>, is used as the exemplary test pattern for the remaining description of <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0052" num="0051">At step <b>725</b>, the one or more baseline characteristics of the test pattern (&#x201c;characteristic(s) of the baseline image of the test pattern&#x201d;), generated and/or calculated during calibration, are obtained. In embodiments where the calibration process is not performed prior to the start of the surgical procedure, the calibration process may be performed at step <b>725</b>. In other embodiments, as noted above, the characteristic(s) of the baseline images of the test pattern may be stored in memory <b>35</b>. In such embodiments, the corresponding characteristic(s) of the baseline image of the test pattern are retrieved from a lookup table and/or database stored in memory <b>35</b>.</p><p id="p-0053" num="0052">At step <b>730</b>, the image of pseudo test pattern <b>255</b><i>c </i>received at step <b>715</b> is analyzed and one or more characteristics are determined from the images of pseudo test pattern <b>255</b><i>c</i>. In an embodiment, a MTF is calculated for the images of pseudo test pattern <b>255</b><i>c </i>in order to determine the characteristics of the images of pseudo test pattern <b>255</b><i>c</i>. The determined characteristic(s) of the images of pseudo test pattern <b>255</b><i>c </i>may be in the form of a percentage at LW/PH. For example, at step <b>730</b>, the MTF may yield values of 100% at 105 mm and 45% at 50 mm. In other embodiments, various other types of analysis functions may be applied to the images of pseudo test pattern <b>255</b><i>c</i>. In any case, step <b>730</b> is continuously reiterated, so that changes in the determined characteristic(s) of the images of pseudo test pattern <b>255</b><i>c </i>may be detected from the images of pseudo test pattern <b>255</b><i>c </i>over time.</p><p id="p-0054" num="0053">In step <b>735</b>, the determined characteristic(s) of the images of pseudo test pattern <b>255</b><i>c </i>are compared to the one or more characteristics of the baseline images of pseudo test pattern <b>255</b><i>c</i>. In one embodiment, it is contemplated that either the percentage or the LW/PH are selected at a specific value for the characteristic(s) of the baseline images of pseudo test pattern <b>255</b><i>c </i>to be compared with percentages or LW/PH of the determined characteristic(s) of images of pseudo test pattern <b>255</b><i>c</i>. For example, if the LW/PH is selected for the characteristic(s) of the baseline images of pseudo test pattern <b>255</b><i>c </i>at a specific value of 50 mm and the corresponding percentage is 50% (thus yielding a MTF graph amplitude ranging from 50 to &#x2212;50 at LW/PH of 50 mm), as described above in the description of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, then at step <b>735</b> the determined characteristic(s) of images of pseudo test pattern <b>255</b><i>c </i>at LW/PH of 55 m (45% at 50 mm, as described in step <b>30</b>) are compared to 50% at 50 mm. Thus, using the examples above, the comparison between the characteristic(s) of the baseline images of pseudo test pattern <b>255</b><i>c </i>and the determined characteristic(s) of pseudo test pattern <b>255</b><i>c </i>yields a 5% difference.</p><p id="p-0055" num="0054">In an embodiment, at step <b>740</b>, a determination is made as to whether the difference between the characteristic(s) of the baseline images of pseudo test pattern <b>255</b><i>c </i>and the determined characteristic(s) of the images of pseudo test pattern <b>255</b><i>c </i>is greater than a predetermined threshold. The predetermined threshold is a percentage decrease between the characteristic(s) of the baseline images of pseudo test pattern <b>255</b><i>c </i>and the determined characteristic(s) of pseudo test pattern <b>255</b><i>c</i>. For example, a predetermined threshold of the modulation function may translate to a decrease of a 15% of line widths per picture height (LW/PH), which may indicate image marrying from material on the lens of one or both images. If the difference is not greater than a predetermined threshold (&#x201c;NO&#x201d; at step <b>740</b>), method <b>700</b> proceeds to step <b>745</b>, where new images of surgical site &#x201c;S&#x201d; are received. Next, at step <b>747</b>, similar to step <b>715</b>, the new images of surgical site &#x201c;S&#x201d; are processed to identify pseudo test pattern <b>255</b><i>c</i>. Following step <b>747</b>, method <b>700</b> returns to step <b>730</b> where the received images of pseudo test pattern <b>255</b><i>c </i>are analyzed by processor <b>32</b> and one or more characteristics of the images of pseudo test pattern <b>255</b><i>c </i>are determined.</p><p id="p-0056" num="0055">It is contemplated that steps <b>730</b> through step <b>740</b> and returning to step <b>730</b> may be performed iteratively and repeated at regular intervals. In one embodiment, it is contemplated that processor <b>32</b> will proceed from steps <b>730</b> through step <b>740</b> returning to step <b>730</b> processing newly received images of surgical site &#x201c;S&#x201d; at 60 Hz, possibly 30 Hz, and even as low as 10 Hz. Thus, relatively frequesly, new images of surgical site &#x201c;S&#x201d; is received, processed, and a determination made as to whether the difference between the characteristic(s) of the baseline image of pseudo test pattern <b>255</b><i>c </i>and the determined characteristic(s) of the images of pseudo test pattern <b>255</b><i>c</i>, is greater than a predetermined threshold. In other embodiments, based on the surgical procedure, the intervals from steps <b>730</b> through step <b>740</b> returning to step <b>730</b> may be shortened in order to increase the frequency of the determination of image degradation, for example, every half, quarter, or one-tenth of a second.</p><p id="p-0057" num="0056">In a further embodiment, a determination is made as to whether a trend is detected from the differences determined at step <b>735</b>. It is contemplated that the differences from step <b>735</b> are stored in memory <b>35</b>, and the data is monitored to determine the trend. For example, processor <b>32</b> may determine that image degradation has likely occurred due to tissue occluding one or both of lenses <b>210</b>, <b>220</b>, where image degradation occurs rapidly following a small number of passes from steps <b>730</b> through <b>740</b> and returning to step <b>730</b>. Alternatively, processor <b>32</b> may determine that image degradation has occurred due to a gradual build-up of fluid or other anatomical material where image degradation occurs more slowly.</p><p id="p-0058" num="0057">If, at step <b>740</b>, it is determined that the difference between the characteristic(s) of the baseline images of pseudo test pattern <b>255</b><i>c </i>and the determined characteristic(s) of the images of pseudo test pattern <b>255</b><i>c </i>is greater than a predetermined threshold (&#x201c;YES&#x201d; at step <b>740</b>), method <b>700</b> proceeds to step <b>742</b>. At step <b>742</b>, a determination is made as to whether image degradation has occurred, based on the result of the determination at step <b>740</b>. For example, image degradation may include the images being distorted, out of focus, partially or wholly occluded, and/or a mismatch between the images captured by left-eye lens <b>210</b> and right-eye lens <b>220</b>, thereby causing a stereoscopic visual distortion even if the images, when viewed separately, do not appear distorted. Thereafter, method <b>700</b> proceeds to step <b>750</b>, where a notification is generated and provided to the user indicating that image degradation may have occurred. The notification may be displayed, for example, via display device <b>44</b> or portable display device <b>45</b> of user interface <b>40</b>, and/or be provided audibly or tactilely via gimbals <b>70</b>.</p><p id="p-0059" num="0058">Following step <b>750</b>, method <b>700</b> proceeds to step <b>755</b> where feedback is provided to the user indicating how image quality may be improved. For example, the feedback provided may be in the form of a notification via display device <b>44</b> or portable display device <b>45</b>, that the user should remove and clean one or both of lenses <b>210</b>, <b>220</b> in order to improve image quality. After feedback is provided indicating how to improve image quality at step <b>755</b>, the method <b>700</b> proceeds to step <b>760</b> where it is determined whether the image quality can be improved during the surgical procedure. For example, it may be determined whether image capture device <b>200</b> needs to be removed from surgical site &#x201c;S&#x201d; and cleaned, or be replaced. Those skilled in the art will envision various other actions that may be taken to improve the image quality of images captured by image capture device <b>200</b>, and thus, for purpose of brevity, all such alternative actions will not be described here. If it is determined at step <b>760</b> that the image quality cannot be improved during the surgical procedure (&#x201c;NO&#x201d; at step <b>760</b>), the method <b>700</b> proceeds to step <b>775</b> where the surgical procedure ends. Alternatively, if it is determined at step <b>760</b> that the image quality can be improved during the surgical procedure (&#x201c;YES&#x201d; at step <b>760</b>), the method <b>700</b> proceeds to step <b>765</b>.</p><p id="p-0060" num="0059">At step <b>765</b> it is determined whether the image quality has been improved. For example, the processes described above with reference to steps <b>745</b>, <b>747</b>, <b>730</b>, <b>735</b>, and <b>740</b> may be repeated to determine whether the image quality of images captured by image capture device <b>200</b> has been improved. If it is determined at step <b>765</b> that the image quality has not been improved (&#x201c;NO&#x201d; at step <b>765</b>), processing returns to step <b>755</b>. Alternatively, if it is determined at step <b>765</b> that the image quality has been improved (&#x201c;YES&#x201d; at step <b>765</b>), the method <b>700</b> proceeds to step <b>770</b>.</p><p id="p-0061" num="0060">At step <b>770</b> it is determined whether the surgical procedure has been completed. For example, it may be determined whether the user has provided an instruction and/or indication that the surgical procedure has been completed. If it is determined at step <b>770</b> that the surgical procedure has not been completed (&#x201c;NO&#x201d; at step <b>770</b>), processing returns to step <b>745</b>. Alternatively, if it is determined at step <b>770</b> that the surgical procedure has been completed (&#x201c;YES&#x201d; at step <b>770</b>), processing ends.</p><p id="p-0062" num="0061">Referring back to the computer-readable media of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, memory <b>35</b> includes any non-transitory computer-readable storage media for storing data and/or software that is executable by processor <b>32</b> and which controls the operation of controller <b>30</b>. In an embodiment, memory <b>35</b> may include one or more solid-state storage devices such as flash memory chips. Alternatively or in addition to the one or more solid-state storage devices, memory <b>35</b> may include one or more mass storage devices connected to processor <b>32</b> through a mass storage controller (not shown) and a communications bus (not shown). Although the description of computer-readable media contained herein refers to a solid-state storage, it should be appreciated by those skilled in the art that computer-readable storage media can be any available media that can be accessed by processor <b>32</b>. That is, computer readable storage media includes non-transitory, volatile and non-volatile, removable and non-removable media implemented in any method or technology for storage of information such as computer-readable instructions, data structures, program modules or other data. For example, computer-readable storage media includes RAM, ROM, EPROM, EEPROM, flash memory or other solid state memory technology, CD-ROM, DVD, Blu-Ray or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by controller <b>30</b>. Further aspects of the system and method are described in U.S. Pat. No. 8,828,023 entitled &#x201c;MEDICAL WORKSTATION,&#x201d; filed on Nov. 3, 2011, the entire contents of all of which are hereby incorporated by reference.</p><p id="p-0063" num="0062">Detailed embodiments of devices, systems incorporating such devices, and methods using the same have been described herein. However, these detailed embodiments are merely examples of the disclosure, which may be embodied in various forms. Therefore, specific structural and functional details disclosed herein are not to be interpreted as limiting, but merely as a basis for the claims and as a representative basis for allowing one skilled in the art to employ the present disclosure in virtually any appropriately detailed structure.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. (canceled)</claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. A method for detecting image degradation during a surgical procedure, the method comprising:<claim-text>receiving an image of a surgical instrument;</claim-text><claim-text>obtaining a characteristic of a test pattern disposed on an outer surface of the surgical instrument, wherein the test pattern includes a plurality of straight-edge portions defining two zones of different and contrasted luminance;</claim-text><claim-text>processing the image to identify the test pattern, which is captured in the received image and which is disposed on the outer surface of the surgical instrument;</claim-text><claim-text>determining a characteristic of the identified test pattern in the image of the surgical instrument based on a luminance of a first zone of the two zones of the test pattern and a second zone of the two zones of the test pattern;</claim-text><claim-text>calculating a difference between the determined characteristic and the obtained characteristic; and</claim-text><claim-text>generating an image degradation notification based on the difference.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising capturing the image of the surgical instrument by an image capture device.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the image capture device is a stereoscopic endoscope including a left-eye lens and a right-eye lens.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising obtaining the characteristic of the test pattern based on a baseline image of the test pattern.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the characteristic of the test pattern is obtained from a lookup table or a database.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the test pattern corresponds to a type of the surgical instrument.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising determining the characteristic of the identified test pattern based on a modulation transfer function (MTF) on the test pattern.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the test pattern includes black bars and white bars.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the MTF is a percentage of contrast between the black bars and the white bars.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising periodically receiving another image of the surgical instrument at a predetermined interval.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A system for detecting image degradation during a surgical procedure, the system comprising:<claim-text>a surgical instrument;</claim-text><claim-text>an image capture device configured to capture an image of the surgical instrument, the image of the surgical instrument including a test pattern disposed on an outer surface of the surgical instrument;</claim-text><claim-text>at least one processor coupled to the image capture device; and</claim-text><claim-text>a memory coupled to the at least one processor and having instructions stored thereon, that, when executed by the at least one processor, cause the at least one processor to:<claim-text>obtain a characteristic of the test pattern;</claim-text><claim-text>process the image to identify the test pattern, which is captured in the image and which is disposed on the outer surface of the surgical instrument;</claim-text><claim-text>determine a characteristic of the identified test pattern in the image of the surgical instrument;</claim-text><claim-text>calculate a difference between the determined characteristic and the obtained characteristic; and</claim-text><claim-text>generate an image degradation notification based on the difference.</claim-text></claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the image capture device is a stereoscopic endoscope including a left-eye lens and a right-eye lens.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the characteristic of the test pattern is obtained based on a baseline image of the test pattern.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the characteristic of the test pattern is obtained from a lookup table or a database.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the test pattern corresponds to a type of the surgical instrument.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the characteristic of the identified test pattern is determined based on a modulation transfer function (MTF) on the test pattern.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the test pattern includes black bars and white bars.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the MTF is a percentage of contrast between the black bars and the white bars.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising wherein the at least one processor periodically receiving another image of the surgical instrument at a predetermined interval.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. A non-transitory computer-readable medium storing instructions that, when executed by a processor, cause the processor to:<claim-text>receive an image of a surgical instrument;</claim-text><claim-text>obtain a characteristic of a test pattern disposed on an outer surface of the surgical instrument;</claim-text><claim-text>process the image to identify the test pattern, which is captured in the received image and which is disposed on the outer surface of the surgical instrument;</claim-text><claim-text>determine a characteristic of the identified test pattern in the image of the surgical instrument;</claim-text><claim-text>calculate a difference between the determined characteristic and the obtained characteristic;</claim-text><claim-text>obtain baseline images of one or more straight outer edges of the surgical instrument;</claim-text><claim-text>compare a characteristic of one or more straight outer edges of the surgical instrument in the images to a characteristic of the one or more straight outer edges of the surgical instrument in the baseline images, the images of the surgical instrument being received subsequent to the obtaining of the baseline images and being received while the surgical instrument is disposed at a surgical site in a patient;</claim-text><claim-text>determine whether the images of the surgical instrument are degraded, based on the comparison of the characteristic of one or more straight outer edges of the surgical instrument in the images and the characteristic of the one or more straight outer edges of the surgical instrument in the baseline images; and</claim-text><claim-text>generate an image degradation notification based on the difference between the determined characteristic and the obtained characteristic, and in response to a determination that the images of the surgical instrument are degraded.</claim-text></claim-text></claim></claims></us-patent-application>