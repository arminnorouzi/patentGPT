<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230006733A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230006733</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17785346</doc-number><date>20201216</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>B</subclass><main-group>7</main-group><subgroup>185</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>B</subclass><main-group>7</main-group><subgroup>18513</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>B</subclass><main-group>7</main-group><subgroup>18515</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>B</subclass><main-group>7</main-group><subgroup>18517</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>B</subclass><main-group>7</main-group><subgroup>1858</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>B</subclass><main-group>7</main-group><subgroup>18584</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEM AND METHOD FOR MANAGING CHANNEL BANDWIDTH OF A COMMUNICATION SIGNAL</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>62948599</doc-number><date>20191216</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>KRATOS INTEGRAL HOLDINGS, LLC</orgname><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>King</last-name><first-name>Brandon Gregory</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Jarriel</last-name><first-name>Jeffrey David</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Stoltenberg</last-name><first-name>Matthew James</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Sutton</last-name><first-name>Daniel Joseph</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/US2020/065358</doc-number><date>20201216</date></document-id><us-371c12-date><date>20220614</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Embodiments of systems and methods for managing channel bandwidth of signals are provided herein. Example method include receiving signals from one or more antenna feeds, each signal having a first bandwidth. Some example methods include, in a plurality of processing blocks operating in parallel in one or more processors, performing one or more channelizer operations on portions of the signals, each channelizer operation creates a plurality of channels having a bandwidth smaller than the first bandwidth. Some methods may include, in a plurality of processing blocks in the one or more processors, performing one or more combiner operations on the channels, each operation combines the bandwidth of a subset of the channels into a combined channel, the plurality of processing blocks operating in parallel. The method then outputs the combined channel to a network.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="82.80mm" wi="158.75mm" file="US20230006733A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="171.87mm" wi="129.46mm" orientation="landscape" file="US20230006733A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="227.25mm" wi="163.15mm" file="US20230006733A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="206.67mm" wi="163.15mm" file="US20230006733A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="242.82mm" wi="165.52mm" orientation="landscape" file="US20230006733A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="257.13mm" wi="162.22mm" orientation="landscape" file="US20230006733A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="227.92mm" wi="163.75mm" orientation="landscape" file="US20230006733A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="249.85mm" wi="167.47mm" file="US20230006733A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="276.10mm" wi="159.94mm" orientation="landscape" file="US20230006733A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="262.04mm" wi="141.65mm" orientation="landscape" file="US20230006733A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="163.15mm" wi="152.65mm" file="US20230006733A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims priority to U.S. Provisional Patent App. No. 62/948,599, filed on Dec. 16, 2019, which is hereby incorporated herein by reference as if set forth in full.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">Technical Field</heading><p id="p-0003" num="0002">This disclosure relates to signal processing. More specifically, this disclosure relates to implementing distributed computing using a general-purpose processor to achieve high-rate processing.</p><heading id="h-0004" level="1">Description of the Related Art</heading><p id="p-0004" num="0003">In some examples, a satellite communication signal can require large ground stations and other facilities to transmit and/or receive and process data locally. This can include extensive antenna arrays, associated radio frequency terminals (RFTs), and significant electronics (modems, signal processors, etc.) to receive, process, and use the data received from an associated satellite.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0005" num="0004">This disclosure provides for an improved communication system. The following summary is not intended to define every aspect of the invention, and other features and advantages of the present disclosure will become apparent from the following detailed description, including the drawings. The present disclosure is intended to be related as a unified document, and it should be understood that all combinations of features described herein are contemplated, even if the combination of features are not found together in the same sentence, paragraph, or section of this disclosure. In addition, the disclosure includes, as an additional aspect, all embodiments of the invention narrower in scope in any way than the variations specifically mentioned herein.</p><p id="p-0006" num="0005">As disclosed herein, digital signal processing (DSP) can be performed in many different ways using general purpose processors, or central processing units (CPUs). Example techniques executed on a general purpose processor to achieve high rate processing that can perform the disclosed functions include, but are not limited to:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0006">using multiple CPUs and the parallel processing on the many cores of each CPU;</li>        <li id="ul0002-0002" num="0007">employing single instruction, multiple data (SIMD) techniques;</li>        <li id="ul0002-0003" num="0008">feed-forward processing to break up feed-back loops;</li>        <li id="ul0002-0004" num="0009">pre-calculation of metadata (or state information) to divide the heavy processing over several CPUs; and</li>        <li id="ul0002-0005" num="0010">collection of multiple functions into a single function in a manner that increases CPU performance or lowers memory bandwidth utilization.</li>    </ul>    </li></ul></p><p id="p-0007" num="0011">One way to increase through-put on a general-purpose CPU is to utilize as many cores as possible that exist on the CPU. Great care must be taken to ensure data is properly shared amongst several cores within the CPU but this allows for processing throughput to increase with the addition of more CPU cores. It is also possible to use several CPUs on the same system, with each CPU containing multiple cores. All embodiments within this disclosure take advantage of using multiple cores within a CPU, and some embodiments take advantage of having multiple CPUs per system and/or even groups of systems in a server environment.</p><p id="p-0008" num="0012">Another way to achieve high processing rates is to take advantage of single instruction, multiple data (SIMD) capabilities of general-purpose CPUs. This allows a single CPU core to perform up 16 floating point operations on a single instruction, as is the case of AVX512 SIMD operations. One example of employing SIMD is using a finite impulse response (FIR) filter function in which 16 floating point results are calculated at once. Another example is when multiplying complex numbers together. Instead of calculating one pair of quadrature signals (IQ data), it is possible with AVX512 to calculate eight IQ pairs at a time. Complex multiplication is used in nearly every processing algorithm described in this disclosure. Other examples using SIMD includes the correlator in the diversity combiner, the decimation in the signal analyzer, and again adjustment in the channelizer/combiner.</p><p id="p-0009" num="0013">Some processing systems implement various forms of feedback, often including a phase lock loop (PLL) or a delay lock loop (DLL). However, feedback in general as is the case for PLLs and DLLs can be problematic because the very nature of the feedback causes bottlenecking. The feedback loop forces all the incoming data to be processed on a single (e.g., linear) process that cannot not be easily spilt or otherwise divided. In addition to the feedback, there are other obstacles to overcome using PLLs and DLLs including how often to calculate the error term. The feedback loop can be replaced with a feed-forward loop in which error states can be processed on a block of data and then the calculated error term is fed-forward to another block which applies the error term. If proper overlap is used, the error calculation and application of that term can be split over several CPU cores to further increase through-put. One example of this is in the diversity combiner where the timing and phase correction is calculated in one block and the timing adjustment is applied in another block and the phase correction in yet another block. This method as a set can then be parallelized over several CPU cores to further increase throughput.</p><p id="p-0010" num="0014">In addition to feed-forward approach to processing data, it can be beneficial to perform pre-calculation of metadata in a single block that then splits up the processing of the data over several CPU cores. This method is similar to the feed-forward methodology already discussed, but in this case, it is not breaking up a loop (such as feed-back loop) but simply leveraging many CPU cores to increase the amount of data that can be processed. In this way the block that performs the pre-calculation does not perform the CPU intensive processing but calculates the needed steps such as iterations within a for-loop and start indices and slope points between interpolation phases values. One such example of this is Doppler Compensation performed in the Diversity combiner. The needed phase adjustments are created in the first block but the CPU intensive calculation to perform the phase adjustment is handed off to subsequent blocks downstream. If the second portion of the processing is the CPU intensive portion, this allows for any number of CPU cores to be utilized and therefore increase processing rate that otherwise could not be achieved within a single block.</p><p id="p-0011" num="0015">Another technique that can be employed on general purpose CPUs to achieve high through-put is the way the set of functions is employed and memory type used. In some cases, memory bandwidth becomes the limiting factor in performance. If this is the case, the goal is to limit the amount of data that needs to transferred to and from random-access memory (RAM) (not faster memory like CPU cache). In order to do this, functions need to be collapsed so that they all run together instead of individually with goal of accessing slower RAM as little as possible as compared to accessing faster CPU cache. Another method to lowering memory bandwidth is utilize properly space memory types, e.g., using int<b>8</b> when possible versus floats or doubles.</p><p id="p-0012" num="0016">In an embodiment, a method for managing channel bandwidth of one or more downlink signals is provided herein. The method comprises receiving the one or more downlink signals from one or more antenna feeds, the one or more downlink signals having a first bandwidth. The method also comprises, in a first one or more processing blocks in one or more processors, performing a first channelizer operation on a first portion of the one or more downlink signals that creates a first plurality of channels, each of the first plurality of channels having a bandwidth smaller than the first bandwidth. The method also includes, in a second one or more processing blocks in the one or more processors in parallel with the first one or more processing blocks, performing a second channelizer operation on a second portion of the one or more downlink signals that creates a second plurality of channels, each of the second plurality of channels having a bandwidth smaller than the first bandwidth. The method further includes, in a third one or more processing blocks in the one or more processors, performing a combiner operation that combines a subset of the first plurality of channels and a subset of the second plurality of channels into a combined channel, the combined channels having a second bandwidth smaller than the first bandwidth, and then outputting the combined channel to a network.</p><p id="p-0013" num="0017">In another embodiment, another method is provided for managing channel bandwidth of one or more downlink signals is provided herein. The method comprises receiving the one or more downlink signals from one or more antenna feeds, the one or more downlink signals having a first bandwidth. The method also includes, in a first one or more processing blocks in one or more processors, performing a first channelizer operation on a first portion of the one or more downlink signals that creates a first plurality of channels, each of the first plurality of channels having a bandwidth smaller than the first bandwidth. Additionally, the method includes, in a second one or more processing blocks in the one or more processors in parallel with the first one or more processing blocks, performing a second channelizer operation on a second portion of the one or more downlink signals that creates a second plurality of channels, each of the second plurality of channels having a bandwidth smaller than the first bandwidth. The method then outputs the first and second plurality of channels to a network.</p><p id="p-0014" num="0018">In another embodiment, another method is provided for managing channel bandwidth of one or more downlink signals. The method comprises receiving input signals of the one or more downlink signals on a plurality of input channels, each input channel having a spectral bandwidth, and, in a plurality of processing blocks in one or more processors, performing a combiner operation on the input channels that combines the spectral bandwidth of a selected subset of the plurality of input channels into a combined channel, the plurality of processing blocks operating in parallel. The method then outputs the combined channel to a network.</p><p id="p-0015" num="0019">In another embodiment, systems are provided for or managing channel bandwidth one or more downlink signals. The systems comprise one or more antennas configured to receive the plurality of downlink signals a digitizer operable to convert a received analog signal into the digital bitstream and one or more processors communicatively coupled to the plurality of antennas. The one or more processors have a plurality of processing blocks and are operable to perform one or more of the methods described above.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0016" num="0020">The details of the present invention, both as to its structure and operation, may be gleaned in part by study of the accompanying drawings, in which like reference numerals refer to like parts, and in which:</p><p id="p-0017" num="0021"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a graphical representation of an embodiment of a communication system, in accordance with the embodiments disclosed herein.</p><p id="p-0018" num="0022"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a functional block diagram of a wired or wireless communication device for use as one or more components of the system of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0019" num="0023"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a graphical depiction of an embodiment of feedforward or pre-calculation signal processing, in accordance with embodiments disclosed herein.</p><p id="p-0020" num="0024"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a graphical depiction of another embodiment of feedforward or pre-calculation signal processing of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in accordance with embodiments disclosed herein.</p><p id="p-0021" num="0025"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a functional block diagram of an embodiment of a digital signal diversity combiner, in accordance with embodiments disclosed herein.</p><p id="p-0022" num="0026"><figref idref="DRAWINGS">FIGS. <b>6</b>-<b>8</b></figref> are functional block diagrams of example processing blocks included in the diversity combiner of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, in accordance with embodiments disclosed herein.</p><p id="p-0023" num="0027"><figref idref="DRAWINGS">FIGS. <b>9</b> and <b>10</b></figref> are functional block diagrams of example function blocks of the processing block of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, in accordance with embodiments disclosed herein.</p><p id="p-0024" num="0028"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is functional block diagram of an example function block of the processing block of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, in accordance with embodiments disclosed herein.</p><p id="p-0025" num="0029"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a functional block diagram of an embodiment of a digital signal channelizer, in accordance with embodiments disclosed herein</p><p id="p-0026" num="0030"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a functional block diagram of an embodiment of a digital signal combiner, in accordance with embodiments disclosed herein.</p><p id="p-0027" num="0031"><figref idref="DRAWINGS">FIGS. <b>14</b>A and <b>14</b>B</figref> are a functional block diagrams of embodiments of digital signal channelizer(s) and combiner(s), in accordance with embodiments disclosed herein.</p><p id="p-0028" num="0032"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a functional block diagram of an example processing block for the digital signal channelizer of <figref idref="DRAWINGS">FIG. <b>12</b></figref>, in accordance with embodiments disclosed herein.</p><p id="p-0029" num="0033"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a functional block diagram of an example processing block for the digital signal combiner of <figref idref="DRAWINGS">FIG. <b>13</b></figref>, in accordance with embodiments disclosed herein.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0030" num="0034">Embodiments of an improved communication system using a general-purpose processor to achieve high-rate processing are disclosed. Embodiments disclosed herein provide for improved communication systems capable of utilizing a general-purpose processor to efficiently achieve a high-rate of signal processing. After reading this description, it will become apparent to one skilled in the art how to implement the invention in various alternative embodiments and alternative applications. However, although various embodiments of the present invention will be described herein, it is understood that these embodiments are presented by way of example and illustration only, and not limitation. As such, this detailed description of various embodiments should not be construed to limit the scope or breadth of the present invention as set forth in the appended claims.</p><p id="p-0031" num="0035">Reference throughout this specification to &#x201c;one embodiment&#x201d; or &#x201c;an embodiment&#x201d; means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment. Thus, appearances of the phrases &#x201c;in one embodiment&#x201d; or &#x201c;in an embodiment&#x201d; in various places throughout this specification are not necessarily all referring to the same embodiment. Furthermore, the particular features, structures, or characteristics may be combined in any suitable manner in one or more embodiments.</p><p id="p-0032" num="0036">A communication system is used as a primary example throughout the description, however, the application of the disclosed methods is not so limited. For example, any wireless or radio communication system requiring the use of digital signal processing, a modem, etc. can implement the systems, methods, and computer readable media described herein.</p><p id="p-0033" num="0037">This disclosure provides systems and methods for performing Digital Signal Processing using general purpose central processing units (CPUs) in either a standard server environment or a virtualized cloud environment. In some examples, the systems can employ single-instruction multiple data (SIMD) techniques to achieve high throughput including SSE, SSE2, SSE3, SSE4.1, SSE4.2, AVX, AVX2 and AVX512 instruction sets. This disclosure describes how the data processing is managed over multiple processing cores of the processors (e.g., CPUs) to achieve the necessary throughput without the use of dedicated signal processing hardware such as Field Programmable Gate Arrays (FPGAs) or High Performance Computing (HPC) hardware such as Graphics Processing Units (GPUs). The ability to perform this processing in general-purpose server CPUs, including but not limited to x86 architecture made by Intel and AMD micro-processors, as well as ARM processors like Cortex-A76, NEON and AWS Graviton and Graviton2, allows the functions to be deployed within a general-purpose cloud processing environment using a virtualized processing architecture without the need for dedicated hardware. The processing in general purpose CPUs is enabled by a Digital IF appliance that samples the analog signal and feeds the digitized samples into the CPU over an Ethernet connection. The Digital IF appliance can also accept digitized samples and covert to an analog signal, similar to that described in U.S. Pat. No. 9,577,936, issued Feb. 21, 2017, entitled &#x201c;Packetized Radio Frequency Transport System&#x201d; the contents of which are incorporated by reference in their entirety.</p><p id="p-0034" num="0038"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a graphical representation of an embodiment of a communication system. A communication system (system) <b>100</b> can have a platform <b>110</b> and a satellite <b>111</b> that communicate with a plurality of a ground stations. The platform <b>110</b> can be an aircraft (e.g., an airplane, helicopter, or unmanned aerial vehicle (UAV), etc.) A plurality of ground stations <b>120</b>, <b>130</b>, <b>140</b> can be associated with a terrestrial radiofrequency (RF) antenna <b>122</b> or one or more satellite antennas <b>132</b>, <b>142</b>. The ground station <b>120</b> can have an antenna <b>122</b> coupled to a digitizer <b>124</b>. The digitizer <b>124</b> can have one or more analog to digital converters (A2D) for converting analog signals received at the antenna <b>122</b> into a digital bit stream for transmission via a network. The digitizer <b>124</b> can also include corresponding digital to analog converters (D2A) for operations on the uplink to the platform <b>110</b> and the satellite <b>111</b>.</p><p id="p-0035" num="0039">Similarly, the ground station <b>130</b> can have an antenna <b>132</b> and a digitizer <b>134</b>, and the ground station <b>140</b> can have an antenna <b>142</b> and a digitizer <b>144</b>.</p><p id="p-0036" num="0040">The ground stations <b>120</b>, <b>130</b>, <b>140</b> can each receive downlink signals <b>160</b> (labeled <b>160</b><i>a, </i><b>160</b><i>b, </i><b>160</b><i>c</i>) from the platform <b>110</b> and the downlink signals <b>170</b> (labeled <b>170</b><i>a, </i><b>170</b><i>b, </i><b>170</b><i>c</i>) from the satellite <b>111</b> in a receive chain. The ground stations <b>120</b>, <b>130</b>, <b>140</b> can also transmit uplink signals via the respective antennas <b>122</b>, <b>132</b>, <b>142</b> in a transmit chain. The digitizers <b>124</b>, <b>134</b>, <b>144</b> can digitize the received downlink signals <b>160</b>, <b>170</b> for transmission as a digital bit stream <b>152</b>. The digital bit stream <b>134</b> can then be transmitted, via a network <b>154</b> to a cloud processing system.</p><p id="p-0037" num="0041">In some examples, the ground stations <b>120</b>, <b>130</b>, <b>140</b> can process all of the data (e.g., contained in the downlink signals) locally, however this can be exceptionally expensive from a time, resource, and efficiency perspective. Therefore, in some embodiments, the downlink signals can be digitized and transmitted as the digital bit stream <b>152</b> to a remote signal processing server (SPS) <b>150</b>. In some implementations, the SPS <b>150</b> can be positioned in a physical location, such as a data center located in an offsite facility that is accessible via a wide area network (WAN). Such a WAN can be the Internet, for example. The SPS <b>150</b> can demodulate the downlink signals from the digital bit stream <b>152</b> and output the data or information bits from the downlink signals. In some other implementations, the SPS <b>150</b> can use cloud computing or cloud processing to perform the signal processing and other methods described herein. The SPS <b>150</b> can also be referred to as a cloud server.</p><p id="p-0038" num="0042">The SPS <b>150</b> can then provide the processed data to the user or sent to a different site. The data and information can be mission-dependent. In addition, the information contained in the data can be the main purpose of the satellite, including weather data, image data, and satellite communication (SATCOM) payload data. As noted above, SATCOM is used as a primary example herein, but any communication or signal processing system using DSP can implement the methods described herein.</p><p id="p-0039" num="0043">In order to achieve high processing rates with software, a phase lock loop (PLL) or delay lock loop (DLL) approach can be problematic due to the feedback within the loop. The feedback loop forces all of the incoming data (e.g., the downlink signal <b>132</b>) to be processed on a single (e.g., linear) process that cannot not be easily spilt or otherwise divided. In addition to the feedback, there are other obstacles to overcome using the PLL/DLL including, for example, how often to calculate the error term.</p><p id="p-0040" num="0044"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a functional block diagram of a wired or wireless communication device for use as one or more components of the system of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. A processing device (device) <b>200</b> may be implemented as, for example, the SPS <b>150</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The device <b>200</b> can be implemented as needed to perform one or more of the signal processing methods or steps disclosed herein.</p><p id="p-0041" num="0045">The device <b>200</b> may include a processor <b>202</b> which controls operation of the device <b>200</b>. The processor <b>202</b> may also be referred to as a CPU. The processor <b>202</b> can direct and/or perform the functions, for example, attributed to SPS <b>150</b>. Certain aspects of the device <b>200</b>, including the processor <b>202</b> can be implemented as various cloud-based elements, such as cloud-based processing. Accordingly, the processor <b>202</b> can represent cloud processing, distributed over several disparate processors via a network (e.g., the Internet). Alternatively, certain components can be implemented in hardware. The processor <b>202</b> may be implemented with any combination of one or more of general-purpose microprocessors, microcontrollers, digital signal processors (DSPs), field programmable gate array (FPGAs), programmable logic devices (PLDs), controllers, state machines, gated logic, discrete hardware components, dedicated hardware finite state machines, or any other suitable entities that can perform calculations or other manipulations of information.</p><p id="p-0042" num="0046">The processor <b>202</b> can have one or more cores <b>204</b> (shown as core <b>204</b><i>a </i>through core <b>204</b><i>n</i>) on which the computations can be performed. In implementations using cloud processing, the cores <b>204</b> can represent multiple iterations of distributed cloud processing. In some embodiments, using hardware, the processor <b>202</b> can be a complex, integrated circuit on which all the computations for the receiver are taking place. As used herein, the cores <b>204</b> can each be one processing element of the processor <b>202</b>. The processor <b>202</b> can implement multiple cores <b>204</b> to perform the necessary parallel processing for the methods disclosed herein. In some embodiments, the processor <b>202</b> may be distributed across multiple CPUs as in cloud computing.</p><p id="p-0043" num="0047">The device <b>200</b> may further include a memory <b>206</b> operably coupled to the processor <b>202</b>. The memory <b>206</b> can be cloud-based storage or local hardware storage. The memory <b>206</b> can include both read-only memory (ROM) and random access memory (RAM), providing instructions and data to the processor <b>202</b>. A portion of the memory <b>206</b> may also include non-volatile random access memory (NVRAM). The processor <b>202</b> typically performs logical and arithmetic operations based on program instructions stored within the memory <b>206</b>. The instructions in the memory <b>206</b> may be executable to implement the methods described herein. The memory <b>206</b> can further include removable media or multiple distributed databases.</p><p id="p-0044" num="0048">The memory <b>206</b> may also include machine-readable media for storing software. Software shall be construed broadly to mean any type of instructions, whether referred to as software, firmware, middleware, microcode, hardware description language, or otherwise. Instructions may include code (e.g., in source code format, binary code format, executable code format, or any other suitable format of code). The instructions, when executed by the processor <b>202</b> or the one or more cores <b>204</b>, cause the device <b>200</b> (e.g., the SPS <b>150</b>) to perform the various functions described herein.</p><p id="p-0045" num="0049">The device <b>200</b> may also include a transmitter <b>210</b> and a receiver <b>212</b> to allow transmission and reception of data between the communication device <b>200</b> and a remote location. Such communication can occur between the ground station <b>120</b> and the SPS <b>150</b> via the network <b>124</b>, for example. Such communications can be wireless or conducted via wireline communications. The transmitter <b>210</b> and receiver <b>212</b> may be combined into a transceiver <b>214</b>. The transceiver <b>214</b> can be communicatively coupled to the network <b>124</b>. In some examples the transceiver <b>214</b> can include or be a portion of a network interface card (NIC).</p><p id="p-0046" num="0050">The device <b>200</b> may further comprise a user interface <b>222</b>. The user interface <b>222</b> may comprise a keypad, a microphone, a speaker, and/or a display. The user interface <b>222</b> may include any element or component that conveys information to a user of the device <b>200</b> and/or receives input from the user.</p><p id="p-0047" num="0051">The various components of the device <b>200</b> described herein may be coupled together by a bus system <b>226</b>. The bus system <b>226</b> may include a data bus, for example, as well as a power bus, a control signal bus, and a status signal bus in addition to the data bus. In some embodiments, the bus system <b>226</b> can be communicatively coupled to the network <b>124</b>. The network <b>124</b> can provide a communication link between the device <b>200</b> (e.g., the processor <b>202</b>) and the ground station <b>120</b>, for example. Those of skill in the art will appreciate the components of the device <b>200</b> may be coupled together or accept or provide inputs to each other using some other mechanism such as a local- or wide area network for distributed processing.</p><p id="p-0048" num="0052"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a graphical depiction of schematic block diagram of an embodiment of feedforward or pre-calculation signal processing <b>300</b>. A method <b>300</b> can occur as a generalized process incorporating a plurality of functions by, for example, the processor <b>202</b>. The processor <b>202</b> can perform the plurality of functions in a series or in parallel arrangement as shown to perform one or more desired processes. Each function may refer to a block or collection of instructions or software executable by the processor <b>202</b> and stored in a memory <b>206</b>.</p><p id="p-0049" num="0053">A first function <b>302</b> can be performed by the processor <b>202</b>. In some embodiments, a second function <b>304</b> can be performed serially, following the first function <b>302</b>. Accordingly, the processor <b>202</b> can split blocks of data with the different functionality for processing over multiple cores <b>204</b> to perform the first function <b>302</b> and the second function <b>304</b>.</p><p id="p-0050" num="0054">The processor <b>202</b> can perform distributed processing of a third function <b>306</b> (shown as <b>306</b><i>a, </i><b>306</b><i>b </i>. . . <b>306</b><i>n</i>) in parallel, following the second function <b>304</b>. The parallel processing of the third function <b>306</b> can include, for example, splitting blocks of data associated with the same functionality over several cores <b>204</b> (e.g., processing blocks) of the processor <b>202</b>. For example, &#x201c;blocks of data&#x201d; can mean a group of samples that need to be processed.</p><p id="p-0051" num="0055">The processor <b>202</b> can then perform a fourth function <b>308</b>, and a fifth function <b>309</b> in series. Similar to the first function <b>302</b> and the second function <b>304</b>, the serial performance of the fourth function <b>308</b> and the fifth function <b>309</b> can include splitting blocks of data associated with the different functionality for processing over multiple cores <b>204</b>. In general, each of the first function <b>302</b>, the second function <b>304</b>, the third function <b>306</b>, the fourth function <b>308</b>, and the fifth function <b>309</b> can each be performed in a different processing block. As used herein, a processing block can refer to a specific task performed on a block of data. The processing block can be associated with one or more of the cores <b>204</b>, for example.</p><p id="p-0052" num="0056">Therefore, the method <b>300</b> can split blocks of data with the same functionality to process over multiple cores <b>204</b>, for example. Similarly, the method <b>300</b> can split blocks of data with different functionality to process over multiple cores <b>204</b>.</p><p id="p-0053" num="0057">In some other implementations of the method <b>300</b>, the same processing blocks (e.g., the cores <b>204</b>) can perform processing of data with single instruction, multiple data (SIMD), irrespective of the same or different functionality.</p><p id="p-0054" num="0058">In other implementations, the embodiments of the method <b>300</b> can support processing blocks of data with minimal state information by using overlapping data. As used herein, state information can include variables needed during feedback (e.g., feedback processing)), data frame boundaries, etc. For example, in the case of feedback loops, state information can include the variables calculated within the loop that are needed during feedback in processing a continuous stream of data. State information can also include the location of a frame boundary within a data stream. Other examples can include things such as FIR filters where the state information includes values stored in buffers (e.g., possibly many delay elements) that are needed to keep continuous data flowing.</p><p id="p-0055" num="0059">By ignoring state information and overlapping portions of adjacent blocks of data, processes can take advantage of parallel processing, using a variable level of overlap amongst the blocks of data.</p><p id="p-0056" num="0060"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a graphical depiction of an embodiment of a method for feedforward or pre-calculation signal processing of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. A method <b>400</b> can use the principles of the method <b>300</b> for series-parallel and/or parallel-series processing for multiple functions. In one example, the first function <b>302</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>) can be a data ingest function <b>305</b>, in which the processor <b>202</b> receives data for processing. The second function <b>304</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>) can be a data split function <b>310</b>, in which the processor <b>202</b> can parse data in overlapping blocks of data. The overlapped blocks of data can then be processed in parallel in various, parallel iterations of the third function <b>306</b><i>a</i>-<b>306</b><i>n </i>as processing blocks <b>315</b><i>a</i>-<b>315</b><i>n. </i>The overlap in the blocks of data can provide a level of redundancy that is not heavily reliant (or not reliant at all) on state information. The less state information that is needed, the easier it is to process the blocks of data in parallel as opposed to a continuous stream.</p><p id="p-0057" num="0061">The method <b>400</b> can further include a data combine function <b>320</b>, similar to the fourth function <b>308</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>), combining the processed data, and a data output function <b>325</b>, similar to the fifth function <b>309</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>).</p><p id="p-0058" num="0062">In a further example, the adjustable series-parallel or parallel-series arrangement of the various functions of the method <b>300</b> provide several methods of implementing feedforward processing to replace feedback loops. This is advantageous as it can increase throughput and avoid bottlenecks caused by delays in feedback processing.</p><p id="p-0059" num="0063">An additional advantage of the series-parallel or parallel-series processing provided by the method <b>300</b> and the method <b>400</b>, is that arranging one or more of desired algorithms within a processing block (e.g., one of the five processing blocks of the method <b>300</b>), allows the processor <b>202</b> to distribute the processing load (e.g., across multiple cores <b>204</b>) without concern for the speed of a given algorithm within a processing block (e.g., core <b>204</b>). Thus, each core <b>204</b> shares the exact same processing load and eliminates bottle necking issues caused by individual algorithms.</p><p id="p-0060" num="0064">An additional benefit of embodiments of the method <b>300</b> can include customizing a specific order of algorithms (e.g., processing blocks) to lower the computational burden within the processor <b>202</b>. As described below, the overall, multi-stage processing of a given process may be agnostic to the order of multiple sub-processes. Therefore, in some examples, ordering the fourth function <b>308</b> may have certain advantages if performed prior to the third function <b>306</b>.</p><p id="p-0061" num="0065">The method <b>300</b> can further implement different variable types for memory bandwidth optimization, such as int8, int16 and floats, for example. This can accelerate certain algorithms (e.g., based on type). In addition, this can provide increased flexibility to maximize memory bandwidth.</p><p id="p-0062" num="0066"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a functional block diagram of an embodiment of a digital signal diversity combiner. A method <b>500</b> for diversity combining can include feedforward block processing as described above. The method <b>500</b> comprises a plurality of blocks. In some example, each block may represent a processing block and perform functions in a similar manner as the processing blocks <b>315</b><i>a, </i><b>315</b><i>b </i>. . . <b>315</b><i>n </i>(<figref idref="DRAWINGS">FIG. <b>4</b></figref>), etc. In another example, a plurality of blocks can be grouped together as a single &#x201c;processing block&#x201d; that perform functions in a similar manner as the processing blocks <b>315</b><i>a, </i><b>315</b><i>b </i>. . . <b>315</b><i>n </i>(<figref idref="DRAWINGS">FIG. <b>4</b></figref>), etc.</p><p id="p-0063" num="0067"><figref idref="DRAWINGS">FIGS. <b>12</b>-<b>16</b></figref> are functional block diagrams of various embodiments of a channelizer and combiner. The methods illustrated in <figref idref="DRAWINGS">FIGS. <b>12</b>-<b>14</b>B</figref> depict example processes including a pre-calculation signal processing. As with the method <b>500</b>, one or more of methods <b>1200</b>, <b>1300</b>, <b>1400</b><i>a, </i><b>1400</b><i>b, </i><b>1500</b>, and/or <b>1600</b> can comprise a plurality processing blocks. In some examples, each block may represent a processing block and perform functions in a similar manner as the processing blocks <b>315</b><i>a, </i><b>315</b><i>b </i>. . . <b>315</b><i>n </i>(<figref idref="DRAWINGS">FIG. <b>4</b></figref>), etc. In another example, a plurality of blocks can be grouped together as a single &#x201c;processing block&#x201d; that perform functions in a similar manner as the processing blocks <b>315</b><i>a, </i><b>315</b><i>b </i>. . . <b>315</b><i>n </i>(e.g., <figref idref="DRAWINGS">FIG. <b>4</b></figref>), etc. For example, <figref idref="DRAWINGS">FIG. <b>15</b></figref> graphically depicts an example processing block implemented as a channelizer processing block <b>1500</b> and <figref idref="DRAWINGS">FIG. <b>16</b></figref> graphically depicts an example pre-calculation processing block implemented as a combiner processing block <b>1600</b>. The sub-elements or blocks of block <b>1500</b> may be executed individually as shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref> or combined into a single block. Similarly, the sub-elements or blocks of block <b>1600</b> may be executed individually as shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref> or combined into a single block.</p><p id="p-0064" num="0068">At block <b>305</b>, the SPS <b>150</b> can ingest or otherwise receive the digital bit stream <b>134</b> (e.g., via the network <b>124</b>). The data ingest at block <b>305</b> can receive the digital bit stream <b>134</b> data from a network connection (e.g., Ethernet).</p><p id="p-0065" num="0069">At block <b>310</b>, the data can be split into parallel data streams by a data splitter. In some embodiments, the processor <b>202</b> can perform data splitting functions required in block <b>310</b>. In some other embodiments, a separate data splitting component (e.g., a data splitter) can be included in the device <b>200</b> (<figref idref="DRAWINGS">FIG. <b>2</b></figref>). Splitting the data into multiple parallel streams can allow parallel processing of the downlink signal <b>132</b>. The method <b>300</b> can therefore take advantage of feedforward or pre-calculation processing to allow the incoming digitized signal data to be broken into smaller pieces and then processed on multiple cores <b>204</b>. The digital bit stream <b>134</b> can be split to form overlapping packets in in-phase/quadrature (I/Q) pairs. In some embodiments, the &#x201c;overlapping packets&#x201d; can include data packets in which successive packets are overlapped with adjacent data packets. In some embodiments the data packets may all be the same length, but overlapped. The overlap in data packets can be at the beginning of the data packet or at the end. In addition, a data packet can overlap with both the preceding and the following data packets. The data packets can also have different lengths (e.g., varying amounts of data). Therefore, a first packet sent to the processing block <b>315</b><i>a </i>may overlap or otherwise repeat certain data of a second packet sent to the processing block <b>315</b><i>b. </i></p><p id="p-0066" num="0070">The amount of overlap between packets, or overlap size can be programmable and set as needed. In some examples, the overlap can be set to one percent (1%) of the packet size. This overlap size can be increased or decreased depending on need. For example, one particular parameter that can impact the overlap size is the uncertainty of the symbol rate in the data stream <b>134</b>. For most signals, the worst case uncertainty is less than 1%, so a 1% covers most cases. In some other embodiments, the overlap can be 2%, 3%, 4%, 5%, 6%, 7%, 8%, 9%, or as high as 10%, or anywhere in between, as needed. It is also possible to have less than 1% overlap as well. The overlap could be 0.1% or lower if the data rate uncertainty is less than 0.1%.</p><p id="p-0067" num="0071">The processor <b>202</b> can implement single instruction, multiple data (SIMD) processing on the digital bit stream <b>134</b>. In some examples, SIMD can include Advanced Vector Extensions using 512 bits (AVX-512) allowing 16 floating point operations on a single CPU core on a single CPU instruction. AVX-512, for example, can process enormous amounts of data with the CPU (e.g., the CPU <b>202</b>). For example, the processor <b>202</b> (and the device <b>200</b>) can receive a 500 MHZ bandwidth data stream. 500 MHz of bandwidth is significant in some respects because that is a generally accepted practical limit of a 10 Gigabit Ethernet link. Sampling the data at 500 MHz, with 8 bits samples for an I/Q pair and including parity bits, can saturate a 10 Gbit Ethernet link. The 500 MHz example is not limiting on the disclosure. Data pipes larger than a 10 Gbit Ethernet link are possible. In addition, the processing can be split into n-number of parallel blocks (e.g., block <b>315</b>) to accommodate any amount of data.</p><p id="p-0068" num="0072">Block <b>315</b> is shown in dashed lines and depicts a processing step of the method <b>300</b>. Block <b>315</b> is shown in multiple, parallel steps, or block <b>315</b><i>a, </i><b>315</b><i>b </i>through <b>315</b><i>n. </i>The term &#x201c;parallel&#x201d; is used herein to describe that processing occurs in the processing blocks <b>315</b><i>a</i>-<b>315</b><i>n </i>concurrently, or at the same time. The packets being processed may be of different lengths from one processing block <b>315</b> to another, so the processing of packets may have the same rate or speed from one processing block <b>315</b> to the next. As noted below, some of the processing blocks <b>315</b> may proceed faster or slower than others. Accordingly, the term parallel should not be limited to simultaneous or concurrent processing within the processing blocks <b>315</b>.</p><p id="p-0069" num="0073">The processing blocks <b>315</b> as used herein, can refer to a collection of processing functions performed by the processor <b>202</b>, for example. The digital bit stream <b>134</b> can be sent into multiple parallel processing blocks <b>315</b><i>a, </i><b>315</b><i>b </i>. . . <b>315</b><i>n </i>to spread the processing load across several cores <b>204</b>. Individual processing blocks <b>315</b><i>a, </i><b>315</b><i>b </i>. . . <b>315</b><i>n </i>can represent individual iterations of cloud processing. Thus, the processing of each of the processing blocks <b>315</b><i>a</i>-<b>315</b><i>n </i>can be associated with a (cloud-based) core <b>204</b><i>a</i>-<b>204</b><i>n. </i>The number of processing blocks <b>315</b> needed varies based on the amount of data being processed. In some embodiments, the number of processing blocks <b>315</b> can be limited by the number of logical cores available via the network <b>154</b> or, for local hardware processing, within the processor <b>202</b>. In some other embodiments, memory bandwidth constraints can cause a bottle neck in the signal processing. Memory bandwidth can refer to the rate at which data can be read from or stored into a semiconductor memory (e.g., the memory <b>206</b>) by a processor (e.g., the processor <b>202</b>).</p><p id="p-0070" num="0074">In some embodiments, the number of processing blocks <b>315</b> can vary. In general, the fewer processing blocks <b>315</b> present, the better to limit the number of cores needed for the entire process. This can further enable the system to fit into smaller virtual private cloud (VPC) machines which are cheaper to operate. A VPC can include the SPS <b>150</b> having several CPUs, for example. In some embodiments, 8 processing blocks <b>315</b> can be used for a <b>10</b>Gbit Ethernet link. Such an embodiment may not include forward error correction processing blocks. In some other embodiments, the only practical limitation on the number of processing blocks <b>315</b> needed is the bitrate and bandwidth of the communication link (e.g., size of the pipe). Accordingly, any number (n) of processing blocks <b>315</b> is possible. In some embodiments, however a practical limitation on the number (n) processing blocks <b>315</b> may be present based on the number of threads that can be run on a CPU or the number of cores <b>204</b> in the processor <b>202</b>. However, if the limits are reached within a single CPU, multiple CPUs (e.g., the processor <b>202</b>) together within the SPS <b>150</b> (e.g., a VPC) and have, an unlimited number of cloud-based CPUs or cores <b>204</b> to perform the processing. In addition, the processor <b>202</b> can create new processing block <b>315</b> as needed. The processing cores <b>204</b> can be spread across multiple distributed processors (e.g., the processor <b>202</b>) as needed for throughput and efficiency.</p><p id="p-0071" num="0075">The processing blocks <b>315</b> are arranged in such a way that it does not matter which processing block <b>315</b><i>a, </i><b>315</b><i>b </i>. . . <b>315</b><i>n </i>are performed the slowest (or fastest). The method <b>300</b> can share the processing load across the processing blocks <b>315</b> and therefore alleviate any processing delays caused by bottle necking issues at individual processing blocks <b>315</b>. For example, individual subprocesses of the processing blocks <b>315</b> (see description of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, below) may not be performed or occur at equal rates (e.g., some are faster than others). Accordingly, larger process of the method <b>400</b> (<figref idref="DRAWINGS">FIG. <b>4</b></figref>), for example, can account for variations in performance or processing times. The processing blocks <b>315</b> can then be created as many times as needed to handle the incoming data.</p><p id="p-0072" num="0076">In some embodiments, each processing block <b>315</b> can represent a collection of signal processing algorithms performed by the processor <b>202</b>. As used herein, an algorithm can refer to the smallest collection of functions or method steps that perform a desired function. Multiple exemplary algorithms are described herein.</p><p id="p-0073" num="0077">An exemplary benefit of the method <b>300</b> is the ability to create more processing blocks <b>315</b> when needed. In general, the processing blocks <b>315</b> can be implemented in software, and so can be created or eliminated as needed to suit a given data rate or processing load. Each processing block <b>315</b> can be rearranged to fit the needs of different received waveforms (e.g., the downlink signal <b>132</b>) and the associated digital bit streams <b>134</b>.</p><p id="p-0074" num="0078">At block <b>320</b> the processed signal data from the multiple processing blocks <b>315</b> can be recombined to form the original data encoded and modulated on the downlink signal <b>134</b>. In some embodiments, the processor <b>202</b> can perform the functions of a data recombiner. In other embodiments, the device <b>200</b> can have an additional component to perform such functions. Each data packet or processed block of data can have a time stamp. The data recombiner (e.g., the processor <b>202</b>) can order the data blocks based on the time stamps and compare the phase between the ordered blocks. The recombiner can further adjust the phase of adjacent blocks reorder the data stream. In some embodiments, the phase of a subsequent data block can be adjusted to match the phase of a previous data block.</p><p id="p-0075" num="0079">For all processing blocks shown <b>315</b>, there are at least four options for running:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0080">1) Multiple blocks running, with each sub-element (e.g., each block <b>315</b><i>a</i>-<b>315</b><i>n</i>) within the processing block <b>315</b> getting its own core (e.g., cores <b>204</b><i>a</i>-<b>204</b><i>n</i>);</li>        <li id="ul0004-0002" num="0081">2) Multiple blocks running, with the processing block <b>315</b> getting just one dedicated core for the entire block;</li>        <li id="ul0004-0003" num="0082">3) Single Block running with each sub-element within the processing block getting its own core; and</li>        <li id="ul0004-0004" num="0083">4) Single Block running with the processing block getting just 1 dedicated core for the entire block.</li>    </ul>    </li></ul></p><p id="p-0076" num="0084">The more cores that can be run, the higher the rates that may be achievable.</p><p id="p-0077" num="0085">At block <b>325</b>, the device <b>200</b> can output the data to an appropriate receiver. In some examples such a receiver can be one or more mission operations centers. This data can be mission dependent (e.g., the purpose of the satellite), and can include, among other things, weather data, image data, and SATCOM payload data.</p><p id="p-0078" num="0086">In a general-purpose CPUs, there are at least three main factors that may limit high rate performance: 1) Data ingest, 2) CPU capacity, and 3) memory bandwidth utilization. Data Ingest refers to how fast data can be fed into the CPU. CPU capacity is driven the CPU clock speed and the number of cores within the CPU. Memory bandwidth refers to how quickly data can be transferred to/from the CPU to external DDR RAM (not CPU cache). Memory bandwidth may be determined by the number of memory lanes and the DDR RAM clock speed. In certain cases, the limiting factor for achieving high rate processing is CPU capacity but it in other cases it is memory bandwidth. Care must be taken to determine which of the above cases is impacting the performance and if it is memory bandwidth limited, the embodiments described below are non-limiting examples of ways to lower the memory bandwidth utilization within the proposed patent approach.</p><p id="p-0079" num="0087">Function calls within a given processing block can be arranged in such a manner optimize CPU computation or memory bandwidth utilization. For example, referring to function calls (illustratively depicted as blocks) shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, for the given example, the various functions calls (e.g., raise to N power block, mixing block, and decimation block) can be grouped in such a way to minimize memory bandwidth. These functions calls can be called independently so that each function is completed on a set of data before another function starts, so to simplify each function. In another example, a plurality of or all of the function calls can be combined into one block, such that data is not transferred to RAM after each executed function and the memory bandwidth for the combined function is much smaller then called independently. In the case of independently called functions, a first function call (e.g., the raise to N power) may be performed over the whole data set before a second function call (e.g. the mixing block) would occur. In the case of combining, just a portion of data would be processed in the first function call before the second is executed. In this way, memory bandwidth drops. This method can apply to any grouping of functions, not just those illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. For example, the method may be applied to the timing and phase error calculation shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref> or any other grouping for function calls to be executed in a processing block as disclosed herein (e.g., the various function call blocks illustrated in <figref idref="DRAWINGS">FIGS. <b>7</b>-<b>16</b></figref>).</p><p id="p-0080" num="0088">Another way to improve memory bandwidth utilization may be to collapse several function call blocks into one block similar to the approach described above. For example, for a channelizer, three main functions may be necessary to separate one channel to N channels: 1) a finite impulse response (FIR) filter, 2) a Circular Buffer, and 3) an Inverse fast Fourier transform (IFFT). For a combiner, three main functions may be necessary to combine M channels into one c: 1) an IFFT, 2) a Circular Buffer, and 3) a finite impulse response (FIR) filter. Normally, for ease of operation and CPU optimization, each function would require its own block as shown in <figref idref="DRAWINGS">FIGS. <b>15</b> and <b>16</b></figref>, but to lower memory bandwidth utilization, all functions can be combined into one block. This tradeoff lowers memory bandwidth utilization for a hit in CPU performance.</p><p id="p-0081" num="0000">Example Embodiment of Diversity Combiner with Blind Detection and Doppler Compensation Running on General Purpose CPUs Employing Parallel Processing on Multiple Cores to Achieve High-Throughput Operating in a Cloud Environment</p><p id="p-0082" num="0089">As described above, <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a functional block diagram of an embodiment of a method <b>500</b>. In an example, the method <b>500</b> may be referred to as Diversity Combiner with Blind Detection and Doppler Compensation method <b>500</b>. Diversity combining may be used to combine multiple antenna feeds together such that the signals are aligned in time and phase, and each are weighted based on signal quality to optimize information transfer of the multiple channels. Signal quality may be determined using, for example but not limited to, one or more of signal-to-noise ratio, energy per symbol to noise power spectral density (Es/No), power estimates, received signal strength indicators (RSSI), and the like. The multiple antenna feeds can be from one or more remote locations, such as the platform <b>110</b> or the satellite <b>111</b>. Satellites are used as an example herein, but other wireless transmission systems may be implemented such as radio antennas (e.g., the antenna <b>122</b>) or other type of transmitter. Accordingly, the use of a satellite is not limiting on this disclosure.</p><p id="p-0083" num="0090">In the case of a satellite as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, diversity combining can also be used during an antenna handover event when the platform <b>110</b> and the satellite <b>111</b> are visible from the same ground station (e.g., the ground station <b>122</b>) but, for example, the satellite <b>111</b> is falling below the horizon (e.g., in the east) and the platform <b>110</b> is rising over the horizon (e.g., in the west). In order to properly combine the downlink signals, several calculations must be performed. The disclosed system can digitize and convert the signals into digital samples which are then transported to a signal processing element. The system can further compute and compensate for Doppler effects. The system can also determine the residual phase and frequency delta (e.g., difference) between the downlink signals as well as the time differential and the estimated signal-to-noise ratios of each channel. Following these operations, the signals are then combined together.</p><p id="p-0084" num="0091">As described above, FIG. depicts a plurality of blocks that may each represent a processing block and may be implemented as one or more of the elements <b>306</b><i>a, </i><b>306</b><i>b, </i>. . . <b>306</b><i>c </i>(<figref idref="DRAWINGS">FIG. <b>4</b></figref>) and/or as one or more of the processing blocks <b>315</b><i>a, </i><b>315</b><i>b </i>. . . <b>315</b><i>n </i>(<figref idref="DRAWINGS">FIG. <b>4</b></figref>). In another example, a plurality of blocks shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> can be grouped together as a single &#x201c;processing block&#x201d; that perform functions in a similar manner as the processing blocks <b>315</b><i>a, </i><b>315</b><i>b </i>. . . <b>315</b><i>n </i>(<figref idref="DRAWINGS">FIG. <b>4</b></figref>) and/or elements <b>306</b><i>a, </i><b>306</b><i>b </i>. . . <b>306</b><i>c </i>(<figref idref="DRAWINGS">FIG. <b>3</b></figref>).</p><p id="p-0085" num="0092">The method <b>500</b> is illustratively shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> at the highest level. The method <b>500</b> comprises a plurality of processing blocks, for example, one or more Signal Analyzer processing blocks <b>510</b><i>a</i>-<b>510</b><i>n </i>(collectively referred to as Signal Analyzer processing block(s) <b>510</b> or processing block(s) <b>510</b>), one or more Doppler Compensator processing block <b>520</b><i>a</i>-<b>520</b><i>n </i>(collectively referred to as Doppler Compensator processing block(s) <b>520</b> or processing block(s) <b>520</b>), and a Diversity Combiner processing block <b>530</b>. In the illustrated example, a plurality of signal analyzer processing blocks <b>510</b><i>a</i>-<b>510</b><i>n </i>and Doppler Compensator processing block <b>520</b><i>a</i>-<b>520</b><i>n </i>are shown for executing functions on a plurality of signals, where each block is executed on a corresponding signal. Any number of signals are possible; however, the example herein will be described with reference to two signals.</p><p id="p-0086" num="0093">An example of the Signal Analyzer processing block <b>510</b> is graphically depicted in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, an example of the Doppler Compensator processing block <b>520</b> is graphically depicted in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, and an example of the Diversity Combiner processing block <b>530</b> is graphically depicted in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The method <b>300</b> and/or the method <b>400</b> may be used for processing of downlink signals <b>160</b>, <b>170</b> (e.g., <figref idref="DRAWINGS">FIG. <b>1</b></figref>) for each of the processing blocks <b>510</b>-<b>530</b>. One or more of the processing blocks <b>510</b>, <b>520</b>, and/or <b>530</b> may be implemented as the one or more of elements <b>306</b><i>a</i>-<i>c </i>in <b>306</b> of method <b>300</b> described in connection with <figref idref="DRAWINGS">FIG. <b>3</b></figref>, or one or more of elements <b>315</b><i>a</i>-<i>n </i>in <b>315</b> of method <b>315</b> described in connection with <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0087" num="0094">It is possible to execute each of processing blocks <b>510</b>, <b>520</b>, and <b>530</b> separately as well. One example would be to use only the diversity combiner processing block <b>530</b> in a case in which there is one transmitting satellite that has two independent downlink signals, such as a Right-Hand and Left-Hand Polarized outputs as opposed to the antenna handover case described earlier. In this case, the timing and Doppler effects can be ignored and therefore the Signal Analyzer processing block <b>510</b> and Doppler Compensator processing block <b>520</b> may not be required.</p><p id="p-0088" num="0095">An example Signal Analyzer processing block <b>510</b> is shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, which can be used for blind detection where the symbol rate, modulation type (referred to herein as &#x201c;mod type&#x201d;), and/or center frequency can each be estimated without any input from the user. The processing block <b>510</b> may comprise a plurality of sub-elements, such as for example, a Coarse Symbol Rate Estimator function block <b>605</b>, a Timing Recovery Error Calculator function block <b>610</b>, Mod Type and Carrier Estimator function block <b>620</b>, and the (Es/No estimator function block <b>625</b>. The processing block <b>510</b> may also comprise a Timing Recovery function block <b>615</b>.</p><p id="p-0089" num="0096">An example Coarse Symbol Rate Estimator function block <b>610</b> is graphically depicted in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. The illustrated example of the function block <b>610</b> comprises a plurality of sub-elements or sub-function blocks <b>905</b>-<b>920</b>. A first sub-function block <b>905</b> estimates a symbol rate, for example methods, using a Gardner calculation and/or executing a Diff Conjugate calculation. An example of the Gardner calculation is described in more detail in U.S. Pat. No. 10,790,920, the disclosure of which is hereby incorporated herein by reference as if set forth in full. An example of a Diff Conjugate is vector calculation where y[n]=a[n]*conj(a[n+1]); where n ranges from 0 to length of the input&#x2212;1. In either calculation, the sub-function block <b>905</b> outputs an estimated symbol rate to sub-function block <b>910</b>, where an FFT of the output of sub-function block <b>905</b> is taken and a max peak frequency detected at sub-function block <b>920</b>. The detected max peak frequency corresponds to the symbol rate, the symbol rate can be estimated based on the detected max frequency. It is also possible, in various embodiments, to measure a coarse carrier estimate of the signal from the phase of Diff Conjugate calculation y[n] at the phase calculation function block <b>915</b>.</p><p id="p-0090" num="0097">Referring back to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, examples of the Timing Recovery Error Calc <b>610</b> and Timing Recovery <b>615</b> are described in U.S. Pat. No. 10,790,920, the disclosure of which is hereby incorporated herein by reference as if set forth in full. For example, Gardner Timing Error Detector estimated in function block <b>605</b> can be applied to incoming data to create timing information, as is known in the art. In another embodiment, the incoming sample stream can be delayed by one sample. Then the non-delayed data can be multiplied by the conjugate (conjugate multiplication) of the delayed data. Both have advantages and drawbacks so it is an engineering tradeoff on which may be implemented.</p><p id="p-0091" num="0098">An example Mod Type Detect and Carrier Estimation function block <b>620</b> is graphically depicted in in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. The illustrative example of function block <b>620</b> comprises a plurality of sub-function blocks, including but not limited to, Raise to N Power function block <b>1005</b>, Mix by Coarse Carrier Estimate function block <b>1010</b>, Decimate function block <b>1015</b>, FFT Trials function block <b>1020</b>, and Peak Detection function block <b>1025</b>.</p><p id="p-0092" num="0099">After timing recovery <b>615</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the signal output from block <b>615</b> is now symbol synchronized and mod type detection becomes more accurate in the symbol space than sample space. At sub-function block <b>1005</b>, the signal input into the Mod Type Detect and Carrier Estimation function block <b>620</b> is raised to the appropriate power based on the number of symbols (N) in the outer ring of the constellation (2 for BPSK, 4 for QPSK/OQPSK, 8 for 8 PSK, 12 for 16 APSK, etc.) and then mixed by coarse carrier frequency provided by Diff Conjugate calculation (if provided) at the sub-function block <b>1010</b>. The mixed signal is then decimated at sub-function block <b>1015</b> and an FFT is then run on the signal at sub-function block <b>1020</b> to determine the peak-to-average ratio for the chosen modulation type at sub-function block <b>1025</b>. This process is then repeated for all of the desired modulation types to be detected. The result with the highest peak-to-average is the most likely modulation type. As a way to minimize memory bandwidth the sub-function block <b>1005</b>, sub-function block <b>1010</b> and sub-function block <b>1015</b> may be combined to form one sub-function block, which may decrease memory bandwidth. In order to further increase data rates, it is possible to run each modulation type trial at sub-function block <b>1020</b> on its own thread to further increase throughput.</p><p id="p-0093" num="0100">The next processing block in method <b>500</b> is Doppler Compensation processing block <b>520</b>, an example of which is graphically depicted in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. The processing block <b>520</b> may comprise a plurality of function blocks, such as but not limited to, a phase pre-calculator function block <b>705</b> and a continuous phase adjustment function block <b>710</b>. The phase corrections are precalculated at function block<b>705</b> based on the carrier estimation from the Signal Analyzer processing block <b>510</b>. The Doppler Compensator processing block <b>520</b>, in various embodiments, smooths compensation so that PLL based receivers can track compensated signals. This precalculated phase information from function block <b>705</b> is then fed into the Continuous Phase Adjustment function block <b>710</b> that removes the majority of the measured Doppler.</p><p id="p-0094" num="0101">Referring back to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the Es/No estimator function block <b>625</b> measures the Es/No. There are several approaches to measuring Es/No that are employed depending on the modulation type. One illustrative example for measuring Es/No is to calculate (C/N)&#xd7;(B/fs), where C/N is one of the carrier-to-noise ratio or signal-to-noise ratio, B is the channel bandwidth in hertz, and fs is the symbol rate or symbols per second. However, it will be appreciated that any approach for measuring Es/No will be equally applicable to the embodiments disclosed herein.</p><p id="p-0095" num="0102"><figref idref="DRAWINGS">FIG. <b>8</b></figref> graphically depicts an example of the Diversity Combiner processing block <b>530</b>. Processing block <b>530</b> can be used as a stand-alone application for cases where the timing and Doppler differences between multiple signals (e.g., two in this example) is small and the modulation type is known before-hand. In various embodiments, processing block <b>530</b> can also be used in method <b>300</b> and/or <b>400</b>. Processing block comprises a plurality of function blocks, for example but not limited to, Coarse Timing Estimator function block <b>805</b>, Timing and Phase Error Calc function block <b>810</b>, one or more Timing Adjust function blocks <b>815</b><i>a</i>-<b>815</b><i>n </i>(collectively referred to as Timing Adjust(s) <b>815</b>), one or more Phase Adjust function blocks <b>820</b><i>a</i>-<b>820</b><i>n </i>(collectively referred to as Phase Adjust(s) <b>820</b>), and Weighted Combiner function block <b>825</b>. In the illustrated example, a plurality of timing adjust function blocks <b>815</b><i>a</i>-<b>815</b><i>n </i>and phase adjust function blocks <b>820</b><i>a</i>-<i>n </i>are shown for executing functions on a plurality of signals. Any number of signals are possible, however the example described herein will be described with reference to two signals.</p><p id="p-0096" num="0103">Coarse Timing Estimator function block <b>805</b> may be used when the time delta of the two arriving signals is non-negligible. This may be required for antenna hand-over cases. The Estimator function block <b>805</b> looks for a correlation spike between multiple signals (e.g., two in this example) to determine time differences. Estimator function block <b>805</b> may utilize FFTs and/or IFFTs to quickly perform correlation; however, any correlation technique may be utilized. If many correlations are required, method <b>300</b> and <b>400</b> may be applied to increase throughput.</p><p id="p-0097" num="0104">An example of Timing and Phase Error Calculation function block <b>810</b> is graphically depicted in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. The illustrative example of the Timing and Phase Error Calculation function block <b>810</b> comprises a plurality of sub-elements or sub-function blocks, for example but not limited to, Cross Calculator sub-function block <b>1105</b>, Decimate sub-function block <b>1110</b>, Update timing estimate sub-function block <b>1115</b>, phase delta generation sub-function block <b>1120</b>, and Phase Unwrap sub-function block <b>1125</b>. The Cross Correlator sub-function block <b>1105</b> may calculate the Early, Prompt and Late (EPL) terms between the multiple input signals (e.g., two in this example) used in a Delay Lock Loop (DLL). However, SIMD techniques may be employed to calculate EPL terms efficiently. Once the EPL terms are calculated the signal is decimated at sub-function block <b>1110</b>. The delta between the Early and Late terms may be used for timing updates sub-function block <b>1115</b> on the cross correlator sub-function block <b>1105</b>, as well as a feed-forward error term for later timing adjustment.</p><p id="p-0098" num="0105">The phase of the Prompt term is then calculated at Phase Delta Generation sub-function block <b>1120</b> and fed into a phase unwrap sub-function block <b>1125</b>. In an example, the phase unwrap sub-function block <b>1125</b> may use a phase lock loop (PLL) in various embodiments. Phase unwrap function block <b>1125</b> include a phase calculation of the decimated signal that is performed before the phase unwrap function block <b>1125</b>. The phase unwrap calculation can provide continuous phase information about the data samples. The phase unwrap calculation stitches the phase together when the phase wraps either from &#x3c0; (pi) to &#x2212;&#x3c0; or &#x2212;&#x3c0; to &#x3c0; radians. This unwrapping on the angle allows for a curve fitting function to run on a phase signal without any discontinuities. This can allow the processor <b>202</b> to reassemble the demodulated signal based on timing and phase of the processed signals. It may be possible to replace the phase unwrap calculation with a Kalman filter to obtain the phase, frequency and Doppler Rate information or use a PLL.</p><p id="p-0099" num="0106">Referring back to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, this phase information from function block <b>810</b> is then supplied to the one or more timing adjust blocks function block <b>815</b> and phase adjust blocks function block <b>820</b>, to adjust the phase of the signals to properly align them for combining later on. Examples of timing adjustment and phase adjust sub-elements are described in U.S. Pat. No. 10,790,920, the disclosure of which is hereby incorporated herein by reference as if set forth in full. For example, each timing adjustment sub-element function block <b>815</b> may apply timing phase information calculated by the Timing and Phase Error Calculation function block <b>810</b>. The function block <b>815</b> may then use a filter, for example but not limited to, a polyphase FIR filter in which the appropriate bank of filters is chosen based on the provided phase information as is known in the art. The timing may then be adjusted by function block <b>815</b> such that the timing adjustment is efficient both in CPU usage and in terms of bandwidth usage. The filter used in function block <b>815</b> may employ SIMD techniques to further increase through-put. It is also possible to use linear, cubic, parabolic or other forms of interpolation. Each function block <b>815</b> performs the above described adjustment on a corresponding signal of the multiple signals (e.g., two in the illustrated example).</p><p id="p-0100" num="0107">Each of Phase Adjust function block <b>820</b> may apply carrier phase information calculated by the Timing and Phase Error Calculation function block <b>810</b>. The function block <b>820</b> may apply the phase information and use SIMD techniques to adjust the phase of the entire data block so the multiple signals are properly aligned for combining. Each function block <b>820</b> performs the above described adjustment on a corresponding signal of the multiple signals (e.g., two in the illustrated example).</p><p id="p-0101" num="0108">Once the signals have been time and phase aligned, a weighted combiner function block <b>825</b> may apply scaling based on Es/No estimates and power estimates calculated in the signal analyzer processing block <b>510</b>. For example, a signal having a better signal-to-noise ratio as compared to another signal may be assigned a higher weight than the other signal and scaled accordingly. Similarly, higher Es/No estimates and/or power estimates may be assigned greater weights and scaled accordingly. SIMD techniques may be employed to efficiently scale and combine the multiple signals (e.g., two signals in this example).</p><heading id="h-0008" level="2">Example Embodiments of Digital Signal Channelizers and Combiners on General Purpose CPUs Employing Parallel Processing on Multiple Cores to Achieve High-Throughput Operating in a Cloud Environment</heading><p id="p-0102" num="0109"><figref idref="DRAWINGS">FIGS. <b>12</b>-<b>16</b></figref> graphically depict various embodiments of a method for digital signal channelization and combination in accordance with the embodiments disclosed herein. In various embodiments, the channelization and combination depicted in <figref idref="DRAWINGS">FIGS. <b>12</b>-<b>16</b></figref> may be implemented for managing spectral bandwidth of one or more downlink signals. In some implementations, the methods shown in <figref idref="DRAWINGS">FIGS. <b>12</b>-<b>14</b>A</figref> for digital signal channelization and/or combination may be performed using the method <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> and/or method <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. A channelizer may be configured to execute a DSP algorithm in which spectral bandwidth can be separated from one channel into many channels (1-to-N) as shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>. In another example, a combiner may be configured to execute a DSP algorithm in which spectral bandwidth are combined from many channels into one channel (M-to-1) as shown in <figref idref="DRAWINGS">FIG. <b>13</b></figref>. In some implementations, the processing starts with a network appliance (e.g., one or more of the digitizers as discussed in connection to <figref idref="DRAWINGS">FIG. <b>1</b></figref>) that digitize an analog signal and the samples are then transported to a channelizer (1-to-N approach illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>) or samples from a combiner are fed to the network appliance which the convert them to an analog signal (M-to-1 approach illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>).</p><p id="p-0103" num="0110">In the case of a satellite as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a digital signal channelizer and combiner according the embodiments herein can be used for bandwidth compression where a digitizer (e.g., one of digitizers <b>124</b>, <b>134</b>, and/or <b>144</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) digitizes downlink signals (e.g., one of down link signals <b>160</b> and/or <b>170</b>) having a wide bandwidth but not all of the bandwidth is useful in processing. In one example, a downlink signal having 500 MHz of bandwidth can be digitized, but only a subset of slices (for example. 2 slices) of the bandwidth actually contain useful data. In this case, the channelizer creates 512 channels from the 500 MHz of bandwidth spectrum and then a combiner would combine two channels together, in which one channel could be 50 MHz and the other 100 MHz, for example. These two smaller channels (50 and 100 MHZ) would they be sent to a processing server <b>150</b> to be handled appropriately, whatever the case may be. In this example, only the sum of the subset of smaller channels (150 MHz) would need to be sent to the data center instead the entire 500 MHz. This, could save on transportation cost of sending data from the antenna to the data center. In this case, the digitized bandwidth is compressed, which also cuts down on the network bandwidth sent over a LAN or a WAN. Satellites are used as an example herein, but other wireless transmission systems may be implemented such as radio antennas (e.g., the antenna <b>122</b>) or other type of transmitter. Accordingly, the use of a satellite is not limiting on this disclosure.</p><p id="p-0104" num="0111">In another example, the digital signal channelizer and combiner according the embodiments herein can be used for channel splitting, in which an entire digitized bandwidth many contain many independent carriers. The channelizer in conjunction with the combiner in accordance with embodiments herein would then create the appropriate channels to be processed by receivers such, as the receiver described in U.S. Pat. No. 10,790,920, the disclosure of which is hereby incorporated herein by reference as if set forth in full.</p><p id="p-0105" num="0112">In another example, the digital signal channelizer and combiner according the embodiments herein can be used for channel combining. In this case, several downlink signals received from various types of sources, either other antennas or modulators, can be combined to create a larger composite bandwidth which could then be digitally transmitted to an antenna for broadcasting to a satellite.</p><p id="p-0106" num="0113"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates an example channelizer processing block <b>1200</b> for separating one channel into N channels. The illustrated example channelizer processing block <b>1200</b> comprises a plurality of function blocks, for example, but not limited to an N path filter function block <b>1210</b>, a N point circular buffer function block <b>1220</b>, and a N point IFFT function block <b>1230</b>. The channelizer block processing block <b>1200</b> may be configured to ingest a digitized sample having a first bandwidth from network appliance (e.g., a digitizer such as those described in connection to <figref idref="DRAWINGS">FIG. <b>1</b></figref>) through a given network protocol, for example but not limited to, TCP/IP or UDP. The sample stream is then processed at the N path filter function block <b>1210</b> using a polyphase filter bank followed by the N point circular buffer function block <b>1220</b>, and followed by the N point IFFT function block <b>1230</b> to break the signal into N channels, each N channel having a corresponding bandwidth that is smaller than the first bandwidth of the ingested digitized sample.</p><p id="p-0107" num="0114">Example sources for the input shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref> include, but are not limited to, modulators, digitizers (e.g., one of digitizers <b>124</b>, <b>134</b>, and/or <b>144</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>), outputs of Diversity Combiners (such as those described above in connection to <figref idref="DRAWINGS">FIGS. <b>5</b>-<b>11</b></figref>, for example), outputs of Doppler Compensators (such as those described above in connection to <figref idref="DRAWINGS">FIGS. <b>5</b>-<b>11</b></figref>, for example), and digital sample file players.</p><p id="p-0108" num="0115">As illustrated in <figref idref="DRAWINGS">FIGS. <b>12</b>, <b>14</b>A, and <b>14</b>B</figref>, the input feeding into <b>1210</b> may represent an input signal that is interpolated by two (as shown by the split in the input signal into two arrows), as is known in the art. This allows the channelizer <b>1200</b> to avoid aliasing at the outputs since all N outputs are interpolated by two from the start of the processing.</p><p id="p-0109" num="0116"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates an example combiner processing block <b>1300</b> for combining M channels into one channel. The illustrated example combiner processing block <b>1300</b> comprises a plurality of function blocks, for example, but not limited to a M path filter and summer function block <b>1310</b>, a M point circular buffer function block <b>1320</b>, and a M point IFFT function block <b>1330</b>. For the M-to-1 processing block <b>1300</b>, the process is reversed as compared to the channelizer processing block <b>1200</b> of <figref idref="DRAWINGS">FIG. <b>12</b></figref>. For example, M channels are fed to the M point IFFT function block <b>1330</b>, followed by a M point circular buffer function block <b>1320</b>, and then followed by an M point filter function block <b>1310</b> using a polyphase filter bank. The filtered channels are then combined using the summer of function block <b>1310</b>.</p><p id="p-0110" num="0117">Example sources for the input shown in <figref idref="DRAWINGS">FIG. <b>13</b></figref> include, but are not limited to, modulators, digitizers (e.g., one of digitizers <b>124</b>, <b>134</b>, and/or <b>144</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>), outputs of Diversity Combiners (such as those described above in connection to <figref idref="DRAWINGS">FIGS. <b>5</b>-<b>11</b></figref>, for example), outputs of Doppler Compensators (such as those described above in connection to <figref idref="DRAWINGS">FIGS. <b>5</b>-<b>11</b></figref>, for example), channelizers (such as those described in connection to <figref idref="DRAWINGS">FIG. <b>12</b></figref>), and digital sample file players. As described above in connection to <figref idref="DRAWINGS">FIG. <b>12</b></figref> and as illustrated in <figref idref="DRAWINGS">FIGS. <b>14</b>A and <b>14</b>B</figref>, each input feeding into <b>1200</b><i>a</i>-<b>1200</b><i>n, </i>respectively, may represent an input signal that is interpolated by two (as shown by the split in the input signal into two arrows), as is known in the art. This allows each channelizer <b>1200</b><i>a</i>-<b>1200</b><i>n </i>to avoid aliasing at the outputs since all N outputs are interpolated by two from the start of the processing.</p><p id="p-0111" num="0118">In some embodiments, it is possible to use one or more channelizers and one or more combiners in combination to achieve any desired bandwidth out of the one or more channelizers as shown in <figref idref="DRAWINGS">FIGS. <b>14</b>A and <b>14</b>B</figref>. For example, as shown in <figref idref="DRAWINGS">FIG. <b>14</b>A</figref>, a digitized downlink signal may be broken in to N channels (e.g., 512 channels in this example) using one or more channelizers, for example, channelizer processing blocks <b>1200</b><i>a</i>-<b>1200</b><i>n. </i>Each of the N channels may have a bandwidth that is smaller than the digitized downlink signal. In some embodiments, each channelizer processing block <b>1200</b><i>a</i>-<b>1200</b><i>n </i>may receive a discrete downlink signal, such that each Input a through Input n is not necessarily part of the same downlink signal. Thus, each input may be independent of the other inputs.</p><p id="p-0112" num="0119">Once the input signals are each broken into respective N channels, M (e.g., 20 in this example) of those channels may be recombined to form a larger channel using a combiner, for example, combiner processing block <b>1300</b> (e.g., where only a single combiner <b>1300</b> as shown in <figref idref="DRAWINGS">FIG. <b>14</b>A</figref>).</p><p id="p-0113" num="0120">In some embodiments, a plurality of combiner processing blocks <b>1300</b><i>a</i>-<b>1300</b><i>n </i>may be utilized, as shown in <figref idref="DRAWINGS">FIG. <b>14</b>B</figref>. As shown in <figref idref="DRAWINGS">FIG. <b>14</b>B</figref>, a digitized downlink signal may be broken in to N channels (e.g., 512 channels in this example) using a channelizer, for example, a channelizer processing block <b>1200</b>. Once the input signal is broken into respective N channels, M (e.g., 20 in this example) of those channels may be recombined to form a larger channel using a combiner, for example, a plurality of combiner processing blocks <b>1300</b><i>a</i>-<i>n. </i>Each combiner processing block <b>1300</b><i>a</i>-<i>n </i>may ingest N channels and combine M channels to form a larger combined channel that is output from each processing block <b>1300</b> as Output a through Output n.</p><p id="p-0114" num="0121">The selected channels for recombination may include be those channels on which the useful data is transmitted. Which data is useful may be dependent on the system processing the downlink. This methodology can be used split the digitized signal into N channels of smaller bandwidth and then to gather any of the N channels into a wider channel of any size M. Thereby, compressing the bandwidth and reducing the network resources needed for processing and transmitting the downlink signal. Thus, while the illustrative example describes 512 elements may be channelized by a given channelizer <b>1200</b> in the above example and 20 elements are combined by a given combiner <b>1300</b>, it will be appreciated that any number of elements as desired may be channelized by the one or more channelizers and any number of elements may be combined as desired may be combined using the combiner. This allows the channelizer channel bandwidths to be fully programmable and have any number of channels.</p><p id="p-0115" num="0122">In <figref idref="DRAWINGS">FIG. <b>14</b>A</figref>, two input channels are shown, but it is possible to run any number of input channels as desired, for example, one, two or more input channels, 50 or more input channels, 100 or more input channels. Where one input channel is used, <figref idref="DRAWINGS">FIG. <b>14</b></figref> may utilize a single channelizer processing block. Similarly, <figref idref="DRAWINGS">FIG. <b>14</b>B</figref> depicts two output channels, but it is possible to run any number of combiner processing blocks as desired, for example, one, two, or more output channels. Where one output channel is used, <figref idref="DRAWINGS">FIG. <b>14</b>A</figref> may utilize a single combiner processing block. Thus, in some embodiments, multiple combiners (e.g., combiner processing blocks <b>1300</b><i>a</i>-<i>n</i>) may be placed after one or more channelizer processing blocks to output many channels of scaled bandwidths of M/N. In addition, a gain stage (not shown) after the channelizer and combiner can be added to achieve either manual gain control or automated gain control of each channel.</p><p id="p-0116" num="0123">Example sources for each of the inputs shown in <figref idref="DRAWINGS">FIGS. <b>14</b>A and <b>14</b>B</figref> include, but are not limited to, modulators, digitizers (e.g., one of digitizers <b>124</b>, <b>134</b>, and/or <b>144</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>), outputs of Diversity Combiners (such as those described above in connection to <figref idref="DRAWINGS">FIGS. <b>5</b>-<b>11</b></figref>, for example), outputs of Doppler Compensators (such as those described above in connection to <figref idref="DRAWINGS">FIGS. <b>5</b>-<b>11</b></figref>, for example), and digital sample file players. In some examples, channelizers and combiners may be cascaded together. For example, a first channelizer/combiner may feed into another channelizer/combiner. In this case, the first channelizer/combiner may be configured to process any number of inputs at low sample rates and output a combined signal to another channelizer/combiner, which may output at a higher rate. For example, 100 modulators each running at 10 kSPS could be combined into a one channel running at 1 MSPS. This 1 MSPS channel may then be fed into another channelizer/combiner in which the final output sample rate is 512 MSPS. In some embodiments, each channelizer/combiner may be performed by separate processing blocks, such as one or more of blocks <b>315</b><i>a, </i><b>315</b><i>b, </i>. . . <b>315</b><i>n </i>of <figref idref="DRAWINGS">FIG. <b>4</b></figref> and/or one or more of blocks <b>306</b><i>a, </i><b>306</b><i>b, </i>. . . <b>306</b><i>n </i>of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. That is, for example, a first channelizer/combiner may be performed in a first one or more processing block and the second channelizer/combiner performed in a second one or more processing blocks.</p><p id="p-0117" num="0124">In some embodiments, method <b>1400</b><i>a </i>and <b>1400</b><i>b </i>may include an optional combiner input control <b>1410</b> (as illustrated in <figref idref="DRAWINGS">FIG. <b>14</b>A</figref>, but not shown in <figref idref="DRAWINGS">FIG. <b>14</b>B</figref>) configured to time align inputs received from the one or more channelizer processing blocks <b>1200</b>. For example, since one possible example implementation of the channelizer/combiner shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref> is to replace existing analog radio frequency (RF) switch Matrices, it may be desirable to maintain and match RF switch Matrices current capabilities. One such capability is time alignment. Since RF switch Matrices are near-zero delay combination or splitting of signals, time-alignment is trivial in the analog domain. However, once signals have been digitized and sent into networks, such as Cloud environments in which is transported over LANs or WANs, time alignment may no longer be as trivial. Time stamps may be applied to each input channel and maintained through-out all processing in one or more of channelizer processing blocks <b>1200</b>. Then at a combiner input control <b>1410</b>, data may be collected and buffered for a short, programmable (e.g., preset) duration of time in order to allow inputs to arrive within the duration of time. If all inputs arrive in time, then they may be carefully time aligned based on the corresponding input rate and time stamp of each channel by the combiner input control <b>1410</b>. However, if a channel does not arrive on time, the combiner input control <b>1410</b> may replace that channel with all zeros data source so the timely channels are not blocked or delayed further. In this way, the channelizer/combiner of <figref idref="DRAWINGS">FIG. <b>14</b>A</figref> can replicate the near-zero delay combination present in RF switch Matrices. Similarly, a combiner input control <b>1410</b> may be included in method <b>1400</b><i>b </i>between the channelizer <b>1200</b> and the plurality of combiners <b>1300</b><i>a</i>-<i>n </i>and configured similar to that described herein.</p><p id="p-0118" num="0125">In some embodiments, the methods <b>1400</b><i>a </i>and <b>1400</b><i>b </i>may also include a data split function that splits data of a given downlink signal into parallel data streams by a data splitter. Each of the channelizer of <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the combiner of <figref idref="DRAWINGS">FIG. <b>13</b></figref>, and/or the combined channelizer/combiner of <figref idref="DRAWINGS">FIGS. <b>14</b>A and <b>14</b>B</figref> may be preceded by a data split function, such as block <b>310</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. For example, each channelizer <b>1200</b> and/or each combiner <b>1300</b> as shown in <figref idref="DRAWINGS">FIGS. <b>14</b>A and <b>14</b>B</figref> may be an example of one or more processing blocks <b>315</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> and block <b>310</b> can split the data into parallel data streams as described in connection with <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The block <b>310</b> may be referred to as a manager (or dealer) processing block in some embodiments. Splitting the data into multiple parallel streams can allow for a plurality of channelizer portions <b>1200</b><i>a</i>-<i>n </i>to function in parallel threads for channelizing a downlink signal, a plurality of combiner portions <b>1300</b><i>a</i>-<i>n </i>to function in parallel threads to combine a downlink signal, and/or both one or more channelizers <b>1200</b> to function in parallel with one or more combiners <b>1300</b>. For example, in case of channelizer processing block <b>1200</b> of <figref idref="DRAWINGS">FIG. <b>14</b>A</figref>, each channelizer processing block <b>1200</b> may be implemented as one or more blocks <b>315</b><i>a</i>-<i>n </i>of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In the case of combiner processing block <b>1300</b> of <figref idref="DRAWINGS">FIG. <b>14</b>B</figref>, each combiner <b>1300</b> may be implemented as one or more of blocks <b>315</b><i>a</i>-<i>n </i>of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In both cases, block <b>310</b> can split the data from a downlink signal into parallel data streams each having overlap to an adjacent data stream (e.g., as described above in connection to <figref idref="DRAWINGS">FIG. <b>4</b></figref>). Then each parallel data stream can be sent to one of processing block <b>315</b><i>a</i>-<i>n </i>for processing. Once each processing block <b>315</b><i>a</i>-<i>n </i>has completed processing its portion of data, the processed portion of data is then sent to, for example, block <b>320</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> which outputs the processed data. Block <b>320</b> then waits for the next processing block (e.g., the second block <b>315</b><i>b</i>) to complete its processing and so on to output each portion of processed data. This process may be referred to as a round-robin dealing scheme.</p><p id="p-0119" num="0126">The higher the throughput desired for a given application, the more processing blocks <b>315</b> that are required. For example, with reference to <figref idref="DRAWINGS">FIG. <b>14</b>B</figref>, if 512 MSPS is required as the output of a given combiner processing block <b>1300</b>, it may possible to create up to &#x201c;n&#x201d; number of processing blocks <b>315</b> each implemented as a combiner processing block <b>1300</b> (e.g., one of combiner processing blocks <b>1300</b><i>a</i>-<i>n</i>) to send outputs a through n. In this case, the first portion of input data would be sent from or dealt by the block <b>310</b> to a first processing block <b>315</b><i>a </i>(e.g., implemented first combiner processing block <b>1300</b><i>a</i>) to be processed. A small portion of the data of the processed data from block <b>315</b><i>a </i>may be saved and prepended by the block <b>310</b> as overlap data on the next portion of the data stream. This next portion of the data stream, including the overlap data, is then sent to a second processing block <b>315</b><i>b </i>(e.g., implemented as a second combiner processing block <b>1300</b><i>a</i>). This pattern is repeated with as many processing blocks as is required to achieve the required high-throughput. When each of the processing blocks (e.g., the block <b>315</b><i>a</i>-<i>n</i>) is done processing its portion of data, the processed portion of data is then sent to, for example, block <b>320</b> to be output as combined processed data. Block <b>320</b> then waits for the next processing block (e.g., the second block <b>315</b><i>b</i>) to complete its processing and so on to output blocks of data. In some embodiments, block <b>310</b> may receive outputs from channelizer <b>1200</b> and split the outputs as described above for feeding into the plurality of combiners <b>1300</b><i>a</i>-<i>n. </i>In another embodiment, the block <b>310</b> may precede the channelizer <b>1200</b> for splitting the downlink signal. The processing blocks of <b>315</b><i>a </i>to <b>315</b><i>n </i>are the used in this round-robin dealing scheme in which the block <b>310</b> as the manager or dealer cycles through all the processing blocks.</p><p id="p-0120" num="0127">While the above example is described in connection to one or more combiners <b>1300</b>, it will be appreciated that the round-robin dealing scheme described above for splitting and dealing portions of data to each processing block may be utilized in any of the embodiments described herein. For example, the round-robin dealer scheme may be used for channelizer processing blocks <b>1200</b><i>a</i>-<i>n </i>of <figref idref="DRAWINGS">FIG. <b>14</b><i>a </i></figref>as well to also achieve high-throughput. That is, each channelizer processing block <b>1200</b> may be a implemented as a processing block <b>315</b><i>a</i>-<i>n </i>and a data splitter block <b>310</b> may split data into parallel streams and deal each parallel portion to a given processing block <b>315</b><i>a</i>-<i>n </i>as channelizers. Each channelizer <b>1200</b><i>a</i>-<i>n </i>may process its portion and output each plurality of N channels from each channelizer <b>1200</b><i>a</i>-<i>n </i>to the block <b>320</b> upon completion. Block <b>320</b> outputs the processed portion of data as described above and waits for the next processing block <b>315</b> to complete its processing. Similarly, this round-robin dealing scheme may be utilized in connection with the Diversity Combiner with Blind Detection and Doppler Compensation method of <figref idref="DRAWINGS">FIGS. <b>5</b>-<b>11</b></figref>. That is, any one of the processing blocks <b>510</b>-<b>530</b> may be implemented as one or processing blocks <b>315</b><i>a</i>-<i>n </i>and a data splitter function at block <b>310</b> may split input downlink signals into parallel data streams for feeding into each processing block <b>315</b><i>a</i>-<i>n </i>in the manner described above. The resulting processed data, for each processing block <b>315</b>, may then be sent to the block <b>320</b> for output as described above.</p><p id="p-0121" num="0128">In some implementations, a digital channelizer and/or combiner processing block (e.g., such as <b>1200</b>, <b>1300</b>, and/or <b>1400</b> described above) enabled by the SPS <b>150</b> (e.g., using general purpose CPUs and employing SIMD techniques, for example but not limited to, including SSE, SSE2, SSE3, SSE4.1, SSE4.2, AVX, AVX2 and AVX512 instruction sets) can process data spread over several cores of the CPU to increase throughput. The data processing can be managed over multiple cores (e.g., the blocks <b>315</b>) of the processors to achieve the necessary throughput without the use of dedicated signal processing hardware such as FPGAs or High Performance Computing (HPC) hardware such as Graphics Processing Units (GPUs). An example of a representative block <b>315</b> implemented as a channelizer is shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref>. The ability to perform this processing in general-purpose server CPU allows the functions to be deployed within a general-purpose cloud processing environment using a virtualized processing architecture without the need for dedicated hardware, such as but not limited to, x86 architecture, Cortex-A76, NEON and AWS Graviton, Graviton2, and the like.</p><p id="p-0122" num="0129"><figref idref="DRAWINGS">FIGS. <b>15</b> and <b>16</b></figref> graphically depict example processing blocks implemented as a channelizer processing block <b>1500</b> or a combiner processing block <b>1600</b>, respectively. The channelizer processing block <b>1500</b> may be substantively similar to the channelizer processing block <b>1200</b> described in connection to <figref idref="DRAWINGS">FIG. <b>12</b></figref>. For example, the channelizer processing block <b>1500</b> may separate one channel into N channels using, for example, a N path filter function block <b>1510</b>, a N point circular buffer function block <b>1520</b>, and a N point IFFT function block <b>1530</b>. The channelizer processing block <b>1500</b> is an illustrative example of a channelizer implemented as one of processing blocks <b>315</b><i>a</i>-<b>315</b><i>n </i>(e.g., <figref idref="DRAWINGS">FIG. <b>4</b></figref>) and/or one of functions <b>306</b><i>a</i>-<b>306</b><i>n </i>(e.g., <figref idref="DRAWINGS">FIG. <b>3</b></figref>). A plurality of channelizer processing blocks <b>1500</b> may be provided each as one of processing blocks <b>315</b><i>a</i>-<b>315</b><i>n </i>(e.g., <figref idref="DRAWINGS">FIG. <b>4</b></figref>) and/or one of functions <b>306</b><i>a</i>-<b>306</b><i>n </i>(e.g., <figref idref="DRAWINGS">FIG. <b>3</b></figref>). In another embodiments, functions for executing a single channelizer processing block <b>1500</b> may be distributed amongst a plurality of processing blocks <b>315</b><i>a</i>-<b>315</b><i>n </i>(e.g., <figref idref="DRAWINGS">FIG. <b>4</b></figref>) and/or one of functions <b>306</b><i>a</i>-<b>306</b><i>n </i>(e.g., <figref idref="DRAWINGS">FIG. <b>3</b></figref>). In some embodiments, the round-robin dealing scheme described above in connection to <figref idref="DRAWINGS">FIGS. <b>12</b>-<b>14</b>B</figref> may be implemented using a plurality of processing blocks <b>1500</b>. For example, a plurality of processing block <b>1500</b> may be implemented as processing blocks <b>315</b><i>a</i>-<i>n. </i>Thus, the plurality of processing blocks <b>1500</b> may be operable in parallel using the round-robin dealing scheme as described above.</p><p id="p-0123" num="0130">Similarly, the combiner processing block <b>1600</b> may be substantively similar to the combiner processing block <b>1300</b> described in connection to <figref idref="DRAWINGS">FIG. <b>13</b></figref>. For example, the combiner processing block <b>1600</b> may combine M channels into one channel using, for example, a M path filter and summer function block <b>1610</b>, a M point circular buffer function block <b>1620</b>, and a M point IFFT function block <b>1630</b>. The combiner processing block <b>1600</b> is an illustrative example of a combiner implemented as one of processing blocks <b>315</b><i>a</i>-<b>315</b><i>n </i>(e.g., <figref idref="DRAWINGS">FIG. <b>4</b></figref>) and/or one of functions <b>306</b><i>a</i>-<b>306</b><i>n </i>(e.g., <figref idref="DRAWINGS">FIG. <b>3</b></figref>). A plurality of combiner processing blocks <b>1600</b> may be provided each as one of processing blocks <b>315</b><i>a</i>-<b>315</b><i>n </i>(e.g., <figref idref="DRAWINGS">FIG. <b>4</b></figref>) and/or one of functions <b>306</b><i>a</i>-<b>306</b><i>n </i>(e.g., <figref idref="DRAWINGS">FIG. <b>3</b></figref>). In another embodiments, functions for executing a single combiner processing block <b>1600</b> may be distributed amongst a plurality of processing blocks <b>315</b><i>a</i>-<b>315</b><i>n </i>(e.g., <figref idref="DRAWINGS">FIG. <b>4</b></figref>) and/or one of functions <b>306</b><i>a</i>-<b>306</b><i>n </i>(e.g., <figref idref="DRAWINGS">FIG. <b>3</b></figref>). In some embodiments, the round-robin dealing scheme described above in connection to <figref idref="DRAWINGS">FIGS. <b>12</b>-<b>14</b>B</figref> may be implemented using a plurality of processing blocks <b>1600</b>. For example, a plurality of processing block <b>1600</b> may be implemented as processing blocks <b>315</b><i>a</i>-<i>n. </i>Thus, the plurality of processing blocks <b>1600</b> may be operable in parallel using the round-robin dealing scheme as described above.</p><p id="p-0124" num="0131">The channelizer and/or combiner described herein are examples of memory bandwidth optimization described above. Functions of a channelizer can be separated into separate blocks as shown in <figref idref="DRAWINGS">FIGS. <b>15</b> and <b>16</b></figref> or combined into one function sub-element or block in which small portions of data is processed. In this illustrative examples, a small portion may refer to one IFFT's worth of data. In the case where the functions are separated for a channelizer, many filter calculations may be ran, followed by many circular buffers, and then followed by many IFFTs. Similarly, in the case where the functions are separated for a combiner, many IFFTs may be ran, followed by many circular buffers, and followed by many filter calculations.</p><p id="p-0125" num="0132">An example non-limiting advantage of using general purpose CPUs is the dynamic nature of resource allocation. For example, in the case of the channelizer/combiner as described in connection to <figref idref="DRAWINGS">FIGS. <b>14</b>A and <b>14</b>B</figref>, it may be desirable to reconfigure the system in real time in which input channels can come and go at any moment. As shown in <figref idref="DRAWINGS">FIG. <b>14</b>A</figref>, by the processing blocks of channelizers <b>1200</b><i>a</i>-<i>n, </i>these channelizers <b>1200</b><i>a</i>-<i>n </i>can each be created and destroyed to accomplish this goal. Any value of N may be employed to accommodate an input bandwidth as well as any number of channelizers <b>1200</b><i>n </i>can be instantiated to handle the appropriate amount of input channels. In this way, as an illustrative example, at one moment, ten channels of 15 MHz each could be combined, and then one second later, five channels of 45 MHz could be combined. The distribution of processing the data at high-throughputs is possible because of all the techniques discussed in the present disclosure, such as multiple cores, SIMD techniques and memory bandwidth optimization approaches. While the example provided herein is described in connection to the channelizer/combiner as illustrated in <figref idref="DRAWINGS">FIG. <b>14</b>A</figref>, this beneficial result is equally achievable by using the techniques described herein on diversity combiners and doppler compensation, for example, as described in connection to <figref idref="DRAWINGS">FIGS. <b>5</b>-<b>11</b> and <b>14</b>B</figref>. For example, any value of number of processing blocks <b>510</b>, processing block <b>520</b>, and/or processing block <b>530</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref> may be employed to achieve a desired throughput. The number of each processing blocks <b>510</b>-<b>530</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref> may be dynamically created and destroyed to accomplish in a manner similar to that described above to achieve a desired throughput.</p><heading id="h-0009" level="2">Other Aspects</heading><p id="p-0126" num="0133">The accompanying claims and their equivalents are intended to cover such forms or modifications as would fall within the scope of the disclosure. The various components illustrated in the figures may be implemented as, for example, but not limited to, software and/or firmware on a processor or dedicated hardware. Also, the features and attributes of the specific example embodiments disclosed above may be combined in different ways to form additional embodiments, all of which fall within the scope of the disclosure.</p><p id="p-0127" num="0134">The foregoing method descriptions and the process flow diagrams are provided merely as illustrative examples and are not intended to require or imply that the operations of the various embodiments must be performed in the order presented. As will be appreciated by one of skill in the art the order of operations in the foregoing embodiments may be performed in any order. Words such as &#x201c;thereafter,&#x201d; &#x201c;then,&#x201d; &#x201c;next,&#x201d; etc. are not intended to limit the order of the operations; these words are simply used to guide the reader through the description of the methods. Further, any reference to claim elements in the singular, for example, using the articles &#x201c;a,&#x201d; &#x201c;an,&#x201d; or &#x201c;the&#x201d; is not to be construed as limiting the element to the singular.</p><p id="p-0128" num="0135">The various illustrative logical blocks, modules, and algorithm operations described in connection with the embodiments disclosed herein may be implemented as electronic hardware, computer software, or combinations of both. To clearly illustrate this interchangeability of hardware and software, various illustrative components, blocks, modules, and operations have been described above generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled artisans may implement the described functionality in varying ways for each particular application, but such implementation decisions should not be interpreted as causing a departure from the scope of the present inventive concept.</p><p id="p-0129" num="0136">The hardware used to implement the various illustrative logics, logical blocks, and modules described in connection with the various embodiments disclosed herein may be implemented or performed with a general purpose processor, a digital signal processor (DSP), an application specific integrated circuit (ASIC), a field programmable gate array (FPGA) or other programmable logic device, discrete gate or transistor logic, discrete hardware components, or any combination thereof designed to perform the functions described herein. A general-purpose processor may be a microprocessor, but, in the alternative, the processor may be any conventional processor, controller, microcontroller, or state machine. A processor may also be implemented as a combination of receiver devices, e.g., a combination of a DSP and a microprocessor, a plurality of microprocessors, one or more microprocessors in conjunction with a DSP core, or any other such configuration. Alternatively, some operations or methods may be performed by circuitry that is specific to a given function.</p><p id="p-0130" num="0137">In one or more exemplary embodiments, the functions described may be implemented in hardware, software, firmware, or any combination thereof. If implemented in software, the functions may be stored as one or more instructions or code on a non-transitory computer-readable storage medium or non-transitory processor-readable storage medium. The operations of a method or algorithm disclosed herein may be embodied in processor-executable instructions that may reside on a non-transitory computer-readable or processor-readable storage medium. Non-transitory computer-readable or processor-readable storage media may be any storage media that may be accessed by a computer or a processor. By way of example but not limitation, such non-transitory computer-readable or processor-readable storage media may include random access memory (RAM), read-only memory (ROM), electrically erasable programmable read-only memory (EEPROM), FLASH memory, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium that may be used to store desired program code in the form of instructions or data structures and that may be accessed by a computer. Disk and disc, as used herein, includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk, and Blu-ray disc where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above are also included within the scope of non-transitory computer-readable and processor-readable media. Additionally, the operations of a method or algorithm may reside as one or any combination or set of codes and/or instructions on a non-transitory processor-readable storage medium and/or computer-readable storage medium, which may be incorporated into a computer program product.</p><p id="p-0131" num="0138">It is understood that the specific order or hierarchy of blocks in the processes/flowcharts disclosed is an illustration of exemplary approaches. Based upon design preferences, it is understood that the specific order or hierarchy of blocks in the processes/flowcharts may be rearranged. Further, some blocks may be combined or omitted. The accompanying method claims present elements of the various blocks in a sample order, and are not meant to be limited to the specific order or hierarchy presented.</p><p id="p-0132" num="0139">The previous description is provided to enable any person skilled in the art to practice the various aspects described herein. Various modifications to these aspects will be readily apparent to those skilled in the art, and the generic principles defined herein may be applied to other aspects.</p><p id="p-0133" num="0140">Thus, the claims are not intended to be limited to the aspects shown herein, but is to be accorded the full scope consistent with the language claims, wherein reference to an element in the singular is not intended to mean &#x201c;one and only one&#x201d; unless specifically so stated, but rather &#x201c;one or more.&#x201d;</p><p id="p-0134" num="0141">The word &#x201c;exemplary&#x201d; is used herein to mean &#x201c;serving as an example, instance, or illustration.&#x201d; Any aspect described herein as &#x201c;exemplary&#x201d; is not necessarily to be construed as preferred or advantageous over other aspects. Unless specifically stated otherwise, the term &#x201c;some&#x201d; refers to one or more.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for managing channel bandwidth of one or more downlink signals, the method comprising:<claim-text>receiving the one or more downlink signals from one or more antenna feeds, the one or more downlink signals having a first bandwidth;</claim-text><claim-text>in a first one or more processing blocks in one or more processors,<claim-text>performing a first channelizer operation on a first portion of the one or more downlink signals that creates a first plurality of channels, each of the first plurality of channels having a bandwidth smaller than the first bandwidth;</claim-text></claim-text><claim-text>in a second one or more processing blocks in the one or more processors in parallel with the first one or more processing blocks,<claim-text>performing a second channelizer operation on a second portion of the one or more downlink signals that creates a second plurality of channels, each of the second plurality of channels having a bandwidth smaller than the first bandwidth; and</claim-text></claim-text><claim-text>in a third one or more processing blocks in the one or more processors,</claim-text><claim-text>performing a combiner operation that combines a subset of the first plurality of channels and a subset of the second plurality of channels into a combined channel, the combined channels having a second bandwidth smaller than the first bandwidth, and</claim-text><claim-text>outputting the combined channel to a network.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first one or more processing blocks comprises a first one or more central processing unit (CPU) cores and the second one or more processing blocks comprises a second one or more CPU cores.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processors comprises a plurality of processors, wherein in the first one or more processing blocks are comprised in a first processor of the plurality and the second one or more processing blocks are comprised in a second processor of the plurality of processors.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processors comprises a single processor comprising the first one or more processing blocks and the second one or more processing blocks.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein at least one of the third one or more processing blocks is comprised as part of one or more of (i) the one or more first processing blocks and (ii) the one or more second processing blocks.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the third one or more processing blocks are separate from the first one or more processing blocks and the second one or more processing blocks.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first channelizer operation comprises a first plurality of functions including:<claim-text>ingesting samples from an input channel, the samples received as first data packets included in the first portion of the one or more downlink signals;</claim-text><claim-text>applying a multiple path filter on the samples;</claim-text><claim-text>executing a multiple point circular buffer on the filtered samples; and</claim-text><claim-text>separating the samples into multiple channels based applying a multiple point Inverse Fast Fourier Transform to the samples.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the first one or more processing blocks comprises a first plurality of processing blocks, wherein performing the first channelizer operation comprises distributing the first plurality of functions amongst the first plurality of processing blocks and performing the distributed first plurality of functions in parallel.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the second channelizer operation comprises a second plurality of functions including:<claim-text>ingesting samples from an input channel, the samples received as second data packets included in the second portion of the one or more downlink signals;</claim-text><claim-text>applying a multiple path filter on the samples;</claim-text><claim-text>executing a multiple point circular buffer on the filtered samples; and</claim-text><claim-text>separating the samples into multiple channels based applying a multiple point Inverse Fast Fourier Transform to the samples.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the second one or more processing blocks comprises a second plurality of processing blocks, wherein performing the second channelizer operation comprises distributing the second plurality of functions amongst the second plurality of processing blocks and performing the distributed second plurality of functions in parallel.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein one or more of the second plurality of functions are performed in the second one or more processing blocks in parallel with one or more of the first plurality of functions performed in the first one or more processing blocks.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the combiner operation comprises combining spectral bandwidth from a plurality of input channels into the combined channel, the plurality of input channels comprising the subset of the first plurality of channels and the subset of the second plurality of channels.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the combiner operation comprises a third plurality of functions including:<claim-text>ingesting samples from the subset of the first plurality of channels and the subset of the second plurality of channels as a plurality of input channels, the samples received as data packets included in the subset of the first plurality of channels and the subset of the second plurality of channels;</claim-text><claim-text>applying a multiple point Inverse Fast Fourier Transform to the samples on each of the multiple channels;</claim-text><claim-text>executing a multiple point circular buffer for each of the multiple channels based on the application of the multiple point Inverse Fast Fourier Transform on the samples; and</claim-text><claim-text>combining the samples into the combined channel based on applying a multiple path filter and a summer.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. (canceled)</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. (canceled)</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. (canceled)</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising dividing, at one or more processors, the digital bit stream into a plurality of data packets, each of the data packets of the plurality of data packets including an overlap of data from an adjacent packet.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. (canceled)</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. (canceled)</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processors are one or more general-purpose central processing units (CPU).</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processors employ single instructions, multiple data (SIMD) techniques to achieve high throughput.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. A method for managing channel bandwidth of one or more downlink signals, the method comprising:<claim-text>receiving the one or more downlink signals from one or more antenna feeds, the one or more downlink signals having a first bandwidth;</claim-text><claim-text>in a first one or more processing blocks in one or more processors,<claim-text>performing a first channelizer operation on a first portion of the one or more downlink signals that creates a first plurality of channels, each of the first plurality of channels having a bandwidth smaller than the first bandwidth;</claim-text></claim-text><claim-text>in a second one or more processing blocks in the one or more processors in parallel with the first one or more processing blocks,<claim-text>performing a second channelizer operation on a second portion of the one or more downlink signals that creates a second plurality of channels, each of the second plurality of channels having a bandwidth smaller than the first bandwidth; and</claim-text></claim-text><claim-text>outputting the first and second plurality of channels to a network.</claim-text></claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the first one or more processing blocks comprises a first one or more central processing unit (CPU) cores and the second one or more processing blocks comprises a second one or more CPU cores.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the one or more processors comprises a plurality of processors, wherein in the first one or more processing blocks are comprised in a first processor of the plurality and the second one or more processing blocks are comprised in a second processor of the plurality of processors.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the one or more processors comprises a single processor comprising the first one or more processing blocks and the second one or more processing blocks.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the first channelizer operation comprises a first plurality of functions including:<claim-text>ingesting samples from an input channel, the samples received as first data packets included in the first portion of the one or more downlink signals;</claim-text><claim-text>applying a multiple path filter on the samples;</claim-text><claim-text>executing a multiple point circular buffer on the filtered samples; and</claim-text><claim-text>separating the samples into multiple channels based applying a multiple point Inverse Fast Fourier Transform to the samples.</claim-text></claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The method of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein the first one or more processing blocks comprises a first plurality of processing blocks, wherein performing the first channelizer operation comprises distributing the first plurality of functions amongst the first plurality of processing blocks and performing the distributed first plurality of functions in parallel.</claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The method of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein the second channelizer operation comprises a second plurality of functions including:<claim-text>ingesting samples from an input channel, the samples received as second data packets included in the second portion of the one or more downlink signals;</claim-text><claim-text>applying a multiple path filter on the samples;</claim-text><claim-text>executing a multiple point circular buffer on the filtered samples; and</claim-text><claim-text>separating the samples into multiple channels based applying a multiple point Inverse Fast Fourier Transform to the samples.</claim-text></claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. The method of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the second one or more processing blocks comprises a second plurality of processing blocks, wherein performing the second channelizer operation comprises distributing the second plurality of functions amongst the second plurality of processing blocks and performing the distributed second plurality of functions in parallel.</claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The method of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein one or more of the second plurality of functions are performed in the second one or more processing blocks in parallel with one or more of the first plurality of functions performed in the first one or more processing blocks.</claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, further comprising dividing, at one or more processors, the digital bit stream into a plurality of data packets, each of the data packets of the plurality of data packets including an overlap of data from an adjacent packet.</claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. (canceled)</claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. (canceled)</claim-text></claim><claim id="CLM-00034" num="00034"><claim-text><b>34</b>. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the one or more processors are one or more general-purpose central processing units (CPU).</claim-text></claim><claim id="CLM-00035" num="00035"><claim-text><b>35</b>. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the one or more processors employ single instructions, multiple data (SIMD) techniques to achieve high throughput.</claim-text></claim><claim id="CLM-00036" num="00036"><claim-text><b>36</b>. A method for managing channel bandwidth of one or more downlink signals, the method comprising:<claim-text>receiving input signals of the one or more downlink signals on a plurality of input channels, each input channel having a spectral bandwidth;</claim-text><claim-text>in a plurality of processing blocks in one or more processors, performing a combiner operation on the input channels that combines the spectral bandwidth of a selected subset of the plurality of input channels into a combined channel, the plurality of processing blocks operating in parallel, and</claim-text><claim-text>outputting the combined channel to a network.</claim-text></claim-text></claim><claim id="CLM-00037" num="00037"><claim-text><b>37</b>. The method of <claim-ref idref="CLM-00036">claim 36</claim-ref>, wherein the plurality of processing blocks comprises a plurality of central processing unit (CPU) cores.</claim-text></claim><claim id="CLM-00038" num="00038"><claim-text><b>38</b>. The method of <claim-ref idref="CLM-00036">claim 36</claim-ref>, wherein the one or more processors comprises a plurality of processors, wherein in at least a first processing block of the plurality of processing blocks is comprised in a first processor of the plurality and a second processing block of the plurality of processing blocks is comprised in a second processor of the plurality of processors.</claim-text></claim><claim id="CLM-00039" num="00039"><claim-text><b>39</b>. (canceled)</claim-text></claim><claim id="CLM-00040" num="00040"><claim-text><b>40</b>. The method of <claim-ref idref="CLM-00036">claim 36</claim-ref>, wherein the combiner operation comprises a plurality of functions including:<claim-text>ingesting samples from the selected subset of the input channels, the samples received as data packets included in the subset of the input channels;</claim-text><claim-text>applying a multiple point Inverse Fast Fourier Transform to the samples on each of the multiple channels;</claim-text><claim-text>executing a multiple point circular buffer for each of the multiple channels based on the application of the multiple point Inverse Fast Fourier Transform on the samples; and</claim-text><claim-text>combining the samples into the combined channel based on applying a multiple path filter and a summer.</claim-text></claim-text></claim><claim id="CLM-00041" num="00041"><claim-text><b>41</b>. The method of <claim-ref idref="CLM-00036">claim 36</claim-ref>, wherein performing the combiner operation comprises distributing the plurality of functions amongst the plurality of processing blocks and performing the distributed plurality of functions in parallel.</claim-text></claim><claim id="CLM-00042" num="00042"><claim-text><b>42</b>. The method of <claim-ref idref="CLM-00036">claim 36</claim-ref>, wherein at least two of the plurality of functions are performed by the separate processing blocks of the plurality of processing blocks in parallel.</claim-text></claim><claim id="CLM-00043" num="00043"><claim-text><b>43</b>. The method of <claim-ref idref="CLM-00036">claim 36</claim-ref>, wherein the plurality of processing blocks comprises a first one or more processing blocks and a second one or more processing blocks, wherein:<claim-text>in the first one or more processing blocks, performing a first combiner operation performing on a first subset of input signals corresponding to a first subset of the plurality of input channels, and</claim-text><claim-text>in the second one or more processing blocks in parallel with the first one or more processing blocks, performing a second combiner operation on a second subset of input signals corresponding to a second subset of the plurality of input channels,</claim-text><claim-text>wherein the first and second combiner options combines the spectral bandwidth of the first and second subset of the plurality of input channels into the combined channel.</claim-text></claim-text></claim><claim id="CLM-00044" num="00044"><claim-text><b>44</b>. The method of <claim-ref idref="CLM-00036">claim 36</claim-ref>, further comprising dividing, at one or more processors, the digital bit stream into a plurality of data packets, each of the data packets of the plurality of data packets including an overlap of data from an adjacent packet.</claim-text></claim><claim id="CLM-00045" num="00045"><claim-text><b>45</b>. (canceled)</claim-text></claim><claim id="CLM-00046" num="00046"><claim-text><b>46</b>. (canceled)</claim-text></claim><claim id="CLM-00047" num="00047"><claim-text><b>47</b>. The method of <claim-ref idref="CLM-00036">claim 36</claim-ref>, wherein the one or more processors are one or more general-purpose central processing units (CPU).</claim-text></claim><claim id="CLM-00048" num="00048"><claim-text><b>48</b>. The method of <claim-ref idref="CLM-00036">claim 36</claim-ref>, wherein the one or more processors employ single instructions, multiple data (SIMD) techniques to achieve high throughput.</claim-text></claim><claim id="CLM-00049" num="00049"><claim-text><b>49</b>. A system managing channel bandwidth one or more downlink signals, the system comprising:<claim-text>one or more antennas configured to receive the plurality of downlink signals; and</claim-text><claim-text>one or more processors communicatively coupled to the plurality of antennas, the one or more processors having a plurality of processing blocks and operable to perform the method of <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim-text></claim></claims></us-patent-application>