<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004831A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004831</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364293</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEM AND METHOD FOR FINE AND COARSE ANOMALY DETECTION WITH MULTIPLE AGGREGATION LAYERS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>NXP B.V.</orgname><address><city>Eindhoven</city><country>NL</country></address></addressbook><residence><country>NL</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Renes</last-name><first-name>Joost Roland</first-name><address><city>Eindhoven</city><country>NL</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Bos</last-name><first-name>Joppe Willem</first-name><address><city>Wijgmaal</city><country>BE</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Veshchikov</last-name><first-name>Nikita</first-name><address><city>Brussels</city><country>BE</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>NXP B.V.</orgname><role>03</role><address><city>Eindhoven</city><country>NL</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Embodiments address the problem of detecting anomalies in data sets with respect to well-defined normal behavior. Deviations of data collected in real-time are detected using a previously observed distribution of data known to be benign. Embodiments provide techniques to detect varying types of anomalies by creating multiple aggregation layers having varying granularities on top of the lowest level of data collection. This allows detection of fine anomalies that strongly impact single data points, as well as coarse anomalies that detect multiple data points less strongly. Machine learning models are trained and used to compare real-time data sets against behavior of a benign data set in order to detect differences and to flag anomalous behavior.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="78.23mm" wi="158.75mm" file="US20230004831A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="210.82mm" wi="118.28mm" orientation="landscape" file="US20230004831A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="224.20mm" wi="121.07mm" orientation="landscape" file="US20230004831A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="231.31mm" wi="122.00mm" orientation="landscape" file="US20230004831A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="224.45mm" wi="139.70mm" orientation="landscape" file="US20230004831A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><heading id="h-0002" level="1">Field</heading><p id="p-0002" num="0001">This disclosure relates generally to information system security, and more specifically, to anomaly detection in data sets with respect to well-defined normal behavior.</p><heading id="h-0003" level="1">Related Art</heading><p id="p-0003" num="0002">As the value and use of information continues to increase, individuals and businesses seek additional ways to process and store information. An information handling system generally processes, compiles, stores, or communicates information or data for business, personal, or other purposes, thereby allowing users to take advantage of the value of the information. Because technology and information handling needs and requirements vary between different users or applications, information handling systems may also vary regarding what information is handled, how the information is handled, how much information is processed, stored, or communicated, how quickly and efficiently the information may be processed, stored, or communicated, and security of the information processing, storage, or communication.</p><p id="p-0004" num="0003">Attacks on information handling systems can have a variety of profiles, including a single significant attack or a series of smaller attacks. While a single significant attack may be readily detectable as an anomaly, a series of smaller attacks (e.g., malicious behavior that stretches over a long period of time or a large set of small fraudulent transactions) can fly under the radar of traditional detection systems. A stealthy anomaly hides malicious behavior by attacking more data points, but with less variance from benign behavior (e.g., having a limited effect on the single data points, with a significant effect over the aggregate). Detecting both a significant attack on a single data point and a set of smaller attacks on multiple data points is important to protecting information handling systems and the data they provide.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0005" num="0004">Embodiments of the present invention may be better understood by referencing the accompanying drawings.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a chart illustrating an example of a time series of hardware performance counters on an internet of things device subject to analysis by embodiments of the present invention.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a simplified block diagram illustrating an example of machine-learning algorithms using varying aggregation windows collected in parallel, such as that performed by embodiments of the present invention.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a simplified flow diagram illustrating an example of a data flow for training machine-learning models, in accordance with embodiments of the present invention.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a simplified flow diagram illustrating an example of a data flow for making inferences by trained machine learning models against new inputs, in accordance with embodiments of the present invention.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a simplified block diagram illustrating an example of a multi-core applications processor incorporating hardware that can be used to implement the system and method of the present media presentation system.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0011" num="0010">The use of the same reference symbols in different drawings indicates identical items unless otherwise noted. The figures are not necessarily drawn to scale.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0012" num="0011">Embodiments of the present invention are intended to address the problem of detecting anomalies in data sets with respect to well-defined normal behavior. Embodiments detect deviations of data collected in real-time from a previously observed distribution of data known to be benign. Such anomalies can often be difficult to detect due to a large variance of data points in normal behavior. Reducing the variance by aggregating multiple data points, such as averaging, loses information about deviations of single points. Embodiments provide techniques to detect varying types of anomalies by creating multiple aggregation layers having varying granularities on top of the lowest level of data collection. This allows detection of fine anomalies that strongly impact single data points, as well as coarse anomalies that impact multiple data points less strongly. Machine learning models are trained and used to compare real-time data sets against behavior of a benign data set in order to detect differences and to flag anomalous behavior.</p><p id="p-0013" num="0012">Embodiments of the present invention apply these machine learning techniques to detect stealthy as well as non-stealthy types of anomalies in behavior. A non-stealthy anomaly is characterized as strongly affecting data behavior in a small window, such that within a small number of data points, or even a single data point, the behavior is significantly changed from normal behavior. By comparison with a benign data set, using a trained model, embodiments can observe differences and flag anomalous behavior. On the other hand, a stealthy anomaly attack hides its behavior by affecting more data points, but less strongly. That is, a stealthy anomaly has only a limited effect on a single data point. Thus, in instances where variance in data point values is large, the effect of the anomaly can be hidden in the noise of the system and therefore be undetectable when investigating a single data point.</p><p id="p-0014" num="0013">In order to capture attacks that result in both non-stealthy and stealthy anomalies, embodiments utilize parallel data aggregation methods that transform a single data set into several data sets that range from very fine (e.g., having little aggregation and high variance in data points) to very coarse (e.g., having strong aggregation and low variance in data points). By executing anomaly detection methods on those types of data sets in parallel there will be stronger detection of the variety of anomalies. For the sake of clarity and simplicity of explanations within this disclosure, examples are focused on temporal data. But the techniques described herein can be applied to any data types that can be analyzed on different levels of detail (e.g., single data point, several data points aggregated together because of their position in space or in time, and the like).</p><p id="p-0015" num="0014">One example of a scenario that can be analyzed using embodiments of the present invention is anomaly detection for Internet of things (IoT) devices that is performed using monitoring of a hardware performance counter (HPC). One example of such an HPC is performance monitoring units (PMUs) defined for various families of ARM architecture devices. In such devices, to avoid detection, malware can be used that minimizes an effect on HPCs for a given time period, for example by stretching the malicious behavior over a longer period of time. Embodiments can enable detecting both attacks impacting performance strongly for a short period of time, as well as attacks that impact performance less for a longer period of time.</p><p id="p-0016" num="0015">Another scenario that can be analyzed using embodiments of the present invention is financial fraud detection. In such scenarios, while single large transactions that differ from normal behavior can be easy to detect, small fraudulent deviations can be more difficult to detect. Further, even though single transactions can vary widely, the behavior over a longer period of time will be more stable. As small deviations need to be extended over a long period of time to lead to meaningful gain for a malicious entity, data aggregation will enable analysis of such attacks.</p><p id="p-0017" num="0016">Yet another example scenario is detecting anomalies and data that is not time dependent. For example, embodiments can analyze images or three-dimensional data. In the instance of an image, individual pixels and aggregations of regions of pixels can both be examined for deviation. Separate models are then used to analyze individual pixels and their relations as well as regions of the image.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a chart illustrating an example of a time series of hardware performance counters (HPCs) on an IoT device subject to analysis by embodiments of the present invention. Chart <b>110</b> illustrates results of sampling performance counters every millisecond, which leads to relatively large variance between each data point. Chart <b>120</b> illustrates results of aggregating the data points over every minute, which leads to count data having much smaller variance. It should be noted that the vertical scale of Chart <b>120</b> is not the same as that of Chart <b>110</b> (e.g., there are significantly higher count values in Chart <b>120</b> over the minute time period versus the millisecond time period of Chart <b>110</b>). The relative portion of HPC's that are used by malware is about the same between the shorter or the longer sampling periods. But HPC data collected over a longer time averages out noise seen between each millisecond data point and therefore there is a smaller variance in the number of counters used by malware versus the number of counters used for benign purposes. Therefore, stealthy or attacks can potentially be detected better using longer aggregated sampling periods.</p><p id="p-0019" num="0018">Since some system attacks can result in significant deviance from normal behavior of a few, or one, data point while other attacks may require analysis over a significant period of time to average out the noise in the aggregate, as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, embodiments apply machine-learning trained techniques in parallel to data sets aggregated at various granularities. In this manner, anomalies that impact a data point strongly for a short period of time can be detected and responded to, as well as anomalies that have a low impact on a multitude data points.</p><p id="p-0020" num="0019">Detection techniques having a multitude of granularities provides other advantages. For single point analysis, since anomaly inference has to be run for each data point separately, of which there can be many, there is little time for classification of the anomalies. One main advantage of running detection on single data points, on the other hand, is that the system can respond quickly when anomalies are detected and clearly determine where and when the anomaly took place.</p><p id="p-0021" num="0020">When running anomaly detection on an aggregated version of the data set, relative change in behavior is the same. But, as illustrated above, aggregated data can cancel out noise and therefore there is smaller variance in each time sample. Further, as anomaly inference and classification is executed fewer times for a given time, as compared with analysis of each single point, more resources can be dedicated to the analysis leading to a more powerful classification.</p><p id="p-0022" num="0021">For effective anomaly detection, embodiments can provide the best of both techniques. It is desirable to have quick response to anomalies, as well as detectability of stealthy anomalies. Embodiments, therefore, train a machine learning system on known data aggregated using various granularities, create models for these different granularity data sets, and perform inference for those models in an execution environment. During analysis of an executing environment, data aggregation is performed on the fly and the machine learning models are applied to the aggregated execution environment data sets.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a simplified block diagram illustrating an example of machine learning algorithms using varying aggregation windows collected in parallel, such as that performed by embodiments of the present invention. ML<sub>i </sub>is a machine learning algorithm for a sampling granularity &#x201c;i&#x201d;. A choice of the granularities of aggregation windows and a number of machine learning algorithms to run in parallel depends on the nature of the application. For scenarios that are resource constrained (e.g., many embedded systems), fewer aggregation windows and analysis models may be used (e.g., aggregating every 1000 data points, for example), while applications having more resources may run many aggregation windows and analysis models (e.g., aggregation every 10, 100, and 1000 data points) at the same time. <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates four different granularities of aggregation windows <b>210</b>, <b>220</b>, <b>230</b>, and <b>240</b>. Aggregation window <b>210</b> analyzes a short data period (e.g., HPC counters measured at 1 ms intervals), aggregation window <b>220</b> aggregates three data periods, aggregation window <b>230</b> aggregates six data periods, and aggregation window aggregates 12 data periods.</p><p id="p-0024" num="0023">Factors that can be taken into account when determining the number of data points included in an aggregation window for analysis include, for example, the normal activity of the application. That is, does the application typically perform activities on a short timescale or does the application typically perform activities over a long timescale (e.g., one time every four days). Another normal activity factor can be how long an application functions once the application is activated (for example, milliseconds or minutes). As discussed above, another factor can be the resources available to perform detection operations. Longer aggregation periods or lower powered classification can be used for resource-limited applications, where quick response may not be required.</p><p id="p-0025" num="0024">Embodiments can also use a rolling window for aggregation. For example, an aggregation layer that having a granularity of ten samples can aggregate data points 1-10, 2-11, 3-12, and so on, as opposed to 1-10, 11-20, 21-30, as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Using a rolling window will result in more training examples from a known dataset due to the overlap in the aggregate samples. A rolling window can also result in better detection in an execution environment because single data points including anomalous behavior will be in several consecutive aggregate samples. But analysis will need to be run more often (e.g., for every data point, as opposed to every ten data points). This will be a preferred method for applications executing on devices with more resources, while resource-constrained devices may not opt for a rolling window.</p><p id="p-0026" num="0025">In a typical implementation of embodiments of the present invention, multiple machine learning models can be executing simultaneously. Therefore classifications can be received from the multiple models at the same time. Handling the various classifications is dependent upon the nature the application. For example, if it is important to avoid false positives, then one can wait until several anomaly detections have been registered by the models. On the other hand, if avoiding malicious behavior is a top priority, then as soon as an anomaly is detected the system can respond.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a simplified flow diagram illustrating an example of a data flow <b>300</b> for training machine learning models, in accordance with embodiments of the present invention. Combining several machine learning models into a single model is known in the art of machine learning. There are several common approaches such as, for example, stacking, ensemble learning, and boosting. Each scheme has a small variant that can have other identifying names. &#x201c;Stacking&#x201d; employs several machine learning models to produce an intermediate output and then combines the outputs using yet another machine learning model to make a final prediction. &#x201c;Ensemble Learning&#x201d; employs several different machine learning algorithms to build several models, and then the final prediction is made based on a combination (e.g., voting) of the outcomes predicted by all the models. &#x201c;Boosting,&#x201d; on the other hand, uses different partial subsets of a data set to create several machine learning models from those subsets, and then each model generates a prediction, and finally each prediction is provided to a voting scheme to determine a final prediction.</p><p id="p-0028" num="0027">Embodiments employ a different method for combining models into a single model. As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, training <b>300</b> involves providing machine learning models <b>320</b>, <b>330</b>, and <b>340</b> different sized aggregations of training data set <b>315</b>. As illustrated, a first machine learning model <b>320</b> is trained from individual data points from training data set <b>315</b> (e.g., ML<sub>1 </sub><b>210</b> from <figref idref="DRAWINGS">FIG. <b>2</b></figref>). A second machine learning model <b>330</b> is trained from a first aggregation level from the training data set (e.g., ML<sub>2 </sub>from <figref idref="DRAWINGS">FIG. <b>2</b></figref>). A first aggregation mechanism <b>335</b> is employed to perform the aggregation, which can include one of sequential aggregation or a rolling aggregation, as discussed above. A third machine learning model <b>340</b> is trained from a second aggregation level from the training data set (e.g., ML<sub>3 </sub>from <figref idref="DRAWINGS">FIG. <b>2</b></figref>). A second aggregation mechanism <b>345</b> is employed to perform this second aggregation level, which will be the same method of sequential or rolling aggregation as that chosen for the first aggregation mechanism. The second aggregation mechanism will typically aggregate a larger number of data points from training data set <b>315</b> than will the first aggregation mechanism. As illustrated, second aggregation mechanism <b>345</b> can either draw single data points from training data set <b>315</b> to perform the aggregation, or use already aggregated data sets from first aggregation mechanism <b>335</b>.</p><p id="p-0029" num="0028">Training data set <b>315</b> contains data associated with a desired mode of operation of the application being classified. For example, the training data set can include hardware performance counter data associated with a processor performing typical operations over an extended period of time. Supervised learning algorithms are used to build models <b>320</b>, <b>330</b>, and <b>340</b> of the different aggregation levels (e.g., granularities) associated with the training data set. Depending on the nature the application, either a same machine learning algorithm can be used for each aggregation level or different machine learning algorithms can be utilized. When the same machine learning algorithm is utilized for each aggregation level, that makes it fairly easy to re-use existing code bases with a different data set. On the other hand, one anomaly detection algorithm may be better suited for dealing with a large amount of noise in the data sets (e.g., little aggregation) or another may be better suited for dealing with a low noise situation (e.g., significant aggregation). Thus, utilizing the same machine learning algorithm for multiple aggregation layers can lead to suboptimal performance. Utilizing different machine learning algorithms for the various aggregation layers can lead to better performance, allowing selection of an optimal algorithm for each aggregation layer. But this increases the complexity of the machine learning scheme, as multiple models will need to be tuned and optimized.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a simplified flow diagram illustrating an example of a data flow <b>400</b> for making inferences by trained machine learning models against new inputs, in accordance with embodiments of the present invention. In this flow, new inputs <b>410</b> (e.g., an operational dataset including information from an operational environment) are generated by the application whose performance is being analyzed by the various models (e.g., hardware performance counters, financial data, and the like). The stream of new inputs are provided to the trained models from <figref idref="DRAWINGS">FIG. <b>3</b></figref> (e.g., models <b>320</b>, <b>330</b>, and <b>340</b>) either directly for single data points to be analyzed by first machine learning model <b>320</b>, or subsequent to aggregation by first aggregation mechanism <b>420</b> to machine learning model <b>330</b> or second aggregation mechanism <b>430</b> to machine learning model <b>340</b>. Each aggregation mechanism waits until additional data arrives at the aggregation mechanism before submitting the aggregated version of the data to the associated model. As discussed above, aggregation can be either sequential or rolling, depending upon the nature the application.</p><p id="p-0031" num="0030">First machine learning model <b>320</b> generates first results <b>440</b> quickly over a small or single number of inputs. In certain applications, this allows for the system to react quickly to anomalous data that has a significant effect on a single or small number of data points. Second machine learning model <b>330</b> generates second results <b>450</b> after analyzing more data generated and then aggregated by the application. Similarly, third machine learning model <b>340</b> generates third results <b>460</b> after analyzing an even greater amount of data generated and then aggregated by the application. As discussed above, the analysis performed over the greater number of aggregated data points by the second and third machine learning models allows for detection of anomalous behavior that is only exhibited or detectable when averaging out noise from individual or a small aggregate number of data points.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a simplified block diagram illustrating an example of a multi-core applications processor <b>500</b> incorporating hardware that can be used to implement the system and method of the present media presentation system. A system interconnect <b>515</b> communicatively couples all illustrated components of the multi-core applications processor. A set of processor cores <b>510</b>(<b>1</b>)-(N) are coupled to system interconnect <b>915</b>. Each processor core includes at least one CPU and local cache memory. Further coupled to the system interconnect are input/output devices <b>520</b>, including necessary input/output devices for an application, such as display, keyboard, mouse, and other associated controllers. The applications processor also includes a network port <b>525</b> operable to connect to a network <b>530</b>, which is likewise accessible to one or more remote servers <b>535</b>. The remote servers can provide deep learning data sets for the portions of the present system that utilize artificial intelligence/machine learning operations, as discussed above.</p><p id="p-0033" num="0032">An accelerator <b>540</b> is also communicatively coupled to processor cores <b>510</b>. Accelerator <b>540</b> is circuitry dedicated to performing specialized tasks, such as machine learning associated with anomaly detection for an application, a process, or data, as discussed above. Through the system interconnect, any of the processor cores can provide instructions to the machine learning accelerator.</p><p id="p-0034" num="0033">In addition to the machine learning accelerator and image signal processor, other peripherals or peripheral controllers <b>550</b> and disk storage or disk controllers <b>555</b> are communicatively coupled to system interconnect <b>515</b>. Peripherals <b>550</b> can include, for example, circuitry to perform power management, flash management, interconnect management, USB, and other PHY type tasks.</p><p id="p-0035" num="0034">Applications processor <b>500</b> further includes a system memory <b>570</b>, which is interconnected to the foregoing by system interconnect <b>515</b> via a memory controller <b>560</b>. System memory <b>570</b> further comprises an operating system <b>572</b> and in various embodiments also comprises anomaly detection system <b>575</b>. Anomaly detection system <b>575</b> performs the tasks described above with regard to accessing application data (e.g., performance data associated with the applications processor) and analyzing the application data for anomalous behavior. The anomaly detection system can access accelerator <b>540</b> if such an accelerator is present and configured for acceleration of machine learning functions associated with anomaly detection. Anomaly detection system <b>575</b> includes the instructions necessary to configure the applications processor, and all implicated portions thereof, to perform the processes discussed herein.</p><p id="p-0036" num="0035">Embodiments of the present invention can detect anomalies in data associated with normally well-defined behavior. Machine-learning models are trained against a data set containing data associated with the normal behavior. Each machine-learning model is trained against data gathered from the data set at different granularities, which subsequently allows for detection of anomalous behavior in a new set of data (e.g., data gathered during execution of a system) at different granularities associated with the data (e.g., time, number of transactions, number of pixels). In so doing, the anomaly detection system can respond to anomalies quickly for anomalies detectable at small granularities, or analyze behavior over a longer period of time for anomalies detectable at larger granularities. Embodiments also allow for selection of anomaly detection models that impact resource consumption of a system in an appropriate manner for the types of anomalies anticipated and available computational resources.</p><p id="p-0037" num="0036">By now it should be appreciated that there has been provided a method for detecting anomalies in an operational data set from an operational environment with respect to well-defined normal behavior. The method includes providing a training data set with the training data set includes data points associated with the normal behavior, forming a plurality of aggregated data sets, training a plurality of machine learning models where each machine learning model of the plurality of machine learning models is trained using an associated aggregated data set, generating a plurality of operational data set data points, analyzing the plurality of operational data set data points using the plurality of machine learning models. Each aggregated data set includes information generated from the entire training data set and includes entries generated from an associated aggregate of data points from the training data set. Each associated aggregate of data points includes a unique granularity. For each of the plurality of machine learning models, the plurality of operational data set data points are aggregated at the same granularity as that of the associated aggregated data set used to train the machine learning model.</p><p id="p-0038" num="0037">In one aspect of the above embodiment, the analyzing includes determining whether the operational data set data points exhibit anomalous behavior of the environment generating the operational data set from the normal behavior. In a further aspect, the determining includes examining results of said analyzing by each machine learning model for anomalous behavior at the associated granularity of that machine learning model, and determining whether the results from any one of the machine learning models exhibit anomalous behavior.</p><p id="p-0039" num="0038">In another aspect of the above embodiment, each of the machine learning models includes a same machine learning algorithm for detecting anomalous behavior. In another aspect, each of the machine learning models includes a unique machine learning algorithm for detecting anomalous behavior. In a further aspect, each of the machine learning models includes a machine learning algorithm for detecting anomalous behavior at the granularity of the associated aggregated data set.</p><p id="p-0040" num="0039">In another aspect of the above embodiment, a first machine learning model of the plurality machine learning models is trained using an aggregated data set including single data points from the training data set. In another aspect of the above embodiment, an environment generating the operational data set includes one of a processor performance monitor, a transaction environment, imaging data, and three-dimensional data.</p><p id="p-0041" num="0040">Another embodiment of the present invention provides a system for detecting anomalies in an operational data set generated by an environment with respect to well-defined normal behavior. The system includes: a processor; a first memory coupled to the processor and storing a training data set including data points associated with the normal behavior; a second memory, coupled to the processor, and storing instructions executable by the processor. The instructions are configured to form a plurality of aggregated data sets, train a plurality of machine learning models, generate a plurality of operational data set points by the environment, and analyze the plurality of operational data set data points using the plurality of machine learning models. Each aggregated data set includes information generated from the entire training data set and each aggregated data set includes entries generated from an associated aggregate of data points from the training data set. Each associated aggregated data points includes a unique granularity. Each machine learning model of the plurality machine learning models is trained using an associated aggregated data set. For each of the plurality of machine learning models, the plurality of operational data set data points are aggregated at the same granularity as that of the associated aggregated data set used to train the machine learning model.</p><p id="p-0042" num="0041">In one aspect of the above embodiment, the instructions configured to analyze include further instructions configured to determine whether the operational data set data points exhibit anomalous behavior of the environment from the normal behavior. In a further aspect, the instructions configured to determine include further instructions configured to examine results of the analyzing by each machine learning model for anomalous behavior at the associated granularity of that machine learning model, and determine whether the results from any one of the machine learning models exhibits anomalous behavior.</p><p id="p-0043" num="0042">In another aspect of the above embodiment, each machine learning model includes a same machine learning algorithm for detecting anomalous behavior. In another aspect of the above embodiment, each machine learning model includes a unique machine learning algorithm for detecting anomalous behavior. In a further aspect, each machine learning model includes a machine learning algorithm for detecting anomalous behavior at the granularity of the associated aggregated data set.</p><p id="p-0044" num="0043">In another aspect of the above embodiment, a first machine learning model of the plurality machine learning models is trained using an aggregated data set including single data points from the training data set. In yet another aspect, the environment generating the operational data set includes one of a processor performance monitor, a transaction environment, imaging data, and three-dimensional data.</p><p id="p-0045" num="0044">Another embodiment of the present invention provides a system that includes: a processor; a performance monitoring unit configured to periodically track a performance statistic associated with the processor; and, a memory coupled to the processor and storing instructions executable by the processor. The instructions are configured to analyze the performance statistic over time using a plurality of machine learning models. Each machine learning model of the plurality machine learning models is trained using an associated aggregated data set. Each aggregated data set includes information generated from entire training data set. Each aggregated data set includes entries generated from associated aggregate of data points from the training data set. Each associated aggregate of data points includes a unique granularity. For each of the plurality of machine learning models, the performance statistic is aggregated at the same granularity as that of the associated data set used to train the machine learning model. The analyzing includes determining whether the performance statistic exhibits anomalous behavior from the training data set.</p><p id="p-0046" num="0045">In one aspect of the above embodiment, the instructions for the determining include further instructions configured to examine results of the analyzing by each machine learning model for anomalous behavior at the associated granularity of that machine learning model, and determine whether the results from any one of the machine learning models exhibits anomalous behavior. In another aspect, each of the machine learning models includes a same machine learning algorithm for detecting anomalous behavior. In yet another aspect, each of the machine learning models includes a unique machine learning algorithm for detecting anomalous behavior.</p><p id="p-0047" num="0046">Because the apparatus implementing the present invention is, for the most part, composed of electronic components and circuits known to those skilled in the art, circuit details will not be explained in any greater extent than that considered necessary as illustrated above, for the understanding and appreciation of the underlying concepts of the present invention and in order not to obfuscate or distract from the teachings of the present invention.</p><p id="p-0048" num="0047">Although the invention has been described with respect to specific conductivity types or polarity of potentials, skilled artisans appreciated that conductivity types and polarities of potentials may be reversed.</p><p id="p-0049" num="0048">Moreover, the terms &#x201c;front,&#x201d; &#x201c;back,&#x201d; &#x201c;top,&#x201d; &#x201c;bottom,&#x201d; &#x201c;over,&#x201d; &#x201c;under&#x201d; and the like in the description and in the claims, if any, are used for descriptive purposes and not necessarily for describing permanent relative positions. It is understood that the terms so used are interchangeable under appropriate circumstances such that the embodiments of the invention described herein are, for example, capable of operation in other orientations than those illustrated or otherwise described herein.</p><p id="p-0050" num="0049">The term &#x201c;program,&#x201d; as used herein, is defined as a sequence of instructions designed for execution on a computer system. A program, or computer program, may include a subroutine, a function, a procedure, an object method, an object implementation, an executable application, an applet, a servlet, a source code, an object code, a shared library/dynamic load library and/or other sequence of instructions designed for execution on a computer system.</p><p id="p-0051" num="0050">Some of the above embodiments, as applicable, may be implemented using a variety of different information processing systems. For example, although <figref idref="DRAWINGS">FIG. <b>1</b></figref> and the discussion thereof describe an exemplary information processing architecture, this exemplary architecture is presented merely to provide a useful reference in discussing various aspects of the invention. Of course, the description of the architecture has been simplified for purposes of discussion, and it is just one of many different types of appropriate architectures that may be used in accordance with the invention. Those skilled in the art will recognize that the boundaries between logic blocks are merely illustrative and that alternative embodiments may merge logic blocks or circuit elements or impose an alternate decomposition of functionality upon various logic blocks or circuit elements.</p><p id="p-0052" num="0051">Thus, it is to be understood that the architectures depicted herein are merely exemplary, and that in fact many other architectures can be implemented which achieve the same functionality. In an abstract, but still definite sense, any arrangement of components to achieve the same functionality is effectively &#x201c;associated&#x201d; such that the desired functionality is achieved. Hence, any two components herein combined to achieve a particular functionality can be seen as &#x201c;associated with&#x201d; each other such that the desired functionality is achieved, irrespective of architectures or intermedial components. Likewise, any two components so associated can also be viewed as being &#x201c;operably connected,&#x201d; or &#x201c;operably coupled,&#x201d; to each other to achieve the desired functionality.</p><p id="p-0053" num="0052">Also for example, in one embodiment, the illustrated elements of system <b>500</b> are circuitry located on a single integrated circuit or within a same device. Alternatively, system <b>500</b> may include any number of separate integrated circuits or separate devices interconnected with each other. For example, memory <b>570</b> may be located on a same integrated circuit as processor cores <b>510</b>(<b>1</b>)-(N) or on a separate integrated circuit or located within another peripheral or slave discretely separate from other elements of system <b>500</b>. Peripherals <b>550</b> and I/O circuitry <b>520</b> may also be located on separate integrated circuits or devices. Also for example, system <b>500</b> or portions thereof may be soft or code representations of physical circuitry or of logical representations convertible into physical circuitry. As such, system <b>500</b> may be embodied in a hardware description language of any appropriate type.</p><p id="p-0054" num="0053">Furthermore, those skilled in the art will recognize that boundaries between the functionality of the above described operations merely illustrative. The functionality of multiple operations may be combined into a single operation, and/or the functionality of a single operation may be distributed in additional operations. Moreover, alternative embodiments may include multiple instances of a particular operation, and the order of operations may be altered in various other embodiments.</p><p id="p-0055" num="0054">All or some of the software described herein may be received elements of system <b>500</b>, for example, from computer readable media such as memory <b>570</b> or other media on other computer systems. Such computer readable media may be permanently, removably or remotely coupled to an information processing system such as system <b>500</b>. The computer readable media may include, for example and without limitation, any number of the following: magnetic storage media including disk and tape storage media; optical storage media such as compact disk media (e.g., CD-ROM, CD-R, etc.) and digital video disk storage media; nonvolatile memory storage media including semiconductor-based memory units such as FLASH memory, EEPROM, EPROM, ROM; ferromagnetic digital memories; M RAM; volatile storage media including registers, buffers or caches, main memory, RAM, and the like; and data transmission media including computer networks, point-to-point telecommunication equipment, and carrier wave transmission media, just to name a few.</p><p id="p-0056" num="0055">In one embodiment, system <b>500</b> is a computer system such as a personal computer system. Other embodiments may include different types of computer systems. Computer systems are information handling systems which can be designed to give independent computing power to one or more users. Computer systems may be found in many forms including but not limited to mainframes, minicomputers, servers, workstations, personal computers, notepads, personal digital assistants, electronic games, automotive and other embedded systems, cell phones and various other wireless devices. A typical computer system includes at least one processing unit, associated memory and a number of input/output (I/O) devices.</p><p id="p-0057" num="0056">A computer system processes information according to a program and produces resultant output information via I/O devices. A program is a list of instructions such as a particular application program and/or an operating system. A computer program is typically stored internally on computer readable storage medium or transmitted to the computer system via a computer readable transmission medium. A computer process typically includes an executing (running) program or portion of a program, current program values and state information, and the resources used by the operating system to manage the execution of the process. A parent process may spawn other, child processes to help perform the overall functionality of the parent process. Because the parent process specifically spawns the child processes to perform a portion of the overall functionality of the parent process, the functions performed by child processes (and grandchild processes, etc.) may sometimes be described as being performed by the parent process.</p><p id="p-0058" num="0057">Although the invention is described herein with reference to specific embodiments, various modifications and changes can be made without departing from the scope of the present invention as set forth in the claims below. For example, the number of machine-learning models and associated granularities used and the nature of the application generating the well-defined normal behavior data. Accordingly, the specification and figures are to be regarded in an illustrative rather than a restrictive sense, and all such modifications are intended to be included within the scope of the present invention. Any benefits, advantages, or solutions to problems that are described herein with regard to specific embodiments are not intended to be construed as a critical, required, or essential feature or element of any or all the claims.</p><p id="p-0059" num="0058">The term &#x201c;coupled,&#x201d; as used herein, is not intended to be limited to a direct coupling or a mechanical coupling.</p><p id="p-0060" num="0059">Furthermore, the terms &#x201c;a&#x201d; or &#x201c;an,&#x201d; as used herein, are defined as one or more than one. Also, the use of introductory phrases such as &#x201c;at least one&#x201d; and &#x201c;one or more&#x201d; in the claims should not be construed to imply that the introduction of another claim element by the indefinite articles &#x201c;a&#x201d; or &#x201c;an&#x201d; limits any particular claim containing such introduced claim element to inventions containing only one such element, even when the same claim includes the introductory phrases &#x201c;one or more&#x201d; or &#x201c;at least one&#x201d; and indefinite articles such as &#x201c;a&#x201d; or &#x201c;an.&#x201d; The same holds true for the use of definite articles.</p><p id="p-0061" num="0060">Unless stated otherwise, terms such as &#x201c;first&#x201d; and &#x201c;second&#x201d; are used to arbitrarily distinguish between the elements such terms describe. Thus, these terms are not necessarily intended to indicate temporal or other prioritization of such elements.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for detecting anomalies in an operational data set with respect to well-defined normal behavior of an application, the method comprising:<claim-text>providing a training data set, wherein the training data set comprises data points associated with the normal behavior;</claim-text><claim-text>forming a plurality of aggregated data sets, wherein<claim-text>each aggregated data set comprises information generated from the entire training data set,</claim-text><claim-text>each aggregated data set comprises entries generated from an associated aggregate of data points from the training data set,</claim-text><claim-text>each associated aggregate of data points comprises a unique granularity;</claim-text></claim-text><claim-text>training a plurality of machine learning models, wherein each machine learning model of the plurality of machine learning models is trained using an associated aggregated data set of the plurality of aggregated data sets;</claim-text><claim-text>analyzing a plurality of operational data set data points, generated by the application, using the plurality of machine learning models, wherein<claim-text>for each of the plurality of machine learning models, the plurality of operational data set data points are aggregated at the same granularity as that of the associated aggregated data set used to train the machine learning model.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said analyzing comprises determining whether the operational data set data points exhibit anomalous behavior of the environment generating the operational data set from the normal behavior.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref> wherein said determining comprises:<claim-text>examining results of said analyzing by each machine learning model for anomalous behavior at the associated granularity of that machine learning model; and</claim-text><claim-text>determining whether the results from any one of the machine learning models exhibits anomalous behavior.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein each of the machine learning models comprises a same machine learning algorithm for detecting anomalous behavior.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein each of the machine learning models comprises a unique machine learning algorithm for detecting anomalous behavior.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein each of the machine learning models comprises a machine learning algorithm for detecting anomalous behavior at the granularity of the associated aggregated data set.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein a first machine learning model of the plurality of machine learning models is trained using an aggregated data set comprising single data points from the training data set.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein an environment generating the operational data set comprises one of a processor performance monitor, a transaction environment, imaging data, and three-dimensional data.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A system for detecting anomalies in an operational data set generated by an environment with respect to well-defined normal behavior, the system comprising:<claim-text>a processor;</claim-text><claim-text>a first memory, coupled to the processor, and storing a training data set comprising data points associated with the normal behavior;</claim-text><claim-text>a second memory, coupled to the processor, and storing instructions executable by the processor, the instructions configured to<claim-text>form a plurality of aggregated data sets, wherein<claim-text>each aggregated data set comprises information generated from the entire training data set,</claim-text><claim-text>each aggregated data set comprises entries generated from an associated aggregate of data points from the training data set,</claim-text><claim-text>each associated aggregate of data points comprises a unique granularity;</claim-text></claim-text><claim-text>train a plurality of machine learning models, wherein each machine learning model of the plurality of machine learning models is trained using an associated aggregated data set;</claim-text><claim-text>analyze a plurality of operational data set data points, generated by the environment, using the plurality of machine learning models, wherein<claim-text>for each of the plurality of machine learning models, the plurality of operational data set data points are aggregated at the same granularity as that of the associated aggregated data set used to train the machine learning model.</claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein the instructions configured to analyze comprise further instructions configured to determine whether the operational data set data points exhibit anomalous behavior of the environment from the normal behavior.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref> wherein the instructions configured to determine comprise further instructions configured to<claim-text>examine results of said analyzing by each machine learning model for anomalous behavior at the associated granularity of that machine learning model; and</claim-text><claim-text>determine whether the results from any one of the machine learning models exhibits anomalous behavior.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein each machine learning model comprises a same machine learning algorithm for detecting anomalous behavior.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein each machine learning model comprises a unique machine learning algorithm for detecting anomalous behavior.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref> wherein each machine learning model comprises a machine learning algorithm for detecting anomalous behavior at the granularity of the associated aggregated data set.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein a first machine learning model of the plurality of machine learning models is trained using an aggregated data set comprising single data points from the training data set.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein the environment generating the operational data set comprises one of a processor performance monitor, a transaction environment, imaging data, and three-dimensional data.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A system comprising:<claim-text>a processor;</claim-text><claim-text>a performance monitoring unit configured to periodically track a performance statistic associated with the processor;</claim-text><claim-text>a memory, coupled to the processor, and storing instructions executable by the processor, the instructions configured to<claim-text>analyze the performance statistic over time using a plurality of machine learning models, wherein</claim-text><claim-text>each machine learning model of the plurality of machine learning models is trained using an associated aggregated data set,</claim-text><claim-text>each aggregated data set comprises information generated from an entire training data set,</claim-text><claim-text>each aggregated data set comprises entries generated from an associated aggregate of data points from the training data set,</claim-text><claim-text>each associated aggregate of data points comprises a unique granularity,</claim-text><claim-text>for each of the plurality of machine learning models, the performance statistic is aggregated at the same granularity as that of the associated data set used to train the machine learning model, and</claim-text><claim-text>said analyzing comprises determining whether the performance statistic exhibits anomalous behavior from the training data set.</claim-text></claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref> wherein the instructions for said determining comprise further instructions configured to:<claim-text>examine results of said analyzing by each machine learning model for anomalous behavior at the associated granularity of that machine learning model, and</claim-text><claim-text>determine whether the results from any one of the machine learning models exhibits anomalous behavior.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref> wherein each of the machine learning models comprises a same machine learning algorithm for detecting anomalous behavior.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref> wherein each of the machine learning models comprises a unique machine learning algorithm for detecting anomalous behavior.</claim-text></claim></claims></us-patent-application>