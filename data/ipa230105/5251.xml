<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005252A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005252</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17781081</doc-number><date>20201216</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>776</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>764</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>279</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>16</class><subclass>H</subclass><main-group>15</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>16</class><subclass>H</subclass><main-group>30</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>7753</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>776</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>7747</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>764</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>279</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180101</date></cpc-version-indicator><section>G</section><class>16</class><subclass>H</subclass><main-group>15</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180101</date></cpc-version-indicator><section>G</section><class>16</class><subclass>H</subclass><main-group>30</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>2201</main-group><subgroup>03</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">A CO-TRAINING FRAMEWORK TO MUTUALLY IMPROVE CONCEPT EXTRACTION FROM CLINICAL NOTES AND MEDICAL IMAGE CLASSIFICATION</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>62949836</doc-number><date>20191218</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>KONINKLIJKE PHILIPS N.V.</orgname><address><city>EINDHOVEN</city><country>NL</country></address></addressbook><residence><country>NL</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>QADIR</last-name><first-name>ASHEQUL</first-name><address><city>MELROSE</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>LEE</last-name><first-name>KATHY MI YOUNG</first-name><address><city>WESTFORD</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>ZHAO</last-name><first-name>CLAIRE YUNZHU</first-name><address><city>BOSTON</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>XU</last-name><first-name>MINNAN</first-name><address><city>CAMBRIDGE</city><state>MA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/EP2020/086320</doc-number><date>20201216</date></document-id><us-371c12-date><date>20220531</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A system and method for training a text report identification machine learning model and an image identification machine learning model, including: initially training a text report machine learning model, using a labeled set of text reports including text pre-processing the text report and extracting features from the pre-processed text report, wherein the extracted features are input into the text report machine learning model; initially training an image machine learning model, using a labeled set of images; applying the initially trained text report machine learning model to a first set of unlabeled text reports with associated images to label the associated images; selecting a first portion of labeled associated images; re-training the image machine learning model using the selected first portion of labeled associated images; applying the initially trained image machine learning model to a first set of unlabeled images with associated text reports to label the associated text reports; selecting a first portion of labeled associated text reports; and re-training the text report machine learning model using the selected first portion of labeled associated text reports.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="59.18mm" wi="158.75mm" file="US20230005252A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="192.45mm" wi="124.29mm" orientation="landscape" file="US20230005252A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="204.72mm" wi="136.91mm" orientation="landscape" file="US20230005252A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="202.01mm" wi="93.64mm" orientation="landscape" file="US20230005252A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="199.56mm" wi="109.22mm" orientation="landscape" file="US20230005252A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="204.30mm" wi="102.79mm" orientation="landscape" file="US20230005252A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="185.76mm" wi="116.92mm" orientation="landscape" file="US20230005252A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">Various exemplary embodiments disclosed herein relate generally to a co-training framework to mutually improve concept extraction from clinical notes and medical image classification.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Various types of clinical tests (e.g., X-ray, MRI, ultrasound) generate both image and text data. These tests are often imaging tests, and the results are documented in reports and/or notes written by skilled professionals (e.g., radiologists) where they describe various observations and findings from the tests. The notes/reports contain different medical concepts (e.g., disease, symptom, anatomy, severity) that need to be identified and extracted so that they can be used for other downstream applications such as AI-enabled clinical diagnosis support, patient status visualization on information dashboards, etc. Traditional natural language processing-based information extraction systems primarily work on the text of the reports/notes, and do not take advantage of the accompanying images. Similarly, medical image classification algorithms primarily rely on image features and do not utilize the accompanying text when they may be available.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0004" num="0003">A summary of various exemplary embodiments is presented below. Some simplifications and omissions may be made in the following summary, which is intended to highlight and introduce some aspects of the various exemplary embodiments, but not to limit the scope of the invention. Detailed descriptions of an exemplary embodiment adequate to allow those of ordinary skill in the art to make and use the inventive concepts will follow in later sections.</p><p id="p-0005" num="0004">Various embodiments relate to a system for training a text report identification machine learning model and an image identification machine learning model, including: a memory; a processor connected to the memory, the processor configured to: initially train a text report machine learning model, using a labeled set of text reports including text pre-processing the text report and extracting features from the pre-processed text report, wherein the extracted features are input into the text report machine learning model; initially train an image machine learning model, using a labeled set of images; apply the initially trained text report machine learning model to a first set of unlabeled text reports with associated images to label the associated images; select a first portion of labeled associated images; re-train the image machine learning model using the selected first portion of labeled associated images; apply the initially trained image machine learning model to a first set of unlabeled images with associated text reports to label the associated text reports; select a first portion of labeled associated text reports; and re-train the text report machine learning model using the selected first portion of labeled associated text reports.</p><p id="p-0006" num="0005">Various embodiments are described, wherein selecting a portion of labeled associated images includes selecting associated images with text report machine learning model outputs having a confidence level above a first confidence threshold, and selecting a portion of labeled associated text reports includes selecting associated text reports with image machine learning model outputs having a confidence level above a second confidence threshold.</p><p id="p-0007" num="0006">Various embodiments are described, wherein selecting a portion of labeled associated images further includes selecting the N associated images with text report machine learning model outputs having the highest confidence levels, wherein N is a predetermined value, and selecting a portion of labeled associated text reports further includes selecting the M associated text reports with image machine learning model outputs having the highest confidence levels, wherein M is a predetermined value.</p><p id="p-0008" num="0007">Various embodiments are described, selecting a portion of labeled associated images includes selecting associated images with text report machine learning model outputs having a confidence level above a first confidence threshold, when there are more than N selected associated images, further selecting N associated images with text reports that have the highest confidence levels, selecting a portion of labeled associated text reports includes selecting associated text reports with image machine learning model outputs having a confidence level above a second confidence threshold, and when there are more than M selected associated text reports, further selecting M associated text reports with image that have the highest confidence levels.</p><p id="p-0009" num="0008">Various embodiments are described, wherein the outputs of the text report machine learning model and image machine learning model indicated the same set of classification concepts.</p><p id="p-0010" num="0009">Various embodiments are described, wherein the image machine learning model is re-trained until the initially trained text report machine learning model is applied to all the unlabeled text reports with associated images, and the text report machine learning model is re-trained until the initially trained image machine learning model is applied to all the unlabeled images with associated text reports.</p><p id="p-0011" num="0010">Various embodiments are described, wherein the image machine learning model is re-trained until the image machine learning model performance on validation set of input image data does not improve, and the text report machine learning model is re-trained until the text report machine learning model performance on a validation set of input text report data does not improve.</p><p id="p-0012" num="0011">Various embodiments are described, wherein the processor is further configured to: apply the retrained text report machine learning model to a second set of unlabeled text reports with associated images to label the associated images; select a second portion of labeled associated images; re-train the retrained image machine learning model using the selected second portion of labeled associated images; apply the retrained image machine learning model to a second set of unlabeled images with associated text reports to label the associated text reports; select a second portion of labeled associated text reports; and re-train the retrained text report machine learning model using the selected second portion of labeled associated text reports.</p><p id="p-0013" num="0012">Further various embodiments relate to a method for training a text report identification machine learning model and an image identification machine learning model, including: initially training a text report machine learning model, using a labeled set of text reports including text pre-processing the text report and extracting features from the pre-processed text report, wherein the extracted features are input into the text report machine learning model; initially training an image machine learning model, using a labeled set of images; applying the initially trained text report machine learning model to a first set of unlabeled text reports with associated images to label the associated images; selecting a first portion of labeled associated images; re-training the image machine learning model using the selected first portion of labeled associated images; applying the initially trained image machine learning model to a first set of unlabeled images with associated text reports to label the associated text reports; selecting a first portion of labeled associated text reports; and re-training the text report machine learning model using the selected first portion of labeled associated text reports.</p><p id="p-0014" num="0013">Various embodiments are described, wherein selecting a portion of labeled associated images includes selecting associated images with text report machine learning model outputs having a confidence level above a first confidence threshold, and selecting a portion of labeled associated text reports includes selecting associated text reports with image machine learning model outputs having a confidence level above a second confidence threshold.</p><p id="p-0015" num="0014">Various embodiments are described, wherein selecting a portion of labeled associated images further includes selecting the N associated images with text report machine learning model outputs having the highest confidence levels, and selecting a portion of labeled associated text reports further includes selecting the M associated text reports with image machine learning model outputs having the highest confidence levels.</p><p id="p-0016" num="0015">Various embodiments are described, wherein selecting a portion of labeled associated images includes selecting associated images with text report machine learning model outputs having a confidence level above a first confidence threshold, when there are more than N selected associated images, further selecting N associated images with text reports that have the highest confidence levels, selecting a portion of labeled associated text reports includes selecting associated text reports with image machine learning model outputs having a confidence level above a second confidence threshold, and when there are more than M selected associated text reports, further selecting M associated text reports with image that have the highest confidence levels.</p><p id="p-0017" num="0016">Various embodiments are described, wherein the outputs of the text report machine learning model and image machine learning model indicated the same set of classification concepts.</p><p id="p-0018" num="0017">Various embodiments are described, wherein the image machine learning model is re-trained until the initially trained text report machine learning model is applied to all the unlabeled text reports with associated images, and the text report machine learning model is re-trained until the initially trained image machine learning model is applied to all the unlabeled images with associated text reports.</p><p id="p-0019" num="0018">Various embodiments are described, wherein the image machine learning model is re-trained until the image machine learning model performance on validation set of input image data does not improve, and the text report machine learning model is re-trained until the text report machine learning model performance on a validation set of input text report data does not improve.</p><p id="p-0020" num="0019">Various embodiments are described, further including: applying the retrained text report machine learning model to a second set of unlabeled text reports with associated images to label the associated images; selecting a second portion of labeled associated images; re-training the retrained image machine learning model using the selected second portion of labeled associated images; applying the retrained image machine learning model to a second set of unlabeled images with associated text reports to label the associated text reports; selecting a second portion of labeled associated text reports; and re-training the retrained text report machine learning model using the selected second portion of labeled associated text reports.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0021" num="0020">In order to better understand various exemplary embodiments, reference is made to the accompanying drawings, wherein:</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a co-training system that leverages two views of the data&#x2014;a text view and an image view;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a flow diagram illustrating the training of the text identification model;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates the use of the trained text identification model on a set of input unlabeled text reports to produce a set of labels based upon a set of extracted concepts;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a flow diagram illustrating the image identification model; and</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an exemplary hardware diagram for implementing co-training system.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0027" num="0026">To facilitate understanding, identical reference numerals have been used to designate elements having substantially the same or similar structure and/or substantially the same or similar function.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0028" num="0027">The description and drawings illustrate the principles of the invention. It will thus be appreciated that those skilled in the art will be able to devise various arrangements that, although not explicitly described or shown herein, embody the principles of the invention and are included within its scope. Furthermore, all examples recited herein are principally intended expressly to be for pedagogical purposes to aid the reader in understanding the principles of the invention and the concepts contributed by the inventor(s) to furthering the art and are to be construed as being without limitation to such specifically recited examples and conditions. Additionally, the term, &#x201c;or,&#x201d; as used herein, refers to a non-exclusive or (i.e., and/or), unless otherwise indicated (e.g., &#x201c;or else&#x201d; or &#x201c;or in the alternative&#x201d;). Also, the various embodiments described herein are not necessarily mutually exclusive, as some embodiments can be combined with one or more other embodiments to form new embodiments.</p><p id="p-0029" num="0028">Embodiments of a co-training system will be described herein that implement a co-training framework where an image-based classifier and a text-based classifier mutually generate supplemental training instances for each other in an iterative semi-supervised learning paradigm to gradually improve their individual performances.</p><p id="p-0030" num="0029">Iterative semi-supervised learning increases an initial small collection of labeled training data with supplemental training data that are automatically labeled from a large collection of unlabeled data. However, a text-based learning algorithm that completely relies on text data stays limited to what it can already learn from the text data, and the additionally iterative incremental training data may suffer from monotonicity and lack feature diversity which are important for continued learning. Similar limitations will be faced by an image-based classification algorithm as it will only rely on image features, but could benefit from additional text data which may offer complementary or redundant information. The embodiment of a co-training framework will address this problem by individually training text-based and image-based classifiers with text-based and image-based data that each identify new instances from unlabeled data to generate supplemental training instances for the other, thus allowing each classifier to improve over time as the labeled training data expands in each iteration.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a co-training system that leverages two views of the data&#x2014;(1) a text view, and (2) an image view. First, a text-based concept identification model <b>104</b> is trained using labeled reports <b>102</b> associated with images. The text identification model <b>104</b> may be a text classifier that uses various textual data features and a machine learning algorithm suitable for sequence labeling. Example machine learning algorithms may include conditional random field (CRF) classifier, bidirectional long short-term memory (BiLSTM) networks, BiLSTM-CRF, etc. A small collection of initial labeled training data <b>102</b> is used to train the text identification model <b>104</b>.</p><p id="p-0032" num="0031">Unlabeled text reports <b>106</b> are input into the text identification model <b>104</b> to generate labeled reports <b>108</b>. The unlabeled images <b>110</b> associated with the unlabeled reports <b>108</b> are then labeled using the labels from the labeled reports <b>108</b> produced by the text identification model <b>104</b>. The text identification model <b>104</b> may produce a confidence value associated with its labeled outputs. These labeled images <b>112</b> may then be used as further training samples for an image identification model <b>124</b>. Not all of the labeled images <b>112</b> may be used as further training samples. In one embodiment, only labeled images <b>112</b> associated with labeled reports <b>108</b> that have a confidence level above a specified threshold value may be used to further train the image identification model <b>124</b>. In another embodiment, the number of further training samples may be limited to a threshold number N of training samples, by selecting the N samples with the highest confidence levels. In yet another embodiment if there are more than N samples that exceed the threshold value, only the N values with the highest confidence values may be selected as the training samples. Other methods of limiting the further training samples may be used to ensure that the further training samples are of high enough quality to improve the training of the image identification model <b>124</b>. These selected training samples may then be sent <b>114</b> to further train the image identification model <b>124</b>.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a flow diagram illustrating the training <b>200</b> of the text identification model <b>104</b>. The labeled reports <b>202</b> first undergo text preprocessing <b>204</b>. The text pre-processing <b>204</b> may include tokenization, lemmatization, case normalization, stopwords removal processing, etc. This text pre-processing <b>204</b> takes in the raw text of the labeled reports and processes them into a consistent format in order to facilitate feature extraction. Next, the preprocessed text undergoes feature extraction <b>206</b>. Feature extraction may include looking at each current word in the context of prior words and next words. Also, context words may be identified. Various types of features may be extracted including morphological, orthographic, lexical, and syntactic features. The feature extraction is used to provide inputs into the text identification model <b>212</b>. Such features need to be defined in a consistent matter so that model may be trained to generate a consistent known set of extracted concepts out of the text identification model <b>212</b>.</p><p id="p-0034" num="0033">The table below gives examples of feature names, followed by example text, and the resulting feature value. For example, the first four entries use the example text of Cardiomegaly with the following feature names: Word/Phrase, Lowercase, 1 Character suffix, and 2 Character suffix. The associated values are: Cardiomegaly, cardiomegaly, y, and ly. Many other text feature examples are further demonstrated.</p><p id="p-0035" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="70pt" align="left"/><colspec colname="2" colwidth="70pt" align="left"/><colspec colname="3" colwidth="77pt" align="left"/><thead><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row><row><entry>Feature Name</entry><entry>Example Text</entry><entry>Feature Value</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>Word/Phrase</entry><entry>Cardiomegaly</entry><entry>Cardiomegaly</entry></row><row><entry>Lowercase</entry><entry>Cardiomegaly</entry><entry>Cardiomegaly</entry></row><row><entry>1 Character suffix</entry><entry>Cardiomegaly</entry><entry>Y</entry></row><row><entry>2 Character suffix</entry><entry>Cardiomegaly</entry><entry>ly</entry></row><row><entry>If text is</entry><entry>SVC</entry><entry>True</entry></row><row><entry>uppercase</entry></row><row><entry>If text is title</entry><entry>Superior Vena Cava</entry><entry>True</entry></row><row><entry>If text is a digit</entry><entry>4</entry><entry>True</entry></row><row><entry>Lemma form</entry><entry>Increased, increasing,</entry><entry>increase</entry></row><row><entry/><entry>increases</entry></row><row><entry>Coarse-grained</entry><entry>Increased</entry><entry>VERB</entry></row><row><entry>position of speech</entry></row><row><entry>(POS)</entry></row><row><entry>Fine-grained POS</entry><entry>Increased</entry><entry>VBN (Verb, past</entry></row><row><entry/><entry/><entry>participle)</entry></row><row><entry>Syntactic relation</entry><entry>shift of the trachea</entry><entry>ROOT prep det pobj</entry></row><row><entry>Syntactic parent</entry><entry>shift of the trachea</entry><entry>shift shift trachea of</entry></row><row><entry>Orthographic</entry><entry>77F, 2.2 cm</entry><entry>ddX, d.d xx</entry></row><row><entry>shape</entry></row><row><entry>If text is</entry><entry>Cardiomegaly, 77F</entry><entry>True, False</entry></row><row><entry>alphabetic</entry></row><row><entry>If text is a</entry><entry>shift of the trachea</entry><entry>False True True False</entry></row><row><entry>stop word</entry></row><row><entry>Left edge</entry><entry>shift of the trachea</entry><entry>shift of the the</entry></row><row><entry>Right edge</entry><entry>shift of the trachea</entry><entry>trachea trachea</entry></row><row><entry/><entry/><entry>the trachea</entry></row><row><entry>If text is a</entry><entry>?</entry><entry>True</entry></row><row><entry>punctuation</entry></row><row><entry>If text starts</entry><entry>Sternal wires are</entry><entry>True False False</entry></row><row><entry>sentence</entry><entry>unremarkable.</entry><entry>False False</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0036" num="0034">Once the features have been extracted for each of the labeled reports <b>202</b>, these are used to train <b>210</b> the machine learning model to produce the text identification model <b>212</b>. This may correspond to the generating the text identification model <b>104</b> using labeled reports <b>104</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0037" num="0035"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates the use of the trained text identification model <b>212</b> on a set of input unlabeled text reports to produce a set of labels based upon a set of extracted concepts <b>304</b>. This may correspond to steps <b>106</b> and <b>108</b> from <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In this example, a large number of unlabeled text reports <b>302</b> may be input into the text identification model <b>212</b> to produce outputs of concepts identified in the unlabeled reports <b>302</b>. These are the concepts that may be used to label the associated unlabeled images <b>110</b> to produce labeled images <b>112</b>.</p><p id="p-0038" num="0036">Next, an image-based concept identification model <b>124</b> will be trained using labeled images <b>122</b> associated with text reports. The image identification model <b>124</b> may be an image classifier that uses various image features and a machine learning algorithm suitable for image processing. An example machine learning algorithm may include convolutional neural networks-based (CNN) class activation mapping model, but other image classifying models may be used. A small collection of initial labeled training data <b>122</b> is used to train the image identification model <b>124</b>.</p><p id="p-0039" num="0037">Unlabeled images <b>126</b> are input into the image identification model <b>124</b> to generate labeled images <b>128</b>. The unlabeled text reports <b>130</b> associated with the unlabeled images <b>126</b> are then labeled using the labels from the labeled images <b>128</b> produced by the image identification model <b>124</b>. The image identification model <b>124</b> may produce a confidence value associated with its labeled outputs. The documents associated with these labeled images <b>132</b> may then be used as further training samples for the text identification model <b>104</b>. Not all of the labeled reports <b>132</b> may be used as further training samples. In one embodiment, only labeled reports <b>132</b> associated with labeled images <b>128</b> that have a confidence level above a specified threshold value may be used to further train the text identification model <b>104</b>. In another embodiment, the number of further training samples may be limited to a threshold number M of training samples, by selecting the M samples with the highest confidence levels. In yet another embodiment if there are more than M samples that exceed the threshold value, only the M values with the highest confidence values may be selected as the training samples. Other methods of limiting the further training samples may be used to ensure that the further training samples are of high enough quality to improve the training of the report identification model <b>104</b>. These selected training samples may then be sent <b>134</b> to further train the text identification model <b>104</b>.</p><p id="p-0040" num="0038"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a flow diagram <b>400</b> illustrating the image identification model <b>424</b>. The unlabeled images <b>426</b> are input into the image identification model <b>424</b>. The image identification model <b>424</b> corresponds to the image identification model <b>124</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The image identification model <b>424</b> may include a CNN applied to the image to extract features from the unlabeled input images <b>426</b>. The CNN may employ various layers including convolution layers and pooling layers in various configurations along with the application of rectified linear units (ReLU). Other types of layers may also be used in various configurations. Then the extracted features are input into classification layers to identify various concepts associated with the image. The classification layers may be fully connected layers that flatten the data and then use a softmax layer to produce outputs <b>428</b> indicating the presence of the concepts found in the image. The concepts output from the image identification model <b>424</b> are consistent with the concepts output from the text identification model. The image identification model <b>424</b> may be trained with a small set of initial labeled images as previously described and this initial training is supplemented with selected outputs <b>114</b> from applying the text identification model <b>104</b> on unlabeled text reports <b>106</b>.</p><p id="p-0041" num="0039">The features classified by the two models need to be defined in a consistent matter so that models may be trained to generate a consistent known set of extracted concepts out of the two models.</p><p id="p-0042" num="0040">As a result, both trained models <b>104</b>, <b>124</b> will be individually applied to unlabeled data that has clinical reports and their accompanying images paired with each other. The text identification model <b>104</b> will be applied to the unstructured text reports to identify a predefined set of medical concepts in the notes in the associated text reports, which will then be used to label the associated unlabeled images. The image identification based model <b>124</b> will be applied to unlabeled images to identify a set of medical concepts in the images which will then be used to label the associated unlabeled text reports.</p><p id="p-0043" num="0041">The text reports with identified medical concepts and their corresponding images <b>114</b> will be used to supplement the training data <b>122</b> of the image identification model <b>124</b> to increase its training data for re-training the image identification model <b>124</b>. The images with identified medical concepts and their corresponding text reports <b>134</b> will be used to supplement the training data <b>102</b> of the text identification model to increase its training data for re-training the text identification model <b>104</b>.</p><p id="p-0044" num="0042">The above steps will be repeated until no new reports or images can be identified from the unlabeled data to supplement the labeled training data or concept extraction results starts to degrade as tested on a validation data set.</p><p id="p-0045" num="0043">In another embodiment, first portion of the unlabeled text reports and images may be used to cross train the machine learning models. Then a second portion of the unlabeled text reports and images may be used to cross train the machine learning models. This process is repeated until no new reports or images can be identified from the unlabeled data to supplement the labeled training data or concept extraction results starts to degrade as tested on a validation data set.</p><p id="p-0046" num="0044">The embodiments of the co-training system described herein have various benefits. The co-training system leverages text modality of data in text reports associated with images to improve an image-based classifier to classify test images for concepts and identify a target set of concepts in the images. The co-training system further leverages image modality of data in images associated with text reports to improve a text-based classifier for information extraction from clinical reports. The co-training system also utilizes information from unlabeled clinical test images and reports which is a limitation of supervised learning systems. The co-training system further expands the initial labeled training data in iterations for both image-based and text-based models by incorporating image and text data views in co-training where the data modalities are associated but not overlapping. Finally, the co-training system leverages multimodal data associated with clinical tests to improve computational models in each modality.</p><p id="p-0047" num="0045">While the co-training system is described herein using medical images, such as X-ray, MRI, ultrasound, etc., and their associated medical reports, other images with associated text descriptions may be included. Such examples could be images and associated text found in catalogs, instruction and installations manuals, books, product web sites, social media web sites, news web sites, etc. The co-training system described herein may be used to co-train a text identification and image identification models for use in classifying images and text reports in such situations. As described above, a small set of labeled information may be used to initially train the models, which training can then be supplemented using the co-training system to expand the training data from unlabeled data that includes both an image and associated text. This leads to be better and more robust text identification and image identification models when only a small labeled training set is available.</p><p id="p-0048" num="0046"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an exemplary hardware diagram <b>500</b> for implementing co-training system. As shown, the device <b>500</b> includes a processor <b>520</b>, memory <b>530</b>, user interface <b>540</b>, network interface <b>550</b>, and storage <b>560</b> interconnected via one or more system buses <b>510</b>. It will be understood that <figref idref="DRAWINGS">FIG. <b>5</b></figref> constitutes, in some respects, an abstraction and that the actual organization of the components of the device <b>500</b> may be more complex than illustrated.</p><p id="p-0049" num="0047">The processor <b>520</b> may be any hardware device capable of executing instructions stored in memory <b>530</b> or storage <b>560</b> or otherwise processing data. As such, the processor may include a microprocessor, a graphics processing unit (GPU), field programmable gate array (FPGA), application-specific integrated circuit (ASIC), any processor capable of parallel computing, or other similar devices.</p><p id="p-0050" num="0048">The memory <b>530</b> may include various memories such as, for example L1, L2, or L3 cache or system memory. As such, the memory <b>530</b> may include static random-access memory (SRAM), dynamic RAM (DRAM), flash memory, read only memory (ROM), or other similar memory devices.</p><p id="p-0051" num="0049">The user interface <b>540</b> may include one or more devices for enabling communication with a user and may present information such. For example, the user interface <b>540</b> may include a display, a touch interface, a mouse, and/or a keyboard for receiving user commands. In some embodiments, the user interface <b>540</b> may include a command line interface or graphical user interface that may be presented to a remote terminal via the network interface <b>550</b>.</p><p id="p-0052" num="0050">The network interface <b>550</b> may include one or more devices for enabling communication with other hardware devices. For example, the network interface <b>550</b> may include a network interface card (NIC) configured to communicate according to the Ethernet protocol or other communications protocols, including wireless protocols. Additionally, the network interface <b>550</b> may implement a TCP/IP stack for communication according to the TCP/IP protocols. Various alternative or additional hardware or configurations for the network interface <b>550</b> will be apparent.</p><p id="p-0053" num="0051">The storage <b>560</b> may include one or more machine-readable storage media such as read-only memory (ROM), random-access memory (RAM), magnetic disk storage media, optical storage media, flash-memory devices, or similar storage media. In various embodiments, the storage <b>560</b> may store instructions for execution by the processor <b>520</b> or data upon with the processor <b>520</b> may operate. For example, the storage <b>560</b> may store a base operating system <b>561</b> for controlling various basic operations of the hardware <b>500</b>. The storage <b>562</b> may store instructions for implementing the co-training system described above including training the machine learning models and running the machine learning models on data to classify.</p><p id="p-0054" num="0052">It will be apparent that various information described as stored in the storage <b>560</b> may be additionally or alternatively stored in the memory <b>530</b>. In this respect, the memory <b>530</b> may also be considered to constitute a &#x201c;storage device&#x201d; and the storage <b>560</b> may be considered a &#x201c;memory.&#x201d; Various other arrangements will be apparent. Further, the memory <b>530</b> and storage <b>560</b> may both be considered to be &#x201c;non-transitory machine-readable media.&#x201d; As used herein, the term &#x201c;non-transitory&#x201d; will be understood to exclude transitory signals but to include all forms of storage, including both volatile and non-volatile memories.</p><p id="p-0055" num="0053">While the host device <b>500</b> is shown as including one of each described component, the various components may be duplicated in various embodiments. For example, the processor <b>520</b> may include multiple microprocessors that are configured to independently execute the methods described herein or are configured to perform steps or subroutines of the methods described herein such that the multiple processors cooperate to achieve the functionality described herein. Such plurality of processors may be of the same or different types. Further, where the device <b>500</b> is implemented in a cloud computing system, the various hardware components may belong to separate physical systems. For example, the processor <b>520</b> may include a first processor in a first server and a second processor in a second server.</p><p id="p-0056" num="0054">The co-training system described herein provides many benefits as described above. The co-training system improves the classification of images and associated text, by using a co-training framework that allows for the training of the machine learning models using a small set of labeled training data supplemented by cross training using unlabeled data processed by the machine learning models. This system provides a technical improvement in image and text identification systems.</p><p id="p-0057" num="0055">Any combination of specific software running on a processor to implement the embodiments of the invention, constitute a specific dedicated machine.</p><p id="p-0058" num="0056">As used herein, the term &#x201c;non-transitory machine-readable storage medium&#x201d; will be understood to exclude a transitory propagation signal but to include all forms of volatile and non-volatile memory.</p><p id="p-0059" num="0057">Although the various exemplary embodiments have been described in detail with particular reference to certain exemplary aspects thereof, it should be understood that the invention is capable of other embodiments and its details are capable of modifications in various obvious respects. As is readily apparent to those skilled in the art, variations and modifications can be affected while remaining within the spirit and scope of the invention. Accordingly, the foregoing disclosure, description, and figures are for illustrative purposes only and do not in any way limit the invention, which is defined only by the claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system for training a text report identification machine learning model and an image identification machine learning model, comprising:<claim-text>a memory;</claim-text><claim-text>a processor connected to the memory, the processor configured to:<claim-text>initially train a text report machine learning model, using a labeled set of clinical text reports including text pre-processing the clinical text report and extracting features from the pre-processed clinical text report, wherein the extracted features are input into the text report machine learning model;</claim-text><claim-text>initially train an image machine learning model, using a labeled set of medical images;</claim-text><claim-text>apply the initially trained text report machine learning model to a first set of unlabeled clinical text reports with associated medical images to label the associated medical images;</claim-text><claim-text>select a first portion of labeled associated medical images;</claim-text><claim-text>re-train the image machine learning model using the selected first portion of labeled associated medical images;</claim-text><claim-text>apply the initially trained image machine learning model to a first set of unlabeled medical images with associated clinical text reports to label the associated clinical text reports;</claim-text><claim-text>select a first portion of labeled associated clinical text reports; and</claim-text><claim-text>re-train the text report machine learning model using the selected first portion of labeled associated clinical text reports,</claim-text><claim-text>wherein the labels relate to a predefined set of medical concepts.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>selecting a portion of labeled associated medical images includes selecting associated medical images with text report machine learning model outputs having a confidence level above a first confidence threshold, and</claim-text><claim-text>selecting a portion of labeled associated clinical text reports includes selecting associated clinical text reports with image machine learning model outputs having a confidence level above a second confidence threshold.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>selecting a portion of labeled associated medical images further includes selecting the N associated medical images with text report machine learning model outputs having the highest confidence levels, wherein N is a predetermined value, and</claim-text><claim-text>selecting a portion of labeled associated clinical text reports further includes selecting the M associated clinical text reports with image machine learning model outputs having the highest confidence levels, wherein M is a predetermined value.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>selecting a portion of labeled associated medical images includes selecting associated medical images with text report machine learning model outputs having a confidence level above a first confidence threshold,</claim-text><claim-text>when there are more than N selected associated medical images, further selecting N associated medical images with clinical text reports that have the highest confidence levels,</claim-text><claim-text>selecting a portion of labeled associated clinical text reports includes selecting associated clinical text reports with image machine learning model outputs having a confidence level above a second confidence threshold, and</claim-text><claim-text>when there are more than M selected associated clinical text reports, further selecting M associated clinical text reports with medical image that have the highest confidence levels.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the outputs of the text report machine learning model and image machine learning model indicated the same set of classification concepts.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the image machine learning model is re-trained until the initially trained text report machine learning model is applied to all the unlabeled clinical text reports with associated medical images, and</claim-text><claim-text>the text report machine learning model is re-trained until the initially trained image machine learning model is applied to all the unlabeled medical images with associated clinical text reports.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the image machine learning model is re-trained until the image machine learning model performance on validation set of input medical image data does not improve, and</claim-text><claim-text>the text report machine learning model is re-trained until the text report machine learning model performance on a validation set of input clinical text report data does not improve.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor is further configured to:<claim-text>apply the retrained text report machine learning model to a second set of unlabeled clinical text reports with associated medical images to label the associated medical images;</claim-text><claim-text>select a second portion of labeled associated medical images;</claim-text><claim-text>re-train the retrained image machine learning model using the selected second portion of labeled associated medical images;</claim-text><claim-text>apply the retrained image machine learning model to a second set of unlabeled medical images with associated clinical text reports to label the associated clinical text reports;</claim-text><claim-text>select a second portion of labeled associated clinical text reports; and</claim-text><claim-text>re-train the retrained text report machine learning model using the selected second portion of labeled associated clinical text reports.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A method for training a text report identification machine learning model and an image identification machine learning model, comprising:<claim-text>initially training a text report machine learning model, using a labeled set of clinical text reports including text pre-processing the clinical text report and extracting features from the pre-processed clinical text report, wherein the extracted features are input into the text report machine learning model;</claim-text><claim-text>initially training an image machine learning model, using a labeled set of medical images;</claim-text><claim-text>applying the initially trained text report machine learning model to a first set of unlabeled clinical text reports with associated medical images to label the associated medical images;</claim-text><claim-text>selecting a first portion of labeled associated medical images;</claim-text><claim-text>re-training the image machine learning model using the selected first portion of labeled associated medical images;</claim-text><claim-text>applying the initially trained image machine learning model to a first set of unlabeled medical images with associated clinical text reports to label the associated clinical text reports;</claim-text><claim-text>selecting a first portion of labeled associated clinical text reports; and</claim-text><claim-text>re-training the text report machine learning model using the selected first portion of labeled associated clinical text reports,</claim-text><claim-text>wherein the labels relate to a predefined set of medical concepts.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>selecting a portion of labeled associated medical images includes selecting associated medical images with text report machine learning model outputs having a confidence level above a first confidence threshold, and</claim-text><claim-text>selecting a portion of labeled associated clinical text reports includes selecting associated clinical text reports with image machine learning model outputs having a confidence level above a second confidence threshold.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>selecting a portion of labeled associated medical images further includes selecting the N associated medical images with text report machine learning model outputs having the highest confidence levels, and</claim-text><claim-text>selecting a portion of labeled associated clinical text reports further includes selecting the M associated clinical text reports with image machine learning model outputs having the highest confidence levels.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>selecting a portion of labeled associated medical images includes selecting associated medical images with text report machine learning model outputs having a confidence level above a first confidence threshold,</claim-text><claim-text>when there are more than N selected associated medical images, further selecting N associated medical images with clinical text reports that have the highest confidence levels,</claim-text><claim-text>selecting a portion of labeled associated clinical text reports includes selecting associated clinical text reports with image machine learning model outputs having a confidence level above a second confidence threshold, and</claim-text><claim-text>when there are more than M selected associated clinical text reports, further selecting M associated clinical text reports with medical image that have the highest confidence levels.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the outputs of the text report machine learning model and image machine learning model indicated the same set of classification concepts.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>the image machine learning model is re-trained until the initially trained text report machine learning model is applied to all the unlabeled clinical text reports with associated medical images, and</claim-text><claim-text>the text report machine learning model is re-trained until the initially trained image machine learning model is applied to all the unlabeled medical images with associated clinical text reports.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>the image machine learning model is re-trained until the image machine learning model performance on validation set of input medical image data does not improve, and</claim-text><claim-text>the text report machine learning model is re-trained until the text report machine learning model performance on a validation set of input clinical text report data does not improve.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>applying the retrained text report machine learning model to a second set of unlabeled text reports with associated images to label the associated images;</claim-text><claim-text>selecting a second portion of the labeled associated images;</claim-text><claim-text>re-training the retrained image machine learning model using the selected second portion of labeled associated images;</claim-text><claim-text>applying the retrained image machine learning model to a second set of unlabeled images with associated text reports to label the associated text reports;</claim-text><claim-text>selecting a second portion of labeled associated text reports; and</claim-text><claim-text>re-training the retrained text report machine learning model using the selected second portion of the labeled text reports.</claim-text></claim-text></claim></claims></us-patent-application>