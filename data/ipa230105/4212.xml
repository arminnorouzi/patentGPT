<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004213A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004213</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364254</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>167</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">PROCESSING PART OF A USER INPUT TO PRODUCE AN EARLY RESPONSE</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>Microsoft Technology Licensing, LLC</orgname><address><city>Redmond</city><state>WA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>SIU</last-name><first-name>Chun Hin Nelson</first-name><address><city>Woodinville</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>KHALIL</last-name><first-name>Hosam Adel</first-name><address><city>Issaquah</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>NANDI</last-name><first-name>Ajoy</first-name><address><city>Redmond</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>QUAN</last-name><first-name>Carmen</first-name><address><city>Seattle</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>FISENKO</last-name><first-name>Denis</first-name><address><city>Redmond</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>CHY</last-name><first-name>Md Nizam Uddin</first-name><address><city>Seattle</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="06" designation="us-only"><addressbook><last-name>HU</last-name><first-name>Min</first-name><address><city>Redmond</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="07" designation="us-only"><addressbook><last-name>BASOGLU</last-name><first-name>Christopher Hakan</first-name><address><city>Everett</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="08" designation="us-only"><addressbook><last-name>PATHAK</last-name><first-name>Sayan Dev</first-name><address><city>Kirkland</city><state>WA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Techniques are provided for early processing of a part of a user input to produce a response to the entire or final user input. While the user input is being received, a partial user input, which is a part of the final user input, is processed to produce a response. The response is a candidate response for the final user input. After the final user input is received, and if the partial user input is determined to match or be equivalent to the final user input, the first response, which is already available, is provided to one or more output devices for presentation. If the final user input is determined to differ from the partial user input, the final user input is processed to produce a second response to the final user input, and the second response is provided for presentation. In some instances, multiple partial user inputs are received and processed.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="47.33mm" wi="125.73mm" file="US20230004213A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="194.65mm" wi="132.25mm" file="US20230004213A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="116.59mm" wi="127.76mm" file="US20230004213A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="183.98mm" wi="90.00mm" file="US20230004213A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="206.16mm" wi="124.21mm" file="US20230004213A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="169.08mm" wi="146.56mm" file="US20230004213A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="221.49mm" wi="145.12mm" file="US20230004213A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="211.50mm" wi="153.08mm" file="US20230004213A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="189.23mm" wi="157.23mm" orientation="landscape" file="US20230004213A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="181.27mm" wi="123.36mm" file="US20230004213A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="177.46mm" wi="161.29mm" file="US20230004213A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="174.50mm" wi="162.98mm" file="US20230004213A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">Many software applications today are able to receive spoken inputs, gesture inputs, and/or text inputs (&#x201c;user inputs&#x201d;). The user inputs can be commands, requests, search queries, shortcut inputs (e.g., function keys) and the like. After inputting the user input, the user provides an indication of the end of the entry of the spoken or text input, such as not typing or speaking for a given period of time or by inputting an &#x201c;enter&#x201d; or a &#x201c;return&#x201d; command. The user input is then processed to provide a response to the user input. Thus, applications or systems must wait to receive the entire user input before the user input can be processed.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0003" num="0002">Embodiments disclosed herein provide techniques for early processing of a part of a user input to produce a response to the entire or final user input (referred to herein as &#x201c;final user input&#x201d;). The user input can be any input that is processed or acted upon. Example user inputs include, but are not limited to, search queries, requests, commands, and shortcuts. While the user input is being received, a partial user input, which is a part of the final user input, is processed to produce a first response. The response is a candidate response for the final user input. After the final user input is received, and if the partial user input matches, or is determined to be equivalent to, the final user input, the first response is provided to one or more output devices for presentation. If the final user input is determined to differ from the partial user input, the final user input is processed to produce a second response to the final user input, and the second response is provided for presentation.</p><p id="p-0004" num="0003">In one aspect, a method includes receiving a user input. The user input may be received as a spoken user input, a text user input, a gesture user input, or another type of user input. During a time period in which the user input is received, a triggering condition associated with the user input is detected. The part of the user input that is received when the triggering condition is detected constitutes a partial user input. The partial user input is caused to be processed and a response to the partial user input is received. For example, the partial user input may be processed by the application that received the user input, or the partial user input can be transmitted to one or more other applications for processing. All of the user input is received to produce a final user input. A determination is made as to whether the partial user input matches or is equivalent to the final user input. In one embodiment, the partial user input is compared to the final user input to determine if the partial user input matches or is equivalent to the final user input. When a determination is made that the partial user input matches or is equivalent to the final user input, the response is provided for presentation. The response can be provided to one or more output devices, such as a display.</p><p id="p-0005" num="0004">In another aspect, a system includes a processing device and a storage device operably connected to the processing device. The storage device stores instructions, that when executed by the processing device, cause operations to be performed. The operations include receiving a user input and detecting a triggering condition that is associated with the user input. The triggering condition is detected during a time period in which the user input is received. The part of the user input that is received when the triggering condition is detected constitutes a partial user input. The partial user input is caused to be processed and a response to the partial user input is received. All of the user input is received to produce a final user input. Based on a determination that the partial user input matches or is equivalent to the final user input, the response is provided for presentation. The response can be provided to one or more output devices, such as a display.</p><p id="p-0006" num="0005">In yet another aspect, a method includes receiving a user input and detecting a first triggering condition for the user input. The first triggering condition is detected during a time period in which the user input is received. The first part of the user input that is received when the first triggering condition is detected constitutes a first partial user input. The first partial user input is provided for processing and a first response to the first partial user input is received. A second triggering condition for the user input is detected. The part of the user input that has been received when the second triggering condition is detected constitutes a second partial user input (e.g., the first partial user input and the part received when the second triggering condition is detected). The second partial user input is provided for processing and a second response to the second partial user input is received. All of the user input is received to produce a final user input. Based on a determination that the second partial user input matches or is equivalent to the final user input, the second response is provided to an output device.</p><p id="p-0007" num="0006">This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007">Non-limiting and non-exhaustive examples are described with reference to the following Figures. The elements of the drawings are not necessarily to scale relative to each other. Identical reference numerals have been used, where possible, to designate identical features that are common to the figures.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a first process flow of processing a user input in accordance with some embodiments;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a second process flow of processing a user input in accordance with some embodiments;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a third process flow of processing a user input in accordance with some embodiments;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a block diagram of a first system in which aspects of the present disclosure may be practiced;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a block diagram of a second system in which aspects of the present disclosure may be practiced;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates example triggering conditions and elements that can be used to define or adjust the triggering conditions in accordance with some embodiments;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIGS. <b>7</b>A-<b>7</b>B</figref> illustrate a flowchart of a method of processing a user input in accordance with some embodiments;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a block diagram depicting example physical components of a computing device with which aspects of the disclosure may be practiced;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIGS. <b>9</b>A-<b>9</b>B</figref> illustrate block diagrams illustrating a mobile computing device with which aspects of the present disclosure may be practiced; and</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates a block diagram of a distributed computing system in which aspects of the present disclosure may be practiced.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0019" num="0018">In the following detailed description, references are made to the accompanying drawings that form a part hereof, and in which are shown by way of illustrations, specific embodiments, or examples. These aspects may be combined, other aspects may be utilized, and structural changes may be made without departing from the present disclosure. Embodiments may be practiced as methods, systems, or devices. Accordingly, embodiments may take the form of a hardware implementation, an entirely software implementation, or an implementation combining software and hardware aspects. The following detailed description is therefore not to be taken in a limiting sense, and the scope of the present disclosure is defined by the appended claims and their equivalents.</p><p id="p-0020" num="0019">Generally, embodiments disclosed herein provide techniques for early processing of one or more parts of a user input while the user input is being received. Processing of a part of the user input produces a response to the part of the user input. The response is a candidate response for the final user input. The user input can be any input that is processed or acted upon. Example user inputs include, but are not limited to, requests, search queries, dictation with inline commanding, commands, and shortcuts.</p><p id="p-0021" num="0020">One or more triggering conditions are detected in the user input while the user input is received, which causes the part of the user input that has been received at the time of the detection to be a partial user input. The triggering condition can be within the partial user input or may be one or more characteristics associated with the partial user input. As will be described in more detail later, a triggering condition can include, but is not limited to, a gap or a pause within the spoken or typed user input (e.g., the user stops speaking or typing for a given period of time), a linguistic feature in the user input (e.g., a specific word or words), an expected user input (e.g., a prediction of the content of the user input), and the lapse of an amount of time.</p><p id="p-0022" num="0021">The partial user input is processed to produce a first response while the user input continues to be received. The first response is a candidate response for the final user input. If the partial user input is determined to match, or to be equivalent to, the final user input, the first response, which is already available, is provided to one or more output devices for presentation. Generally, a partial user input is &#x201c;equivalent&#x201d; to a final user input when the partial user input matches or is determined to be sufficiently similar to the final user input. For example, a classifier may be used to compare the partial and the final user inputs and determine whether the partial and the final user inputs are considered equivalent. If the final user input is determined to differ from the partial user input, the final user input is processed to produce a second response to the final user input. The second response is then provided for presentation.</p><p id="p-0023" num="0022">In some instances, multiple partial user inputs are created before the final user input is received (e.g., while a text or audio stream of the user input is received). The multiple partial user inputs are processed to produce responses to the partial user inputs. If the last received partial user input is determined to be equivalent to the final user input, the response to that partial user input is provided to an output device for presentation. When it is determined that none of the partial user inputs match or are equivalent to the final user input, the final user input is processed to produce a response, and the response is provided to one or more output devices for presentation.</p><p id="p-0024" num="0023">When the partial user input is equivalent to the final user input, the processing of the partial user input to produce a candidate response can cause a response to the final user input to be presented to a user more quickly. In some instances, such as when the amount of time needed to process the final user input is large, the early processing of the partial user response can conceal that large amount of processing time. For example, in a non-limiting nonexclusive example, if the amount of time needed to process a final user input is five hundred milliseconds, and processing of a partial user input that is equivalent to the final user input begins three hundred milliseconds earlier, the user will experience a two hundred millisecond wait for a response to the final user input. Since the three hundred millisecond processing time of the partial user input occurs during the time period the user is submitting the final user input, the user is unaware of that three hundred milliseconds processing time.</p><p id="p-0025" num="0024">In some embodiments, the user input is analyzed continuously to detect a partial user input. Thus, in some instances, the partial user input that is received last in time can be the final user input when an indication that the user has finished entering the user input has not yet been received. For example, the user may not have submitted an &#x201c;enter&#x201d; or a &#x201c;return&#x201d; command, or the requisite amount of time in which the user does not type or speak may not have passed. Accordingly, in such embodiments, the partial use input matches the final user input, and the processing of the partial user input (which matches the final user input) begins earlier than the processing of the final user input would have begun. Again, a response to the final user input can therefore be presented to the user in less time. In other embodiments, the user input can be analyzed at selected times.</p><p id="p-0026" num="0025">The determination of whether a partial user input matches or is equivalent to the final user input can be based on a range of equivalency levels. For example, some systems can be very conservative in this determination by requiring the partial user input to match, character by character, the final user input. Other systems can be less conservative and base the determination on whether the final user input includes one or more words that render the final user input literally different from the partial user input but not functionally different from the partial user input. For example, a classifier may be used to compare the partial and the final user inputs and determine whether the partial and the final user inputs are considered equivalent.</p><p id="p-0027" num="0026">In some embodiments, the type of user input can affect the level of equivalency that is used. For example, a partial user input may be more likely to match or be equivalent to the final user input when the user input is a command to perform an action (e.g., turn off music) than a request that includes one or more requirements (e.g., a date, a time, a name of a person). Additionally or alternatively, the domain in which the user input is associated with may affect the level of equivalency that a system requires between a final and a partial user input. Some domains, such as the weather domain, can have more frequently used user inputs (e.g., what is today's weather forecast) compared to other domains, such as a travel domain where user inputs can have multiple requirements.</p><p id="p-0028" num="0027">Technical advantages of the disclosed embodiments include providing a response to a user input more quickly. At least a portion of the time that is required to process a user input can be concealed from the user. The shorter response time can improve the user experience in that the amount of time the user waits for a response is reduced. Thus, search results may be provided more quickly and/or commands can be executed more quickly.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a first process flow of processing a user input in accordance with some embodiments. A user input is received during the time period t<b>1</b> to t<b>2</b> (&#x201c;user input time period&#x201d;). The user input can be received via a spoken input, a text input, a touch input, a gesture input, or by another input mechanism. After the final user input is received at time t<b>2</b>, the user input is processed by an application between the time period t<b>2</b> to t<b>3</b> (&#x201c;processing time period&#x201d;). A response to the final user input is produced at time t<b>3</b>. In a non-limiting nonexclusive embodiment, the user input is a search query for a search operation. The search query is received during the user input period. The search operation is executed during the processing time period and the search results are produced at time t<b>3</b>.</p><p id="p-0030" num="0029">In embodiments disclosed herein, one or more partial user inputs are processed during the user input time period. If at least one partial user input matches the final user input, the response from that partial user input is provided (or caused to be provided) for presentation on an output device. In one embodiment, the response that is presented to the output device is the response to the partial user input that is received last in time.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a second process flow of processing a user input in accordance with some embodiments. <figref idref="DRAWINGS">FIG. <b>2</b></figref> is described in conjunction with one partial user input, but other embodiments are not limited to this implementation. Multiple partial user inputs can be processed in other embodiments.</p><p id="p-0032" num="0031">A user input is received during the user input time period (time t<b>1</b> to t<b>4</b>). Similar to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the user input can be received via a spoken input, a text input, a touch input, a gesture input, or some other type of input. A triggering condition is detected at time t<b>2</b> when only a part of the user input has been received. The triggering condition can be within the partial user input, may be one or more characteristics associated with the partial user input, or may be time-based. As will be described in more detail later in conjunction with <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a triggering condition can include, but is not limited to, a gap or a pause within the spoken or typed user input (e.g., the user stops speaking or typing for a given period of time), a prosodic characteristic of the user input, a linguistic feature in the user input (e.g., a specific word or words), an expected user input (e.g., a prediction of the content of the user input), and the lapse of an amount of time.</p><p id="p-0033" num="0032">The partial user input is processed between the times t<b>2</b> to t<b>3</b> to produce a first response at time t<b>3</b>. Thus, the first response is available at time t<b>3</b> and is a candidate response for the final user input. The final user input is received at time t<b>4</b>. If the partial user input matches or is determined to be equivalent to the final user input, the response received at time t<b>3</b> is provided to an output device. However, if the remaining part of the user input that is received between time t<b>3</b> and time t<b>4</b> provides one or more additional conditions or requirements to the partial user input, the partial user input and the first response can be discarded and the final user input is processed during the processing time period (between times t<b>4</b> to t<b>5</b>) to produce a second response. The second response is then provided for presentation to an output device. Thus, processing the partial user input can enable a system to produce a response in less time compared to processing the final user input. In the illustrated embodiment, when the first response is provided for presentation, a time savings of time t<b>4</b> to time t<b>5</b> is realized (e.g., time t<b>5</b>&#x2212;time t<b>4</b>=time savings).</p><p id="p-0034" num="0033">In a non-limiting nonexclusive embodiment, a user may input a spoken user input of &#x201c;schedule meeting at 10 on Tuesday with John.&#x201d; As the user is speaking the user input, an analyzer application (e.g., a speech recognition application) detects a triggering condition to create a partial user input of &#x201c;schedule meeting at 10 on Tuesday.&#x201d; The partial user input is provided to an application for processing (e.g., a virtual assistant application) while the remaining part of the user input &#x201c;with John&#x201d; is received. Since the remaining part of the user input adds an additional condition to the partial user input, the partial user input &#x201c;schedule a meeting at 10 am on Tuesday&#x201d; is discarded and the final user input of &#x201c;schedule meeting at 10 on Tuesday with John&#x201d; is provided to the application for processing to cause the meeting to be scheduled.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a third process flow of processing a user input in accordance with some embodiments. The process shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> is similar to the process of <figref idref="DRAWINGS">FIG. <b>2</b></figref> except that there are two partial user inputs. A user input is received during the user input time period (time t<b>1</b> to time t<b>5</b>). A first triggering condition is detected at time t<b>2</b> when only a first part of the user input has been received. The first partial user input is processed between the times t<b>2</b> to t<b>3</b> to produce a first response at time t<b>3</b>. Thus, the first response is available at time t<b>3</b>.</p><p id="p-0036" num="0035">A second triggering condition is detected at time t<b>4</b> when a second part of the user input has been received. In one embodiment, the second partial input includes the first partial input and the second part of the user input, the first partial user input and the first response are discarded, and the second partial user input is processed during the processing time period (between times t<b>4</b> to t<b>6</b>) to produce a second response. In the illustrated embodiment, the processing of the second partial user input begins before the final user input is received at time t<b>5</b> and continues until time t<b>6</b>, a time that is after time t<b>5</b>.</p><p id="p-0037" num="0036">Processing of the final user input begins at time t<b>5</b>. In some embodiments, when the second partial user input matches the final user input, processing of the final user input ends and the second response is provided after time t<b>6</b> for presentation. Thus, a time savings of (time t<b>7</b>&#x2212;time t<b>6</b>) is realized. Alternatively, when the second partial user input matches the final user input, processing of the final user input continues until time t<b>7</b> to produce a third response, but the second response is provided for presentation after time t<b>6</b> and the third response is discarded. Again, a time savings of (time t<b>7</b>&#x2212;time t<b>6</b>) is obtained.</p><p id="p-0038" num="0037">If the final user input is determined to differ from the second partial user input, the second partial user input and the second response may be discarded, and the final user input is processed to produce a third response. The third response is provided for presentation after time t<b>7</b>.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a block diagram of a first system in which aspects of the present disclosure may be practiced. The system <b>400</b> includes a computing device <b>402</b> that includes one or more storage devices (collectively referred to as storage device <b>404</b>) and one or more processing devices (collectively referred to as processing device <b>406</b>). The storage device <b>404</b> stores computer-executable instructions or one or more software applications <b>408</b>, a search application <b>410</b>, an analyzer application <b>412</b>, and a virtual assistant application <b>414</b>. The search application <b>410</b> is operable to perform intranet and/or internet searches using communication device <b>416</b>. In one embodiment, the analyzer application <b>412</b> is operable to recognize text that is in or represents a user input that is received by the computing device <b>402</b> from a user <b>418</b>, triggering conditions within or associated with the text, and determine the equivalency of partial user inputs to final user inputs. The virtual assistant application <b>414</b> is an intelligent assistant that is operable to perform or execute operations, such as tasks and commands. A non-limiting nonexclusive example of a virtual assistant application <b>414</b> is MICROSOFT CORTANA. In some embodiments, the analyzer application <b>412</b> is included in the software application(s) <b>408</b>, the search application <b>410</b>, and/or the virtual assistant application <b>414</b>.</p><p id="p-0040" num="0039">The user <b>418</b> interacts with the software application(s) <b>408</b>, the search application <b>410</b>, and/or the virtual assistant application <b>414</b> to perform various activities. The activities can include sending, receiving, redirecting, creating, modifying, deleting, and viewing electronic communications, such emails, text messages, instant messages, online chats, video messages, audio messages, and posts in social media. The activities may further include: (1) creating, deleting, viewing, and/or editing documents; (2) creating, deleting, viewing, and/or editing and calendars; (3) organizing and/or attending meetings; (4) setting, modifying, deleting, monitoring, and/or completing tasks; and/or (5) setting, modifying, and/or deleting reminders.</p><p id="p-0041" num="0040">A text-to-speech (TTS) and speech-to-text (STT) application <b>420</b> is stored in the storage device <b>404</b>. The TTS application is operable to convert text into speech (an audio output). The STT application is operable to recognize and convert speech (an audio input) into text.</p><p id="p-0042" num="0041">In some embodiments, the analyzer application <b>412</b> includes one or more machine learning mechanisms (e.g., models, algorithms, or applications) that are operable to detect one or more triggering conditions in a user input and to determine the equivalency of a partial user input to the final user input. Generally, the analyzer application <b>412</b> use natural language processing to detect the one or more partial user inputs. The machine learning mechanism(s) is adaptable over time such that the analyzer application <b>412</b> learn and become more efficient and effective at detecting partial user inputs, determining a partial user input is to be processed while the remaining part of a user input is received, and determining whether a partial user input is equivalent to a final user input. In a non-limiting nonexclusive example, the machine learning mechanism(s) learns over time based on the user's <b>418</b> interactions with the presentation of the responses to the partial user inputs, information the user <b>418</b> accesses or interacts with, and other types of user interactions.</p><p id="p-0043" num="0042">A user input is received at the computing device <b>402</b> through one or more input devices (collectively referred to as input device <b>422</b>). As discussed earlier, the user input can be a question, a shortcut, a statement, a command, a request, or other input that will cause one or more applications <b>408</b>, the search application <b>410</b>, and/or the virtual assistant application <b>414</b> to provide one or more responses to one or more output devices (collective referred to as output device <b>424</b>). In one embodiment, the application that receives the user input processes the user input to provide a response. Additionally or alternatively, the application that received the user input can provide the user input to one or more other applications for processing and a response to the user input is received by the application that received the user input.</p><p id="p-0044" num="0043">The input device <b>422</b> and the output device <b>424</b> can each be included in the computing device <b>402</b> or may be operably connected to the computing device <b>402</b>. An example input device <b>422</b> includes, but is not limited to, a touchscreen, a microphone (e.g., in combination with STT application <b>420</b>), a physical or a virtual keyboard, and a wireless stylus. An example output device <b>424</b> includes, but is not limited to, a display, a speaker (e.g., in combination with TTS application <b>420</b>), a printer, a television, and a projector.</p><p id="p-0045" num="0044">The user <b>418</b> can input user inputs to the computing device <b>402</b> using an input device. The user input can be input as an audio input (e.g., a spoken user input), a text input (e.g., typed text), and one or more gestures (e.g., a slide or a swipe type input). When the user input is input as an audio input, the STT application <b>420</b> coverts the audio to text prior to processing by the analyzer application. The analyzer application <b>412</b> processes the text in real-time (or substantially real-time) to detect one or more triggering conditions in or associated with the user input. As will be described in more detail later in conjunction with <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a triggering condition can be a gap or a pause within the spoken or typed user input (e.g., the user stops speaking or typing for a given period of time), a prosodic characteristic of the user input, a linguistic feature in the user input (e.g., a specific word or words), a prediction of the content of the user input, a lapse of time, and so on. When the analyzer application <b>412</b> detects a triggering condition, the analyzer application <b>412</b> can treat the part of the user input that has been received as a partial user input and send the partial user input to an application for processing to produce a first response. In some embodiments, the analyzer application <b>412</b> analyzes the part of the user input that has been received to determine if that part should be treated as a partial user input or if more of the user input should be received. For example, as discussed in more detail in conjunction with <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a demographic or regional characteristic may be considered when determining whether a triggering condition is met.</p><p id="p-0046" num="0045">The remaining part of the user input continues to be received when the partial user input is processed. The first response is provided to the output device <b>424</b> when the partial user input corresponds to the final user input. When the final user input differs from the partial user input, the partial user input and the first response are discarded, and the final user input is provided to an application for processing to produce a second response to the final user input.</p><p id="p-0047" num="0046">The computing device <b>402</b> can be any suitable type of computing device. Example computing devices include a laptop computer, a tablet, a mobile telephone, a smart phone, a smart watch, a wearable computer, a desktop computer, a gaming device/computer (e.g., Xbox), a television, or a server computing device. These example computing devices are for example purposes only and should not be considered as limiting.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a block diagram of a second system in which aspects of the present disclosure may be practiced. The system <b>500</b> is a distributed system that includes the computing device <b>402</b>, a second computing device <b>502</b>, and a third computing device <b>504</b>. The second and the third computing devices <b>502</b>, <b>504</b> are each operably connected to the computing device <b>402</b> through one or more networks (collectively network <b>506</b>).</p><p id="p-0049" num="0048">The second computing device <b>502</b> includes one or more storage devices (collectively storage device <b>508</b>) that stores one or more applications <b>510</b>. The application(s) <b>510</b> can at least be one of the applications <b>408</b>, or the application(s) <b>510</b> may differ from the applications <b>408</b>. One or more processing devices (collectively processing device <b>512</b>) are operable to execute the application(s) <b>510</b>.</p><p id="p-0050" num="0049">The third computing device <b>504</b> includes one or more storage devices (collectively storage device <b>514</b>) that stores the analyzer application <b>412</b> and the virtual assistant application <b>414</b>. One or more processing devices (collectively processing device <b>516</b>) are operable to execute the assistant application <b>414</b>. When executed by the processing device <b>516</b>, the analyzer application <b>412</b> can receive STT data through the network <b>506</b> to detect one or more triggering conditions.</p><p id="p-0051" num="0050">The network <b>506</b> is illustrative of any suitable type of network, for example, an intranet, and/or a distributed computing network (e.g., the Internet) over which the computing devices <b>402</b>, <b>502</b>, <b>504</b> may communicate with each other. Additionally, the computing devices <b>502</b>, <b>504</b> can each be any suitable computing device, such as a mobile telephone, a smart phone, a tablet, a smart watch, a wearable computer, a personal computer a desktop computer, a laptop computer, a gaming device/computer (e.g., Xbox), a television, or a server computing device. Although <figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts three computing devices <b>402</b>, <b>502</b>, <b>504</b>, other embodiments are not limited to this configuration. The system <b>500</b> can include any suitable number of computing devices.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates example triggering conditions and elements that can be used to define or adjust the triggering conditions in accordance with some embodiments. The example triggering conditions are within a user input or are a characteristic of a user input and are detected by an analyzer application (e.g., analyzer application <b>412</b> in <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>5</b></figref>). Embodiments can require that a single triggering condition is to be met or a combination of triggering conditions are to be met to create a partial user input.</p><p id="p-0053" num="0052">As discussed earlier, a user input <b>600</b> can be received as a spoken input or a text input. When the user input <b>600</b> is received as a text input, the analyzer application <b>412</b> receives the text input as a text stream <b>602</b>. When the user input <b>600</b> is received as a spoken input, an STT application <b>420</b> converts the audio stream <b>604</b> into a text stream <b>606</b> that is received by the analyzer application <b>412</b>. The analyzer application <b>412</b> analyzes the text stream <b>602</b>, <b>606</b> in real-time or substantially real-time to detect one or more triggering conditions.</p><p id="p-0054" num="0053">In some embodiments, the analyzer application <b>412</b> includes or is operably connected to a language understanding (LU) application <b>607</b>. The language understanding application <b>607</b> is operable to analyze the text stream <b>602</b>, <b>606</b> in real-time or in substantially real-time to determine whether the part of the text stream <b>602</b>, <b>606</b> that has been received so far constitutes a partial user input or not. When the language understanding application <b>607</b> determines the received text stream <b>602</b>, <b>606</b> forms a partial user input, the partial user input is provided for processing to produce a response to the partial user input.</p><p id="p-0055" num="0054">A time gap <b>608</b> in the user input that equals or exceeds a given amount of time can be a triggering condition. Generally, the human ear can perceive a gap in speech of two hundred to two hundred and fifty milliseconds (&#x201c;gap time period&#x201d;). The time period for the analyzer application <b>412</b> can be the same, higher, or lower than two hundred to two hundred and fifty milliseconds. Additionally, the amount of time in the time gap <b>608</b> can be different for some users or for groups of users. For example, children can be given more time to submit a user input than adults, so the time gap for children may be longer than for adults. The time gap for users in geographic areas, such as cities, states, regions, or countries can vary based on the speech or communication patterns in the geographic areas.</p><p id="p-0056" num="0055">As the text stream <b>602</b>, <b>606</b> is received, the analyzer application <b>412</b> is reviewing the text stream <b>602</b>, <b>606</b> to detect a time gap <b>608</b> in the text stream <b>602</b>, <b>606</b> that at least meets the minimum time gap period. When the analyzer application <b>412</b> detects a time gap <b>608</b>, the analyzer application can transmit, or can cause to be transmitted, the partial user input for processing.</p><p id="p-0057" num="0056">One or more heuristics <b>610</b> may be a triggering condition. In a non-limiting nonexclusive embodiment, prior and current user inputs can be analyzed to determine the top N user inputs for one or more domains, where N is a number equal to or greater than one. A domain is associated with a specific topic or subject matter, where the subject matter is associated with tasks that are related to that specific subject matter. Example domains include, but are not limited to, weather, travel, stock quotes, ticket purchases, technical support, language learning, and restaurants. In the weather domain, for example, the user input of &#x201c;what is the weather forecast&#x201d; is a top user input. Using the top user inputs as a guide, the analyzer application <b>412</b> can be more aggressive in detecting a partial user input when the analyzer application <b>412</b> detects a user input that is currently being received is likely one of the top user inputs. Additionally or alternatively, a heuristic may be based on content typically included in a user input in general, and/or content typically included in a user input on a particular system (e.g., a MICROSOFT product versus a GOOGLE product).</p><p id="p-0058" num="0057">A predicted user input <b>612</b> can be a triggering condition. A predicted user input <b>612</b> may be based on what typically constitutes a final user input. For such user inputs, the analyzer application <b>412</b> may determine a triggering condition has been met when an expected part or most of the user input is received. For example, when the user input begins with the phrase &#x201c;set a reminder&#x201d;, a subject typically follows that phrase. Example subjects include a meeting, a lunch appointment, an event, a task, and the like. Thus, the analyzer application <b>412</b> may wait to detect a triggering condition until most, if not all, of the user input is received. Alternatively, when a user is playing audio (e.g., music), and the user input is &#x201c;turn off the music&#x201d;, the analyzer application <b>412</b> can determine a triggering condition is met after &#x201c;turn off the&#x201d; is received.</p><p id="p-0059" num="0058">An attentive user input <b>613</b> may be a triggering condition. Example attentive user inputs <b>613</b> include, but are not limited to, eye tracking and head tracking mechanisms (e.g., systems or applications). For example, a user may be typing a user input into a text box, such as a search field. The analyzer application <b>412</b> may detect the gaze of the user's eyes have shifted away from the text box or a display, or the analyzer application <b>412</b> can receive a signal from an eye tracking application or device. Based on the change in the gaze or the eye position of the user, the analyzer application <b>412</b> can determine if the part of the user input received at the time of the change in the user's gaze forms a partial input or not.</p><p id="p-0060" num="0059">Demographics <b>614</b> associated with the user can be an element that is used to define or adjust one or more triggering condition. Characteristics such as age and education level may influence the triggering condition. In a non-limiting non-exclusive example, a text or spoken input by a child may be different from that of a teenager or an adult, and a text or spoken input from an adult can be different from that of a senior adult. The text or spoken input from the child can take more time than that of an adult because the child is learning words and how to construct the words into phrases or sentences. In some embodiments, the analyzer application <b>412</b> may define or adjust one or more triggering conditions (e.g., a time gap) to have additional time for the receipt of the text or spoken input from a child. Alternatively, the analyzer application <b>412</b> may provide less time for a triggering condition for a text or spoken input that is input by an adult.</p><p id="p-0061" num="0060">The fluency <b>616</b> of a user input may be an element that is used to define or adjust one or more triggering conditions. In one embodiment, fluency <b>616</b> refers to a continuity, a smoothness, and/or an effort of the user input. For example, when the user input is received as a spoken input, the fluency <b>616</b> is speech fluency that refers to the continuity, smoothness, and/or effortlessness of the speech in the spoken input. For example, when a user stammers, stutters, hesitates, or mumbles, the analyzer application <b>412</b> may define or adjust one or more triggering conditions to have more time for the receipt of the user input before a determination is made that the triggering condition(s) is met.</p><p id="p-0062" num="0061">Similarly, when the user input is received as a text input, the fluency <b>616</b> can be text fluency that refers to the continuity, rapidness, and accuracy of the entry of the text in the text input. For example, when the text entry is discontinuous or the user changes the text previously entered, the analyzer application <b>412</b> may define or adjust one or more triggering conditions to have more time for the receipt of the text input before a determination is made that the triggering condition(s) is met.</p><p id="p-0063" num="0062">Regional features <b>618</b> associated with the user can be an element that is used to define or adjust one or more triggering conditions. The regional features <b>618</b> can be based on the educational levels of the people in a region, the speech or communication patterns in the region, and the like. The regional features can be local, national, and/or international. For example, the overall or average education level of a first country can be higher than for a second country. Compared to the first country, the analyzer application <b>412</b> operating in devices located in the second country can define or adjust one or more triggering conditions based on this regional characteristic. For example, the analyzer application <b>412</b> may define or adjust the triggering condition(s) to have additional time to receive a user input.</p><p id="p-0064" num="0063">One or more prosodic characteristics <b>620</b> of the user input that is received as a spoken input may be an element that is used to define or adjust one or more triggering conditions. Example prosodic characteristics include, but are not limited to, accent, intonation, stress pattern, loudness variations, and rhythm. In some embodiments, the analyzer application <b>412</b> can define or adjust one or more triggering conditions based on one or more prosodic characteristics. For example, the analyzer application <b>412</b> may define or adjust the triggering condition(s) to have additional time to receive a user input.</p><p id="p-0065" num="0064">Other inputs <b>622</b> may be a consideration in the determination of whether a triggering condition is met. The other inputs <b>622</b> can be associated with the computing device that is receiving the user input. The other inputs <b>622</b> include, but are not limited to, location data (e.g., GPS), an application that is executing on the computing device or producing an output of the computing device, or an input of the computing device. For example, if the location data indicates the user is located in a particular city (e.g., Boston), and the user input is &#x201c;what is the weather forecast for Boston&#x201d;, the analyzer application <b>412</b> can determine a triggering condition is met when &#x201c;what is the weather forecast&#x201d; is received based on the location data. The analyzer application <b>412</b> may append &#x201c;for Boston&#x201d; to the partial user input of &#x201c;what is the weather forecast.&#x201d; The response is provided in less time when the remainder of the user input is &#x201c;for Boston.&#x201d; Otherwise, the final input is processed if the city is not Boston. For example, if the final user input is &#x201c;what is the weather forecast for Dallas&#x201d;, the final input is processed.</p><p id="p-0066" num="0065">Additionally or alternatively, the partial user input of &#x201c;what is the weather forecast&#x201d; may be used to prime or activate a weather service application or connection. The partial user input can be used to prepare the weather service application to provide a weather forecast for a location. The weather service application is then ready to receive the final result of &#x201c;what is the weather forecast in Boston.&#x201d;</p><p id="p-0067" num="0066">In another non-limiting nonexclusive example, a user may be listening to music that is output from a computing device. The analyzer application <b>412</b> may determine a triggering condition is met when the user input is a command to &#x201c;turn off the music&#x201d; and &#x201c;turn off the&#x201d; has been received. The analyzer application <b>412</b> can append &#x201c;music&#x201d; to the partial user input of &#x201c;turn off the.&#x201d;</p><p id="p-0068" num="0067">Additional or different triggering conditions may be used in other embodiments. Generally, a triggering condition is used by the analyzer application <b>412</b> to determine whether to identify the user input that is received at a point in time as a partial user input that can be processed to produce a response to the partial user input.</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIGS. <b>7</b>A-<b>7</b>B</figref> illustrate a flowchart of a method of processing a user input in accordance with some embodiments. Initially, as shown in block <b>700</b>, an indication that a user input is to be entered is received. For example, a user may activate a virtual assistant application or begin entering text into a search box presented in a graphical user interface of an application, such as a word processing application, a web browser application, or a search application.</p><p id="p-0070" num="0069">Next, as shown in block <b>702</b>, receipt of the user input (e.g., the text stream <b>602</b>, <b>606</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>) begins. A determination is made at block <b>704</b> as to whether one or more triggering conditions are detected. In one embodiment, the one or more triggering conditions are the example triggering conditions shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. If a determination is made that one or more triggering conditions are detected, the method passes to block <b>706</b> where the partial user input (the part of the user input that has been received) is provided to an application for processing and a first response is received. The first response is stored (e.g., cached) as the user input continues to be received (block <b>708</b>).</p><p id="p-0071" num="0070">When a determination is made at block <b>704</b> that one or more triggering conditions have not been detected, or after block <b>708</b> has been performed, the method continues at block <b>710</b> where a determination is made as to whether the final user input has been received. In a non-limiting nonexclusive example, the determination may be based on not receiving the user input (e.g., the text stream <b>602</b>, <b>606</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>) for a given period of time. Alternatively, the determination can be based on the receipt of an &#x201c;enter&#x201d; or a &#x201c;return&#x201d; command.</p><p id="p-0072" num="0071">If a determination is made that the final input has been received, the method passes to block <b>712</b> where a determination is made as to whether the partial user input processed at block <b>706</b> is equivalent to the final user input. As noted previously, a partial user input is &#x201c;equivalent&#x201d; to a final user input when the partial user input matches or is considered equivalent to the final user input. If a determination is made that the partial user input matches or is equivalent to the final user input, the method continues at block <b>714</b> where the response received at block <b>706</b> is provided (or caused to be provided) for presentation. For example, the response can be provided to one or more output devices.</p><p id="p-0073" num="0072">When a determination is made at block <b>712</b> that the partial user input processed at block <b>706</b> does not match or is not equivalent to the final user input, the method continues at block <b>716</b> where the final user input is provided for processing and a response to the final user input is received. The response is then provided for presentation at block <b>714</b>.</p><p id="p-0074" num="0073">Returning to block <b>710</b>, when a determination is made that the final user input has not been received, the method passes to block <b>718</b> where a determination is made as to whether one or more triggering conditions have been detected. If a determination is made that one or more triggering conditions have not been detected, the method returns to block <b>710</b>. If a determination is made that one or more triggering conditions have been detected, the method passes to block <b>720</b> where the previous partial user input and the first response are discarded and the new partial user input is provided for processing and a response to the new partial user input is received. As discussed earlier, the new partial user input includes the prior partial input and the additional text that has been received when the triggering condition(s) is detected at block <b>718</b>.</p><p id="p-0075" num="0074">Next, as shown in block <b>722</b>, the new response is stored (e.g., cached) as the user input continues to be received. A determination is made at block <b>724</b> as to whether the final user input has been received. If a determination is made that the final input has been received, the method continues at block <b>726</b> where a determination is made as to whether the partial user input processed at block <b>720</b> matches or is equivalent to the final user input. If a determination is made that the partial user input matches or is equivalent to the final user input, the method continues at block <b>714</b> where the response received at block <b>720</b> is provided (or caused to be provided) for presentation.</p><p id="p-0076" num="0075">When a determination is made at block <b>726</b> that the partial user input processed at block <b>720</b> does not match or is not equivalent to the final user input, the method passes to block <b>716</b> where the final user input is provided for processing and a response to the final user input is received. The response is then provided for presentation at block <b>714</b>.</p><p id="p-0077" num="0076">Returning to block <b>724</b>, when a determination is made that the final user input has not been received, the method continues at block <b>728</b> where a determination is made as to whether one or more triggering conditions have been detected. If a determination is made that one or more triggering conditions have not been detected, the method returns to block <b>724</b>. If a determination is made that one or more triggering conditions have been detected, the method returns to block <b>720</b>, where blocks <b>720</b>, <b>722</b>, <b>724</b>, <b>728</b> repeat until the final user input is received.</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIGS. <b>8</b>-<b>10</b></figref> and the associated descriptions provide a discussion of a variety of operating environments in which aspects of the disclosure may be practiced. However, the devices and systems illustrated and discussed with respect to <figref idref="DRAWINGS">FIGS. <b>8</b>-<b>10</b></figref> are for purposes of example and illustration and are not limiting of a vast number of electronic device configurations that may be utilize for practicing aspects of the disclosure, as described herein.</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a block diagram illustrating physical components (e.g., hardware) of an electronic device with which aspects of the disclosure may be practiced. In a basic configuration, the electronic device <b>800</b> may include at least one processing device <b>802</b> and a memory <b>804</b>. Any suitable processing device <b>802</b> can be used. For example, the processing device <b>802</b> may be a microprocessor, an application specific integrated circuit, a field programmable gate array, or combinations thereof.</p><p id="p-0080" num="0079">Depending on the configuration and type of the electronic device <b>800</b>, the memory <b>804</b> may comprise, but is not limited to, volatile storage (e.g., random access memory), non-volatile storage (e.g., read-only memory), flash memory, or any combination of such memories. The memory <b>804</b> may include a number of program modules and data files, such as an operating system <b>806</b>, program modules <b>808</b>, and an analyzer application <b>810</b>. While executing on the processing device <b>802</b>, the analyzer application <b>810</b> may perform and/or cause to be performed processes including, but not limited to, the aspects as described herein.</p><p id="p-0081" num="0080">The operating system <b>806</b>, for example, may be suitable for controlling the operation of the electronic device <b>800</b>. Furthermore, embodiments of the disclosure may be practiced in conjunction with a graphics library, other operating systems, or any other application program and is not limited to any particular application or system. This basic configuration is illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref> by those components within a dashed line <b>812</b>.</p><p id="p-0082" num="0081">The electronic device <b>800</b> may have additional features or functionality. For example, the electronic device <b>800</b> may also include additional data storage devices (removable and/or non-removable) such as, for example, magnetic disks, optical disks, or tape. Such additional storage is illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref> by a removable storage device <b>814</b> and a non-removable storage device <b>816</b>.</p><p id="p-0083" num="0082">The electronic device <b>800</b> may also have one or more input device(s) <b>818</b> such as a keyboard, a trackpad, a mouse, a pen, a sound or voice input device, a touch, force and/or swipe input device, etc. The output device(s) <b>820</b> such as a display, speakers, a printer, etc. may also be included. The aforementioned devices are examples and others may be used. The electronic device <b>800</b> may include one or more communication devices <b>822</b> allowing communications with other electronic devices <b>824</b>. Examples of suitable communication devices <b>822</b> include, but are not limited to, radio frequency (RF) transmitter, receiver, and/or transceiver circuitry; universal serial bus (USB), parallel, and/or serial ports.</p><p id="p-0084" num="0083">The term computer-readable media as used herein may include storage media or devices. The storage media or devices may include volatile and nonvolatile, removable, and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, or program modules.</p><p id="p-0085" num="0084">The memory <b>804</b>, the removable storage device <b>814</b>, and the non-removable storage device <b>816</b> are all examples of storage devices. Each storage device may include RAM, ROM, electrically erasable read-only memory (EEPROM), flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other article of manufacture which can be used to store information and which can be accessed by the electronic device <b>800</b>. Any such storage device may be part of the electronic device <b>800</b>. In one embodiment, the storage device does not include a carrier wave or other propagated or modulated data signal.</p><p id="p-0086" num="0085">Communication media may be embodied by computer readable instructions, data structures, program modules, or other data in a modulated data signal, such as a carrier wave or other transport mechanism, and includes any information delivery media. The term &#x201c;modulated data signal&#x201d; may describe a signal that has one or more characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media may include wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, radio frequency (RF), infrared, and other wireless media.</p><p id="p-0087" num="0086">Furthermore, embodiments of the disclosure may be practiced in an electrical circuit comprising discrete electronic elements, packaged or integrated electronic chips containing logic gates, a circuit utilizing a microprocessor, or on a single chip containing electronic elements or microprocessors. For example, embodiments of the disclosure may be practiced via a system-on-a-chip (SOC) where each or many of the components illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref> may be integrated onto a single integrated circuit. Such an SOC device may include one or more processing units, graphics units, communications units, system virtualization units and various application functionality all of which are integrated (or &#x201c;burned&#x201d;) onto the chip substrate as a single integrated circuit.</p><p id="p-0088" num="0087">When operating via an SOC, the functionality described herein, with respect to the capability of client to switch protocols may be operated via application-specific logic integrated with other components of the electronic device <b>800</b> on the single integrated circuit (chip). Embodiments of the disclosure may also be practiced using other technologies capable of performing logical operations such as, for example, AND, OR, and NOT, including but not limited to mechanical, optical, fluidic, and quantum technologies. In addition, embodiments of the disclosure may be practiced within a general-purpose computer or in any other circuits or systems.</p><p id="p-0089" num="0088"><figref idref="DRAWINGS">FIGS. <b>9</b>A-<b>9</b>B</figref> illustrate a mobile electronic device <b>900</b>, for example, a mobile telephone, a smart phone, wearable computer (such as a smart watch), a tablet computer, a laptop computer, and the like, with which embodiments of the disclosure may be practiced. With reference to <figref idref="DRAWINGS">FIG. <b>9</b>A</figref>, one aspect of a mobile electronic device <b>900</b> for implementing the aspects described herein is illustrated.</p><p id="p-0090" num="0089">In a basic configuration, the mobile electronic device <b>900</b> is a handheld computer having both input elements and output elements. The mobile electronic device <b>900</b> typically includes a display <b>902</b> and one or more input buttons <b>904</b> that allow the user to enter information into the mobile electronic device <b>900</b>. The display <b>902</b> of the mobile electronic device <b>900</b> may also function as an input device (e.g., a display that accepts touch and/or force input).</p><p id="p-0091" num="0090">If included, an optional side input element <b>906</b> allows further user input. The side input element <b>906</b> may be a rotary switch, a button, or any other type of manual input element. In alternative aspects, mobile electronic device <b>900</b> may incorporate more or less input elements. For example, the display <b>902</b> may not be a touch screen in some embodiments. In yet another alternative embodiment, the mobile electronic device <b>900</b> is a portable phone system, such as a cellular phone. The mobile electronic device <b>900</b> may also include an optional keypad <b>908</b>. Optional keypad <b>908</b> may be a physical keypad or a &#x201c;soft&#x201d; keypad generated on the touch screen display.</p><p id="p-0092" num="0091">In various embodiments, the output elements include the display <b>902</b> for showing a graphical user interface (GUI) of a client or developer portal, a visual indicator <b>910</b> (e.g., a light emitting diode), and/or an audio transducer <b>912</b> (e.g., a speaker). In some aspects, the mobile electronic device <b>900</b> incorporates a vibration transducer for providing the user with tactile feedback. In yet another aspect, the mobile electronic device <b>900</b> incorporates input and/or output ports, such as an audio input (e.g., a microphone jack), an audio output (e.g., a headphone jack), and a video output (e.g., a HDMI port) for sending signals to or receiving signals from an external device.</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. <b>9</b>B</figref> is a block diagram illustrating the architecture of one aspect of a mobile electronic device <b>900</b>. That is, the mobile electronic device <b>900</b> can incorporate a system (e.g., an architecture) <b>914</b> to implement some aspects. In one embodiment, the system <b>914</b> is implemented as a &#x201c;smart phone&#x201d; capable of running one or more applications (e.g., browser, e-mail, calendaring, contact managers, messaging clients, games, media clients/players, diagramming, and sharing applications and so on). In some aspects, the system <b>914</b> is integrated as an electronic device, such as an integrated personal digital assistant (PDA) and wireless phone.</p><p id="p-0094" num="0093">One or more application programs <b>916</b> may be loaded into the memory <b>918</b> and run on or in association with the operating system <b>920</b>. Examples of the application programs include a phone dialer program, an electronic communication program (e.g., email program, instant message program), a triggering application program, a word processing program, a spreadsheet program, an Internet browser program, and so forth.</p><p id="p-0095" num="0094">The system <b>914</b> also includes a non-volatile storage area <b>922</b> within the memory <b>918</b>. The non-volatile storage area <b>922</b> may be used to store persistent information that should not be lost when the system <b>914</b> is powered down.</p><p id="p-0096" num="0095">The application programs <b>916</b> may use and store information in the non-volatile storage area <b>922</b>, such as email, attachments or other messages used by an email application, and the like. A synchronization application (not shown) also resides on the system <b>914</b> and is programmed to interact with a corresponding synchronization application resident on a host computer to keep the information stored in the non-volatile storage area <b>922</b> synchronized with corresponding information stored at the host computer.</p><p id="p-0097" num="0096">The system <b>914</b> has a power supply <b>924</b>, which may be implemented as one or more batteries. The power supply <b>924</b> may further include an external power source, such as an AC adapter or a powered docking cradle that supplements or recharges the batteries.</p><p id="p-0098" num="0097">The system <b>914</b> may also include a radio interface layer <b>926</b> that performs the function of transmitting and receiving radio frequency communications. The radio interface layer <b>926</b> facilitates wireless connectivity between the system <b>914</b> and the &#x201c;outside world,&#x201d; via a communications carrier or service provider. Transmissions to and from the radio interface layer <b>926</b> are conducted under control of the operating system <b>920</b>. In other words, communications received by the radio interface layer <b>926</b> may be disseminated to the application programs <b>916</b> via the operating system <b>920</b>, and vice versa.</p><p id="p-0099" num="0098">The visual indicator <b>910</b> may be used to provide visual notifications, and/or an audio interface <b>928</b> may be used for producing audible notifications via an audio transducer (e.g., audio transducer <b>912</b> illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref>). In the illustrated embodiment, the visual indicator <b>910</b> is a light emitting diode (LED) and the audio transducer <b>912</b> may be a speaker. These devices may be directly coupled to the power supply <b>924</b> so that when activated, they remain on for a duration dictated by the notification mechanism even though the processor <b>930</b> and other components might shut down for conserving battery power. The LED may be programmed to remain on indefinitely until the user takes action to indicate the powered-on status of the device.</p><p id="p-0100" num="0099">The audio interface <b>928</b> is used to provide audible signals to and receive audible signals from the user (e.g., voice input such as described above). For example, in addition to being coupled to the audio transducer <b>912</b>, the audio interface <b>928</b> may also be coupled to a microphone to receive audible input, such as to facilitate a telephone conversation.</p><p id="p-0101" num="0100">The system <b>914</b> may further include a video interface <b>932</b> that enables an operation of peripheral device <b>934</b> (e.g., on-board camera) to record still images, video stream, and the like.</p><p id="p-0102" num="0101">A mobile electronic device <b>900</b> implementing the system <b>914</b> may have additional features or functionality. For example, the mobile electronic device <b>900</b> may also include additional data storage devices (removable and/or non-removable) such as, magnetic disks, optical disks, or tape. Such additional storage is illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>B</figref> by the non-volatile storage area <b>922</b>.</p><p id="p-0103" num="0102">Data/information generated or captured by the mobile electronic device <b>900</b> and stored via the system <b>914</b> may be stored locally on the mobile electronic device <b>900</b>, as described above, or the data may be stored on any number of storage media that may be accessed by the device via the radio interface layer <b>926</b> or via a wired connection between the mobile electronic device <b>900</b> and a separate electronic device associated with the mobile electronic device <b>900</b>, for example, a server-computing device in a distributed computing network, such as the Internet (e.g., server computing device <b>1014</b> in <figref idref="DRAWINGS">FIG. <b>10</b></figref>). As should be appreciated such data/information may be accessed via the mobile electronic device <b>900</b> via the radio interface layer <b>926</b> or via a distributed computing network. Similarly, such data/information may be readily transferred between electronic devices for storage and use according to well-known data/information transfer and storage means, including email and collaborative data/information sharing systems.</p><p id="p-0104" num="0103">As should be appreciated, <figref idref="DRAWINGS">FIG. <b>9</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>9</b>B</figref> are described for purposes of illustrating the present methods and systems and is not intended to limit the disclosure to a particular sequence of steps or a particular combination of hardware or software components.</p><p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates a block diagram of a distributed system in which aspects of the disclosure may be practiced. The system <b>1000</b> includes a general computing device <b>1002</b> (e.g., a desktop computer), a tablet computing device <b>1004</b>, and/or a mobile computing device <b>1006</b>. The general computing device <b>1002</b>, the tablet computing device <b>1004</b>, and the mobile computing device <b>1006</b> can each include the components, or be connected to the components, that are shown associated with the electronic device <b>800</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref> or the mobile electronic device <b>900</b> in <figref idref="DRAWINGS">FIGS. <b>9</b>A-<b>9</b>B</figref>.</p><p id="p-0106" num="0105">The general computing device <b>1002</b>, the tablet computing device <b>1004</b>, and the mobile computing device <b>1006</b> are each configured to access one or more networks (represented by network <b>1008</b>) to interact with the analyzer application <b>1010</b> stored in one or more storage devices (represented by storage device <b>1012</b>) and executed on one or more server computing devices (represented by server computing device <b>1014</b>). In some aspects, the server computing device <b>1014</b> can access and/or receive various types of services, communications, documents and information transmitted from other sources, such as a web portal <b>1016</b>, an electronic communications services <b>1018</b>, directory services <b>1020</b>, instant messaging and/or text services <b>1022</b>, and/or social networking services <b>1024</b>. In some instances, these sources may provide robust reporting, analytics, data compilation and/or storage service, etc., whereas other services may provide search engines or other access to data and information, images, graphics, videos, document processing and the like.</p><p id="p-0107" num="0106">As should be appreciated, <figref idref="DRAWINGS">FIG. <b>10</b></figref> is described for purposes of illustrating the present methods and systems and is not intended to limit the disclosure to a particular sequence of steps or a particular combination of hardware or software components.</p><p id="p-0108" num="0107">Aspects of the present disclosure, for example, are described above with reference to block diagrams and/or operational illustrations of methods, systems, GUIs, and computer program products according to aspects of the disclosure. As discussed earlier, the operations noted in the blocks may occur out of the order as shown in any flowchart. For example, two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order, depending upon the functionality/acts involved. Additionally, the functionality or elements shown in one GUI can be used in another GUI, and vice versa.</p><p id="p-0109" num="0108">The description and illustration of one or more aspects provided in this application are not intended to limit or restrict the scope of the disclosure as claimed in any way. The aspects, examples, and details provided in this application are considered sufficient to convey possession and enable others to make and use the best mode of claimed disclosure. The claimed disclosure should not be construed as being limited to any aspect, example, or detail provided in this application. Regardless of whether shown and described in combination or separately, the various features (both structural and methodological) are intended to be selectively included or omitted to produce an embodiment with a particular set of features. Having been provided with the description and illustration of the present application, one skilled in the art may envision variations, modifications, and alternative aspects falling within the spirit of the broader aspects of the general inventive concept embodied in this application that do not depart from the broader scope of the claimed disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method, comprising:<claim-text>receiving a user input;</claim-text><claim-text>during a time period in which the user input is received, detecting a triggering condition that is associated with the user input, wherein a part of the user input that is received when the triggering condition is detected constitutes a partial user input;</claim-text><claim-text>causing the partial user input to be processed;</claim-text><claim-text>receiving a response to the partial user input;</claim-text><claim-text>receiving a remainder of the user input to produce a final user input;</claim-text><claim-text>comparing the partial user input to the final user input to determine whether the partial user input is equivalent to the final user input; and</claim-text><claim-text>when a determination is made that the partial user input is equivalent to the final user input, providing the response to the partial user input for presentation.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the response is a first response; and</claim-text><claim-text>the method further comprises:<claim-text>when a determination is made that the partial user input is not equivalent to the final user input, causing the final input to be processed;</claim-text><claim-text>receiving a second response to the final user input; and</claim-text><claim-text>providing the second response for presentation.</claim-text></claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising discarding the partial user input and the first response.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the user input is received as a spoken user input; and</claim-text><claim-text>the method further comprises causing the spoken user input to be converted to text prior to detecting the triggering condition in the user input.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the triggering condition is a first triggering condition;</claim-text><claim-text>the response is a first response;</claim-text><claim-text>the part of the user input is a first part of the user input;</claim-text><claim-text>the partial user input is a first partial user input; and</claim-text><claim-text>the method further comprises:<claim-text>during the time period in which the user input is received, detecting a second triggering condition in the user input, wherein a second part of the user input that is received when the second triggering condition is detected constitutes a second partial user input;</claim-text><claim-text>causing the second partial user input to be processed;</claim-text><claim-text>receiving a second response to the second partial input;</claim-text><claim-text>determining whether the second partial user input is equivalent to the final user input; and</claim-text><claim-text>when a determination is made that the second partial user input is equivalent to the final user input, providing the second response for presentation.</claim-text></claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising:<claim-text>when a determination is made that the second partial user input does is not equivalent to the final user input, causing the final input to be processed;</claim-text><claim-text>receiving a third response to the final user input; and</claim-text><claim-text>providing the third response for presentation.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein an element for defining the triggering condition comprises a prosodic characteristic of the user input, the prosodic characteristic comprising one of a variation in an intonation, a stress pattern, a variation in loudness, or a variation in rhythm.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the triggering condition comprises a time gap in the user input that meets or exceeds a predefined amount of time.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the triggering condition comprises a predicted user input that represents an expected user input.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A system, comprising:<claim-text>a processing device; and</claim-text><claim-text>a storage device storing instructions, that when executed by the processing device, cause operations to be performed, the operations comprising:<claim-text>receiving a user input;</claim-text><claim-text>during a time period in which the user input is received, detecting a triggering condition that is associated with the user input, wherein a part of the user input that is received when the triggering condition is detected constitutes a partial user input;</claim-text><claim-text>causing the partial user input to be processed;</claim-text><claim-text>receiving a response to the partial user input;</claim-text><claim-text>receiving all of the user input to produce a final user input; and</claim-text><claim-text>based on a determination that the partial user input is equivalent to the final user input, providing the response to the partial user input for presentation.</claim-text></claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the storage device stores further instructions for analyzing, in at least substantially real time, the user input to detect the triggering condition.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein:<claim-text>the response is a first response; and</claim-text><claim-text>the storage device stores further instructions for:<claim-text>causing, based on a determination that the partial user input is not equivalent to the final user input, the final input to be processed;</claim-text><claim-text>receiving a second response to the final user input; and</claim-text><claim-text>providing the second response for presentation.</claim-text></claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein:<claim-text>the user input is received as a spoken user input; and</claim-text><claim-text>the storage device stores further instructions for causing the spoken user input to be converted to text prior to detecting the triggering condition in the user input.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein:<claim-text>the triggering condition is a first triggering condition;</claim-text><claim-text>the response is a first response;</claim-text><claim-text>the part of the user input is a first part of the user input;</claim-text><claim-text>the partial user input is a first partial user input; and</claim-text><claim-text>the storage device stores further instructions for:<claim-text>during the time period in which the user input is received, detecting a second triggering condition in the user input, wherein a second part of the user input that is received when the second triggering condition is detected constitutes a second partial user input;</claim-text><claim-text>causing the second partial user input to be processed;</claim-text><claim-text>receiving a second response to the second partial input; and</claim-text><claim-text>based on a determination that the second partial user input is equivalent to the final user input, providing the second response for presentation.</claim-text></claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the storage device stores further instructions for:<claim-text>causing, based on a determination that the second partial user input is not equivalent to the final user input, the final input to be processed;</claim-text><claim-text>receiving a third response to the final user input; and</claim-text><claim-text>providing the third response for presentation.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the triggering condition that is associated with the user input is included in a top N user inputs for a particular domain, where N is a number that equals or is greater than one.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the triggering condition comprises a characteristic of a geographic region associated with a user inputting the user input.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. A method, comprising:<claim-text>receiving a user input;</claim-text><claim-text>during a time period in which the user input is received, detecting a first triggering condition for the user input, wherein a first part of the user input that is received when the first triggering condition is detected constitutes a first partial user input;</claim-text><claim-text>receiving a first response to the first partial user input;</claim-text><claim-text>detecting a second triggering condition for the user input, wherein a second part of the user input that is received when the second triggering condition is detected constitutes a second partial user input;</claim-text><claim-text>receiving a second response to the second partial user input;</claim-text><claim-text>receiving all of the user input to produce a final user input; and</claim-text><claim-text>based on a determination that the second partial user input is equivalent to the final user input, providing the second response to an output device.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, further comprising:<claim-text>based on a determination that the second partial user input is not equivalent to the final user input, receiving a third response to the final user input; and</claim-text><claim-text>providing the third response to the output device.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein an element that is used to define the first triggering condition and the second triggering condition comprises at least one of:<claim-text>a demographic characteristic associated with a user inputting the user input; or</claim-text><claim-text>a fluency of the user input as the user input is received.</claim-text></claim-text></claim></claims></us-patent-application>