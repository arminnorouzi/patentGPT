<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004864A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004864</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17765807</doc-number><date>20191028</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc></classifications-cpc><invention-title id="d2e43">End-to-End Machine-Learning for Wireless Networks</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Google LLC</orgname><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Wang</last-name><first-name>Jibing</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Stauffer</last-name><first-name>Erik Richard</first-name><address><city>Sunnyvale</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Google LLC</orgname><role>02</role><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/US2019/058328</doc-number><date>20191028</date></document-id><us-371c12-date><date>20220331</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Techniques and apparatuses are described for generating an end-to-end machine-learning configuration for wireless networks. An end-to-end machine-learning controller determines an end-to-end machine-learning configuration for processing information exchanged through an end-to-end communication in a wireless network. The end-to-end machine-learning controller obtains capabilities of one or more devices that are utilized in the end-to-end communication. The end-to-end machine-learning controller determines the end-to-end machine-learning configuration for processing the information exchanged through the end-to-end communication, and directs the one or more devices to process the information exchanged through the end-to-end communication by forming one or more deep neural networks based on the end-to-end machine-learning configuration.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="234.95mm" wi="147.57mm" file="US20230004864A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="260.10mm" wi="152.82mm" file="US20230004864A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="260.10mm" wi="165.02mm" file="US20230004864A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="254.76mm" wi="106.85mm" file="US20230004864A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="234.19mm" wi="168.91mm" file="US20230004864A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="259.08mm" wi="173.23mm" file="US20230004864A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="255.95mm" wi="163.58mm" file="US20230004864A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="259.08mm" wi="164.76mm" file="US20230004864A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="260.69mm" wi="168.99mm" file="US20230004864A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="259.08mm" wi="173.23mm" file="US20230004864A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="255.44mm" wi="173.57mm" file="US20230004864A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="251.80mm" wi="152.57mm" file="US20230004864A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="255.95mm" wi="154.94mm" file="US20230004864A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="251.54mm" wi="173.23mm" file="US20230004864A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="257.89mm" wi="165.10mm" file="US20230004864A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="254.76mm" wi="171.53mm" file="US20230004864A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="259.08mm" wi="167.30mm" file="US20230004864A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="233.51mm" wi="154.26mm" file="US20230004864A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="241.98mm" wi="153.84mm" file="US20230004864A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="254.76mm" wi="170.26mm" file="US20230004864A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="259.08mm" wi="168.91mm" file="US20230004864A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="239.10mm" wi="172.38mm" file="US20230004864A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="256.96mm" wi="167.39mm" file="US20230004864A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="259.33mm" wi="167.39mm" file="US20230004864A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="254.76mm" wi="172.21mm" file="US20230004864A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="261.11mm" wi="167.39mm" file="US20230004864A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="235.71mm" wi="154.18mm" file="US20230004864A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="239.18mm" wi="154.18mm" file="US20230004864A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">The evolution of wireless communication systems oftentimes stems from a demand for data throughput. As one example, the demand for data increases as more and more devices gain access to wireless communication systems. Evolving devices also execute data-intensive applications that utilize more data than traditional applications, such as data-intensive streaming-video applications, data-intensive social media applications, data-intensive audio services, etc. This increased demand can, at times, deplete the data resources of a wireless communication system. Thus, to accommodate increased data usage, evolving wireless communication systems utilize increasingly complex architectures to provide more data throughput relative to legacy wireless communication systems.</p><p id="p-0003" num="0002">As one example, fifth generation (5G) standards and technologies transmit data using higher frequency ranges, such as the above-6 Gigahertz (GHz) band (e.g., 5G millimeter wave (mmW) technologies) to increase data capacity. However, transmitting and recovering information using these higher frequency ranges poses challenges. To illustrate, higher frequency signals are more susceptible to multipath fading, scattering, atmospheric absorption, diffraction, interference, and so forth, relative to lower-frequency signals. These signal distortions oftentimes lead to errors when recovering the information at a receiver. As another example, hardware capable of transmitting, receiving, routing, and/or otherwise using these higher frequencies can be complex and expensive, which increases the processing costs in a wirelessly-networked device.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0004" num="0003">This document describes techniques and apparatuses for end-to-end (E2E) machine learning for wireless systems. Aspects describe an end-to-end machine-learning controller that obtains capabilities of device(s) associated with end-to-end communications in a wireless network. In some implementations, the end-to-end machine-learning controller obtains machine-learning (ML) capabilities for at least one device. In response to obtaining the capabilities, the end-to-end machine-learning controller determines an end-to-end machine-learning configuration (E2E ML configuration) based on the capabilities of the device(s) and communicates the end-to-end machine-learning configuration to the one or more devices. Alternately or additionally, the end-to-end machine-learning controller directs each of these devices to form a respective deep neural network based on the end-to-end machine-learning configuration and to process the end-to-end communications using the respective deep neural network.</p><p id="p-0005" num="0004">One or more aspects describe a user equipment (UE) transmitting one or more capabilities supported by the user equipment, such as to an end-to-end machine-learning controller implemented at a network entity. The UE receives a neural network formation configuration based on an end-to-end machine-learning configuration for the processing information exchanged through the end-to-end communication. The UE forms a deep neural network using the neural network formation configuration, and then processes information exchanged through the end-to-end communication using the deep neural network.</p><p id="p-0006" num="0005">The details of one or more implementations of end-to-end machine learning for wireless networks are set forth in the accompanying drawings and the following description. Other features and advantages will be apparent from the description and drawings, and from the claims. This summary is provided to introduce subject matter that is further described in the Detailed Description and Drawings. Accordingly, this summary should not be considered to describe essential features nor used to limit the scope of the claimed subject matter.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0007" num="0006">The details of one or more aspects of end-to-end machine learning for wireless networks are described below. The use of the same reference numbers in different instances in the description and the figures indicate similar elements:</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example environment in which various aspects of end-to-end machine learning for wireless networks can be implemented.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example device diagram of devices that can implement various aspects of end-to-end machine learning for wireless networks.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example device diagram of a device that can implement various aspects of end-to-end machine learning for wireless networks.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example machine-learning module that can implement various aspects of end-to-end machine learning for wireless networks.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates example block diagrams of processing chains utilized by devices to process communications transmitted over a wireless communication system.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example operating environment in which multiple deep neural networks are utilized in a wireless communication system.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example transaction diagram between various devices for configuring a neural network using a neural network formation configuration.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an example transaction diagram between various devices for updating a neural network using a neural network formation configuration.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIGS. <b>9</b>-<b>1</b> and <b>9</b>-<b>2</b></figref> illustrate an example transaction diagram between various devices for configuring a neural network using a neural network formation configuration.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an example method for configuring a neural network for processing communications transmitted over a wireless communication system.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates an example method for forming a neural network based on a neural network formation configuration.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates an example of generating multiple neural network formation configurations.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates an example transaction diagram between various devices for communicating neural network formation configurations.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates an example transaction diagram between various network entities for communicating neural network formation configurations.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates an example transaction diagram between various network entities for communicating neural network formation configurations.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates an example method for communicating neural network formation configurations over a wireless communication system.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates an example method for communicating neural network formation configurations over a wireless communication system.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>18</b></figref> illustrates an example environment that demonstrates example variations between QoS flows.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>19</b></figref> illustrates an example environment in which various aspects of end-to-end machine learning for wireless networks can be implemented.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>20</b></figref> illustrates an example environment in which various aspects of end-to-end machine learning for wireless networks can be implemented.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>21</b></figref> illustrates an example transaction diagram between various devices in accordance with various implementations of end-to-end machine learning for wireless networks.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>22</b></figref> illustrates an example transaction diagram between various devices in accordance with various implementations of end-to-end machine learning for wireless networks.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>23</b></figref> illustrates an example transaction diagram between various devices in accordance with various implementations of end-to-end machine learning for wireless networks.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>24</b></figref> illustrates an example transaction diagram between various devices in accordance with various implementations of end-to-end machine learning for wireless networks.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>25</b></figref> illustrates an example method for end-to-end machine learning for wireless networks.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>26</b></figref> illustrates an example method for end-to-end machine learning for wireless networks.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0034" num="0033">In conventional wireless communication systems, transmitter and receiver processing chains include complex functionality. For instance, a channel estimation block in the processing chain estimates or predicts how a transmission environment distorts a signal propagating through the transmission environment. As another example, channel equalizer blocks reverse the distortions identified by the channel estimation block from the signal. These complex functions oftentimes become more complicated when processing higher frequency ranges, such as 5G mmW signals that are at or around the 6 GHz range. For instance, transmission environments add more distortion to the higher frequency ranges relative to lower frequency ranges, thus making information recovery more complex. Further, hardware capable of processing and routing the higher frequency ranges oftentimes adds increased costs and complex physical constraints.</p><p id="p-0035" num="0034">DNNs provide solutions to complex processing, such as the complex functionality used in a wireless communication system. By training a DNN on transmitter and/or receiver processing chain operations, the DNN can replace the conventional complex functionality in a variety of ways, such as by replacing some or all of the conventional processing blocks used in end-to-end processing of wireless communication signals, replacing individual processing chain blocks, etc. Dynamic reconfiguration of a DNN, such as by modifying various parameter configurations (e.g., coefficients, layer connections, kernel sizes) also provides an ability to adapt to changing operating conditions.</p><p id="p-0036" num="0035">This document describes aspects of end-to-end machine learning for wireless networks, which allows the system to process communications, and dynamically reconfigure DNNs used in E2E communications as operating conditions change. Aspects can be implemented by network entities that operate in the wireless communication system (e.g., a base station, a core network server, a user equipment).</p><p id="p-0037" num="0036">In implementations, a network entity obtains capabilities of device(s) associated with E2E communications in a wireless network. In some implementations, the network entity requests the capabilities, such as by requesting UE capabilities, machine-learning capabilities (ML capabilities), base station (BS) capabilities, and so forth, from one or more devices participating in the E2E communications. In response to obtaining the capabilities, the network entity determines an end-to-end machine-learning configuration (E2E ML configuration) based on the received capabilities. After determining the E2E ML configuration, the network entity communicates the E2E ML configuration to the devices, such as by communicating a first neural network (NN) format configuration to a first device, a second NN format configuration to a second device, etc. The network entity then directs devices participating in the E2E communications to form a respective deep neural network based on the E2E ML configuration (e.g., a first DNN formed using the first NN format configuration, a second DNN formed using the second NN format configuration) and to process the E2E communications using the respective DNNs.</p><p id="p-0038" num="0037">In implementations, a UE transmits one or more capabilities supported by the user equipment to a network entity. For example, the UE transmits processing capabilities, memory capabilities, ML-support capabilities, and so forth. Afterward, the UE receives a NN formation configuration associated with an E2E ML configuration. The UE then forms a DNN using the NN formation configuration and processes UE-side communications (e.g., UE portion(s) of the E2E communications) using the DNN.</p><p id="p-0039" num="0038">Determining an E2E ML configuration, where the E2E ML configuration can be partitioned between multiple devices enables the network entity to select an ML configuration that improves communication exchanges in the system. For example, in some implementations, the network entity partitions the E2E ML configuration based on device capabilities, such that the network entity directs devices with less processing resources and/or memory to form DNNs with less processing (e.g., less data, less memory, less CPU cycles, less layers) relative to devices with more processing resources and/or memory. In other words, the network entity determines an E2E ML configuration that improves an overall performance of E2E communications exchanged between devices (e.g., lower bit errors, improved signal quality, improved latency) and partitions the E2E ML configuration based upon device resources to allocate and/or appropriate the processing in an efficient manner.</p><p id="p-0040" num="0039">The phrases &#x201c;transmitted over,&#x201d; &#x201c;communications exchanged,&#x201d; and &#x201c;communications associated with&#x201d; include generating communications to be transmitted over the wireless communication system (e.g. processing pre-transmission communications) and/or processing communications received over the wireless communication system. Thus, &#x201c;processing communications transmitted over the wireless communication system,&#x201d; &#x201c;communications exchanged over the wireless communication system,&#x201d; as well as &#x201c;communications associated with the wireless communication system&#x201d; include generating the transmissions (e.g., pre-transmission processing), processing received transmissions, or any combination thereof.</p><heading id="h-0005" level="1">Example Environment</heading><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example environment <b>100</b> which includes a user equipment <b>110</b> (UE <b>110</b>) that can communicate with base stations <b>120</b> (illustrated as base stations <b>121</b> and <b>122</b>) through one or more wireless communication links <b>130</b> (wireless link <b>130</b>), illustrated as wireless links <b>131</b> and <b>132</b>. For simplicity, the UE <b>110</b> is implemented as a smartphone but may be implemented as any suitable computing or electronic device, such as a mobile communication device, modem, cellular phone, gaming device, navigation device, media device, laptop computer, desktop computer, tablet computer, smart appliance, vehicle-based communication system, or an Internet-of-Things (IoT) device such as a sensor or an actuator. The base stations <b>120</b> (e.g., an Evolved Universal Terrestrial Radio Access Network Node B, E-UTRAN Node B, evolved Node B, eNodeB, eNB, Next Generation Node B, gNode B, gNB, ng-eNB, or the like) may be implemented in a macrocell, microcell, small cell, picocell, and the like, or any combination thereof.</p><p id="p-0042" num="0041">The base stations <b>120</b> communicate with the user equipment <b>110</b> using the wireless links <b>131</b> and <b>132</b>, which may be implemented as any suitable type of wireless link. The wireless links <b>131</b> and <b>132</b> include control and data communication, such as downlink of data and control information communicated from the base stations <b>120</b> to the user equipment <b>110</b>, uplink of other data and control information communicated from the user equipment <b>110</b> to the base stations <b>120</b>, or both. The wireless links <b>130</b> may include one or more wireless links (e.g., radio links) or bearers implemented using any suitable communication protocol or standard, or combination of communication protocols or standards, such as 3rd Generation Partnership Project Long-Term Evolution (3GPP LTE), Fifth Generation New Radio (5G NR), and so forth. Multiple wireless links <b>130</b> may be aggregated in a carrier aggregation to provide a higher data rate for the UE <b>110</b>. Multiple wireless links <b>130</b> from multiple base stations <b>120</b> may be configured for Coordinated Multipoint (CoMP) communication with the UE <b>110</b>.</p><p id="p-0043" num="0042">The base stations <b>120</b> are collectively a Radio Access Network <b>140</b> (e.g., RAN, Evolved Universal Terrestrial Radio Access Network, E-UTRAN, 5G NR RAN or NR RAN). The base stations <b>121</b> and <b>122</b> in the RAN <b>140</b> are connected to a core network <b>150</b>. The base stations <b>121</b> and <b>122</b> connect, at <b>102</b> and <b>104</b> respectively, to the core network <b>150</b> through an NG2 interface for control-plane signaling and using an NG3 interface for user-plane data communications when connecting to a 5G core network, or using an S1 interface for control-plane signaling and user-plane data communications when connecting to an Evolved Packet Core (EPC) network. The base stations <b>121</b> and <b>122</b> can communicate using an Xn Application Protocol (XnAP) through an Xn interface, or using an X2 Application Protocol (X2AP) through an X2 interface, at <b>106</b>, to exchange user-plane and control-plane data. The user equipment <b>110</b> may connect, via the core network <b>150</b>, to public networks, such as the Internet <b>160</b> to interact with a remote service <b>170</b>. The remote service <b>170</b> represents the computing, communication, and storage devices used to provide any of a multitude of services including interactive voice or video communication, file transfer, streaming voice or video, and other technical services implemented in any manner such as voice calls, video calls, website access, messaging services (e.g., text messaging or multi-media messaging), photo file transfer, enterprise software applications, social media applications, videogaming, streaming video services, and podcasts.</p><heading id="h-0006" level="1">Example Devices</heading><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example device diagram <b>200</b> of the user equipment <b>110</b> and one of the base stations <b>120</b>. <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example device diagram <b>300</b> of a core network server <b>302</b>. The user equipment <b>110</b>, the base station <b>120</b>, and/or the core network server <b>302</b> may include additional functions and interfaces that are omitted from <figref idref="DRAWINGS">FIG. <b>2</b></figref> or <figref idref="DRAWINGS">FIG. <b>3</b></figref> for the sake of clarity.</p><p id="p-0045" num="0044">The user equipment <b>110</b> includes antennas <b>202</b>, a radio frequency front end <b>204</b> (RF front end <b>204</b>), a wireless transceiver (e.g., an LTE transceiver <b>206</b>, and/or a 5G NR transceiver <b>208</b>) for communicating with the base station <b>120</b> in the RAN <b>140</b>. The RF front end <b>204</b> of the user equipment <b>110</b> can couple or connect the LTE transceiver <b>206</b>, and the 5G NR transceiver <b>208</b> to the antennas <b>202</b> to facilitate various types of wireless communication. The antennas <b>202</b> of the user equipment <b>110</b> may include an array of multiple antennas that are configured similar to or differently from each other. The antennas <b>202</b> and the RF front end <b>204</b> can be tuned to, and/or be tunable to, one or more frequency bands defined by the 3GPP LTE and 5G NR communication standards and implemented by the LTE transceiver <b>206</b>, and/or the 5G NR transceiver <b>208</b>. Additionally, the antennas <b>202</b>, the RF front end <b>204</b>, the LTE transceiver <b>206</b>, and/or the 5G NR transceiver <b>208</b> may be configured to support beamforming for the transmission and reception of communications with the base station <b>120</b>. By way of example and not limitation, the antennas <b>202</b> and the RF front end <b>204</b> can be implemented for operation in sub-gigahertz bands, sub-6 GHz bands, and/or above 6 GHz bands that are defined by the 3GPP LTE and 5G NR communication standards.</p><p id="p-0046" num="0045">The user equipment <b>110</b> also includes processor(s) <b>210</b> and computer-readable storage media <b>212</b> (CRM <b>212</b>). The processor <b>210</b> may be a single core processor or a multiple core processor composed of a variety of materials, such as silicon, polysilicon, high-K dielectric, copper, and so on. The computer-readable storage media described herein excludes propagating signals. CRM <b>212</b> may include any suitable memory or storage device such as random-access memory (RAM), static RAM (SRAM), dynamic RAM (DRAM), non-volatile RAM (NVRAM), read-only memory (ROM), or Flash memory useable to store device data <b>214</b> of the user equipment <b>110</b>. The device data <b>214</b> includes user data, multimedia data, beamforming codebooks, applications, neural network tables, and/or an operating system of the user equipment <b>110</b>, which are executable by processor(s) <b>210</b> to enable user-plane communication, control-plane signaling, and user interaction with the user equipment <b>110</b>.</p><p id="p-0047" num="0046">In some implementations, the computer-readable storage media <b>212</b> includes a neural network table <b>216</b> that stores various architecture and/or parameter configurations that form a neural network, such as, by way of example and not of limitation, parameters that specify a fully connected layer neural network architecture, a convolutional layer neural network architecture, a recurrent neural network layer, a number of connected hidden neural network layers, an input layer architecture, an output layer architecture, a number of nodes utilized by the neural network, coefficients (e.g., weights and biases) utilized by the neural network, kernel parameters, a number of filters utilized by the neural network, strides/pooling configurations utilized by the neural network, an activation function of each neural network layer, interconnections between neural network layers, neural network layers to skip, and so forth. Accordingly, the neural network table <b>216</b> includes any combination of NN formation configuration elements (e.g., architecture and/or parameter configurations) that can be used to create a NN formation configuration (e.g., a combination of one or more NN formation configuration elements) that defines and/or forms a DNN. In some implementations, a single index value of the neural network table <b>216</b> maps to a single NN formation configuration element (e.g., a 1:1 correspondence). Alternately or additionally, a single index value of the neural network table <b>216</b> maps to a NN formation configuration (e.g., a combination of NN formation configuration elements). In some implementations, the neural network table includes input characteristics for each NN formation configuration element and/or NN formation configuration, where the input characteristics describe properties about the training data used to generate the NN formation configuration element and/or NN formation configuration as further described.</p><p id="p-0048" num="0047">In some implementations, the CRM <b>212</b> may also include a user equipment neural network manager <b>218</b> (UE neural network manager <b>218</b>). Alternately or additionally, the UE neural network manager <b>218</b> may be implemented in whole or part as hardware logic or circuitry integrated with or separate from other components of the user equipment <b>110</b>. The UE neural network manager <b>218</b> accesses the neural network table <b>216</b>, such as by way of an index value, and forms a DNN using the NN formation configuration elements specified by a NN formation configuration. In implementations, UE neural network manager forms multiple DNNs to process wireless communications (e.g., downlink communications and/or uplink communications exchanged with the base station <b>120</b>).</p><p id="p-0049" num="0048">The device diagram for the base station <b>120</b>, shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, includes a single network node (e.g., a gNode B). The functionality of the base station <b>120</b> may be distributed across multiple network nodes or devices and may be distributed in any fashion suitable to perform the functions described herein. The base station <b>120</b> include antennas <b>252</b>, a radio frequency front end <b>254</b> (RF front end <b>254</b>), one or more wireless transceivers (e.g. one or more LTE transceivers <b>256</b>, and/or one or more 5G NR transceivers <b>258</b>) for communicating with the UE <b>110</b>. The RF front end <b>254</b> of the base station <b>120</b> can couple or connect the LTE transceivers <b>256</b> and the 5G NR transceivers <b>258</b> to the antennas <b>252</b> to facilitate various types of wireless communication. The antennas <b>252</b> of the base station <b>120</b> may include an array of multiple antennas that are configured similar to, or different from, each other. The antennas <b>252</b> and the RF front end <b>254</b> can be tuned to, and/or be tunable to, one or more frequency band defined by the 3GPP LTE and 5G NR communication standards, and implemented by the LTE transceivers <b>256</b>, and/or the 5G NR transceivers <b>258</b>. Additionally, the antennas <b>252</b>, the RF front end <b>254</b>, the LTE transceivers <b>256</b>, and/or the 5G NR transceivers <b>258</b> may be configured to support beamforming, such as Massive-MIMO, for the transmission and reception of communications with the UE <b>110</b>.</p><p id="p-0050" num="0049">The base station <b>120</b> also include processor(s) <b>260</b> and computer-readable storage media <b>262</b> (CRM <b>262</b>). The processor <b>260</b> may be a single core processor or a multiple core processor composed of a variety of materials, such as silicon, polysilicon, high-K dielectric, copper, and so on. CRM <b>262</b> may include any suitable memory or storage device such as random-access memory (RAM), static RAM (SRAM), dynamic RAM (DRAM), non-volatile RAM (NVRAM), read-only memory (ROM), or Flash memory useable to store device data <b>264</b> of the base station <b>120</b>. The device data <b>264</b> includes network scheduling data, radio resource management data, beamforming codebooks, applications, and/or an operating system of the base station <b>120</b>, which are executable by processor(s) <b>260</b> to enable communication with the user equipment <b>110</b>.</p><p id="p-0051" num="0050">CRM <b>262</b> also includes a base station manager <b>266</b>. Alternately or additionally, the base station manager <b>266</b> may be implemented in whole or part as hardware logic or circuitry integrated with or separate from other components of the base station <b>120</b>. In at least some aspects, the base station manager <b>266</b> configures the LTE transceivers <b>256</b> and the 5G NR transceivers <b>258</b> for communication with the user equipment <b>110</b>, as well as communication with a core network, such as the core network <b>150</b>.</p><p id="p-0052" num="0051">CRM <b>262</b> also includes a base station neural network manager <b>268</b> (BS neural network manager <b>268</b>). Alternately or additionally, the BS neural network manager <b>268</b> may be implemented in whole or part as hardware logic or circuitry integrated with or separate from other components of the base station <b>120</b>. In at least some aspects, the BS neural network manager <b>268</b> selects the NN formation configurations utilized by the base station <b>120</b> and/or UE <b>110</b> to configure deep neural networks for processing wireless communications, such as by selecting a combination of NN formation configuration elements. In some implementations, the BS neural network manager receives feedback from the UE <b>110</b>, and selects the neural network formation configuration based on the feedback. Alternately or additionally, the BS neural network manager <b>268</b> receives neural network formation configuration directions from core network <b>150</b> elements through a core network interface <b>276</b> or an inter-base station interface <b>274</b> and forwards the neural network formation configuration directions to UE <b>110</b>.</p><p id="p-0053" num="0052">CRM <b>262</b> includes training module <b>270</b> and neural network table <b>272</b>. In implementations, the base station <b>120</b> manage and deploy NN formation configurations to UE <b>110</b>. Alternately or additionally, the base station <b>120</b> maintain the neural network table <b>272</b>. The training module <b>270</b> teaches and/or trains DNNs using known input data. For instance, the training module <b>270</b> trains DNN(s) for different purposes, such as processing communications transmitted over a wireless communication system (e.g., encoding downlink communications, modulating downlink communications, demodulating downlink communications, decoding downlink communications, encoding uplink communications, modulating uplink communications, demodulating uplink communications, decoding uplink communications). This includes training the DNN(s) offline (e.g., while the DNN is not actively engaged in processing the communications) and/or online (e.g., while the DNN is actively engaged in processing the communications).</p><p id="p-0054" num="0053">In implementations, the training module <b>270</b> extracts learned parameter configurations from the DNN to identify the NN formation configuration elements and/or NN formation configuration, and then adds and/or updates the NN formation configuration elements and/or NN formation configuration in the neural network table <b>272</b>. The extracted parameter configurations include any combination of information that defines the behavior of a neural network, such as node connections, coefficients, active layers, weights, biases, pooling, etc.</p><p id="p-0055" num="0054">The neural network table <b>272</b> stores multiple different NN formation configuration elements and/or NN formation configurations generated using the training module <b>270</b>. In some implementations, the neural network table includes input characteristics for each NN formation configuration element and/or NN formation configuration, where the input characteristics describe properties about the training data used to generate the NN formation configuration element and/or NN formation configuration. For instance, the input characteristics includes, by way of example and not of limitation, power information, signal-to-interference-plus-noise ratio (SINR) information, channel quality indicator (CQI) information, channel state information (CSI), Doppler feedback, frequency bands, BLock Error Rate (BLER), Quality of Service (QoS), Hybrid Automatic Repeat reQuest (HARQ) information (e.g., first transmission error rate, second transmission error rate, maximum retransmissions), latency, Radio Link Control (RLC), Automatic Repeat reQuest (ARQ) metrics, received signal strength (RSS), uplink SINR, timing measurements, error metrics, UE capabilities, BS capabilities, power mode, Internet Protocol (IP) layer throughput, end2end latency, end2end packet loss ratio, etc. Accordingly, the input characteristics include, at times, Layer 1, Layer 2, and/or Layer 3 metrics. In some implementations, a single index value of the neural network table <b>272</b> maps to a single NN formation configuration element (e.g., a 1:1 correspondence). Alternately or additionally, a single index value of the neural network table <b>272</b> maps to a NN formation configuration (e.g., a combination of NN formation configuration elements).</p><p id="p-0056" num="0055">In implementations, the base station <b>120</b> synchronizes the neural network table <b>272</b> with the neural network table <b>216</b> such that the NN formation configuration elements and/or input characteristics stored in one neural network table is replicated in the second neural network table. Alternately or additionally, the base station <b>120</b> synchronizes the neural network table <b>272</b> with the neural network table <b>216</b> such that the NN formation configuration elements and/or input characteristics stored in one neural network table represent complementary functionality in the second neural network table (e.g., NN formation configuration elements for transmitter path processing in the first neural network table, NN formation configuration elements for receiver path processing in the second neural network table).</p><p id="p-0057" num="0056">The base station <b>120</b> also include an inter-base station interface <b>274</b>, such as an Xn and/or X2 interface, which the base station manager <b>266</b> configures to exchange user-plane, control-plane, and other information between other base station <b>120</b>, to manage the communication of the base station <b>120</b> with the user equipment <b>110</b>. The base station <b>120</b> include a core network interface <b>276</b> that the base station manager <b>266</b> configures to exchange user-plane, control-plane, and other information with core network functions and/or entities.</p><p id="p-0058" num="0057">In <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the core network server <b>302</b> may provide all or part of a function, entity, service, and/or gateway in the core network <b>150</b>. Each function, entity, service, and/or gateway in the core network <b>150</b> may be provided as a service in the core network <b>150</b>, distributed across multiple servers, or embodied on a dedicated server. For example, the core network server <b>302</b> may provide the all or a portion of the services or functions of a User Plane Function (UPF), an Access and Mobility Management Function (AMF), a Serving Gateway (S-GW), a Packet Data Network Gateway (P-GW), a Mobility Management Entity (MME), an Evolved Packet Data Gateway (ePDG), and so forth. The core network server <b>302</b> is illustrated as being embodied on a single server that includes processor(s) <b>304</b> and computer-readable storage media <b>306</b> (CRM <b>306</b>). The processor <b>304</b> may be a single core processor or a multiple core processor composed of a variety of materials, such as silicon, polysilicon, high-K dielectric, copper, and so on. CRM <b>306</b> may include any suitable memory or storage device such as random-access memory (RAM), static RAM (SRAM), dynamic RAM (DRAM), non-volatile RAM (NVRAM), read-only memory (ROM), hard disk drives, or Flash memory useful to store device data <b>308</b> of the core network server <b>302</b>. The device data <b>308</b> includes data to support a core network function or entity, and/or an operating system of the core network server <b>302</b>, which are executable by processor(s) <b>304</b>.</p><p id="p-0059" num="0058">CRM <b>306</b> also includes one or more core network applications <b>310</b>, which, in one implementation, is embodied on CRM <b>306</b> (as shown). The one or more core network applications <b>310</b> may implement the functionality such as UPF, AMF, S-GW, P-GW, MME, ePDG, and so forth. Alternately or additionally, the one or more core network applications <b>310</b> may be implemented in whole or part as hardware logic or circuitry integrated with or separate from other components of the core network server <b>302</b>.</p><p id="p-0060" num="0059">CRM <b>306</b> also includes a core network neural network manager <b>312</b> that manages NN formation configurations used to process communications exchanged between UE <b>110</b> and the base stations <b>120</b>. In some implementations, the core network neural network manager <b>312</b> analyzes various parameters, such as current signal channel conditions (e.g., as reported by base stations <b>120</b>, as reported by other wireless access points, as reported by UEs <b>110</b> (via base stations or other wireless access points)), capabilities at base stations <b>120</b> (e.g., antenna configurations, cell configurations, MIMO capabilities, radio capabilities, processing capabilities), capabilities of UE <b>110</b> (e.g., antenna configurations, MIMO capabilities, radio capabilities, processing capabilities), and so forth. For example, the base stations <b>120</b> obtain the various parameters during the communications with the UE and forward the parameters to the core network neural network manager <b>312</b>. The core network neural network manager selects, based on these parameters, a NN formation configuration that improves the accuracy of a DNN processing the communications. Improving the accuracy signifies an improved accuracy in the output, such as lower bit errors, generated by the neural network relative to a neural network configured with another NN formation configuration. The core network neural network manager <b>312</b> then communicates the selected NN formation configuration to the base stations <b>120</b> and/or the UE <b>110</b>. In implementations, the core network neural network manager <b>312</b> receives UE and/or BS feedback from the base station <b>120</b> and selects an updated NN formation configuration based on the feedback.</p><p id="p-0061" num="0060">CRM <b>306</b> includes training module <b>314</b> and neural network table <b>316</b>. In implementations, the core network server <b>302</b> manages and deploys NN formation configurations to multiple devices in a wireless communication system, such as UEs <b>110</b> and base stations <b>120</b>. Alternately or additionally, the core network server maintains the neural network table <b>316</b> outside of the CRM <b>306</b>. The training module <b>314</b> teaches and/or trains DNNs using known input data. For instance, the training module <b>314</b> trains DNN(s) to process different types of pilot communications transmitted over a wireless communication system. This includes training the DNN(s) offline and/or online. In implementations, the training module <b>314</b> extracts a learned NN formation configuration and/or learned NN formation configuration elements from the DNN and stores the learned NN formation configuration elements in the neural network table <b>316</b>. Thus, a NN formation configuration includes any combination of architecture configurations (e.g., node connections, layer connections) and/or parameter configurations (e.g., weights, biases, pooling) that define or influence the behavior of a DNN. In some implementations, a single index value of the neural network table <b>316</b> maps to a single NN formation configuration element (e.g., a 1:1 correspondence). Alternately or additionally, a single index value of the neural network table <b>316</b> maps to a NN formation configuration (e.g., a combination of NN formation configuration elements).</p><p id="p-0062" num="0061">In some implementations, the training module <b>314</b> of the core network neural network manager <b>312</b> generates complementary NN formation configurations and/or NN formation configuration elements to those stored in the neural network table <b>216</b> at the UE <b>110</b> and/or the neural network table <b>272</b> at the base station <b>121</b>. As one example, the training module <b>314</b> generates neural network table <b>316</b> with NN formation configurations and/or NN formation configuration elements that have a high variation in the architecture and/or parameter configurations relative to medium and/or low variations used to generate the neural network table <b>272</b> and/or the neural network table <b>216</b>. For instance, the NN formation configurations and/or NN formation configuration elements generated by the training module <b>314</b> correspond to fully connected layers, a full kernel size, frequent sampling and/or pooling, high weighting accuracy, and so forth. Accordingly, the neural network table <b>316</b> includes, at times, high accuracy neural networks at the trade-off of increased processing complexity and/or time.</p><p id="p-0063" num="0062">The NN formation configurations and/or NN formation configuration elements generated by the training module <b>270</b> have, at times, more fixed architecture and/or parameter configurations (e.g., fixed connection layers, fixed kernel size, etc.), and less variation, relative to those generated by the training module <b>314</b>. The training module <b>270</b>, for example, generates streamlined NN formation configurations (e.g., faster computation times, less data processing), relative to those generated by the training module <b>314</b>, to optimize or improve a performance of end2end network communications at the base station <b>121</b> and/or the UE <b>110</b>. Alternately or additionally, the NN formation configurations and/or NN formation configuration elements stored at the neural network table <b>216</b> at the UE <b>110</b> include more fixed architecture and/or parameter configurations, relative to those stored in the neural network table <b>316</b> and/or the neural network table <b>272</b>, that reduce requirements (e.g., computation speed, less data processing points, less computations, less power consumption, etc.) at the UE <b>110</b> relative to the base station <b>121</b> and/or the core network server <b>302</b>. In implementations, the variations in fixed (or flexible) architecture and/or parameter configurations at each neural network are based on the processing resources (e.g., processing capabilities, memory constraints, quantization constraints (e.g., 8-bit vs. 16-bit), fixed-point vs. floating point computations, floating point operations per second (FLOPS), power availability) of the devices targeted to form the corresponding DNNs. Thus, UEs or access points with less processing resources relative to a core network server or base station receive NN formation configurations optimized for the available processing resources.</p><p id="p-0064" num="0063">The neural network table <b>316</b> stores multiple different NN formation configuration elements generated using the training module <b>314</b>. In some implementations, the neural network table includes input characteristics for each NN formation configuration element and/or NN formation configuration, where the input characteristics describe properties about the training data used to generate the NN formation configuration. For instance, the input characteristics can include power information, SINR information, CQI, CSI, Doppler feedback, RSS, error metrics, minimum end-to-end (E2E) latency, desired E2E latency, E2E QoS, E2E throughput, E2E packet loss ratio, cost of service, etc.</p><p id="p-0065" num="0064">CRM <b>302</b> also includes an end-to-end machine-learning controller <b>318</b> (E2E ML controller <b>318</b>). The E2E ML controller <b>318</b> determines an end-to-end machine-learning configuration (E2E ML configuration) for processing information exchanged through an E2E communication, such as a QoS flow. In implementations, the E2E ML controller analyzes any combination of ML capabilities (e.g., supported ML architectures, supported number of layers, available processing power, memory limitations, available power budget, fixed-point processing vs. floating point processing, maximum kernel size capability, computation capability) of devices participating in the E2E communication. Alternately or additionally, the E2E ML controller analyzes any combination of QoS requirements, QoS parameters, and/or QoS characteristics to determine an E2E ML configuration that satisfies the associated requirements, parameters, and/or characteristics. In some implementations, the E2E ML controller obtains metrics that characterize a current operating environment and analyzes the current operating environment to determine the E2E ML configuration. This includes determining an E2E ML configuration that includes an architecture configuration in combination with parameter configuration(s) that define a DNN or determining an E2E ML configuration that simply includes parameter configurations used to update the DNN.</p><p id="p-0066" num="0065">In determining the E2E ML configuration, the E2E ML controller sometimes determines partitions to the E2E ML configuration that distribute the processing functionality associated with the E2E ML configuration across multiple devices. For clarity, <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates the end-to-end machine-learning controller <b>318</b> as separate from the core network neural network manager <b>312</b>, but in alternate or additional implementations, the core network neural network manager <b>312</b> includes functionality performed by the end-to-end machine-learning controller <b>318</b> or vice versa. Further, while <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates the core network server <b>302</b> implementing the E2E ML controller <b>318</b>, alternate or additional devices can implement the E2E ML controller, such as the base station <b>120</b> and/or other network elements.</p><p id="p-0067" num="0066">The core network server <b>302</b> also includes a core network interface <b>320</b> for communication of user-plane, control-plane, and other information with the other functions or entities in the core network <b>150</b>, base stations <b>120</b>, or UE <b>110</b>. In implementations, the core network server <b>302</b> communicates NN formation configurations to the base station <b>120</b> using the core network interface <b>320</b>. The core network server <b>302</b> alternately or additionally receives feedback from the base stations <b>120</b> and/or the UE <b>110</b>, by way of the base stations <b>120</b>, using the core network interface <b>320</b>.</p><p id="p-0068" num="0067">Having described an example environment and example devices that can be utilized for neural network formation configuration feedback in wireless communications, consider now a discussion of configurable machine-learning modules that is in accordance with one or more implementations.</p><p id="p-0069" num="0068">Configurable Machine-Learning Modules</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example machine-learning module <b>400</b>. The machine-learning module <b>400</b> implements a set of adaptive algorithms that learn and identify patterns within data. The machine-learning module <b>400</b> can be implemented using any combination of software, hardware, and/or firmware.</p><p id="p-0071" num="0070">In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the machine-learning module <b>400</b> includes a deep neural network <b>402</b> (DNN <b>402</b>) with groups of connected nodes (e.g., neurons and/or perceptrons) that are organized into three or more layers. The nodes between layers are configurable in a variety of ways, such as a partially-connected configuration where a first subset of nodes in a first layer are connected with a second subset of nodes in a second layer, a fully-connected configuration where each node in a first layer are connected to each node in a second layer, etc. A neuron processes input data to produce a continuous output value, such as any real number between 0 and 1. In some cases, the output value indicates how close the input data is to a desired category. A perceptron performs linear classifications on the input data, such as a binary classification. The nodes, whether neurons or perceptrons, can use a variety of algorithms to generate output information based upon adaptive learning. Using the DNN, the machine-learning module <b>400</b> performs a variety of different types of analysis, including single linear regression, multiple linear regression, logistic regression, step-wise regression, binary classification, multiclass classification, multi-variate adaptive regression splines, locally estimated scatterplot smoothing, and so forth.</p><p id="p-0072" num="0071">In some implementations, the machine-learning module <b>400</b> adaptively learns based on supervised learning. In supervised learning, the machine-learning module receives various types of input data as training data. The machine-learning module processes the training data to learn how to map the input to a desired output. As one example, the machine-learning module <b>400</b> receives digital samples of a signal as input data and learns how to map the signal samples to binary data that reflects information embedded within the signal. As another example, the machine-learning module <b>400</b> receives binary data as input data and learns how to map the binary data to digital samples of a signal with the binary data embedded within the signal. During a training procedure, the machine-learning module <b>400</b> uses labeled or known data as an input to the DNN. The DNN analyzes the input using the nodes and generates a corresponding output. The machine-learning module <b>400</b> compares the corresponding output to truth data and adapts the algorithms implemented by the nodes to improve the accuracy of the output data. Afterwards, the DNN applies the adapted algorithms to unlabeled input data to generate corresponding output data.</p><p id="p-0073" num="0072">The machine-learning module <b>400</b> uses statistical analyses and/or adaptive learning to map an input to an output. For instance, the machine-learning module <b>400</b> uses characteristics learned from training data to correlate an unknown input to an output that is statistically likely within a threshold range or value. This allows the machine-learning module <b>400</b> to receive complex input and identify a corresponding output. Some implementations train the machine-learning module <b>400</b> on characteristics of communications transmitted over a wireless communication system (e.g., time/frequency interleaving, time/frequency deinterleaving, convolutional encoding, convolutional decoding, power levels, channel equalization, inter-symbol interference, quadrature amplitude modulation/demodulation, frequency-division multiplexing/de-multiplexing, transmission channel characteristics). This allows the trained machine-learning module <b>400</b> to receive samples of a signal as an input, such as samples of a downlink signal received at a user equipment, and recover information from the downlink signal, such as the binary data embedded in the downlink signal.</p><p id="p-0074" num="0073">In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the DNN includes an input layer <b>404</b>, an output layer <b>406</b>, and one or more hidden layer(s) <b>408</b> that are positioned between the input layer and the output layer. Each layer has an arbitrary number of nodes, where the number of nodes between layers can be the same or different. In other words, input layer <b>404</b> can have a same number and/or different number of nodes as output layer <b>406</b>, output layer <b>406</b> can have a same number and/or different number of nodes than hidden layer(s) <b>408</b>, and so forth.</p><p id="p-0075" num="0074">Node <b>410</b> corresponds to one of several nodes included in input layer <b>404</b>, where the nodes perform independent computations from one another. As further described, a node receives input data, and processes the input data using algorithm(s) to produce output data. At times, the algorithm(s) include weights and/or coefficients that change based on adaptive learning. Thus, the weights and/or coefficients reflect information learned by the neural network. Each node can, in some cases, determine whether to pass the processed input data to the next node(s). To illustrate, after processing input data, node <b>410</b> can determine whether to pass the processed input data to node <b>412</b> and/or node <b>414</b> of hidden layer(s) <b>408</b>. Alternately or additionally, node <b>410</b> passes the processed input data to nodes based upon a layer connection architecture. This process can repeat throughout multiple layers until the DNN generates an output using the nodes of output layer <b>406</b>.</p><p id="p-0076" num="0075">A neural network can also employ a variety of architectures that determine what nodes within the neural network are connected, how data is advanced and/or retained in the neural network, what weights and coefficients are used to process the input data, how the data is processed, and so forth. These various factors collectively describe a NN formation configuration. To illustrate, a recurrent neural network, such as a long short-term memory (LSTM) neural network, forms cycles between node connections in order to retain information from a previous portion of an input data sequence. The recurrent neural network then uses the retained information for a subsequent portion of the input data sequence. As another example, a feed-forward neural network passes information to forward connections without forming cycles to retain information. While described in the context of node connections, it is to be appreciated that the NN formation configuration can include a variety of parameter configurations that influence how the neural network processes input data.</p><p id="p-0077" num="0076">A NN formation configuration of a neural network can be characterized by various architecture and/or parameter configurations. To illustrate, consider an example in which the DNN implements a convolutional neural network. Generally, a convolutional neural network corresponds to a type of DNN in which the layers process data using convolutional operations to filter the input data. Accordingly, the convolutional NN formation configuration can be characterized with, by way of example and not of limitation, pooling parameter(s), kernel parameter(s), weights, and/or layer parameter(s).</p><p id="p-0078" num="0077">A pooling parameter corresponds to a parameter that specifies pooling layers within the convolutional neural network that reduce the dimensions of the input data. To illustrate, a pooling layer can combine the output of nodes at a first layer into a node input at a second layer. Alternately or additionally, the pooling parameter specifies how and where in the layers of data processing the neural network pools data. A pooling parameter that indicates &#x201c;max pooling,&#x201d; for instance, configures the neural network to pool by selecting a maximum value from the grouping of data generated by the nodes of a first layer, and use the maximum value as the input into the single node of a second layer. A pooling parameter that indicates &#x201c;average pooling&#x201d; configures the neural network to generate an average value from the grouping of data generated by the nodes of the first layer and use the average value as the input to the single node of the second layer.</p><p id="p-0079" num="0078">A kernel parameter indicates a filter size (e.g., a width and height) to use in processing input data. Alternately or additionally, the kernel parameter specifies a type of kernel method used in filtering and processing the input data. A support vector machine, for instance, corresponds to a kernel method that uses regression analysis to identify and/or classify data. Other types of kernel methods include Gaussian processes, canonical correlation analysis, spectral clustering methods, and so forth. Accordingly, the kernel parameter can indicate a filter size and/or a type of kernel method to apply in the neural network.</p><p id="p-0080" num="0079">Weight parameters specify weights and biases used by the algorithms within the nodes to classify input data. In implementations, the weights and biases are learned parameter configurations, such as parameter configurations generated from training data.</p><p id="p-0081" num="0080">A layer parameter specifies layer connections and/or layer types, such as a fully-connected layer type that indicates to connect every node in a first layer (e.g., output layer <b>406</b>) to every node in a second layer (e.g., hidden layer(s) <b>408</b>), a partially-connected layer type that indicates which nodes in the first layer to disconnect from the second layer, an activation layer type that indicates which filters and/or layers to activate within the neural network, and so forth. Alternately or additionally, the layer parameter specifies types of node layers, such as a normalization layer type, a convolutional layer type, a pooling layer type, etc.</p><p id="p-0082" num="0081">While described in the context of pooling parameters, kernel parameters, weight parameters, and layer parameters, it is to be appreciated that other parameter configurations can be used to form a DNN without departing from the scope of the claimed subject matter. Accordingly, a NN formation configuration can include any other type of parameter that can be applied to a DNN that influences how the DNN processes input data to generate output data.</p><p id="p-0083" num="0082">Some implementations configure machine-learning module <b>400</b> based on a current operating environment. To illustrate, consider a machine-learning module trained to generate binary data from digital samples of a signal. A transmission environment oftentimes modifies the characteristics of a signal traveling through the environment. Transmission environments oftentimes change, which impacts how the environment modifies the signal. A first transmission environment, for instance, modifies a signal in a first manner, while a second transmission environment modifies the signal in a different manner than the first. These differences impact an accuracy of the output results generated by a machine-learning module. For instance, a neural network configured to process communications transmitted over the first transmission environment may generate errors when processing communications transmitted over the second transmission environment (e.g., bit errors that exceed a threshold value).</p><p id="p-0084" num="0083">Various implementations generate and store NN formation configurations and/or NN formation configuration elements (e.g., various architecture and/or parameter configurations) for different transmission environments. Base stations <b>120</b> and/or core network server <b>302</b>, for example, train the machine-learning module <b>400</b> using any combination of BS neural network manager <b>268</b>, training module <b>270</b>, core network neural network manager <b>312</b>, and/or training module <b>314</b>. The training can occur offline when no active communication exchanges are occurring, or online during active communication exchanges. For example, the base stations <b>120</b> and/or core network server <b>302</b> can mathematically generate training data, access files that store the training data, obtain real-world communications data, etc. The base stations <b>120</b> and/or core network server <b>302</b> then extract and store the various learned NN formation configurations in a neural network table. Some implementations store input characteristics with each NN formation configuration, where the input characteristics describe various properties of the transmission environment corresponding to the respective NN formation configuration. In implementations, a neural network manager selects a NN formation configuration and/or NN formation configuration element(s) by matching a current transmission environment and/or current operating environment to the input characteristics.</p><p id="p-0085" num="0084">Having described configurable machine-learning modules, consider now a discussion of deep neural networks in wireless communication systems that is in accordance with one or more implementations.</p><p id="p-0086" num="0085">Deep Neural Networks in Wireless Communication Systems</p><p id="p-0087" num="0086">Wireless communication systems include a variety of complex components and/or functions, such as the various devices and modules described with reference to the example environment <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the example device diagram <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, and the example device diagram <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In some implementations, the devices participating in the wireless communication system chain together a series of functions to enable the exchange of information over wireless connections.</p><p id="p-0088" num="0087">To demonstrate, consider now <figref idref="DRAWINGS">FIG. <b>5</b></figref> that illustrates example block diagram <b>500</b> and example block diagram <b>502</b>, each of which depicts an example processing chain utilized by devices in a wireless communication system. For simplicity, the block diagrams illustrate high-level functionality, and it is to be appreciated that the block diagrams may include additional functions that are omitted from <figref idref="DRAWINGS">FIG. <b>5</b></figref> for the sake of clarity.</p><p id="p-0089" num="0088">In the upper portion of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, block diagram <b>500</b> includes a transmitter block <b>504</b> and a receiver block <b>506</b>. Transmitter block <b>504</b> includes a transmitter processing chain that progresses from top to bottom. The transmitter processing chain begins with input data that progresses to an encoding stage, followed by a modulating stage, and then a radio frequency (RF) analog transmit (Tx) stage. The encoding stage can include any type and number of encoding stages employed by a device to transmit data over the wireless communication system.</p><p id="p-0090" num="0089">To illustrate, an example encoding stage receives binary data as input, and processes the binary data using various encoding algorithms to append information to the binary data, such as frame information. Alternately or additionally, the encoding stage transforms the binary data, such as by applying forward error correction that adds redundancies to help information recovery at a receiver. As another example, the encoding stage converts the binary data into symbols.</p><p id="p-0091" num="0090">An example modulating stage receives an output generated by the encoding stage as input and embeds the input onto a signal. For instance, the modulating stage generates digital samples of signal(s) embedded with the input from the encoding stage. Thus, in transmitter block <b>504</b>, the encoding stage and the modulating stage represent a high-level transmitter processing chain that oftentimes includes lower-level complex functions, such as convolutional encoding, serial-to-parallel conversion, cyclic prefix insertion, channel coding, time/frequency interleaving, and so forth. The RF analog Tx stage receives the output from the modulating stage, generates an analog RF signal based on the modulating stage output, and transmits the analog RF signal to receiver block <b>506</b>.</p><p id="p-0092" num="0091">Receiver block <b>506</b> performs complementary processing relative to transceiver block <b>504</b> using a receiver processing chain. The receiver processing chain illustrated in receiver block <b>506</b> progresses from top to bottom and includes an RF analog receive (Rx) stage, followed by a demodulating stage, and a decoding stage.</p><p id="p-0093" num="0092">The RF analog Rx stage receives signals transmitted by the transmitter block <b>504</b>, and generates an input used by the demodulating stage. As one example, the RF analog Rx stage includes a down-conversion component and/or an analog-to-digital converter (ADC) to generate samples of the received signal. The demodulating stage processes input from the RF analog Rx stage to extract data embedded on the signal (e.g., data embedded by the modulating stage of the transmitter block <b>504</b>). The demodulating stage, for instance, recovers symbols and/or binary data.</p><p id="p-0094" num="0093">The decoding stage receives input from the demodulating stage, such as recovered symbols and/or binary data, and processes the input to recover the transmitted information. To illustrate, the decoding stage corrects for data errors based on forward error correction applied at the transmitter block, extracts payload data from frames and/or slots, and so forth. Thus, the decoding stage generates the recovered information.</p><p id="p-0095" num="0094">As noted, the transmitter and receiver processing chains illustrated by transmitter block <b>504</b> and receiver block <b>506</b> have been simplified for clarity and can include multiple complex modules. At times, these complex modules are specific to particular functions and/or conditions. Consider, for example, a receiver processing chain that processes Orthogonal Frequency Division Modulation (OFDM) transmissions. To recover information from OFDM transmissions, the receiver block oftentimes includes multiple processing blocks, each of which is dedicated to a particular function, such as an equalization block that corrects for distortion in a received signal, a channel estimation block that estimates transmission channel properties to identify the effects of scattering, power decay, and so forth, on a transmission, etc. At high frequencies, such as 5G mmW signals in the 6 GHz band, these blocks can be computationally and/or monetarily expensive (e.g., require substantial processing power, require expensive hardware). Further, implementing blocks that generate outputs with an accuracy within a desired threshold oftentimes requires more specific and less flexible components. To illustrate, an equalization block that functions for 5G mmW signals in the 6 GHz band may not perform with the same accuracy at other frequency bands, thus necessitating different equalization blocks for different bands and adding complexity to the corresponding devices.</p><p id="p-0096" num="0095">Some implementations include DNNs in the transmission and/or receiver processing chains. In block diagram <b>502</b>, transmitter block <b>508</b> includes one or more deep neural network(s) <b>510</b> (DNNs <b>510</b>) in the transmitter processing chain, while receiver block <b>512</b> includes one or more deep neural network(s) <b>514</b> (DNNs <b>514</b>) in the receiver processing chain.</p><p id="p-0097" num="0096">For simplicity, the DNNs <b>510</b> in the transmitter block <b>508</b> correspond to the encoding stage and the modulating stage of transmitter block <b>504</b>. It is to be appreciated, however, that the DNNs can perform any high-level and/or low-level operation found within the transmitter processing chain. For instance, a first DNN performs low-level transmitter-side forward error correction, a second DNN performs low-level transmitter-side convolutional encoding, and so forth. Alternately or additionally, the DNNs <b>510</b> perform high-level processing, such as end-to-end processing that corresponds to the encoding stage and the modulating stage of transmitter block <b>508</b>.</p><p id="p-0098" num="0097">In a similar manner, the DNNs <b>514</b> in receiver block <b>512</b> perform receiver processing chain functionality (e.g., demodulating stage, decoding stage). The DNNs <b>514</b> can perform any high-level and/or low-level operation found within the receiver processing chain, such as low-level receiver-side bit error correction, low-level receiver-side symbol recovery, high-level end-to-end demodulating and decoding, etc. Accordingly, DNNs in wireless communication systems can be configured to replace high-level operations and/or low-level operations in transmitter and receiver processing chains. At times, the DNNs performing the high-level operations and/or low-level operations can be configured and/or reconfigured based on a current operating environment as further described. This provides more flexibility and adaptability to the processing chains relative to the more specific and less flexible components.</p><p id="p-0099" num="0098">Some implementations process communication exchanges over the wireless communication system using multiple DNNs, where each DNN has a respective purpose (e.g., uplink processing, downlink processing, uplink encoding processing, downlink decoding processing, etc.). To demonstrate, consider now <figref idref="DRAWINGS">FIG. <b>6</b></figref> that illustrates an example operating environment <b>600</b> that includes UE <b>110</b> and base station <b>121</b>. While <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates base station <b>121</b>, an alternate base station can be utilized, as indicated by <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In implementations, the UE <b>110</b> and base station <b>121</b> exchange communications with one another over a wireless communication system by processing the communications using multiple DNNs.</p><p id="p-0100" num="0099">In <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the base station neural network manager <b>268</b> of the base station <b>121</b> includes a downlink processing module <b>602</b> for processing downlink communications, such as for generating downlink communications transmitted to the UE <b>110</b>. To illustrate, the base station neural network manager <b>268</b> forms deep neural network(s) <b>604</b> (DNNs <b>604</b>) in the downlink processing module <b>602</b> using NN formation configurations as further described. In some examples, the DNNs <b>604</b> correspond to the DNNs <b>510</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>. In other words, the DNNs <b>604</b> perform some or all of the transmitter processing functionality used to generate downlink communications.</p><p id="p-0101" num="0100">Similarly, the UE neural network manager <b>218</b> of the UE <b>110</b> includes a downlink processing module <b>606</b>, where the downlink processing module <b>606</b> includes deep neural network(s) <b>608</b> (DNNs <b>608</b>) for processing (received) downlink communications. In various implementations, the UE neural network manager <b>218</b> forms the DNNs <b>608</b> using NN formation configurations. In <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the DNNs <b>608</b> correspond to the DNNs <b>514</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, where the deep neural network(s) <b>606</b> of UE <b>110</b> perform some or all receiver processing functionality for (received) downlink communications. Accordingly, the DNNs <b>604</b> and the DNNs <b>608</b> perform complementary processing to one another (e.g., encoding/decoding, modulating/demodulating).</p><p id="p-0102" num="0101">The DNNs <b>604</b> and/or DNNs <b>608</b> can include multiple deep neural networks, where each DNN is dedicated to a respective channel, a respective purpose, and so forth. The base station <b>121</b>, as one example, processes downlink control channel information using a first DNN of the DNNs <b>604</b>, processes downlink data channel information using a second DNN of the DNNs <b>604</b>, and so forth. As another example, the UE <b>110</b> processes downlink control channel information using a first DNN of the DNNs <b>608</b>, processes downlink data channel information using a second DNN of the DNNs <b>608</b>, etc.</p><p id="p-0103" num="0102">The base station <b>121</b> and/or the UE <b>110</b> also process uplink communications using DNNs. In environment <b>600</b>, the UE neural network manager <b>218</b> includes an uplink processing module <b>610</b>, where the uplink processing module <b>610</b> includes deep neural network(s) <b>612</b> (DNNs <b>612</b>) for generating and/or processing uplink communications (e.g., encoding, modulating). In other words, uplink processing module <b>610</b> processes pre-transmission communications as part of processing the uplink communications. The UE neural network manager <b>218</b>, for example, forms the DNNs <b>612</b> using NN formation configurations. At times, the DNNs <b>612</b> correspond to the DNNs <b>510</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>. Thus, the DNNs <b>612</b> perform some or all of the transmitter processing functionality used to generate uplink communications transmitted from the UE <b>110</b> to the base station <b>121</b>.</p><p id="p-0104" num="0103">Similarly, uplink processing module <b>614</b> of the base station <b>121</b> includes deep neural network(s) <b>616</b> (DNNs <b>616</b>) for processing (received) uplink communications, where base station neural network manager <b>268</b> forms DNNs <b>616</b> using NN formation configurations as further described. In examples, the DNNs <b>616</b> of the base station <b>121</b> correspond to the DNNs <b>514</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, and perform some or all receiver processing functionality for (received) uplink communications, such as uplink communications received from UE <b>110</b>. At times, the DNNs <b>612</b> and the DNNs <b>616</b> perform complementary functionality of one another. Alternately or additionally, the uplink processing module <b>610</b> and/or the uplink processing module <b>614</b> include multiple DNNs, where each DNN has a dedicated purpose (e.g., processes a respective channel, performs respective uplink functionality, and so forth). <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates the DNNs <b>604</b>, <b>608</b>, <b>612</b>, and <b>616</b> as residing within the respective neural network managers to signify that the neural network managers form the DNNs, and it is to be appreciated that the DNNs can be formed external to the neural network managers (e.g., UE neural network manager <b>218</b> and base station neural network manager <b>268</b>) within different components, processing chains, modules, etc.</p><p id="p-0105" num="0104">Having described deep neural networks in wireless communication systems, consider now a discussion of signaling and control transactions over a wireless communication system that can be used to configure deep neural networks for downlink and uplink communications that is in accordance with one or more implementations.</p><p id="p-0106" num="0105">Signaling and Control Transactions to Configure Deep Neural Networks</p><p id="p-0107" num="0106"><figref idref="DRAWINGS">FIGS. <b>7</b>, <b>8</b>, and <b>9</b></figref> illustrate example signaling and control transaction diagrams between a base station, a user equipment, and/or a core network server in accordance with one or more aspects of neural network formation configuration feedback in wireless communications regarding deep neural networks. The signaling and control transactions may be performed by the base station <b>120</b> and the UE <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, or the core network server <b>302</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, using elements of <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>6</b></figref>. For example, the core network server <b>302</b> performs, in some implementations, various signaling and control actions performed by the base station <b>120</b> as illustrated by <figref idref="DRAWINGS">FIGS. <b>7</b> and <b>8</b></figref>.</p><p id="p-0108" num="0107">A first example of signaling and control transactions for neural network formation configuration feedback in wireless communications is illustrated by the signaling and control transaction diagram <b>700</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>. As illustrated, at <b>705</b>, the UE <b>110</b> optionally indicates UE capabilities (e.g., capabilities supported by the UE). In some implementations, the UE capabilities include ML-related capabilities, such as a maximum kernel size capability, a memory limitation, a computation capability, supported ML architectures, supported number of layers, available processing power, memory limitation, available power budget, and fixed-point processing vs. floating point processing.</p><p id="p-0109" num="0108">At <b>710</b> the base station <b>121</b> determines a neural network formation configuration. In determining the neural network formation configuration, the base station analyzes any combination of information, such as a channel type being processed by the deep neural network (e.g., downlink, uplink, data, control, etc.), transmission medium properties (e.g., power measurements, signal-to-interference-plus-noise ratio (SINR) measurements, channel quality indicator (CQI) measurements), encoding schemes, UE capabilities, BS capabilities, and so forth. In some implementations, the base station <b>121</b> determines the neural network formation configuration based upon the UE capabilities indicated at <b>705</b>. Alternately or additionally, the base station <b>121</b> obtains the UE capabilities from a networked storage device, such as a server.</p><p id="p-0110" num="0109">The base station <b>121</b>, for instance, receives message(s) from the UE <b>110</b> (not shown) that indicates one or more capabilities of the UE, such as, by way of example and not of limitation, connectivity information, dual-connectivity information, carrier aggregation capabilities, downlink physical parameter values, uplink physical parameter values, supported downlink/uplink categories, inter-frequency handover, and ML-capabilities (e.g., a maximum kernel size capability, a memory limitation, a computation capability, supported ML architectures, supported number of layers, available processing power, memory limitation, available power budget, fixed-point processing vs. floating point processing). The base station <b>121</b> identifies, from the message(s), the UE capabilities that impact how the UE processes communications, and/or how the base station processes communications from the UE and selects a neural network formation configuration with improved output accuracy relative to other neural network formation configurations.</p><p id="p-0111" num="0110">In some implementations, the base station <b>121</b> selects the neural network formation configuration from multiple neural network formation configurations. Alternately or additionally, the base station <b>121</b> selects the neural network formation configuration by selecting a subset of neural network architecture formation elements in a neural network table. At times, the base station <b>121</b> analyzes multiple neural network formation configurations and/or multiple neural network formation configuration elements included in a neural network table, and determines the neural network formation configuration by selects and/or creates a neural network formation configuration that aligns with current channel conditions, such as by matching the channel type, transmission medium properties, etc., to input characteristics as further described. Alternately or additionally, the base station <b>121</b> selects the neural network formation configuration based on network parameters, such as scheduling parameters (e.g., scheduling Multiple User, Multiple Input, Multiple Output (MU-MIMO) for downlink communications, scheduling MU-MIMO for uplink communications).</p><p id="p-0112" num="0111">At <b>715</b>, the base station <b>121</b> communicates the neural network formation configuration to the UE <b>110</b>. In some implementations, the base station transmits a message that specifies the neural network formation configuration, such as by transmitting a message that includes an index value that maps to an entry in a neural network table, such as neural network table <b>216</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Alternately or additionally, the base station transmits a message that includes neural network parameter configurations (e.g., weight values, coefficient values, number of filters). In some cases, the base station <b>121</b> specifies a purpose and/or processing assignment in the message, where the processing assignment indicates what channels, and/or where in a processing chain, the configured neural network applies to, such as a downlink control channel processing, an uplink data channel processing, downlink decoding processing, uplink encoding processing, etc. Accordingly, the base station can communicate a processing assignment with a neural network formation configuration.</p><p id="p-0113" num="0112">In some implementations, the base station <b>121</b> communicates multiple neural network formation configurations to the UE <b>110</b>. For example, the base station transmits a first message that directs the UE to use a first neural network formation configuration for uplink encoding, and a second message that directs the UE to use a second neural network formation configuration for downlink decoding. In some scenarios, the base station <b>121</b> communicates multiple neural network formation configurations, and the respective processing assignments, in a single message. As yet another example, the base station communicates the multiple neural network formation configurations using different radio access technologies (RATs). The base station can, for instance, transmit a first neural network formation configuration for downlink communication processing to the UE <b>110</b> using a first RAT and/or carrier, and transmit a second neural network formation configuration for uplink communication processing to the UE <b>110</b> using a second RAT and/or carrier.</p><p id="p-0114" num="0113">At <b>720</b>, the UE <b>110</b> forms a first neural network based on the neural network formation configuration. For instance, the UE <b>110</b> accesses a neural network table using the index value(s) communicated by the base station to obtain the neural network formation configuration and/or the neural network formation configuration elements. Alternately or additionally, the UE <b>110</b> extracts neural network architecture and/or parameter configurations from the message. The UE <b>110</b> then forms the neural network using the neural network formation configuration, the extracted architecture and/or parameter configurations, etc. In some implementations, the UE processes all communications using the first neural network, while in other implementations, the UE processes select communications using the first neural network based on a processing assignment.</p><p id="p-0115" num="0114">At <b>725</b>, the base station <b>121</b> communicates information based on the neural network formation configuration. For instance, with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the base station <b>121</b> processes downlink communications using a second neural network configured with complementary functionality to the first neural network. In other words, the second neural network uses a second neural network formation configuration that is complementary to the neural network formation configuration. In turn, at <b>730</b>, the UE <b>110</b> recovers the information using the first neural network.</p><p id="p-0116" num="0115">A second example of signaling and control transactions for neural network formation configuration feedback in wireless communications regarding deep neural networks is illustrated by the signaling and control transaction diagram <b>800</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>. In some implementations, the signaling and control transaction diagram <b>800</b> represents a continuation of the signaling and control transaction diagram <b>700</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0117" num="0116">As illustrated, at <b>805</b>, the base station <b>121</b> communicates with the UE <b>110</b> based on a first neural network formation configuration. Similarly, at <b>810</b>, the UE <b>110</b> communicates with the base station <b>121</b> based on the first neural network formation configuration. The base station <b>121</b>, for instance, communicates with the UE <b>110</b> by processing one or more downlink communications using the DNNs <b>604</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, while the UE <b>110</b> communicates with the base station <b>121</b> by processing downlink communications received from the base station <b>121</b> using DNNs <b>608</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0118" num="0117">In implementations, the DNNs <b>604</b> and the DNNs <b>608</b> are formed based on the first neural network formation configuration as described. To illustrate, the DNNs <b>604</b> and the DNNs <b>608</b> perform complementary functionality of one another, where the first neural network formation configuration specifies the complementary functionality for each deep neural network (e.g., the base station forms a first neural network using the first neural network formation configuration, the UE forms a second neural network that is complementary to the first neural network by using a complementary neural network formation configuration to the first neural network formation configuration). The complementary functionality performed by the deep neural networks allows each side exchanging communications to stay synchronized (e.g., accurately recover information). Thus, the first neural network formation configuration specifies any combination of a base station-side neural network formation configuration, a complementary user equipment-side neural network formation configuration, and/or a general neural network formation configuration used by each device participating in the communication exchange.</p><p id="p-0119" num="0118">As another example, the UE <b>110</b> processes one or more uplink communications to the base station <b>121</b> using the DNNs <b>612</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, while the base station <b>121</b> processes the uplink communications received from the UE <b>110</b> using the deep neural network(s) <b>614</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>. Similar to the downlink communications, some implementations form the DNNs <b>612</b> and the deep neural network(s) <b>614</b> based on the first neural network formation configuration (e.g., the UE forms a first neural network using the first neural network formation configuration, the base station forms a second neural network with a complementary neural network formation configuration to the first neural network formation configuration). Accordingly, the base station <b>121</b> and the UE <b>110</b> communicate with one another based on the first neural network formation configuration by forming deep neural networks based on the first neural network formation configuration, and processing communications with the deep neural networks.</p><p id="p-0120" num="0119">At <b>815</b>, the base station generates base station metrics, such as metrics based upon uplink communications received from the UE <b>110</b>. Similarly, at <b>820</b>, the UE <b>110</b> generates UE metrics, such as metrics based upon downlink communications received from the base station <b>121</b>. Any type of metric can be generated by the base station <b>121</b> and/or UE <b>110</b>, such as power measurements (e.g., RSS), error metrics, timing metrics, QoS, latency, and so forth.</p><p id="p-0121" num="0120">At <b>825</b>, the UE <b>110</b> communicates the metrics to the base station <b>121</b>. In implementations, the UE <b>110</b> processes the metric communications using a deep neural network based on the first neural network formation configuration. Alternately or additionally, the UE <b>110</b> processes the metric communications using a neural network formed using a second neural network formation configuration. Thus, as further described, UE <b>110</b> maintains, in some implementations, multiple deep neural networks, where each deep neural network has a designated purpose and/or processing assignment (e.g., a first neural network for downlink control channel processing, a second neural network for downlink data channel processing, a third neural network for uplink control channel processing, a fourth neural network for uplink data channel processing). At times, the base station <b>121</b> communicates the multiple neural network formation configurations, used to form the multiple deep neural networks, to the UE <b>110</b>.</p><p id="p-0122" num="0121">At <b>830</b>, the base station <b>121</b> identifies a second neural network formation configuration based on the metrics at <b>830</b>. In some implementations, the base station <b>121</b> identifies the second neural network formation configuration based on the UE metrics, the base station metrics, or any combination thereof. This includes identifying any combination of architectural changes and/or parameter changes to the neural network formation configuration as further described, such as a small change to the neural network formation configuration that involves updating coefficient parameters to address changes in returned metrics (e.g., SINR changes, Doppler feedback changes, power level changes, BLER changes). Alternately or additionally, identifying the second neural network formation configuration includes a large change, such as reconfiguring node and/or layer connections, based on metrics such as a change in a power state (e.g., a transition from a radio resource connected state to idle state).</p><p id="p-0123" num="0122">In some implementations, the base station <b>121</b> identifies a partial and/or delta neural network formation configuration as the second neural network formation configuration, where the partial and/or delta neural network formation configuration indicates changes to a full neural network formation configuration. A full neural network formation configuration, for example, includes an architectural configuration for a neural network and parameter configurations, while a partial and/or delta neural network formation configuration specifies changes and/or updates to the parameter configurations based on using the same architectural configuration indicated in the full neural network formation configuration.</p><p id="p-0124" num="0123">In some implementations, the base station identifies the second neural network formation configuration by identifying a neural network formation configuration in a neural network table that improves the UE's ability, and/or the base station's ability, to recover data from the communications (e.g., improve an accuracy of the recovered information). To illustrate, the base station <b>121</b> identifies, by way of the base station neural network manager <b>268</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a neural network formation configuration that compensates for problems identified by the UE metrics and/or the base station metrics. As another example, the base station <b>121</b> identifies a neural network formation configuration with one or more input characteristics that align with changing operating conditions identified by the UE metrics and/or the base station metrics. Alternately or additionally, the base station <b>121</b> identifies a neural network formation configuration that produces similar results but with less processing, such as for a scenario in which a UE moves to a lower power state.</p><p id="p-0125" num="0124">At <b>840</b>, the base station <b>121</b> directs the UE <b>110</b> to update the first neural network with the second neural network formation configuration. The base station, for instance, generates an update message that includes an index value to the second neural network formation configuration in the neural network table <b>216</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In some implementations, the base station <b>121</b> indicates, in the message, a time instance that directs the UE <b>110</b> on when to apply the second neural network formation configuration. In other words, the time instance directs the UE <b>110</b> to switch from processing communications using the first neural network formation configuration to processing communications using the second neural network formation configuration at the time specified in the time instance. In implementations, the base station transmits updates to downlink neural networks using a first carrier or RAT, and transmits updates to uplink neural networks using a second carrier or RAT.</p><p id="p-0126" num="0125">At <b>845</b>, the base station <b>121</b> updates a base station neural network based on the second neural network formation configuration, such as the deep neural network formed based on the first neural network formation configuration and used to communicate with the UE <b>110</b> at <b>805</b> (e.g., a deep neural network for processing downlink communications, a deep neural network for processing uplink communications). Similarly, at <b>850</b>, the UE <b>110</b> updates a user equipment neural network based on the second neural network formation configuration, such as a deep neural network that performs complementary functionality to the base station neural network updated at <b>845</b>. The UE, as one example, extracts the index value and/or time value from the update message transmitted by the base station at <b>840</b>. The UE <b>110</b> obtains the second neural network formation configuration and modifies the user equipment neural network at the time specified in the update message. Thus, the UE <b>110</b> uses the first neural network formation configuration for processing communications until the specified time in the update message, at which point the UE <b>110</b> switches to processing communications using the second neural network formation configuration.</p><p id="p-0127" num="0126">In implementations, the base station <b>121</b> and/or the UE <b>110</b> iteratively perform the signaling and control transactions described in the signaling and control transaction diagram <b>800</b>, signified in <figref idref="DRAWINGS">FIG. <b>8</b></figref> with dashed lines. These iterations allow the base station <b>121</b> and/or the UE <b>110</b> to dynamically modify communication processing chains based upon changing operating conditions as further described.</p><p id="p-0128" num="0127">A third example of signaling and control transactions for neural network formation configuration feedback in wireless communications is illustrated by the signaling and control transaction diagram <b>900</b> of <figref idref="DRAWINGS">FIGS. <b>9</b>-<b>1</b> and <b>9</b>-<b>2</b></figref>. As illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>-<b>1</b></figref>, at <b>905</b>, the core network server <b>302</b> determines a neural network formation configuration based on a variety of metrics and/or parameters, such as metrics from the UE <b>110</b>, metrics from the base station <b>121</b>, UE capabilities, etc. For example, the core network server receives any combination of metrics and/or parameters from the base station <b>121</b> and/or the UE <b>110</b>, such as power information, SINR information, CQI, CSI, Doppler feedback, QoS, latency, UE capabilities, a base station type (e.g., eNB, gNB or ng-eNB), protocol versions, error metrics, UE capabilities, BS capabilities, power mode, and so forth. The core network server <b>302</b> then determines the neural network formation configuration based on the metrics, parameters, etc.</p><p id="p-0129" num="0128">In some implementations, the core network server <b>302</b> determines multiple different neural network formation configurations, each of which is specific to a respective base station and/or respective UE. Alternately or additionally, the core network server <b>302</b> determines a neural network formation configuration used by multiple base stations and/or UEs. At times, the core network server <b>302</b> determines a default neural network formation configuration used by base stations and/or UEs to initially connect with one another. As further described, a default neural network formation configuration corresponds to a general neural network formation configuration that configures the deep neural network to process a variety of input data and/or channel conditions with an accuracy within a threshold range or value. A dedicated neural network formation configuration, however, corresponds to a deep neural network that is tuned to a particular type of input data and/or particular channel conditions. In determining the neural network formation configuration, some implementations of the core network server determine complementary neural network formation configurations (e.g., a base station-side neural network formation configuration that is complementary to a user equipment-side neural network formation configuration).</p><p id="p-0130" num="0129">At <b>910</b>, the core network server <b>302</b> communicates the neural network formation configuration to the base station <b>121</b>. For instance, the core network server <b>302</b> communicates an index value to the base station <b>121</b> over the core network interface <b>320</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, where the index value maps to an entry in the neural network table <b>272</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Alternately or additionally, the core network server <b>302</b> communicates various parameter configurations, such as coefficients, weights, layer connection values, etc.</p><p id="p-0131" num="0130">At <b>915</b>, the base station <b>121</b> forwards the neural network formation configuration to the UE <b>110</b>. The base station, for instance, wirelessly transmits the index value to the UE, wirelessly transmits the parameter configurations to the UE, and so forth. In communicating the neural network formation configuration, the core network server <b>302</b> and/or the base station <b>121</b> sometimes indicates a processing assignment for the neural network formation configuration (e.g., downlink data channel processing, uplink control channel processing, decoding processing, uplink encoding processing, uplink modulating processing, downlink demodulating processing).</p><p id="p-0132" num="0131">In some implementations, the base station <b>121</b> adds modifications the neural network formation configuration before forwarding to the UE <b>110</b>. For example, the base station <b>121</b> can have access to more updated information than the core network server, such as through UE capabilities received from the UE <b>110</b>. The base station <b>121</b>, at times, adapts the neural network formation configuration based upon the updated information (e.g., UE capabilities particular to the UE <b>110</b>), such as by removing layers and/or nodes to reduce a corresponding complexity of the deep neural network at the UE <b>110</b> based on available processing capabilities, battery power, available radios, etc. at the UE. As another example, the base station <b>121</b> adds convolutional layers to the neural network formation configuration based on the updated information. Afterwards, the base station <b>121</b> forwards the modified neural network formation configuration to the UE <b>110</b>, in lieu of the neural network formation configuration received from the core network server <b>302</b>.</p><p id="p-0133" num="0132">At <b>920</b>, the base station <b>121</b> forms a first deep neural network using the neural network formation configuration, such as by identifying a base station-side neural network formation configuration and forming the first deep neural network with the base station-side neural network formation configuration. To illustrate, the base station <b>121</b> obtains the neural network formation configuration by using an index value to access neural network table <b>272</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Similarly, at <b>925</b>, the UE <b>110</b> forms a second deep neural network using the neural network formation configuration, such as by identifying a complementary and/or user equipment-side neural network formation configuration and forming the second neural network with the complementary and/or user equipment-side neural network formation configuration. In some implementations, the UE <b>110</b> obtains the complementary and/or user equipment-side neural network formation configuration by using an index value to access the neural network table <b>216</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Accordingly, the first deep neural network and the second deep neural network are synchronized neural networks based on the neural network formation configuration determined and communicated by the core network server at <b>905</b> and at <b>910</b>.</p><p id="p-0134" num="0133">In <figref idref="DRAWINGS">FIG. <b>9</b>-<b>2</b></figref>, at <b>930</b>, the base station <b>121</b> communicates with the UE <b>110</b> using the first deep neural network, such as by generating and/or processing downlink communications to the UE <b>110</b>, by receiving and/or processing uplink communications from the UE <b>110</b>, etc. Similarly, at <b>935</b>, the UE <b>110</b> communicates with the base station <b>121</b> using the second deep neural network at <b>935</b>. In other words, the UE <b>110</b> communicates with the base station <b>121</b> using a complementary deep neural network (e.g., the second deep neural network) that is based on the neural network formation configuration as further described.</p><p id="p-0135" num="0134">At <b>940</b>, the base station generates base station metrics, where the base station <b>121</b> generates the metrics based on the communicating at <b>930</b> and/or at <b>935</b>. Thus, the base station metrics can be based on uplink communications received from the UE <b>110</b>. For example, the base station <b>121</b> generates uplink received power, uplink SINR, uplink packet errors, uplink throughput, timing measurements, and so forth. In some implementations, the base station includes base station capabilities (BS capabilities), such as processing power (e.g., macro base station, small-cell base station), power state, etc., in the base station metrics.</p><p id="p-0136" num="0135">Similarly, at <b>945</b>, the UE <b>110</b> generates UE metrics (e.g., power information, SINR information, CQI, CSI, Doppler feedback, QoS, latency) based on downlink communications from the base station <b>121</b> and communicates the UE metrics to the base station <b>121</b> at <b>950</b>.</p><p id="p-0137" num="0136">At <b>955</b>, the base station <b>121</b> forwards the metrics to the core network server <b>302</b>, such as through core network interface <b>320</b>. This includes any combination of the base station metrics and the UE metrics generated at <b>940</b> and/or at <b>945</b>. Afterwards, the core network server <b>302</b> determines updates to the neural network formation configuration at <b>960</b>. This can include any combination of architectural structure changes (e.g., reconfiguring node connections, reconfiguring active layers/inactive layers), changing applied processing parameters (e.g., coefficients, kernels), and so forth. Thus, the core network server <b>302</b>, at times, identifies small changes and/or large changes, such as those described with reference to the base station <b>121</b> at <b>830</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>. By receiving metrics generated from UE <b>110</b>, by way of base station <b>121</b>, the core network server <b>302</b> receives feedback about the communication channel between the base station and the UE and/or an indication of how well the neural network processes communications transmitted over the communication channel. The core network server <b>302</b> analyzes the feedback to identify adjustments to the neural network formation configuration that, when applied to the neural network, improve the accuracy of how the deep neural network processes communications (e.g., more accurate recovery of data, less erroneous data) and/or how efficiently the neural network processes the communications (e.g. selecting configurations that reduce processing time of the deep neural network).</p><p id="p-0138" num="0137">In implementations, the core network server <b>302</b>, base station <b>121</b> and/or the UE <b>110</b> iteratively perform the signaling and control transactions described in the signaling and control transaction diagram <b>900</b>, signified in <figref idref="DRAWINGS">FIGS. <b>9</b>-<b>1</b> and <b>9</b>-<b>2</b></figref> with a dashed line that returns from <b>960</b> to <b>910</b>. These iterations allow the core network server <b>302</b>, the base station <b>121</b> and/or the UE <b>110</b> to dynamically modify communication processing chains based upon changing operating conditions as further described.</p><p id="p-0139" num="0138">In some implementations, core network server <b>302</b> receives feedback from multiple UEs and/or base stations in the wireless communication system. This provides the core network server with a larger view of how well the wireless communication system performs, what devices communicate over the wireless communication system, how well the devices communicate, and so forth. In various implementations, the core network server <b>302</b> determines updates to the neural network formation configuration based on optimizing the communications of the multiple devices and/or an overall system performance, rather than optimizing the communications of a particular device.</p><p id="p-0140" num="0139">Having described signaling and control transactions that can be used to configure neural networks for processing communications, consider now some example methods that are in accordance with one or more implementations.</p><heading id="h-0007" level="1">Example Methods</heading><p id="p-0141" num="0140">Example methods <b>1000</b> and <b>1100</b> are described with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref> and <figref idref="DRAWINGS">FIG. <b>11</b></figref> in accordance with one or more aspects of neural network formation configuration feedback in wireless communications. The order in which the method blocks are described are not intended to be construed as a limitation, and any number of the described method blocks can be skipped or combined in any order to implement a method or an alternate method. Generally, any of the components, modules, methods, and operations described herein can be implemented using software, firmware, hardware (e.g., fixed logic circuitry), manual processing, or any combination thereof. Some operations of the example methods may be described in the general context of executable instructions stored on computer-readable storage memory that is local and/or remote to a computer processing system, and implementations can include software applications, programs, functions, and the like. Alternatively, or additionally, any of the functionality described herein can be performed, at least in part, by one or more hardware logic components, such as, and without limitation, Field-programmable Gate Arrays (FPGAs), Application-specific Integrated Circuits (ASICs), Application-specific Standard Products (ASSPs), System-on-a-chip systems (SoCs), Complex Programmable Logic Devices (CPLDs), and the like.</p><p id="p-0142" num="0141"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an example method <b>1000</b> for configuring a neural network for processing communications exchanged over a wireless communication system. In some implementations, operations of method <b>1000</b> are performed by a network entity, such as any one of the base stations <b>120</b> or the core network server <b>302</b>.</p><p id="p-0143" num="0142">At <b>1005</b>, a network entity determines a neural network formation configuration for a deep neural network for processing communications transmitted over the wireless communication system. The network entity (e.g., base station <b>121</b>, core network server <b>302</b>) for example, determines the neural network formation configuration based, at least in part, on metrics, feedback, and/or other types of information from the user equipment (e.g., UE <b>110</b>). To illustrate, the base station <b>121</b> receives a message from the UE <b>110</b> that indicates one or more capabilities of the UE <b>110</b>. The base station <b>121</b> then determines the neural network formation configuration based, at least in part, on the capabilities received from the UE <b>110</b>. Alternately or additionally, base station <b>121</b> forwards the UE capabilities to the core network server <b>302</b>, and the core network server <b>302</b> determines the neural network formation configuration based on the capabilities of the UE <b>110</b>. As another example, the base station <b>121</b> determines the neural network formation configuration based on scheduling MU-MIMO downlink and/or uplink transmissions in the wireless communication system. In determining the neural network formation configuration, the network selects any combination of neural network formation configuration elements for user equipment-side deep neural networks, base station-side deep neural networks, and/or a deep neural network that corresponds to both the user equipment-side deep neural networks and the base station-side deep neural networks.</p><p id="p-0144" num="0143">In determining the neural network formation configuration, the network entity (e.g., base station <b>121</b>, the core network server <b>302</b>) sometimes selects a default neural network formation configuration. Alternately or additionally, the network entity analyzes multiple neural network formation configurations, and selects a neural network formation configuration, from the multiple, that aligns with current channel conditions, capabilities of a particular UE, etc.</p><p id="p-0145" num="0144">At <b>1010</b>, the network entity generates a message that includes an indication of the neural network formation configuration for the deep neural network. The network entity (e.g., base station <b>121</b>, core network server <b>302</b>), for instance, generates a message that includes index value(s) that map(s) to one or more entries of a neural network table (e.g., neural network table <b>216</b>, neural network table <b>272</b>, neural network table <b>316</b>). In some implementations, the network entity includes an indication of a processing assignment for the deep neural network, where the processing assignment specifies a processing chain function for the deep neural network formed with the neural network formation configuration. Alternately or additionally, the network entity specifies a time instance in the message that indicates a time and/or location to start processing communications with the deep neural network.</p><p id="p-0146" num="0145">At <b>1015</b>, the network entity transmits the message to a user equipment to direct the user equipment to form the deep neural network using the neural network formation configuration and to process the communications transmitted over the wireless communication system using the deep neural network. The network entity (e.g., core network server <b>302</b>), for example, transmits the message to the user equipment (e.g., UE <b>110</b>) by communicating the message to the base station <b>121</b>, which transmits the message to the user equipment. Alternately or additionally, the network entity (e.g., base station <b>121</b>) transmits the message to the user equipment (e.g., UE <b>110</b>). In implementations the deep neural network corresponds to a user equipment-side deep neural network.</p><p id="p-0147" num="0146">In some implementations, the network entity (e.g., base station <b>121</b>) transmits the message using a particular RAT and/or carrier based upon a processing assignment of the deep neural network. For instance, the base station <b>121</b> transmits a first neural network formation configuration with a first processing assignment (e.g., downlink data channel processing) using a first RAT and/or carrier, and a second neural network formation configuration with a second processing assignment (e.g., downlink control channel processing) using a second RAT and/or carrier.</p><p id="p-0148" num="0147">In some implementations, at <b>1020</b>, the network entity analyzes feedback received from the user equipment. The network entity (e.g., base station <b>121</b>), as one example, receives metrics from the user equipment (e.g., UE <b>110</b>), and analyzes the metrics to determine whether to update and/or reconfigure the neural network with a different neural network formation configuration. Alternately or additionally, a base station forwards the feedback to the network entity (e.g., core network server <b>302</b>), and the network entity analyzes the feedback to determine whether to update and/or reconfigure the neural network with a different neural network formation configuration.</p><p id="p-0149" num="0148">At <b>1025</b>, the network entity updates the deep neural network based on the feedback. For example, the network entity (e.g., base station <b>121</b>, core network server <b>302</b>) analyzes multiple neural network formation configurations included in a neural network table (e.g., neural network table <b>216</b>, neural network table <b>272</b>, neural network table <b>316</b>), and selects a second neural network formation configuration that aligns with new channel conditions indicated by the feedback. The network entity generates a second message that includes an indication of the second neural network formation configuration, and transmits the second message to the user equipment. This includes configuring and transmitting the second message as described at <b>1010</b> and/or at <b>1015</b>. In implementations, the network entity iteratively performs various operations of the method <b>1000</b>, indicated here with a dashed line that returns from <b>1025</b> to <b>1010</b>.</p><p id="p-0150" num="0149"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates an example method <b>1100</b> for forming a neural network based on a neural network formation configuration. In some aspects, operations of method <b>1100</b> are implemented by a UE, such as UE <b>110</b>.</p><p id="p-0151" num="0150">At <b>1105</b>, a user equipment receives a message that indicates a neural network formation configuration for a deep neural network for processing communications transmitted over a wireless communication system. The user equipment (e.g., UE <b>110</b>), for instance, receives a message from a base station (e.g., the base station <b>121</b>) that indicates the neural network formation configuration and/or a processing assignment for the deep neural network formed using the neural network formation configuration. In some implementations, the user equipment receives multiple messages, each of which pertains to a different neural network formation configuration (e.g., a first message that indicates a first neural network formation configuration for a first deep neural network for processing downlink communications, a second message that indicates a second neural network formation configuration for a second deep neural network for processing uplink communications). In implementations, the user equipment receives the message as a broadcast message from the base station, or as a UE-dedicated message.</p><p id="p-0152" num="0151">At <b>1110</b>, the user equipment forms the deep neural network using the neural network formation configuration indicated in the message. To illustrate, the user equipment (e.g., UE <b>110</b>) extracts parameter configurations from the message, such as coefficients, weights, layer connections, etc., and forms the deep neural network using the extracted parameter configurations. As another example, the user equipment (e.g., UE <b>110</b>) extracts index value(s) from the message and obtains the parameter configurations from a neural network table as further described. Alternately or additionally, the user equipment extracts a time instance from the message, where the time instance indicates a time and/or location to start processing communications using the deep neural network formed with the neural network formation configuration.</p><p id="p-0153" num="0152">At <b>1115</b>, the user equipment receives communications from a base station. For example, the user equipment (e.g., UE <b>110</b>) receives downlink data channel communications from a base station (e.g., base station <b>121</b>), downlink control channel communications from the base station, etc. At <b>1120</b>, the user equipment processes the communications using the deep neural network to extract information transmitted in the communications. The user equipment (e.g., UE <b>110</b>), as one example, processes the communications using the deep neural network based on a processing assignment included in the message, such as by demodulating and/or decoding downlink communications from the base station (e.g., base station <b>121</b>) using the deep neural network, encoding and/or modulating uplink communications to the base station, and so forth.</p><p id="p-0154" num="0153">In some implementations, the user equipment optionally transmits feedback based on the communications at <b>1125</b>. For instance, the user equipment (e.g., UE <b>110</b>) generates metrics based on the communications (e.g., error metrics, SINR information, CQI, CSI, Doppler feedback) and transmits the metrics as feedback to the base station (e.g., base station <b>121</b>).</p><p id="p-0155" num="0154">Having described some example methods that can be used to implement aspects of base station-user equipment messaging using a deep neural network, consider now a discussion of generating and communicating neural network formation configurations that is in accordance with one or more implementations.</p><p id="p-0156" num="0155">Generating and Communicating Neural Network Formation Configurations</p><p id="p-0157" num="0156">In supervised learning, machine-learning modules process labeled training data to generate an output. The machine-learning modules receive feedback on an accuracy of the generated output and modify processing parameters to improve the accuracy of the output. <figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates an example <b>1200</b> that describes aspects of generating multiple NN formation configurations. At times, various aspects of the example <b>1200</b> are implemented by any combination of training module <b>270</b>, base station neural network manager <b>268</b>, core network neural network manager <b>312</b>, and/or training module <b>314</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> and <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0158" num="0157">The upper portion of <figref idref="DRAWINGS">FIG. <b>12</b></figref> includes machine-learning module <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In implementations, a neural network manager determines to generate different NN formation configurations. To illustrate, consider a scenario in which the base station neural network manager <b>268</b> determines to generate a NN formation configuration by selecting a combination of NN formation configuration elements from a neural network table, where the NN formation configuration corresponds to a UE decoding and/or demodulating downlink communications. In other words, the NN formation configuration (by way of the combination of NN formation configuration elements) forms a DNN that processes downlink communications received by a UE. Oftentimes, however, transmission channel conditions vary which, in turn, affects the characteristics of the downlink communications. For instance, a first transmission channel distorts the downlink communications by introducing frequency offsets, a second transmission channel distorts the downlink communications by introducing Doppler effects, a third transmission channel distorts the downlink communications by introducing multipath channel effects, and so forth. To accurately process the downlink communications (e.g., reduce bit errors), various implementations select multiple NN formation configurations, where each NN formation configuration (and associated combination of NN formation configuration elements) corresponds to a respective input condition, such as a first transmission channel, a second transmission channel, etc.</p><p id="p-0159" num="0158">Training data <b>1202</b> represents an example input to the machine-learning module <b>400</b>. In <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the training data represents data corresponding to a downlink communication. Training data <b>1202</b>, for instance, can include digital samples of a downlink communications signal, recovered symbols, recovered frame data, etc. In some implementations, the training module generates the training data mathematically or accesses a file that stores the training data. Other times, the training module obtains real-world communications data. Thus, the training module can train the machine-learning module using mathematically generated data, static data, and/or real-world data. Some implementations generate input characteristics <b>1204</b> that describe various qualities of the training data, such as transmission channel metrics, UE capabilities, UE velocity, and so forth.</p><p id="p-0160" num="0159">Machine-learning module <b>400</b> analyzes the training data, and generates an output <b>1206</b>, represented here as binary data. Some implementations iteratively train the machine-learning module <b>400</b> using the same set of training data and/or additional training data that has the same input characteristics to improve the accuracy of the machine-learning module. During training, the machine-learning module modifies some or all of the architecture and/or parameter configurations of a neural network included in the machine-learning module, such as node connections, coefficients, kernel sizes, etc. At some point in the training, the training module determines to extract the architecture and/or parameter configurations <b>1208</b> of the neural network (e.g., pooling parameter(s), kernel parameter(s), layer parameter(s), weights), such as when the training module determines that the accuracy meets or exceeds a desired threshold, the training process meets or exceeds an iteration number, and so forth. The training module then extracts the architecture and/or parameter configurations from the machine-learning module to use as a NN formation configuration and/or NN formation configuration element(s). The architecture and/or parameter configurations can include any combination of fixed architecture and/or parameter configurations, and/or variable architectures and/or parameter configurations.</p><p id="p-0161" num="0160">The lower portion of <figref idref="DRAWINGS">FIG. <b>12</b></figref> includes neural network table <b>1212</b> that represents a collection of NN formation configuration elements, such as neural network table <b>216</b>, neural network table <b>272</b>, and/or neural network table <b>316</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> and <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The neural network table <b>1212</b> stores various combinations of architecture configurations, parameter configurations, and input characteristics, but alternate implementations exclude the input characteristics from the table. Various implementations update and/or maintain the NN formation configuration elements and/or the input characteristics as the machine-learning module learns additional information. For example, at index <b>1214</b>, the neural network manager and/or the training module updates neural network table <b>1212</b> to include architecture and/or parameter configurations <b>1208</b> generated by the machine-learning module <b>400</b> while analyzing the training data <b>1202</b>.</p><p id="p-0162" num="0161">The neural network manager and/or the training module alternately or additionally adds the input characteristics <b>1204</b> to the neural network table and links the input characteristics to the architecture and/or parameter configurations <b>1208</b>. This allows the input characteristics to be obtained at a same time as the architecture and/or parameter configurations, such as through using an index value that references into the neural network table (e.g., references NN formation configurations, references NN formation configuration elements). In some implementations, the neural network manager selects a NN formation configuration by matching the input characteristics to a current operating environment, such as by matching the input characteristics to current channel conditions, UE capabilities, UE characteristics (e.g., velocity, location, etc.) and so forth.</p><p id="p-0163" num="0162">Having described generating and communicating neural network formation configurations, consider now a discussion of signaling and control transactions over a wireless communication system that can be used to communicate neural network formation configurations that is in accordance with one or more implementations</p><p id="p-0164" num="0163">Signaling and Control Transactions to Communicate Neural Network Formation Configurations</p><p id="p-0165" num="0164"><figref idref="DRAWINGS">FIGS. <b>13</b>-<b>15</b></figref> illustrate example signaling and control transaction diagrams between a base station, a user equipment, and/or a core network server in accordance with one or more aspects of communicating neural network formation configurations, such as communicating a NN formation configuration. In implementations, the signaling and control transactions may be performed by any one of the base stations <b>120</b> and the UE <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, or the core network server <b>302</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, using elements of <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>12</b></figref>.</p><p id="p-0166" num="0165">A first example of signaling and control transactions for communicating neural network formation configurations is illustrated by the signaling and control transaction diagram <b>1300</b> of <figref idref="DRAWINGS">FIG. <b>13</b></figref>. In implementations, portions or all of the signaling and control transactions described with reference to the signaling and control transaction diagram correspond to signaling and control transactions described with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref> and/or <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0167" num="0166">As illustrated, at <b>1305</b> the base station <b>121</b> maintains a neural network table. For example, a base station neural network manager and/or the training module of the base station <b>121</b> (e.g., base station neural network manager <b>268</b>, training module <b>270</b>) generate and/or maintain a neural network table (e.g., neural network table <b>272</b>) using any combination of mathematically generated training data, data extracted from real-world communications, files, etc. In various implementations, the base station <b>121</b> maintains multiple neural network tables, where each neural network table includes multiple neural network formation configurations and/or neural network formation configuration elements for a designated purpose, such as a first neural network table designated for data channel communications, a second neural network table designated for control channel communications, and so forth.</p><p id="p-0168" num="0167">At <b>1310</b>, the base station <b>121</b> transmits the neural network table to the UE <b>110</b>. As one example, the base station transmits the neural network table using layer 3 messaging (e.g., Radio Resource Control (RRC) messages). In transmitting the neural network table, the base station transmits any combination of architecture and/or parameter configurations that can be used to form a deep neural network, examples of which are provided in this disclosure. Alternately or additionally the base station transmits an indication with the neural network table that designates a processing assignment for the neural network table. Accordingly, the base station transmits multiple neural network tables to the UE, with a respective processing assignment designated for each neural network table. In some implementations, the base station <b>121</b> broadcasts the neural network table(s) to a group of UEs. Other times, the base station <b>121</b> transmits a UE-dedicated neural network table to the UE <b>110</b>.</p><p id="p-0169" num="0168">At <b>1315</b>, the base station <b>121</b> identifies a neural network formation configuration to use in processing communications. For example, the base station determines a neural network formation configuration to use in processing the communications by selecting a combination of neural network formation architecture elements, such as that described at <b>710</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, by analyzing any combination of information, such as a channel type being processed by a deep neural network (e.g., downlink, uplink, data, control, etc.), transmission medium properties (e.g., power measurements, signal-to-interference-plus-noise ratio (SINR) measurements, channel quality indicator (CQI) measurements), encoding schemes, UE capabilities, BS capabilities, and so forth. Thus, in some implementations, the base station <b>121</b> identifies the neural network formation configuration base on receiving various metrics and/or parameters, such as that described at <b>815</b>, <b>820</b>, and <b>830</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>. In some implementations, the base station <b>121</b> identifies a default neural network formation configuration to use as the neural network formation configuration. Thus, identifying the neural network formation configuration can include identifying default neural network formation configuration and/or updates to a neural network formation configuration.</p><p id="p-0170" num="0169">In identifying the neural network formation configuration, the base station <b>121</b> ascertains a neural network formation configuration in the neural network table that corresponds to the determined neural network formation configuration. In other words, the base station <b>121</b> identifies a neural network formation configuration and/or neural network formation configuration elements in neural network table <b>272</b> and/or neural network table <b>216</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> that align with the determined neural network formation configuration, such as by correlating and or matching input characteristics. In identifying the neural network formation configuration and/or neural network formation configuration elements in the neural network table, the base station identifies index value(s) of the neural network formation configuration and/or neural network formation configuration elements.</p><p id="p-0171" num="0170">At <b>1320</b>, the base station <b>121</b> transmits an indication that directs the UE <b>110</b> to form a deep neural network using a neural network formation configuration from the neural network table. For example, similar to that described at <b>715</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref> and/or at <b>840</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the base station <b>121</b> communicates the index value(s) to the UE <b>110</b>, and directs the UE <b>110</b> to form the deep neural network using the neural network formation configuration indicated by the index value(s). The base station can transmit the indication to the UE in any suitable manner. As one example, the base station transmits index value(s) that corresponds to the neural network formation configuration using a layer 2 message (e.g., a Radio Link Control (RLC) message, Medium Access Control (MAC) control element(s)). In some implementations, the base station compares a current operating environment (e.g., one or more of channel conditions, UE capabilities, BS capabilities, metrics) to input characteristics stored within the neural network table and identifies stored input characteristics aligned with the current operating environment. In turn, the base station obtains the index value of stored input characteristics which, in turn, provides the index value of the neural network formation configuration and/or neural network formation configuration elements. The base station <b>121</b> then transmits the index value(s) as the indication. At times, the base station <b>121</b> includes a processing assignment to indicate a position in a processing chain to apply the deep neural network. In some implementations, the base station transmits the index value(s) and/or processing assignment using a downlink control channel.</p><p id="p-0172" num="0171">At times, the base station transmits rule(s) to the UE specifying operating parameters related to applying the neural network formation configuration. In one example, the rules include a time instance that indicates when to process communications with the deep neural network formed using the neural network formation configuration. Alternately or additionally, the rules specify a time threshold value that directs the UE to use a default neural network formation configuration instead of the specified neural network formation configuration when a data channel and control channel are within the time threshold value. Additionally, or alternatively, a rule may direct the user equipment to use the same neural network formation configuration for data channel communications and control channel communications when a data channel and control channel are within the time threshold value. To illustrate, consider an example in which the UE processes data channel communications using a first deep neural network (formed using a first neural network formation configuration), and control channel communications using a second deep neural network (formed using a second neural network formation configuration). If the data channel communications and control channel communications fall within the time threshold value specified by the time instance, the UE processes both channels using a default deep neural network (formed with the default neural network formation configuration) and/or the same deep neural network, since there may not be enough time to switch between the first deep neural network and the second deep neural network.</p><p id="p-0173" num="0172">At times, the base station specifies a default neural network formation configuration to UE(s) using a downlink control channel to communicate the default neural network formation configuration, where the default neural network formation configuration forms a deep neural network that processes a variety of input data. In some implementations, the default neural network formation configuration forms a deep neural network that processes the variety of input data with an accuracy within a threshold range. The default neural network formation configuration can include a generic neural network formation configuration.</p><p id="p-0174" num="0173">To illustrate, some implementations generate or select neural network formation configurations for specific operating conditions, such as a first neural network formation configuration specific to UE downlink control channel processing (e.g., demodulating and/or deciding) with a current operating environment &#x201c;X&#x201d;, a second neural network formation configuration specific to UE downlink control channel processing with a current operating environment &#x201c;Y&#x201d;, and so forth. For example, a first neural network formation configuration can correlate to a current operating environment in which a detected interference level is high, a second neural network formation configuration can correlate to a current operating environment in which a detected interference level is low, a third neural network formation configuration can correlate to a current operating environment in which a connected UE appears stationary, a fourth neural network formation configuration can correlate to a current operating environment in which the connected UE appears to be moving and with a particular velocity, and so forth.</p><p id="p-0175" num="0174">Forming a deep neural network using a neural network formation configuration for specific operating conditions improves (relative to forming the deep neural network with other neural network formation configurations) an accuracy of the output generated by the deep neural network when processing input data corresponding to the specific operating conditions. However, this introduces a tradeoff insofar as the deep neural network formed with the neural network formation configuration for specific operating conditions generates output with less accuracy when processing input associated with other operating conditions. Conversely, a default neural network formation configuration corresponds to a neural network formation configuration that processes a wider variety of input, such as a variety of input that spans more operating conditions. In other words, a deep neural network configured with a default neural network formation configuration processes a larger variety of communications relative to neural network formation configurations directed to specific operating conditions.</p><p id="p-0176" num="0175">At <b>1325</b>, the UE <b>110</b> forms the deep neural network using the neural network formation configuration. The UE, as one example, extracts the index value(s) transmitted by the base station <b>121</b>, and obtains the neural network formation configuration and/or neural network formation configuration elements by accessing the neural network table using the index value(s). Alternately or additionally, the UE <b>110</b> extracts the processing assignment, and forms the deep neural network in the processing chain as specified by the processing assignment.</p><p id="p-0177" num="0176">At <b>1330</b>, the base station <b>121</b> transmits communications to the UE <b>110</b>, such as downlink data channel communications. At <b>1335</b>, the UE <b>110</b> processes the communications using the deep neural network. For instance, the UE <b>110</b> processes the downlink data channel communications using the deep neural network to recover the data. As another example, processing the communications includes processing a reply to the communications, where the UE <b>110</b> processes, using the deep neural network, uplink communications in reply to the downlink communications.</p><p id="p-0178" num="0177">A second example of signaling and control transactions for communicating neural network formation configurations is illustrated by the signaling and control transaction diagram <b>1400</b> of <figref idref="DRAWINGS">FIG. <b>14</b></figref>. As illustrated, at <b>1405</b> the core network server <b>302</b> maintains a neural network table. The core network neural network manager <b>312</b> and/or the training module <b>314</b> of the core network server <b>302</b>, for instance, generate and/or maintain the neural network table <b>316</b> using any combination of mathematically generated training data, data extracted from real-world communications, files, etc. In various implementations, the core neural network server <b>302</b> maintains multiple neural network tables, where each neural network table includes multiple neural network formation configuration elements for a designated processing assignment (e.g., a first neural network table designated for data channel communications, a second neural network table designated for control channel communications).</p><p id="p-0179" num="0178">At <b>1410</b>, the core network server <b>302</b> communicates the neural network table to the base station <b>121</b>, such as by using the core network interface <b>320</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In some implementations, the core network server communicates multiple neural network tables to the base station. At <b>1415</b>, the base station <b>121</b> transmits the neural network table to the UE <b>110</b>, such as by transmitting the neural network table using layer 3 messaging, by transmitting the neural network table using a downlink control channel, by broadcasting the neural network table, by transmitting the neural network table using a UE-dedicated message, and so forth.</p><p id="p-0180" num="0179">At <b>1420</b>, the core network server <b>302</b> selects a neural network formation configuration. As one example, the core network server <b>302</b> compares a current operating environment to input characteristics stored within the neural network table and identifies stored input characteristics aligned with the current operating environment (e.g., one or more of channel conditions, UE capabilities, BS capabilities, metrics). The core network server then obtains the index value(s) of the aligned input characteristics which, in turn, provides the index value(s) of the neural network formation configuration and/or neural network formation configuration elements. The core network server <b>302</b> then communicates the selected neural network formation configuration to the base station at <b>1425</b>, such as by communicating the index value(s) using core network interface <b>320</b>. In some implementations, the core network server communicates a processing assignment with the neural network formation configuration.</p><p id="p-0181" num="0180">At <b>1430</b>, the base station <b>121</b> forwards the neural network formation configuration to the UE <b>110</b>. As an example, the base station <b>121</b> transmits the index value(s) to the UE <b>110</b>, such as through layer 2 messaging (e.g., an RLC message, MAC control element(s)), to direct the UE to form the deep neural network using the neural network formation configuration, such as that described at <b>1320</b> of <figref idref="DRAWINGS">FIG. <b>13</b></figref>. In implementations, the base station <b>121</b> additionally forms a complementary deep neural network based on the neural network formation configuration (not shown here), such as that described in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, at <b>725</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, at <b>845</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, and/or at <b>920</b> of <figref idref="DRAWINGS">FIG. <b>9</b>-<b>1</b></figref>. At times, the base station communicates the processing assignment received from the core network server with the index value. In response to receiving the neural network formation configuration, the UE <b>110</b> forms the deep neural network at <b>1435</b>, and processes communications received from the base station <b>121</b> using the deep neural network at <b>1440</b>, such as that described at <b>730</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, at <b>810</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, at <b>935</b> of <figref idref="DRAWINGS">FIGS. <b>9</b></figref>, and/or <b>1335</b> of <figref idref="DRAWINGS">FIG. <b>13</b></figref>.</p><p id="p-0182" num="0181">In some implementations, a UE derives a first neural network formation configuration from a second neural network formation configuration when similarities are present in a wireless communication system. To illustrate, consider an example of quasi-correspondent channels. Quasi-correspondent channels are channels within a wireless communication system that have shared or the same properties, such as a same delay spread, a same Doppler spread, a same spatial signature, a same spatial beam, a same spatial direction, same data rate, and so forth. Alternately or additionally, quasi-correspondent channels have correlated physical properties within a threshold range or value. In various implementations, a UE derives a first neural network formation configuration from a second neural network formation configuration in response to identifying these similarities.</p><p id="p-0183" num="0182">To illustrate, consider now a third example of signaling and control transactions for communicating neural network formation configurations, illustrated in <figref idref="DRAWINGS">FIG. <b>15</b></figref> by the signaling and control transaction diagram <b>1500</b>. In the third example, a UE derives a neural network formation configuration based on identified similarities. While the signaling and control transaction diagram <b>1500</b> illustrates the base station <b>121</b> and the UE <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, alternate implementations include a core network server, such as core network server <b>302</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> performing some or all of the functionality performed by the base station <b>121</b>.</p><p id="p-0184" num="0183">At <b>1505</b>, the base station <b>121</b> identifies two or more channels that are quasi-correspondent channels. The base station <b>121</b> or core network server <b>302</b> (using base station neural network manager <b>268</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> or core network neural network manager <b>312</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>), for example, compares various beam properties of two or more channels, such as direction, intensity, divergence, profile, quality, etc. The base station <b>121</b> determines that the two or more channels are quasi-correspondent when an arbitrary number of properties match and/or correlate to one another within a predefined threshold.</p><p id="p-0185" num="0184">At <b>1510</b>, the base station <b>121</b> transmits an indication of the quasi-correspondent channels to the UE <b>110</b>, such as by transmitting the indication of quasi-correspondent channels using layer 2 messaging, layer 3 messaging, and/or layer 1 signaling. The indication denotes any arbitrary number of channels as being quasi-correspondent with one another, where the channels can be physical channels and/or logical channels. As one example, the indication denotes that a downlink control channel is quasi-correspondent with a downlink data channel when the channels have similar data rates. As another example, the indication denotes that two physical channels at different carrier frequencies are quasi-correspondent based on the physical channels having a similar spatial beam.</p><p id="p-0186" num="0185">At <b>1515</b>, the base station <b>121</b> transmits an indication of a neural network formation configuration for one of the quasi-correspondent channels. For example, the base station transmits index value(s) and processing assignment that designates the formed deep neural network for processing a first channel, such as a downlink control channel.</p><p id="p-0187" num="0186">At <b>1520</b>, the UE <b>110</b> forms a first deep neural network using the neural network formation configuration for the first channel (e.g., the downlink control channel). At <b>1525</b>, the UE <b>110</b> identifies the first channel as one of the quasi-correspondent channels. The UE, for instance, compares the first channel to the quasi-correspondent channels identified by, and received from, the base station <b>121</b>. In response to identifying the first channel as being one of the quasi-correspondent channels, the UE determines to apply the neural network formation configuration to the other channels that are quasi-correspondent to the first channel. Accordingly, at <b>1530</b>, the UE <b>110</b> forms deep neural networks for the other quasi-correspondent channels using the neural network formation configuration. By identifying channels as quasi-correspondent channels, the neural network formation configuration need only be changed for one of the quasi-correspondent channels and this will result in the neural network formation configuration being changed for all the quasi-correspondent channels without the need to change individually the neural network formation configuration for each of the other quasi-correspondent channels.</p><p id="p-0188" num="0187">Having described signaling and control transactions that can be used to communicate neural network formation configurations, consider now some example methods that can be used to communicate neural network formation configurations that are in accordance with one or more implementations.</p><heading id="h-0008" level="1">Example Methods</heading><p id="p-0189" num="0188">Example methods <b>1600</b> and <b>1700</b> are described with reference to <figref idref="DRAWINGS">FIG. <b>16</b></figref> and <figref idref="DRAWINGS">FIG. <b>17</b></figref> in accordance with one or more aspects of communicating neural network formation configurations. The order in which the method blocks are described are not intended to be construed as a limitation, and any number of the described method blocks can be skipped or combined in any order to implement a method or an alternate method. Generally, any of the components, modules, methods, and operations described herein can be implemented using software, firmware, hardware (e.g., fixed logic circuitry), manual processing, or any combination thereof. Some operations of the example methods may be described in the general context of executable instructions stored on computer-readable storage memory that is local and/or remote to a computer processing system, and implementations can include software applications, programs, functions, and the like. Alternatively, or additionally, any of the functionality described herein can be performed, at least in part, by one or more hardware logic components, such as, and without limitation, Field-programmable Gate Arrays (FPGAs), Application-specific Integrated Circuits (ASICs), Application-specific Standard Products (ASSPs), System-on-a-chip systems (SoCs), Complex Programmable Logic Devices (CPLDs), and the like.</p><p id="p-0190" num="0189"><figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates an example method <b>1600</b> for communicating a neural network formation configuration for processing communications transmitted over a wireless communication system. In some implementations, operations of the method <b>1600</b> are performed by a network entity, such as one of the base stations <b>120</b> and/or the core network server <b>302</b>.</p><p id="p-0191" num="0190">At <b>1605</b>, the network entity transmits a neural network table that includes a plurality of neural network formation configuration elements, where each neural network formation configuration element of the plurality of neural network formation configuration elements configures at least a portion of a deep neural network for processing communications transmitted over a wireless communication system. For example, the network entity (e.g., base station <b>121</b>) transmits the neural network table (e.g., neural network table <b>272</b>, neural network table <b>316</b>) to the UE (e.g., UE <b>110</b>) using a broadcast or multicast message to a group of UEs. As another example, the network entity (e.g., base station <b>121</b>) transmits the neural network table to the UE (e.g., UE <b>110</b>) using a UE-dedicated message. In some implementations, the network entity (e.g., core network server <b>302</b>) communicates the neural network table to a base station (e.g., base station <b>121</b>), and directs the base station to transmit the neural network table to the UE (e.g., UE <b>110</b>). At times, the network entity transmits the neural network table using layer 3 messaging. Alternately or additionally, the network entity transmits multiple neural network tables, where each neural network table has a designated processing assignment as further described.</p><p id="p-0192" num="0191">At <b>1610</b>, the network entity selects one or more neural network formation configuration elements from the plurality of neural network formation configuration elements to create a neural network formation configuration. For instance, the network entity (e.g., base station <b>121</b>, core network server <b>302</b>) selects the neural network formation configuration elements by comparing current operating environment(s) with input characteristics stored in the neural network table, and selecting the neural network formation configuration elements by correlating or matching the input characteristics to the current operating environment. In some implementations, the network entity selects a set of candidate neural network formation configurations.</p><p id="p-0193" num="0192">At <b>1615</b>, the network entity transmits an indication to a user equipment to direct the user equipment to form a deep neural network using the neural network formation configuration and to process communications using the deep neural network. As one example, the network entity (e.g., base station <b>121</b>) determines index value(s) of the neural network table that corresponds to the neural network formation configuration and/or a set of neural network formation configuration elements, and transmits the index value(s) to UE <b>110</b>, such as by transmitting the index value(s) using a downlink control channel, by transmitting the index value(s) in layer 2 message(s), etc. As another example, the network entity (e.g., core network server <b>302</b>) communicates an indication to a base station (e.g., base station <b>121</b>), and directs the base station to transmit the indication to the user equipment (e.g., UE <b>110</b>). The network entity alternately or additionally indicates a processing assignment for the deep neural network formed with the neural network formation configuration, such as by indicating a processing assignment that directs the user equipment to process downlink control channel communications with the deep neural network.</p><p id="p-0194" num="0193">In transmitting the index value(s), the network entity sometimes specifies rule(s) on when to process communications with the corresponding deep neural network. As one example, the network entity (e.g., base station <b>121</b>, core network server <b>302</b>) determines a time threshold value between data channel communications and control channel communications. When transmitting the index value to direct the user equipment (e.g., UE <b>110</b>) to form a deep neural network, the network entity transmits the time value threshold and a rule that directs the user equipment to form the deep neural network using a default formation configuration when a timing between the data channel communications and the control channel communications is below the time threshold value. Additionally, or alternatively, a rule may direct the user equipment to use the same neural network formation configuration for data channel communications and control channel communications when a data channel and control channel are within a time threshold value and so there is not enough time to switch between different DNNs.</p><p id="p-0195" num="0194"><figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates an example method <b>1700</b> for communicating a neural network formation configuration for processing communications transmitted over a wireless communication system. In some implementations, operations of the method <b>1700</b> are performed by a user equipment, such as UE <b>110</b>.</p><p id="p-0196" num="0195">At <b>1705</b>, a user equipment receives a neural network table that includes a plurality of neural network formation configuration elements that provide a user equipment with an ability to configure a deep neural network for processing communications transmitted over a wireless communication system. The user equipment (e.g., UE <b>110</b>), for example, receives the neural network table (e.g., neural network table <b>272</b>) from a base station (e.g., base station <b>121</b>) in layer 3 messaging. As another example, the user equipment receives the neural network table in a multicast or broadcast message. Alternately or additionally, the user equipment receives the neural network table in a UE-dedicated message. In some cases, the user equipment receives multiple neural network tables, where each neural network table has a designated processing assignment.</p><p id="p-0197" num="0196">At <b>1710</b>, the user equipment receives a message that directs the user equipment to form the deep neural network using a neural network formation configuration based on one or more neural network formation configuration elements in the plurality of neural network formation configuration elements. For example, the user equipment (e.g., UE <b>110</b>) receives, in the message, a set of index values to the neural network table, where the set of index values corresponds to a set of neural network formation configuration elements, such as that described at <b>1325</b> of <figref idref="DRAWINGS">FIG. <b>13</b></figref>, at <b>1435</b> of <figref idref="DRAWINGS">FIG. <b>14</b></figref>, and/or at <b>1520</b> of <figref idref="DRAWINGS">FIG. <b>15</b></figref>. In implementations, the user equipment (e.g., UE <b>110</b>) receives a downlink control channel message that includes index value(s) corresponding to an entry (or entries) in the neural network table. In some implementations, the message includes an indication of a processing assignment for a deep neural network formed with the neural network formation configuration, such as a communication channel processing assignment, a processing chain processing assignment, etc.</p><p id="p-0198" num="0197">At <b>1715</b>, the user equipment forms the deep neural network with the neural network formation configuration by accessing the neural network table to obtain the neural network formation configuration elements. In one example, the user equipment (e.g., UE <b>110</b>) accesses the neural network table using the index value(s). Alternatively or additionally, in some implementations, when forming the deep neural network, the user equipment (e.g., UE <b>110</b>) determines to process a first communication channel using a first deep neural network formed with the neural network formation configuration, and identifies a second communication channel that is quasi-correspondent to the first communication channel. In response to identifying the second communication channel is quasi-correspondent, the user equipment (e.g., UE <b>110</b>) determines to process the second communication channel using a second deep neural network formed with the neural network formation configuration.</p><p id="p-0199" num="0198">In response to forming the deep neural network, the user equipment processes the communications transmitted over the wireless communication system using the deep neural network at <b>1720</b>. For example, the user equipment (e.g., UE <b>110</b>), processes a downlink communication channel using the deep neural network.</p><p id="p-0200" num="0199">Having described example methods that can be used to communicate neural network formation configurations, consider now a discussion of E2E ML for wireless networks that is in accordance with one or more implementations.</p><p id="p-0201" num="0200">E2E ML for Wireless Networks</p><p id="p-0202" num="0201">Aspects of an end-to-end communication (E2E communication) involve two endpoints exchanging information over a communication path, such as through a wireless network. At times, the E2E communication performs a single-directional exchange of information, where a first endpoint sends information and a second endpoint receives the information. Other times, the E2E communication performs bi-directional exchanges of information, where both endpoints send and receive the information. The endpoints of an E2E communication can include any entity capable of consuming and/or generating the information, such as a computing device, an application, a service, and so forth. To illustrate, consider an example in which an application executing at a UE exchanges information with a remote service over a wireless network. In this example, the E2E communication corresponds to the communication path between the application and the remote service, where the application and the remote service act as endpoints.</p><p id="p-0203" num="0202">While the E2E communication involves endpoints that exchange information, the E2E communication alternately or additionally includes intermediate, entities (e.g., devices, applications, services) that participate in the exchange of information. To illustrate, consider again the example of an E2E communication established through a wireless network where an application at a UE functions as a first endpoint and a remote service functions as a second endpoint. In establishing the E2E communication between the endpoints, the wireless network utilizes any combination of UE(s), base station(s), core network server(s), remote network(s), remote service(s), and so forth, such as that described with reference to the environment <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Thus, intermediary entities, such as the base station <b>120</b> and the core network <b>150</b>, participate in establishing the E2E communication and/or participating in the E2E communication to enable an exchange of information between the endpoints.</p><p id="p-0204" num="0203">Different factors impact the operational efficiency of the E2E communication and how the network elements process information exchanged through the E2E communication. For instance, with reference to an E2E communication established using a wireless network, a current operating environment (e.g., current channel conditions, UE location, UE movement, UE capabilities) impacts how accurately (e.g., bit error rate, packet loss) a receiving endpoint recovers the information. As one example, an E2E communication implemented using 5G mmW technologies becomes susceptible to more signal distortions relative to lower frequency sub-6 GHz signals as further described.</p><p id="p-0205" num="0204">As another example, various implementations partition wireless network resources differently based on an end-to-end analysis of an E2E communication, where the wireless network resources include any combination of, by way of example and not of limitation, physical hardware, physical spectrum, logical channels, network functions, services provided, quality of service, latency, and so forth. Wireless network-resource partitioning allows the wireless network to dynamically allocate the wireless network resources based on an expected usage to improve an efficiency of how the wireless network resources are used (e.g., reduce the occurrence of unused and/or wasted resources). To illustrate, consider a variety of devices connecting to a wireless network, where the devices have different performance requirements relative to one another (e.g., a first device has secure data transfer requirements, a second device has high priority/low latency data transfer requirements, a third device has high data rate requirements). For at least some devices, a fixed and/or static distribution of wireless network resources (e.g., a fixed configuration for the wireless network resources used to implement an E2E communication) can lead to unused resources and/or fail to meet the performance requirements of some services. Thus, partitioning the wireless network resources can improve an overall efficiency of how the wireless network resources are utilized. However, the partitioning causes variations in how one pair of E2E endpoints exchanges information relative to a second pair of E2E endpoints.</p><p id="p-0206" num="0205">To further demonstrate, consider a Quality-of-Service flow (QoS flow) that corresponds to information exchanged in a wireless network. In various implementations, an E2E communication includes and/or corresponds to a QoS flow. Some wireless networks configure a QoS flow with operating rules, priority levels, classifications, and so forth, that influence how information is exchanged through the QoS flow. For example, a QoS profile indicates to a wireless network the QoS parameters and/or QoS characteristics of a particular QoS flow, such as a Guaranteed Flow Bit Rate (GFBR) parameter used to indicate an uplink and/or downlink guaranteed bit rate for the QoS flow, a Maximum Flow Bit Rate (MFBR) parameters used to indicate a maximum uplink and/or downlink bit rate for the QoS flow, an Allocation and Retention Priority (ARP) parameter that indicates a priority level, a pre-emption capability, and/or pre-emption vulnerability of the QoS flow, a Reflective QoS attribute (RQA) that indicates a type of traffic carried on the QoS flow is subject to Reflective QoS (e.g., implicit updates), a Notification Control parameter that indicates whether notifications are requested when a guaranteed flow bit rate cannot be guaranteed, or resumes, for the QoS flow, an aggregate bit rate parameter that indicates an expected aggregate bit rate for the collective non-guaranteed-bit-rate (Non-GBR) flows associated with a particular UE, default parameters for 5QI and ARP priority levels, a Maximum Packet Loss Rate (MPLR) for uplink and/or downlink that indicates a maximum rate for lost packets of the QoS flow, a Resource Type characteristic that indicates types of resources that can be used by the QoS flow (e.g., GBR resource type, Delay-critical GBR resource type, non-GBR resource type), a scheduling priority level characteristic that distinguishes between multiple QoS flows of a same UE, a Packet Delay Budget characteristic that provides an upper bound to how long a packet may be delayed, a Packet Error Rate characteristic that indicates an upper bound for a rate of PDUs unsuccessfully received, an Averaging Window characteristics that indicates a window of data over which to calculate the GFBR and/or MFBR, a Maximum Data Burst Volume characteristic that indicates a largest amount of data that is required to be served over a pre-defined time period, and so forth. In some implementations, the parameters and/or characteristics that specify the configuration of a QoS flow can be pre-configured (e.g., default) and/or dynamically communicated, such as through the QoS profile. These variations impact how the wireless network partitions the various wireless network resources to support the QoS flow configuration.</p><p id="p-0207" num="0206"><figref idref="DRAWINGS">FIG. <b>18</b></figref> illustrates an example environment <b>1800</b> that demonstrates example variations between QoS flows in accordance with one or more implementations. The environment <b>1800</b> includes the UE <b>110</b> and the base station <b>120</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, and the core network server <b>302</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In the environment <b>1800</b>, the core network server <b>302</b> implements aspects of a User Plane Function <b>1802</b> (UPF <b>1802</b>), but it is to be appreciated that alternate or additional devices can be utilized to implement the UPF. The UPF <b>1802</b> provides various services and/or features to the wireless network system, such as packet forwarding, packet routing, interconnection to a data network (e.g., the Internet), data buffering, packet inspection, QoS handling, policy enforcement, etc.</p><p id="p-0208" num="0207">In the environment <b>1800</b>, the UE <b>110</b> establishes multiple QoS flows through the UPF <b>1802</b> to gain access to a data network. For example, the UE <b>110</b> includes three applications: an application <b>1804</b>, an application <b>1806</b>, and an application <b>1808</b>, where each application has a different performance requirement (e.g., resource type, priority level, packet delay budget, packet error rate, maximum data burst volume, averaging window, security level). These different performance requirements cause the wireless network to partition the wireless network resources assigned to the corresponding QoS flows differently from one another. For instance, consider a scenario in which application <b>1804</b> corresponds to a gaming application, application <b>1806</b> corresponds to an augmented reality application, and application <b>1808</b> corresponds to a social media application. In some instances, the gaming application interacts with a remote service (through the data network) to connect with another gaming application to exchange audio in real-time, video in real-time, commands, views, and so forth, such that the gaming application has performance requirements with high data volume and low latency. The augmented reality application also interacts with a remote service through the data network to transmit location information and subsequently receive image data that overlays on top of a camera image generated at the UE <b>110</b>. Relative to the gaming application, the augmented reality application utilizes less data, but has some time-sensitivity to maintain synchronization between a current location and a corresponding image overlay. Finally, the social media application interacts with a remote service through the data network to receive feed information, where the feed information has less data volume and time-criticality relative to data consumed by the augmented reality application and/or the gaming application.</p><p id="p-0209" num="0208">Based upon these performance requirements, the wireless network establishes QoS flows between the applications and the data network (through the UPF), where each QoS flow has a different configuration associated with partitioning the wireless network resources. For instance, QoS flow <b>1810</b> connects the application <b>1804</b> to the data network through the UPF <b>1802</b>, where the wireless network constructs the QoS flow based on QoS requirements, QoS parameters and/or QoS characteristics (e.g., resource type, priority level, packet delay budget, packet error rate, maximum data burst volume, averaging window, security level) that indicate a high data volume performance requirement and a time-sensitivity performance requirement. In implementations, the QoS requirements, the QoS parameters and/or the QoS characteristics included in a QoS profile correspond to the performance requirements of the QoS flow.</p><p id="p-0210" num="0209">To illustrate, the wireless network (e.g., the base station <b>120</b>, the core network server <b>302</b>) process a QoS profile associated with the QoS flow <b>1810</b> that configures any combination of a GFBR parameter, a Maximum Data Burst Volume characteristic, an ARP parameter, and so forth, based on the performance requirements of the QoS flow <b>1810</b>. Afterwards, the wireless network constructs the QoS flow by partitioning the wireless network resources based on the QoS parameters and/or characteristics, where the QoS flow <b>1810</b> connects the application <b>1804</b> to the data network through the UPF <b>1802</b>. Similarly, QoS flow <b>1812</b> connects the application <b>1806</b> to the data network, through the UPF <b>1802</b>, where the wireless network partitions the wireless network resources used by the QoS flow based upon the performance requirements associated with the application <b>1806</b>. Finally, QoS flow <b>1814</b> connects the application <b>1808</b> to the data network (through the UPF <b>1802</b>), where the wireless network partitions the wireless network resources used by the QoS flow based upon the performance requirements associated with the application <b>1808</b>, such as by processing a QoS profile where the QoS parameters and/or characteristics reflect the lesser data volume and lesser time-critical needs of the QoS flow <b>1814</b>. In various implementations, the UPF <b>1802</b> acts as an edge of the wireless network insofar as the UPF <b>1802</b> provides connectivity to the data networks external to the wireless network. Thus, within the wireless network, the application <b>1804</b> and the UPF <b>1802</b> act as endpoints of an E2E communication based on the QoS flow, <b>1810</b>, the application <b>1806</b> and the UPF <b>1802</b> act as endpoints of an E2E communication based on the QoS flow <b>1812</b>, and the application <b>1808</b> and the UPF <b>1802</b> act as endpoints of an E2E communication based on the QoS flow <b>1814</b>.</p><p id="p-0211" num="0210">The environment <b>1800</b> illustrates an overview of an example architecture used by a wireless network to construct the QoS flows. In the example architecture, the wireless network constructs a Protocol Data Unit session (PDU session <b>1816</b>) that supplies the data connectivity between the UE <b>110</b> and through the UPF <b>1802</b> to an external data network. In implementations, the PDU session transfers data based on a particular service type, such as an Internet Protocol (IP) service, and Ethernet type service, unstructured type service, and so forth. In this example, the QoS flow <b>1810</b>, the QoS flow <b>1812</b>, and the QoS flow <b>1814</b> use a same service type and belong to the PDU session <b>1816</b> which corresponds to transporting data associated with the service type.</p><p id="p-0212" num="0211">To support the data connectivity, data radio bearer <b>1818</b> and data radio bearer <b>1820</b> provide a radio interface that connects the UE <b>110</b> and the base station <b>120</b>. In some implementations, the data radio bearer <b>1818</b> and the data radio bearer <b>1820</b> communicate information over the radio interface with different techniques, resource allocations, etc. Accordingly, the example architecture maps each QoS flow to a data radio bearer with a configuration that supports the respective performance requirement(s) of the QoS flow. In this example, the wireless network maps data radio bearer <b>1818</b> to the QoS flow <b>1810</b> and the QoS flow <b>1812</b>, and the data radio bearer <b>1820</b> to the QoS flow <b>1814</b>. Thus, a data radio bearer can support multiple QoS flows.</p><p id="p-0213" num="0212">The tunnel <b>1822</b> corresponds to non-radio interface functionality associated with the QoS flows provided by the UPF <b>1802</b>, such as packet forwarding. In implementations, the tunnel <b>1822</b> corresponds to a tunneling protocol used to transport data between entities. Thus, the base station <b>120</b> communicates information exchanged on different QoS flows to data networks through the UPF <b>1802</b> using the tunnel <b>1822</b>. In implementations, the tunnel <b>1822</b> provides the wireless network with an ability to exchange traffic associated with the PDU session <b>1816</b>.</p><p id="p-0214" num="0213">While the configurability of the QoS flows provide flexibility to the wireless network to dynamically modify how the wireless network resources are allocated, the configurability adds complexity in how the wireless network processes information that is exchanged between the endpoints. Some implementations train DNNs to perform some or all of the complex processing associated with exchanging information using E2E communications with various configurations. By training a DNN on the differing processing chain operations and/or wireless network resource partitioning, the DNN can replace the conventional complex functionality as further described. The usage of DNNs in an E2E communication also allows a network entity to adapt the DNN to changing operating conditions, such as by modifying various parameter configurations (e.g., coefficients, layer connections, kernel sizes).</p><p id="p-0215" num="0214">One or more implementations determine an E2E ML configuration (E2E ML configuration) for processing information exchanged through an E2E communication. For example, with reference to <figref idref="DRAWINGS">FIG. <b>18</b></figref>, some implementations determine a first E2E ML configuration for processing communications exchanged using QoS flow <b>1810</b>, a second E2E ML configuration for processing communications exchanged using QoS flow <b>1812</b>, and a third E2E ML configuration for processing communications exchanged using QoS flow <b>1814</b>. In some cases, an end-to-end machine-learning controller (E2E ML controller) obtains capabilities of device(s) associated with end-to-end communications in a wireless network, such as machine-learning (ML) capabilities of device(s) participating in the E2E communication, and determines an E2E ML configuration based on the ML capabilities (e.g., supported ML architectures, supported number of layers, available processing power, memory limitation, available power budget, fixed-point processing vs. floating point processing, maximum kernel size capability, computation capability) of the device(s). Alternately or additionally, the E2E ML controller identifies a current operating environment and determines the E2E ML configuration based on the current operating environment. In determining the E2E ML configuration, some implementations of the E2E ML controller partition the E2E ML configuration based on the device(s) participating in the E2E communication and communicate a respective partition of the E2E ML configuration to each respective device.</p><p id="p-0216" num="0215">To demonstrate, consider <figref idref="DRAWINGS">FIG. <b>19</b></figref> that illustrates an example environment <b>1900</b> in which example E2E ML configurations for E2E communications include DNNs operating at multiple devices. The environment <b>1900</b> includes the UE <b>110</b>, the base station <b>120</b>, and the remote service <b>170</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, and the core network server <b>302</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In implementations, the UE <b>110</b> and the remote service <b>170</b> exchange information with one another using E2E communication <b>1902</b> and E2E communication <b>1904</b>. For clarity, the E2E communications <b>1902</b> and <b>1904</b> are illustrated as being separate and single-directional E2E communications such that the exchange of information over each communication E2E corresponds to one direction (e.g., E2E communication <b>1902</b> includes uplink transmissions, E2E communication <b>1904</b> includes downlink transmissions), but it is to be appreciated that in alternate or additional implementations, the E2E communication <b>1902</b> and the E2E communication <b>1904</b> correspond to a single, bi-directional E2E communication that includes both, signified in the environment <b>1900</b> with a dashed line <b>1906</b>. In implementations, the E2E communication <b>1902</b> and the E2E communication <b>1904</b> (in combination) correspond to a single QoS flow, such as one of the QoS flow <b>1810</b>, the QoS flow <b>1812</b>, or the QoS flow <b>1814</b> of <figref idref="DRAWINGS">FIG. <b>18</b></figref>. Accordingly, each of the multiple QoS flows illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref> has a respective E2E ML configuration.</p><p id="p-0217" num="0216">The environment <b>1900</b> also includes the E2E ML controller <b>318</b> that is implemented by the core network server <b>302</b>, where the E2E ML controller <b>318</b> determines an E2E ML configuration for the E2E communication <b>1902</b> and/or the E2E communication <b>1904</b>. In some implementations, the E2E ML controller determines a first E2E ML configuration for the E2E communication <b>1902</b> and a second E2E ML configuration for the E2E communication <b>1904</b>, such as when each E2E communication corresponds to single-directional information exchanges. In other implementations, the E2E ML controller determines an E2E ML configuration for a bi-directional E2E communication that includes both E2E communications <b>1902</b> and <b>1904</b>. For example, in response to the UE <b>110</b> requesting a connection to the remote server <b>170</b>, such as through the invocation of an application, the E2E ML controller determines an E2E ML configuration for a corresponding connection based on any combination of ML capabilities of the UE <b>110</b> (e.g., supported ML architectures, supported number of layers, processing power available for ML processing, memory constraints applied to ML processing, power budget available for ML processing, fixed-point processing vs. floating point processing), performance requirements associated with the requested connection (e.g., resource type, priority level, packet delay budget, packet error rate, maximum data burst volume, averaging window, security level), available wireless network resources, ML capabilities of intermediary devices (e.g., the base station <b>120</b>, the core network server <b>302</b>), a current operating environment (e.g., channel conditions, UE location), and so forth. As one example, with reference to <figref idref="DRAWINGS">FIGS. <b>9</b>-<b>1</b>, <b>9</b>-<b>2</b></figref>, and <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the E2E ML controller <b>318</b>, by way of the core network server <b>302</b>, receives base station metrics and/or UE metrics that describe the current operating environment. As another example, with reference to <figref idref="DRAWINGS">FIG. <b>22</b></figref>, the E2E ML controller, by way of the core network server <b>302</b>, receives base station ML capabilities and/or UE capabilities.</p><p id="p-0218" num="0217">In one or more implementations, the E2E ML controller <b>318</b> analyzes a neural network table based upon any combination of the device capabilities, the wireless network resource partitioning, the operating parameters, the current operating environment, the ML capabilities, and so forth, to determine the E2E ML configuration. While described as being implemented by the core network server <b>302</b>, in alternate or additional implementations, the E2E ML controller <b>318</b> may be implemented by another network entity, such as the base station <b>120</b>.</p><p id="p-0219" num="0218">To illustrate, and with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the training module <b>270</b> and/or the training module <b>314</b> train the machine-learning module <b>400</b> with variations of the training data <b>1202</b> that reflect different combinations of the capabilities, wireless network resource partitioning, the operating parameters, the current operating environment, and so forth. The training module extracts and stores the architecture and/or parameter configurations (e.g., architecture and/or parameter configurations <b>1208</b>) in a neural network table such that, at a later point in time, the E2E ML controller <b>318</b> accesses the neural network table to obtain and/or identify neural network formation configurations that correspond to a determined E2E ML configuration. The E2E ML controller <b>318</b> then communicates the neural network formation configurations to various devices and directs the respective device to form a respective DNN as further described.</p><p id="p-0220" num="0219">In determining the E2E ML configuration, the E2E ML controller <b>318</b> sometimes partitions the E2E ML configuration based on devices participating in the corresponding E2E communication. For example, the E2E ML controller <b>318</b> determines a first partition of the E2E ML configuration that corresponds to processing information at the UE <b>110</b>, a second partition of the E2E ML configuration that corresponds to processing information at the base station <b>120</b>, and a third partition of the E2E ML configuration that corresponds to processing information at the core network server <b>302</b>, where determining the partitions can be based on any combination of the capabilities, wireless network resource partitioning, the operating parameters, the current operating environment, and so forth.</p><p id="p-0221" num="0220">As one example, consider an E2E communication that corresponds to voice transmissions over a wireless network, such as the E2E communication <b>1902</b>, the E2E communication <b>1904</b>, and/or a combination of both E2E communications. In determining an E2E ML configuration for the E2E communication, the E2E ML controller <b>318</b> alternately or additionally identifies that performance requirement(s) of the E2E communication indicates large volumes of data transfer with low latency requirements. Based on the performance requirement(s), the E2E ML controller identifies an E2E ML configuration that, when formed by the respective DNN(s), performs end-to-end functionality that exchanges voice communications and satisfies the performance requirement(s). To illustrate, the E2E ML controller determines an E2E ML configuration that performs end-to-end functionality for transmitting voice from a UE to a core network server, such as signal processing, voice encoding, channel encoding, and/or channel modulation at the UE side, channel decoding, demodulation, and/or signal processing at the base station side, decoding voice at the core network server side, and so forth, and selects a configuration designed to satisfy the performance requirements.</p><p id="p-0222" num="0221">Some implementations partition an E2E ML configuration based the ML capabilities of devices participating in the E2E communication and/or the performance requirements. A UE, for instance, may have less processing resources (e.g., processing capabilities, memory constraints, quantization constraints, fixed-point vs. floating point computations, FLOPS, power availability relative to a base station and/or a core network server, which can be indicated through the ML capabilities. In response to identifying the different processing resources through an analysis of the ML capabilities, the E2E ML controller partitions the E2E ML configuration such that a first partition (e.g., at the UE <b>110</b>) forms a DNN that performs less processing than a DNN formed by a second or third partition (e.g., at the base station, at the core network server). Alternately or additionally, the E2E ML controller partitions the E2E ML configuration to produce neural networks designed to not exceed device capabilities. For example, based on analyzing the capabilities, the E2E ML controller directs the UE to form a DNN with less layers and a smaller kernel size relative to a DNN formed by the base station and/or the core network server based on processing constraints of the UE. Alternately or additionally, the E2E ML controller partitions the E2E ML configuration to form, at the UE) a neural network with an architecture (e.g., a convolutional neural network, a long short-term memory (LSTM) network, partially connected, fully connected) that processes information without exceeding memory constraints of the UE. In some instances, the E2E ML controller calculates whether an amount of computation performed at each device collectively meets a performance requirement corresponding to a latency budget and determines a E2E ML configuration designed to meet the performance requirement.</p><p id="p-0223" num="0222">In the environment <b>1900</b>, the E2E ML controller <b>318</b> determines a first E2E ML configuration for processing information exchanged through the E2E communication <b>1902</b> and determines to partition the first E2E ML configuration across multiple devices such as by partitioning the first E2E ML configuration across the UE <b>110</b>, the base station <b>120</b>, and the core network server <b>302</b> based on device capabilities. In other words, some implementations determine an E2E ML configuration that corresponds to a distributed DNN in which multiple devices implement and/or form portions of the DNN. To communicate the partitioning, the E2E ML controller <b>318</b> identifies a first neural network formation configuration (NN formation configuration) that corresponds to a first partition of the E2E ML configuration and communicates, by using the core network server <b>302</b>, the first NN formation configuration to the UE <b>110</b>. The E2E ML controller <b>318</b> and/or the core network server <b>302</b> then directs the UE to form a user equipment-side deep neural network <b>1908</b> (UE-side DNN <b>1908</b>) for processing information exchanged through the E2E communication <b>1902</b>. Similarly, the E2E ML controller <b>318</b> identifies a second NN formation configuration that corresponds to a second partition of the E2E ML configuration and communicates the second NN formation configuration to the base station <b>120</b>. The E2E ML controller <b>318</b> and/or the core network server <b>302</b> then directs the base station <b>120</b> to form, using the second NN formation configuration, a base station-side deep neural network <b>1910</b> (BS-side DNN <b>1910</b>) for processing information exchanged through the E2E communication <b>1902</b>. The E2E ML controller <b>318</b> also identifies and communicates a third NN formation configuration to the core network server <b>302</b> to use in forming a core network server-side deep neural network <b>1912</b> (CNS-side DNN <b>1912</b>) for processing information exchanged through the E2E communication <b>1902</b>.</p><p id="p-0224" num="0223">In implementations, the E2E ML controller <b>318</b> partitions the E2E ML configuration to distribute processing computations performed over the E2E communication such that the UE-side DNN <b>1908</b> performs less processing relative to the BS-side DNN <b>1910</b> (e.g., a UE-side DNN <b>1908</b> that uses less layers, less data processing points, and so forth, relative to the BS-side DNN <b>1910</b>). Alternately or additionally, the E2E ML controller <b>318</b> partitions the E2E ML configuration such that the BS-side DNN <b>1910</b> performs less processing relative to CNS-side DNN <b>1912</b>. In combination, the processing performed by the UE-side DNN <b>1908</b>, the BS-side DNN <b>1910</b>, and the CNS-side DNN <b>1912</b> exchange information across the E2E communication <b>1902</b>.</p><p id="p-0225" num="0224">In a similar manner, the E2E ML controller <b>318</b> determines a second E2E ML configuration for processing information exchanged through the E2E communication <b>1904</b>, where the E2E ML controller partitions the second E2E ML configuration across multiple devices. In the environment <b>1900</b>, this partitioning corresponds to a core network server-side deep neural network <b>1914</b> (CNS-side DNN <b>1914</b>), a base station-side deep neural network <b>1916</b> (BS-side DNN <b>1916</b>), and a user equipment-side deep neural network <b>1918</b> (UE-side DNN <b>1918</b>). In combination, the processing performed by the CNS-side DNN <b>1914</b>, the BS-side DNN <b>1916</b>, and the UE-side DNN <b>1918</b> corresponds to exchanging information using the E2E communication <b>1904</b>. While the E2E ML controller determines the first and second E2E ML configurations separately in the environment <b>1900</b> for single-directional E2E communications (e.g., the E2E communications <b>1902</b> and <b>1904</b>), it is to be appreciated that in alternate or additional implementations, the E2E ML controller <b>318</b> determines a single E2E ML configuration that corresponds to exchanging bi-directional information using an E2E communication. For example, the E2E controller <b>318</b> may determine a first (bi-directional) E2E ML configuration for the QoS flow <b>1810</b>, a second (bi-directional) E2E ML configuration for the QoS flow <b>1812</b>, and a third (bi-directional) E2E ML configuration for the QoS flow <b>1814</b> of <figref idref="DRAWINGS">FIG. <b>18</b></figref>.</p><p id="p-0226" num="0225"><figref idref="DRAWINGS">FIG. <b>20</b></figref> illustrates another example environment <b>2000</b> in which an E2E ML configuration for a E2E communication includes DNNs operating across multiple devices. The environment <b>2000</b> includes a first instance of the UE <b>110</b> (designated UE <b>2002</b>), a first instance of the base station <b>120</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> (designated base station <b>2004</b>), the core network server <b>302</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a second instance of the base station <b>120</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> (designated base station <b>2006</b>), and a second instance of the UE <b>110</b> (designated UE <b>2008</b>).</p><p id="p-0227" num="0226">In the environment <b>2000</b>, the core network server <b>302</b> operates in a wireless network that exchanges information between the UE <b>2002</b> and the UE <b>2008</b> (through the base station <b>2004</b> and the base station <b>2006</b>). E2E communication <b>2010</b> represents a single direction E2E communication between a first endpoint (e.g., UE <b>2002</b>) and a second endpoint (e.g., UE <b>2008</b>) and E2E communication <b>2012</b> represents a single-direction E2E communication between the UE <b>2008</b> and the UE <b>2002</b>. However, as signified by dashed line <b>2014</b>, the E2E communication <b>2010</b> and the E2E communication <b>2012</b> can collectively represent a bi-directional E2E communication as further described.</p><p id="p-0228" num="0227">Similar to that described with reference to <figref idref="DRAWINGS">FIG. <b>19</b></figref>, the E2E ML controller <b>318</b> determines a first E2E ML configuration for processing (single-directional) communications exchanged using the E2E communication <b>2010</b>, a second E2E ML configuration for processing (single-directional) communications exchanged using the E2E communication <b>2012</b>, and/or an E2E ML communication for processing (bi-directional) communications exchanged using the E2E communication <b>2010</b> and the E2E communication <b>2012</b>. In determining an E2E ML configuration, E2E ML controller <b>318</b> analyzes any combination of device capabilities, wireless network resource partitioning, operating parameters, a current operating environment, performance requirements, etc. Alternately or additionally, the E2E ML controller <b>318</b> determines partitions to the E2E ML configuration to distribute processing functionality associated with the E2E ML configuration across multiple devices.</p><p id="p-0229" num="0228">To illustrate, for processing information exchanged through the E2E communication <b>2010</b>, the UE <b>2002</b> receives, from the E2E ML controller <b>318</b> by way of the core network server <b>302</b>, a first neural network formation configuration that corresponds to a first partition of the E2E ML configuration and maps to entries in a neural network table that can be used to form a user equipment-side deep neural network <b>2016</b> (UE-side DNN <b>2016</b>). The base station <b>2004</b> receives a second neural network formation configuration that corresponds to a second partition of the E2E ML configuration and can be used to form a base station-side deep neural network <b>2018</b> (BS-side DNN <b>2018</b>). In a similar manner, the core network server <b>302</b> forms a core network-side deep neural network <b>2020</b> (CNS-side DNN <b>2020</b>) based on a third neural network formation configuration, the base station <b>2006</b> forms a base station-side deep neural network <b>2022</b> (BS-side DNN <b>2022</b>) based on a fourth neural network formation configuration, and the UE <b>2008</b> forms a user equipment-side deep neural network <b>2024</b> (UE-side DNN <b>2024</b>) based on a fifth neural network formation configuration. Thus, in the environment <b>2000</b>, the UE-side DNN <b>2016</b>, the BS-side DNN <b>2018</b>, the CNS-side DNN <b>2020</b>, the BS-side DNN <b>2022</b>, and the UE-side DNN <b>2024</b> correspond to a partitioned E2E ML configuration determined by the E2E ML controller <b>318</b> and, collectively, perform processing that exchanges information from the UE <b>2002</b> to the UE <b>2008</b> using the E2E communication <b>2010</b>.</p><p id="p-0230" num="0229">With respect to the E2E communication <b>2012</b>, the E2E ML controller <b>318</b> determines a partitioned E2E ML configuration and communicates respective portions of the partitioned E2E ML configuration to the devices participating in the E2E communication <b>2012</b>. For exchanging information using the E2E communication <b>2012</b>, the UE <b>2008</b> forms a user equipment-side deep neural network <b>2026</b> (UE-side DNN <b>2026</b>), the base station <b>2006</b> forms a base station-side deep neural network <b>2028</b> (BS-side DNN <b>2028</b>), the core network server <b>302</b> forms a core network-side deep neural network <b>2030</b> (CNS-side DNN <b>2030</b>), the base station <b>2004</b> forms a base station-side deep neural network <b>2032</b> (BS-side DNN <b>2032</b>), and the UE <b>2002</b> forms a user equipment-side deep neural network <b>2034</b> (UE-side DNN <b>2034</b>).</p><p id="p-0231" num="0230">In implementations, the E2E ML controller <b>318</b> periodically reassess metrics, performance requirements, wireless link performance, processing capabilities of devices or other aspects affecting, or providing an indication of, a current operating environment and/or a current performance (e.g., bit errors, BLER) to determine whether to update the E2E ML configuration. For example, the E2E ML controller <b>318</b> determines modifications (e.g., parameter changes) to an existing DNN to better accommodate the performance requirements of devices, applications, and/or transmissions in a wireless network. A UE changing location may impact on the wireless link performance, or a user opening an application at the UE may reduce the processing capability the user equipment can provide for machine learning. By reassessing dynamically changing conditions (e.g., changes in the operating environment, changes in the devices), the E2E ML controller can modify or update the E2E ML configuration to improve an overall efficiency of how the wireless network resources are utilized.</p><p id="p-0232" num="0231">Having described E2E ML for wireless networks, consider now a discussion of signaling and control transactions over a wireless communication system that can be used in various aspects of E2E ML for wireless networks.</p><p id="p-0233" num="0232">Signaling and Control Transactions for E2E ML for Wireless Networks</p><p id="p-0234" num="0233"><figref idref="DRAWINGS">FIGS. <b>21</b>, <b>22</b>, <b>23</b>, and <b>24</b></figref> illustrate example signaling and control transaction diagrams between a core network server, a base station, and a user equipment in accordance with one or more aspects of E2E ML for wireless networks. The signaling and control transactions may be performed by the base station <b>120</b> and the UE <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, or the core network server <b>302</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, using elements of <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>20</b></figref>.</p><p id="p-0235" num="0234">A first example of signaling and control transactions for E2E ML for wireless networks is illustrated by the signaling and control transaction diagram <b>2100</b> of <figref idref="DRAWINGS">FIG. <b>21</b></figref>. As illustrated, at <b>2105</b>, the UE <b>110</b> requests to establish an E2E communication. For instance, the request implicitly requests to establish the E2E communication by requesting to establish a communication path for an application in response to detecting invocation of the application. Example applications include a virtual reality application, an augmented-reality application, a Voice-over-Internet-Protocol (VoIP) application, a real-time gaming application, a video streaming application, an audio streaming application, a file transfer application (e.g., photo/image transfer application, portable document format (PEF) file transfer application), a social media application, and other forms of interactive communications. In various implementations, the UE <b>110</b> explicitly requests to establish the E2E communication, such as by requesting to establish a QoS flow with performance requirements as further described. Alternately or additionally, the UE <b>110</b> requests (implicitly or explicitly) to establish the E2E communication for other types of communications, such as IoT messages and/or communications, vehicle-to-everything (V2X) messages and/or communications, IP Multimedia Core Network Subsystem (IMS) signaling, remote control communications, intelligent transport systems, or discrete automation.</p><p id="p-0236" num="0235">In some implementations, the UE <b>110</b> includes a QoS identifier in the request, where the QoS identifier indicates characteristics about the requested E2E communication. To illustrate, the UE <b>110</b> includes a fifth-generation Quality-of-Service Identifier (5QI) or a Quality-of-Service Class Identifier (QCI) in the request, where the 5GCI and/or the QCI can be analyzed to determine the QoS requirements, QoS parameters, QoS characteristics, and so forth.</p><p id="p-0237" num="0236">At <b>2110</b>, the base station <b>120</b> relays and/or forwards the request to the core network server. In some cases, the base station <b>120</b> generates and sends a new message that encapsulates the request to the core network server <b>302</b>, while in other cases, the base station <b>120</b> relays the request from the UE <b>110</b> to the core network server <b>302</b>.</p><p id="p-0238" num="0237">At <b>2115</b>, the core network server <b>302</b> determines, by way of the E2E ML controller <b>318</b>, an E2E ML configuration. The E2E ML controller, for instance, analyzes performance requirements of an E2E communication, such as the processing requirements associated with establishing the QoS flow <b>1810</b>, the QoS flow <b>1812</b>, the QoS flow <b>1814</b>, the E2E communication <b>1902</b> and/or the E2E communication <b>1904</b>, the E2E communication <b>2010</b> and/or the E2E communication <b>2012</b>, etc. For instance, the E2E ML controller <b>318</b> analyzes a QoS identifier to determine the performance requirements. Alternately or additionally, the E2E ML controller analyzes capabilities of devices participating in the E2E communication, such as that described with reference to <figref idref="DRAWINGS">FIG. <b>22</b></figref>. Sometimes, the E2E ML controller analyzes wireless network resource partitioning associated with the E2E communication. As another example, the E2E ML controller <b>318</b> analyzes metrics that characterize a current operating environment, as described with reference to <figref idref="DRAWINGS">FIG. <b>9</b>-<b>1</b>, <b>9</b>-<b>2</b></figref>, or <b>22</b>, and determines the E2E ML configuration based on the current operating environment. In implementations, the E2E ML configuration corresponds to a single-directional E2E communication, while in other implementations, the E2E communication corresponds to a bi-directional E2E communication.</p><p id="p-0239" num="0238">In implementations, determining the E2E ML configuration includes determining any combination architecture and/or parameter configurations that can be used to create an NN formation configuration (e.g., a combination of one or more NN formation configuration elements) that defines and/or forms a DNN. For example, in some implementations, the E2E ML controller determines a first E2E ML configuration that includes architecture configuration(s) and parameter configuration(s) used to form a DNN. Alternately or additionally, the E2E ML controller subsequently determines a second E2E ML configuration that includes parameter changes (and excludes architecture configurations) used to update the first E2E ML configuration based on a changing operating environment, such as that described with reference to <figref idref="DRAWINGS">FIGS. <b>9</b>-<b>1</b>, <b>9</b>-<b>2</b>, and <b>23</b></figref>.</p><p id="p-0240" num="0239">In some implementations, the core network server <b>302</b>, by way of the E2E ML controller <b>318</b>, analyzes neural network table(s) to obtain one or more neural network formation configurations that correspond to the E2E ML configuration. For instance, the core network server partitions the E2E ML configuration across multiple devices as described with reference to <figref idref="DRAWINGS">FIGS. <b>19</b> and <b>20</b></figref> and determines a respective neural network formation configuration for each partition, where each respective neural network formation configuration maps to one or more entries in the neural network table(s).</p><p id="p-0241" num="0240">At <b>2125</b>, the core network server <b>302</b> communicates the E2E ML configuration to devices associated with the E2E communication. To illustrate, the core network server communicates a first neural network formation configuration to a first device (e.g., the base station <b>120</b>), a second neural network formation configuration to a second device (e.g., the UE <b>110</b> by way of the base station <b>120</b> at <b>2130</b>), and so forth. This includes identifying a neural network configuration that corresponds to forming a DNN at the core network server. In some implementations, the core network server <b>302</b> communicates the neural network formation configurations similar to that described with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0242" num="0241">At <b>2135</b>, the core network server <b>302</b> forms a DNN (e.g., DNN <b>1912</b>, DNN <b>1914</b>, DNN <b>2020</b>, DNN <b>2030</b>) based on the E2E ML configuration determined at <b>2115</b>. In implementations, the DNN formed by the core network server performs at least some processing for exchanging information using the E2E communication. Similarly, at <b>2140</b>, the base station <b>120</b> forms a DNN (e.g., DNN <b>1910</b>, DNN <b>1916</b>, DNN <b>2018</b>, DNN <b>2022</b>, DNN <b>2028</b>, DNN <b>2032</b>) based on the E2E ML configuration determined at <b>2115</b>. For instance, the base station <b>120</b> accesses a neural network table to obtain one or more parameters as described with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref>. In implementations, the DNN formed by the base station <b>120</b> performs at least some processing for exchanging information using the E2E communication. At <b>2145</b>, the UE <b>110</b> forms a DNN (e.g., DNN <b>1908</b>, DNN <b>1918</b>, DNN <b>2016</b>, DNN <b>2024</b>, DNN <b>2026</b>, DNN <b>2034</b>) based on the E2E ML configuration determined at <b>2115</b>. For instance, the UE <b>110</b> accesses a neural network table to obtain one or more parameters as described with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref>. In implementations, the DNN formed by the UE <b>110</b> performs at least some processing for exchanging information using the E2E communication.</p><p id="p-0243" num="0242">Afterwards, at <b>2150</b>, at least the core network server <b>302</b>, the base station <b>120</b>, and the UE <b>110</b> process communications using the DNNs, where the communications correspond to information exchanged through the E2E communication. For example, with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the DNNs at the devices process uplink and/or downlink information.</p><p id="p-0244" num="0243">A second example of signaling and control transactions for E2E ML for wireless networks is illustrated by the signaling and control transaction diagram <b>2200</b> of <figref idref="DRAWINGS">FIG. <b>22</b></figref>. As illustrated, at <b>2205</b> the core network server <b>302</b> requests machine-learning (ML) capabilities from one or more devices in a wireless network. The E2E ML controller <b>318</b>, for example, initiates a request for ML capabilities, where the core network server <b>302</b> sends the request to the base station <b>120</b> through NG2 interface, an S1 interface, etc. In some implementations, the capabilities request explicitly indicates a request for ML capabilities, such as through the inclusion of a field and/or information element in the request. In other implementations the capabilities request implicitly indicates a request for ML capabilities, such as through a capability request without any explicit ML capability fields and/or information elements. Various implementations request, explicitly or implicitly, the ML capabilities as part of constructing an E2E communication, such as through a Non-Access Stratum (NAS) message used to establish the E2E communication and/or a QoS flow.</p><p id="p-0245" num="0244">At <b>2210</b>, the base station <b>120</b> transmits a request for ML capabilities to the UE <b>110</b>, where the request explicitly or implicitly requests ML capabilities. As one example, the base station <b>120</b> relays the ML capabilities request from the core network server <b>302</b>. As another example, the base station <b>120</b> generates and transmits an ML capabilities request message to the UE <b>110</b> in response to receiving the ML capabilities request from the core network server <b>302</b>. In various implementations, the base station <b>120</b> transmits an explicit or implicit request for the ML capabilities as part of establishing the E2E communication.</p><p id="p-0246" num="0245">At <b>2215</b>, the base station <b>120</b> sends base station machine-learning capabilities (BS ML capabilities) to the core network server <b>302</b>. Similarly, at <b>2220</b>, the UE <b>110</b> transmits user equipment machine-learning capabilities (UE ML capabilities) to the base station, where the base station <b>120</b> relays the UE ML capabilities to the core network server at <b>2225</b>. At times, the core network server forwards the BS ML capabilities and/or the UE ML capabilities to the E2E ML controller <b>318</b>. The BS ML capabilities and/or the UE ML capabilities include any combination of capabilities information, such as supported ML architectures, supported number of layers, available processing power, memory limitations, maximum kernel size, available power budget, computation capability and so forth.</p><p id="p-0247" num="0246">At <b>2230</b>, the core network server <b>302</b> determines, by way of the E2E ML controller <b>318</b>, an E2E ML configuration. The E2E ML controller, for instance, analyzes performance requirements of an E2E communication, such as the processing requirements of the QoS flow <b>1810</b>, the QoS flow <b>1812</b>, the QoS flow <b>1814</b>, the E2E communication <b>1902</b> and/or the E2E communication <b>1904</b>, the E2E communication <b>2010</b> and/or the E2E communication <b>2012</b>, etc. Alternately or additionally, the E2E ML controller analyzes capabilities of devices participating in the E2E communication, such as the BS ML capabilities received by the core network server at <b>2215</b>, the UE ML capabilities received by the core network server at <b>2225</b>, processing capabilities, etc. Sometimes the E2E ML controller analyzes wireless network resource partitioning associated with the E2E communication. As another example, the E2E ML controller <b>318</b> analyzes metrics that characterize a current operating environment, as described with reference to <figref idref="DRAWINGS">FIGS. <b>9</b>-<b>1</b> and <b>9</b>-<b>2</b></figref> and determines the E2E ML configuration based on the current operating environment. In implementations, the E2E ML configuration corresponds to a single-directional E2E communication, while in other implementations, the E2E communication corresponds to a bi-directional E2E communication.</p><p id="p-0248" num="0247">At times, the core network server <b>302</b>, by way of the E2E ML controller <b>318</b>, analyzes neural network table(s) to obtain one or more neural network formation configurations that correspond to the E2E ML configuration. For instance, the core network server partitions the E2E ML configuration across multiple devices as described with reference to <figref idref="DRAWINGS">FIGS. <b>18</b>, <b>19</b>, and <b>20</b></figref>, and determines a respective neural network formation configuration for each partition, where each respective neural network formation configuration map to entries in the neural network table(s).</p><p id="p-0249" num="0248">At <b>2235</b>, the core network server <b>302</b> communicates the E2E ML configuration to devices associated with the E2E communication. To illustrate, the core network server communicates a first neural network formation configuration to a first device (e.g., the base station <b>120</b>), a second neural network formation configuration to a second device (e.g., the UE <b>110</b> by way of the base station <b>120</b> at <b>2240</b>), and so forth. This includes identifying a neural network configuration that corresponds to forming a DNN at the core network server. In some implementations, the core network server <b>302</b> communicates the neural network formation configurations in a manner similar to that described with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0250" num="0249">At <b>2245</b>, the core network server <b>302</b> forms a DNN (e.g., DNN <b>1912</b>, DNN <b>1914</b>, DNN <b>2020</b>, DNN <b>2030</b>) based on the E2E ML configuration determined at <b>2230</b>. In implementations, the DNN formed by the core network server performs at least some processing that exchanges information using the E2E communication. Similarly, at <b>2250</b>, the base station <b>120</b> forms a DNN (e.g., DNN <b>1910</b>, DNN <b>1916</b>, DNN <b>2018</b>, DNN <b>2022</b>, DNN <b>2028</b>, DNN <b>2032</b>) based on the E2E ML configuration determined at <b>2230</b>. For instance, the base station <b>120</b> accesses a neural network table to obtain one or more parameters as described with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref>. In implementations, the DNN formed by the base station <b>120</b> performs at least some processing that exchanges information using the E2E communication. At <b>2255</b>, the UE <b>110</b> forms a DNN (e.g., DNN <b>1908</b>, DNN <b>1918</b>, DNN <b>2016</b>, DNN <b>2024</b>, DNN <b>2026</b>, DNN <b>2034</b>) based on the E2E ML configuration determined at <b>2230</b>. For instance, the UE <b>110</b> accesses a neural network table to obtain one or more parameters as described with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref>. In implementations, the DNN formed by the UE <b>110</b> performs at least some processing that exchanges information using the E2E communication.</p><p id="p-0251" num="0250">Afterwards, at <b>2260</b>, at least the core network server <b>302</b>, the base station <b>120</b>, and the UE <b>110</b> process communications using the DNNs, where the communications correspond to information exchanged through the E2E communication. For example, with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the DNNs at the devices process uplink and/or downlink information.</p><p id="p-0252" num="0251">A third example of signaling and control transactions for E2E ML for wireless networks is illustrated by the signaling and control transaction diagram <b>2300</b> of <figref idref="DRAWINGS">FIG. <b>23</b></figref>. In implementations, aspects of the diagram <b>2300</b> operate in conjunction with, or a continuation of, the diagram <b>2100</b> of <figref idref="DRAWINGS">FIG. <b>21</b></figref> and/or the diagram <b>2200</b> of <figref idref="DRAWINGS">FIG. <b>22</b></figref>.</p><p id="p-0253" num="0252">As illustrated, at <b>2305</b> the core network server <b>302</b> receives one or more metrics associated with the wireless network. For example, with reference to <figref idref="DRAWINGS">FIGS. <b>9</b>-<b>1</b> and <b>9</b>-<b>2</b></figref>, the core network server <b>302</b> receives UE metrics from the UE <b>110</b> and/or BS metrics from the base station <b>120</b> based on communications in the wireless network, such as uplink received power, uplink SINR, uplink packet errors, uplink throughput, timing measurements, power information, SINR information, CQI, CSI, Doppler feedback, QoS, latency, etc. This can include metrics generated based on the information exchanged through the E2E communication and/or metrics associated with information exchanged through other communication paths. In implementations, the UE <b>110</b>, the base station <b>120</b>, and the core network server <b>302</b> participate in a same E2E communication to exchange information from a first endpoint to a second endpoint of the E2E communication, such as the QoS flow <b>1810</b>, the QoS flow <b>1812</b>, the QoS flow <b>1814</b>, the E2E communication <b>1902</b> and/or the E2E communication <b>1904</b>, the E2E communication <b>2012</b> and/or the E2E communication <b>2012</b>, etc.</p><p id="p-0254" num="0253">At <b>2310</b>, the core network server <b>302</b>, by way of the E2E ML controller <b>318</b>, determines an E2E ML configuration based on the metric(s). To illustrate, with reference to <figref idref="DRAWINGS">FIGS. <b>9</b>-<b>1</b> and <b>9</b>-<b>2</b></figref>, some implementations identify changes in a current operating environment, and determine to update DNNs used to process information exchanged through the E2E communication, such as by determining parameter changes applied to a DNN architecture. Alternately or additionally, some implementations determine an initial E2E ML configuration that is used to establish the E2E communication. As further described, determining the E2E ML configuration includes, at times, determining a partitioned E2E ML configuration in which different devices form and use respective DNNs to process the information exchanged through the E2E communication, such as by determining respective neural network formation configurations for each respective device participating in the E2E communication.</p><p id="p-0255" num="0254">Determining the E2E ML configuration may include identifying any combination of architectural configurations and/or parameter configurations/changes to the E2E ML configuration. The determined E2E ML configuration can correspond to a small change to a current E2E ML configuration, where the small change updates the parameters (e.g., coefficients, weights, filters) applied to an architecture configuration of a DNN. For instance, the small changes update the DNN to address changes in the operating environment identified by analyzing the metrics received at <b>2305</b> (e.g., SINR changes, Doppler feedback changes, power level changes, BLER changes). Alternately or additionally, the determined E2E ML configuration corresponds to a large change that modifies a ML architecture of the DNN by reconfiguring node and/or layer connections based on analyzing the metrics received at <b>2305</b>, such as a change in a power state (e.g., a transition from a radio resource connected state to idle state).</p><p id="p-0256" num="0255">At <b>2315</b>, the core network server communicates the E2E ML configuration. To illustrate, the core network server communicates a partitioned E2E ML configuration by communicating a first neural network formation configuration to a first device (e.g., the base station <b>120</b>), a second neural network formation configuration to a second device (e.g., the UE <b>110</b> by way of the base station <b>120</b> at <b>2320</b>), and so forth. At times, the core network server identifies a third neural network configuration that corresponds to forming a DNN at the core network server. In some implementations, the core network server <b>302</b> communicates respective neural network formation configurations that correspond to portions of the E2E ML configuration, such as by communicating index value(s) that map into entries of a neural network table as described with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0257" num="0256">At <b>2325</b>, the core network server <b>302</b> forms a DNN (e.g., DNN <b>1912</b>, DNN <b>1914</b>, DNN <b>2020</b>, DNN <b>2030</b>) based on the E2E ML configuration determined at <b>2315</b>. For instance, with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the core network server accesses a neural network table to obtain parameters that form the DNN. In forming the DNN, the core network server <b>302</b> forms a new DNN as part of establishing the E2E communication and/or updates an existing DNN used to exchange information using the E2E communication. In implementations, the DNN formed by the core network server corresponds to a portion of the E2E ML configuration as further described.</p><p id="p-0258" num="0257">Similarly, at <b>2330</b>, the base station <b>120</b> forms a DNN (e.g., DNN <b>1910</b>, DNN <b>1916</b>, DNN <b>2018</b>, DNN <b>2022</b>, DNN <b>2028</b>, DNN <b>2032</b>) based on the E2E ML configuration determined at <b>2315</b>. For instance, with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the base station <b>120</b> accesses a neural network table to obtain parameters that form the DNN. In forming the DNN, the base station forms a new DNN as part of establishing the E2E communication and/or updates an existing DNN used to exchange information using the E2E communication. In implementations, the DNN formed by the base station <b>120</b> corresponds to a portion of the E2E ML configuration as further described.</p><p id="p-0259" num="0258">At <b>2335</b>, the UE <b>110</b> forms a DNN (e.g., DNN <b>1908</b>, DNN <b>1918</b>, DNN <b>2016</b>, DNN <b>2024</b>, DNN <b>2026</b>, DNN <b>2034</b>) based on the E2E ML configuration determined at <b>2310</b>. For instance, with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the UE <b>110</b> accesses a neural network table to obtain parameters that form the DNN. In forming the DNN, the UE <b>110</b> forms a new DNN as part of establishing the E2E communication and/or updates an existing DNN used to exchange information using the E2E communication. In implementations, the DNN formed by the UE <b>110</b> corresponds to a portion of the E2E ML configuration as further described.</p><p id="p-0260" num="0259">Afterwards, at <b>2340</b>, at least the core network server <b>302</b>, the base station <b>120</b>, and the UE <b>110</b> process communications using the DNNs, where the communications correspond to information exchanged through the E2E communication. For example, with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the DNNs at the devices process uplink and/or downlink information. Alternately or additionally, the DNN's at the devices process information exchanged through a QoS flow (e.g., <figref idref="DRAWINGS">FIG. <b>18</b></figref>).</p><p id="p-0261" num="0260">In implementations, the core network server <b>302</b>, base station <b>120</b> and/or the UE <b>110</b> iteratively perform the signaling and control transactions described in the signaling and control transaction diagram <b>2300</b>, signified in <figref idref="DRAWINGS">FIG. <b>23</b></figref> with dashed line <b>2345</b>. These iterations allow the core network server <b>302</b>, base station <b>120</b> and/or the UE <b>110</b> to dynamically modify DNNs used to process information exchanged through an E2E communication based upon changing operating conditions as further described.</p><p id="p-0262" num="0261">A fourth example of signaling and control transactions for E2E ML for wireless networks is illustrated by the signaling and control transaction diagram <b>2400</b> of <figref idref="DRAWINGS">FIG. <b>24</b></figref>. In implementations, aspects of the diagram <b>2400</b> operate in conjunction with, and/or as a continuation of, the diagram <b>2100</b> of <figref idref="DRAWINGS">FIG. <b>21</b></figref>, the diagram <b>2200</b> of <figref idref="DRAWINGS">FIG. <b>22</b></figref>, and/or the diagram <b>2300</b> of <figref idref="DRAWINGS">FIG. <b>23</b></figref>.</p><p id="p-0263" num="0262">As illustrated, at <b>2405</b>, the UE <b>110</b> transmits a request for a machine-learning configuration (ML configuration) to the core network server <b>302</b>, by way of the base station <b>120</b> at <b>2410</b>. To illustrate, the UE <b>110</b> transmits a request to the base station <b>120</b>, where the requests indicates a specific ML configuration, such as a specific neural network formation configuration, and the base station <b>120</b> relays or forwards the request to the core network server <b>302</b>. In response to detecting invocation of an application, the UE <b>110</b> sometimes determines performance requirements associated with the application and identifies a ML configuration designed to satisfy the performance requirements (e.g., by analyzing a neural network table as described with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref>). The UE <b>110</b> then requests the ML configuration, such as by sending an indication of the ML configuration in a NAS message used to establish the E2E communication. In various implementations, the E2E communication corresponds to a QoS flow (e.g., QoS flow <b>1810</b>, QoS flow <b>1812</b>, QoS flow <b>1814</b>). In some implementations, the UE <b>110</b> includes a QoS identifier in the request, such as a 5QI or QCI.</p><p id="p-0264" num="0263">At <b>2415</b>, the core network server <b>302</b>, by way of the E2E ML controller <b>318</b>, determines an E2E ML configuration based on the ML configuration requested by the UE <b>110</b>. For instance, the E2E ML controller <b>318</b> analyzes any combination of ML capabilities, a current operating environment, wireless network resource partitioning, performance requirements, and so forth, to determine the E2E ML configuration. In some implementations, the E2E ML controller partitions the E2E ML configuration based on the ML configuration requested by the UE <b>110</b> such that at least one partition of the E2E ML configuration corresponds to the ML configuration requested by the UE.</p><p id="p-0265" num="0264">At <b>2420</b>, the core network server <b>302</b> communicates the E2E ML configuration, such as by communicating a first neural network formation configuration to a first device (e.g., the base station <b>120</b>), a second neural network formation configuration to a second device (e.g., the UE <b>110</b> by way of the base station <b>120</b> at <b>2425</b>), and so forth. In communicating the E2E ML configuration, the core network server <b>302</b> sometimes indicates, to the UE <b>110</b>, to use the ML configuration requested at <b>2405</b>. At times, the core network server identifies a third neural network configuration that corresponds to forming a DNN at the core network server <b>302</b>. As one example, the core network server <b>302</b> communicates respective neural network formation configurations that correspond to portions of the E2E ML configuration, such as by communicating index value(s) that map into entries of a neural network table as described with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0266" num="0265">At <b>2430</b>, the core network server <b>302</b> forms a DNN (e.g., DNN <b>1912</b>, DNN <b>1914</b>, DNN <b>2020</b>, DNN <b>2030</b>) based on the E2E ML configuration determined at <b>2415</b>. For instance, with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the core network server accesses a neural network table to obtain parameters that form the DNN. In forming the DNN, the core network server <b>302</b> forms a new DNN as part of establishing the E2E communication and/or updates an existing DNN used to exchange information using the E2E communication. In implementations, the DNN formed by the core network server corresponds to a portion of the E2E ML configuration as further described.</p><p id="p-0267" num="0266">Similarly, at <b>2435</b>, the base station <b>120</b> forms a DNN (e.g., DNN <b>1910</b>, DNN <b>1916</b>, DNN <b>2018</b>, DNN <b>2022</b>, DNN <b>2028</b>, DNN <b>2032</b>) based on the E2E ML configuration determined at <b>2315</b>. For instance, with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the base station <b>120</b> accesses a neural network table to obtain parameters that form the DNN. In forming the DNN, the base station forms a new DNN as part of establishing the E2E communication and/or updates an existing DNN used to exchange information using the E2E communication. In implementations, the DNN formed by the base station <b>120</b> corresponds to a portion of the E2E ML configuration as further described.</p><p id="p-0268" num="0267">At <b>2440</b>, the UE <b>110</b> forms a DNN (e.g., DNN <b>1908</b>, DNN <b>1918</b>, DNN <b>2016</b>, DNN <b>2024</b>, DNN <b>2026</b>, DNN <b>2034</b>) based on the E2E ML configuration determined at <b>2415</b> and/or the ML configuration requested at <b>2405</b>. For instance, with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the UE <b>110</b> accesses a neural network table to obtain parameters that form the DNN. In forming the DNN, the UE <b>110</b> forms a new DNN as part of establishing the E2E communication and/or updates an existing DNN used to exchange information using the E2E communication. In implementations, the DNN formed by the UE <b>110</b> corresponds to a portion of the E2E ML configuration as further described.</p><p id="p-0269" num="0268">Afterwards, at <b>2445</b>, at least the core network server <b>302</b>, the base station <b>120</b>, and the UE <b>110</b> process communications using the DNNs, where the communications correspond to information exchanged through the E2E communication. For example, with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the DNNs at the devices process uplink and/or downlink information. Alternately or additionally, the DNN's at the devices process information exchanged through a QoS flow (e.g., <figref idref="DRAWINGS">FIG. <b>18</b></figref>).</p><p id="p-0270" num="0269">Having described signaling and control transactions that can be used to E2E ML for wireless networks, consider now some example methods that can be used to E2E ML for wireless networks that are in accordance with one or more implementations.</p><heading id="h-0009" level="1">Example Methods</heading><p id="p-0271" num="0270">Example methods <b>2500</b> and <b>2600</b> are described with reference to <figref idref="DRAWINGS">FIG. <b>25</b></figref> and <figref idref="DRAWINGS">FIG. <b>26</b></figref> in accordance with one or more aspects of communicating neural network formation configurations. The order in which the method blocks are described are not intended to be construed as a limitation, and any number of the described method blocks can be skipped or combined in any order to implement a method or an alternate method. Generally, any of the components, modules, methods, and operations described herein can be implemented using software, firmware, hardware (e.g., fixed logic circuitry), manual processing, or any combination thereof. Some operations of the example methods may be described in the general context of executable instructions stored on computer-readable storage memory that is local and/or remote to a computer processing system, and implementations can include software applications, programs, functions, and the like. Alternatively, or additionally, any of the functionality described herein can be performed, at least in part, by one or more hardware logic components, such as, and without limitation, Field-programmable Gate Arrays (FPGAs), Application-specific Integrated Circuits (ASICs), Application-specific Standard Products (ASSPs), System-on-a-chip systems (SoCs), Complex Programmable Logic Devices (CPLDs), and the like.</p><p id="p-0272" num="0271"><figref idref="DRAWINGS">FIG. <b>25</b></figref> illustrates an example method <b>2500</b> used for E2E ML for wireless networks. In some implementations, operations of the method <b>2500</b> are performed by an E2E ML controller, such as E2E ML controller <b>318</b> implemented by the core network server <b>302</b>.</p><p id="p-0273" num="0272">At <b>2505</b>, an E2E ML controller obtains capabilities of at least two devices that are utilized in an end-to end communication. For example, an E2E ML controller implemented at a core network server (e.g., E2E ML controller <b>318</b>, core network server <b>302</b>) obtains ML capabilities from a base station (e.g., base station <b>120</b>) and/or a UE (e.g., UE <b>110</b>). The ML capabilities can include any combination of ML-specific capabilities and/or processing capabilities, such as supported ML architectures, supported number of layers, available processing power, memory limitations, available power budget, fixed-point processing vs. floating point processing, maximum kernel size, computation capability, etc. In some implementations, the E2E ML controller receives the capabilities in response to sending a request for the capabilities from the devices, such as that described at <b>2205</b> of <figref idref="DRAWINGS">FIG. <b>22</b></figref>.</p><p id="p-0274" num="0273">At <b>2510</b>, the E2E ML controller determines, based on the capabilities of the at least two devices, an E2E ML configuration for processing information exchanged through the end-to-end communication. For example, the E2E ML controller implemented at a core network server (e.g., E2E ML controller <b>318</b>, core network server <b>302</b>) analyzes the ML capabilities from the base station (e.g., base station <b>120</b>) and/or the UE (e.g., UE <b>110</b>) to determine the E2E ML configuration, such as that described at <b>2115</b> of <figref idref="DRAWINGS">FIG. <b>21</b></figref>, at <b>2230</b> of <figref idref="DRAWINGS">FIG. <b>22</b></figref>, at <b>2310</b> of <figref idref="DRAWINGS">FIG. <b>23</b></figref>, and at <b>2415</b> of <figref idref="DRAWINGS">FIG. <b>24</b></figref>. In some implementations, the E2E ML controller receives metric(s) and analyzes current operating conditions to determine the E2E ML configuration.</p><p id="p-0275" num="0274">At times, the E2E ML controller receives a QoS profile and analyzes QoS requirements, QoS parameters, and/or QoS characteristics, and so forth, to determine the E2E ML configuration. Alternately or additionally, the E2E ML controller identifies one or more partitions to the E2E ML configuration to distribute functionality and/or processing associated with the E2E ML configuration across to the one or more devices (e.g., the core network server <b>302</b>, the base station <b>120</b>, the UE <b>110</b>). For instance, the E2E ML controller analyzes a neural network table, and determines a first neural network formation configuration corresponding to a first partition of the E2E ML configuration, a second neural network formation configuration corresponding to a second partition of the E2E ML configuration, etc.</p><p id="p-0276" num="0275">In some implementations, the E2E ML controller determines a distributed ML configuration that forms a distributed DNN. For example, the E2E ML controller determines first NN formation configuration that corresponds to a first partition of the distributed DNN, a second NN formation configuration that corresponds to a second partition of the distributed DNN, etc., where respective devices implement and/or form the respective portions of the distributed DNN.</p><p id="p-0277" num="0276">At <b>2515</b>, the E2E ML controller directs the at least two devices to process the information exchanged through the end-to-end communication by forming a deep neural network based on the end-to-end machine-learning configuration. For instance, the E2E ML controller (e.g., E2E ML controller <b>318</b>) by way of the core network server (e.g., core network server <b>302</b>) transmits a respective neural network formation configuration to a respective device as described at <b>2125</b> of <figref idref="DRAWINGS">FIG. <b>21</b></figref>, at <b>2235</b> of <figref idref="DRAWINGS">FIG. <b>22</b></figref>, at <b>2315</b> of <figref idref="DRAWINGS">FIG. <b>23</b></figref>, and at <b>2420</b> of <figref idref="DRAWINGS">FIG. <b>24</b></figref>. In implementations, the DNN corresponds to a distributed DNN.</p><p id="p-0278" num="0277"><figref idref="DRAWINGS">FIG. <b>26</b></figref> illustrates an example method <b>2600</b> used for E2E ML for wireless networks. In some implementations, operations of the method <b>2600</b> are performed by a UE, such as the UE <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0279" num="0278">At <b>2605</b>, a UE transmits one or more capabilities supported by the UE. In one example, the UE (e.g., UE <b>110</b>) transmits one or more ML capabilities to a core network server (e.g., core network server <b>302</b>) through a base station (e.g., base station <b>120</b>) and in response to receiving a request for ML capabilities, such as that described at <b>2205</b> and at <b>2220</b> of <figref idref="DRAWINGS">FIG. <b>22</b></figref>. Alternately or additionally, the UE transmits processing capabilities as further described. In another example, the UE (e.g., UE <b>110</b>) transmits one or more ML capabilities in response to detecting the invocation of an application For instance, in response to detecting invocation of the application, the UE <b>110</b> sends a request to establish an E2E connection for the application, and includes the one or more ML capabilities in the request.</p><p id="p-0280" num="0279">At <b>2610</b>, the UE receives a neural network formation configuration based on an E2E ML configuration for processing information exchanged through the end-to-end communication. For example, the UE (e.g., UE <b>110</b>) receives a neural network formation configuration from the core network server and through the base station (e.g., core network server <b>302</b>, base station <b>120</b>) that corresponds to a portion of the E2E ML configuration, such as that described at <b>2125</b> of <figref idref="DRAWINGS">FIG. <b>21</b></figref>, at <b>2235</b> of <figref idref="DRAWINGS">FIG. <b>22</b></figref>, at <b>2315</b> of <figref idref="DRAWINGS">FIG. <b>23</b></figref>, and at <b>2420</b> of <figref idref="DRAWINGS">FIG. <b>24</b></figref>.</p><p id="p-0281" num="0280">The UE forms a deep neural network using the neural network formation configuration at <b>2615</b>. For example, as described at <b>2145</b> of <figref idref="DRAWINGS">FIG. <b>21</b></figref>, at <b>2255</b> of <figref idref="DRAWINGS">FIG. <b>22</b></figref>, at <b>2335</b> of <figref idref="DRAWINGS">FIG. <b>23</b></figref>, and at <b>2440</b> of <figref idref="DRAWINGS">FIG. <b>24</b></figref>, the UE (e.g., UE <b>110</b>) forms a DNN, such as by accessing a neural network table to obtain parameters and/or an architecture configuration. In response to forming the DNN, at <b>2620</b>, the UE uses the DNN to process the information exchanged through the E2E communication, such as that described at <b>2150</b> of <figref idref="DRAWINGS">FIG. <b>21</b></figref>, at <b>2260</b> of <figref idref="DRAWINGS">FIG. <b>22</b></figref>, at <b>2340</b> of <figref idref="DRAWINGS">FIG. <b>23</b></figref>, and at <b>2445</b> of <figref idref="DRAWINGS">FIG. <b>24</b></figref>.</p><p id="p-0282" num="0281">The order in which the method blocks of <figref idref="DRAWINGS">FIGS. <b>25</b> and <b>26</b></figref> are described are not intended to be construed as a limitation, and any number of the described method blocks can be combined in any order to implement a method or an alternate method.</p><p id="p-0283" num="0282">Generally, any of the components, modules, methods, and operations described herein can be implemented using software, firmware, hardware (e.g., fixed logic circuitry), manual processing, or any combination thereof. Some operations of the example methods may be described in the general context of executable instructions stored on computer-readable storage memory that is local and/or remote to a computer processing system, and implementations can include software applications, programs, functions, and the like. Alternatively or in addition, any of the functionality described herein can be performed, at least in part, by one or more hardware logic components, such as, and without limitation, Field-programmable Gate Arrays (FPGAs), Application-specific Integrated Circuits (ASICs), Application-specific Standard Products (ASSPs), System-on-a-chip systems (SoCs), Complex Programmable Logic Devices (CPLDs), and the like.</p><p id="p-0284" num="0283">Although techniques and devices for E2E ML for wireless networks have been described in language specific to features and/or methods, it is to be understood that the subject of the appended claims is not necessarily limited to the specific features or methods described. Rather, the specific features and methods are disclosed as example implementations of E2E ML for wireless networks.</p><p id="p-0285" num="0284">In the following, several examples are described.</p><p id="p-0286" num="0285">Example 1: A method performed by an end-to-end machine-learning controller for determining an end-to-end machine-learning configuration for processing information exchanged through an end-to-end communication in a wireless network, the method comprising: obtaining, by the end-to-end machine-learning controller, capabilities of at least two devices that are utilized in the end-to-end communication; determining, based on the capabilities of the at least two devices, the end-to-end machine-learning configuration for processing the information exchanged through the end-to-end communication; and directing the at least two devices to process the information exchanged through the end-to-end communication by forming a deep neural network based on the end-to-end machine-learning configuration.</p><p id="p-0287" num="0286">Example 2: The method of example 1, wherein determining the end-to-end machine-learning configuration further comprises: partitioning the end-to-end machine-learning configuration across the at least two devices by: determining a first neural network formation configuration that corresponds to a first portion of the end-to-end machine-learning configuration based on capabilities of a first device of the at least two devices; and determining a second neural network formation configuration that corresponds to a second portion of the end-to-end machine-learning configuration based on capabilities of a second device of the at least two devices, wherein the capabilities of the first device include available processing power of the first device, and wherein the capabilities of the second device include available processing power of the second device.</p><p id="p-0288" num="0287">Example 3: The method as recited in example 1 or example 2, wherein the end-to-end machine-learning configuration is a first end-to-end machine-learning configuration and the method further comprises: obtaining one or more metrics that indicate a current operating environment for the end-to-end communication; identifying a second end-to-end machine-learning configuration based on at least the one or more metrics that indicate the current operating environment; and directing the at least two devices to update the one or more deep neural networks based on the second end-to-end machine-learning configuration.</p><p id="p-0289" num="0288">Example 4: The method as recited in any one of examples 1 to 3, wherein determining the end-to-end machine-learning configuration comprises: determining, as a first portion of the end-to-end machine-learning configuration, a first neural network formation configuration for a user equipment-side deep neural network; determining, as a second portion of the end-to-end machine-learning configuration, a second neural network formation configuration for a base station-side deep neural network; and determining, as a third portion of the end-to-end machine-learning configuration, a third neural network formation configuration for a core network server-side deep neural network.</p><p id="p-0290" num="0289">Example 5: The method as recited in any one of examples 1 to 4, wherein the determining the end-to-end machine-learning configuration further comprises: obtaining at least one quality-of-service parameter or quality-of-service characteristic associated with the end-to-end communication; and determining the end-to-end machine-learning configuration based, at least in part, on the at least one quality-of-service parameter or quality-of-service characteristic.</p><p id="p-0291" num="0290">Example 6: The method as recited in example 5, wherein the at least one quality-of-service parameter or quality-of-service characteristic comprises at least one of: a priority level; a packet delay budget; a packet error rate; a maximum data burst volume; or an averaging window.</p><p id="p-0292" num="0291">Example 7: The method as recited in any one of example 1 to 6, wherein the end-to-end machine-learning configuration is a first end-to-end machine-learning configuration, the end-to-end communication is a first quality-of-service flow, the deep neural network is a first deep neural network, and the method further comprises: determining to establish a second quality-of-service flow between the at least two devices; determining a second end-to-end machine-learning configuration for processing information exchanged through the second quality-of-service flow; and directing the at least two devices to process the information exchanged through the second quality-of-service flow by forming a second deep neural network based on the second end-to-end machine-learning configuration.</p><p id="p-0293" num="0292">Example 8: The method as recited in any of examples 1 to 7, wherein end-to-end communication is associated with exchanging information associated with one of: an augmented reality application; a virtual reality application; an audio streaming application; a real-time gaming application; a video streaming application; a Voice-over-Internet-Protocol application; a social media application; a file transfer; a vehicle-to-everything communication; an Internet-of-things communication; automation; or remote control.</p><p id="p-0294" num="0293">Example 9: The method as recited in any one of examples 1 to 8, wherein the at least two devices include a user equipment, and wherein the determining the end-to-end machine-learning configuration further comprises: receiving, from the user equipment a request for a specific neural network formation configuration; and determining the end-to-end machine-learning configuration based on the specific neural network formation configuration.</p><p id="p-0295" num="0294">Example 10: The method as recited in any one of examples 1 to 9, wherein the obtaining the capabilities of the one or more devices comprises obtaining machine-learning capabilities of the at least two devices.</p><p id="p-0296" num="0295">Example 11: The method as recited in any one of examples 1 to 10, wherein the determining, the end-to-end machine-learning configuration comprises: determining, as the end-to-end machine-learning configuration, an architecture configuration and one or more parameter configurations that define a deep neural network.</p><p id="p-0297" num="0296">Example 12: The method as recited in any one of examples 1 to 10, wherein the determining, the end-to-end machine-learning configuration comprises: analyzing one or more metrics of a current operating environment; and determining, as the end-to-end machine-learning configuration and based, in part, on the one or more metrics, one or more parameter configurations that define an update to a deep neural network.</p><p id="p-0298" num="0297">Example 13: A method performed by a user equipment, the method comprising: transmitting, by the user equipment, one or more capabilities supported by the user equipment; receiving a neural network formation configuration based on an end-to-end machine-learning configuration for processing information exchanged through the end-to-end communication; forming a deep neural network using the neural network formation configuration; and using the deep neural network to process the information exchanged through the end-to-end communication.</p><p id="p-0299" num="0298">Example 14: The method as recited in example 13, wherein the transmitting the one or more capabilities supported by the user equipment comprises transmitting at least one of: a maximum kernel size capability; a memory limitation; or a computation capability.</p><p id="p-0300" num="0299">Example 15: The method as recited in example 13 or example 14, wherein the end-to-end communication is a first quality-of-service flow associated with interactive communications, the neural network formation configuration is a first neural network formation configuration, the deep neural network is a first deep neural network, the information exchanged through the end-to-end communication comprises information associated with the interactive communications, and the method further comprises: transmitting a request to establish a second quality-of-service flow, receiving a second neural network formation configuration for processing information exchanged through the second quality-of-service flow; forming a second deep neural network based on the second neural network formation configuration; and using the second deep neural network for the processing information exchanged through the second quality-of-service flow.</p><p id="p-0301" num="0300">Example 16: A network entity comprising: a processor; and computer-readable storage media comprising instructions that implement an end-to-end machine-learning controller for performing any one of the methods of the examples 1 to 12.</p><p id="p-0302" num="0301">Example 17: A user equipment comprising: a processor; and computer-readable storage media comprising instructions, responsive to execution by the processor, for directing the user equipment to perform one of the methods of examples 13 to 15.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method performed by an end-to-end machine-learning controller for determining an end-to-end machine-learning configuration for processing information exchanged through an end-to-end communication in a wireless network, the method comprising:<claim-text>obtaining, by the end-to-end machine-learning controller, capabilities of at least two devices that are utilized in the end-to-end communication;</claim-text><claim-text>determining, based on the capabilities of the at least two devices, the end-to-end machine-learning configuration for processing the information exchanged through the end-to-end communication; and</claim-text><claim-text>directing the at least two devices to process the information exchanged through the end-to-end communication by forming a deep neural network based on the end-to-end machine-learning configuration.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the end-to-end machine-learning configuration further comprises:<claim-text>partitioning the end-to-end machine-learning configuration across the at least two devices by:<claim-text>determining a first neural network formation configuration that corresponds to a first portion of the end-to-end machine-learning configuration based on capabilities of a first device of the at least two devices; and</claim-text><claim-text>determining a second neural network formation configuration that corresponds to a second portion of the end-to-end machine-learning configuration based on capabilities of a second device of the at least two devices,</claim-text></claim-text><claim-text>wherein the capabilities of the first device include available processing power of the first device, and</claim-text><claim-text>wherein the capabilities of the second device include available processing power of the second device.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the end-to-end machine-learning configuration is a first end-to-end machine-learning configuration and the method further comprises:<claim-text>obtaining one or more metrics that indicate a current operating environment for the end-to-end communication;</claim-text><claim-text>identifying a second end-to-end machine-learning configuration based on at least the one or more metrics that indicate the current operating environment; and</claim-text><claim-text>directing the at least two devices to update the one or more deep neural networks based on the second end-to-end machine-learning configuration.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the end-to-end machine-learning configuration comprises:<claim-text>determining, as a first portion of the end-to-end machine-learning configuration, a first neural network formation configuration for a user equipment-side deep neural network;</claim-text><claim-text>determining, as a second portion of the end-to-end machine-learning configuration, a second neural network formation configuration for a base station-side deep neural network; and</claim-text><claim-text>determining, as a third portion of the end-to-end machine-learning configuration, a third neural network formation configuration for a core network server-side deep neural network.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining the end-to-end machine-learning configuration further comprises:<claim-text>obtaining at least one quality-of-service parameter or quality-of-service characteristic associated with the end-to-end communication; and</claim-text><claim-text>determining the end-to-end machine-learning configuration based, at least in part, on the at least one quality-of-service parameter or quality-of-service characteristic.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method as recited in <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the at least one quality-of-service parameter or quality-of-service characteristic comprises at least one of:<claim-text>a priority level;</claim-text><claim-text>a packet delay budget;</claim-text><claim-text>a packet error rate;</claim-text><claim-text>a maximum data burst volume; or</claim-text><claim-text>an averaging window.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the end-to-end machine-learning configuration is a first end-to-end machine-learning configuration, the end-to-end communication is a first quality-of-service flow, the deep neural network is a first deep neural network, and the method further comprises:<claim-text>determining to establish a second quality-of-service flow between the at least two devices;</claim-text><claim-text>determining a second end-to-end machine-learning configuration for processing information exchanged through the second quality-of-service flow; and</claim-text><claim-text>directing the at least two devices to process the information exchanged through the second quality-of-service flow by forming a second deep neural network based on the second end-to-end machine-learning configuration.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein end-to-end communication is associated with exchanging information associated with one of:<claim-text>an augmented reality application;</claim-text><claim-text>a virtual reality application;</claim-text><claim-text>an audio streaming application;</claim-text><claim-text>a real-time gaming application;</claim-text><claim-text>a video streaming application;</claim-text><claim-text>a Voice-over-Internet-Protocol application;</claim-text><claim-text>a social media application;</claim-text><claim-text>a file transfer;</claim-text><claim-text>a vehicle-to-everything communication;</claim-text><claim-text>an Internet-of-things communication;</claim-text><claim-text>automation; or</claim-text><claim-text>remote control.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least two devices include a user equipment, and<claim-text>wherein the determining the end-to-end machine-learning configuration further comprises:<claim-text>receiving, from the user equipment a request for a specific neural network formation configuration; and</claim-text><claim-text>determining the end-to-end machine-learning configuration based on the specific neural network formation configuration.</claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the obtaining the capabilities of the at least two devices comprises obtaining machine-learning capabilities of the at least two devices.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining, the end-to-end machine-learning configuration comprises:<claim-text>determining, as the end-to-end machine-learning configuration, an architecture configuration and one or more parameter configurations that define a deep neural network.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining, the end-to-end machine-learning configuration comprises:<claim-text>analyzing one or more metrics of a current operating environment; and</claim-text><claim-text>determining, as the end-to-end machine-learning configuration and based, in part, on the one or more metrics, one or more parameter configurations that define an update to a deep neural network.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A method performed by a user equipment, the method comprising:<claim-text>transmitting, by the user equipment, one or more capabilities supported by the user equipment;</claim-text><claim-text>receiving a neural network formation configuration based on an end-to-end machine-learning configuration for processing information exchanged through end-to-end communication;</claim-text><claim-text>forming a deep neural network using the neural network formation configuration; and</claim-text><claim-text>using the deep neural network to process the information exchanged through the end-to-end communication.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method as recited in <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the transmitting the one or more capabilities supported by the user equipment comprises transmitting at least one of:<claim-text>a maximum kernel size capability;</claim-text><claim-text>a memory limitation; or</claim-text><claim-text>a computation capability.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method as recited in <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the end-to-end communication is a first quality-of-service flow associated with interactive communications, the neural network formation configuration is a first neural network formation configuration, the deep neural network is a first deep neural network, the information exchanged through the end-to-end communication comprises information associated with the interactive communications, and the method further comprises:<claim-text>transmitting a request to establish a second quality-of-service flow,</claim-text><claim-text>receiving a second neural network formation configuration for processing information exchanged through the second quality-of-service flow;</claim-text><claim-text>forming a second deep neural network based on the second neural network formation configuration; and</claim-text><claim-text>using the second deep neural network for the processing information exchanged through the second quality-of-service flow.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A network entity comprising:<claim-text>a processor; and</claim-text><claim-text>computer-readable storage media comprising instructions that implement an end-to-end machine-learning controller for determining an end-to-end machine-learning configuration for processing information exchanged through an end-to-end communication in a wireless network, the instructions executable by the processor to configure the network entity to:<claim-text>obtain capabilities of at least two devices that are utilized in the end-to-end communication;</claim-text><claim-text>determine, based on the capabilities of the at least two devices, the end-to-end machine-learning configuration for processing the information exchanged through the end-to-end communication; and</claim-text><claim-text>direct the at least two devices to process the information exchanged through the end-to-end communication by forming a deep neural network based on the end-to-end machine-learning configuration, for performing any one of the methods of <claim-ref idref="CLM-00001">claims 1</claim-ref> to <claim-ref idref="CLM-00012">12</claim-ref>.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A user equipment comprising:<claim-text>a processor; and</claim-text><claim-text>computer-readable storage media comprising instructions, responsive to execution by the processor, the instructions executable to configure the user equipment to:</claim-text><claim-text>transmit one or more capabilities supported by the user equipment;</claim-text><claim-text>receive a neural network formation configuration based on an end-to-end machine-learning configuration for processing information exchanged through end-to-end communication;</claim-text><claim-text>form a deep neural network using the neural network formation configuration; and</claim-text><claim-text>use the deep neural network to process the information exchanged through the end-to-end communication.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The user equipment of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the transmission of the one or more capabilities supported by the user equipment comprises transmitting at least one of:<claim-text>a maximum kernel size capability;</claim-text><claim-text>a memory limitation; or</claim-text><claim-text>a computation capability.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The network entity of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the instructions for the determination of the end-to-end machine-learning configuration further configure the network entity to:<claim-text>partition the end-to-end machine-learning configuration across the at least two devices, wherein the instructions to partition the end-to-end machine-learning configuration configure the network entity to:<claim-text>determine a first neural network formation configuration that corresponds to a first portion of the end-to-end machine-learning configuration based on capabilities of a first device of the at least two devices; and</claim-text><claim-text>determine a second neural network formation configuration that corresponds to a second portion of the end-to-end machine-learning configuration based on capabilities of a second device of the at least two devices,</claim-text></claim-text><claim-text>wherein the capabilities of the first device include available processing power of the first device, and</claim-text><claim-text>wherein the capabilities of the second device include available processing power of the second device.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The network entity of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the end-to-end machine-learning configuration is a first end-to-end machine-learning configuration and wherein the instructions further configure the network entity to:<claim-text>obtain one or more metrics that indicate a current operating environment for the end-to-end communication;</claim-text><claim-text>identify a second end-to-end machine-learning configuration based on at least the one or more metrics that indicate the current operating environment; and</claim-text><claim-text>direct the at least two devices to update the one or more deep neural networks based on the second end-to-end machine-learning configuration.</claim-text></claim-text></claim></claims></us-patent-application>