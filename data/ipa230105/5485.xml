<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005486A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005486</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17855149</doc-number><date>20220630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>17</main-group><subgroup>02</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>17</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>17</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>17</main-group><subgroup>02</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>17</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>17</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SPEAKER EMBEDDING CONVERSION FOR BACKWARD AND CROSS-CHANNEL COMPATABILITY</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63218174</doc-number><date>20210702</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Pindrop Security, Inc.</orgname><address><city>Atlanta</city><state>GA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Chen</last-name><first-name>Tianxiang</first-name><address><city>Atlanta</city><state>GA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Khoury</last-name><first-name>Elie</first-name><address><city>Atlanta</city><state>GA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Pindrop Security, Inc.</orgname><role>02</role><address><city>Atlanta</city><state>GA</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Embodiments include a computer executing voice biometric machine-learning for speaker recognition. The machine-learning architecture includes embedding extractors that extract embeddings for enrollment or for verifying inbound speakers, and embedding convertors that convert enrollment voiceprints from a first type of embedding to a second type of embedding. The embedding convertor maps the feature vector space of the first type of embedding to the feature vector space of the second type of embedding. The embedding convertor takes as input enrollment embeddings of the first type of embedding and generates as output converted enrolled embeddings that are aggregated into a converted enrolled voiceprint of the second type of embedding. To verify an inbound speaker, a second embedding extractor generates an inbound voiceprint of the second type of embedding, and scoring layers determine a similarity between the inbound voiceprint and the converted enrolled voiceprint, both of which are the second type of embedding.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="142.16mm" wi="101.85mm" file="US20230005486A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="168.49mm" wi="103.89mm" file="US20230005486A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="149.35mm" wi="153.42mm" file="US20230005486A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="169.84mm" wi="117.86mm" file="US20230005486A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="216.49mm" wi="160.44mm" file="US20230005486A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims priority to U.S. Provisional Application No. 63/218,174, filed Jul. 2, 2021, which is incorporated by reference in its entirety.</p><p id="p-0003" num="0002">This application generally relates to U.S. application Ser. No. 17/165,180, entitled &#x201c;Cross-Channel Enrollment and Authentication of Voice Biometrics,&#x201d; filed Feb. 2, 2021, which claims priority to U.S. Provisional Application No. 62/969,484, filed Feb. 3, 2020, each of which is incorporated by reference in its entirety.</p><p id="p-0004" num="0003">This application generally relates to U.S. application Ser. No. 17/066,210, entitled &#x201c;Z-Vectors: Speaker Embeddings from Raw Audio Using SincNet, Extended CNN Architecture and In-Network Augmentation Techniques,&#x201d; filed Oct. 8, 2020, which claims priority to U.S. Provisional Application No. 62/914,182, filed Oct. 11, 2019, each of which is incorporated by reference in its entirety.</p><p id="p-0005" num="0004">This application generally relates to U.S. application Ser. No. 15/709,290, entitled &#x201c;Improvements of Speaker Recognition in the Call Center,&#x201d; filed Sep. 19, 2017, which claims priority to U.S. Provisional Application No. 62/396,670, filed Sep. 19, 2016, entitled titled &#x201c;Improvements of Speaker recognition in the Call Center,&#x201d; each of which is incorporated by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0006" num="0005">This application generally relates to systems and methods for managing, training, and deploying a machine learning architecture for audio processing.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0007" num="0006">The accuracy of automatic speaker verification (ASV) systems has been greatly improved due to the recent breakthroughs in low-rank speaker representations and deep learning techniques, leading to the success of ASV in real-world applications from call centers to mobile applications and smart devices. ASV systems, among other types of voice biometrics systems, rely upon speaker embeddings extracted as vectors from one or more voice samples and from a current inbound call requiring authentication. The machine-learning architecture determines a similarity level (e.g., cosine distance) between an enrolled speaker embedding (sometimes called a &#x201c;voiceprint&#x201d;) generated for a registered user (or &#x201c;enrollee&#x201d;), and an inbound speaker embedding for an inbound user (sometimes called an &#x201c;inbound voiceprint&#x201d;). If the distances satisfy a threshold similarity, then the machine-learning architecture determines the inbound user and the enrolled user are likely the same speaker.</p><p id="p-0008" num="0007">Recently, some voice biometric service providers have been shifting the voice biometric systems away from traditional Gaussian Mixture Model (GMM) approaches, from the GMM-based i-vector paradigm towards the deep learning-based x-vector paradigm employing various types of neural networks. Moreover, some voice biometric providers implement different types of communications systems requiring different sampling rates (e.g. 8 kHz over a telephone channel; 16 kHz for communications from VoIP software or virtual assistants).</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0009" num="0008">A problem with conventional systems is that speaker embeddings extracted from one ASV system are often not compatible with another ASV system. This renders interchangeability between systems very cumbersome and costly. For example, the different types of machine-learning architectures, generating different types of embedding vectors (e.g., i-vectors or x-vectors), cause incompatibility between the ASV systems. As another example, different sampling rates may cause incompatibility between the ASV systems using the extracted embeddings. As a result, registered enrollees may have to provide additional enrollment signals for the system to generate updated or new enrollment voiceprints for the enrollees, which is cumbersome or aggravating for the enrollees. What is needed is a means for addressing the incompatibility between various types of ASV systems, for backward compatibility to older systems or for cross-compatibility among various types of channel requirements (e.g., sampling rates, bandwidths).</p><p id="p-0010" num="0009">Disclosed herein are systems and methods capable of addressing the above-described shortcomings and may also provide any number of additional or alternative benefits and advantages. Embodiments include a computing device that executes software routines of one or more machine-learning architectures for audio processing and speaker recognition. The machine-learning architecture executes software programming for speaker recognition that include embedding extractors that extract the voiceprint embeddings for enrollees or inbound speakers. The machine-learning architecture further includes embedding convertors that convert existing enrollment voiceprints from a first type of embedding to a second type of embedding. The embedding convertors are trained to map the feature vector space of the first type of embedding to the feature vector space of the second type of embedding. The embedding convertor takes as input the enrollment embeddings or enrolled voiceprint of the first type of embedding and generates a converted enrolled voiceprint of the second type of embedding. To verify an inbound speaker is the enrolled speaker, the second embedding extractor generates an inbound voiceprint of the second type of embedding. Scoring layers of the machine-learning architecture determines a similarity level (e.g., cosine distance) between the inbound voiceprint and the converted enrolled voiceprint. If the scoring layers determine that the similarity score satisfies a threshold similarity score, then the machine-learning architecture determines that the inbound speaker and the enrolled speaker are likely the same speaker.</p><p id="p-0011" num="0010">In an embodiment, a computer-implemented method comprises obtaining, by a computer, a plurality of enrollment embeddings extracted using a plurality of enrollment signals for an enrolled user by applying a first embedding extractor for a first attribute-type; generating, by the computer, a plurality of converted embeddings corresponding to the plurality of enrollment embeddings by applying an embedding convertor comprising a plurality of machine-learning layers trained to generate a converted embedding having a second attribute-type for an enrollment embedding having a first attribute-type; generating, by the computer, a converted enrolled voiceprint having the second attribute-type for the enrolled user based upon the plurality of converted embeddings; generating, by the computer, an inbound voiceprint for an inbound user extracted using an inbound signal for an inbound user by applying a second embedding extractor for the second attribute-type; and generating, by the computer, a similarity score for the inbound signal using the converted enrolled voiceprint and the inbound voiceprint, the similarity score indicating a likelihood that the inbound user is the enrolled user.</p><p id="p-0012" num="0011">In another embodiment, a system comprises a non-transitory machine-readable memory configured to store machine-readable instructions for one or more neural networks; and a computer comprising a processor. The computer configured to obtain a plurality of enrollment embeddings extracted using a plurality of enrollment signals for an enrolled user by applying a first embedding extractor for a first attribute-type; generate a plurality of converted embeddings corresponding to the plurality of enrollment embeddings by applying an embedding convertor comprising a plurality of machine-learning layers trained to generate a converted embedding having a second attribute-type for an enrollment embedding having a first attribute-type; generate a converted enrolled voiceprint having the second attribute-type for the enrolled user based upon the plurality of converted embeddings; generate an inbound voiceprint for an inbound user extracted using an inbound signal for an inbound user by applying a second embedding extractor for the second attribute-type; and generate a similarity score for the inbound signal using the converted enrolled voiceprint and the inbound voiceprint, the similarity score indicating a likelihood that the inbound user is the enrolled user.</p><p id="p-0013" num="0012">It is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory and are intended to provide further explanation of the invention as claimed.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0014" num="0013">The present disclosure can be better understood by referring to the following figures. The components in the figures are not necessarily to scale, emphasis instead being placed upon illustrating the principles of the disclosure. In the figures, reference numerals designate corresponding parts throughout the different views.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows components of a system for receiving and analyzing telephone calls for voice biometrics received from various types of communication channels or devices, according to an embodiment.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows steps of a method for training a machine-learning architecture for speaker verification, including functions or layers defining embedding extractors and one or more embedding convertors, according to an embodiment.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows steps of a method for implementing a machine-learning architecture for speaker verification, including functions or layers defining embedding extractors and one or more embedding convertors, according to an embodiment.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows data flow amongst layers of a machine-learning architecture for speaker recognition including embedding convertors, according to an embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0019" num="0018">Reference will now be made to the illustrative embodiments illustrated in the drawings, and specific language will be used here to describe the same. It will nevertheless be understood that no limitation of the scope of the invention is thereby intended. Alterations and further modifications of the inventive features illustrated here, and additional applications of the principles of the inventions as illustrated here, which would occur to a person skilled in the relevant art and having possession of this disclosure, are to be considered within the scope of the invention.</p><p id="p-0020" num="0019">In a typical voice biometric system, the embedding extraction engine computes or extracts an embedding as a feature vector mathematically representing low-level features of a speaker's utterance contained in an audio signal. When enrolling the enrolled speaker, the computing device mathematically combines or aggregates multiple enrollment embeddings from the same enrollee-speaker to create the enrolled voiceprint for a user profile of the enrolled speaker. Once the user profile is created, and the machine-learning architecture receives a new verification or authentication request, the embedding extractor generates an inbound voiceprint for an inbound speaker. Scoring layers generate a similarity score (e.g., cosine distance, probabilistic linear discriminant analysis (PLDA)) to compare the enrolled voiceprint embedding against the inbound voiceprint and verify the likelihood that the enrolled speaker is the inbound speaker.</p><p id="p-0021" num="0020">Embodiments described herein include a server or other computing devices executing software implementing a machine-learning architecture that comprises layers and functions defining a voice biometric system, such as ASV or voice biometrics authentication. Layers of the machine-learning architecture define speaker embedding extraction engines (sometimes referred to as &#x201c;embedding extractors&#x201d;) and one or more embedding conversion engines (sometimes referred to as &#x201c;embedding convertors&#x201d;). The voice biometrics engine is trained to recognize and distinguish instances of speech of audio signals, such as a speaker recognition engine. Layers of the speaker recognition engine define an input layer that performs pre-processing operations for extracting various types of features from the audio signals and applies a transform operation on the features. An embedding extractor of the machine-learning architecture extracts a feature vector embedding representing the features of an utterance of the particular audio signal. One or more output or scoring layers then generates certain results according to corresponding input audio signals and evaluates the results, which may include a classifier or other scoring layer.</p><p id="p-0022" num="0021">During a training phase, the machine-learning architecture includes a loss function that compares an observed (or &#x201c;predicted&#x201d;) output (e.g., predicted embedding) against an expected output (e.g., expected embedding), based upon a training label or cluster centroid. The training operations then tailor the weighted values of the machine-learning architecture (sometimes called &#x201c;hyper-parameters&#x201d;) and reapply the machine-learning architecture on the same or additional training input signals until the expected outputs and observed outputs converge. The server then fixes (e.g., freezes or sets) the hyper-parameters and, in some cases, disables one or more layers of the machine-learning architecture used for training.</p><p id="p-0023" num="0022">The server can further train the speaker recognition engine to recognize a particular speaker during an enrollment phase for the particular enrollee-speaker. The speech recognition engine can generate an enrollee voice feature vector (sometimes called a &#x201c;voiceprint&#x201d;) using enrollee audio signals having speech segments involving the enrollee. During a later deployment phase (sometimes referred to as &#x201c;test phase,&#x201d; &#x201c;testing,&#x201d; &#x201c;production,&#x201d; or &#x201c;verification&#x201d;), the server receives and applies the machine-learning architecture on an inbound phone call to verify an inbound speaker. The speaker recognition engine compares the enrolled voiceprints in order to confirm whether the inbound speaker in the inbound audio signal is the enrolled user, based upon matching an inbound feature vector embedding extracted from the inbound audio signal against the enrolled voiceprint of the enrollee. These approaches are generally successful and adequate for detecting the enrollee in the inbound audio signal.</p><p id="p-0024" num="0023">Voice biometrics for speaker recognition and other operations (e.g., authentication) often rely upon vectors generated from a universe of training speaker samples and samples of a particular enrolled speaker. For example, enrollment audio signals having lower sampling rates (e.g., 8 kHz of enrollment voice samples received via landline phone channel) resulting in enrollment feature vectors or enrolled voiceprints based upon those features generated from the lower-sampling rate enrollment signals. These enrolled voiceprints are often incompatible with, or not suitable for comparison against, inbound voiceprints extracted from an inbound calls arriving through a higher-sampling rate communication channel (e.g., 16 kHz of an inbound call received via data channel). As another example, a first embedding extractor may apply a Gaussian Matrix Model (GMM) machine-learning technique for extracting enrolled feature vectors and generating an enrolled voiceprint. These enrolled voiceprints are often incompatible with, or not suitable for comparison against, inbound voiceprints generated using a second embedding extractor that applies a neural network architecture.</p><p id="p-0025" num="0024">The embedding convertors enable the voice biometric machine-learning architecture to perform speaker recognition for an enrolled-speaker based upon enrolled audio signals or enrolled embeddings having disparate signal or embedding attributes, different from the attributes of the inbound embeddings or inbound signals. Each embedding convertor maps an input embedding vector space to an output embedding vector space to generate a converted embedding resulting from and corresponding to the input embedding. The embedding convertor comprises a neural network architecture having layers for a deep learning neural network (DNN). The DNN takes an existing embedding vector (for a first type of embedding from a first embedding extractor) as input and outputs the new converted embedding (for a second type of embedding from a second embedding extractor).</p><p id="p-0026" num="0025">During a training phase, the embedding convertor is trained to minimize a distance (e.g., cosine distance) between predicted converted embeddings when compared against expected converted embeddings. The expected converted embeddings may be indicated by training labeled data associated with the training audio signals or training embeddings. Additionally or alternatively, the expected converted embeddings may be actual embeddings generated by the second type of embedding extractor for the training audio signals. A database stores the trained components or sub-components of the machine-learning architecture, including the trained embedding extractors and embedding convertors.</p><p id="p-0027" num="0026">During an enrollment phase, when registering a new user through a particular channel or using an existing embedding extraction technique, the machine-learning architecture may apply the first embedding extractor on enrollment signals from the enrollee-user. The speaker recognition engine extracts enrollment feature vectors from the enrollment signals and may algorithmically combine the enrollment feature vectors to generate a first type of enrollment voiceprint for the first embedding extractor. For instance, the first embedding extractor may apply the GMM technique for generating feature vectors (e.g., i-vectors).</p><p id="p-0028" num="0027">At a following point-in-time, the machine-learning architecture applies the embedding convertor on the existing enrolled voiceprint based on the first embedding extractor, thereby outputting a converted enrolled voiceprint as though generated by a second embedding extractor. For instance, the second embedding extractor applies a DNN technique for generating feature vectors (e.g., x-vectors). Additionally or alternatively, the machine-learning architecture applies the embedding convertor on the set of enrollment embedding vectors, thereby outputting a set of corresponding enrollment converted embeddings, which the machine-learning architecture aggregates to generate a converted enrolled voiceprint. The machine-learning architecture may execute the embedding convertor at any later point-in-time or triggering operation. Non-limiting examples may include: contemporaneously or shortly after generating the first type of enrollment voiceprint; during a deployment phase in response to instructions to verify an inbound speaker; when implementing a new machine-learning architecture for different types of audio signal attributes (e.g., 8 kHz sampling rate signals, 16 kHz sampling rate signals); and when implementing a new machine-learning architecture for different types of embeddings (e.g., i-vectors, x-vectors), among other circumstances. For a particular enrolled user, the database stores information about the enrolled user as a user profile, including, for example, the enrollment audio signals, enrollment embeddings, enrolled voiceprints, converted enrollment embeddings, and converted enrolled voiceprints, among other types of data.</p><p id="p-0029" num="0028">During a deployment phase, the machine-learning architecture applies the second embedding extractor to extract an inbound voiceprint for an inbound user. The machine-learning architecture then applies scoring layers on the inbound voiceprint and the converted enrolled voiceprint to generate a similarity score. The similarity score indicates a similarity or distance between the inbound voiceprint and the enrolled voiceprint, and represents a likelihood that the inbound speaker is the enrolled speaker. The server determines that the inbound speaker is the enrolled speaker when the similarity score satisfies a matching threshold value.</p><p id="p-0030" num="0029">The machine-learning architecture may include any number of layers configured to perform certain operations, such as input layers for audio data ingestion, pre-processing operations, data augmentation operations, loss function operations, and classification operations, among others. It should be appreciated that the layers or operations may be performed by any number of machine-learning architectures. Moreover, certain operations, such as pre-processing operations and data augmentation operations or may be performed by a computing device separately from the machine-learning architecture or as layers of the machine-learning architecture.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows components of a system <b>100</b> for receiving and analyzing telephone calls for voice biometrics received from various types of communication channels or devices. The system <b>100</b> comprises a call analytics system <b>101</b>, any number of call center systems <b>110</b> of service provider enterprise infrastructures (e.g., companies, government entities, universities), and end-user devices <b>114</b><i>a</i>-<b>114</b><i>d </i>(collectively referred to as &#x201c;end-user device <b>114</b>&#x201d; or &#x201c;end-user devices <b>114</b>&#x201d;). The call analytics system <b>101</b> includes analytics servers <b>102</b>, analytics databases <b>104</b>, and admin devices <b>103</b>. The call center system <b>110</b> includes call center servers <b>111</b>, call center databases <b>112</b>, and agent devices <b>116</b>. Embodiments may comprise additional or alternative components or omit certain components from those of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, and still fall within the scope of this disclosure. It may be common, for example, to include multiple call center systems <b>110</b> or for the call analytics system <b>101</b> to have multiple analytics servers <b>102</b>. Embodiments may include or otherwise implement any number of devices capable of performing the various features and tasks described herein. For example, the <figref idref="DRAWINGS">FIG. <b>1</b></figref> shows the analytics server <b>102</b> in as a distinct computing device from the analytics database <b>104</b>. In some embodiments, the analytics database <b>104</b> may be integrated into the analytics server <b>102</b>.</p><p id="p-0032" num="0031">It should be appreciated that embodiments described with respect to <figref idref="DRAWINGS">FIG. <b>1</b></figref> are merely examples of a voice biometrics processing system described herein and not necessarily limiting on other potential embodiments. The description of <figref idref="DRAWINGS">FIG. <b>1</b></figref> mentions circumstances in which a caller places a call through various communications channels or using various types of end-user devices <b>114</b> to contact and interact with the services offered by the call center system <b>110</b>, though the operations and features of the analytics system <b>101</b>, including the voice biometrics machine-learning techniques, described herein may be applicable to any circumstances involving a voice-based interface or speaker recognition and not limited to uses cases between the caller and the services offered by the call center system <b>110</b>. The features described herein may be implemented in any system that receives and authenticates speaker audio inputs via multiple communications channels or end-user devices <b>114</b>.</p><p id="p-0033" num="0032">The end-users may access user accounts or other features of the service provider and the call center system <b>110</b> and interact with human agents of or with software applications (e.g., cloud application) hosted by call center servers <b>111</b>. In some implementations, the users of the service call center system <b>110</b> may access the user accounts or other features of the service provider by placing calls using the various types of user devices <b>114</b>. The users may also access the user accounts or other features of the service provider using software executed by certain user devices <b>114</b> configured to exchange data and instructions with software programming (e.g., the cloud application) hosted by the call center servers <b>111</b>. The call center system <b>110</b> may include, for example, human agents who converse with callers during telephone calls, Interactive Voice Response (IVR) software executed by the call center server <b>111</b>, or cloud software programming executed by the call center server <b>111</b> accessible to software executed by the end-user devices <b>114</b>. The call center system <b>110</b> need not include any human agents such that the end-user interacts only with the IVR system or the cloud software application.</p><p id="p-0034" num="0033">The customer call center system <b>110</b> includes human agents (operating the agent devices <b>116</b>) and/or an IVR system (hosted by the call center server <b>111</b>) that handle telephone calls originating from, for example, landline devices <b>114</b><i>a </i>or mobile devices <b>114</b><i>b </i>having different types of attributes. Additionally or alternatively, the call center server <b>111</b> executes the cloud application that is accessible to a corresponding software application on a user device <b>114</b>, such as a mobile device <b>114</b><i>b</i>, computing device <b>114</b><i>c</i>, or edge device <b>114</b><i>d</i>. The user interacts with the user account or other features of the service provider using the user-side software application. In such cases, the call center system <b>110</b> need not include a human agent or the user could instruct call center server <b>111</b> to redirect the software application to connect with an agent device <b>116</b> via another channel, thereby allowing the user to speak with a human agent when the user is having difficulty.</p><p id="p-0035" num="0034">Various hardware and software components of one or more public or private networks may interconnect the various components of the system <b>100</b> via various communications channels. Non-limiting examples of such networks may include: Local Area Network (LAN), Wireless Local Area Network (WLAN), Metropolitan Area Network (MAN), Wide Area Network (WAN), and the Internet. The communication over the network may be performed in accordance with various communication protocols, such as Transmission Control Protocol and Internet Protocol (TCP/IP), User Datagram Protocol (UDP), and IEEE communication protocols. Likewise, the end-user devices <b>114</b> may communicate with callees (e.g., call center systems <b>110</b>) via telephony and telecommunications protocols, hardware, and software capable of hosting, transporting, and exchanging audio data associated with telephone calls. Non-limiting examples of telecommunications hardware may include switches and trunks, among other additional or alternative hardware used for hosting, routing, or managing telephone calls, circuits, and signaling. Non-limiting examples of software and protocols for telecommunications may include SS7, SIGTRAN, SCTP, ISDN, and DNIS among other additional or alternative software and protocols used for hosting, routing, or managing telephone calls, circuits, and signaling. Components for telecommunications may be organized into or managed by various different entities, such as carriers, exchanges, and networks, among others.</p><p id="p-0036" num="0035">The end-user devices <b>114</b> may be any communications device or computing device that the caller operates to access the services of the call center system <b>100</b> through the various types of communications channels. For instance, the caller may place the call to the call center system <b>110</b> through a telephony network or through a software application executed by the end-user device <b>114</b>. Non-limiting examples of end-user devices <b>114</b> may include landline phones <b>114</b><i>a</i>, mobile phones <b>114</b><i>b</i>, computing devices <b>114</b><i>c</i>, or edge devices <b>114</b><i>d</i>. The landline phones <b>114</b><i>a </i>and mobile phones <b>114</b><i>b </i>are telecommunications-oriented devices (e.g., telephones) that communicate via telephony channels. The end-user device <b>114</b> is not limited to the telecommunications-oriented devices or channels. For instance, in some cases, the mobile phones <b>114</b><i>b </i>may communicate via a computing network channel (e.g., the Internet). The end-user device <b>114</b> may also include an electronic device comprising a processor and/or software, such as a computing device <b>114</b><i>c </i>or edge device <b>114</b><i>d </i>implementing, for example, voice-over-IP (VoIP) telecommunications, data streaming via a TCP/IP network, or other computing network channel. The edge device <b>114</b><i>d </i>may include any Internet of Things (IoT) device or other electronic device for network communications. The edge device <b>114</b><i>d </i>could be any smart device capable of executing software applications and/or performing voice interface operations. Non-limiting examples of the edge device <b>114</b><i>d </i>may include voice assistant devices (e.g., Amazon Echo&#xae;, Google Home&#xae;), automobiles, smart appliances, and the like.</p><p id="p-0037" num="0036">The call center system <b>110</b> comprises various hardware and software components that capture and store various types of data or metadata related to the caller's contact with the call center system <b>110</b>. This data may include, for example, audio recordings of the call or the caller's voice and metadata related to the protocols and software employed for the particular communication channel. The audio signal captured with the caller's voice has a quality based on the particular communication used. For example, the audio signals from the landline phone <b>114</b><i>a </i>will have a lower sampling rate and/or lower sampling rate compared to the sampling rate and/or sampling rate of the audio signals from the edge device <b>114</b><i>d. </i></p><p id="p-0038" num="0037">The call analytics system <b>101</b> and the call center system <b>110</b> represent network infrastructures <b>101</b>, <b>110</b> comprising physically and logically related software and electronic devices managed or operated by various enterprise organizations. The devices of each network system infrastructure <b>101</b>, <b>110</b> are configured to provide the intended services of the particular enterprise organization.</p><p id="p-0039" num="0038">The analytics server <b>102</b> of the call analytics system <b>101</b> may be any computing device comprising one or more processors and software, and capable of performing the various processes and tasks described herein. The analytics server <b>102</b> may host or be in communication with the analytics database <b>104</b>, and receives and processes call data (e.g., audio recordings, metadata) received from the one or more call center systems <b>110</b>. Although <figref idref="DRAWINGS">FIG. <b>1</b></figref> shows only a single analytics server <b>102</b>, the analytics server <b>102</b> may include any number of computing devices. In some cases, the computing devices of the analytics server <b>102</b> may perform all or sub-parts of the processes and benefits of the analytics server <b>102</b>. The analytics server <b>102</b> may comprise computing devices operating in a distributed or cloud computing configuration and/or in a virtual machine configuration. It should also be appreciated that, in some embodiments, functions of the analytics server <b>102</b> may be partly or entirely performed by the computing devices of the call center system <b>110</b> (e.g., the call center server <b>111</b>).</p><p id="p-0040" num="0039">The analytics server <b>102</b> executes software programming for performing the various audio or data processing operations for verifying speakers or users. The software includes one or more machine-learning architectures, which may include functions or layers that define certain functional operations (sometimes referred to as &#x201c;engines&#x201d;) and/or perform various types of machine-learning techniques, such as Gaussian Mixture Models (GMMs), convolutional neural networks (CNNs), and deep neural networks (DNNs), among others. For ease of description, the analytics server <b>102</b> in the example system <b>100</b> executes a single machine-learning architecture, though the analytics server <b>102</b> may execute any number of machine-learning architectures in other embodiments.</p><p id="p-0041" num="0040">The machine-learning architecture includes functions or layers defining embedding extractors and embedding convertors. The embedding extractors extract feature vectors according to various types of signal or embedding attributes, such as a types or dimensions of feature vectors (e.g., i-vectors, x-vectors, z-vectors), a sampling rate, a bandwidth of a communication channel through which an audio signal is received, or other attributes of the signals. In some cases, an embedding extractor may include functions and layers for extracting a type of feature vector embedding using a GMM-based machine-learning technique, which outputs GMM-based feature vectors (e.g., i-vectors). Non-limiting examples of embodiments implementing machine-learning architectures for generating feature vectors using GMMs are described in U.S. application Ser. No. 15/709,290, entitled &#x201c;Improvements of Speaker Recognition in the Call Center,&#x201d; which is incorporated by reference in its entirety. In some cases, an embedding extractor may include functions and layers for extracting another type of feature vector embedding using a DNN-based machine-learning technique, which output DNN-based feature vectors (e.g., x-vectors). Non-limiting examples of embodiments implementing machine-learning architectures for generating feature vectors using DNNs or CNNs are described in U.S. application Ser. No. 17/165,180, entitled &#x201c;Cross-Channel Enrollment and Authentication of Voice Biometrics,&#x201d; filed Feb. 2, 2021.</p><p id="p-0042" num="0041">The analytics server <b>102</b> executes the machine-learning architecture operates logically in several operational phases, including a training phase, an enrollment phase, and a deployment phase (sometimes referred to as a &#x201c;test phase,&#x201d; &#x201c;testing,&#x201d; or &#x201c;production&#x201d;), though some embodiments need not perform the enrollment phase. The inputted audio signals processed by the analytics server <b>102</b> and the machine-learning architecture include training audio signals, enrollment audio signals, and inbound audio signals processed during the deployment phase. The analytics server <b>102</b> applies the machine-learning architecture to each type of inputted audio signal (e.g., training signal, enrollment signal, inbound signal) during the corresponding operational phase (e.g., training phase, enrollment phase, deployment phase).</p><p id="p-0043" num="0042">The analytics server <b>102</b> or other computing device of the system <b>100</b> (e.g., call center server <b>111</b>) performs various pre-processing operations or data augmentation operations on the input audio signals. Non-limiting examples of pre-processing operations on inputted audio signals include: extracting low-level features, parsing or segmenting the audio signal into frames or segments, and performing one or more transformation functions (e.g., FFT, SFT), among other potential pre-processing operations. Non-limiting examples of augmentation operations include performing down-sampling, audio clipping, noise augmentation, frequency augmentation, and duration augmentation, among others. The analytics server <b>102</b> may perform the pre-processing or data augmentation operations prior to feeding the input audio signals into input layers of the machine-learning architecture. Additionally or alternatively, the analytics server <b>102</b> performs pre-processing or data augmentation operations when executing the machine-learning architecture, where the input layers (or other layers) of the machine-learning architecture perform the pre-processing or data augmentation operations. In some cases, the machine-learning architecture may comprise in-network data augmentation layers that perform data augmentation operations on the input audio signals fed into the neural network architecture.</p><p id="p-0044" num="0043">During the training phase, the analytics server <b>102</b> receives training audio signals of various lengths and attributes (e.g., sample rate, types of degradation, bandwidth) from one or more corpora, which may be stored in an analytics database <b>104</b> or other storage medium. In some embodiments, the training audio signals may include clean audio signals (sometimes referred to as samples) and simulated or degraded audio signals, each of which the analytics server <b>102</b> uses to train the various sub-components of the machine-learning architecture. The analytics server <b>102</b> applies the machine-learning architecture on the training audio signals to train the various sub-components, such as the embedding extractors, embedding convertors, scoring layers, classifier layers, feature extraction layers, and pre-processing layers, among others.</p><p id="p-0045" num="0044">In some cases, the data augmentation operations of the machine-learning architecture may generate simulated or degraded audio signals for a given input audio signal (e.g., training signal, enrollment signal), in which the simulated or degraded audio signal contains manipulated features of the input audio signal mimicking the effects a particular type of signal degradation or distortion on the input audio signal. The analytics server <b>102</b> stores the training audio signals into the non-transitory medium of the analytics server <b>102</b> and/or the analytics database <b>104</b> for future reference or operations of the machine-learning architecture, including current or future data augmentation operations for the training or enrollment phases.</p><p id="p-0046" num="0045">During the training phase and, in some implementations, the enrollment phase, the embedding extractor outputs predicted embeddings and the embedding convertors outputs predicted converted embeddings. One or more fully-connected and/or feed-forward layers generate and output predicted embeddings or the predicted converted embeddings for the training audio signals. Loss layers perform various loss functions to evaluate the distances between the predicted embeddings or predicted converted embedding and expected embeddings or expected converted embeddings, as indicated by labels associated with training signals or enrollment signals. The loss layers, or other functions of the machine-learning architecture, tune the hyper-parameters of the sub-components of the machine-learning architecture until the distance between the predicted embedding and the expected embedding satisfies a training threshold distance, or the predicted converted embedding and the expected converted embedding satisfies a training threshold distance.</p><p id="p-0047" num="0046">During the enrollment operational phase, an enrollee speaker provides (to the call analytics system <b>101</b>) a number of enrollee audio signals containing examples of the enrollee speech. As an example, the enrollee could respond to various interactive voice response (IVR) prompts of IVR software executed by a call center server <b>111</b> via a telephone channel. As another example, the enrollee could respond to various prompts generated by the call center server <b>111</b> and exchanged with a software application of the edge device <b>114</b><i>d </i>via a corresponding data communications channel. The call center server <b>111</b> then forwards the recorded responses containing bona fide enrollment audio signals to the analytics server <b>102</b>. The analytics server <b>102</b> applies the trained machine-learning architecture to each of the enrollee audio samples and generates corresponding enrollment feature vectors or converted enrollment embedding. In some implementations, the analytics server <b>102</b> disables certain layers, such as layers employed for training the machine-learning architecture. In some cases, the analytics server <b>102</b> generate an averages or otherwise algorithmically combines the enrollment embeddings extracted by the embedding extractor into an enrolled voiceprint and stores the enrollee embeddings and the enrolled voiceprint into the analytics database <b>104</b> or the call center database <b>112</b>. Additionally or alternatively, the analytics server <b>102</b> generates converted embeddings of a second-type corresponding to the enrollment embeddings of the first-type, as extracted by the embedding extractor. The machine-learning architecture averages or otherwise algorithmically combines the converted enrollment embeddings into a converted enrolled voiceprint and stores the converted enrolled voiceprint into the analytics database <b>104</b> or the call center database <b>112</b>. Similar details of the training and enrollment phases for the speaker verification neural network have described in U.S. patent application Ser. No. 17/066,210, which is incorporated by reference.</p><p id="p-0048" num="0047">Following the training phase, the analytics server <b>102</b> stores the trained machine-learning architecture and sub-components (e.g., trained embedding extractors, trained embedding convertors) into the analytics database <b>104</b> or call center database <b>112</b>. When a call center server <b>111</b>, agent device <b>116</b>, admin device <b>103</b>, or user device <b>114</b> instructs the analytics server <b>102</b> to enter an enrollment phase for extracting features of enrollee audio signals or tuning the tuning the machine-learning architecture for the enrollee audio signals, the analytics server <b>102</b> retrieves the trained machine-learning architecture from the database <b>104</b>, <b>112</b>. The analytics server <b>102</b> then stores the extracted enrollee embeddings and trained machine-learning architecture into the database <b>104</b>, <b>112</b> for the deployment phase.</p><p id="p-0049" num="0048">During the deployment phase, the analytics server <b>102</b> receives the inbound audio signal of the inbound call, as originated from the end-user device <b>114</b> of an inbound caller through a particular communications channel. The analytics server <b>102</b> applies machine-learning architecture on the inbound audio signal to extract the inbound features from the inbound audio signal, generates an inbound voiceprint, and determines whether the inbound speaker is an enrollee by comparing the inbound voiceprint against the converted enrolled voiceprint for enrollee.</p><p id="p-0050" num="0049">During deployment, the analytics server <b>102</b> applies the operational layers of the machine-learning architecture, such as the input layers (e.g., pre-processing layers) and the embedding extraction layers on the inbound audio signal, thereby extracting the inbound voiceprint. In some embodiments, the analytics server <b>102</b> may disable certain layers employed for training or enrollment (e.g., classification layers, loss layers). The machine-learning architecture applies the scoring layers of the machine-learning architecture on the inbound voiceprint and the converted enrolled voiceprint, where both the inbound voiceprint and the converted enrolled voiceprint are the same type of embedding.</p><p id="p-0051" num="0050">In some cases, the machine-learning architecture includes a plurality of trained embedding convertors and converted enrolled voiceprints. For instance, the identification server <b>102</b> may train three embedding convertors for converting embeddings between the three types of embeddings. In such cases, the analytics server <b>102</b> evaluates the types of signal or embedding attributes of the inbound signal or inbound voiceprint to determine which of the converted enrollment embeddings to compare against the inbound embedding.</p><p id="p-0052" num="0051">In some embodiments, following the deployment phase, the analytics server <b>102</b> (or another device of the system <b>100</b>) may execute any number of various downstream operations employing the converted enrollment voiceprint, the verification determine, or other outputs of the scoring layers, where the downstream operations may include, for example, a speaker authentication operation or speaker diarization.</p><p id="p-0053" num="0052">The analytics database <b>104</b> and/or the call center database <b>112</b> may contain any number of corpora of training audio signals that are accessible to the analytics server <b>102</b> via one or more networks. In some embodiments, the analytics server <b>102</b> employs supervised training to train the neural network, where the analytics database <b>104</b> includes labels associated with the training audio signals that indicate, for example, the signal attributes (e.g., sampling rate, bandwidth) or features of the training signals. The analytics server <b>102</b> may also query an external database (not shown) to access a third-party corpus of training audio signals or training embeddings. An administrator may configure the analytics server <b>102</b> to select the training audio signals having certain sampling rates or other characteristics.</p><p id="p-0054" num="0053">In some embodiments, the call center server <b>111</b> of a call center system <b>110</b> executes software processes for managing a call queue and/or routing calls made to the call center system <b>110</b> through the various channels, where the processes may include, for example, routing calls to the appropriate call center agent devices <b>116</b> based on the inbound caller's comments, instructions, IVR inputs, or other inputs submitted during the inbound call. The call center server <b>111</b> can capture, query, or generate various types of information about the call, the caller, and/or the end-user device <b>114</b> and forward the information to the agent device <b>116</b>, where a graphical user interface (GUI) of the agent device <b>116</b> displays the information to the call center agent. The call center server <b>111</b> also transmits the information about the inbound call to the call analytics system <b>101</b> to preform various analytics processes on the inbound audio signal and any other audio data. The call center server <b>111</b> may transmit the information and the audio data based upon preconfigured triggering conditions (e.g., receiving the inbound phone call), instructions or queries received from another device of the system <b>100</b> (e.g., agent device <b>116</b>, admin device <b>103</b>, analytics server <b>102</b>), or as part of a batch transmitted at a regular interval or predetermined time.</p><p id="p-0055" num="0054">The admin device <b>103</b> of the call analytics system <b>101</b> is a computing device allowing personnel of the call analytics system <b>101</b> to perform various administrative tasks or user-prompted analytics operations. The admin device <b>103</b> may be any computing device comprising a processor and software, and capable of performing the various tasks and processes described herein. Non-limiting examples of the admin device <b>103</b> may include a server, personal computer, laptop computer, tablet computer, or the like. In operation, the user employs the admin device <b>103</b> to configure the operations of the various components of the call analytics system <b>101</b> or call center system <b>110</b> and to issue queries and instructions to such components.</p><p id="p-0056" num="0055">The agent device <b>116</b> of the call center system <b>110</b> may allow agents or other users of the call center system <b>110</b> to configure operations of devices of the call center system <b>110</b>. For calls made to the call center system <b>110</b>, the agent device <b>116</b> receives and displays some or all of the relevant information associated with the call routed from the call center server <b>111</b>.</p><p id="p-0057" num="0056">Machine-Learning Architecture Training</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows steps of a method <b>200</b> for training a machine-learning architecture for speaker verification, including functions or layers defining embedding extractors and one or more embedding convertors. Embodiments may include additional, fewer, or different operations than those described in the method <b>200</b>. The method <b>200</b> is performed by a server executing machine-readable software code of the neural network architectures, though it should be appreciated that the various operations may be performed by one or more computing devices and/or processors.</p><p id="p-0059" num="0058">In operation <b>202</b>, the server obtains one or more training signals and extracts various types of features used for generating feature vectors, which the machine-learning architecture references, in turn, for verifying speakers or speaker devices. The server obtains the training signals by retrieving the training signals from a database or other data source. The server places the machine-learning architecture or sub-components of the machine-learning architecture (e.g., embedding extractors, embedding convertors) into a training operational phase, and obtains any number of training audio signals (sometimes thousands, hundreds of thousands, or more). The training audio signals may include any combination or permutation of types of signal attributes, such as a mixture of sampling rates (e.g., 8 kHz training signals, 16 kHz training signals).</p><p id="p-0060" num="0059">The machine-learning architecture may include various types of input layers for performing pre-processing operations and, in some implementations, data augmentation operations. The server or layers of the machine-learning architecture may perform various pre-processing operations on an input audio signal (e.g., training audio signal, enrollment audio signal, inbound audio signal). The pre-processing operations may include, for example, extracting low-level features from the audio signals and transforming such features from a time-domain representation into a frequency-domain representation by executing a transform operation, such as Fast-Fourier Transform (FFT) or Sparse-Fourier Transform (SFT) operations. The pre-processing operations may also include parsing the audio signals into frames or sub-frames, and performing various normalization or scaling operations. Optionally, the server performs any number of pre-processing operations prior to feeding the audio data into input layers of the machine-learning architecture. The server may perform the various pre-processing operations in one or more of the operational phases, though the particular pre-processing operations performed may vary across the operational phases. The server may perform the various pre-processing operations separately from the machine-learning architecture or as in-network layer of the machine-learning architecture.</p><p id="p-0061" num="0060">The server or layers of the machine-learning architecture may perform various augmentation operations on the input audio signal (e.g., training audio signal, enrollment audio signal). The augmentation operations generate various types of distortion or degradation for the input audio signal, such that the resulting audio signals are ingested by, for example, convolutional operations. The server may perform the various augmentation operations as separate operations from the neural network architecture or as in-network augmentation layers. Moreover, the server may perform the various augmentation operations in one or more of the operational phases, though in some cases, the particular augmentation operations performed may be different across each of the operational phases.</p><p id="p-0062" num="0061">In operation <b>204</b>, the server trains one or more the embedding extractors by applying each embedding extractors on the training features extracted from the training signals. The server applies each embedding extractor to each of the training audio signal to train the layers of embedding extractor, thereby training the embedding extractor to produce a predicted embedding for a given input signal.</p><p id="p-0063" num="0062">In operation <b>206</b>, the server performs one or more loss functions of the embedding extractors using the predicted embedding and updates any number of hyper-parameters of the machine-learning architecture. The embedding extractor (or other layers of the machine-learning architecture) comprises one or more loss layers for evaluating the level of error of the embedding extractor. The loss function determines the level of error of the embedding extractor based upon a similarity score indicating an amount of similarity (e.g., cosine distance) between a predicted output (e.g., predicted embedding, predicted classification) generated by the embedding extractor against an expected output (e.g., expected embedding, expected classification).</p><p id="p-0064" num="0063">In some implementations, the server references training label data associated with the training signals indicating the expected output for the training signal. The training signals include various information indicating, for example, the values or features of an expected embedding corresponding to the training signal. The loss layers may perform various loss functions (e.g., means-square error loss function) based upon the level of error (e.g., differences, similarities) between the predicted embedding and the expected embedding. The loss layers may adjust the weights or hyper-parameters of the embedding extractor (or other component of the machine-learning architecture) to improve the level of error for the embedding extractor, until a threshold level of error is satisfied.</p><p id="p-0065" num="0064">In operation <b>212</b>, when training is completed, the server stores the hyper-parameters into a database or memory of the server. In some implementations, the server may enable or disable one or more layers of the embedding extractor in order to keep the hyper-parameters fixed.</p><p id="p-0066" num="0065">Optionally, in operation <b>203</b>, the server obtains training embeddings for training one or more embedding convertors. In some cases, the server obtains the training embeddings by retrieving the training embeddings from a database. In some cases, the server obtains the training embeddings by applying an embedding extractor on training signals (obtained in <b>202</b>).</p><p id="p-0067" num="0066">In operation <b>208</b>, the server trains the embedding convertors of the machine-learning architecture by applying each embedding convertor on training embeddings of one or more types of embeddings. The server applies the machine-learning architecture on each type of the training embedding to train layers of the embedding convertor and, in some cases, one or more additional layers (e.g., fully-connected layers), thereby training the neural network architecture to convert embeddings of one type to embeddings of another type.</p><p id="p-0068" num="0067">In operation <b>210</b>, the server performs one or more loss functions of the embedding convertors using predicted converted embeddings and expected converted embeddings and updates any number of hyper-parameters of the embedding convertors. The embedding convertor (or other layers of the machine-learning architecture) comprises one or more loss layers for evaluating the level of error of the embedding convertor. The loss function determines the level of error of the embedding convertor based upon a similarity score indicating an amount of similarity (e.g., cosine distance) between a predicted output (e.g., predicted converted embedding, predicted classification) generated by the embedding convertor against an expected output (e.g., expected converted embedding, expected classification).</p><p id="p-0069" num="0068">In some implementations, the server references training label data associated with the training signals (and/or training embeddings) indicating the expected converted embedding for the training signal. The training signals include various information indicating, for example, the values or features of an expected converted embedding corresponding to the training signal. The loss layers may perform various loss functions (e.g., means-square error loss function) based upon the level of error (e.g., differences, similarities) between the predicted converted embedding and the expected converted embedding. The loss layers may adjust the weights or hyper-parameters of the embedding convertor (or other component of the machine-learning architecture) to improve the level of error for the embedding convertor, until a threshold level of error is satisfied.</p><p id="p-0070" num="0069">In operation <b>212</b>, when training is completed, the server stores the hyper-parameters for the embedding convertor into a database or memory of the server. In some implementations, the server may enable or disable one or more layers of the embedding convertor in order to keep the hyper-parameters fixed.</p><p id="p-0071" num="0070">User Enrollment and Speaker Verification</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows steps of a method <b>300</b> for implementing a machine-learning architecture for speaker verification, including functions or layers defining embedding extractors and one or more embedding convertors. Embodiments may include additional, fewer, or different operations than those described in the method <b>300</b>. The method <b>300</b> is performed by a server executing machine-readable software code of the neural network architectures, though it should be appreciated that the various operations may be performed by one or more computing devices and/or processors.</p><p id="p-0073" num="0072">In operation <b>302</b>, the server obtains enrollment signals, extracts features from the enrollment signals, and extracts the enrollment embeddings by applying one or more trained embedding extractors. The server may obtain the enrollment signals by retrieving the enrollment signals for an enrollee-speaker from a database, or by prompting the enrollee-speaker to provide spoken responses to various prompts. The server places the machine-learning architecture or sub-components of the machine-learning architecture (e.g., embedding extractors, embedding convertors) into an enrollment operational phase, and obtains any number of enrollment audio signals for the enrollee.</p><p id="p-0074" num="0073">The machine-learning architecture may include various types of input layers for performing pre-processing operations and, in some implementations, data augmentation operations for the enrollment signals. The server or layers of the machine-learning architecture may perform various pre-processing operations on the enrollment audio signal. The pre-processing operations may include, for example, extracting low-level features from the enrollment audio signals and transforming such features from a time-domain representation into a frequency-domain representation by executing a transform operation, such as Fast-Fourier Transform (FFT) or Sparse-Fourier Transform (SFT) operations. The pre-processing operations may also include parsing the enrollment audio signals into frames or sub-frames, and performing various normalization or scaling operations. Optionally, the server performs any number of pre-processing operations prior to feeding the enrollment audio signal into the input layers of the machine-learning architecture.</p><p id="p-0075" num="0074">The server or layers of the machine-learning architecture may perform various augmentation operations on the enrollment audio signal. The augmentation operations generate various types of distortion or degradation for the enrollment audio signal, such that the resulting enrollment audio signals are ingested by, for example, convolutional operations of the embedding extractors or embedding convertors. The server may perform the various augmentation operations as separate operations from the neural network architecture or as in-network augmentation layers. Moreover, the server may perform the various augmentation operations in one or more of the operational phases, though in some cases, the particular augmentation operations performed may be different across each of the operational phases.</p><p id="p-0076" num="0075">After extracting the enrollment features from the enrollment signals, the server applies a trained embedding extractor on the enrollment features. The embedding extractor outputs an enrollment embedding based upon certain types of attributes of the enrollment signals and/or a type of machine-learning technique employed by the embedding extractor. For example, where the enrollment signals have 8 kHz sampling rate, the enrollment embeddings reflect the 8 kHz enrollment signals and a first embedding extractor is trained to extract the enrollment embeddings having the 8 kHz sampling rate. As another example, the first embedding extractor implementing layers of a GMM technique, trained to extract the enrollment embeddings that reflect the GMM technique implemented by the first embedding extractor.</p><p id="p-0077" num="0076">In operation <b>304</b>, the server generates converted embedding(s) by applying trained embedding convertor(s). The embedding convertor comprises layers that map the feature vector space of a first type of embedding to the feature vector space of a second type of embedding. The embedding convertor takes as input the enrollment embeddings of the first type of embedding and generates as output corresponding, converted enrollment embeddings of the second type of embedding.</p><p id="p-0078" num="0077">For example, the first type of embedding includes the enrollment embeddings extracted for the enrollment signals having 8 kHz sampling rate using the first embedding extractor. The server applies the embedding convertor on the enrollment embeddings to convert the enrollment embeddings to a second type of embedding that a second enrollment extractor would otherwise generate. In this example, the embedding convertor generates converted enrollment embeddings that reflect signals having 16 kHz sampling rate.</p><p id="p-0079" num="0078">As another example, the first type of embedding includes the enrollment embeddings extracted by the first embedding extractor according to the GMM technique. The server applies the embedding convertor on the enrollment embeddings to convert the enrollment embeddings to the second type of embedding that a second enrollment extractor would otherwise generate according to a DNN technique. In this example, the embedding convertor generates converted enrollment embeddings that reflect the DNN technique.</p><p id="p-0080" num="0079">In operation <b>306</b>, the server generates and stores voiceprint(s) for a user profile by combining converted embedding(s). The server algorithmically combines the converted embeddings (e.g., determines an average) to generate a converted voiceprint of the second type of embedding. The server stores the converted voiceprint into the user profile of the enrollee, in a database.</p><p id="p-0081" num="0080">After completing the enrollment phase by generating the converted embeddings and voiceprints for the enrollee, the server then places the machine-learning architecture into a deployment phase.</p><p id="p-0082" num="0081">In operation <b>308</b>, the server receives an inbound signal, extracts inbound features from the inbound signals, and extracts an inbound voiceprint embedding by applying the second embedding extractor. The input layers of the machine-learning architecture perform the various pre-processing operations for ingesting the inbound signal and extracting the inbound features. The pre-processing operations may include, for example, extracting low-level features from the enrollment audio signals and transforming such features from a time-domain representation into a frequency-domain representation by executing a transform operation, such as Fast-Fourier Transform (FFT) or Sparse-Fourier Transform (SFT) operations. The pre-processing operations may also include parsing the inbound signal into frames or sub-frames, and performing various normalization or scaling operations. Optionally, the server performs any number of pre-processing operations prior to feeding the inbound signal into the input layers of the machine-learning architecture.</p><p id="p-0083" num="0082">For example, where the inbound signal has 16 kHz sampling rate, the inbound voiceprint embedding reflects the 16 kHz inbound signal and a second embedding extractor is trained to extract the inbound voiceprint based upon the 16 kHz sampling rate. As another example, the second embedding extractor includes layers implementing the DNN technique, trained to extract the inbound embedding that reflects the DNN technique implemented by the second embedding extractor.</p><p id="p-0084" num="0083">In operation <b>312</b>, the server generates a similarity score based on the inbound voiceprint and the converted voiceprint for the enrollee stored in the database. The server applies scoring layers of the machine-learning architecture on the inbound voiceprint and the converted voiceprint, where the inbound voiceprint and the converted voiceprint include the same type of embedding. Continuing with the earlier examples, both the inbound voiceprint and the converted voiceprint include embeddings based upon a vector space for 16 kHz signals or based upon the DNN technique of the second embedding extractor.</p><p id="p-0085" num="0084">The scoring layers generate, for example, a similarity score indicating the similarity (e.g., cosine distance) between the inbound voiceprint (of the inbound speaker) and the converted embedding (of the enrollee-speaker). The server identifies a match (or a likely match) between the inbound speaker and the enrollee when the similarity score satisfies a threshold value. In some embodiments, one or more downstream operations (e.g., speaker authentication, speaker diarization) reference the match determination, the similarity score, and/or the inbound voiceprint to perform the particular downstream operations.</p><p id="p-0086" num="0085">Enrollment Embeddings and Authentication Operations</p><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows data flow amongst layers of a machine-learning architecture <b>400</b> for speaker recognition including embedding convertors. Components of the machine-learning architecture <b>400</b> comprises input layers <b>402</b>, any number of embedding extractors <b>404</b> (e.g., first embedding extractor <b>404</b><i>a</i>, second embedding extractor <b>404</b><i>b</i>), any number of embedding convertors <b>406</b><i>a</i>-<b>406</b><i>n </i>(collectively referred to as &#x201c;embedding convertors <b>406</b>&#x201d;), and scoring layers <b>410</b>. For ease of description, the machine-learning architecture <b>400</b> is described as a single machine-learning architecture <b>400</b>, though embodiments may comprise a plurality of distinct machine-learning architectures <b>400</b> comprising software programming for performing the functions described herein. Moreover, embodiments may comprise additional or alternative components or functional layers than those described herein.</p><p id="p-0088" num="0087">The machine-learning architecture <b>400</b> is described as being executed by a server during enrollment and deployment operational phases for enrolling a new enrollee-speaker using enrollment signals <b>401</b><i>a</i>-<b>401</b><i>n </i>(collectively referred to as &#x201c;enrollment signals <b>401</b>&#x201d;) and verifying an inbound speaker using an inbound signal <b>409</b>. However, any computing device comprising a processor capable of performing the operations of the machine-learning architecture <b>400</b> may execute components of the machine-learning architecture <b>400</b>. Moreover, any number of such computing devices may perform the functions of the machine-learning architecture <b>400</b>. The machine-learning architecture <b>400</b> includes input layers <b>402</b> for ingesting the audio signals <b>401</b>, <b>409</b>, which includes layers for pre-processing (e.g., feature extraction, feature transforms) and data augmentation operations; layers that define any number of embedding extractors <b>404</b> (e.g., first embedding extractor <b>404</b><i>a</i>, second embedding extractor <b>404</b><i>b</i>) for generating speaker embeddings <b>403</b>, <b>411</b>; layers that define embedding convertors <b>406</b><i>a</i>-<b>406</b><i>n </i>(collectively referred to as &#x201c;embedding convertors <b>406</b>&#x201d;); and one or more scoring layers <b>410</b> that perform various scoring and verification operations, such as a distance scoring operation, to produce one or more verification outputs <b>413</b>.</p><p id="p-0089" num="0088">The input layers <b>402</b> perform one or more pre-processing operations on the input audio signals <b>401</b>, <b>409</b> (e.g., enrollment signals <b>401</b>, inbound signals <b>409</b>), such as parsing the input audio signals <b>401</b>, <b>409</b> into frames or segments, extracting low-level features, and transforming the input audio signals <b>401</b>, <b>409</b> from a time-domain representation to a frequency-domain (or energy domain) representation, among other pre-processing operations.</p><p id="p-0090" num="0089">During the enrollment phase, the input layers <b>402</b> receive the enrollment audio signals <b>401</b> for the enrollee. In some implementations, the input layers <b>402</b> perform data augmentation operations on the enrollment audio signals <b>401</b> to, for example, manipulate the audio signals within the enrollment audio signals <b>401</b>, manipulate the low-level features, or generate simulated enrollment audio signals <b>401</b> that have manipulated features or audio signal based on corresponding enrollment audio signals <b>401</b>. An enrollee-speaker contacts a service provider's system and supplies several example enrollment signals <b>401</b>. For example, the speaker responds to various questions or prompts with spoken responses that serve as the enrollment signals <b>401</b>, where the service provider system presents the questions or prompts to the enrollee by an IVR system or by a human agent of the service provider system. The server receives the enrollee's spoken responses in the enrollment signals <b>401</b>. The server feeds the resulting enrollment signals <b>401</b> into the input layers <b>402</b> to begin applying machine-learning architecture <b>400</b>.</p><p id="p-0091" num="0090">During the deployment phase, the input layers <b>402</b> may perform the pre-processing operations to prepare an inbound signal <b>409</b> for the embedding extractor <b>404</b>. The server may disable some or all of the pre-processing and/or augmentation operations of the input layers <b>402</b>, such that the second embedding extractor <b>404</b><i>b </i>evaluates the features of the inbound signal <b>409</b> as received.</p><p id="p-0092" num="0091">The embedding extractors <b>404</b> include layers that extract embedding based upon the types of attributes of the input signals (e.g., enrollment signals <b>401</b>, inbound signal <b>409</b>) or the types of embeddings (e.g., GMM-based embeddings, DNN-based embeddings). Each embedding extractor <b>404</b> comprises one or more layers of the machine-learning architecture <b>400</b> trained (during a training phase) to detect speech, extract features, and generate feature vectors based on the features extracted from the input audio signals <b>401</b>, <b>409</b>.</p><p id="p-0093" num="0092">During the enrollment phase, the embedding extractor <b>404</b> outputs the feature vectors as enrollment embeddings <b>403</b> or as enrolled voiceprints. The server applies the one or more embedding extractors <b>404</b> on the features extracted from each of the enrollment signals <b>401</b> to produce enrollment feature vectors (e.g., enrollment embeddings <b>403</b>, voiceprints). In some implementations, the embedding extractor <b>404</b> (or other layers of the machine-learning architecture <b>400</b>) performs various statistical or algorithmic operations to combine the enrollment embeddings <b>403</b> of the same type (i.e., generated from the same embedding extractor <b>404</b>) to generate a corresponding enrolled voiceprint (not shown) for a user profile of the enrollee.</p><p id="p-0094" num="0093">For instance, the server applies the first embedding extractor <b>404</b><i>a </i>on the enrollment signals <b>401</b> to extract the enrollment embeddings <b>403</b> of the first type of embeddings, as generated by the first embedding extractor <b>404</b><i>a</i>. The first embedding extractor <b>404</b><i>a </i>(or other layers of the machine-learning architecture <b>400</b>) algorithmically combines the first type of enrollment embeddings <b>403</b> to form a first type of enrolled voiceprint (not shown), which the server stores into the user profile in a database <b>424</b> or other non-transitory storage medium. The machine-learning architecture <b>400</b> may reference the first-type of enrollment embeddings <b>403</b> or the first type of enrolled voiceprint (not shown) during the deployment phase, allowing the server to verify an inbound speaker of a future, inbound signal <b>409</b> using the first embedding extractor <b>404</b><i>a </i>and the scoring layers <b>410</b>.</p><p id="p-0095" num="0094">The embedding convertors <b>406</b> include layers of machine-learning architecture <b>400</b> that convert a first type of embedding to a second type of embedding. The machine-learning architecture <b>400</b> includes any number of embedding convertors <b>406</b>, where the amount of embedding convertors <b>406</b> is based upon the number of trained embedding extractors <b>404</b> employed by the machine-learning architecture <b>400</b>. For instance, where the server employs two embedding extractors <b>404</b>, the machine-learning architecture <b>400</b> may include one or two embedding convertors <b>406</b>. Each embedding convertor <b>406</b> is trained to take an enrollment embedding <b>403</b> of a particular type of embedding as input, and generate corresponding converted enrollment embeddings <b>405</b> of a different type of embedding. For instance, the embedding convertor <b>406</b> takes as input the enrollment embeddings <b>403</b> of the first type of embedding (produced by the first embedding extractor <b>404</b><i>a</i>) and outputs the converted enrollment embeddings <b>405</b> of the second type of embedding (produced by the second embedding extractor <b>404</b><i>b</i>). The embedding convertor <b>406</b> (or other layers of the machine-learning architecture <b>400</b>) algorithmically combines the converted enrollment embeddings <b>405</b> of the second to generate a converted enrolled voiceprint <b>407</b> of the different type of embedding for the enrollee-user. The server stores the converted enrollment embeddings <b>405</b> and/or the converted enrolled voiceprints <b>407</b> for future reference during the deployment phase. The server may apply the embedding convertors <b>406</b> at any point-in-time after generating the enrollment embeddings <b>403</b>.</p><p id="p-0096" num="0095">As an example, the first embedding extractor <b>404</b><i>a </i>is trained to extract embeddings based on 8 kHz input signals (as the first type of embedding), and the second embedding extractor <b>404</b><i>b </i>is trained to extract embeddings based on 16 kHz input signals (as the second type of embedding). In this example, the first embedding extractor <b>404</b><i>a </i>extracts the enrollment embeddings <b>403</b> based on the 8 kHz enrollment signals <b>401</b>. A first embedding convertor <b>406</b><i>a </i>is trained to take the 8 kHz-based enrollment embeddings <b>403</b> as input and generate the corresponding converted enrollment embeddings <b>405</b> as the second type of embedding (as though generated by the second embedding extractor <b>404</b><i>b </i>using 16 kHz audio signals).</p><p id="p-0097" num="0096">As another example, the first embedding extractor <b>404</b><i>a </i>includes a trained GMM for extracting embeddings (as the first type of embedding), and the second embedding extractor <b>404</b><i>b </i>includes a trained neural network architecture for extracting embeddings (as the second type of embedding). In this example, the first embedding convertor <b>406</b><i>a </i>is trained to take the first type of enrollment embeddings <b>403</b> (GMM-based embeddings) as input, and generate the corresponding converted enrollment embeddings <b>405</b> of the second type of embedding (DNN-based embeddings, as though generated by the second embedding extractor <b>404</b><i>b</i>).</p><p id="p-0098" num="0097">The scoring layers <b>410</b> perform various scoring operations and generating various types of verification outputs <b>413</b> for an inbound signal <b>409</b> involving an inbound speaker. The second embedding extractor <b>404</b><i>b </i>extracts an inbound voiceprint <b>411</b> representing the features extracted from the inbound signal <b>409</b>. The scoring layers <b>410</b> perform a distance scoring operation that determines the distance (e.g., similarities, differences) between the converted enrolled voiceprint <b>407</b> stored in the voiceprint database <b>424</b> and the inbound voiceprint <b>411</b>, indicating the likelihood that the inbound speaker is the enrollee. For instance, a lower distance score (or higher similarity score) for the inbound signal <b>409</b> indicates nearer or more similarities between the converted enrolled voiceprint <b>407</b> and the inbound voiceprint <b>411</b>, thereby indicating a higher-likelihood that the inbound speaker is the enrollee. The scoring layer <b>410</b> may generate and produce a verification output <b>413</b> based upon the scoring operations. The verification output(s) <b>413</b> may include, for example, a value generated by the scoring layer <b>410</b> based upon one or more scoring operations (e.g., cosine distance scoring), visual indicator for a GUI, and/or instructions or data for a downstream application.</p><p id="p-0099" num="0098">During the deployment phase, the input layer <b>402</b> extracts the inbound features for the inbound signal <b>409</b> and performs various pre-processing operations on the inbound features, such as a transform operation (e.g., Fast-Fourier Transform). The server applies the second embedding extractor <b>404</b><i>b </i>on the inbound features and extracts the inbound voiceprint <b>411</b> for the inbound speaker. To verify or authenticate the inbound speaker of the inbound signal <b>409</b> as the enrolled speaker, the server applies the scoring layer <b>410</b> on the inbound voiceprint <b>411</b> (for the inbound speaker) and the converted enrolled voiceprint <b>407</b> (for the enrolled speaker). The scoring layer <b>410</b> performs various scoring operations to generate one or more verification outputs <b>413</b>, which includes a scoring operation that generates similarity score that indicates the similarity (e.g., cosine distance) between the inbound voiceprint <b>411</b> and the converted enrolled voiceprint <b>407</b>. The scoring layers <b>410</b> determine whether the similarity score or other outputted values satisfy corresponding threshold values.</p><p id="p-0100" num="0099">The verification output <b>413</b> need not be a numeric output. For example, the verification output <b>413</b> may be a human-readable indicator (e.g., plain language, visual display) that indicates whether the machine-learning architecture <b>400</b> has recognized or authenticated the inbound speaker as the enrolled speaker. As another example, the verification output <b>413</b> may include a machine-readable detection indicator or authentication instruction, which the server transmits via one or more networks to computing devices performing one or more downstream applications.</p><p id="p-0101" num="0100">In some embodiments, a computer-implemented method comprises obtaining, by a computer, a plurality of enrollment embeddings extracted using a plurality of enrollment signals for an enrolled user by applying a first embedding extractor for a first attribute-type; generating, by the computer, a plurality of converted embeddings corresponding to the plurality of enrollment embeddings by applying an embedding convertor comprising a plurality of machine-learning layers trained to generate a converted embedding having a second attribute-type for an enrollment embedding having a first attribute-type; generating, by the computer, a converted enrolled voiceprint having the second attribute-type for the enrolled user based upon the plurality of converted embeddings; generating, by the computer, an inbound voiceprint for an inbound user extracted using an inbound signal for an inbound user by applying a second embedding extractor for the second attribute-type; and generating, by the computer, a similarity score for the inbound signal using the converted enrolled voiceprint and the inbound voiceprint, the similarity score indicating a likelihood that the inbound user is the enrolled user.</p><p id="p-0102" num="0101">In some implementations, the method comprises obtaining, by the computer, a plurality of training embeddings extracted using a plurality of training signals by applying the first embedding extractor for the first attribute-type; and training, by the computer, the embedding convertor by applying the machine-learning layers of the embedding convertor on the plurality of training embeddings.</p><p id="p-0103" num="0102">In some implementations, training the embedding extractor includes performing, by the computer, a loss function of the embedding extractor according to a predicted converted embedding outputted by the embedding extractor for a training audio signal, the loss function instructing the computer to update one or more hyper-parameters of one or more layers of the embedding extractor.</p><p id="p-0104" num="0103">In some implementations, training the embedding extractor includes executing, by the computer, one or more data augmentation operations on at least of a training audio signal and an enrollment signal.</p><p id="p-0105" num="0104">In some implementations, the computer trains a plurality of embedding convertors according to a plurality of attribute-types.</p><p id="p-0106" num="0105">In some implementations, the computer generates a plurality of converted enrolled voiceprints by applying the plurality of embedding convertors corresponding to the plurality of attribute-types on the plurality of embedding signals.</p><p id="p-0107" num="0106">In some implementations, the method further comprises identifying, by the computer, the second attribute-type of the inbound embedding; and selecting, by the computer, the converted enrolled voiceprint from a plurality of converted enrolled voiceprints according to the second attribute-type.</p><p id="p-0108" num="0107">In some implementations, generating the converted enrolled voiceprint having the second attribute-type includes storing, by the computer, the converted enrolled voiceprint into a user profile database.</p><p id="p-0109" num="0108">In some implementations, generating the converted enrolled voiceprint having the second attribute-type includes algorithmically combining, by the computer, the converted enrollment embeddings having the second attribute-type.</p><p id="p-0110" num="0109">In some implementations, generating a plurality of converted embeddings includes, for each enrollment signal: extracting, by the computer, a set of enrollment features from an enrollment signal; and extracting, by the computer, an enrollment embedding based upon the set of features extracted from the enrollment audio signal by applying the first embedding extractor for the first attribute-type.</p><p id="p-0111" num="0110">In some embodiments, a system comprises a non-transitory machine-readable memory configured to store machine-readable instructions for one or more neural networks; and a computer comprising a processor. The computer configured to obtain a plurality of enrollment embeddings extracted using a plurality of enrollment signals for an enrolled user by applying a first embedding extractor for a first attribute-type; generate a plurality of converted embeddings corresponding to the plurality of enrollment embeddings by applying an embedding convertor comprising a plurality of machine-learning layers trained to generate a converted embedding having a second attribute-type for an enrollment embedding having a first attribute-type; generate a converted enrolled voiceprint having the second attribute-type for the enrolled user based upon the plurality of converted embeddings; generate an inbound voiceprint for an inbound user extracted using an inbound signal for an inbound user by applying a second embedding extractor for the second attribute-type; and generate a similarity score for the inbound signal using the converted enrolled voiceprint and the inbound voiceprint, the similarity score indicating a likelihood that the inbound user is the enrolled user.</p><p id="p-0112" num="0111">In some implementations, the computer is further configured to obtain a plurality of training embeddings extracted using a plurality of training signals by applying the first embedding extractor for the first attribute-type; and train the embedding convertor by applying the machine-learning layers of the embedding convertor on the plurality of training embeddings.</p><p id="p-0113" num="0112">In some implementations, when training the embedding extractor the computer is further configured to perform a loss function of the embedding extractor according to a predicted converted embedding outputted by the embedding extractor for a training audio signal, the loss function instructing the computer to update one or more hyper-parameters of one or more layers of the embedding extractor.</p><p id="p-0114" num="0113">In some implementations, when training the embedding extractor the computer is further configured to execute one or more data augmentation operations on at least of a training signal and an enrollment signal.</p><p id="p-0115" num="0114">In some implementations, the computer trains a plurality of embedding convertors according to a plurality of attribute-types.</p><p id="p-0116" num="0115">In some implementations, the computer generates a plurality of converted enrolled voiceprints by applying the plurality of embedding convertors corresponding to the plurality of attribute-types on the plurality of embedding signals.</p><p id="p-0117" num="0116">In some implementations, the computer is further configured to identify the second attribute-type of the inbound embedding; and select the converted enrolled voiceprint from a plurality of converted enrolled voiceprints according to the second attribute-type.</p><p id="p-0118" num="0117">In some implementations, when generating the converted enrolled voiceprint having the second attribute-type the computer is further configured to store the converted enrolled voiceprint into a user profile database.</p><p id="p-0119" num="0118">In some implementations, when generating the converted enrolled voiceprint having the second attribute-type the computer is further configured to algorithmically combine the converted enrollment embeddings having the second attribute-type.</p><p id="p-0120" num="0119">In some implementations, when generating a plurality of converted embeddings the computer is further configured to, for each enrollment signal: extract a set of enrollment features from an enrollment signal; and extract an enrollment embedding based upon the set of features extracted from the enrollment audio signal by applying the first embedding extractor for the first attribute-type.</p><p id="p-0121" num="0120">The various illustrative logical blocks, modules, circuits, and algorithm steps described in connection with the embodiments disclosed herein may be implemented as electronic hardware, computer software, or combinations of both. To clearly illustrate this interchangeability of hardware and software, various illustrative components, blocks, modules, circuits, and steps have been described above generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled artisans may implement the described functionality in varying ways for each particular application, but such implementation decisions should not be interpreted as causing a departure from the scope of the present invention.</p><p id="p-0122" num="0121">Embodiments implemented in computer software may be implemented in software, firmware, middleware, microcode, hardware description languages, or any combination thereof. A code segment or machine-executable instructions may represent a procedure, a function, a subprogram, a program, a routine, a subroutine, a module, a software package, a class, or any combination of instructions, data structures, or program statements. A code segment may be coupled to another code segment or a hardware circuit by passing and/or receiving information, data, arguments, attributes, or memory contents. Information, arguments, attributes, data, etc. may be passed, forwarded, or transmitted via any suitable means including memory sharing, message passing, token passing, network transmission, etc.</p><p id="p-0123" num="0122">The actual software code or specialized control hardware used to implement these systems and methods is not limiting of the invention. Thus, the operation and behavior of the systems and methods were described without reference to the specific software code being understood that software and control hardware can be designed to implement the systems and methods based on the description herein.</p><p id="p-0124" num="0123">When implemented in software, the functions may be stored as one or more instructions or code on a non-transitory computer-readable or processor-readable storage medium. The steps of a method or algorithm disclosed herein may be embodied in a processor-executable software module which may reside on a computer-readable or processor-readable storage medium. A non-transitory computer-readable or processor-readable media includes both computer storage media and tangible storage media that facilitate transfer of a computer program from one place to another. A non-transitory processor-readable storage media may be any available media that may be accessed by a computer. By way of example, and not limitation, such non-transitory processor-readable media may comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other tangible storage medium that may be used to store desired program code in the form of instructions or data structures and that may be accessed by a computer or processor. Disk and disc, as used herein, include compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk, and Blu-Ray disc where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable media. Additionally, the operations of a method or algorithm may reside as one or any combination or set of codes and/or instructions on a non-transitory processor-readable medium and/or computer-readable medium, which may be incorporated into a computer program product.</p><p id="p-0125" num="0124">The preceding description of the disclosed embodiments is provided to enable any person skilled in the art to make or use the present invention. Various modifications to these embodiments will be readily apparent to those skilled in the art, and the generic principles defined herein may be applied to other embodiments without departing from the spirit or scope of the invention. Thus, the present invention is not intended to be limited to the embodiments shown herein but is to be accorded the widest scope consistent with the following claims and the principles and novel features disclosed herein.</p><p id="p-0126" num="0125">While various aspects and embodiments have been disclosed, other aspects and embodiments are contemplated. The various aspects and embodiments disclosed are for purposes of illustration and are not intended to be limiting, with the true scope and spirit being indicated by the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method comprising:<claim-text>obtaining, by a computer, a plurality of enrollment embeddings extracted using a plurality of enrollment signals for an enrolled user by applying a first embedding extractor for a first attribute-type;</claim-text><claim-text>generating, by the computer, a plurality of converted embeddings corresponding to the plurality of enrollment embeddings by applying an embedding convertor comprising a plurality of machine-learning layers trained to generate a converted embedding having a second attribute-type for an enrollment embedding having a first attribute-type;</claim-text><claim-text>generating, by the computer, a converted enrolled voiceprint having the second attribute-type for the enrolled user based upon the plurality of converted embeddings;</claim-text><claim-text>generating, by the computer, an inbound voiceprint for an inbound user extracted using an inbound signal for an inbound user by applying a second embedding extractor for the second attribute-type; and</claim-text><claim-text>generating, by the computer, a similarity score for the inbound signal using the converted enrolled voiceprint and the inbound voiceprint, the similarity score indicating a likelihood that the inbound user is the enrolled user.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>obtaining, by the computer, a plurality of training embeddings extracted using a plurality of training signals by applying the first embedding extractor for the first attribute-type; and</claim-text><claim-text>training, by the computer, the embedding convertor by applying the machine-learning layers of the embedding convertor on the plurality of training embeddings.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein training the embedding extractor includes:<claim-text>performing, by the computer, a loss function of the embedding extractor according to a predicted converted embedding outputted by the embedding extractor for a training audio signal, the loss function instructing the computer to update one or more hyper-parameters of one or more layers of the embedding extractor.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein training the embedding extractor includes executing, by the computer, one or more data augmentation operations on at least of a training audio signal and an enrollment signal.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the computer trains a plurality of embedding convertors according to a plurality of attribute-types.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the computer generates a plurality of converted enrolled voiceprints by applying the plurality of embedding convertors corresponding to the plurality of attribute-types on the plurality of embedding signals.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising:<claim-text>identifying, by the computer, the second attribute-type of the inbound embedding; and</claim-text><claim-text>selecting, by the computer, the converted enrolled voiceprint from a plurality of converted enrolled voiceprints according to the second attribute-type.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the converted enrolled voiceprint having the second attribute-type includes storing, by the computer, the converted enrolled voiceprint into a user profile database.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the converted enrolled voiceprint having the second attribute-type includes algorithmically combining, by the computer, the converted enrollment embeddings having the second attribute-type.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating a plurality of converted embeddings includes, for each enrollment signal:<claim-text>extracting, by the computer, a set of enrollment features from an enrollment signal; and</claim-text><claim-text>extracting, by the computer, an enrollment embedding based upon the set of features extracted from the enrollment audio signal by applying the first embedding extractor for the first attribute-type.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A system comprising:<claim-text>a non-transitory machine-readable memory configured to store machine-readable instructions for one or more neural networks; and</claim-text><claim-text>a computer comprising a processor configured to:<claim-text>obtain a plurality of enrollment embeddings extracted using a plurality of enrollment signals for an enrolled user by applying a first embedding extractor for a first attribute-type;</claim-text><claim-text>generate a plurality of converted embeddings corresponding to the plurality of enrollment embeddings by applying an embedding convertor comprising a plurality of machine-learning layers trained to generate a converted embedding having a second attribute-type for an enrollment embedding having a first attribute-type;</claim-text><claim-text>generate a converted enrolled voiceprint having the second attribute-type for the enrolled user based upon the plurality of converted embeddings;</claim-text><claim-text>generate an inbound voiceprint for an inbound user extracted using an inbound signal for an inbound user by applying a second embedding extractor for the second attribute-type; and</claim-text><claim-text>generate a similarity score for the inbound signal using the converted enrolled voiceprint and the inbound voiceprint, the similarity score indicating a likelihood that the inbound user is the enrolled user.</claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the computer is further configured to:<claim-text>obtain a plurality of training embeddings extracted using a plurality of training signals by applying the first embedding extractor for the first attribute-type; and</claim-text><claim-text>train the embedding convertor by applying the machine-learning layers of the embedding convertor on the plurality of training embeddings.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein when training the embedding extractor the computer is further configured to:<claim-text>perform a loss function of the embedding extractor according to a predicted converted embedding outputted by the embedding extractor for a training audio signal, the loss function instructing the computer to update one or more hyper-parameters of one or more layers of the embedding extractor.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein when training the embedding extractor the computer is further configured to execute one or more data augmentation operations on at least of a training signal and an enrollment signal.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the computer trains a plurality of embedding convertors according to a plurality of attribute-types.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the computer generates a plurality of converted enrolled voiceprints by applying the plurality of embedding convertors corresponding to the plurality of attribute-types on the plurality of embedding signals.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the computer is further configured to:<claim-text>identify the second attribute-type of the inbound embedding; and</claim-text><claim-text>select the converted enrolled voiceprint from a plurality of converted enrolled voiceprints according to the second attribute-type.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein when generating the converted enrolled voiceprint having the second attribute-type the computer is further configured to store the converted enrolled voiceprint into a user profile database.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein when generating the converted enrolled voiceprint having the second attribute-type the computer is further configured to algorithmically combine the converted enrollment embeddings having the second attribute-type.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein when generating a plurality of converted embeddings the computer is further configured to, for each enrollment signal:<claim-text>extract a set of enrollment features from an enrollment signal; and</claim-text><claim-text>extract an enrollment embedding based upon the set of features extracted from the enrollment audio signal by applying the first embedding extractor for the first attribute-type.</claim-text></claim-text></claim></claims></us-patent-application>