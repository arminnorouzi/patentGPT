<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004754A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004754</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17363043</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6257</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6232</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>1208</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>1226</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">ADVERSARIAL IMAGE GENERATOR</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>International Business Machines Corporation</orgname><address><city>Armonk</city><state>NY</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Fan</last-name><first-name>Quanfu</first-name><address><city>Lexington</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Liu</last-name><first-name>Sijia</first-name><address><city>Somerville</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>ZHANG</last-name><first-name>GAOYUAN</first-name><address><city>Medford</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Xu</last-name><first-name>Kaidi</first-name><address><city>Boston</city><state>MA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Adversarial patches can be inserted into sample pictures by an adversarial image generator to realistically depict adversarial images. The adversarial image generator can be utilized to train an adversarial patch generator by inserting generated patches into sample pictures, and submitting the resulting adversarial images to object detection models. This way, the adversarial patch generator can be trained to generate patches capable of defeating object detection models.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="211.24mm" wi="158.75mm" file="US20230004754A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="174.58mm" wi="131.15mm" file="US20230004754A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="231.90mm" wi="165.78mm" file="US20230004754A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="238.25mm" wi="148.93mm" file="US20230004754A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="136.57mm" wi="226.14mm" file="US20230004754A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="172.13mm" wi="234.44mm" file="US20230004754A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="170.52mm" wi="229.28mm" file="US20230004754A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="231.90mm" wi="151.81mm" file="US20230004754A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="136.99mm" wi="213.44mm" file="US20230004754A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="231.90mm" wi="158.67mm" file="US20230004754A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="236.56mm" wi="172.30mm" file="US20230004754A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">The systems and methods of the present disclosure relate to object detection.</p><p id="p-0003" num="0002">Adversarial patches are an emerging technology designed to deceive automated object detection, particularly machine-learning-based recognition. As an example, a camera observing a space may transmit a video feed to a machine learning model trained to identify people seen by the camera. In response, a person may wear clothing with an adversarial patch (for example, an adversarial patch may be a pattern printed on a T-shirt). To a human observer, an adversarial patch may appear as a mass of seemingly random pixels, not unlike corrupted image data. However, the specific pattern of an adversarial patch, when analyzed by a machine learning model, may throw off results of the machine learning model (e.g., a system attempting to identify a person in an image may fail to do so simply due to the image including an adversarial patch). In this way, adversarial patches can serve as a digital form of camouflage.</p><p id="p-0004" num="0003">While adversarial patches may be useful to prevent unauthorized surveillance, malicious actors may use them to fool helpful object detection models. For example, adversarial patches have been placed on street signs in an apparent attempt to prevent automated vehicle safety systems from being able to recognize the signs.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0005" num="0004">Some embodiments of the present disclosure can be illustrated as a method. The method comprises training an adversarial image generator. The method also comprises receiving a sample picture depicting an object. The method also comprises receiving an adversarial patch. The method also comprises generating, via the adversarial image generator, an adversarial image, where the adversarial image depicts the adversarial patch in the sample picture with the object, and where the generating includes transforming the adversarial patch based on environmental attributes of the sample picture (resulting in a transformed adversarial patch) and inserting the transformed adversarial patch into the picture, resulting in the adversarial image.</p><p id="p-0006" num="0005">Some embodiments of the present disclosure can also be illustrated as a computer program product comprising a computer readable storage medium having program instructions embodied therewith, the program instructions executable by a computer to cause the computer to perform the method discussed above.</p><p id="p-0007" num="0006">Some embodiments of the present disclosure can be illustrated as a system. The system may comprise memory and a central processing unit (CPU). The CPU may be configured to execute instructions to perform the method discussed above.</p><p id="p-0008" num="0007">The above summary is not intended to describe each illustrated embodiment or every implementation of the present disclosure.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0009" num="0008">The drawings included in the present application are incorporated into, and form part of, the specification. They illustrate embodiments of the present disclosure and, along with the description, serve to explain the principles of the disclosure. The drawings are only illustrative of certain embodiments and do not limit the disclosure. Features and advantages of various embodiments of the claimed subject matter will become apparent as the following Detailed Description proceeds, and upon reference to the drawings, in which like numerals indicate like parts, and in which:</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a high-level method for training an adversarial patch generator to defeat object detection models via realistic adversarial images, consistent with several embodiments of the present disclosure.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a method for training an adversarial patch adversarial image generator, consistent with several embodiments of the present disclosure.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a method for modifying a sample picture into an adversarial image including an adversarial patch, consistent with several embodiments of the present disclosure.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is an example adversarial patch, consistent with several embodiments of the present disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is an example picture depicting a user wearing a shirt with a placeholder patch on it, consistent with several embodiments of the present disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b>C</figref> is a first example image after an adversarial image generator has inserted an adversarial patch into a picture, consistent with several embodiments of the present disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b>D</figref> is a second example image after a trained adversarial image generator has inserted an adversarial patch into a picture, consistent with several embodiments of the present disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow diagram depicting an example trained patch image generator, consistent with several embodiments of the present disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram showing an example implementation of a patch image generator, consistent with several embodiments of the present disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a detailed method for training an adversarial patch generator, consistent with several embodiments of the present disclosure.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> depicts an adversarial patch, consistent with several embodiments of the present disclosure.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> depicts a sample picture captured of the patch after being printed onto a shirt worn by a user, consistent with several embodiments of the present disclosure.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>8</b>C</figref> depicts an example pre-transformed adversarial patch, consistent with several embodiments of the present disclosure.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>8</b>D</figref> depicts a sample picture including a user wearing a shirt bearing a pre-transformed patch, consistent with several embodiments of the present disclosure.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a detailed method for training an object detection model to defeat adversarial patches, consistent with several embodiments of the present disclosure</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates a high-level block diagram of an example computer system that may be used in implementing embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0026" num="0025">While the invention is amenable to various modifications and alternative forms, specifics thereof have been shown by way of example in the drawings and will be described in detail. It should be understood, however, that the intention is not to limit the invention to the particular embodiments described. On the contrary, the intention is to cover all modifications, equivalents, and alternatives falling within the spirit and scope of the invention.</p><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0027" num="0026">Aspects of the present disclosure relate to systems and methods to design adversarial patches to overcome object detection models. More particular aspects relate to a system and a method to train an adversarial patch image generator, generate an adversarial patch from, modify images to include the patch, submit the modified images to an object detection model, and train an adversarial patch generator based on the object detection model performance and on the modified images.</p><p id="p-0028" num="0027">Adversarial patches are an emerging technology designed to deceive automated object detection, particularly machine-learning-based recognition. As an example, a camera observing a space may transmit a video feed to a machine learning model trained to identify people seen by the camera. In response, a person may wear clothing with an adversarial patch (for example, an adversarial patch may be a pattern printed on a T-shirt). To a human observer, an adversarial patch may appear as a mass of seemingly random pixels, similar to corrupted image data. However, the specific pattern of an adversarial patch, when analyzed by a machine learning model, may throw off results of the machine learning model (e.g., a system attempting to identify a person in an image may fail to do so simply due to the image including an adversarial patch). In this way, adversarial patches can serve as a digital form of camouflage.</p><p id="p-0029" num="0028">As an illustrative example, a user may capture an image of a dog and a cat. The image may be input to an object detection model, which may detect, based on a trained machine learning model, that the image depicts a dog and a cat. A user may then put an adversarial patch on the dog and capture an otherwise-identical image. The second image may be input to the same object detection model. However, due to the adversarial patch included in the second image, the object detection model may incorrectly determine that the second image only depicts a cat (e.g., the system may fail to detect the dog at all). In other examples, the adversarial patch may cause the system to incorrectly determine that the second image depicts a cat and an avocado (e.g., the system may misidentify the dog as an avocado) or that the second image depicts a sunset (e.g., the system may misidentify both the dog and the cat). Notably, the adversarial patch need not simply be a picture of an avocado (or a blank covering) placed over the dog to obscure it; the patch may appear to the user to be an abstract image with no discernable pattern, and the majority of the identifying features of the dog (e.g., the dog's face, overall shape, markings, etc.) may still be visible in the second image. Nonetheless, the adversarial patch may still be able to disrupt the object detection model simply by being present in the image.</p><p id="p-0030" num="0029">A &#x201c;performance&#x201d; of an adversarial patch describes how consistently the patch is able to disrupt an object detection model (e.g., how much of an impact the patch has on the system's accuracy). Performance can vary wildly between patches, models, and images; an image including a first patch may completely fool a first object detection model (e.g., YOLOv3) but be consistently defeated by a second object detection model (e.g., RetinaNet). At the same time, an image including a second patch may be consistently defeated by the first system but completely fool the second system.</p><p id="p-0031" num="0030">Adversarial patch performance often varies with how the patch appears in a given image. For example, a person may be wearing a shirt with an adversarial patch printed on the shirt. A picture of the person may be captured, compressed, and input to an object detection model. The appearance of the patch in the compressed picture (i.e., what the recognition system &#x201c;sees&#x201d;) may differ significantly from an uncompressed image file of the adversarial patch itself. For example, the captured picture may be poorly lit, resulting in portions of the patch appearing black or &#x201c;washed out.&#x201d; As another example, the person's shirt may be wrinkled, resulting in portions of the patch appearing distorted/shaded slightly differently. Further, the patch may not be directly facing the camera capturing the image, which may result in portions of the patch being omitted entirely from the picture, some portions appearing compressed, etc.</p><p id="p-0032" num="0031">Another possible impact on patch performance is distance from a camera. For example, two images may be captured of a person wearing a shirt including an adversarial patch: a first image where the person (and, more importantly, the patch) is relatively close to the camera capturing the image (e.g., within 3 meters), and a second image where the person (and patch) are relatively far from the camera (e.g., 9 meters or farther away). The appearance of the patch in the two images will likely be substantially different, as the patch in the second image will appear smaller, and the distance will result in some lost information (e.g., the patch will appear less detailed). Because appearance within an image is a major factor of performance, the patch can be expected to perform differently between the two images.</p><p id="p-0033" num="0032">Adversarial patches are typically generated and checked for effectiveness at deceiving object detection models. This checking often includes submitting the generated patch directly to an object detection model. A patch generated and evaluated this way may be an image file, such as a bitmap (.bmp). The patch depicted in such an image file may be referred to as the &#x201c;base patch.&#x201d; Due to how it is generated, a base patch may be most effective at deceiving object detection models when the patch is directly inserted into an existing image, such as by manipulating a digital image file to paste the patch into the image. In practice, the patch may be implemented via, for example, a printed sticker, a logo on a shirt, a wearable display, etc., and a picture of the patch may be captured (such as by a camera) and sent to an object detection model. However, conditions surrounding the picture capture, such as lighting, distance from the camera to the patch, viewing angle, and properties of the camera can all result in the patch appearing distorted in the picture. As noted above, this can result in impaired performance of the patch. Even &#x201c;ideal&#x201d; conditions (e.g., when the patch is illuminated by flat/neutral lighting, when the patch was relatively close to a camera capturing a picture of the patch, when the patch is plainly visible in the picture, etc.) are likely to result in at least some deviation between the captured picture and the image of the patch itself. As a result, features of the patch that enable it to mislead an object detection model (whatever those features may be) can become distorted and/or blurred. Thus, when adversarial patch generators are trained primarily based on images of patches themselves (i.e., &#x201c;base patches&#x201d;), the resultant patches may not be as effective in practice as the training process might suggest. As an example, a patch consisting of a regular grid of black and white squares, when viewed from a sufficient distance, may simply appear as a solid block of gray. The sharp contrast between the black squares and the white squares may be the feature primarily responsible for the patch's effectiveness. Thus, viewing the patch from a great distance can result in the patch losing effectiveness, as an object detection model will not &#x201c;see&#x201d; the grid.</p><p id="p-0034" num="0033">A &#x201c;patch generator&#x201d; may be a machine learning model that can generate adversarial patches. As adversarial patches are typically most effective within certain conditions (such as a particular distance range, lighting condition, etc.), different generators may be utilized to generate patches that are effective in different conditions. For example, a first patch generator may generate adversarial patches that are effective at relatively short distances (but ineffective at longer distances), while a second patch generator may generate adversarial patches that are effective at farther distances (but ineffective at near distances).</p><p id="p-0035" num="0034">Systems and methods consistent with the present disclosure enable generation of &#x201c;pre-transformed&#x201d; or &#x201c;primed&#x201d; adversarial patches that are effective in real-life scenarios. In essence, while a &#x201c;base&#x201d; patch may be most effective against object detection models, this effectiveness is hindered by the distortion resulting from implementing the patch. To address this, a primed patch may be &#x201c;preemptively un-distorted&#x201d; so that, when viewed in a real-life environment, the primed patch more closely resembles the base patch, rendering the primed patch more effective. For example, an adversarial patch (a &#x201c;base patch&#x201d;), when observed in a real-life situation, may get distorted in a predictable way. As a simple example, the base patch may get rotated by 10 degrees clockwise. This distortion may reduce performance of the patch, as it can cause the patch to appear different from the base patch. However, as the distortion is predictable, the adversarial patch can be &#x201c;primed&#x201d; for this distortion by preemptively reversing it (e.g., the patch may be rotated by 10 degrees counter-clockwise before it is implemented). This way, when the primed patch is implemented (e.g., when it is printed onto a shirt or sticker) and a camera captures a picture including the primed patch, the predicted distortion will result in the primed patch appearing as the base patch. This way, the primed patch is more likely to achieve the effectiveness of the base patch. An illustrative example of this concept is provided and discussed below with reference to <figref idref="DRAWINGS">FIG. <b>8</b>A-<b>8</b>D</figref>.</p><p id="p-0036" num="0035">Primed patches can be used to train models to be increasingly robust and capable of defeating them. As used herein, &#x201c;defeating&#x201d; an adversarial patch refers to a system successfully identifying an object the patch is attempting to obfuscate (e.g., identifying a person wearing a shirt with an adversarial patch on it). In some instances, adversarial patches may be specifically detected and disregarded (e.g., removed from an image or removed from consideration), allowing a system to recognize objects without considering the patch. In other instances, adversarial patches may simply go unnoticed, and the system may recognize objects in spite of the patch.</p><p id="p-0037" num="0036">Performance of an adversarial patch can be substantially altered by distortion, though the degree to which performance suffers may depend upon the patch. As an example, a first patch may consistently deceive an object detection model, but if the first patch is tinted a slightly different color, the object detection model may suddenly be capable of defeating the first patch. However, a second patch may similarly be capable of deceiving an object detection model (possibly even more reliably than the first patch). If the second patch is tinted in the same manner as the first patch, the second patch may retain some effectiveness (i.e., the tinted second patch may still deceive the object detection model, albeit perhaps with reduced consistency). Further, when implementing an adversarial patch, such as by printing the patch onto an article of clothing, the patch may become distorted. For example, printing the first patch onto a shirt may result in the patch being slightly tinted, which may therefore drastically reduce performance of the first patch. This, in effect, may mean that the first patch is deceptively ineffective, as it performs well in laboratory tests but fails when actually implemented. Similarly, this means that training an object detection model to defeat the first patch may not be a particularly beneficial use of limited computing resources, as the first patch is unlikely to be relevant in practice.</p><p id="p-0038" num="0037">Thus, evaluating the first patch in a &#x201c;realistic&#x201d; scenario may save computing resources in the long run. Similarly, training the object detection model to defeat the raw second patch may also be a waste, as the second patch may perform differently (but still effectively) in practice. One way to evaluate patches in a realistic scenario is to create the scenario; in essence, the patch being evaluated can be printed onto a shirt, a picture may be captured of a user wearing the shirt, and the picture may be submitted to the object detection model. However, in practice, as additional patches and settings are evaluated, this method quickly becomes untenable due to costs of printing multiple shirts, staging multiple pictures, etc. Thus, a scalable way to evaluate real-world performance of adversarial patches would be particularly advantageous.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a high-level method <b>100</b> for training an adversarial patch generator to defeat object detection models via realistic adversarial images, consistent with several embodiments of the present disclosure. Method <b>100</b> comprises training an adversarial image generator at operation <b>102</b>. An example of how adversarial image generators may be trained is provided in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, discussed below with reference to method <b>200</b>. In general, operation <b>102</b> includes receiving a training patch as well as a set of training pictures depicting the patch. The training pictures may have been captured in several different environmental conditions (e.g., environmental attributes such as different lighting conditions, different poses, different distances from a capturing camera, etc. may differ between the pictures). Operation <b>102</b> further includes mapping transformations between the received sample patch and the appearance of the patch in a picture, and training a model based on an association between the environmental conditions and the transformations. This training can be repeated based on a size of the set of training pictures to increase performance of the adversarial image generator.</p><p id="p-0040" num="0039">Performance of an adversarial image generator may be evaluated, for example, by submitting a patch and a test picture to the image generator. The test picture may be based on an input picture depicting the patch, but the patch may be obfuscated or removed from the test picture. The adversarial image generator may then attempt to insert the submitted patch into the test picture, resulting in an output image. The output image may then be compared to the input picture to determine how close the image generator was to the &#x201c;correct&#x201d; or &#x201c;actual&#x201d; picture. An example performance evaluation is depicted and described below with reference to <figref idref="DRAWINGS">FIG. <b>4</b>A-<b>4</b>D</figref>.</p><p id="p-0041" num="0040">A patch generator can be trained as a machine learning model. For example, an initial iteration of a patch generator may generate patches randomly. The patches may then be tested against one or more object detection models to determine an effectiveness rating for the patch. Randomly generated adversarial patches are generally unlikely to be effective, so adjustments may be made to the generator, after which the generator may be used to generate additional patches. This training process can be repeated until the generator produces patches that satisfy given performance criteria.</p><p id="p-0042" num="0041">Pixels may be described in terms of &#x201c;relative position&#x201d; in multiple different images. For example, a first relative position may be a top-left position (e.g., in a top row and a leftmost column). To illustrate, a first image may have a blue pixel in a top-left position, while a second image may have a red pixel in a top-left position. This can be described as a blue pixel in a first relative position in the first image and a red pixel in the first relative position in the second image. This &#x201c;relative location&#x201d; terminology can also refer to patches (e.g., a near patch may have a blue pixel in a second relative location while a far patch may have a red pixel in the second relative location).</p><p id="p-0043" num="0042">Method <b>100</b> further comprises modifying a sample picture to include an adversarial patch at operation <b>104</b>. Operation <b>104</b> may include receiving both the sample picture and the patch. For example, operation <b>104</b> may include receiving an adversarial patch from a database or a user. In some instances, the receiving of operation <b>104</b> may include generating an adversarial patch via an adversarial patch generator. In some instances, operation <b>104</b> may further include training an adversarial patch generator (in which case the received adversarial patch may be generated by the trained generator). Different generators may be utilized to generate patches primed for different kinds of expected distortion. For example, a &#x201c;near&#x201d; patch generator may generate adversarial patches that are effective at relatively short distances (but may be ineffective at longer distances), while a &#x201c;dim lighting&#x201d; patch generator may generate adversarial patches that are effective when included in pictures captured in dim lighting conditions (but may be ineffective in brightly lit conditions). An example of how patch generators may be trained is provided in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, discussed below with reference to method <b>700</b>. As an explanatory example, a camera may capture a picture including an adversarial patch and an object. An object detection model may attempt to identify the object. If the object detection model is unable to identify the object, the patch may be considered an effective patch, and the object detection model may be updated (e.g., one or more weights may be adjusted). The process can be repeated until the object detection model reliably defeats the patch. In some instances, receiving a sample picture at operation <b>104</b> may include capturing a sample picture, such as with a camera.</p><p id="p-0044" num="0043">The sample picture and adversarial patch may be received by the adversarial image generator trained at operation <b>102</b>. As an example, operation <b>104</b> may include receiving a picture of a person wearing a shirt. The shirt may depict a placeholder patch (such as a blank rectangle with markers on each corner). The adversarial image generator may modify the picture to replace the placeholder with the adversarial patch by transforming the patch based on the environment of the picture. As an example, the picture may have been captured in a dimly-lit room and the placeholder patch may not be directly facing the camera (perhaps because the person wearing the shirt is facing a different direction). In such an instance, the adversarial image generator may, based on its training, darken the adversarial patch and compress/rotate the adversarial patch. The adversarial image generator may then replace the placeholder with the modified adversarial patch. In contrast, as an additional example, the picture may have been captured in a well-lit room; in such an instance, rather than darkening, the adversarial image generator may increase a brightness of the adversarial patch. The modifying of operation <b>104</b> results in at least one adversarial image. The adversarial image advantageously depicts the adversarial patch in a realistic scenario (e.g., simulating color, shape, and lighting changes that might be expected in a picture captured of the adversarial patch), without needing to, for example, print a patch onto a shirt and capture a new picture for every patch received at operation <b>104</b>.</p><p id="p-0045" num="0044">Method <b>100</b> further comprises training, at operation <b>106</b>, an adversarial patch generator to defeat an object detection model. A more detailed example of training such a generator is provided as method <b>700</b>, discussed in further detail below with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>. As an overview, operation <b>106</b> may include submitting the modified adversarial image (produced via operation <b>104</b>) to an object detection model, determining whether the system can defeat the patch, and revising the system until it can. Operations <b>104</b> and <b>106</b> may be repeated for multiple modified adversarial images and/or patches. Method <b>100</b> then ends at <b>108</b>.</p><p id="p-0046" num="0045">For example, a &#x201c;sample picture&#x201d; depicting an object and a placeholder patch may be captured by a camera. The sample picture may depict a person standing in a dark area 15 m away from the camera. The person may be wearing a shirt depicting the placeholder patch. Operation <b>104</b> may include digitally altering the sample picture to appear as though the person's shirt has an adversarial patch on it. Operation <b>106</b> may include submitting the altered image to an object detection model and receiving an output from the object detection model. If the object detection model successfully identified the person in the image, then the model has successfully defeated the adversarial patch. If the object detection model misidentified the person in the image, then the model may have failed to defeat the adversarial patch. If the model failed, the model can be revised (e.g., features/layers can be modified), and the test can be run again to determine if the revision was helpful. In some instances, the model may be revised until the most recent revision results in decreased performance (relative to the preceding revision).</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a method <b>200</b> for training an adversarial image generator, consistent with several embodiments of the present disclosure. Method <b>200</b> provides a detailed example of a possible implementation of operation <b>102</b> of method <b>100</b>. As an overview, method <b>200</b> includes comparing pictures of patches to the (digital) patches themselves in order to learn how environmental conditions impact the appearance of the patches. For example, a picture captured in poor lighting conditions will result in a patch in the picture appearing darker than the patch actually is; method <b>200</b> describes learning relationships like this. Once trained, an image generator can then predict how a given patch will appear if it had been included in a given picture.</p><p id="p-0048" num="0047">Method <b>200</b> comprises receiving a training patch at operation <b>202</b>. In some instances, the training patch may be a placeholder patch, such as a blank image, sample Red/Green/Blue (RGB) pallet, or scan target. In some instances, the training patch may be an adversarial patch. The training patch may be received from a database or a user. In some instances, operation <b>202</b> may include generating the training patch (such as via an adversarial patch generator).</p><p id="p-0049" num="0048">Method <b>200</b> further comprises, at operation <b>204</b>, receiving a training picture including the training patch received via operation <b>202</b>. For example, operation <b>204</b> may include causing a camera to capture a picture of a person wearing an item of clothing bearing the training patch. In some instances, operation <b>204</b> may include receiving the training picture from a database.</p><p id="p-0050" num="0049">Method <b>200</b> further comprises identifying, at operation <b>206</b>, environmental attributes of the training picture and, more particularly, of the training patch. Environmental attributes include attributes such as lighting, shape, color, etc. For example, operation <b>206</b> may include identifying how well-lit the patch is in the picture, as well as a shape of the patch as it appears in the picture. In some instances, the environmental attributes can be received via a user input and/or as metadata included with the training picture. As an example, the training picture received at operation <b>204</b> may depict a person wearing a shirt with the training patch embedded into a pattern of the shirt. The person, as depicted, may be standing in a dark corner of a room. The picture may include metadata describing a location of the patch in the picture. For example, the location of the patch may be described as a set of pixel coordinates dictating corners of the patch, or lines or curves representing edges of the patch, etc. The metadata may also include a light level in the region of the picture including the patch. For example, the light level may be a normalized value between 0 (complete darkness) and 1 (entirely washed-out in light). In some instances, operation <b>206</b> may include determining a light level by comparing an RGB value of a given pixel in the patch as it appears in the picture with a corresponding pixel of the patch received at operation <b>202</b>. For example, if a pixel in the patch is grey but its counterpart in the picture is nearly black, this may indicate that the light level is relatively low.</p><p id="p-0051" num="0050">Method <b>200</b> further comprises mapping, at operation <b>208</b>, a transformation between the patch received at operation <b>202</b> and the patch as it appears in the picture received at operation <b>204</b>. Operation <b>208</b> may include, for example, determining differences in size, shape, and color between the &#x201c;apparent&#x201d; patch in the picture and the &#x201c;raw&#x201d; patch received at operation <b>202</b>. In some instances, operation <b>208</b> may include generating a transformation model that can transform the raw patch into the apparent patch.</p><p id="p-0052" num="0051">Method <b>200</b> further comprises updating, at operation <b>210</b>, one or more models of an adversarial image generator based on association between the mapping of operation <b>208</b> and the environmental attributes of operation <b>206</b>. As an example, an adversarial image generator may include a geometric transformation model configured to receive a picture and a patch as inputs. Based on these inputs, the geometric transformation model may output a transformation which, when applied to an input patch, may transform the input patch into a different geometry so that the transformed input patch could be realistically inserted into the received picture. In other words, the transformed input patch may not look &#x201c;out of place&#x201d; in the received picture. Notably, the input patch is not necessarily the same as the received patch; in other words, the transformation can be applied to a different input patch, and the transformed different input patch will also not look &#x201c;out of place&#x201d; if inserted into the received picture. The received picture may include one or more indicators describing where the transformed patch should be. For example, the received picture may include an outlined region, one or more scan targets, etc. The geometric transformation model can warp, stretch, resize, and/or rotate the raw patch so that the transformed patch fits into the picture as indicated.</p><p id="p-0053" num="0052">As another example, the adversarial image generator may include a lighting adaptation model configured to receive a picture and a patch as inputs and to output a transformed patch adapted for a different light level. The received picture may include metadata describing a light level of the patch. For example, the received picture may indicate an illumination level on a scale from 0 to 1 with 0.5 being a &#x201c;neutral&#x201d; illumination.</p><p id="p-0054" num="0053">The mapping of operation <b>208</b> can be associated with the environmental attributes of operation <b>206</b>. This can help the model learn to adapt patches differently for different pictures. For example, a first training picture may have a first set of environmental attributes such as &#x201c;dim lighting,&#x201d; &#x201c;green tint,&#x201d; and/or &#x201c;far from camera.&#x201d; A first transformation may then be mapped at operation <b>208</b>. A second training picture may have a second set of environmental attributes, which may only include a single attribute: &#x201c;bright lighting.&#x201d; A second transformation may then be mapped at operation <b>208</b>. The first transformation may be associated with the first set of environmental attributes and the second transformation may be associated with the second set of environmental attributes. A third picture may be received, having a third set of environmental attributes including &#x201c;bright lighting.&#x201d; Because the third set of environmental attributes matches the second set of environmental attributes, the model may generate a transformation based on the mapping associated with the second set of environmental attributes (i.e., the second mapping). In such an instance, the third transformation may be substantially similar to the second transformation. As this system is scaled by increasing training pictures and environmental attributes, the model can learn to merge multiple established transformations based on environmental attributes of an input picture.</p><p id="p-0055" num="0054">In practice, an adversarial patch may be printed onto clothing (or other objects) by a printer. The printing process may inherently result in some colors being changed. These changes may appear to be minor. For example, a red color of a raw patch may be a slightly different shade of red on the printed patch, this can result in performance impacts on the patch. This change can be specific to a given printer model (e.g., a first printer might shift reds to a different shade of red while a second printer may shift blues to a different shade of blue). Thus, the adversarial image generator may include a printer color transformation model configured to receive a printer profile and a patch as inputs and to output a transformed patch based on an expected color discrepancy between the received patch and a printed patch.</p><p id="p-0056" num="0055">Method <b>200</b> further comprises determining whether additional training pictures are available at operation <b>212</b>. In essence, operations <b>204</b> through <b>212</b> enables iterating through and repeatedly adjusting the adversarial image generator based on a set of pictures. This can result in training the generator to handle a broad range of environmental conditions. If more pictures are available (<b>212</b> &#x201c;Yes&#x201d;), method <b>200</b> loops back to receiving an additional picture at operation <b>204</b> and repeats operations <b>204</b>-<b>212</b>. Once all pictures have been utilized (<b>212</b> &#x201c;No&#x201d;), method <b>200</b> ends at <b>214</b>.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a method <b>300</b> for modifying a sample picture into an adversarial image including an adversarial patch, consistent with several embodiments of the present disclosure. Method <b>300</b> comprises training, at operation <b>302</b>, an adversarial image generator to be able to model transformations. Operation <b>302</b> may be performed in a substantially similar manner to operation <b>102</b> of method <b>100</b> and/or according to method <b>200</b>. Once the image generator is trained, it may include several models trained to determine specific transformations. For example, the image generator may include a geometric transformation model configured to receive a sample picture and output a geometric transformation based on the sample picture. The geometric transformation may enable a system performing method <b>300</b> to transform a shape of an input adversarial patch into a final shape matching that of a placeholder patch in a sample picture.</p><p id="p-0058" num="0057">Method <b>300</b> further comprises receiving a sample picture at operation <b>304</b>. The training process at operation <b>302</b> may include receiving one or more pictures; these may or may not include the sample picture received at operation <b>304</b>. The sample picture received at operation <b>304</b> may be captured by a camera and uploaded by a user to a system performing method <b>300</b>. An example sample picture <b>420</b> is provided and discussed below with reference to <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>. In some instances, the sample picture may include a placeholder patch, such as a scan target indicating a region for an adversarial patch to be inserted. This may advantageously improve accuracy of a geometric transformation by providing known reference points defining a final shape of a patch. In some instances, the sample picture received at operation <b>304</b> may also include metadata describing environmental conditions such as lighting, whether the picture was captured indoors vs. outdoors, etc. In some instances, the sample picture received at operation <b>304</b> may also include metadata describing a printing information such as a printer, a printing technique (e.g., ink printing, hydro transfer printing, embroidery, etc.), printing media (e.g., inks, paints, dyes, etc.), and the like to be simulated.</p><p id="p-0059" num="0058">Method <b>300</b> further comprises determining, at operation <b>306</b> transformations based on the sample picture. As an illustrative example, an output geometric transformation may be a matrix T of value pairs describing how each pixel of a rectangular patch should be translated. For example, T[2,3] may include values describing how pixel [2,3] (i.e., the pixel on the second row, third column) of a rectangular patch should be moved. A pair of values at T[2,3] such as (0.1, 1.0) may indicate that pixel [2,3] should be moved to 10% of its distance from a y-axis and 100% of its distance from an x-axis (i.e., remain stationary relative to the x-axis). Thus, after every pixel is moved according to its corresponding value pair, the rectangular patch may have been warped into a shape resembling the placeholder patch. This is merely provided as an example of one implementation of a geometric transformation; trained geometric transformation models in other instances may output different types of geometric transformations (e.g., an equation rather than a matrix, etc.). Other transformations can be produced by other models as well, such as color consistency transformations configured to convert the patch's colors to match the lighting of the picture. Notably, the trained image generator can include multiple models which can each produce a transformation accounting for a different aspect (e.g., a geometric transformation model, a lighting model, a printer model, etc.).</p><p id="p-0060" num="0059">Method <b>300</b> further comprises receiving an adversarial patch at operation <b>308</b>. Operation <b>308</b> may include, for example, receiving a patch from an online or local database, or being input by a user (e.g., the user may scan or photograph a patch and upload it, etc.). In some instances, operation <b>308</b> may include generating the patch via an adversarial patch generator. Additional detail on patch generation is provided below with reference to method <b>700</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0061" num="0060">Method <b>300</b> further comprises transforming the adversarial patch at operation <b>310</b>. Operation <b>310</b> may include, for example, applying the transformation(s) determined at operation <b>306</b> to the adversarial patch received at operation <b>308</b>. The order of the transformations can vary depending upon embodiment. For example, in some instances, a printer color consistency transformation may be performed on the received adversarial patch, resulting in a &#x201c;first-stage&#x201d; transformed patch. A lighting transformation may then be performed on the first-stage transformed patch, resulting in a second-stage transformed patch. Finally, a geometric transformation may be performed on the second-stage transformed patch, resulting in a final transformed patch (referred to as a &#x201c;transformed patch&#x201d; for brevity). In some instances, the transformations may be performed in any order. In some instances, transformations may result in some data being lost. For example, if a placeholder patch is only partially visible in a sample picture, a corresponding geometric transformation may essentially result in deleting part of the adversarial patch. While colors of the deleted part may not matter for the final transformed patch (as they are deleted anyway), it may be generally preferable to perform &#x201c;lossy&#x201d; transformations last, as some transformations may depend in part on the patch as a whole (e.g., a printing transformation may weight each pixel towards an average brightness of the entire patch; deleting part of the patch may skew the average and thus skew the results). Once transformed, the adversarial patch may appear as if it were in the sample picture when the picture was captured (e.g., it may be distorted to appear as if it were on a shirt worn by a user in the picture, may be recolored according to discrepancies between the &#x201c;raw&#x201d; patch's digital color and a given printer's capability to reproduce those digital colors, etc.).</p><p id="p-0062" num="0061">Method <b>300</b> further comprises inserting the transformed patch into the sample picture at operation <b>312</b>. Operation <b>312</b> may include, for example, replacing a placeholder patch in the sample picture with the transformed patch. In some instances, the placeholder patch may not be entirely obscured by the transformed patch (i.e., a geometric transformation may not necessarily be perfect), and thus portions of the placeholder patch may remain visible. In such instances, it may be preferable to leave such &#x201c;leftover&#x201d; portions the placeholder patch intact, as they may already appear realistic in terms of lighting and geometry (since they may have actually been part of the original sample picture). Some systems may be capable of digitally erasing leftover portions of the placeholder patch. Erasing the leftover portions may prevent the leftover portions from impacting future evaluations based on the final adversarial image (e.g., if an object detection system fails to identify an object, a user may be unsure if this is a success of the adversarial patch or due to the leftover portions). However, erasing the leftover portions is also more likely to introduce additional unwanted digital artifacts which may reduce realism of the overall image (and therefore reduce confidence of any evaluations based on the image) more than simply leaving the leftover portions of the placeholder patch in the image.</p><p id="p-0063" num="0062">If the sample picture does not include a placeholder patch, operation <b>312</b> may include utilizing a region identified at operation <b>306</b> for purposes of determining a geometric transformation. For example, operation <b>306</b> may include identifying, via an object detection model, a person wearing a shirt, and further selecting a region of the shirt as a final destination of a transformed patch (e.g., a shoulder region of a sleeve of the shirt, a chest region of the shirt, etc.). Operation <b>312</b> may then insert the transformed patch onto the identified region. Once the transformed patch is inserted into the sample picture, the resulting adversarial image is output and method <b>300</b> ends at <b>314</b>.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is an example adversarial patch <b>410</b>. Patch <b>410</b> may have been received from an external database, generated by a patch generator, etc. Patch <b>410</b> is merely an abstract example of an adversarial patch; the specific design of patch <b>410</b> as depicted in <figref idref="DRAWINGS">FIG. <b>4</b></figref> may not be particularly effective.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is an example picture <b>420</b> depicting a user <b>422</b> wearing a shirt <b>424</b> with a placeholder patch <b>426</b> on it, consistent with several embodiments of the present disclosure. Picture <b>420</b> may be captured by a camera. In some instances, a developer of a system consistent with the present disclosure may apply placeholder patch <b>426</b> to shirt <b>424</b> and capture picture <b>420</b>. Further, in some instances, placeholder patch <b>426</b> may be designed to enable image tracking. For example, placeholder patch <b>426</b> may include scan targets to further enable quick identification/location of patch <b>426</b> in picture <b>420</b>. This may be advantageous as it can enable a developer to quickly acquire sample pictures in multiple environments. Building <b>428</b> appears in a background of picture <b>420</b>; other sample pictures may be captured in different environments, lighting conditions, with different backdrops, etc. Notably, user <b>422</b> is standing at an angle relative to the perspective of picture <b>420</b>; thus, while placeholder patch <b>426</b> may have been rectangular, it appears roughly as a parallelogram in picture <b>420</b>. This is one aspect where the scan targets of placeholder patch <b>426</b> are helpful. However, sample picture <b>420</b> is provided for exemplary purposes only; sample pictures do not need to depict a person wearing a shirt, nor do they require a placeholder patch.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>4</b>C</figref> is a first example image <b>430</b> after an adversarial image generator has inserted an adversarial patch <b>436</b> into a picture (such as picture <b>420</b>), consistent with several embodiments of the present disclosure. Adversarial patch <b>436</b> may be based on adversarial patch <b>410</b>. Notably, adversarial patch <b>436</b> is simply resized and digitally inserted over shirt <b>424</b> (parts of placeholder patch <b>426</b> can even still be seen &#x201c;underneath&#x201d; patch <b>436</b>). This may be because, for example, the adversarial image generator used to generate image <b>430</b> is untrained (or poorly trained), and as a result is only capable of placing the adversarial patch over placeholder patch <b>426</b>. At first, this may appear to be useful, as this is likely to mean that adversarial patch <b>436</b> will perform similarly to adversarial patch <b>410</b> (because patch appearance is a controlling factor of patch performance). However, image <b>430</b> may not be a realistic example. In other words, a picture captured of user <b>422</b> wearing shirt <b>424</b> with patch <b>410</b> included on shirt <b>424</b> is likely to result in the patch appearing at least slightly different from the &#x201c;raw&#x201d; patch <b>410</b> (due to conditions such as a stance/angle that user <b>422</b> is standing at). Therefore, image <b>430</b> is unlikely to be helpful for purposes of evaluating performance of patch <b>410</b> and/or an object detection model trained to defeat adversarial patches, as user <b>422</b> (and, more particularly, shirt <b>424</b>) is standing at an angle relative to a camera capturing picture <b>420</b>. Thus, a rectangular adversarial patch like patch <b>410</b>, if depicted on shirt <b>424</b>, may be expected to appear at an angle similar to placeholder patch <b>426</b>. Of course, in other instances, it is possible for user <b>422</b> (and thus shirt <b>424</b>) to be facing the camera such that patch <b>436</b> may be a reasonably accurate representation.</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>4</b>D</figref> is a second example image <b>440</b> after a trained adversarial image generator has inserted an adversarial patch <b>446</b> into a picture such as picture <b>420</b>, consistent with several embodiments of the present disclosure. Notably, adversarial patch <b>446</b> is rotated and adjusted such that it matches the orientation of placeholder patch <b>426</b> of picture <b>420</b>. Thus, adversarial patch <b>446</b> may appear slightly warped/distorted compared to adversarial patch <b>436</b> (and, therefore, adversarial patch <b>410</b>). However, this distortion means that image <b>440</b> may be a realistic example of what a picture captured of user <b>422</b> wearing shirt <b>424</b> with patch <b>410</b> imprinted upon shirt <b>424</b> would look like. In other words, patch <b>446</b> may perform differently from patch <b>410</b>, but that is actually an advantage, as it enables evaluating and training object detection models based on more realistic examples. More realistic examples may result in better training. As an example, an object detection model trained on images like image <b>430</b> may be successful at recognizing user <b>422</b> in image <b>430</b>, but may fail to recognize user <b>422</b> in image <b>440</b>. Put differently, an object detection model trained to defeat adversarial patch <b>410</b> may succeed in overcoming the &#x201c;raw&#x201d; patch itself, but may fail to identify objects in images containing the patch if the patch is even slightly distorted (such as in image <b>440</b>).</p><p id="p-0068" num="0067">As an example, adversarial patch <b>410</b> may perform exceptionally well when the &#x201c;raw&#x201d; patch is evaluated (e.g., when image <b>430</b> is input into an object detection model, the model may consistently fail to identify that user <b>422</b> is a person). However, when patch <b>410</b> is actually printed onto clothing worn by users, patch <b>410</b> may fail to deceive models with any consistency. Inputting image <b>440</b> into the object detection model may reveal this fact, as the model may be able to identify that user <b>422</b> is a person in spite of the presence of patch <b>446</b>. While a user may be able to print patch <b>410</b> onto a shirt and capture a picture similar to image <b>440</b>, this can be time- and resource-intensive, particularly when scaled. For example, to evaluate three different patches, the user may need to print a first patch onto a first shirt and capture a first picture while wearing the first shirt, print a second patch onto a second shirt and capture a second picture while wearing the second shirt, and print a third patch onto a third shirt and capture a third picture while wearing the third shirt, and then input each picture into an object detection model. In contrast, using systems and methods consistent with the present disclosure, multiple different patches (not shown in <figref idref="DRAWINGS">FIG. <b>4</b>A-<b>4</b>D</figref>) can be inserted into picture <b>420</b> to quickly and efficiently evaluate each patch without having to print a new patch onto a shirt and/or capture a new picture for every patch. A trained adversarial image generator consistent with the present disclosure, being capable of inserting patch <b>410</b> into picture <b>420</b> to yield image <b>440</b>, can therefore provide a significant boost to several aspects of adversarial patch analysis, including both patch generation (patch generators can be evaluated/trained based on performance of patches they generate) and patch resistance (object detection models can be evaluated/trained based on performance when face with different patches).</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow diagram <b>500</b> depicting an example trained adversarial image generator <b>502</b>, consistent with several embodiments of the present disclosure. In particular, diagram <b>500</b> depicts various component models of adversarial image generator <b>502</b> as well as inputs to the component models and their output transformations. As an overview, once adversarial image generator <b>502</b> is trained, it functions in two stages: first, a sample picture <b>504</b> (including a placeholder patch <b>505</b>) and a &#x201c;raw&#x201d; copy <b>506</b> of the placeholder patch can be input into models <b>512</b> and <b>514</b>, while printer information <b>508</b> may be input to model <b>516</b>. This first stage results in transformations <b>522</b>-<b>526</b>. Then, at a second stage, a different patch (not shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>) can be input into trained adversarial image generator <b>502</b>, at which point the different patch will be transformed via transformations <b>522</b>-<b>526</b> and inserted into sample picture <b>504</b>. This second stage is described in further detail below with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>. In particular, while sample picture <b>504</b> may have been captured to so as to depict placeholder patch <b>505</b>, adversarial image generator <b>502</b> may also receive a &#x201c;raw&#x201d; copy <b>506</b> of placeholder patch <b>505</b>. With inputs <b>504</b>-<b>508</b>, adversarial image generator <b>502</b> can be further configured to generate, through the three models <b>512</b>, <b>514</b>, and <b>516</b>, transformations <b>522</b>, <b>524</b>, and <b>526</b> to account for lighting, geometry, and printer color discrepancies, respectively.</p><p id="p-0070" num="0069">As an example, a user may wish to determine whether an object detection model can identify a person if the person is wearing a shirt with a given adversarial patch printed onto the shirt. Further, the user may opt to indicate that the adversarial patch was printed onto the shirt via a specific printer model. This printer model may have known discrepancies between a color input to the printer and an apparent color visible in a picture of a printed object. For example, the given printer could be sent a solid square of green (e.g., having a Red/Green/Blue (&#x201c;RGB&#x201d;) value of (0, 255, 0), print a corresponding square onto a shirt, and then a picture could be captured of the printed shirt (in ideal lighting conditions). In the captured picture, the square may appear slightly tinted (e.g., RGB of (17, 252, 8)). Notably, this type of discrepancy may be relatively consistent for all printers of the same model, and other models of printer may have different discrepancies. A database of this information may be included in adversarial image generator <b>502</b> (not shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>), so that upon receipt of printer information <b>508</b> listing the model, adversarial image generator <b>502</b> can look up the corresponding mapping. In some instances, trained printer color transformation model <b>516</b> may simply perform such a lookup and output a printer-based color mapping transformation <b>526</b>.</p><p id="p-0071" num="0070">Geometric transformation model <b>514</b> may be configured to receive sample picture <b>504</b> (including placeholder patch <b>505</b>) and &#x201c;raw&#x201d; copy of placeholder patch <b>506</b>. Geometric transformation model <b>514</b> may then identify placeholder patch <b>505</b> based on a comparison with raw patch <b>506</b>, and determine a picture-based geometric transformation <b>524</b> based on a discrepancy between raw patch <b>506</b> and placeholder patch <b>505</b>. For example, geometric transformation model <b>514</b> may include one or more object detection models trained to identify a shape of raw patch <b>506</b> and its counterpart (i.e., placeholder patch <b>505</b>) in picture <b>504</b>. In particular, model <b>514</b> may identify boundaries/border lines of patch <b>505</b> and generate transformation <b>524</b>. Thus, when transformation <b>524</b> is applied to patch <b>506</b>, the resulting transformed patch will have the same shape as patch <b>505</b> within picture <b>504</b>.</p><p id="p-0072" num="0071">Transformation <b>524</b> may include, for example, an array of weights describing how each pixel within a given patch should be translated (moved) in order to recreate the distortion observed in picture <b>504</b>. For example, if raw patch <b>506</b> is a square and placeholder patch <b>505</b> in picture <b>504</b> is a non-square rectangle (e.g., raw patch <b>506</b> may have been shortened along a y-axis), transformation <b>524</b> may include an array of weights that accordingly indicate shifts along the y-axis (but not along the x-axis). In some instances, geometric transformation model <b>514</b> can also analyze color (for example to identify whether a square patch has rotated) 90&#xb0; to transform a later input patch having a shape of raw patch <b>506</b> into an output patch having the shape of patch <b>505</b>.</p><p id="p-0073" num="0072">Lighting adaptation model <b>512</b> may be configured to receive sample picture <b>504</b> (including placeholder patch <b>505</b>) and &#x201c;raw&#x201d; copy <b>506</b> of the placeholder patch. Lighting adaptation model <b>512</b> may then identify placeholder patch <b>505</b> based on a comparison with raw patch <b>506</b>, and determine a color discrepancy between the raw placeholder patch <b>505</b> and the patch as it appears within picture <b>504</b>. In some instances, printer-based color mapping transformation <b>526</b> may be determined and applied to raw patch <b>506</b> prior to model <b>212</b>'s comparison with the apparent patch in picture <b>504</b>. This may account for a possible printer color discrepancy's contribution to an overall difference in color. As an example, raw placeholder patch <b>506</b> may be a solid green (0, 255, 0) square. In sample picture <b>504</b>, patch <b>505</b> may appear tinted, having an RGB of (17, 252, 8). As these colors are different, lighting adaption model <b>512</b> may be configured to determine an impact on the discrepancy caused by lighting effects. However, printer information <b>506</b> may cause printer color transformation model <b>516</b> to generate printer-based color mapping transformation <b>526</b>. Applying transformation <b>526</b> to raw patch <b>506</b> may yield a first-stage transformed patch having an RGB of (17, 252, 8). Thus, patch <b>505</b> in sample picture <b>504</b> may actually appear exactly the same color as an expected patch produced by the printer indicated in information <b>508</b>. This may indicate that the lighting of the picture actually had no impact on the color of patch <b>505</b>. In such an instance, picture-based color constancy transformation <b>522</b> may simply be omitted (or may be a &#x201c;multiplication by 1&#x201d;). In some instances, raw patch <b>506</b> may be or include a color spectrum (e.g., a rainbow, a color wheel, etc.) in order to enable lighting adaptation model <b>512</b> to account for different impacts of lighting on different colors, if any.</p><p id="p-0074" num="0073">In some instances, geometric transformation <b>524</b> may be applied to raw patch <b>506</b> instead of or in addition to applying printer-based color mapping transformation <b>526</b> to raw patch <b>506</b> before model <b>512</b> analyzes the transformed patch. This way, lighting adaptation model <b>512</b> can identify portions of the patch that may be subjected to localized lighting effects. For example, a wrinkled shirt may result in a strip of a patch reflecting a light source while the rest of the patch may be in relative shadow. By transforming raw patch <b>506</b> based on geometric transformation <b>524</b> prior to performing color comparisons, model <b>512</b> can determine a picture-based color constancy transformation <b>522</b> that can recreate such localized lighting effects. As an illustrative example, a user may be wearing a shirt and standing in a pose such that a light source reflects off of an upper portion of a patch, causing the upper portion of the patch to appear &#x201c;washed out&#x201d; in a sample picture. The user may also be standing in a way that causes a leftmost portion of the patch to be obscured from view. Comparing colors of the raw placeholder patch to the placeholder patch as it appears in the sample picture may be difficult if the patches are different shapes and sizes, so adversarial image generator <b>502</b> may apply geometric transformation <b>524</b> to raw patch <b>506</b> in order to produce a patch having a similar shape and size. This may allow model <b>512</b> to identify that the upper portion of the patch is particularly bright. Model <b>512</b> may map the color variations between the geometrically transformed patch and the patch <b>505</b> in sample picture <b>504</b>, such as via a matrix of pixel values ranging from 0 to 1 (where a 0 indicates total darkness, a 1 indicates completely white, and 0.5 indicates no change). In some instances, this mapping may function as the color constancy transformation <b>522</b>. However, as the mapping is generated based on a different shape from the raw patch <b>506</b>, in some instances, geometric transformation <b>524</b> may be applied in reverse to the mapping in order to generate a color mapping corresponding to the shape of the raw patch <b>506</b>.</p><p id="p-0075" num="0074">In some instances, sample picture <b>504</b> may also include metadata describing &#x201c;macro&#x201d; color effects such as, for example, color grading of sample picture <b>504</b>. As an example, sample picture <b>504</b> may have a slight red tint applied to the entire image. If this information is included with sample picture <b>504</b>, model <b>512</b> may adjust transformation <b>524</b> to reflect it. For example, transformation <b>524</b> may include a corresponding slight red tint.</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram <b>600</b> showing an example implementation of a patch image generator <b>602</b>, consistent with several embodiments of the present disclosure. Image generator <b>602</b> may be a trained image generator (e.g., trained according to operation <b>102</b> of method <b>100</b>, method <b>200</b>, etc.). Diagram <b>600</b> is provided as an explanatory example of how an adversarial patch <b>610</b> can be modified into a transformed adversarial patch <b>646</b>, which can then be inserted into a sample picture <b>620</b> in order to produce an adversarial image <b>640</b>. Notably, picture-based color transformation <b>604</b>, picture-based geometric transformation <b>606</b>, and printer-based color mapping transformation <b>608</b> may have been determined by corresponding trained models (not shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>) based on an input of sample picture <b>620</b>. In other words, sample picture <b>620</b> may be input to generator <b>602</b> in order to generate transformations <b>604</b>-<b>608</b>. Once determined, transformations <b>604</b>-<b>608</b> can be performed upon input adversarial patch <b>610</b>. As described above, these transformations modify a shape and/or color of patch <b>610</b> based on the input sample picture <b>620</b>, yielding a transformed patch <b>646</b>.</p><p id="p-0077" num="0076">Transformed patch <b>646</b> may appear, as a result of transformations <b>604</b>-<b>608</b>, as if it was originally included in picture <b>620</b>. Image generator <b>602</b> may then integrate transformed patch <b>646</b> into sample picture <b>620</b>, resulting in adversarial image <b>640</b>. Adversarial image <b>640</b> may then be input to an object detection model (not shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>) to evaluate performance of patch <b>610</b> and/or the object detection model.</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a detailed method <b>700</b> for training an adversarial patch generator, consistent with several embodiments of the present disclosure. Method <b>700</b> comprises receiving a sample picture at operation <b>702</b>. As discussed above, a sample picture may depict an object in order to evaluate performance of an object detection model. Operation <b>702</b> may include, for example, receiving a picture from a database. In some instances, the sample picture received at operation <b>702</b> may include a marking or placeholder patch to designate where an adversarial patch may be inserted. For example, the sample picture may be a picture of a user of a system performing method <b>100</b>, and the user may be wearing a shirt including scan targets designating boundaries of a patch region. In some instances, the sample picture may include metadata tags designating where a patch should be inserted (for example, a set of four pixels defining a quadrilateral region in the image). While sometimes useful, such indicators may not be required; the sample picture received at operation <b>702</b> may, for example, be a stock image or publicly available image without any metadata tags. In some instances, metadata tags may be manually added (for example, a user might designate areas in the image suitable for tags).</p><p id="p-0079" num="0078">Many examples described throughout the present disclosure refer to an image of a person wearing a shirt with an adversarial patch either depicted on the shirt or inserted into the image to appear on the shirt. However, many other examples simulating other use cases for adversarial patches are also considered. For example, adversarial patches may be tested or depicted on road signs, animals, bodies of text such as license plates, etc.</p><p id="p-0080" num="0079">Method <b>700</b> further comprises generating an adversarial patch via an adversarial patch generator at operation <b>704</b>. In general, operation <b>704</b> may include selecting a size of the patch and adding features to a &#x201c;template&#x201d; to generate a patch. For example, operation <b>704</b> may include selecting a 600&#xd7;600 pixel patch (based on a default value or user input). A trained patch generator might add specific features to a particular template in order to yield an effective adversarial patch; for example, a trained patch generator might add curves and edges according to a particular pattern, resulting in modifying the template image into the adversarial patch. However, in some instances, the adversarial patch generator may initially be untrained. For example, operation <b>704</b> may initially include generating a set of random pixels (for example, each pixel may have randomly determined red/green/blue (RGB) values) for use as a template. In other instances, the template may initially simply be a blank (i.e., white) image. Further, an untrained patch generator may modify the template in a random manner. For example, an artificial neural network (ANN) may be utilized, including various layers, connections and weights; an untrained ANN may initially have random or default weights, which may be adjusted over time. In some instances, operation <b>704</b> may utilize an adversarial patch that is known to be effective (a &#x201c;known-good&#x201d; patch) as a template for generating a new patch.</p><p id="p-0081" num="0080">Operation <b>704</b> may include generating a patch based on a given distance. For example, operation <b>704</b> may include generating a patch designed to be effective within 5 meters (which may be considered a &#x201c;near&#x201d; patch/distance in some instances). In some instances, operation <b>704</b> may include generating a patch designed to be effective at 15 meters (considered a &#x201c;far&#x201d; patch/distance).</p><p id="p-0082" num="0081">Over time, generation at operation <b>704</b> may be refined. For example, certain features (such as concentrations of curves/edges, color gradients, etc.) may be learned to be particularly useful in fooling classifiers. Thus, even if initial iterations of operation <b>704</b> may be mostly random, in some instances operation <b>704</b> may include modifying a template based upon learned features to generate an effective adversarial patch. These refinements are often range-specific. For example, a first assortment of curves might be useful for deceiving recognition systems at near distances but useless at far distances. Therefore, modifying the patch generator to include the first assortment of curves in output patches might be useful for a near patch generator but unhelpful for a far patch generator. In view of this, the features implemented at operation <b>704</b> may be vary depending upon whether the generated patch is intended to be effective at a near distance or a far distance.</p><p id="p-0083" num="0082">Method <b>700</b> further comprises modifying, at operation <b>706</b>, the sample picture acquired via operation <b>702</b> to insert the adversarial patch generated via operation <b>704</b> in the image. The modifications of operation <b>706</b> may result in an adversarial image which may appear as if the adversarial patch were a part of the sample picture, as discussed in detail above. For example, a sample picture may depict a person wearing a shirt with a blank patch on it. Operation <b>706</b> may include editing the sample picture to look as if the person's shirt had the adversarial patch on it rather than the blank patch. For example, operation <b>706</b> may include identifying a size and shape of the blank patch and modifying the adversarial patch to match. As an example, an adversarial patch may be a square having sides 600 pixels in length. Operation <b>706</b> may include detecting a blank patch on a person's shirt, identifying that the blank patch is in the shape of a parallelogram having sides that are 50 pixels in length. Operation <b>706</b> may further include converting the adversarial patch from a square shape into the parallelogram shape, and scaling the size by 50/600. The resulting modified adversarial patch may then be overlaid over the blank patch in the image, resulting in the adversarial patch appearing as though the image were captured of a person wearing a shirt bearing the adversarial patch. In some instances, operation <b>706</b> may account for lighting, a disconnect between digital colors of the generated patch and printed colors that might be produced by the manufacture of the shirt depicted in the image, etc. when modifying the image.</p><p id="p-0084" num="0083">Method <b>700</b> further comprises evaluating object detection model performance at operation <b>708</b>. Operation <b>708</b> may include, for example, submitting the modified image as input to an object detection model and receiving an output. For example, an object detection model such as YOLO v3 may receive images as input, analyze the image in an attempt to recognize and identify objects depicted in the image, and output results of the analysis including identification of objects recognized in the image. In some instances, the output may include a classification and a confidence value describing how confident the model is in its classification. For example, the model may output &#x201c;98% dog&#x201d; indicating that the model is 98% confident that the image depicts a dog. Operation <b>708</b> may include evaluating an accuracy of the output of the system based on annotations of the modified image. As an example, operation <b>708</b> may include determining whether an object detection model is able to correctly identify that a person wearing the patch in the modified image was a person.</p><p id="p-0085" num="0084">For example, a sample picture may depict a person wearing a shirt. The sample picture may be known to depict a person (either via metadata tags or via a user manually identifying that the picture depicts a person). The sample picture may be modified to include the adversarial patch on the person's shirt, and the modified image may be submitted to an object detection model. The object detection model may output any recognized objects (including people). If the system determines that no people are present in an image that is known to include a person, the accuracy of the output may be evaluated as 0%. If the system determines that three people are present in an image that is known to include four people, the accuracy of the output may be 75%. If the system determines that four people are present in an image that is known to include two people, the accuracy of the output may be 50%, etc.</p><p id="p-0086" num="0085">The object detection model utilized at operation <b>708</b> may be an existing, pretrained system (e.g., YOLOv3) or can be a newly-trained system. In general, an accurate system is preferable, as an inaccurate system may erroneously result in the generator being designated as effective.</p><p id="p-0087" num="0086">Inclusion of an adversarial patch in an image may negatively impact accuracy of an object detection model. Such an impact can be described as a performance of the patch. Patches can be associated with one or more performance criteria, describing how effective the patch is at disrupting performance of object detection models.</p><p id="p-0088" num="0087">Method <b>700</b> further comprises determining whether performance criteria of the patch are met at operation <b>710</b>. Operation <b>710</b> may include, for example, comparing the performance identified at operation <b>808</b> to one or more thresholds, such as an accuracy threshold. For example, an accuracy threshold of 15% or lower may be a performance criterion; if the system is unable to defeat the patch at least 85% of the time, the patch's performance criteria may be met.</p><p id="p-0089" num="0088">If the patch's performance criteria are not met (<b>710</b> &#x201c;No&#x201d;), method <b>700</b> further comprises updating, at operation <b>812</b>, the patch generator based on the performance. Operation <b>712</b> may include adjusting one or more weights and/or connections in a machine learning model utilized in the patch generator. The adjustments made at operation <b>712</b> may be based on impact of previous adjustments; for example, if a first weight is increased and the patch's performance decreases, then operation <b>712</b> may include decreasing the first weight. In some instances, adjustments made at operation <b>712</b> may be random. In some instances, changes that are evaluated to result in more significant impacts on performance may be focused on. For example, a patch generator may be an artificial neural network (ANN) having a number of interconnected nodes, with each connection having a given weight. Changes to the generator may comprise changes to the weights of the connections. For example, a weight may be increased or decreased, and a resulting impact on generated patch performance can be evaluated.</p><p id="p-0090" num="0089">Once updates are made, method <b>700</b> loops back to operation <b>704</b>, generating a new patch from the updated patch generator. Operations <b>706</b>-<b>710</b> may then be repeated. As stated above, impacts of the adjustments made at operation <b>712</b> may be evaluated at subsequent iterations. For example, once a weight is increased or decreased, the resulting impact can be evaluated by generating a new patch via the revised generator, inserting the new patch into a sample picture, submitting the new modified image to an object detection model, and comparing an accuracy of the object detection model to that previously evaluated at operation <b>708</b>. If increasing a first weight results in a performance improvement, further increases to the first weight may be tested.</p><p id="p-0091" num="0090">Once the patch's performance criteria are met (<b>710</b> &#x201c;Yes&#x201d;), method <b>700</b> proceeds to end at operation <b>714</b>. In some instances, performance criteria may require several successful iterations before the generator is considered fully trained. As an example, a single patch in a single modified adversarial image may successfully deceive an object detection model. However, in order to ensure that this is not a coincidence/fluke, the same patch may be tested using other sample pictures.</p><p id="p-0092" num="0091">In some instances, trained patch generators may include some degree of randomization when generating patches; in other words, the same generator may be able to generate multiple different adversarial patches designed to perform the same task (e.g., hinder performance of an object detection model). Training a patch generator to generate unique adversarial patches that are all consistently able to deceive object detection models may yield a more expansive training set, useful for training an object detection model to defeat adversarial patches. Thus. In some instances, in order to ensure that a single successful iteration is not a fluke, the patch generator may generate additional patches, to be used in additional images, to verify that the patch generator is consistently able to generate patches that can deceive recognition systems.</p><p id="p-0093" num="0092">As different object detection models may perform differently, in some instances, method <b>700</b> may be performed using several different models to attempt to train a patch generator to generate adversarial patches capable of deceiving all available models. However, this may substantially increase training time and resource requirements, and there is no guarantee that a single patch generator may be able to generate patches that consistently deceive all object detection models. Thus, in some instances, method <b>700</b> may be repeated with different models, resulting in multiple patch generators. For example, method <b>700</b> may be performed with a first object detection model, resulting in a first adversarial patch generator capable of consistently generating adversarial patches that can fool the first object detection model. Method <b>700</b> may then be repeated for a second object detection model, resulting in a second adversarial patch generator (different from the first generator) capable of consistently generating adversarial patches that can fool the second object detection model.</p><p id="p-0094" num="0093">The generator trained via method <b>700</b> may be configured to generate patches effective in particular environmental conditions. For example, the generator may be trained to generate patches effective in bright lighting conditions, effective from far distances, effective even when only partially visible, etc. The conditions the generator is &#x201c;tailored&#x201d; toward in this way can be controlled at operations <b>702</b> and <b>706</b> (i.e., the operations involving the picture). For example, in order to train a far-distance patch generator, operation <b>702</b> may include selecting a sample picture depicting a person far from a camera capturing the image. Then, after inserting the adversarial patch into the picture at operation <b>706</b>, the resulting modified image may simulate the adversarial patch relatively far from the camera. Similarly, in order to train a bright-light patch generator (i.e., a generator that generates adversarial patches which are effective in bright-light conditions), operation <b>702</b> may include selecting a sample picture depicting a person standing in particularly bright light. Then, after inserting the adversarial patch into the picture at operation <b>706</b>, the resulting modified image may simulate the adversarial patch in relatively bright lighting conditions.</p><p id="p-0095" num="0094">Further, in some instances, multiple different generators can be trained for each object detection model. For example, method <b>700</b> can be performed a first time to train a first near generator using a first object detection model. Method <b>700</b> can be performed a second time to train a first far generator (distinct from the first near generator) using the same first object detection model, and so on.</p><p id="p-0096" num="0095">In some instances, a patch generator can essentially be trained to generate patches and then effectively &#x201c;pre-distort&#x201d; the patches. <figref idref="DRAWINGS">FIG. <b>8</b>A-<b>8</b>D</figref> depict examples of how such an example may be implemented. <figref idref="DRAWINGS">FIG. <b>8</b>A</figref> depicts an adversarial patch <b>810</b>, consistent with several embodiments of the present disclosure. <figref idref="DRAWINGS">FIG. <b>8</b>B</figref> depicts a sample picture <b>820</b> captured of patch <b>810</b> after being printed onto a shirt <b>824</b> worn by a user <b>822</b>, consistent with several embodiments of the present disclosure. As user <b>822</b> is standing at an angle, patch <b>810</b> may appear distorted (depicted in <figref idref="DRAWINGS">FIG. <b>8</b>B</figref> as distorted patch <b>826</b>). Patch <b>810</b> may be particularly effective at deceiving object detection models, but only when it appears as shown in <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>, or as close to it as possible. In other words, performance of patch <b>810</b> may drop dramatically with relatively minor distortions, so distorted patch <b>826</b> may fail to deceive an object detection model.</p><p id="p-0097" num="0096">To address this, in some instances, the patch may be preemptively transformed in reverse. <figref idref="DRAWINGS">FIG. <b>8</b>C</figref> depicts an example pre-transformed adversarial patch <b>830</b>, consistent with several embodiments of the present disclosure. <figref idref="DRAWINGS">FIG. <b>8</b>D</figref> depicts a sample picture <b>840</b> including user <b>822</b> wearing a shirt <b>824</b> bearing pre-transformed patch <b>830</b>, consistent with several embodiments of the present disclosure. Notably, due to the angle at which user <b>822</b> is standing, patch <b>830</b> is distorted (depicted in <figref idref="DRAWINGS">FIG. <b>8</b>D</figref> as distorted patch <b>846</b>). However, this distortion may result in patch <b>846</b> appearing nearly identical to patch <b>810</b>, resulting in relatively high performance. While the examples depicted in <figref idref="DRAWINGS">FIGS. <b>8</b>A-<b>8</b>D</figref> may be most useful only in relatively narrow circumstances, these are merely provided as illustrative examples of the overall concept that a generator may be trained to generate adversarial patches that are pre-transformed such that, upon being distorted, they appear in pictures in their most effective form. In particular, this may be useful when accounting for color transformations resulting from printer color discrepancies; if colors of a patch are an important factor of the performance of the patch (and colors often are), then preemptively adjusting the patch such that, when printed, it actually has the colors of the &#x201c;raw&#x201d; patch may be particularly advantageous. Training generators to be effective in this manner can correspondingly improve training of object detection systems, as described above with reference to method <b>900</b>.</p><p id="p-0098" num="0097"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a detailed method <b>900</b> for training an object detection model to defeat adversarial patches, consistent with several embodiments of the present disclosure. Method <b>900</b> comprises receiving an adversarial patch at operation <b>902</b>. Operation <b>902</b> may include, for example, receiving an adversarial patch from a local or online database, from a user, or the patch may be generated via an adversarial patch generator.</p><p id="p-0099" num="0098">Method <b>900</b> further comprises modifying, at operation <b>904</b>, a picture to include the adversarial patch. Operation <b>904</b> may be performed by, for example, a trained adversarial image generator, such as one trained via operation <b>102</b>/method <b>200</b>. For example, operation <b>904</b> may include utilizing a series of transformations produced by trained models such as a geometric transformation to transform a shape of the adversarial patch, a lighting adjustment transformation to account for lighting conditions of the picture (e.g., to adjust a brightness of the patch to match its surroundings), and a printer color transformation to adjust a color of the patch in order to simulate a color discrepancy introduced by a printing process. The patch may be transformed via each of these models (order may vary) and then be inserted into the sample picture, resulting in a first adversarial image.</p><p id="p-0100" num="0099">Method <b>900</b> further comprises evaluating an object detection model's performance based on the first adversarial image at operation <b>906</b>. Operation <b>906</b> may include, for example, submitting the first adversarial image to an object detection model and causing the system to attempt to classify/detect objects in the image. Results of the system can then be compared to known values. For example, operation <b>906</b> may include determining whether an object in the first adversarial image bearing the (transformed) adversarial patch was correctly detected and/or identified. Further, a confidence value of the object detection model may be retrieved.</p><p id="p-0101" num="0100">Method <b>900</b> further comprises determining whether object detection model performance criteria have been met at operation <b>908</b>. In some instances, operation <b>908</b> may include evaluating multiple criteria, such as evaluating a maximum overall performance, enforcing a minimum performance, etc., particularly when operation <b>906</b> includes multiple objects to detect. For example, an object detection model may be evaluated to identify a first object in an adversarial image including a transformed adversarial patch with 85% confidence and a second object in the adversarial image with 93% confidence. An overall performance may be a simple arithmetic mean of the two (i.e., (85%+93%)/2=89%). In some instances, performance criteria of the model may include a minimum overall performance of 95%, meaning the overall performance must meet or exceed 95% in order for the criterion to be met (in which case, the example object detection model would have failed, as 89%&#x3c;95%). In some instances, performance criteria of the model may include a minimum absolute performance of 90%, all particular performances must meet or exceed 90% in order for the criterion to be met (in which case, the example object detection model would also have failed, as even though 93%&#x3e;90%, 85%&#x3c;90%). Of course, the specific thresholds can vary and be controlled by users. These thresholds can be changed based upon use case; for example, a higher minimum overall performance of 99% may result in a more adversarial-patch-resilient object detection model, but may require additional resources to train. In some instances, performance criteria can require multiple consecutive successes in order to confirm that a given success if not a coincidence or fluke.</p><p id="p-0102" num="0101">If system performance criteria have not been met (<b>908</b> &#x201c;No&#x201d;), method <b>900</b> further comprises updating, at operation <b>910</b>, the object detection model based on the performances. Operation <b>910</b> may include, for example, adjusting one or more weights of a machine learning model, etc.</p><p id="p-0103" num="0102">Once performance criteria have been met (<b>908</b> &#x201c;Yes&#x201d;), method <b>900</b> ends at <b>912</b>. Thus, method <b>900</b> can train an object detection model to reliably defeat adversarial patches based on realistic examples with minimal investment in producing physical embodiments of the patches. In other words, the model can be trained without printing multiple patches onto multiple shirts and attempting to capture sets of pictures of each patch, etc. Thus, model <b>900</b> is advantageously scalable; adding additional patches/sample pictures to a training pool may only cost additional training time.</p><p id="p-0104" num="0103">Referring now to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, shown is a high-level block diagram of an example computer system <b>1000</b> that may be configured to perform various aspects of the present disclosure, including, for example, methods <b>100</b>, <b>200</b>, <b>300</b>, <b>800</b>, and <b>900</b>. The example computer system <b>800</b> may be used in implementing one or more of the methods or modules, and any related functions or operations, described herein (e.g., using one or more processor circuits or computer processors of the computer), in accordance with embodiments of the present disclosure. In some embodiments, the major components of the computer system <b>1000</b> may comprise one or more CPUs <b>1002</b>, a memory subsystem <b>1008</b>, a terminal interface <b>1016</b>, a storage interface <b>1018</b>, an I/O (Input/Output) device interface <b>1020</b>, and a network interface <b>1022</b>, all of which may be communicatively coupled, directly or indirectly, for inter-component communication via a memory bus <b>1006</b>, an I/O bus <b>1014</b>, and an I/O bus interface unit <b>1012</b>.</p><p id="p-0105" num="0104">The computer system <b>1000</b> may contain one or more general-purpose programmable processors <b>1002</b> (such as central processing units (CPUs)), some or all of which may include one or more cores <b>1004</b>A, <b>1004</b>B, <b>1004</b>C, and <b>1004</b>N, herein generically referred to as the CPU <b>1002</b>. In some embodiments, the computer system <b>1000</b> may contain multiple processors typical of a relatively large system; however, in other embodiments the computer system <b>1000</b> may alternatively be a single CPU system. Each CPU <b>1002</b> may execute instructions stored in the memory subsystem <b>1008</b> on a CPU core <b>1004</b> and may comprise one or more levels of on-board cache.</p><p id="p-0106" num="0105">In some embodiments, the memory subsystem <b>1008</b> may comprise a random-access semiconductor memory, storage device, or storage medium (either volatile or non-volatile) for storing data and programs. In some embodiments, the memory subsystem <b>1008</b> may represent the entire virtual memory of the computer system <b>1000</b> and may also include the virtual memory of other computer systems coupled to the computer system <b>1000</b> or connected via a network. The memory subsystem <b>1008</b> may be conceptually a single monolithic entity, but, in some embodiments, the memory subsystem <b>1008</b> may be a more complex arrangement, such as a hierarchy of caches and other memory devices. For example, memory may exist in multiple levels of caches, and these caches may be further divided by function, so that one cache holds instructions while another holds non-instruction data, which is used by the processor or processors. Memory may be further distributed and associated with different CPUs or sets of CPUs, as is known in any of various so-called non-uniform memory access (NUMA) computer architectures. In some embodiments, the main memory or memory subsystem <b>1008</b> may contain elements for control and flow of memory used by the CPU <b>1002</b>. This may include a memory controller <b>1010</b>.</p><p id="p-0107" num="0106">Although the memory bus <b>1006</b> is shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref> as a single bus structure providing a direct communication path among the CPU <b>1002</b>, the memory subsystem <b>1008</b>, and the I/O bus interface <b>1012</b>, the memory bus <b>1006</b> may, in some embodiments, comprise multiple different buses or communication paths, which may be arranged in any of various forms, such as point-to-point links in hierarchical, star or web configurations, multiple hierarchical buses, parallel and redundant paths, or any other appropriate type of configuration. Furthermore, while the I/O bus interface <b>1012</b> and the I/O bus <b>1014</b> are shown as single respective units, the computer system <b>1000</b> may, in some embodiments, contain multiple I/O bus interface units <b>1012</b>, multiple I/O buses <b>1014</b>, or both. Further, while multiple I/O interface units are shown, which separate the I/O bus <b>1014</b> from various communications paths running to the various I/O devices, in other embodiments some or all of the I/O devices may be connected directly to one or more system I/O buses.</p><p id="p-0108" num="0107">In some embodiments, the computer system <b>1000</b> may be a multi-user mainframe computer system, a single-user system, or a server computer or similar device that has little or no direct user interface but receives requests from other computer systems (clients). Further, in some embodiments, the computer system <b>1000</b> may be implemented as a desktop computer, portable computer, laptop or notebook computer, tablet computer, pocket computer, telephone, smart phone, mobile device, or any other appropriate type of electronic device.</p><p id="p-0109" num="0108">It is noted that <figref idref="DRAWINGS">FIG. <b>10</b></figref> is intended to depict the representative major components of an exemplary computer system <b>1000</b>. In some embodiments, however, individual components may have greater or lesser complexity than as represented in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, components other than or in addition to those shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref> may be present, and the number, type, and configuration of such components may vary.</p><p id="p-0110" num="0109">The present invention may be a system, a method, and/or a computer program product at any possible technical detail level of integration. The computer program product may include a computer readable storage medium (or media) having computer readable program instructions thereon for causing a processor to carry out aspects of the present invention.</p><p id="p-0111" num="0110">The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device. The computer readable storage medium may be, for example, but is not limited to, an electronic storage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing. A non-exhaustive list of more specific examples of the computer readable storage medium includes the following: a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a static random access memory (SRAM), a portable compact disc read-only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanically encoded device such as punch-cards or raised structures in a groove having instructions recorded thereon, and any suitable combination of the foregoing. A computer readable storage medium, as used herein, is not to be construed as being transitory signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through a waveguide or other transmission media (e.g., light pulses passing through a fiber-optic cable), or electrical signals transmitted through a wire.</p><p id="p-0112" num="0111">Computer readable program instructions described herein can be downloaded to respective computing/processing devices from a computer readable storage medium or to an external computer or external storage device via a network, for example, the Internet, a local area network, a wide area network and/or a wireless network. The network may comprise copper transmission cables, optical transmission fibers, wireless transmission, routers, firewalls, switches, gateway computers and/or edge servers. A network adapter card or network interface in each computing/processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium within the respective computing/processing device.</p><p id="p-0113" num="0112">Computer readable program instructions for carrying out operations of the present invention may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, firmware instructions, state-setting data, configuration data for integrated circuitry, or either source code or object code written in any combination of one or more programming languages, including an object oriented programming language such as Smalltalk, C++, or the like, and procedural programming languages, such as the &#x201c;C&#x201d; programming language or similar programming languages. The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider). In some embodiments, electronic circuitry including, for example, programmable logic circuitry, field-programmable gate arrays (FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the electronic circuitry, in order to perform aspects of the present invention.</p><p id="p-0114" num="0113">Aspects of the present invention are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems), and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.</p><p id="p-0115" num="0114">These computer readable program instructions may be provided to a processor of a computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks. These computer readable program instructions may also be stored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein comprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.</p><p id="p-0116" num="0115">The computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operational steps to be performed on the computer, other programmable apparatus or other device to produce a computer implemented process, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.</p><p id="p-0117" num="0116">The flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present invention. In this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s). In some alternative implementations, the functions noted in the blocks may occur out of the order noted in the Figures. For example, two blocks shown in succession may, in fact, be accomplished as one step, executed concurrently, substantially concurrently, in a partially or wholly temporally overlapping manner, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.</p><p id="p-0118" num="0117">The descriptions of the various embodiments of the present disclosure have been presented for purposes of illustration but are not intended to be exhaustive or limited to the embodiments disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the described embodiments. The terminology used herein was chosen to explain the principles of the embodiments, the practical application or technical improvement over technologies found in the marketplace, or to enable others of ordinary skill in the art to understand the embodiments disclosed herein.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method, comprising:<claim-text>training an adversarial image generator;</claim-text><claim-text>receiving a sample picture, the sample picture depicting an object;</claim-text><claim-text>receiving an adversarial patch;</claim-text><claim-text>generating, via the adversarial image generator, an adversarial image, wherein:<claim-text>the adversarial image depicts the adversarial patch in the sample picture; and</claim-text><claim-text>the generating includes:<claim-text>transforming the adversarial patch based on environmental attributes of the sample picture, resulting in a transformed adversarial patch; and</claim-text><claim-text>inserting the transformed adversarial patch into the picture, resulting in the adversarial image.</claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the training the adversarial image generator includes:<claim-text>receiving a raw training patch;</claim-text><claim-text>receiving a first training picture, the first training picture including an embedded training patch; and</claim-text><claim-text>mapping a first transformation between the raw training patch and the embedded training patch.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the training the adversarial image generator further includes:<claim-text>identifying a first set of environmental attributes, the first set of environmental attributes associated with the depicted training patch of the first training picture;</claim-text><claim-text>receiving a second training picture, the second training picture depicting the training patch;</claim-text><claim-text>mapping a second transformation between the received training patch and the depicted training patch of the second training picture;</claim-text><claim-text>identifying a second set of environmental attributes, the second set of environmental attributes associated with the depicted training patch of the second training picture; and</claim-text><claim-text>updating a model to output the first transformation based on receipt of the first set of environmental attributes and to output the second transformation based on receipt of the second set of environmental attributes.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising:<claim-text>receiving a third set of environmental attributes, the third set of environmental attributes associated with the sample patch depicted in the sample picture;</claim-text><claim-text>inputting the third set of environmental attributes into the model; and</claim-text><claim-text>receiving an output transformation from the model based on the inputting, wherein the transforming the adversarial patch is based on the output transformation.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the first set of environmental attributes describes lighting conditions of the first training picture.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the first set of environmental attributes describes a printer color discrepancy.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising training an adversarial patch generator, the training the adversarial patch generator including updating a weight by an amount, wherein the receiving the adversarial patch includes generating the adversarial patch via the adversarial patch generator.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising training an object detection model to detect the object, the training the object detection model including:<claim-text>submitting the adversarial image to the object detection model;</claim-text><claim-text>receiving an output from the object detection model as a result of the submitting the adversarial image;</claim-text><claim-text>evaluating a performance of the object detection model based on the output; and</claim-text><claim-text>updating the object detection model based on the performance.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A system, comprising:<claim-text>a memory; and</claim-text><claim-text>a processor coupled to the memory, the processor configured to:<claim-text>train an adversarial image generator;</claim-text><claim-text>receive a sample picture, the sample picture depicting an object;</claim-text><claim-text>receive an adversarial patch;</claim-text><claim-text>generate, via the adversarial image generator, an adversarial image, wherein:<claim-text>the adversarial image depicts the adversarial patch in the sample picture; and</claim-text><claim-text>the generating includes:<claim-text>transforming the adversarial patch based on environmental attributes of the sample picture, resulting in a transformed adversarial patch; and</claim-text><claim-text>inserting the transformed adversarial patch into the picture, resulting in the adversarial image.</claim-text></claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the training the adversarial image generator includes:<claim-text>receiving a raw training patch;</claim-text><claim-text>receiving a first training picture, the first training picture including an embedded training patch;</claim-text><claim-text>mapping a first transformation between the raw training patch and the embedded training patch.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the training the adversarial image generator further includes:<claim-text>identifying a first set of environmental attributes, the first set of environmental attributes associated with the depicted training patch of the first training picture;</claim-text><claim-text>receiving a second training picture, the second training picture depicting the training patch;</claim-text><claim-text>mapping a second transformation between the received training patch and the depicted training patch of the second training picture;</claim-text><claim-text>identifying a second set of environmental attributes, the second set of environmental attributes associated with the depicted training patch of the second training picture; and</claim-text><claim-text>updating a model to output the first transformation based on receipt of the first set of environmental attributes and to output the second transformation based on receipt of the second set of environmental attributes.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the processor is further configured to:<claim-text>receive a third set of environmental attributes, the third set of environmental attributes associated with the sample patch depicted in the sample picture;</claim-text><claim-text>input the third set of environmental attributes into the model; and</claim-text><claim-text>receive an output transformation from the model based on the inputting, wherein the transforming the adversarial patch is based on the output transformation.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the processor is further configured to train an adversarial patch generator, the training the adversarial patch generator including updating a weight by an amount, wherein the receiving the adversarial patch includes generating the adversarial patch via the adversarial patch generator.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the processor is further configured to train an object detection model to detect the object, the training the object detection model including:<claim-text>submitting the adversarial image to the object detection model;</claim-text><claim-text>receiving an output from the object detection model as a result of the submitting the adversarial image;</claim-text><claim-text>evaluating a performance of the object detection model based on the output; and</claim-text><claim-text>updating the object detection model based on the performance.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A computer program product, the computer program product comprising a computer readable storage medium having program instructions embodied therewith, the program instructions executable by a computer to cause the computer to:<claim-text>train an adversarial image generator;</claim-text><claim-text>receive a sample picture, the sample picture depicting an object;</claim-text><claim-text>receive an adversarial patch;</claim-text><claim-text>generate, via the adversarial image generator, an adversarial image, wherein:<claim-text>the adversarial image depicts the adversarial patch in the sample picture; and</claim-text><claim-text>the generating includes:<claim-text>transforming the adversarial patch based on environmental attributes of the sample picture, resulting in a transformed adversarial patch; and</claim-text><claim-text>inserting the transformed adversarial patch into the picture, resulting in the adversarial image.</claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computer program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the training the adversarial image generator includes:<claim-text>receiving a raw training patch;</claim-text><claim-text>receiving a first training picture, the first training picture including an embedded training patch;</claim-text><claim-text>mapping a first transformation between the raw training patch and the embedded training patch.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The computer program product of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the training the adversarial image generator further includes:<claim-text>identifying a first set of environmental attributes, the first set of environmental attributes associated with the depicted training patch of the first training picture;</claim-text><claim-text>receiving a second training picture, the second training picture depicting the training patch;</claim-text><claim-text>mapping a second transformation between the received training patch and the depicted training patch of the second training picture;</claim-text><claim-text>identifying a second set of environmental attributes, the second set of environmental attributes associated with the depicted training patch of the second training picture; and</claim-text><claim-text>updating a model to output the first transformation based on receipt of the first set of environmental attributes and to output the second transformation based on receipt of the second set of environmental attributes.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computer program product of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the instructions further cause the computer to:<claim-text>receive a third set of environmental attributes, the third set of environmental attributes associated with the sample patch depicted in the sample picture;</claim-text><claim-text>input the third set of environmental attributes into the model; and</claim-text><claim-text>receive an output transformation from the model based on the inputting, wherein the transforming the adversarial patch is based on the output transformation.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The computer program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions further cause the computer to train an adversarial patch generator, the training the adversarial patch generator including updating a weight by an amount, wherein the receiving the adversarial patch includes generating the adversarial patch via the adversarial patch generator.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computer program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions further cause the computer to train an object detection model to detect the object, the training the object detection model including:<claim-text>submitting the adversarial image to the object detection model;</claim-text><claim-text>receiving an output from the object detection model as a result of the submitting the adversarial image;</claim-text><claim-text>evaluating a performance of the object detection model based on the output; and</claim-text><claim-text>updating the object detection model based on the performance.</claim-text></claim-text></claim></claims></us-patent-application>