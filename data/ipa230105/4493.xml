<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004494A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004494</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17942283</doc-number><date>20220912</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0811</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0846</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0882</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0811</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0246</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0848</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0882</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">VIRTUALIZED CACHES</invention-title><us-related-documents><division><relation><parent-doc><document-id><country>US</country><doc-number>17103856</doc-number><date>20201124</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11442856</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17942283</doc-number></document-id></child-doc></relation></division><us-provisional-application><document-id><country>US</country><doc-number>62940235</doc-number><date>20191125</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SiFive, Inc.</orgname><address><city>San Mateo</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Terpstra</last-name><first-name>Wesley Waylon</first-name><address><city>San Mateo</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems and methods are disclosed for virtualized caches. For example, an integrated circuit (e.g., a processor) for executing instructions includes a virtually indexed physically tagged first-level (L1) cache configured to output to an outer memory system one or more bits of a virtual index of a cache access as one or more bits of a requestor identifier. For example, the L1 cache may be configured to operate as multiple logical L1 caches with a cache way of a size less than or equal to a virtual memory page size. For example, the integrated circuit may include an L2 cache of the outer memory system that is configured to receive the requestor identifier and implement a cache coherency protocol to disambiguate an L1 synonym occurring in multiple portions of the virtually indexed physically tagged L1 cache associated with different requestor identifier values.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="171.62mm" wi="106.00mm" file="US20230004494A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="193.63mm" wi="107.95mm" file="US20230004494A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="220.90mm" wi="148.51mm" orientation="landscape" file="US20230004494A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="228.60mm" wi="157.99mm" orientation="landscape" file="US20230004494A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="222.00mm" wi="133.43mm" orientation="landscape" file="US20230004494A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="218.10mm" wi="146.30mm" file="US20230004494A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 17/103,856, which was filed on Nov. 24, 2020, which claims the benefit of U.S. Provisional Application No. 62/940,235, filed on Nov. 25, 2019, which is incorporated herein by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">This disclosure relates to virtualized caches.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Integrated circuits are typically designed and tested in a multi-step process that involves multiple specialized engineers performing a variety of different design and verification tasks on an integrated circuit design. A variety of internal or proprietary (e.g., company-specific) integrated circuit design tool chains are typically used by these engineers to handle different parts of the integrated circuit design workflow of using commercial electronic design automation (EDA) tools.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0005" num="0004">The disclosure is best understood from the following detailed description when read in conjunction with the accompanying drawings. It is emphasized that, according to common practice, the various features of the drawings are not to-scale. On the contrary, the dimensions of the various features are arbitrarily expanded or reduced for clarity.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an example of an integrated circuit for executing instructions using a virtualized cache.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram of an example of an effective memory address used to access memory via a virtualized cache.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is block diagram of an example of a system for facilitating design and manufacture of integrated circuits.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is block diagram of an example of a system for facilitating design of integrated circuits.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow chart of an example of a process for generating an integrated circuit design including a processor core with a virtualized L1 cache based on design parameters specifying multiple logical L1 caches for the processor core.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><heading id="h-0006" level="2">Overview</heading><p id="p-0011" num="0010">In some microprocessor first-level (L1) caches, a virtual address may be used to index cache static random-access memory (SRAM) and the corresponding physical address may be used to tag the entries (i.e., a virtually indexed physically tagged (VIPT) cache). As the virtual address is available earlier, indexing using the virtual address allows designs to overlap or parallelize computation of the physical address (e.g., using a translation lookaside buffer (TLB)) and cache access. However, using a VIPT cache can introduce the possibility of synonyms, where the same physical cache block appears at two different virtual indexes in cache, depending on the active virtual-physical translation.</p><p id="p-0012" num="0011">One solution to the synonym problem is to limit the L1 cache size to be no greater than the virtual memory page size (e.g., 4 KiB, 8 KiB, 16 KiB, or 64 KiB) times the number of ways in the L1 cache. For example, in a system with 4 KiB pages, and a 4-way set-associative L1 cache, the L1 cache size would be limited to 16 KiB. Limiting the cache size to the virtual memory page size times the number of ways in the cache works, because the index bits of the virtual and physical addresses are identical below the virtual memory page size. However, for larger caches, limiting the size of the cache way to the virtual memory page size may entail using more ways and checking a larger number of candidate tags. Limiting the size of the cache way to the virtual memory page size also limits SRAM tag array depth (e.g., to 64 entries in a typical system with 64-byte cache blocks).</p><p id="p-0013" num="0012">Described herein are techniques which overcome this limitation by splitting the L1 cache into multiple logical L1 caches, which share the same underlying physical implementation. These techniques may use the L2 cache to disambiguate L1 synonyms as a normal consequence of cache coherency. In some implementations, these techniques enable the L1 cache to safely hold two shared synonyms. Compared to way speculation, these techniques are simpler and may reduce or eliminate the impact on the pipeline.</p><p id="p-0014" num="0013">There are three key observations which underlie this approach. First, the outer memory system usually has the capability to resolve ownership conflicts between multiple cores with respective L1 caches. Second, only the high bits of the virtual address can actually cause synonyms. Third, synonyms are relatively uncommon in real software, since physical pages shared between processes may often be similarly aligned. For example, larger RISC-V mega/giga-page mappings are always more aligned.</p><p id="p-0015" num="0014">For example, in an L1 cache with 8 KiB per way in a system with 4 KiB pages, the highest virtual index bit can cause synonyms. This highest bit may be called the synonym address (SA). To prevent synonyms, requests with SA=0 may be handled as coming from logical L1 cache ID=0 and requests with SA=1 may be handled as coming from logical L1 cache ID=1. If there is another core with the same L1 cache configuration, it would use cache IDs 2 and 3, respectively. When core <b>1</b> requests a cache block which is owned by core <b>2</b>, the outer memory system must probe core <b>2</b> to retrieve permissions on the block. Similarly, if there is a synonym, when core <b>1</b> ID=0 requests a block, if that block is held by the same core, but under ID=1, the memory system will probe core <b>1</b>. In other words, whenever a synonym would have been created in the L1 cache, the outer memory subsystem eliminates the second copy via a looped back probe. The implementation to support two logical L1 caches associated with one core may be identical to the existing coherence policy used between cores <b>1</b> and <b>2</b>.</p><p id="p-0016" num="0015">These techniques may work equally well with modified exclusive shared invalid (MESI) schemes. In this situation, it is possible for the two synonyms to both be in the shared state. When one copy would be updated, the other copy is invalidated by a looped back probe. In a modified owned exclusive shared invalid (MOESI) system, the &#x2018;transfer&#x2019; between virtual L1 caches of the same core covers a short distance. For example, in an update-based system, the synonym may transfer between virtual L1 caches of the same core and then notify the L2 cache directory of the transfer.</p><p id="p-0017" num="0016">In some implementations, the outer memory system is automatically configured to accommodate a variable number of L1 caches. Therefore, this technique may operate without changes to the L2 caches, physical topology, or underlying coherence protocol. In some implementations, the only change is that the parameterization describing the L1 cache to the memory system now describes two L1 caches.</p><p id="p-0018" num="0017">For a directory-based L2 cache implementation, the physical consequence of this technique is that the L2 cache now maintains more ownership bits (e.g., two ownership bits) per inner cache where it previously stored one ownership bit. Given the relative size of the tag, this is not particularly expensive.</p><p id="p-0019" num="0018">Implementing these techniques in the L1 cache requires only that the requests from the L1 cache indicate which &#x2018;virtual L1 cache&#x2019; sourced the request. Most coherence protocols (e.g., TileLink) include some form of requestor identifier. Thus, this identifier is widened by the number of bits in the synonym address (SA) (e.g., 1 bit or 2 bits) and the value is supplied from the SA. Conversely, when a probe request is received, the L1 cache must retrieve the SA from the destination identifier and use the SA as the high bit(s) when checking the state of the cache block. In some implementations, no other changes to the L1 cache are necessary.</p><p id="p-0020" num="0019">These techniques can be extended to more than a single SA bit. For example, to support 16 KiB per way with a 4 KiB virtual memory page size, two SA bits may be used. For example, there may be four virtual L1 caches and the L2 cache may include four ownership bits per L1 cache. Larger synonym addresses (e.g., 3 bits) may be used, however, increasing the number of SA bits leads to an exponential increase in cost in the L2 cache's directory. In some implementations, if an exponential increase in cost in the L2 cache's directory is undesirable, then the directory may enforce exclusion among the virtual L1 caches, allowing a linear growth in cost with the number of SA bits. These techniques can be combined with way speculation to achieve large VIPT L1 caches.</p><p id="p-0021" num="0020">These techniques may be applied to most coherence protocols (e.g., TileLink).</p><p id="p-0022" num="0021">These techniques may be applied to most coherence policies (e.g., MOESI, MESI, and MI).</p><p id="p-0023" num="0022">These techniques may be applied to coherent VIPT L1 instruction caches and VIPT L1 data caches. These techniques may be applied to virtually indexed virtually tagged (VIVT) L1 instruction caches and VIVT L1 data caches. These techniques may be applied to virtually indexed or virtually tagged L2 caches or at any level in addition to L1 or L2.</p><p id="p-0024" num="0023">Some of these techniques may be combined with any coherent memory system, not just directory-based.</p><p id="p-0025" num="0024">As used herein, the term &#x201c;circuit&#x201d; refers to an arrangement of electronic components (e.g., transistors, resistors, capacitors, and/or inductors) that is structured to implement one or more functions. For example, a circuit may include one or more transistors interconnected to form logic gates that collectively implement a logical function.</p><heading id="h-0007" level="2">Details</heading><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an example of an integrated circuit <b>110</b> for executing instructions using a virtualized cache. The integrated circuit <b>110</b> includes a processor core (e.g., an IP core), which include a physical L1 cache <b>130</b>. The physical L1 cache <b>130</b> is configured to include multiple logical L1 caches, logical L1 cache A <b>132</b> and logical L1 cache B <b>134</b>. The integrated circuit <b>110</b> also includes an outer memory system <b>140</b>. In this example, the outer memory system <b>140</b> includes an L2 cache <b>150</b>, which may be configured to implement a cache coherency protocol/policy to maintain cache coherency across multiple L1 caches. In this example, the L2 cache <b>150</b> includes ownership bits <b>160</b> for multiple logical L1 caches, including the logical L1 cache A <b>132</b> and the logical L1 cache B <b>134</b>. Although not shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the integrated circuit <b>110</b> may include multiple processor cores in some implementations. Although not shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the outer memory system <b>140</b> may include multiple layers.</p><p id="p-0027" num="0026">The physical L1 cache <b>130</b> may be configured to output to the outer memory system <b>140</b> one or more bits of a virtual index (e.g., a synonym address (SA)) of a cache access as one or more bits of a requestor identifier. In some implementations, the physical L1 cache <b>130</b> is configured to receive one or more bits of a destination identifier for a probe request from the outer memory system <b>140</b> and use the one or more bits of the destination identifier as part of an index (e.g., as a synonym address) to check a state of a cache block. For example, the physical L1 cache may be a virtually indexed L1 cache (e.g., a VIPT or a VIVT cache). For example, the logical L1 cache A <b>132</b> and the logical L1 cache B <b>134</b> may have cache way sizes that are less than or equal to a virtual memory page size used by the processor core <b>120</b>. For example, the size of a cache way in the physical L1 cache <b>130</b> may be equal to twice a virtual memory page size used by the processor core <b>120</b> of the integrated circuit <b>110</b> associated with the physical L1 cache <b>130</b>, and the one or more bits of the virtual index may be a single bit. Although not shown in the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a physical L1 cache may be partitioned into more than two logical caches (e.g., four logical caches) to support an even larger physical cache size. For example, the physical L1 cache may be a data cache. For example, the physical L1 cache may be an instruction cache.</p><p id="p-0028" num="0027">The L2 cache <b>150</b> may be configured to receive a requestor identifier and implement a cache coherency protocol to disambiguate an L1 synonym occurring in multiple portions (e.g., the logical L1 cache A <b>132</b> and the logical L1 cache B <b>134</b>) of the physical L1 cache <b>130</b> associated with different requestor identifier values. For example, the L2 cache <b>150</b> may include ownership bits <b>160</b> that respectively correspond to each of multiple portions (e.g., the logical L1 cache A <b>132</b> and the logical L1 cache B <b>134</b>) of the physical L1 cache <b>130</b> associated with different requestor identifier values. For example, the L2 cache <b>150</b> may be directory-based.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram of an example of an effective memory address <b>200</b> used to access memory via a virtualized cache (e.g., the physical L1 cache <b>130</b>). The effective memory address <b>200</b> includes a cache block offset <b>210</b>, a virtual index <b>220</b> used to select a cache block for access by a processor core (e.g., the processor core <b>120</b>), and a tag <b>230</b> (e.g., a physical tag or a virtual tag). The virtual index <b>220</b> includes a synonym address <b>240</b> (SA) that includes one or more bits that can be used to select from among multiple logical caches within the physical cache. For example, the synonym address <b>240</b> may be a single bit to select from among two logical caches. For example, the synonym address <b>240</b> may be a pair of bits to select from among four logical caches. The synonym address <b>240</b> may be used to determine a requestor identifier for a logical cache of the physical cache that is used when communicating with an outer memory system (e.g., the outer memory system <b>140</b>) to identify logical cache associated with a cache access operation.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is block diagram of an example of a system <b>300</b> for facilitating design and manufacture of integrated circuits. The system <b>300</b> includes, a network <b>306</b>, an integrated circuit design service infrastructure <b>310</b>, an FPGA/emulator server <b>320</b>, and a manufacturer server <b>330</b>. For example, a user may utilize a web client or a scripting API client to command the integrated circuit design service infrastructure <b>310</b> to automatically generate an integrated circuit design based a set of design parameter values selected by the user for one or more template integrated circuit designs. In some implementations, the integrated circuit design service infrastructure <b>310</b> may be configured to automatically generate an integrated circuit design (e.g., encoded in a register-transfer logic data structure, a field programmable gate array emulation data structure, and/or a physical design data structure, such as a GDSII file) that includes a processor core with multiple logical L1 caches.</p><p id="p-0031" num="0030">For example, the integrated circuit design service infrastructure <b>310</b> may invoke (e.g., via network communications over the network <b>306</b>) testing of the resulting design that is performed by the FPGA/emulation server <b>320</b> that is running one or more FPGAs or other types of hardware or software emulators. For example, the integrated circuit design service infrastructure <b>310</b> may invoke a test using a field programmable gate array, programmed based on a field programmable gate array emulation data structure, to obtain an emulation result. The field programmable gate array may be operating on the FPGA/emulation server <b>320</b>, which may be a cloud server. Test results may be returned by the FPGA/emulation server <b>320</b> to the integrated circuit design service infrastructure <b>310</b> and relayed in a useful format to the user (e.g., via a web client or a scripting API client).</p><p id="p-0032" num="0031">The integrated circuit design service infrastructure <b>310</b> may also facilitate the manufacture of integrated circuits using the integrated circuit design in a manufacturing facility associated with the manufacturer server <b>330</b>. In some implementations, a physical design specification (e.g., a GDSII file) based on a physical design data structure for the integrated circuit is transmitted to the manufacturer server <b>330</b> to invoke manufacturing of the integrated circuit (e.g., using manufacturing equipment of the associated manufacturer). For example, the manufacturer server <b>330</b> may host a foundry tape out website that is configured to receive physical design specifications (e.g., as a GDSII file or an OASIS file) to schedule or otherwise facilitate fabrication of integrated circuits. In some implementations, the integrated circuit design service infrastructure <b>310</b> supports multi-tenancy to allow multiple integrated circuit designs (e.g., from one or more users) to share fixed costs of manufacturing (e.g., reticle/mask generation, and/or shuttles wafer tests). For example, the integrated circuit design service infrastructure <b>310</b> may use a fixed package (e.g., a quasi-standardized packaging) that is defined to reduce fixed costs and facilitate sharing of reticle/mask, wafer test, and other fixed manufacturing costs. For example, the physical design specification may include one or more physical designs from one or more respective physical design data structures in order to facilitate multi-tenancy manufacturing.</p><p id="p-0033" num="0032">In response to the transmission of the physical design specification, the manufacturer associated with the manufacturer server <b>330</b> may fabricate and/or test integrated circuits based on the integrated circuit design. For example, the associated manufacturer (e.g., a foundry) may perform optical proximity correction (OPC) and similar post-tapeout/pre-production processing, fabricate the integrated circuit(s) <b>332</b>, update the integrated circuit design service infrastructure <b>310</b> (e.g., via communications with a controller or a web application server) periodically or asynchronously on the status of the manufacturing process, perform appropriate testing (e.g., wafer testing), and send to packaging house for packaging. A packaging house may receive the finished wafers or dice from the manufacturer and test materials, and update the integrated circuit design service infrastructure <b>310</b> on the status of the packaging and delivery process periodically or asynchronously. In some implementations, status updates may be relayed to the user when the user checks in using the web interface and/or the controller might email the user that updates are available.</p><p id="p-0034" num="0033">In some implementations, the resulting integrated circuits <b>332</b> (e.g., physical chips) are delivered (e.g., via mail) to a silicon testing service provider associated with a silicon testing server <b>340</b>. In some implementations, the resulting integrated circuits <b>332</b> (e.g., physical chips) are installed in a system controlled by silicon testing server <b>340</b> (e.g., a cloud server) making them quickly accessible to be run and tested remotely using network communications to control the operation of the integrated circuits <b>332</b>. For example, a login to the silicon testing server <b>340</b> controlling manufacture of integrated circuits <b>332</b> may be sent to the integrated circuit design service infrastructure <b>310</b> and relayed to a user (e.g., via a web client). For example, the integrated circuit design service infrastructure <b>310</b> may implement the process <b>500</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref> to automatically generate an integrated circuit design (e.g., including a register-transfer logic data structure and/or a physical design data structure) and to control fabrication and silicon testing of one or more integrated circuits <b>332</b>, which may be structured based on the integrated circuit design.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is block diagram of an example of a system <b>400</b> for facilitating design of integrated circuits. The system <b>400</b> is an example of an internal configuration of a computing device that may be used to implement the integrated circuit design service infrastructure <b>310</b> as a whole or one or more components of the integrated circuit design service infrastructure <b>310</b> of the system <b>300</b> shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The system <b>400</b> can include components or units, such as a processor <b>402</b>, a bus <b>404</b>, a memory <b>406</b>, peripherals <b>414</b>, a power source <b>416</b>, a network communication interface <b>418</b>, a user interface <b>420</b>, other suitable components, or a combination thereof.</p><p id="p-0036" num="0035">The processor <b>402</b> can be a central processing unit (CPU), such as a microprocessor, and can include single or multiple processors having single or multiple processing cores. Alternatively, the processor <b>402</b> can include another type of device, or multiple devices capable of manipulating or processing information. For example, the processor <b>402</b> can include multiple processors interconnected in any manner, including hardwired or networked, including wirelessly networked. In some implementations, the operations of the processor <b>402</b> can be distributed across multiple physical devices or units that can be coupled directly or across a local area or other suitable type of network. In some implementations, the processor <b>402</b> can include a cache, or cache memory, for local storage of operating data or instructions.</p><p id="p-0037" num="0036">The memory <b>406</b> can include volatile memory, non-volatile memory, or a combination thereof. For example, the memory <b>406</b> can include volatile memory, such as one or more dynamic random-access memory (DRAM) modules such as double data rate (DDR) synchronous dynamic random-access memory (SDRAM), and non-volatile memory, such as a disk drive, a solid state drive, flash memory, Phase-Change Memory (PCM), or any form of non-volatile memory capable of persistent electronic information storage, such as in the absence of an active power supply. The memory <b>406</b> can include another type of device, or multiple devices, now existing or hereafter developed, capable of storing data or instructions for processing by the processor <b>402</b>. The processor <b>402</b> can access or manipulate data in the memory <b>406</b> via the bus <b>404</b>. Although shown as a single block in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the memory <b>406</b> can be implemented as multiple units. For example, a system <b>400</b> can include volatile memory, such as RAM, and persistent memory, such as a hard drive or other storage.</p><p id="p-0038" num="0037">The memory <b>406</b> can include executable instructions <b>408</b>; data, such as application data <b>410</b> or an operating system <b>412</b>; or a combination thereof for immediate access by the processor <b>402</b>. The executable instructions <b>408</b> can include, for example, one or more application programs, which can be loaded or copied, in whole or in part, from non-volatile memory to volatile memory to be executed by the processor <b>402</b>. The executable instructions <b>408</b> can be organized into programmable modules or algorithms, functional programs, codes, code segments, or combinations thereof to perform various functions described herein. For example, the executable instructions <b>408</b> can include instructions executable by the processor <b>402</b> to cause the system <b>400</b> to automatically, in response to a command, generate an integrated circuit design and associated test results based on a design parameters data structure. For example, the executable instructions <b>408</b> may include instructions, such as the Chisel code snippet of Appendix A, for generating an integrated circuit design including a processor core with multiple logical L1 caches based on a set of design parameters. The application data <b>410</b> can include, for example, user files, database catalogs or dictionaries, configuration information or functional programs, such as a web browser, a web server, a database server, or a combination thereof. The operating system <b>412</b> can be, for example, Microsoft Windows&#xae;, Mac OS X&#xae;, or Linux&#xae;; an operating system for a small device, such as a smartphone or tablet device; or an operating system for a large device, such as a mainframe computer. The memory <b>406</b> can include one or more devices and can utilize one or more types of storage, such as solid state or magnetic storage.</p><p id="p-0039" num="0038">The peripherals <b>414</b> can be coupled to the processor <b>402</b> via the bus <b>404</b>. The peripherals <b>414</b> can be sensors or detectors, or devices containing any number of sensors or detectors, which can monitor the system <b>400</b> itself or the environment around the system <b>400</b>. For example, a system <b>400</b> can contain a temperature sensor for measuring temperatures of components of the system <b>400</b>, such as the processor <b>402</b>. In some implementations, the power source <b>416</b> can be a battery, and the system <b>400</b> can operate independently of an external power distribution system. Any of the components of the system <b>400</b>, such as the peripherals <b>414</b> or the power source <b>416</b>, can communicate with the processor <b>402</b> via the bus <b>404</b>.</p><p id="p-0040" num="0039">The network communication interface <b>418</b> can also be coupled to the processor <b>402</b> via the bus <b>404</b>. In some implementations, the network communication interface <b>418</b> can include one or more transceivers. The network communication interface <b>418</b> can, for example, provide a connection or link to a network, such as the network <b>306</b>, via a network interface, which can be a wired network interface, such as Ethernet, or a wireless network interface. For example, the system <b>400</b> can communicate with other devices via the network communication interface <b>418</b> and the network interface using one or more network protocols, such as Ethernet, TCP, IP, power line communication (PLC), WiFi, infrared, GPRS, GSM, CDMA, or other suitable protocols.</p><p id="p-0041" num="0040">A user interface <b>420</b> can include a display; a positional input device, such as a mouse, touchpad, touchscreen, or the like; a keyboard; or other suitable human or machine interface devices. The user interface <b>420</b> can be coupled to the processor <b>402</b> via the bus <b>404</b>. Other interface devices that permit a user to program or otherwise use the system <b>400</b> can be provided in addition to or as an alternative to a display. In some implementations, the user interface <b>420</b> can include a display, which can be a liquid crystal display (LCD), a cathode-ray tube (CRT), a light emitting diode (LED) display (e.g., an OLED display), or other suitable display. In some implementations, a client or server can omit the peripherals <b>414</b>. The operations of the processor <b>402</b> can be distributed across multiple clients or servers, which can be coupled directly or across a local area or other suitable type of network. The memory <b>406</b> can be distributed across multiple clients or servers, such as network-based memory or memory in multiple clients or servers performing the operations of clients or servers. Although depicted here as a single bus, the bus <b>404</b> can be composed of multiple buses, which can be connected to one another through various bridges, controllers, or adapters.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow chart of an example of a process <b>500</b> for generating an integrated circuit design including a processor core with a virtualized L1 cache based on design parameters specifying multiple logical L1 caches for the processor core. The process <b>500</b> includes accessing <b>510</b> design parameters for a processor core that indicate multiple logical L1 caches to be included in the processor core; based on the design parameters, automatically generating <b>520</b> a register-transfer logic data structure specifying an integrated circuit design that includes the processor core with multiple logical L1 caches; storing <b>530</b> the register-transfer logic data structure; based on the register-transfer logic data structure, automatically generating <b>540</b> a physical design data structure specifying an integrated circuit design that includes the processor core with multiple logical L1 caches; and invoking <b>550</b> fabrication, based on the physical design data structure, of an integrated circuit that includes the processor core with multiple logical L1 caches. For example, the process <b>500</b> may be implemented by the integrated circuit design service infrastructure <b>310</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. For example, the process <b>500</b> may be implemented by the system <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0043" num="0042">The process <b>500</b> includes accessing <b>510</b> design parameters for a processor core that indicate multiple logical L1 caches to be included in the processor core. In some implementations, the design parameters may include a count of logical caches to be included in a processor core (e.g., an IP core). In some implementations, the design parameters may include a size of logical caches to be included in a processor core. In some implementations, the design parameters may include a virtual memory page size for the processor core. For example, the design parameters may be accessed <b>510</b> by receiving the design parameters (e.g., via network communications using the network communications interface <b>418</b>). For example, the design parameters may be accessed <b>510</b> by reading the design parameters from memory (e.g., reading from the memory <b>406</b> via the bus <b>404</b>).</p><p id="p-0044" num="0043">The process <b>500</b> includes, based on the design parameters, automatically generating <b>520</b> a register-transfer logic data structure (e.g., a file, a database, a repository, or a bitstream) specifying an integrated circuit design that includes the processor core (e.g., the processor core <b>120</b>) with multiple logical L1 caches (e.g., the logical L1 cache A <b>132</b> and the logical L1 cache B <b>134</b>). For example, automatically generating <b>520</b> the register-transfer level data structure for the integrated circuit design may include invoking a register-transfer level service with input data based on the design parameters. For example, the multiple logical L1 caches of the processor core may be portions of a single physical L1 cache in the processor core. For example, the multiple logical L1 caches of the processor core may each have a cache way of a size equal to a virtual memory page size used by the processor core. In some implementations, the multiple logical L1 caches of the processor core are virtually indexed physically tagged caches. In some implementations, automatically generating <b>520</b> the register-transfer logic data structure includes executing Chisel code (e.g., including the code snippet of Appendix A) that takes the design parameters as inputs.</p><p id="p-0045" num="0044">The process <b>500</b> includes storing <b>530</b> the register-transfer logic data structure (e.g., a file, a database, a repository, or a bitstream). For example, the register-transfer level data structure may be stored <b>530</b> in memory (e.g., the memory <b>406</b>). For example, the register-transfer level data structure may be transmitted to an external device (e.g., a personal computing device) for display or storage. For example, the register-transfer level data structure may be transmitted via a network communications interface (e.g., the network communications interface <b>418</b>).</p><p id="p-0046" num="0045">The process <b>500</b> includes based on the register-transfer logic data structure, automatically generating <b>540</b> a physical design data structure (e.g., a physical design file) specifying an integrated circuit design that includes the processor core with multiple logical L1 caches. For example, generating <b>540</b> a physical design data structure for the integrated circuit may include invoking a physical design service with data based on the register-transfer level data structure and/or a design parameters data structure. For example, generating <b>540</b> a physical design data structure for the integrated circuit may include invoking synthesis and place &#x26; route tools (e.g., Synopsys&#x2122;, Cadence&#x2122;, and/or Mentor&#x2122; tools). For example, generating <b>540</b> a physical design data structure for the integrated circuit may include performing logical equivalent checking. For example, generating <b>540</b> a physical design data structure for the integrated circuit may include invoking static timing analysis tools. For example, generating <b>540</b> a physical design data structure for the integrated circuit may include performing design rule checking (DRC) and/or layout versus schematic (LVS) checking. For example, generating <b>540</b> a physical design data structure for the integrated circuit may include determining power, performance, and area estimates for the resulting integrated circuit design and providing these estimates as feedback to a user (e.g., a user of a web client). For example, the physical design data structure may include in less-technical terms whether there are any issues with the physical design. For example, the physical design data structure may highlight important components of the output of the synthesis and place &#x26; route tools. For example, the physical design data structure may include a GDSII file or an OASIS file. For example, generating <b>540</b> a physical design data structure for the integrated circuit may include managing and orchestrating physical design toolchains in a cloud. For example, generating <b>540</b> a physical design data structure for the integrated circuit may include handling database movement from tool to tool, and managing access to third party IP cores. For example, generating <b>540</b> a physical design data structure for the integrated circuit may include accessing template designs, which may allow for significant design reuse. For example, generating <b>540</b> a physical design data structure for the integrated circuit may include identifying those combinations to reduce workload. For example, generating <b>540</b> a physical design data structure for the integrated circuit may provide better or more compact error/issue reporting, by translating tool issues into manageable feedback and providing the actual error/output of tools in a deliverable format to a user (e.g., a user of a web client).</p><p id="p-0047" num="0046">The process <b>500</b> includes invoking <b>550</b> fabrication, based on the physical design data structure, of an integrated circuit that includes the processor core with multiple logical L1 caches. In some implementations, a physical design specification (e.g., a GDSII file) based on a physical design data structure for the integrated circuit is transmitted via a network (e.g., the network <b>306</b>) to a manufacturer server (e.g., the manufacturer server <b>330</b>) to invoke <b>550</b> fabrication of the integrated circuit (e.g., using manufacturing equipment of the associated manufacturer). For example, the manufacturer server <b>330</b> may host a foundry tape out website that is configured to receive physical design specifications (e.g., as a GDSII file or an OASIS file) to schedule or otherwise facilitate fabrication of integrated circuits. In some implementations, fabrication of the integrated circuit may be invoked <b>550</b> by direct control of manufacturing equipment (e.g., via communication over a bus or serial port).</p><p id="p-0048" num="0047">It should be noted that one or more steps of the process <b>500</b> may be omitted. For example, the steps <b>540</b> and/or <b>550</b> may be omitted. One or more steps may be added to the process <b>500</b>, such as automatically generating a software development kit (SDK), documentation, a field programmable gate array emulation data structure, and/or a test plan for the integrated circuit design and/or invoking testing of the integrated circuit and receiving a test results data structure.</p><p id="p-0049" num="0048">In a first aspect, the subject matter described in this specification can be embodied in an integrated circuit for executing instructions that includes a virtually indexed physically tagged L1 cache configured to output to an outer memory system one or more bits of a virtual index of a cache access as one or more bits of a requestor identifier. The integrated circuit may include an L2 cache of the outer memory system that is configured to receive the requestor identifier and implement a cache coherency protocol to disambiguate an L1 synonym occurring in multiple portions of the virtually indexed physically tagged L1 cache associated with different requestor identifier values. For example, the L2 cache may include ownership bits that respectively correspond to each of the multiple portions of the virtually indexed physically tagged L1 cache associated with different requestor identifier values. For example, the L2 cache may be directory-based. For example, the virtually indexed physically tagged L1 cache may be configured to receive one or more bits of a destination identifier for a probe request from the outer memory system and use the one or more bits of the destination identifier as part of an index to check a state of a cache block. In some implementations, the size of the cache way in the virtually indexed physically tagged L1 cache is equal to twice a virtual memory page size used by a core of the integrated circuit associated with the virtually indexed physically tagged L1 cache, and the one or more bits of the virtual index are a single bit. In some implementations, the size of the cache way in the virtually indexed physically tagged L1 cache is equal to four times a virtual memory page size used by a core of the integrated circuit associated with the virtually indexed physically tagged L1 cache, and the one or more bits of the virtual index are two bits. For example, the virtually indexed physically tagged L1 cache may be a data cache. For example, the virtually indexed physically tagged L1 cache may be an instruction cache.</p><p id="p-0050" num="0049">In a second aspect, the subject matter described in this specification can be embodied in methods that include accessing design parameters for a processor core that indicate multiple logical L1 caches to be included in the processor core; based on the design parameters, automatically generating a register-transfer logic data structure specifying an integrated circuit design that includes the processor core with multiple logical L1 caches; and storing the register-transfer logic data structure. For example, the multiple logical L1 caches of the processor core may be portions of a single physical L1 cache in the processor core. In some implementations, the multiple logical L1 caches of the processor core each have a cache way of a size equal to a virtual memory page size used by the processor core. For example, the multiple logical L1 caches of the processor core may be virtually indexed physically tagged caches. In some implementations, automatically generating the register-transfer logic data structure comprises executing Chisel code that takes the design parameters as inputs. For example, automatically generating the register-transfer logic data structure may include executing the Chisel code snippet of Appendix A. In some implementations, the method further includes, based on the register-transfer logic data structure, automatically generating a physical design data structure specifying an integrated circuit design that includes the processor core with multiple logical L1 caches. In some implementations, the method further includes, invoking fabrication, based on the physical design data structure, of an integrated circuit that includes the processor core with multiple logical L1 caches.</p><p id="p-0051" num="0050">In a third aspect, the subject matter described in this specification can be embodied in systems that include a network interface; a memory; and a processor, wherein the memory includes instructions executable by the processor to cause the system to: access design parameters for a processor core that indicate multiple logical L1 caches to be included in the processor core; based on the design parameters, automatically generate a register-transfer logic data structure specifying an integrated circuit design that includes the processor core with multiple logical L1 caches; and store the register-transfer logic data structure. In some implementations, the memory includes instructions executable by the processor to cause the system to: based on the register-transfer logic data structure, automatically generate a physical design data structure specifying an integrated circuit design that includes the processor core with multiple logical L1 caches. In some implementations, the memory includes instructions executable by the processor to cause the system to: invoke fabrication, based on the physical design data structure, of an integrated circuit that includes the processor core with multiple logical L1 caches. For example, the multiple logical L1 caches of the processor core may be portions of a single physical L1 cache in the processor core. In some implementations, the multiple logical L1 caches of the processor core each have a cache way of a size equal to a virtual memory page size used by the processor core. For example, the multiple logical L1 caches of the processor core may be virtually indexed physically tagged caches. In some implementations, automatically generating the register-transfer logic data structure comprises executing Chisel code that takes the design parameters as inputs. For example, the memory may include the Chisel code snippet of Appendix A.</p><p id="p-0052" num="0051">In a fourth aspect, the subject matter described in this specification can be embodied in a non-transitory computer-readable storage medium that includes instructions that, when executed by a processor, facilitate performance of operations comprising: accessing design parameters for a processor core that indicate multiple logical L1 caches to be included in the processor core; based on the design parameters, automatically generating a register-transfer logic data structure specifying an integrated circuit design that includes the processor core with multiple logical L1 caches; and storing the register-transfer logic data structure. For example, the multiple logical L1 caches of the processor core may be portions of a single physical L1 cache in the processor core. In some implementations, the multiple logical L1 caches of the processor core each have a cache way of a size equal to a virtual memory page size used by the processor core. For example, the multiple logical L1 caches of the processor core may be virtually indexed physically tagged caches. In some implementations, automatically generating the register-transfer logic data structure comprises executing Chisel code that takes the design parameters as inputs. For example, the non-transitory computer-readable storage medium may store the Chisel code snippet of Appendix A. In some implementations, the non-transitory computer-readable storage medium includes instructions that, when executed by a processor, facilitate performance of operations comprising: based on the register-transfer logic data structure, automatically generating a physical design data structure specifying an integrated circuit design that includes the processor core with multiple logical L1 caches.</p><p id="p-0053" num="0052">While the disclosure has been described in connection with certain embodiments, it is to be understood that the disclosure is not to be limited to the disclosed embodiments but, on the contrary, is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims, which scope is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>accessing design parameters for a processor core that indicate multiple logical L1 caches to be included in the processor core;</claim-text><claim-text>based on the design parameters, automatically generating a register-transfer logic data structure specifying an integrated circuit design that includes the processor core with multiple logical L1 caches; and</claim-text><claim-text>storing the register-transfer logic data structure.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, in which the multiple logical L1 caches of the processor core are portions of a single physical L1 cache in the processor core.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, in which the multiple logical L1 caches of the processor core each have a cache way of a size equal to a virtual memory page size used by the processor core.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, in which the multiple logical L1 caches of the processor core are virtually indexed physically tagged caches.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, in which automatically generating the register-transfer logic data structure comprises executing Chisel code that takes the design parameters as inputs.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising:<claim-text>based on the register-transfer logic data structure, automatically generating a physical design data structure specifying an integrated circuit design that includes the processor core with multiple logical L1 caches.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, comprising:<claim-text>invoking fabrication, based on the physical design data structure, of an integrated circuit that includes the processor core with multiple logical L1 caches.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A system comprising:<claim-text>a memory; and</claim-text><claim-text>a processor, wherein the memory includes instructions executable by the processor to cause the system to:<claim-text>access design parameters for a processor core that indicate multiple logical L1 caches to be included in the processor core;</claim-text><claim-text>based on the design parameters, automatically generate a register-transfer logic data structure specifying an integrated circuit design that includes the processor core with multiple logical L1 caches; and</claim-text><claim-text>store the register-transfer logic data structure.</claim-text></claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the memory includes instructions executable by the processor to cause the system to:<claim-text>based on the register-transfer logic data structure, automatically generate a physical design data structure specifying an integrated circuit design that includes the processor core with multiple logical L1 caches.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the memory includes instructions executable by the processor to cause the system to:<claim-text>invoke fabrication, based on the physical design data structure, of an integrated circuit that includes the processor core with multiple logical L1 caches.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, in which the multiple logical L1 caches of the processor core are portions of a single physical L1 cache in the processor core.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, in which the multiple logical L1 caches of the processor core each have a cache way of a size equal to a virtual memory page size used by the processor core.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, in which the multiple logical L1 caches of the processor core are virtually indexed physically tagged caches.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the memory includes instructions executable by the processor to cause the system to:<claim-text>execute Chisel code that takes the design parameters as inputs.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory computer-readable storage medium that includes instructions that, when executed by a processor, facilitate performance of operations comprising:<claim-text>accessing design parameters for a processor core that indicate multiple logical L1 caches to be included in the processor core;</claim-text><claim-text>based on the design parameters, automatically generating a register-transfer logic data structure specifying an integrated circuit design that includes the processor core with multiple logical L1 caches; and</claim-text><claim-text>storing the register-transfer logic data structure.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, in which the multiple logical L1 caches of the processor core are portions of a single physical L1 cache in the processor core.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, in which the multiple logical L1 caches of the processor core each have a cache way of a size equal to a virtual memory page size used by the processor core.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, in which the multiple logical L1 caches of the processor core are virtually indexed physically tagged caches.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, in which automatically generating the register-transfer logic data structure comprises executing Chisel code that takes the design parameters as inputs.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, including instructions that, when executed by a processor, facilitate performance of operations comprising:<claim-text>based on the register-transfer logic data structure, automatically generating a physical design data structure specifying an integrated circuit design that includes the processor core with multiple logical L1 caches.</claim-text></claim-text></claim></claims></us-patent-application>