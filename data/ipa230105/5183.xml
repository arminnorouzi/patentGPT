<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005184A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005184</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17693634</doc-number><date>20220314</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>80</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>90</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>593</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>271</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>243</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>60</class><subclass>W</subclass><main-group>40</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>17</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>579</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>246</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>254</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>239</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>282</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>58</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>85</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>97</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>90</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>593</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>271</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>243</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>40</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>17</main-group><subgroup>002</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>579</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>246</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>254</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>239</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>282</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>582</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30252</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10028</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10012</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>2420</main-group><subgroup>42</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10024</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>2013</main-group><subgroup>0081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">NON-RIGID STEREO VISION CAMERA SYSTEM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17365623</doc-number><date>20210701</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11282234</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17693634</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/US2021/012294</doc-number><date>20210106</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17365623</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62964148</doc-number><date>20200122</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>NODAR Inc.</orgname><address><city>Somerville</city><state>MA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Jiang</last-name><first-name>Leaf Alden</first-name><address><city>Concord</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Rosen</last-name><first-name>Philip Bradley</first-name><address><city>Lincoln</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Swierczynski</last-name><first-name>Piotr</first-name><address><city>Providence</city><state>RI</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>NODAR Inc.</orgname><role>02</role><address><city>Somerville</city><state>MA</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A long-baseline and long depth-range stereo vision system is provided that is suitable for use in non-rigid assemblies where relative motion between two or more cameras of the system does not degrade estimates of a depth map. The stereo vision system may include a processor that tracks camera parameters as a function of time to rectify images from the cameras even during fast and slow perturbations to camera positions. Factory calibration of the system is not needed, and manual calibration during regular operation is not needed, thus simplifying manufacturing of the system.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="212.85mm" wi="158.75mm" file="US20230005184A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="229.02mm" wi="150.28mm" file="US20230005184A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="240.37mm" wi="164.76mm" file="US20230005184A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="227.33mm" wi="163.83mm" file="US20230005184A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="161.54mm" wi="171.79mm" file="US20230005184A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="218.95mm" wi="120.90mm" file="US20230005184A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="229.36mm" wi="148.00mm" file="US20230005184A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="204.55mm" wi="131.40mm" file="US20230005184A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="193.97mm" wi="168.23mm" file="US20230005184A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="212.26mm" wi="154.60mm" file="US20230005184A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="200.83mm" wi="160.02mm" file="US20230005184A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="159.68mm" wi="177.72mm" file="US20230005184A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="207.86mm" wi="162.73mm" file="US20230005184A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="190.58mm" wi="169.93mm" file="US20230005184A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="231.73mm" wi="175.09mm" file="US20230005184A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="234.02mm" wi="169.93mm" file="US20230005184A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="231.73mm" wi="173.31mm" file="US20230005184A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="245.45mm" wi="176.78mm" file="US20230005184A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="168.32mm" wi="175.18mm" file="US20230005184A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="238.00mm" wi="175.94mm" file="US20230005184A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="238.25mm" wi="175.43mm" file="US20230005184A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="242.15mm" wi="175.43mm" file="US20230005184A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="242.15mm" wi="175.43mm" file="US20230005184A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="133.35mm" wi="175.51mm" file="US20230005184A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="242.32mm" wi="175.94mm" file="US20230005184A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="222.50mm" wi="183.22mm" file="US20230005184A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="207.69mm" wi="167.13mm" file="US20230005184A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="189.40mm" wi="141.90mm" file="US20230005184A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="238.17mm" wi="174.07mm" file="US20230005184A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="170.86mm" wi="162.90mm" file="US20230005184A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00030" num="00030"><img id="EMI-D00030" he="156.97mm" wi="153.25mm" file="US20230005184A1-20230105-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00031" num="00031"><img id="EMI-D00031" he="206.42mm" wi="158.50mm" file="US20230005184A1-20230105-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00032" num="00032"><img id="EMI-D00032" he="231.99mm" wi="166.62mm" file="US20230005184A1-20230105-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The present application is a continuation of U.S. application Ser. No. 17/365,623, filed Jul. 1, 2021, entitled &#x201c;Non-Rigid Stereo Vision Camera System,&#x201d; which is a continuation of International Application No. PCT/US2021/12294 filed Jan. 6, 2021, entitled &#x201c;Non-Rigid Stereo Vision Camera System,&#x201d; which claims the benefit of priority of U.S. Provisional Application No. 62/964,148 filed Jan. 22, 2020, entitled &#x201c;Untethered Stereo Vision Camera System.&#x201d; The entire contents of these applications is incorporated by reference herein.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The technology of the present invention relates to stereo vision systems. In particular, the present technology relates to a stereo vision system (e.g., a stereo camera system) having characteristics for improved operation in the presence of fast and slow mechanical perturbations, and to methods for autocalibration of the system.</p><heading id="h-0003" level="1">RELATED ART</heading><p id="p-0004" num="0003">Stereo camera systems may be used to acquire three-dimensional information about objects in the field of view by triangulation techniques.</p><p id="p-0005" num="0004">Conventional stereo camera systems may use rigid mounting members to fix the position of cameras with respect to each other. For example, U.S. Pat. No. 6,392,688B1 discloses the use of a thick metal plate as a rigid mounting member for direct attachment of cameras and their associated CMOS sensors and lenses to mechanically stabilize the relative position of the cameras. The thickness of the plate shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> of this document is approximately one-fifth of the distance between the cameras, which makes such a structure very heavy, which may not be suitable for use in a system with wide-baseline stereo cameras. For example, a stereo camera system with a 1-m baseline between cameras would need a 20-cm-thick metal plate, which would weigh 108 kg if made of aluminum (1 m&#xd7;0.2 m&#xd7;0.2 m). The terms &#x201c;wide baseline&#x201d; and &#x201c;long baseline&#x201d; may be used interchangeably herein.</p><p id="p-0006" num="0005">A problem with such existing stereo camera systems is that individual camera modules (sometimes referred herein to as &#x201c;camera sensors&#x201d; or simply &#x201c;cameras&#x201d;) may shift and/or rotate with respect to each other over time, which may make initial calibration parameters become inaccurate over time. It is desirable for the fields of view of the camera modules of a stereo camera system not to move relative to each other in order for the stereo camera system to remain properly calibrated. For some stereo camera systems, a relative camera orientation rotation of only 0.05 degrees can ruin the calibration of the system. Moreover, in a conventional stereo camera system, there is potential for movement of, e.g., lens holders relative to circuit boards and/or other camera components, for relative pointing between camera modules to change, and also for positions of the circuit boards and the frame itself to change over time. The multiple interfaces between components in a conventional stereo camera system make it likely that vibration, shock, and even thermal expansion between components will cause the field of view of the cameras in a stereo camera system to shift over time. The relative movement of the camera components may invalidate the initial factory calibration of the system, which may make stereo vision data obtained by the system unreliable.</p><p id="p-0007" num="0006">Such calibration problems have not been addressed or even recognized in the prior art because prior-art stereo vision systems typically have either been laboratory systems, which generally are not subjected to shocks or vibrations, or have been used in situations such as short-range indoor robotics, which generally are not situations where highly accurate calibration is regarded as being critical. The inherent susceptibility of conventional stereo vision systems to losing calibration has therefore not been recognized as a particular problem to be solved, because conventional systems have been limited to short baseline lengths and utilize large mechanical stiffeners.</p><p id="p-0008" num="0007">Conventional automatic calibration methods typically fall into two categories: sparse keypoint approaches and vanishing point approaches. Keypoints are points of interest that may be easily recognized in images, such as corners or edges, and often may have associated image descriptors. For example, a scale invariant feature transform (SIFT) method developed by David Lowe is an example of an algorithm for finding keypoints and providing a descriptor that is invariant to translations, rotations, and scaling transformations. Vanishing points are used to represent a point at infinite distance. Vanishing points are sensitive to camera-module orientation but not to relative translation between the camera modules, and can be used to align the orientation of a pair of stereo camera modules.</p><p id="p-0009" num="0008">US patents U.S. Pat. No. 8,797,387B2 and U.S. Ser. No. 10/097,812B2 disclose methods for automatically calibrating stereo vision systems using sparse keypoint approaches. The method in U.S. Pat. No. 8,797,387B2 extracts keypoints, matches keypoints in images from first and second cameras, and determines a camera calibration based on a single-value decomposition analysis of a vertical error. The method in patent U.S. Ser. No. 10/097,812B2 is similar to that of U.S. Pat. No. 8,797,387B2 but tracks keypoints over multiple image frames and uses a structure-from-motion technique to apply a three-dimensional constraint. Sparse keypoint approaches can fail when keypoints are incorrectly matched between images from the first and second cameras, which can happen in image scenes with repeating structures (e.g., picket fences, building windows, etc.). Furthermore, sub-pixel accuracy of feature points may be required to obtain camera parameters with sufficient accuracy for long-baseline and long-range operation, which is often not possible with smooth or rounded features, or with slightly defocused or blurred images. Conventional structure-from-motion calculations generally are not fast enough to compensate for calibration errors from frame to frame, and thus typically are appropriate for slow perturbations of short-baseline stereo vision systems. Another problem with sparse keypoint approaches is the sparse sampling of an image, which does not provide enough information to determine the intrinsic or extrinsic camera parameters accurately. For example, images of typical road scenes may have most features clustered in the center of the image away from the sky and away from the textureless road surface, and therefore sampling occurs generally in the center of the image. This clustered sampling does not provide enough information to determine with high accuracy the relative orientation of the stereo cameras (an extrinsic camera parameter), much less lens distortion coefficients (an intrinsic camera parameter), which are sensitive to distortions at the edges of the image. In other words, these conventional techniques do not compensate for camera parameters that may be important for proper calibration of a stereo vision system.</p><p id="p-0010" num="0009">Japanese patent JP2008-509619A discloses a method for automatically calibrating stereo vision equipment by searching for a vanishing point and correcting for pitch and yaw errors. The method does not correct for roll errors or for a relative translation vector between the equipment's two cameras. Furthermore, the method requires straight and reliable road paint markings, which may not be available or may be obstructed by snow or faded by wear or sun exposition, thus limiting angular accuracy of the equipment's calibration.</p><p id="p-0011" num="0010">A 2018 conference paper entitled &#x201c;Flexible Stereo: Constrained, Non-rigid, Wide-baseline Stereo Vision for Fixed-Wing Aerial Platforms,&#x201d; by authors T. Hinzmann, T. Taubner, and R. Siegwart, discloses a method for stereo vision using components mounted on the wings of a model airplane. Because the wings are flexible and may move, the authors developed a system to compensate for relative camera motion. The system requires the use of inertial measurement units (IMUs) attached directly to stereo cameras to measure acceleration and angular rates of the cameras. The system then computes the relative orientation and position of the cameras using an extended Kalman filter. One disadvantage of this system is that it requires IMU hardware and cannot correct for camera position from a video stream alone. Another disadvantage is that the IMUs can be used to compensate for fast perturbations but not for slow perturbation (e.g., a slow drift of positions of the cameras), which means that the cameras may need to be manually calibrated on a frequent basis (e.g., daily).</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0012" num="0011">The calibration problems described above may be exacerbated in long-baseline stereo vision systems, where the distance between camera modules is, e.g., greater than approximately 20 cm, and in systems where it is not possible to connect two or more camera modules with heavy structural beams or mounting members. For styling and for an optimal viewing vantage point, it is sometimes desirable to mount independent cameras on structures that are non-rigid (e.g., a flexible structure). For example, mounting a pair of stereo cameras in a vehicle's headlights, at upper corners of a windshield of a vehicle, or on side-view mirrors of a vehicle, would be advantageous as such locations would be convenient for establishing a long baseline for a stereo vision system in a vehicle (e.g., a car, a truck, a bus, etc.). However, these locations are not sufficiently rigid to maintain calibration over hours or days, much less a 15-year lifetime of a typical vehicle. In some cases, it may not be realistically possible to add rigid mechanical support structures between the headlights, the upper corners of the windshield, and the side-view mirrors, because a shortest path for these support structures may be blocked by an engine block, may block a driver's view, and may be blocked by the driver himself/herself, respectively. Furthermore, even if supporting structures could be added, a thickness and weight of such structures required to provide adequate structural support may be impractical. That is, because a beam's deflection displacement scales as the cube of the beam's length, the longer the beam the greater the beam may displace for the same amount of force applied to the beam. For example, for the same end force, a 2-m baseline stereo vision system will displace 1000 times more than a 20-cm baseline stereo vision system. This scaling has resulted in commercially available stereo vision systems to be physically limited to baselines of less than 20 cm.</p><p id="p-0013" num="0012">There is a need for a stereo vision system that may operate in high vibrational and shock environments, that may be mounted on non-rigid structures, that may support long-baseline and long-range stereo vision, and that may operate with high accuracy.</p><p id="p-0014" num="0013">Automatic calibration technology suitable for long-baseline and non-rigid structures has not been achieved prior to the technology disclosed herein for at least four reasons: (1) magnitude: by physics, beam displacements increase as the cube of beam length (see discussion above) and therefore the magnitude of relative camera displacements increases with beam length. The additional structural support to ensure rigidity and prevent camera displacements may not be realistically achievable; (2) bandwidth: a calibration speed of an autocalibration system should increase with increasing baseline length to account for both slow and fast variations. The processor hardware required to achieve the needed computational speed may be costly and may add unwanted complexity to the system; (3) accuracy: a calibration accuracy needed for long-baseline systems may be much greater than for short-baseline systems, because greater angular accuracies are required to estimate targets at longer distances; and (4) completeness: whereas conventional techniques for automatic calibration of stereo cameras may compensate for a few camera parameters (e.g., the relative orientation of the stereo cameras), a full calibration would require compensating for all extrinsic and intrinsic camera parameters. Prior to the technology presented herein, a system that addresses these four reasons has not yet been achieved.</p><p id="p-0015" num="0014">The inventive technology described herein and claimed in the claims is directed to a stereo vision system in which no rigid mounting member is required. Vision may be achieved via a plurality of vision sensors (e.g., camera modules). In some aspects of the present technology, first and second camera modules (e.g., camera sensors) may be placed on or attached to structures that may flex, shift, bend, or/or move. As noted elsewhere herein, the terms camera sensor, camera module, and camera may be used interchangeably. A stereo camera system according to aspects of the present technology may be comprised of a processor configured to perform active tracking and compensation for movement of a first camera relative to a second camera, to provide accurate depth maps over all motion frequencies from slow material deformations due to, e.g., temperature changes, to fast vibrational motions due to, e.g., road or engine noise. Because there is no requirement for rigid mounting of cameras, aspects of the present technology may enable wide-baseline stereo configurations for long-range measurements without requiring periodic manual calibration.</p><p id="p-0016" num="0015">According to an aspect of the present technology, a stereo vision system is provided. The stereo vision system may be comprised of: a first camera sensor configured to sense first reflected energy of a first image and to generate first sensor signals based on the first reflected energy; a second camera sensor configured to sense second reflected energy of a second image and generate second sensor signals based on the second reflected energy; at least one processor configured to receive the first sensor signals from the first camera sensor and the second sensor signals from the second camera sensor, and to produce three-dimensional (3D) data from the first and second sensor signals. The at least one processor may be configured to: generate rectified stereo images from the first and second sensor signals utilizing stereo calibration parameters, perform a stereo matching on the rectified images, and perform an automatic system calibration using data from a plurality of stereo images obtained by the first and second camera sensors. The automatic system calibration may be based on minimizing a cost function. In some embodiments of this aspect, the first reflected energy and/or the second reflected energy may be comprised of light energy or photons originating from at least one electronic device (e.g., headlight(s), streetlight(s), laser(s), etc.). In some embodiments of this aspect, the first reflected energy and/or the second reflected energy may be comprised of solar light energy or photons originating from the sun. In some embodiments of this aspect, the first reflected energy and/or the second reflected energy may be comprised of a combination of solar light energy and light energy originating from an electronic device.</p><p id="p-0017" num="0016">According to another aspect of the present technology, a computer-implemented method for detection of an object in an image is provided. The method, which may be performed by one or more processor(s), may be comprised of: identifying connected component regions on the object by color; determining an aspect ratio of each of the connected component regions; determining a distance between the connected component regions; and identifying the object to be a known object based on the aspect ratios and the distance between the connected component regions. Any one or any combination of: a presence, a location, and a size of the known object may be determined from the image.</p><p id="p-0018" num="0017">According to another aspect of the present technology, a computer-implemented calibration method to calibrate a stereo vision system is provided. The method may be performed by one or more processor(s). The stereo vision system may include a first camera sensor configured to sense first reflected energy of a first image and to generate first sensor signals based on the first reflected energy and a second camera sensor configured to sense second reflected energy of a second image and generate second sensor signals based on the second reflected energy. The method may be comprised of: generating stereo images from the first and second sensor signals; rectifying the stereo images using stereo calibration parameters, to produce rectified stereo images; performing a stereo matching on the rectified stereo images; and performing an automatic system calibration using a result of the stereo matching. The automatic system calibration may be based on minimizing a cost function. In some embodiments of this aspect, the first reflected energy and/or the second reflected energy may be comprised of light energy or photons originating from at least one electronic device (e.g., headlight(s), streetlight(s), laser(s), etc.). In some embodiments of this aspect, the first reflected energy and/or the second reflected energy may be comprised of solar light energy or photons originating from the sun. In some embodiments of this aspect, the first reflected energy and/or the second reflected energy may be comprised of a combination of solar light energy and light energy originating from an electronic device.</p><p id="p-0019" num="0018">According to another aspect of the present technology, a non-transitory computer readable medium is provided in which is stored computer-executable code that, when executed by one or more processor(s), may cause the one or more processor(s) to calibrate a stereo vision system that may include a first camera sensor configured to sense first reflected energy of a first image and to generate first sensor signals based on the first reflected energy and a second camera sensor configured to sense second reflected energy of a second image and generate second sensor signals based on the second reflected energy. The method may be comprised of: generating stereo images from the first and second sensor signals; rectifying the stereo images using stereo calibration parameters, to produce rectified stereo images; performing a stereo matching on the rectified stereo images; and performing an automatic system calibration using a result of the stereo matching. The automatic system calibration may be based on minimizing a cost function. In some embodiments of this aspect, the first reflected energy and/or the second reflected energy may be comprised of light energy or photons originating from at least one electronic device (e.g., headlight(s), streetlight(s), laser(s), etc.). In some embodiments of this aspect, the first reflected energy and/or the second reflected energy may be comprised of solar light energy or photons originating from the sun. In some embodiments of this aspect, the first reflected energy and/or the second reflected energy may be comprised of a combination of solar light energy and light energy originating from an electronic device.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>C</figref> show three different locations for mounting cameras in or on a car without using or requiring large and heavy structural support material between the cameras, according to some embodiments of the present technology.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>C</figref> show translucent or partially transparent views of the cars of <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>C</figref>, respectively, according to some embodiments of the present technology.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>F</figref> show view of forward-looking stereo-vision camera modules mounted in a car at various locations, according to some embodiments of the present technology.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a view of a forward-looking stereo-vision camera sensors mounted on a truck, according to some embodiments of the present technology.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>C</figref> show a perspective view, a plan view, and a front elevational view of a side- and back-looking camera modules of a stereo vision system, with the camera modules mounted on a side-view mirror of a truck, according to some embodiments of the present technology.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a schematic view of cameras of a stereo vision system mounted on a traffic light, according to some embodiments of the present technology.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a schematic view of cameras of a stereo vision system mounted on a lamp post, according to some embodiments of the present technology.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a schematic view of cameras of a stereo vision system mounted on a construction crane, according to some embodiments of the present technology.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows a schematic view of cameras of a stereo vision system mounted on a fixed-wing aircraft, according to some embodiments of the present technology.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIGS. <b>10</b>A and <b>10</b>B</figref> show a front perspective view and a rear perspective view, respectively, of camera arrangements of a stereo vision system mounted on an off-road or all-terrain vehicle, according to some embodiments of the present technology.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows a schematic view of cameras of two stereo vision systems mounted on a combine harvester, according to some embodiments of the present technology.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIGS. <b>12</b>A, <b>12</b>B, and <b>12</b>C</figref> show a side elevational view, a front elevational view, and a plan view of cameras of a stereo vision system mounted on an automated forklift, according to some embodiments of the present technology.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows a block diagram of a stereo vision system, according to some embodiments of the present technology.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>14</b></figref> shows a block diagram of a processing component of the system of <figref idref="DRAWINGS">FIG. <b>13</b></figref>, according to some embodiments of the present technology.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>15</b>A</figref> shows a flow diagram of a rectification algorithm utilized in some embodiments of the present technology; and <figref idref="DRAWINGS">FIG. <b>15</b>B</figref> shows geometric details of a rectification procedure performed by the algorithm, according to some embodiments of the present technology.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>16</b></figref> shows a block diagram of an autocalibration engine of the processing component of <figref idref="DRAWINGS">FIG. <b>14</b></figref>, according to some embodiments of the present technology.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>17</b>A</figref> shows a block diagram of a calibration engine of the autocalibration engine of <figref idref="DRAWINGS">FIG. <b>16</b></figref>, according to some embodiments of the present technology.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>17</b>B</figref> shows a flow diagram of a fast optimization method, according to some embodiments of the present technology.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>17</b>C-<b>1</b></figref> shows a flow diagram of a medium optimization method, according to some embodiments of the present technology.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>17</b>C-<b>2</b></figref> shows a flow diagram of an angle-search process of the medium optimization method of <figref idref="DRAWINGS">FIG. <b>17</b>C-<b>1</b></figref>, according to some embodiments of the present technology.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIGS. <b>17</b>D-<b>1</b> through <b>17</b>D-<b>4</b></figref> show flow diagrams of a slow optimization method, according to some embodiments of the present technology.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>18</b></figref> shows a flow diagram of a procedure of a stereo image stabilization engine of the autocalibration engine of <figref idref="DRAWINGS">FIG. <b>16</b></figref>, according to some embodiments of the present technology.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>19</b>A</figref> shows a block diagram of a procedure of an absolute range calibration engine of the autocalibration engine of <figref idref="DRAWINGS">FIG. <b>16</b></figref>, in which the procedure has a non-negative disparity constraint, according to some embodiments of the present technology; and <figref idref="DRAWINGS">FIG. <b>19</b>B</figref> is a chart showing how a number of pixels with negative disparity can vary as a function of yaw, according to some embodiments of the present technology.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>20</b>A</figref> shows a flow diagram of a procedure of an absolute range calibration engine of the autocalibration engine of <figref idref="DRAWINGS">FIG. <b>16</b></figref>, in which the procedure is for an object with known dimensions, according to some embodiments of the present technology; and <figref idref="DRAWINGS">FIG. <b>20</b>B</figref> shows an example of imaging of an object with known dimensions, according to some embodiments of the present technology.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>21</b>A</figref> shows a flow diagram of a procedure of an absolute range calibration engine of the autocalibration engine of <figref idref="DRAWINGS">FIG. <b>16</b></figref>, in which the procedure utilizes vehicle odometry, according to some embodiments of the present technology; and <figref idref="DRAWINGS">FIG. <b>21</b>B</figref> is a chart showing how disparity can vary as a function of range, according to some embodiments of the present technology.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>22</b></figref> shows a flow diagram of a procedure of a calibration manager of the autocalibration engine of <figref idref="DRAWINGS">FIG. <b>16</b></figref>, according to some embodiments of the present technology.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>23</b></figref> shows a time series input diagram of the calibration manager of <figref idref="DRAWINGS">FIG. <b>22</b></figref>, according to some embodiments of the present technology.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>24</b></figref> shows a flow diagram of a procedure of a stop sign detector, according to some embodiments of the present technology.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>25</b></figref> shows a diagram illustrating a procedure of a stereo correspondence engine of the processing component of <figref idref="DRAWINGS">FIG. <b>14</b></figref>, according to some embodiments of the present technology.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0049" num="0048">The inventors have developed image processing techniques that may enable a stereo vision system to produce depth maps with high quality even in the presence of vibrations and even when a mounting structure for cameras of the system is not rigid (e.g., the mounting structure may be deformed easily from external forces). The inventors have recognized that a camera autocalibration method that may compensate for both fast and slow perturbations may enable long-baseline (also referred to herein as &#x201c;wide-baseline&#x201d;) stereo vision systems to be used on dynamic platforms, such as robots, cars, trucks, light posts, construction cranes, aircraft, etc.</p><p id="p-0050" num="0049">For vehicles such as automobiles and trucks, there are mounting locations that may be preferred or even ideal for aesthetic design, for manufacturing, and/or for providing optimal vantage points for sensors or cameras of a stereo vision system; however, these mounting locations may not have sufficient rigidity or stability to support a strict angular tolerance of cameras used in conventional stereo vision systems. Perturbing a relative pointing direction of stereo cameras by only 0.05 degrees of some conventional stereo vision systems can result in depth maps that may be ruined because they provide unreliable range estimates.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>C</figref> show three different locations for mounting cameras in or on a vehicle without using or requiring large and heavy structural support material between the cameras, according to some embodiments of the present technology.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> shows a mounting configuration of a stereo vision system according to some embodiments of the present technology, in which a left camera <b>100</b> and a right camera <b>102</b> of the system may be mounted at a top portion of a front windshield of a vehicle. This location may be desirable because the cameras <b>100</b>, <b>102</b> are internal to a cabin of the vehicle and therefore may be protected from dust and rain. For example, windshield wipers may be used to clear debris (e.g., rain, snow, leaves, etc.) away from a field of view of the cameras <b>100</b>, <b>102</b>. In some embodiments, the right camera <b>100</b> and the left camera <b>102</b> may be mounted asymmetrically and may be arranged to be within a cleaning zone of the vehicle's windshield wipers. In some embodiments, a typical baseline width between the right camera <b>100</b> and the left camera <b>102</b> may be approximately 80 cm, which is a width that, with conventional stereo vision systems, would require an impractically thick, heavy, rigid, structural member to connect the two cameras <b>100</b>, <b>102</b>. As will be appreciated, such a structural member could obstruct a human driver's vision. Conventional autonomous test vehicles that have stereo cameras with a baseline that is greater than 50 cm typically use bulky slotted aluminum members with 45 mm&#xd7;45 mm or 90 mm&#xd7;90 mm profiles to provide enough rigidity to maintain camera alignment over a day or sometimes longer.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> shows a mounting configuration of a stereo vision system according to some embodiments of the present technology, in which a left camera <b>104</b> and a right camera <b>106</b> of the system may be mounted on a roof portion of a vehicle. The roof portion may provide a high vantage point, which may allow the vision system to see over objects in front of the vehicle and to be far from splash and spray from wet pavement and road debris. In some embodiments, the camera <b>104</b>, <b>106</b> may be mounted onto a roof headliner without using extra structural material to stabilize the cameras <b>104</b>, <b>106</b>. Furthermore, in some embodiments, camera enclosures used with the camera <b>104</b>, <b>106</b> may contain one or more cleaning devices (e.g., heater elements configured to remove frost and/or to defog, a sprayer for liquid and/or air configured to remove solid debris, etc.). In some embodiments, a typical baseline width between the left camera <b>104</b> and right camera <b>106</b> may be 1.25 m. In some implementations, the cameras modules <b>104</b>, <b>106</b> on the roof portion of the vehicle may have little or no effect on the vehicle's styling. For example, in some embodiments, the cameras <b>104</b>, <b>106</b> may be placed seamlessly in line with a roof rack of the vehicle.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>1</b>C</figref> shows a mounting configuration of a stereo vision system according to some embodiments of the present technology, in which a left camera <b>108</b> and right camera <b>110</b> of the system may be mounted on side-view mirrors of a vehicle. The side-view mirrors may offer a widest baseline, typically 2.1 m, for a forward-looking stereo vision system, but may experience more perturbations than the mounting configurations shown in <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>. For typical vehicles, the side-view mirrors are at the extremities of the vehicle. Typical side-view mirrors are mounted to an articulated joint and are light structures that are sensitive to internal vibration sources (e.g., the vehicle's engine) as well as external perturbations (e.g., from wind, road vibration, etc.). Cameras mounted on side-view mirrors generally experience fast perturbations as well as slow perturbations.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>C</figref> show translucent or partially transparent views of the vehicles of <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>C</figref>, respectively, according to some embodiments of the present technology. In some embodiments, wires <b>200</b>, <b>202</b>, <b>204</b>, <b>206</b>, <b>208</b>, <b>210</b> may connect the cameras <b>100</b>, <b>102</b>, <b>104</b>, <b>106</b>, <b>108</b>, <b>110</b> to electronic control units <b>212</b>, <b>214</b>, <b>216</b>. The wires <b>200</b>, <b>202</b>, <b>204</b>, <b>206</b>, <b>208</b>, <b>210</b> may be configured to transmit data to the electronic control units <b>212</b>, <b>214</b>, <b>216</b> and supplying power to the cameras <b>100</b>, <b>102</b>, <b>104</b>, <b>106</b>, <b>108</b>, <b>110</b>.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> through <figref idref="DRAWINGS">FIG. <b>12</b></figref> show various embodiments of stereo vision systems on different platforms that may move or flex, that may be subject to vibrations and shocks, and that may be inconvenient or costly to calibrate manually, and/or that may need to operate for years without manual calibration and maintenance. In <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> through <figref idref="DRAWINGS">FIG. <b>12</b></figref>, cameras of the systems may be shown mounted on the platforms but electronic control units and wiring may not be shown as they may be mounted inside a chassis of the platforms or inside the platforms themselves, according to some embodiments of the present technology.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>F</figref> show camera modules C<b>1</b>, C<b>2</b> for a forward-looking stereo vision system mounted in or on a car at various locations, according to some embodiments of the present technology. These locations typically do not provide optical rigidity to ensure alignment of stereo vision cameras over the car's lifetime nor even over temperature ranges experienced by typical cars during, e.g., seasonal changes and/or temperature swings from nighttime to daytime. <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> shows the cameras C<b>1</b>, C<b>2</b> mounted on side-view mirrors. <figref idref="DRAWINGS">FIG. <b>3</b>B</figref> shows the cameras C<b>1</b>, C<b>2</b> mounted on a roof. <figref idref="DRAWINGS">FIG. <b>3</b>C</figref> shows the cameras C<b>1</b>, C<b>2</b> mounted behind the car's windshield glass. <figref idref="DRAWINGS">FIG. <b>3</b>D</figref> shows the cameras C<b>1</b>, C<b>2</b> mounted in the car's grille. <figref idref="DRAWINGS">FIG. <b>3</b>E</figref> shows the cameras C<b>1</b>, C<b>2</b> mounted in the car's headlight housings. <figref idref="DRAWINGS">FIG. <b>3</b>F</figref> shows the cameras C<b>1</b>, C<b>2</b> mounted in the car's fog-light housings.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts camera modules C<b>1</b>, C<b>2</b> of a forward-looking stereo vision system according to some embodiments of the present technology, with the camera modules C<b>1</b>, C<b>2</b> mounted on side-view mirrors of a truck. The truck may have a wide baseline length of approximately 3 m from a left side-view to a right side-view mirror, which may enable very long-range stereopsis. In some embodiments, the camera modules C<b>1</b>, C<b>2</b> may be comprised of CMOS sensors.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>C</figref> show an arrangement for a side- and back-looking stereo vision system, according to some embodiments of the present technology, with camera modules <b>500</b>, <b>502</b> shown to be mounted on a right side-view mirror of a truck in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>. <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is an isometric or perspective view, <figref idref="DRAWINGS">FIG. <b>5</b>B</figref> is a top or plan view, and <figref idref="DRAWINGS">FIG. <b>5</b>C</figref> is a front elevational view of the truck. In some embodiments, a duplicate stereo vision system may be also mounted on a left side-view mirror of the truck, to sense objects on the truck's left side. The duplicate stereo vision system is not shown in the drawings, to avoid unnecessary clutter. The two camera modules <b>500</b>, <b>502</b> of the stereo vision system may be mounted vertically and may be spaced apart by approximately 1 m. As will be appreciated, stereo matching may be performed to match images from cameras oriented any angle relative to each other. For example, stereo matching of images from cameras arranged vertically relative to each other may correlate horizontal lines or rows of pixels of the images, stereo matching of images from cameras arranged horizontally relative to each other may correlate vertical lines or columns of pixels of the images, etc. A depth range of a stereo vision system may be proportional to a distance between cameras of the system. For the system shown in <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>C</figref>, the system may have a horizontal field of view <b>504</b> of 135 degrees, as depicted in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, and may have a vertical field of view <b>506</b> of 90 degrees, as depicted in <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>. As will be appreciated, in other embodiments of the present technology other fields of view are possible via, e.g., different camera lenses (e.g., standard lenses, different types of wide-angle lenses, etc.). A side- and back-looking stereo vision configuration, such as that of <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>C</figref>, may be particularly useful for sensing and thus avoiding pedestrians, sensing and thus avoiding cyclists, and sensing vehicle cut throughs. Conventional stereo vision systems typically have very poor depth ranges, especially wide field-of-view systems, because in such systems the ability to estimate angles accurately degrades as each pixel is mapped to a larger range of angles. As will be appreciated, a depth-range resolution of a stereo vision system may be proportional to the system's field-of-view. For a wide-baseline stereo vision system it may be advantageous to use wide field-of-view cameras because the system's depth-range resolution may be inversely proportional to a baseline length of the system's stereo cameras. Therefore, embodiments of the present technology may enable long-range, surround-view three-dimensional (&#x201c;3D&#x201d;) sensing of objects by enabling long stereo-vision baselines (e.g., <b>1</b> m or greater) in comparison with only about 5-cm to 10-cm baselines in conventional stereo imaging systems.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows two cameras <b>600</b>, <b>606</b> of a stereo vision system mounted on a traffic light pole <b>602</b>, according to some embodiments of the present technology. The two cameras <b>600</b>, <b>606</b> may be separated by a long baseline limited by a length of a boom <b>602</b><i>a </i>of the traffic light pole <b>602</b>. Some traffic light poles may support stereo-vision baseline lengths of approximately 5 m, which may allow for extremely long-range depth estimation (e.g., in excess of 300 m), which may enable measurement of a queue length of vehicles in a traffic queue for intelligent switching of traffic lights <b>604</b>. As will be appreciated, booms of traffic light poles may flex (e.g., with high winds, movement of objects dangling from the booms, road vibrations from passage of large trucks nearby, etc.). Such flexing may be experienced by the cameras <b>600</b>, <b>606</b>, and such flexing may be magnified by long poles <b>608</b> holding the cameras <b>600</b>, <b>606</b>. Thus, a long baseline length may cause the two cameras <b>600</b>, <b>606</b> to misalign quickly in a typical operating environment of the traffic light pole <b>602</b>.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows two cameras <b>702</b>, <b>704</b> of a stereo vision system mounted on a lamp post <b>700</b>, according to some embodiments of the present technology. The two cameras <b>702</b>, <b>704</b> may be mounted vertically and may be pointed slightly downward to view a region of interest (e.g., a pedestrian crosswalk). For example, an upper edge <b>706</b> of the field of view of the camera <b>702</b> may be deviated from a horizontal line by less than about 30 degrees (e.g., 20 degrees, 10 degrees, 5 degrees). A baseline distance between the two cameras <b>702</b>, <b>704</b> may be several meters or more (e.g., 3 m, 3.5 m, 4 m, 4.5 m, etc.). A similar vertical configuration of cameras may be mounted on sign poles, telephone/utility poles, and the like.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows cameras <b>804</b>, <b>806</b> of a stereo vision system mounted on a construction crane <b>800</b>, according to some embodiments of the present technology. The cameras <b>804</b>, <b>806</b> may be mounted on a trolley <b>802</b> and arranged to have a downward field of view to, e.g., monitor a payload <b>808</b> and a surrounding construction environment nearby and underneath the payload <b>808</b>. A typical field of view <b>810</b> may be about 90 degrees to about 120 degrees and may extend in the direction parallel or nearly parallel to a boom of the crane <b>800</b>. A distance between the cameras <b>804</b>, <b>806</b> may be about 2 m to 5 m, depending on the desired minimum depth range and depth-range resolution. A boom length of approximately 50 m and a crane height of approximately 200 m may make a wide-baseline, long-range stereo vision system an attractive solution for monitoring an environment of construction cranes, especially when construction shocks and vibrations may make it difficult to maintain a constant a relative position between the cameras <b>804</b>, <b>806</b>.</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows two cameras <b>902</b>, <b>904</b> of a stereo vision system mounted on a fixed-wing aircraft <b>900</b>, according to some embodiments of the present technology. The aircraft <b>900</b> may be, e.g., a crop duster, an unmanned aerial vehicle (UAV), a quad- or hexa-copter, an airplane, and the like. In some embodiments, the cameras <b>902</b>, <b>904</b> may be mounted on an underside of a wing of the aircraft <b>900</b> and pointed downward to view a ground region <b>906</b>. In some embodiments, the two cameras <b>902</b>, <b>904</b> may be pointed in a forward direction or in a backward direction. Because the wings may flex or move during operation of the aircraft <b>900</b>, the cameras <b>902</b>, <b>904</b> may oscillate in position back and forth with respect to each other and stereopsis may not be possible using regular stereo vision techniques.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIGS. <b>10</b>A and <b>10</b>B</figref> show a front perspective view and a rear perspective view, respectively, of camera arrangements of a stereo vision system mounted on an off-road vehicle <b>1000</b>, according to some embodiments of the present technology. The stereo vision system may have a surround-view configuration comprised of four stereo-vision camera pairs: a front stereo-vision camera pair <b>1002</b>, a left stereo-vision camera pair <b>1004</b>, a back stereo-vision camera pair <b>1006</b>, and a right stereo-vision camera pair <b>1008</b>. The four stereo-vision camera pairs <b>1002</b>, <b>1004</b>, <b>106</b>, <b>1008</b> may provide four horizontal fields of view of at least 90 degrees, one in each of the front, back, left, and right directions. As will be appreciated, off-road vehicles encounter extreme amounts of vibration and shock when driving on rugged terrain, which would cause conventional stereo-vision cameras to misalign quickly. Examples of off-road vehicles include, e.g., military vehicles, jeeps, all-terrain vehicles, dump trucks, mining vehicles, and any type of vehicle that may be driven on unpaved surfaces such as gravel, rocky terrain, cobblestones, excavation sites, etc. High-resolution 3D information about a surrounding environment of an off-road vehicle may be critical for navigating uneven surfaces that may be full of rocks, boulders, tree branches, etc., and therefore a reliable system for obtaining such information is of high importance.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows two stereo vision systems mounted on a combine harvester <b>1100</b>, according to some embodiments of the present technology. A forward-facing stereo vision system may include a forward camera <b>1102</b> and a forward camera <b>1104</b>, which may be used, e.g., to obtain information for measuring an amount of crop ahead to be encountered the harvester <b>1100</b>, to enable an informed control a throttle of the harvester <b>1110</b> and therefore maximize a feed rate. The forward cameras <b>1102</b>, <b>1104</b> may, e.g., be mounted on a headliner <b>1116</b> of the harvester <b>1100</b>. A spout-mounted stereo vision system may include a spout camera <b>1108</b> and a spout camera <b>1110</b>, which may be used, e.g., to control a position of a spout <b>1112</b> and obtain information for measuring an amount of material in a collection container. Both the spout <b>1112</b> and the headliner <b>1116</b> of the harvester <b>1100</b> may experience extreme amounts of vibration and shock when the harvester <b>1100</b> moves over farmland, plants, and vegetation. Furthermore, as will be appreciated, the headliner <b>1116</b> and the spout <b>1112</b> may not be structures that are &#x201c;optically rigid&#x201d; such that relative movement of the front cameras <b>1102</b>, <b>1104</b> and relative movement of the spout cameras <b>1108</b>, <b>1110</b> may be avoided. That is, it may not be possible to maintain a constant alignment of the pairs of cameras over an agricultural season much less an entire lifetime of the harvester <b>1100</b>. Camera-alignment issues relevant to the harvester <b>1100</b> may also be issues in other farm equipment (e.g., tractors, hay balers, earth-mover vehicles, and the like).</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIGS. <b>12</b>A, <b>12</b>B, and <b>12</b>C</figref> show a left elevational view, a front elevational view, and a plan view of cameras <b>1202</b>, <b>1204</b> of a forward-looking stereo vision system mounted on an automated forklift <b>1200</b>, according to some embodiments of the present technology. In some embodiments, the cameras <b>1202</b>, <b>1204</b> may provide a wide field of view of 135 degrees, for example, and may enable collision avoidance and object localization, which are desirable features when moving large objects that may obstruct an operator's view. Wide baselines of over 1 m in width are possible on typical forklift platforms. Forklift systems typically require a depth range of about 30 m, which is well beyond the capabilities of conventional stereo-vision solutions. Forklifts also may be subject to extremely high shocks during operation, which is highly desirable if not essential for any stereo vision system used with forklifts to have shock-resistant cameras and as well as shock-resistant electronics (e.g., sensors) that can maintain their calibration. Shock resistance is desirable not only for forklifts but also for automated mobile vehicles used in, e.g., warehouses and factories (e.g., smart automatic guided vehicles).</p><p id="p-0067" num="0066">As can be appreciated from the discussions above regarding <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> through <figref idref="DRAWINGS">FIG. <b>12</b></figref>, stereo vision systems that may be mounted on a non-rigid platform and that may be calibrated automatically to adjust slow alignment perturbations and/or fast alignment perturbations may be desirable for many different situations.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows a block diagram of a stereo vision system, according to some embodiments of the present technology. A processing component <b>1310</b> may be included as part of each of an electronic control unit (e.g., the electronic control units <b>212</b>, <b>214</b>, <b>216</b>). A first camera <b>1300</b> (also referred to as &#x201c;Camera 1&#x201d;) and a second camera <b>1302</b> (also referred to as &#x201c;Camera 2&#x201d;) of the stereo vision system may transmit signals or data of raw images <b>1304</b> and <b>1306</b> to the processing component <b>1310</b>. In some embodiments, the cameras <b>1300</b>, <b>1302</b> may be any of: monochrome CMOS cameras, color CMOS cameras, near-infrared cameras, short-wave infrared cameras, mid-wave infrared cameras, long-wave infrared cameras, and may provide the raw images <b>1304</b> and <b>1306</b> to the processing component <b>1310</b> in real time or nearly real time. In some embodiments, the cameras <b>1300</b>, <b>1302</b> may have memory and/or may be associated with a data storage device, and the cameras <b>1300</b>, <b>1302</b> may to provide a sequence of images (e.g., a sequence of the raw images <b>1304</b> and <b>1306</b>) replayed from the memory and/or the data storage device to the processing component <b>1310</b>. In some embodiments, the images may be two dimensional (e.g., height and width). In some embodiments, the images may be three dimensional (e.g., height and width and color). The processing component <b>1310</b> may be configured to be commanded by a main system controller <b>1316</b> through a command and control line <b>1312</b>. As will be appreciated, the command and control line <b>1312</b> may be a wired communication mechanism (e.g., a data bus, a communication line) or may be a wireless communication mechanism using communication techniques known in the art. In some embodiments, the main system controller <b>1318</b> may be comprised of a computer configured to orchestrate high-level functions (e.g., automatic emergency braking for automobiles), and to communicate with various sub-systems (e.g., a braking system, a sensor system, and the like). In some embodiments, common communication protocols may be used for communication via the command and control line <b>1312</b> (e.g., Ethernet, CAN (Controller Area Network), I2C (Inter-Integrated Circuit), etc.). In some embodiments, the processing component <b>1310</b> may report on a calibration status (e.g., system health status, information quality status, etc.) to the main system controller <b>1318</b> through the command and control line <b>1312</b>. In some embodiments, the command and control line <b>1312</b> may be used to configure autocalibration and stereo correspondence settings of the processing component <b>1310</b>. The command and control line <b>1312</b> may also be used to start, stop, and record both input and output data streams within the processing component <b>1310</b>. A depth maps <b>1308</b> and a corresponding confidence map <b>1314</b> may be output from the processing component <b>1310</b> for each pair of the raw images <b>1304</b> and <b>1306</b>. In some embodiments, the depth map <b>1308</b> may be a two-dimensional matrix with the same width and height as the raw image <b>1304</b>, but whose value indicates a range (e.g., a distance) to a target (e.g., an object in the field of view). The depth map <b>1308</b> may be reported or output as an RGB image, where each 24-bit color value may be encoded to be a range to the target. For example, if values of the depth map <b>1308</b> has units of mm (millimeters), distances from 0 to 16,777.215 m (meters) may be represented with 24 bits. In some embodiments, the confidence map <b>1314</b> may be a two-dimensional matrix with the same height and width as the raw image <b>1304</b>, and each element of the confidence map <b>1314</b> may represent a confidence value or confidence level for a depth estimate of each pixel of the raw image <b>1304</b>. In some embodiments, a confidence value may be an 8-bit unsigned value from 0 to 255, where relatively higher values may indicate a higher level of confidence in a corresponding depth estimate. In some embodiments, using an 8-bit representation for the confidence map <b>1314</b> may enable the confidence map <b>1314</b> to be displayed conveniently as a grayscale image, and also may permit data for the confidence map <b>1314</b> to be transmitted as a monochrome video data stream. Information provided by the confidence map <b>1314</b> may be important for sensor fusion processing pipelines, which function to combine estimates from various sensors (e.g., radar, lidar, sonar, etc.) to provide high-reliability estimates of a surrounding environment.</p><p id="p-0069" num="0068">An illuminator <b>1316</b> may be used to project light for nighttime operation, according to some embodiments of the present technology. The illuminator <b>1316</b> may be comprised of one or more illumination elements (e.g., lamps, LEDs, etc.). In some embodiments, the stereo vision system may be provided on a car, and the illuminator <b>1316</b> may be comprised of the car's headlights, which may emit visible or infrared radiation. In some embodiments, the illuminator <b>1316</b> may be comprised of externally mounted lamps configured to emit visible or infrared radiation. In some embodiments, the illuminator <b>1316</b> may be configured to emit visible or infrared radiation in synchronization with an exposure interval of a first camera sensor and an exposure interval of a second camera sensor. With a modulated waveform, a peak optical power may be higher for the same average optical power, and therefore the number of photons registered by the camera sensors may increase without blinding oncoming drivers or pedestrians, who may be sensitive to the average optical power. In some embodiments, the illuminator <b>1316</b> may be comprised of a vertical cavity surface emitting laser (VCSEL) array, which may be configured to project a pseudo-random dot pattern. Use of a randomized dot pattern may add texture to a scene being illuminated, which may improve the ability of a stereo correspondence engine <b>1420</b> to generate accurate depth maps.</p><p id="p-0070" num="0069">In some embodiments of the present technology, the stereo vision system of <figref idref="DRAWINGS">FIG. <b>13</b></figref> may be modified to be a three-camera system. A first camera and a second camera may form a stereo vision system for short-range sensing, and the first camera and a third camera may form a stereo vision system for long-range sensing. A baseline distance between the first and second cameras may be shorter than a baseline distance between the first and third cameras. The stereo vision system comprised of the first and second cameras may have a range with a lower minimum distance but may have a higher range resolution compared to the stereo vision system comprised of the first and third cameras.</p><p id="p-0071" num="0070">In some embodiments of the present technology, the stereo vision system of <figref idref="DRAWINGS">FIG. <b>13</b></figref> may be modified to be a four-camera system comprised of two stereo vision systems: a short-range system with a first camera and a second camera, and a long-range system with a third camera and a fourth camera. In comparison with the three-camera system, the additional (fourth) camera may provide an additional degree of operational freedom and may allow the short-range system to have a different (e.g., wider) field of view than the long-range system. In some embodiments, the first and second cameras may have a shorter baseline and a wider field of view than the third and fourth cameras.</p><p id="p-0072" num="0071">In some embodiments of the present technology, the stereo vision system of <figref idref="DRAWINGS">FIG. <b>13</b></figref> may be modified to include a time-of-flight camera, which may be configured to measure objects that are closer to the cameras <b>1300</b>, <b>1302</b> than a minimum distance of the stereo vision system.</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. <b>14</b></figref> shows a block diagram of the processing component <b>1310</b>, according to some embodiments of the present technology. As discussed above, the processing component <b>1310</b> may convert the raw images <b>1304</b> and <b>1306</b> into depth maps <b>1308</b> and confidence maps <b>1314</b>. Raw images <b>1304</b> from the first camera may be rectified by a rectification engine <b>1416</b>. Raw images <b>1306</b> from the second camera may be rectified by a rectification engine <b>1418</b>. As will be appreciated, in computer-vision technology, rectification is a transformation process used to warp images so that images from Camera 1 and Camera 2 are aligned row-wise. That is, features in a row of pixels in a rectified image <b>1406</b> produced from the raw image <b>1304</b> from Camera 1 are aligned to a same row of pixels as matching features in a rectified image <b>1408</b> from the raw image <b>1306</b> of Camera 2. Such row alignment of pixels may allow the stereo correspondence engine <b>1420</b> to operate faster, because it enables a search for corresponding features to be a one-dimensional search along rows rather than a two-dimensional search along rows and columns. In some embodiments, the rectification engines <b>1416</b> and <b>1418</b> may utilize camera parameters <b>1410</b> and <b>1412</b>, respectively, to determine a mapping or warping operation. <figref idref="DRAWINGS">FIG. <b>15</b>A</figref> shows a flow diagram of a rectification algorithm that may be utilized by the rectification engines <b>1416</b>, <b>1418</b>, in some embodiments of the present technology. <figref idref="DRAWINGS">FIG. <b>15</b>B</figref> shows a coordinate system and geometric details of a rectification procedure performed by the algorithm, according to some embodiments of the present technology.</p><p id="p-0074" num="0073">According to some embodiments of the present technology, the camera parameters <b>1410</b>, <b>1412</b> may be comprised of six (6) extrinsic camera parameters, which may describe a relative position and orientation of the two cameras <b>1300</b>, <b>1302</b>, and eighteen (18) intrinsic camera parameters, which may characterize a transformation from a camera coordinate system to a pixel coordinate system of each camera <b>1300</b>, <b>1302</b>. The six extrinsic parameters may be comprised of: a relative roll, a relative pitch, a relative yaw, a relative translation x, a relative translation y, and a relative translation z. The eighteen intrinsic camera parameters for both cameras <b>1300</b>, <b>1302</b> may be comprised of, for each camera <b>1300</b>, <b>1302</b>: focal length (Fx, Fy); principal point (Cx, Cy); radial lens distortion coefficients K<b>1</b>, K<b>2</b>, and K<b>3</b>; and tangential lens distortion coefficients P<b>1</b> and P<b>2</b>. These camera parameters are well known and are described by OpenCV (Open Source Computer Vision Library), which is an open-source library that includes hundreds of computer-vision algorithms. When a wide field-of-view lens and/or a fisheye lens is used, one or more additional higher-order lens distortion parameters may need to be used. In some embodiments, the camera parameters <b>1410</b>, <b>1412</b> may not be constant but may change as a function of time, to track actual changes in the cameras <b>1300</b>, <b>1302</b>. In some embodiments, the camera parameters <b>1410</b>, <b>1412</b> may be updated for every frame.</p><p id="p-0075" num="0074">According to some embodiments of the present technology, an autocalibration engine <b>1414</b> may be configured to implement a method that maintains stereo camera calibration of the cameras <b>1300</b>, <b>1302</b>. It some embodiments, the autocalibration engine <b>1414</b> may utilize as inputs the raw images <b>1304</b>, <b>1306</b> of a current frame and the rectified images <b>1406</b>, <b>1408</b> of a previous frame. The autocalibration engine <b>1414</b> may output the camera parameters <b>1410</b> for the first camera <b>1300</b> (Camera 1) and the camera parameters <b>1412</b> for the second camera <b>1302</b> (Camera 2), and may output translation parameters <b>1426</b> for the first camera <b>1300</b> and translation parameters <b>1428</b> for the second camera <b>1302</b>.</p><p id="p-0076" num="0075">According to some embodiments of the present technology, translation engines <b>1422</b>, <b>1424</b> may perform a transformation operation to shift the rectified images <b>1406</b>, <b>1408</b> to compensate for very fast perturbations and to stabilize the rectified images <b>1406</b>, <b>1408</b> with respect to each other. The translation operation may be performed by pixel row and/or pixel column, which may be accomplished quickly using an affine transformation procedure. The translation parameters <b>1426</b>, <b>1428</b> may be row-based and column-based translation offsets. In some embodiments, to reduce computation time, one of the translation engines <b>1422</b>, <b>1424</b> may be dropped (e.g., unused or idle or absent), while the other receives the translation offsets (e.g., as a sum). Images <b>1430</b>, <b>1432</b> output by the translation engines <b>1422</b>, <b>1424</b> may be rectified and stabilized images <b>1430</b>, <b>1432</b>, which may be fed into the stereo correspondence engine <b>1420</b>. The stereo correspondence engine <b>1420</b> may determine a disparity between matching pixels in the images <b>1430</b>, <b>1432</b> (e.g., the images <b>1430</b>, <b>1432</b> may be left and right images), may compute a disparity map, may compute and output the depth map <b>1308</b> from the disparity map, and may compute and output the confidence map <b>1314</b>.</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>16</b></figref> shows a block diagram of the autocalibration engine <b>1414</b>, according to some embodiments of the present technology. The autocalibration engine <b>1414</b> may be comprised of five calibration engines: a fast calibration engine <b>1600</b>, a medium calibration engine <b>1602</b>, a slow calibration engine <b>1604</b>, an absolute range calibration engine <b>1606</b>, and a stereo image stabilization engine <b>1608</b>. In some embodiments, the fast <b>1600</b>, medium <b>1602</b>, and slow <b>1604</b> calibration engines may compensate for camera parameter perturbations that may occur over slow to fast timescales, and the methods used by these calibration engines <b>1600</b>, <b>1602</b>, <b>1604</b> may be based on optimizing a cost function. In some embodiments, the absolute range calibration engine <b>1606</b> may calibrate an absolute range to objects by adjusting an extrinsic camera parameter: relative yaw. In some embodiments, calibration of the absolute range calibration may be one of the most important and hardest parameters to calibrate well because epipolar geometry constraints may be insensitive to changes in yaw. In some embodiments, the stereo image stabilization engine <b>1608</b> may be used to track and compensate for frame-to-frame variations in pitch and yaw. A time-series history of camera parameters may be stored in a storage device <b>1622</b>. Current camera parameters <b>1620</b> may be used as a starting point for the calibration engines <b>1600</b>, <b>1602</b>, <b>1604</b>, <b>1606</b>, <b>1608</b>. In some embodiments, outputs <b>1610</b>, <b>1612</b>, <b>1614</b>, <b>1616</b>, <b>1626</b> of the calibration engines <b>1600</b>, <b>1602</b>, <b>1604</b>, <b>1606</b>, <b>1608</b> may be combined by a calibration manager <b>1624</b>, which uses this information to decide a best estimate of the camera parameters <b>1410</b>, <b>1412</b> for the first camera and second cameras <b>1300</b>, <b>1302</b>. The command and control line <b>1312</b> may be used to initialize values for the camera parameters, which may come from a manual factory calibration, manufacturing specifications, or a random guess. As discussed below, the absolute range calibration engine <b>1606</b> have different embodiments. In some embodiments, any one or any combination of a fast calibration procedure performed by the fast calibration engine <b>1600</b>, a medium calibration procedure performed by the medium calibration engine <b>1602</b>, and a slow calibration procedure performed by the slow calibration engine <b>1604</b> may include an intrinsic-parameter calibration procedure for calibrating one or more intrinsic camera parameter(s). For example, for each camera, any one or any combination of: a focal length, a principal point, at least one radial lens distortion coefficient, and at least one tangential lens distortion coefficient may be calibrated by the intrinsic-parameter calibration procedure. In some embodiments, the intrinsic-parameter calibration procedure may be performed by the fast calibration engine <b>1600</b> and/or the medium calibration engine <b>1602</b> if there is a priori knowledge about the first camera and second cameras <b>1300</b>, <b>1302</b>, for example, if it is known that a lens mount itself is flexible, which may be a rare case. In some embodiments, the intrinsic camera parameters may not need to be calibrated on the fast timescale and/or the medium timescale for a number of reasons: (1) small changes in the piercing point horizontal location (Cx) and focal length (Fx and Fy) may be corrected to first order by changes to yaw, (2) small changes in the piercing point vertical location (Cy) may be corrected to first order by changes to pitch, and (3) the lens distortion coefficients may not typically change since the lens shape may be generally immutable.</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>17</b>A</figref> shows a block diagram of the fast <b>1600</b>, medium <b>1602</b>, and slow <b>1604</b> calibration engines, according to some embodiments of the present technology. The calibration engines <b>1600</b>, <b>1602</b>, <b>1604</b> may have the same block-diagram structure but may utilize different optimization methods <b>1700</b>, <b>1722</b>, <b>1724</b>, respectively. In some embodiments, a basic concept of the methods <b>1700</b>, <b>1722</b>, <b>1724</b> is that, for each of the methods <b>1700</b>, <b>1722</b>, <b>1724</b>, optimal or correct camera parameters may be associated with a minimized cost function <b>1704</b> appropriate for the corresponding method. For example, for a nominal case of twenty four (24) camera parameters (six extrinsic parameters and eighteen intrinsic parameters), a brute-force search of parameters may require evaluating a cost function 100<sup>24 </sup>times, assuming 100 values per parameter, which take an unreasonably long time to compute using conventional computers (it would take longer than the age of the universe using conventional computers). As will be appreciated, to be practical for a real-world setting, an optimization operation should be completed in real time and implemented on a time scale that matches a time scale of a source or cause of a perturbation, or faster. Therefore, in some embodiments, it is highly desirable for the camera parameters to be searched efficiently by separating the search into different time scales.</p><p id="p-0079" num="0078">According to some embodiments of the present technology, one or more of the optimization methods <b>1700</b>, <b>1722</b>, <b>1724</b> may repeatedly evaluate the cost function <b>1704</b> for a subset of camera parameters over a bounded search range. In some embodiments, to speed up evaluation of the cost function <b>1704</b>, one or more of the optimization methods <b>1700</b>, <b>1722</b>, <b>1724</b> may dynamically tune a pyramid level <b>1703</b> of the raw images <b>1304</b>, <b>1306</b> from the first and second cameras <b>1300</b>, <b>1302</b>. The term &#x201c;pyramid&#x201d; is a common computer-vision term that is widely understood to refers to downsampling of images to smaller images. Because an amount of time to compute the cost function (i.e., determine a minimum cost function) may be proportional to a size of an image being processed, by subsampling to half resolution (or pyramid level 1), the time to compute the cost function may be reduced by a factor of four (4). For coarse parameter searches, high pyramid levels may be used to speed up searching. For fine parameter searches, low pyramid levels may be used for enhanced accuracy. In some embodiments, the respective outputs <b>1610</b>, <b>1612</b>, <b>1614</b> of the optimization methods <b>1700</b>, <b>1722</b>, <b>1724</b> may be the camera parameters that minimize the cost function <b>1704</b> of the corresponding optimization method. As will be appreciated, minimizing a cost function may also be considered maximizing a figure of merit, as they may be considered equivalent approaches.</p><p id="p-0080" num="0079">According to some embodiments of the present technology, an indicator of calibration health may be reported to the main system controller <b>1316</b> through the command and control line <b>1312</b>. In some embodiments, the calibration health may be taken to be a negative of the (minimized) cost function (or the maximized figure of merit) corresponding to optimized camera parameters.</p><p id="p-0081" num="0080">According to some embodiments of the present technology, the cost function <b>1704</b> may be comprised of a pyramid down block <b>1701</b> configured to reduces a size of the raw images <b>1304</b>, <b>1306</b>, a rectification block <b>1706</b> configured to warp and row-align reduced images according to test camera parameters <b>1702</b>, a stereo block matching block <b>1710</b>, and a block <b>1714</b> configured to compute a negative of a number of valid pixels. In some embodiments, the stereo block matching block <b>1710</b> may use a stereo block matching algorithm from OpenCV with following parameters that may include: a pixel-window size of 9&#xd7;9, a normalized response prefilter, a prefilter cap of 63, a prefilter size of 11, a speckle window size of 100, a speckle range of 64, a texture threshold of 10, a uniqueness ratio of 10, and a disparity search range of 0 to 255. Such a block matching algorithm may be chosen for its fast execution across many different types of processors. A texture threshold and a uniqueness ratio may be set relatively high (e.g., a value of 10-50) to minimize a number of wrong distance estimates and to replace the corresponding pixels with invalid pixels. In some embodiments, each of the optimization methods <b>1700</b>, <b>1722</b>, <b>1724</b> may find optimal camera parameters that minimize the number of invalid pixels after stereo block matching is performed.</p><p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. <b>17</b>B</figref> shows a flow diagram of the fast optimization method <b>1700</b> of the fast calibration engine <b>1600</b>, according to some embodiments of the present technology. The fast optimization method <b>1700</b> may tune pitch and roll extrinsic camera parameters, because these parameters may be responsible for most of short-timescale distortions. The fast optimization method <b>1700</b> may be used for the most frequent updates among the optimization methods <b>1700</b>, <b>1722</b>, <b>1724</b>. The fast optimization method <b>1700</b> may be considered fast because it searches a relatively small number of camera parameters (two) over a small range.</p><p id="p-0083" num="0082">At step <b>1726</b> of the fast optimization method <b>1700</b>, a quick search is performed for an initial guess of the camera parameters <b>1620</b> by checking costs in a chosen grid points. In some embodiments, the grid points may typically have nine values in pitch from &#x2212;0.4 degrees to 0.4 degrees and three values in roll from &#x2212;0.2 degrees to 0.2 degrees about the initial guess of the camera parameters <b>1620</b>, for a total of 27 grid points. For evaluating the cost function <b>1704</b>, a pyramid level of 1 may be used to speed up the evaluation.</p><p id="p-0084" num="0083">At step <b>1728</b> of fast optimization method <b>1700</b>, a pitch and a roll corresponding to a lowest cost (a minimum of the cost function <b>1704</b>) may be selected among the grid points.</p><p id="p-0085" num="0084">At step <b>1730</b> of fast optimization method <b>1700</b>, a COBYLA (Constrained Optimization BY Linear Approximations) algorithm for derivative-free optimization with nonlinear inequality constraints may be applied with the pitch and the roll found in step <b>1728</b>, other camera parameters of the initial guess of the camera parameters <b>1620</b>, and bounds of 0.1 degrees in both the pitch and the roll. The cost function <b>1704</b> used in step <b>1730</b> may have a pyramid level of 0. The camera parameters <b>1620</b> may be updated with the pitch and the roll corresponding to a lowest cost found by COBYLA, and updated camera parameters <b>1610</b> are returned for the fast optimization method <b>1700</b>.</p><p id="p-0086" num="0085">According to some embodiments of the present technology, the COBYLA algorithm may construct successive linear approximations of an objective function and constraints with a simplex of points and may optimize these approximations in a trust region at each step. In some embodiments, because there are only two search dimensions, the relative pitch and roll of the pair of stereo cameras, over a small search range of 0.1 degrees for pitch and 0.1 degrees for roll, optimization can be computed quickly (e.g., 100 ms on an Intel Core i7 CPU processor) and can compensate every other frame at 20 FPS, if needed. In some embodiments, a search range can be increased or decreased based on expected excursions for a given platform. For example, for suction cup mounts on a car, 0.1 degrees is large enough, whereas for loosely mounted cameras on a mountain bike ridden on a trail, 0.2 degrees might be needed.</p><p id="p-0087" num="0086">It should be noted that in some embodiments, the COBYLA search method could also be replaced by other search methods, such as the Nelder-Mead simplex search method, Broyden-Fletcher-Goldfar-Shanno (BFGS) method, Powell's method, Sequential Least Squares Programming (SLSQP) method, or Monte Carlo methods.</p><p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. <b>17</b>C-<b>1</b></figref> shows a flow diagram of the medium optimization method <b>1722</b> of the medium calibration engine <b>1602</b>, according to some embodiments of the present technology. The medium optimization method <b>1722</b> may update more parameters than the fast optimization method <b>1700</b> but update slower than the fast optimization method <b>1700</b>. In some embodiments, a main objective of the medium optimization method <b>1722</b> is to find a translation vector and then to optimize pitch and roll. <figref idref="DRAWINGS">FIG. <b>17</b>C-<b>2</b></figref> shows a flow diagram of an angle-search process of the medium optimization method of <figref idref="DRAWINGS">FIG. <b>17</b>C-<b>1</b></figref>, according to some embodiments of the present technology.</p><p id="p-0089" num="0088">At step <b>1732</b> of the medium optimization method <b>1722</b> a grid search is performed over translation vector angles, which are represented by two angles: ay and az. The angle ay may be an angle between the translation vector and the x-axis measured on the xz plane. The angle az may be an angle between the translation vector and the x-axis measured on the yz plane. For example, the search may test all of the grid of values (ay, az) from &#x2212;7 degrees to 7 degrees in 1-degree increments around an initial translation vector of the camera parameters <b>1620</b>. For each translation vector, step <b>1732</b> uses an angle-search process <b>1736</b> (see <figref idref="DRAWINGS">FIG. <b>17</b>C-<b>2</b></figref>) to look for a best tuple of pitch and roll angles that yield a lowest cost. In some embodiments of the present technology, the angle-search process <b>1736</b> may break the search process into coarse <b>1738</b>, medium <b>1740</b>, and fine <b>1742</b> angle searches.</p><p id="p-0090" num="0089">The coarse angle search <b>1738</b> may apply nine (9) COBYLA searches, centered at (pitch, yaw, and roll) of (0,0,0), (&#x2212;1,&#x2212;1,&#x2212;1), (&#x2212;1,&#x2212;1,1), (&#x2212;1,1,&#x2212;1), (1,&#x2212;1,&#x2212;1), (1,1,&#x2212;1), (1,&#x2212;1,1), (&#x2212;1,1,1), and (1,1,1) degrees from the initial pitch, yaw, and roll specified in the camera parameters <b>1620</b>.</p><p id="p-0091" num="0090">The COBYLA search may use pyramid level 3 with the cost function <b>1704</b> for a fast evaluation, and the search bounds on all angles may be [&#x2212;1,1] degrees. The pitch, yaw, and roll corresponding to the lowest cost may be used as the starting point for the medium angle search <b>1740</b>.</p><p id="p-0092" num="0091">The medium angle search <b>1740</b> may apply a COBYLA search starting from the pitch, yaw and roll returned from the coarse angle search <b>1738</b>. This COBYLA search may have angular bounds of [&#x2212;0.2, 0.2] degrees for pitch, yaw, and roll, and may use pyramid level 3 for the cost function <b>1704</b>. The pitch, yaw, and roll corresponding to the lowest cost may be used as the starting point for the fine angle search <b>1742</b>.</p><p id="p-0093" num="0092">The fine angle search <b>1742</b> may apply a COBYLA search starting from the pitch, yaw, and roll returned from the medium angle search <b>1740</b>. This COBYLA search may have angular bounds of [&#x2212;0.1, 0.1] degrees for pitch, yaw, and roll, and may use pyramid level 0 for the cost function <b>1704</b>. The pitch, yaw, and roll corresponding to the lowest cost may be returned to step <b>1732</b>.</p><p id="p-0094" num="0093">The best estimate for ay, az, pitch, yaw, and roll from step <b>1732</b> may be used as a starting point for a fine translation vector and angle search <b>1734</b>, which may apply a COBYLA optimization for (ay, az) followed by a COBYLA optimization for (pitch, yaw, roll). Both COBYLA searches may use pyramid level 0 for the cost function <b>1704</b> to maximize sensitivity to the optimization parameters. The two COBYLA searches may be repeated until little improvement is seen in the cost function. The initial camera parameters <b>1620</b> may be updated with the optimal values for ay, az, pitch, yaw, and roll, and then the updated camera parameters <b>1612</b> may be returned from the medium optimization method <b>1722</b>.</p><p id="p-0095" num="0094"><figref idref="DRAWINGS">FIGS. <b>17</b>D-<b>1</b> through <b>17</b>D-<b>4</b></figref> (collectively &#x201c;<figref idref="DRAWINGS">FIG. <b>17</b>D</figref>&#x201d;) show flow diagrams of a slow optimization method <b>1724</b>, according to some embodiments of the present technology. In some embodiments, the slow optimization method <b>1724</b> of <figref idref="DRAWINGS">FIG. <b>17</b>D</figref> may be performed by the slow calibration engine <b>1604</b> once at factory calibration and then when deemed necessary by the main system controller <b>1316</b> (e.g., after an accelerometer or other sensor reports a large shock event). The slow optimization method <b>1724</b> may optimizes the extrinsic camera parameters and the intrinsic camera parameters. The step <b>1744</b>, a global grid search may be performed over the pitch, yaw, and roll angles with a sampling interval (e.g., 0.2 degrees, 0.4 degrees, 0.7 degrees, etc.) over [&#x2212;10, 10] degrees. The remaining steps <b>1746</b> through <b>1796</b> <figref idref="DRAWINGS">FIG. <b>17</b>D</figref> may apply local search methods, such as COBYLA, to a previously best estimate of the camera parameters <b>1602</b>. As noted above, the intrinsic camera parameters may be the focal lengths (FX, FY), the piercing points (CX, CY), and the lens distortion coefficients (K<b>1</b>, K<b>2</b>, K<b>3</b>, P<b>1</b>, P<b>2</b>). The extrinsic camera parameters may be the pitch, the yaw, the roll, and the translation vector (TX, TY, TZ). In some embodiments, optimization of the pitch, the yaw, and the roll may be followed by optimization of the translation vector from step <b>1746</b> to step <b>1748</b>, from step <b>1768</b> to step <b>1770</b>, from step <b>1772</b> to step <b>1774</b>, from step <b>1784</b> to step <b>1786</b>, and from step <b>1788</b> to step <b>1790</b>, because complementary parameters (e.g., pitch and TX; roll and TY; yaw and TZ) may warp the images (e.g., the raw images <b>1304</b>, <b>1306</b>) similarly.</p><p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. <b>18</b></figref> shows a flow diagram for the stereo image stabilization engine <b>1608</b> of the autocalibration engine <b>1414</b>, according to some embodiments of the present technology. The stereo image stabilization engine <b>1608</b> may determine an amount to horizontally and vertically shift images obtained from Camera 1 and Camera 2 so that a relative displacement between these cameras is minimized. The stereo image stabilization engine <b>1608</b> may compensate for fastest perturbations (e.g., engine noise and road noise) and may update every frame. Unlike conventional monocular image stabilization, which may stabilize an image with respect to the Earth, the stereo image stabilization engine <b>1608</b> may stabilize two images with respect to each other. In some embodiments, the stereo image stabilization engine <b>1608</b> receives as input the rectified images <b>1406</b> produced from the raw images <b>1304</b> from Camera 1 and the rectified images <b>1408</b> produced from the raw images <b>1306</b> of Camera 2. The stereo image stabilization engine <b>1608</b> may output translation parameters <b>1426</b> for Camera 1 and translation parameters <b>1428</b> for Camera 2. At <b>1800</b> a forward tracking process may compute an optical flow for a sparse feature set using an iterative Lucas-Kanade method with pyramids for the rectified images <b>1406</b>, <b>1408</b>, which may be left and right rectified images, from a previous frame to a current frame. At <b>1802</b> a backward tracking process may compute an optical flow in a reverse direction, from points in the current frame to the previous frame. At <b>1804</b> a process may determine good tracks from back tracks (e.g. a good track is when the backward tracking process at <b>1802</b> reprojects a point within 1 pixel of a previous point of the track). At <b>1806</b> a process may drop the bad tracks. At <b>1808</b> a process may add remaining good points to the tracks. At <b>1810</b> a process may update an estimate of the translation parameters. In some embodiments, the translation parameters <b>1426</b> for Camera 1 and the translation parameters <b>1428</b> for Camera 2 may be amounts to shift rows and columns of images from Camera 1 and Camera 2 relative to a previous frame. In some embodiments, the translation parameters may be equal to an average relative translation between images from Camera 1 and Camera 2 from frame-to-frame with outliers removed from the average.</p><p id="p-0097" num="0096">After several frames, one or more tracks may be dropped because keypoints may move off screen (e.g., out of the field of view) or may be obstructed. At <b>1812</b> a process may find new keypoints to create new tracks. For example, every n frames, where n may typically be 5, the stereo image stabilization engine <b>1608</b> may search for new keypoints in the rectified images <b>1406</b>, <b>1408</b>. A keypoint detector (not shown) may look for good points to track using, for example, the Shi-Tomasi corner detector. At <b>1814</b> a process may translate the rectified images <b>1406</b>, <b>1408</b> using the translation parameters <b>1426</b>, <b>1428</b> to remove any relative motion between Camera 1 and Camera 2. At <b>1816</b> a process may compute a disparity map from the rectified images <b>1406</b>, <b>1408</b> using a block matching algorithm. At <b>1818</b> a process may ignore currently tracked keypoints for the rectified image <b>1406</b> of Camera 1, and at <b>1820</b> a process may find good features to track in the rectified image <b>1406</b> of Camera 1. At <b>1822</b>, for each keypoint found for the rectified image <b>1406</b> of Camera 1, a process may find a matching keypoint in the rectified image <b>1408</b> of Camera 2 using processes at <b>1824</b>, <b>1826</b>, <b>1828</b>, <b>1830</b>. At <b>1824</b> a process may be performed for creating a Hanning window for a phasecorrelation function. In OpenCV, phasecorrelation refers to a method to check the similarity of two images with equal size. Phasecorrelation is a fast implementation of template matching. At <b>1826</b> a process may be performed to find matching keypoints between Camera 1 and Camera 2. Since images <b>1406</b> and <b>1408</b> are rectified, matching keypoints lie along the same row. Adding the disparity values <b>1816</b> to the Camera 1 keypoint column values yield the expected location for the Camera 2 keypoint column values. At <b>1828</b> a process may be performed to verify the matches of <b>1826</b> by computing the phasecorrelation function for the matched keypoints. The phase correlation function is computed over image patches (typically 40&#xd7;40 pixels) centered on the pairs of matching keypoints between Camera 1 and Camera 2. The phasecorrelation function returns the corrected coordinate shift and the response (typically, a value between 0 and 1 that indicates the similarity of the two image patches). At <b>1830</b> a process is performed to record the matching keypoints, correct the keypoint coordinates according to the corrected coordinate shift from <b>1828</b>, and update the track information if the phasecorrelation response is greater than a threshold value (typically, a threshold value of 0.3).</p><p id="p-0098" num="0097">In some embodiments of the present technology, the absolute range calibration engine <b>1606</b> may have three different implementations (referred to as <b>1606</b><i>a</i>, <b>1606</b><i>b</i>, and <b>1606</b><i>c</i>) any or all of which may be incorporated in the autocalibration engine <b>1414</b>. The three implementations <b>1606</b><i>a</i>, <b>1606</b><i>b</i>, and <b>1606</b><i>c </i>may use different absolute distance references: the implementation <b>1606</b><i>a </i>may use a non-negative disparity constraint (see <figref idref="DRAWINGS">FIGS. <b>19</b>A, <b>19</b>B</figref>), the implementation <b>1606</b><i>b </i>may use a known object such as a stop sign (see <figref idref="DRAWINGS">FIGS. <b>20</b>A, <b>20</b>B</figref>), and the implementation <b>1606</b><i>c </i>may use vehicle odometry (see <figref idref="DRAWINGS">FIGS. <b>21</b>A, <b>21</b>B</figref>). In some embodiments, a relative yaw between the cameras <b>1300</b>, <b>1302</b> may have a large influence on calibration of an absolute range of the stereo vision system. The absolute range calibration engine <b>1606</b> may be used to calibrate the yaw.</p><p id="p-0099" num="0098"><figref idref="DRAWINGS">FIG. <b>19</b>A</figref> shows a flow diagram of the implementation <b>1606</b><i>a</i>, in which the absolute range calibration engine <b>1606</b> has a non-negative disparity constraint, according to some embodiments of the present technology. The implementation <b>1606</b><i>a </i>may find the yaw where principal rays of the first and second cameras <b>1300</b>, <b>1302</b> do not cross. When the principal rays cross in front of the cameras, a corresponding disparity value may be negative at ranges farther than the crossing point. By tuning the yaw until the rays do not cross, the cameras <b>1300</b>, <b>1302</b> may become parallel, a number of negative values in the disparity map <b>1712</b> may decreases (sometimes drastically) to a minimum value, and a correct absolute range may be found.</p><p id="p-0100" num="0099"><figref idref="DRAWINGS">FIG. <b>19</b>B</figref> is a chart <b>1900</b> showing how a number of pixels with a negative disparity value in a disparity map (e.g., the disparity map computed from raw images <b>1304</b>, <b>1306</b> after rectification using the initial camera parameters <b>1620</b> but with various values of yaw) can vary as a function of yaw, according to some embodiments of the present technology. For yaw values less than approximately 1.8 degrees in the chart <b>1900</b>, the number of negative disparity values greatly increases, indicating that the principal rays of the cameras <b>1300</b>, <b>1302</b> may start to cross at a long range and then at increasingly closer range. For yaw values less than 1.65 degrees, the number of negative disparity values decreases. This may be because a block matcher used in this analysis may only search disparity values down to &#x2212;32 pixels. Any pixel with disparity values of &#x2212;33 pixels or lower may not be counted. For yaw values greater than 1.85 degrees, the number of pixels with negative values is small but not zero. This may be because mismatching pixels sometimes may have negative disparity values. A desired yaw is in a transition region where the chart <b>1900</b> transitions from a region where the number of pixels is small and generally constant (yaw &#x3e;1.85 degrees in the chart <b>1900</b>) and includes a region where the number of pixels is rapidly increasing (e.g., a region indicated by an upward pointing arrow <b>1912</b>). The chart <b>1900</b> also shows a region in which disparity values of pixels may be more negative than a minimum disparity limit (e.g., a region indicated by a downward pointing arrow <b>1914</b>). Returning to the flow diagram of the implementation <b>1606</b><i>a </i>shown in <figref idref="DRAWINGS">FIG. <b>19</b>A</figref>, at <b>1920</b> a coarse search is performed from &#x2212;0.5 degrees (see <b>1902</b>) to 1.0 degrees (see <b>1906</b>) around an initial guess <b>1904</b> for the yaw for which the disparity map is computed, and the number of negative disparity values is tallied. The search may be performed in 0.1-degree steps. A result of the coarse search may be an array of sixteen (16) values. The chart <b>1900</b> is an example of a plot of sixteen datapoints for a coarse curve. A derivative of the coarse curve with respect to yaw may be computed. The derivative of the coarse curve may be sensitive to changes and therefore may be useful for finding transitions in the coarse curve. At <b>1922</b> a quality of the coarse search is checked by making sure that the transition region (e.g., the region from 1.65 deg to 1.8 deg in the chart <b>1900</b>) is greater than five times a tail noise <b>1908</b>. The tail noise <b>1908</b> may be defined to be a standard deviation of the last 5 points of the derivative of the coarse curve. In other words, the transition region should have a significant change in the number of negative disparity values compared to a region where the number of negative disparities should be zero. Next, at <b>1924</b> a fine search range is determined by searching around the yaw corresponding to the most negative slope (e.g., a yaw of &#x2dc;1.7 degrees in the chart <b>1900</b>). Starting from the largest coarse yaw value and searching toward lower coarse yaw values, an upper bound is determined to be a value where the derivative of the coarse curve is greater than &#x2212;3 times the tail noise. Starting from the yaw corresponding to the most negative slope and increasing to the largest yaw, the lower bound is determined to be a value where the derivative of the coarse curve is less than &#x2212;3 times the tail noise. Next, at <b>1926</b> a fine search grid is created from the lower to upper bounds, and a fine search is performed. This grid may typically have twenty-one (21) points. At <b>1928</b> a yaw value corresponding to a minimum number of negative disparity pixels is determined. In some embodiments, a transition point in the curve may be estimated in this manner.</p><p id="p-0101" num="0100"><figref idref="DRAWINGS">FIG. <b>20</b>A</figref> shows a flow diagram of the implementation <b>1606</b><i>b</i>, in which the absolute range calibration engine <b>1606</b> uses an object with known dimensions, according to some embodiments of the present technology. <figref idref="DRAWINGS">FIG. <b>20</b>B</figref> illustrates an example of imaging optics for imaging an object <b>2006</b> with known dimensions, according to some embodiments of the present technology. In some embodiments, because the dimensions of the object <b>2006</b> are known and because focal lengths of camera lenses of the cameras <b>1300</b>, <b>1302</b> are known, the range or distance to the object (target) <b>2006</b> may be determined by an equation <b>2020</b>. More specifically, according to an equation <b>2020</b>, a ratio of a width W of the object (see <b>2012</b>) to a range R to the object (see <b>2014</b>) is equal to a ratio of a focal length F of a camera lens <b>2008</b> (see <b>2016</b>) to a width H of an image (see <b>2018</b>) sensed by a camera sensor <b>2010</b> (e.g., a CMOS sensor). In some embodiments, a known object can be recognized in an automated fashion using known detection technology (e.g., a traffic sign recognition system, license plate detector, an object detector, etc.). In some embodiments, a known object may be specified manually by a manual input of a bounding box and a distance to the known object. An example of an object detector for detecting stop signs is shown in <figref idref="DRAWINGS">FIG. <b>24</b></figref>.</p><p id="p-0102" num="0101">Returning to the flow diagram of the implementation <b>1606</b><i>b </i>shown in <figref idref="DRAWINGS">FIG. <b>20</b>A</figref>, at <b>2000</b> the absolute range calibration engine <b>1600</b> finds the yaw for which the mean distance to the object equals the known distance. In some embodiments of the present technology, the mean distance may be computed over a region of interest of the object. At <b>2002</b> a good starting interval for a root finding algorithm is found by first initializing an interval to &#xb1;0.1 degrees of the yaw of the initial camera parameters <b>1620</b>, and then modifying left and right ends by 0.1 degrees until a mean depth over the region of interest is positive at the left end of the interval and negative at the right end of the interval. At <b>2004</b> the root finding algorithm is run. In some embodiments, the root finding algorithm may be Brent's method, in which with the search interval found at <b>2002</b> is used to determine a best estimate of yaw. Subsequently, the initial camera parameters <b>1620</b> are returned with an updated yaw value <b>1616</b>.</p><p id="p-0103" num="0102"><figref idref="DRAWINGS">FIG. <b>21</b>A</figref> shows a flow diagram of the implementation <b>1606</b><i>c</i>, in which the absolute range calibration engine <b>1606</b> utilizes vehicle odometry <b>1628</b>, according to some embodiments of the present technology. <figref idref="DRAWINGS">FIG. <b>21</b>B</figref> is a chart <b>2108</b> showing how disparity can vary as a function of range, according to some embodiments of the present technology. Odometry (e.g., speed, distance, heading, etc.) may be information already available from in-car (&#x201c;on-board&#x201d;) sensors, and may be used to help calculate the yaw of the stereo vision system. In some embodiments, odometry may be available from on-board radar and/or lidar systems. In the implementation <b>1606</b><i>c</i>, at <b>2100</b>, for each track <b>1626</b> from the stereo image stabilization engine <b>1608</b>, a change in range, &#x394;R<sub>ODO</sub>, is computer from odometry data from a time t1 to time a t2. A corresponding change in range, AR, is measured by the stereo vision system and might not match &#x394;R<sub>ODO </sub>because of miscalibration. At <b>2102</b> a disparity offset, d<sub>offset</sub>, is computed using a function expressed by: d=fB/R, where f is the focal length of the lens and B is the baseline width between the first and second cameras <b>1300</b>, <b>1302</b>, so that a corrected range &#x394;R&#x2032; equals &#x394;R<sub>ODO</sub>. Tracks of stationary objects may give the same or similar values for d<sub>offset </sub>but moving objects may not. At <b>2104</b> outlier values for d<sub>offset </sub>are removed and remaining values are averaged. At <b>2104</b> yaw is calibrated to be equal to the initial yaw plus the average disparity offset divided by the focal length of the cameras. Subsequently, the initial camera parameters <b>1620</b> are returned with an updated yaw value <b>1616</b>.</p><p id="p-0104" num="0103"><figref idref="DRAWINGS">FIG. <b>22</b></figref> shows a flow diagram of a procedure performed by the calibration manager <b>1624</b>, according to some embodiments of the present technology. At <b>2200</b> camera parameters are initialized the command and control interface <b>1312</b> configures the calibration engines <b>1600</b>, <b>1602</b>, <b>1604</b>, <b>1606</b>, <b>1608</b>. The initial camera parameters may be extrinsic and intrinsic camera parameters obtained from blueprints, CAD drawings, manufacturer's datasheets, etc., or may be obtained from manual calibration procedures. At <b>2202</b> the calibration manager <b>1624</b> waits for outputs from the calibration engines <b>1600</b>, <b>1602</b>, <b>1604</b>, <b>1606</b>, <b>1608</b>. Once an output of an estimate of a camera parameter is received, at <b>2204</b> the estimate is used in a tracking filter (e.g., a Kalman filter) that uses a series of measurements over time and produces a new estimate of the camera parameter, which may be more accurate than an estimate on a single measurement alone. This may be done for each of a plurality of camera parameters. At <b>2206</b>, new estimates for the camera parameters may be saved in the storage device <b>1622</b> and the updated camera parameters <b>1410</b>, <b>1412</b> for Camera 1 and Camera 2 may be output from the calibration manager <b>1624</b>.</p><p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. <b>23</b></figref> shows a time series input diagram of the calibration manager <b>1624</b>, according to some embodiments of the present technology. The time axis is shown for different frame numbers, which may correspond to different frame rates (e.g., 10, 30, or 60 FPS). Upward arrows indicate the initialization or start of a calibration method <b>1600</b>, <b>1602</b>, <b>1604</b>, <b>1606</b>, <b>1608</b> by the calibration manager <b>1624</b>, and the downward arrows indicate the completion of the calibration method, where optimal parameters are returned to the calibration manager <b>1624</b>. The execution times for calibration methods in <figref idref="DRAWINGS">FIG. <b>23</b></figref> is an example of one embodiment.</p><p id="p-0106" num="0105"><figref idref="DRAWINGS">FIG. <b>24</b></figref> shows a flow diagram of a procedure that may be performed by a stop sign detector <b>2400</b> that may be used for absolute range calibration, according to some embodiments of the present technology. Input to the detector <b>2400</b> may be a color image <b>2402</b>. For example, the detector <b>2400</b> may look for an arrangement of red connected components in the letters &#x201c;STOP.&#x201d; These components may include a red region inside an oval (e.g., the letter &#x201c;0&#x201d; <b>2406</b>), a red region inside a particular shape (e.g., the letter &#x201c;P&#x201d; <b>2408</b>), and a red octagonal region <b>2404</b>. The relative locations and sizes of the connected components may give a unique signature, which may be detected quickly with few computational resources.</p><p id="p-0107" num="0106"><figref idref="DRAWINGS">FIG. <b>25</b></figref> shows a flow diagram of a procedure performed by the stereo correspondence engine <b>1420</b>, according to some embodiments of the present technology. Input to the stereo correspondence engine <b>1420</b> may be the rectified and stabilized images <b>1430</b>, <b>1432</b> from Camera 1 and Camera 2. The stereo correspondence engine <b>1420</b> may output depth maps <b>1308</b> and confidence maps <b>1314</b>. In some embodiments, a matching window <b>2500</b> in the rectified and stabilized image <b>1430</b> may be compared to a corresponding row <b>2502</b> in the rectified and stabilized image <b>1432</b>. For each pixel, a resulting matching value <b>2506</b> may be a negative value of a sum of absolute differences between the matching window and each offset along the corresponding row <b>2502</b>. The matching value for each offset may be computed, where a best match <b>2510</b> may correspond to a highest matching value and where a second-best match <b>2512</b> may correspond to a second-highest matching value. A uniqueness ratio for a given pixel may be a ratio determined as the best match <b>2510</b> divided by the second-best match <b>2512</b>. A higher confidence corresponds to a higher uniqueness ratio. The depth map <b>1308</b> may be derived from the best match <b>2510</b>. The confidence map may be derived from the ratio of best match <b>2510</b> to second-best match <b>2512</b>.</p><p id="p-0108" num="0107">According to some embodiments of the present technology, the processing component <b>1310</b> may implemented in hardware (e.g., a computer processor programmed to perform the procedures and methods described above). According to some embodiments of the present technology, the processing component <b>1310</b> may be implemented in software (e.g., computer-executable code), which may be stored on a non-transitory computer-readable storage medium or on a plurality of non-transitory computer-readable storage media, and which may be accessed and executed by a computer processor. According to some embodiments of the present technology, the processing component <b>1310</b> may be implemented in a combination of hardware and software. In some embodiments, aspects of the processing component <b>1310</b> may be implemented as one or more software modules. For example, one of more the engines of the processing component <b>1310</b> may be implemented as software module(s) stored on a non-transitory computer-readable storage medium.</p><p id="p-0109" num="0108">A stereo imaging system according to the technology described herein may be embodied in different configurations. Example configurations include combinations of configurations (1) through (27), as follows:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0109">(1) A stereo vision system, comprising: a first camera sensor configured to sense first reflected energy of a first image and to generate first sensor signals based on the first reflected energy; a second camera sensor configured to sense second reflected energy of a second image and generate second sensor signals based on the second reflected energy; at least one processor configured to receive the first sensor signals from the first camera sensor and the second sensor signals from the second camera sensor, and to produce three-dimensional (3D) data from the first and second sensor signals, wherein the at least one processor is configured to: generate rectified stereo images from the first and second sensor signals utilizing stereo calibration parameters, perform a stereo matching on the rectified images, and perform an automatic system calibration using data from a plurality of stereo images obtained by the first and second camera sensors, wherein the automatic system calibration is based on minimizing a cost function.</li>        <li id="ul0002-0002" num="0110">(2) The stereo vision system of configuration (1), wherein the automatic system calibration is comprised of a fast calibration procedure configured to generate first stereo calibration parameters to perform one or both of:        <ul id="ul0003" list-style="none">            <li id="ul0003-0001" num="0111">to compensate for high-frequency perturbations based on tracking image points corresponding to features in the first and second images, and</li>            <li id="ul0003-0002" num="0112">to calibrate, for each camera, any one or any combination of: a focal length, a principal point, at least one radial lens distortion coefficient, and at least one tangential lens distortion coefficient.</li>        </ul>        </li>        <li id="ul0002-0003" num="0113">(3) The stereo vision system of any of configurations (1) to (2), wherein the automatic system calibration is comprised of a medium calibration procedure configured to generate second stereo calibration parameters to perform one or both of:        <ul id="ul0004" list-style="none">            <li id="ul0004-0001" num="0114">to compensate for medium-frequency perturbations based on optimizing a number of valid pixels for dense stereo block matching as a function of a relative camera pitch and a relative camera roll, and</li>            <li id="ul0004-0002" num="0115">to calibrate, for each camera, any one or any combination of: a focal length, a principal point, at least one radial lens distortion coefficient, and at least one tangential lens distortion coefficient.</li>        </ul>        </li>        <li id="ul0002-0004" num="0116">(4) The stereo vision system of any of configurations (1) to (3), wherein the automatic system calibration is comprised of a slow calibration procedure configured to generate third stereo calibration parameters to perform one or both of:        <ul id="ul0005" list-style="none">            <li id="ul0005-0001" num="0117">to compensate for low-frequency perturbations based on optimizing a number of valid pixels for dense stereo block matching as a function of any one or any combination of: a relative camera pitch, a relative camera roll, a relative camera yaw, a relative camera position, and</li>            <li id="ul0005-0002" num="0118">to calibrate, for each camera, any one or any combination of: a focal length, a principal point, at least one radial lens distortion coefficient, and at least one tangential lens distortion coefficient.</li>        </ul>        </li>        <li id="ul0002-0005" num="0119">(5) The stereo vision system of any of configurations (1) to (4), wherein the automatic system calibration is comprised of an absolute range calibration procedure configured to generate a stereo calibration parameter for a relative camera yaw based on a non-negative disparity constraint.</li>        <li id="ul0002-0006" num="0120">(6) The stereo vision system of any of configurations (1) to (5), wherein the automatic system calibration is comprised of an absolute range calibration procedure configured to generate a stereo calibration parameter for a relative camera yaw based on dimensions of an object in a scene of a plurality of stereo images and a focal length of the first and second camera sensors.</li>        <li id="ul0002-0007" num="0121">(7) The stereo vision system of configuration (6), wherein the object is a traffic sign.</li>        <li id="ul0002-0008" num="0122">(8) The stereo vision system of configuration (6), wherein the object is a license plate.</li>        <li id="ul0002-0009" num="0123">(9) The stereo vision system of any of configurations (1) to (8), wherein the automatic system calibration is comprised of an absolute range calibration procedure configured to generate a stereo calibration parameter for a relative camera yaw based on vehicle odometry.</li>        <li id="ul0002-0010" num="0124">(10) The stereo vision system of any of configurations (1) to (9), wherein the automatic system calibration is comprised of a calibration management procedure configured to manage an operation schedule of the slow calibration procedure, the medium calibration procedure, the fast calibration procedure, and an absolute range calibration procedure.</li>        <li id="ul0002-0011" num="0125">(11) The stereo vision system of any of configurations (1) to (10), wherein the automatic system calibration is comprised of a calibration health indicator that indicates an accuracy of the automatic system calibration.</li>        <li id="ul0002-0012" num="0126">(12) The stereo vision system of any of configurations (1) to (11), wherein the stereo matching is comprised of a depth estimate confidence score for each pixel of the plurality of stereo images.</li>        <li id="ul0002-0013" num="0127">(13) The stereo vision system of any of configurations (1) to (12), wherein the first and second camera sensors are mounted on any of: a vehicle, a car, a truck, a traffic light, a lamp post, left-side and right-side view mirrors of a vehicle, respectively, a roof line of a vehicle, an upper-left-side and an upper-right-side of a windshield, respectively, and left and right head lamps, respectively.</li>        <li id="ul0002-0014" num="0128">(14) The stereo vision system of any of configurations (1) to (13), wherein the cost function is based on a number of valid pixels in a disparity map.</li>        <li id="ul0002-0015" num="0129">(15) The stereo vision system of any of configurations (1) to (14), wherein the cost function is optimized based on intrinsic camera parameters.</li>        <li id="ul0002-0016" num="0130">(16) The stereo vision system of an of configurations (1) to (15), wherein the cost function is optimized based on extrinsic camera parameters.</li>        <li id="ul0002-0017" num="0131">(17) The stereo vision system of any of configurations (1) to (16), wherein the at least one processor is configured to provide a calibration health indicator.</li>        <li id="ul0002-0018" num="0132">(18) The stereo vision system of any of configurations (1) to (17), further comprising: a close-range sensor system configured to provide 3D data for objects in a range that is closer than a minimum depth-range of the first and second camera sensors, wherein the at least one processor is configured to combine the 3D data produced from the first and second sensor signals with the 3D data provided by the close-range sensor system.</li>        <li id="ul0002-0019" num="0133">(19) The stereo vision system of configuration (18), wherein the close-range sensor system is comprised of a pair of camera sensors having a wider field-of-view and a shorter baseline distance than the first and second camera sensors.</li>        <li id="ul0002-0020" num="0134">(20) The stereo vision system of configuration (18), wherein the close-range sensor system is comprised of a third camera sensor that forms a trinocular stereo system with the first and second camera sensors, such that the first and second camera sensors have a shorter baseline length than the first and third camera sensors.</li>        <li id="ul0002-0021" num="0135">(21) The stereo vision system of configuration (18), wherein the close-range sensor system is comprised of a time-of-flight camera.</li>        <li id="ul0002-0022" num="0136">(22) The stereo vision system of any of configurations (1) to (21), further comprising an active illumination device configured to emit visible or infrared radiation towards a field of view of the first and second camera sensors.</li>        <li id="ul0002-0023" num="0137">(23) The stereo vision system of configuration (22), wherein the active illumination device is configured to: alternate between emitting radiation and not emitting radiation, and emit radiation in synchronization with an exposure interval of the first camera sensor and an exposure interval of the second camera sensor.</li>        <li id="ul0002-0024" num="0138">(24) The stereo vision system of configuration (22), wherein the active illumination component is comprised of any one or any combination of: a vertical cavity surface emitting laser array, a radiation lamp that emits in a visible spectrum range, and a radiation lamp that emits in a near-infrared spectrum range.</li>        <li id="ul0002-0025" num="0139">(25). The stereo vision system of any of configurations (1) to (24), wherein the at least one processor is configured to: compute structure-from-motion data from the first sensor signal and from the second sensor signal, and estimate, using the structure-from-motion data, 3D positions of objects that are closer than a minimum depth-range of the first and second camera sensors.</li>    </ul>    </li></ul></p><p id="p-0110" num="0140">Methods for detection of an object according to the technology described herein may be include various processes. Example methods include combinations of processes (26) and (27), as follows:<ul id="ul0006" list-style="none">    <li id="ul0006-0001" num="0000">    <ul id="ul0007" list-style="none">        <li id="ul0007-0001" num="0141">(26) A computer-implemented method performed by one or more processors for detection of an object in an image, the method comprising: identifying connected component regions on the object by color; determining an aspect ratio of each of the connected component regions; determining a distance between the connected component regions; and identifying the object to be a known object based on the aspect ratios and the distance between the connected component regions, wherein by a presence, a location, and a size of the known object is determined from the image.</li>        <li id="ul0007-0002" num="0142">(27) The method of process (26), wherein: the identifying of the connected component regions identifies the color to be red, and the identifying of the object identifies the object to be a stop sign.</li>    </ul>    </li></ul></p><p id="p-0111" num="0143">Methods to calibrate a stereo vision system according to the technology described herein may be include various processes. Example methods include combinations of processes (28) through (33), as follows:<ul id="ul0008" list-style="none">    <li id="ul0008-0001" num="0000">    <ul id="ul0009" list-style="none">        <li id="ul0009-0001" num="0144">(28) A computer-implemented calibration method performed by one or more processors to calibrate a stereo vision system that includes a first camera sensor configured to sense first reflected energy of a first image and to generate first sensor signals based on the first reflected energy and a second camera sensor configured to sense second reflected energy of a second image and generate second sensor signals based on the second reflected energy, the method comprising: generating stereo images from the first and second sensor signals; rectifying the stereo images using stereo calibration parameters, to produce rectified stereo images; performing a stereo matching on the rectified stereo images; and performing an automatic system calibration using a result of the stereo matching, wherein the automatic system calibration is based on minimizing a cost function.</li>        <li id="ul0009-0002" num="0145">(29) The method of process (28), wherein the automatic system calibration is comprised of any one or any combination of: a fast calibration procedure configured to generate first stereo calibration parameters to compensate for high-frequency perturbations based on tracking image points corresponding to features in the first and second images; a medium calibration procedure configured to generate second stereo calibration parameters to compensate for medium-frequency perturbations based on optimizing a number of valid pixels for dense stereo block matching as a function of a relative camera pitch and a relative camera roll, and a slow calibration procedure configured to generate third stereo calibration parameters to compensate for low-frequency perturbations based on optimizing a number of valid pixels for dense stereo block matching as a function of a relative camera pitch, a relative camera roll, a relative camera yaw, and a relative camera position.</li>        <li id="ul0009-0003" num="0146">(30) The method of process (28) or process (29), wherein the automatic system calibration is comprised of an absolute range calibration procedure configured to generate a stereo calibration parameter for a relative camera yaw based on a non-negative disparity constraint.</li>        <li id="ul0009-0004" num="0147">(31) The method of any one of processes (28) to (30), wherein the automatic system calibration is comprised of an absolute range calibration procedure configured to generate a stereo calibration parameter for a relative camera yaw based on dimensions of an object in a scene of a plurality of stereo images and a focal length of the first and second camera sensors.</li>        <li id="ul0009-0005" num="0148">(32) The method of any one of processes (28) to (31), wherein the automatic system calibration is comprised of an absolute range calibration procedure configured to generate a stereo calibration parameter for a relative camera yaw based on vehicle odometry.</li>        <li id="ul0009-0006" num="0149">(33) The method of any one of processes (28) to (32), wherein the automatic system calibration is comprised of a calibration management procedure configured to manage an operation schedule of the slow calibration procedure, the medium calibration procedure, the fast calibration procedure, and an absolute range calibration procedure.</li>    </ul>    </li></ul></p><p id="p-0112" num="0150">A non-transitory computer readable medium storing computer-executable code to calibrate a stereo vision system according to the technology described herein may be embodied in different configurations. Example configurations include combinations of configurations (34) through (39), as follows:<ul id="ul0010" list-style="none">    <li id="ul0010-0001" num="0000">    <ul id="ul0011" list-style="none">        <li id="ul0011-0001" num="0151">(34) A non-transitory computer readable medium storing computer-executable code that, when executed by one or more processors, causes the one or more processors to calibrate a stereo vision system that includes a first camera sensor configured to sense first reflected energy of a first image and to generate first sensor signals based on the first reflected energy and a second camera sensor configured to sense second reflected energy of a second image and generate second sensor signals based on the second reflected energy, wherein the method is comprised of: generating stereo images from the first and second sensor signals; rectifying the stereo images using stereo calibration parameters, to produce rectified stereo images; performing a stereo matching on the rectified stereo images; and performing an automatic system calibration using a result of the stereo matching, wherein the automatic system calibration is based on minimizing a cost function.</li>        <li id="ul0011-0002" num="0152">(35) The non-transitory computer readable medium of configuration (34), wherein the automatic system calibration is comprised of any one or any combination of: a fast calibration procedure configured to generate first stereo calibration parameters to compensate for high-frequency perturbations based on tracking image points corresponding to features in the first and second images; a medium calibration procedure configured to generate second stereo calibration parameters to compensate for medium-frequency perturbations based on optimizing a number of valid pixels for dense stereo block matching as a function of a relative camera pitch and a relative camera roll, and a slow calibration procedure configured to generate third stereo calibration parameters to compensate for low-frequency perturbations based on optimizing a number of valid pixels for dense stereo block matching as a function of a relative camera pitch, a relative camera roll, a relative camera yaw, and a relative camera position.</li>        <li id="ul0011-0003" num="0153">(36) The non-transitory computer readable medium of configuration (34) or configuration (35), wherein the automatic system calibration is comprised of an absolute range calibration procedure configured to generate a stereo calibration parameter for a relative camera yaw based on a non-negative disparity constraint.</li>        <li id="ul0011-0004" num="0154">(37) The non-transitory computer readable medium of any one of configurations (34) to (36), wherein the automatic system calibration is comprised of an absolute range calibration procedure configured to generate a stereo calibration parameter for a relative camera yaw based on dimensions of an object in a scene of a plurality of stereo images and a focal length of the first and second camera sensors.</li>        <li id="ul0011-0005" num="0155">(38) The non-transitory computer readable medium of any one of configurations (34) to (37), wherein the automatic system calibration is comprised of an absolute range calibration procedure configured to generate a stereo calibration parameter for a relative camera yaw based on vehicle odometry.</li>        <li id="ul0011-0006" num="0156">(39) The non-transitory computer readable medium of any one of configurations (34) to (38), wherein the automatic system calibration is comprised of a calibration management procedure configured to manage an operation schedule of the slow calibration procedure, the medium calibration procedure, the fast calibration procedure, and an absolute range calibration procedure.</li>    </ul>    </li></ul></p><p id="p-0113" num="0157">It should be understood that the embodiments and examples described herein have been chosen and described in order to illustrate the principles, methods, and processes of the inventive technology and its practical applications to thereby enable one of ordinary skill in the art to utilize the inventive technology in various embodiments and with various modifications as are suited for particular uses contemplated. Even though specific embodiments of the inventive technology have been described, they are not to be taken as exhaustive. Other embodiments and variations that will be apparent to those skilled in the art but that are not specifically described herein are within the scope of the present technology.</p><p id="p-0114" num="0158">Unless stated otherwise, the terms &#x201c;approximately&#x201d; and &#x201c;about&#x201d; are used to mean within &#xb1;20% of a target value in some embodiments, within &#xb1;10% of a target value in some embodiments, within &#xb1;5% of a target value in some embodiments, and yet within &#xb1;2% of a target value in some embodiments. The terms &#x201c;approximately&#x201d; and &#x201c;about&#x201d; can include the target value. The term &#x201c;essentially&#x201d; is used to mean within &#xb1;3% of a target value.</p><p id="p-0115" num="0159">The technology described herein may be embodied as a method, of which at least some acts have been described. The acts performed as part of the method may be ordered in any suitable way. Accordingly, embodiments may be implemented in which acts are performed in an order different than described, which may include performing some acts simultaneously, even though described as sequential acts in illustrative embodiments. Additionally, a method may include more acts than those described, in some embodiments, and fewer acts than those described in other embodiments.</p><p id="p-0116" num="0160">Various aspects of the present disclosure may be used alone, in combination, or in a variety of arrangements not specifically discussed in the embodiments described in the foregoing and is therefore not limited in its application to the details and arrangement of components set forth in the foregoing description or illustrated in the drawings. For example, aspects described in one embodiment may be combined in any manner with aspects described in other embodiments.</p><p id="p-0117" num="0161">Use of ordinal terms such as &#x201c;first,&#x201d; &#x201c;second,&#x201d; &#x201c;third,&#x201d; etc., in the claims to modify a claim element does not by itself connote any priority, precedence, or order of one claim element over another or the temporal order in which acts of a method are performed, but are used merely as labels to distinguish one claim element having a certain name from another element having a same name (but for use of the ordinal term) to distinguish the claim elements.</p><p id="p-0118" num="0162">Phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting. The use of &#x201c;including,&#x201d; &#x201c;comprising,&#x201d; or &#x201c;having,&#x201d; &#x201c;containing,&#x201d; &#x201c;involving,&#x201d; and variations thereof herein, is meant to encompass the items listed thereafter and equivalents thereof as well as additional items.</p><p id="p-0119" num="0163">The indefinite articles &#x201c;a&#x201d; and &#x201c;an,&#x201d; as used herein in the specification and in the claims, unless clearly indicated to the contrary, should be understood to mean &#x201c;at least one.&#x201d;</p><p id="p-0120" num="0164">Any use of the phrase &#x201c;at least one,&#x201d; in reference to a list of one or more elements, should be understood to mean at least one element selected from any one or more of the elements in the list of elements, but not necessarily including at least one of each and every element specifically listed within the list of elements and not excluding any combinations of elements in the list of elements. This definition also allows that elements may optionally be present other than the elements specifically identified within the list of elements to which the phrase &#x201c;at least one&#x201d; refers, whether related or unrelated to those elements specifically identified.</p><p id="p-0121" num="0165">Any use of the phrase &#x201c;equal&#x201d; or &#x201c;the same&#x201d; in reference to two values (e.g., distances, widths, etc.) means that two values are the same within manufacturing tolerances. Thus, two values being equal, or the same, may mean that the two values are different from one another by &#xb1;5%.</p><p id="p-0122" num="0166">The phrase &#x201c;and/or,&#x201d; as used herein in the specification and in the claims, should be understood to mean &#x201c;either or both&#x201d; of the elements so conjoined, i.e., elements that are conjunctively present in some cases and disjunctively present in other cases. Multiple elements listed with &#x201c;and/or&#x201d; should be construed in the same fashion, i.e., &#x201c;one or more&#x201d; of the elements so conjoined. Other elements may optionally be present other than the elements specifically identified by the &#x201c;and/or&#x201d; clause, whether related or unrelated to those elements specifically identified. Thus, as a non-limiting example, a reference to &#x201c;A and/or B&#x201d;, when used in conjunction with open-ended language such as &#x201c;comprising&#x201d; can refer, in one embodiment, to A only (optionally including elements other than B); in another embodiment, to B only (optionally including elements other than A); in yet another embodiment, to both A and B (optionally including other elements); etc.</p><p id="p-0123" num="0167">As used herein in the specification and in the claims, &#x201c;or&#x201d; should be understood to have the same meaning as &#x201c;and/or&#x201d; as defined above. For example, when separating items in a list, &#x201c;or&#x201d; or &#x201c;and/or&#x201d; shall be interpreted as being inclusive, i.e., the inclusion of at least one, but also including more than one, of a number or list of elements, and, optionally, additional unlisted items. Only terms clearly indicated to the contrary, such as &#x201c;only one of&#x201d; or &#x201c;exactly one of,&#x201d; or, when used in the claims, &#x201c;consisting of,&#x201d; will refer to the inclusion of exactly one element of a number or list of elements. In general, the term &#x201c;or&#x201d; as used herein shall only be interpreted as indicating exclusive alternatives (i.e. &#x201c;one or the other but not both&#x201d;) when preceded by terms of exclusivity, such as &#x201c;either,&#x201d; &#x201c;one of,&#x201d; &#x201c;only one of,&#x201d; or &#x201c;exactly one of.&#x201d; &#x201c;Consisting essentially of,&#x201d; if used in the claims, shall have its ordinary meaning as used in the field of patent law.</p><p id="p-0124" num="0168">The term &#x201c;substantially&#x201d; if used herein may be construed to mean within 95% of a target value in some embodiments, within 98% of a target value in some embodiments, within 99% of a target value in some embodiments, and within 99.5% of a target value in some embodiments. In some embodiments, the term &#x201c;substantially&#x201d; may equal 100% of the target value.</p><p id="p-0125" num="0169">Also, some of the embodiments described above may be implemented as one or more method(s), of which some examples have been provided. The acts performed as part of the method(s) may be ordered in any suitable way. Accordingly, embodiments may be constructed in which acts are performed in an order different than illustrated or described herein, which may include performing some acts simultaneously, even though shown as sequential acts in illustrative embodiments.</p><p id="p-0126" num="0170">Further, although advantages of the present invention may be indicated, it should be appreciated that not every embodiment of the invention will include every described advantage. Some embodiments may not implement any features described as advantageous herein. Accordingly, the foregoing description and attached drawings are by way of example only.</p><heading id="h-0007" level="1">REFERENCES CITED</heading><p id="p-0127" num="0000"><ul id="ul0012" list-style="none">    <li id="ul0012-0001" num="0171">U.S. Pat. No. 6,392,688B1;</li>    <li id="ul0012-0002" num="0172">U.S. Pat. No. 8,797,387B2;</li>    <li id="ul0012-0003" num="0173">U.S. Ser. No. 10/097,812B2;</li>    <li id="ul0012-0004" num="0174">JP2008-509619A; and</li>    <li id="ul0012-0005" num="0175">Timo Hinzmann, Tim Taubner, and Roland Siegwart, &#x201c;Flexible Stereo: Constrained, Non-rigid, Wide-baseline Stereo Vision for Fixed-Wing Aerial Platforms,&#x201d; IEEE International Conference on Robotics and Automation (ICRA), Brisbane AU (2018).</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-01-39" num="01-39"><claim-text><b>1</b>.-<b>39</b>. (canceled)</claim-text></claim><claim id="CLM-00040" num="00040"><claim-text><b>40</b>. A stereo vision system for a vehicle, comprising:<claim-text>at least one computer processor in communication with at least two camera sensors, the at least one computer processor being configured to:<claim-text>receive, in real time or nearly real time, video signals from the at least two camera sensors, each of the at least two camera sensors being configured to sense reflected energy of a first scene and to generate a video signal based on the reflected energy; and</claim-text><claim-text>process the video signals to:<claim-text>produce three-dimensional (3D) data corresponding to the first scene, and</claim-text><claim-text>calibrate the at least two camera sensors by automatically updating camera parameters for the at least two camera sensors on a frame-by-frame basis for frames of the video signals from the at least two camera sensors.</claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00041" num="00041"><claim-text><b>41</b>. The stereo vision system of <claim-ref idref="CLM-00040">claim 40</claim-ref>, further comprising:<claim-text>a first camera sensor configured to sense first reflected energy of the first scene and to generate a first video signal based on the first reflected energy; and</claim-text><claim-text>a second camera sensor configured to sense second reflected energy of the first scene and to generate a second video signal based on the second reflected energy,</claim-text><claim-text>wherein the at least one computer processor is configured to:<claim-text>generate rectified stereo images from the first and second video signals utilizing stereo calibration parameters,</claim-text><claim-text>perform a stereo matching on the rectified stereo images,</claim-text><claim-text>produce a disparity map based on the rectified stereo images, and</claim-text><claim-text>calibrate the first and second camera sensors by performing an automatic system calibration based on optimizing a cost function, wherein the automatic system calibration includes an absolute range calibration procedure configured to generate a stereo calibration parameter for calibrating a relative camera yaw of the first and second camera sensors.</claim-text></claim-text></claim-text></claim><claim id="CLM-00042" num="00042"><claim-text><b>42</b>. The stereo vision system of <claim-ref idref="CLM-00041">claim 41</claim-ref>, wherein the absolute range calibration procedure is configured to generate the stereo calibration parameter based on a vehicle odometry of the vehicle.</claim-text></claim><claim id="CLM-00043" num="00043"><claim-text><b>43</b>. The stereo vision system of <claim-ref idref="CLM-00041">claim 41</claim-ref>, wherein the absolute range calibration procedure is configured to generate the stereo calibration parameter based on a non-negative disparity constraint.</claim-text></claim><claim id="CLM-00044" num="00044"><claim-text><b>44</b>. The stereo vision system of <claim-ref idref="CLM-00041">claim 41</claim-ref>, wherein the absolute range calibration procedure is configured to generate the stereo calibration parameter based on dimensions of an object in of the rectified stereo images and a focal length of the first and second camera sensors.</claim-text></claim><claim id="CLM-00045" num="00045"><claim-text><b>45</b>. The stereo vision system of <claim-ref idref="CLM-00041">claim 41</claim-ref>, wherein the cost function:<claim-text>is based on a number of valid pixels in the disparity map, and</claim-text><claim-text>is optimized based on intrinsic camera parameters and extrinsic camera parameters of the first and second camera sensors.</claim-text></claim-text></claim><claim id="CLM-00046" num="00046"><claim-text><b>46</b>. The stereo vision system of <claim-ref idref="CLM-00041">claim 41</claim-ref>, wherein the automatic system calibration includes:<claim-text>a fast calibration procedure to generate first stereo calibration parameters to calibrate the first and second camera sensors to compensate for high-frequency perturbations,</claim-text><claim-text>a medium calibration procedure to generate second stereo calibration parameters to calibrate the first and second camera sensors to compensate for medium-frequency perturbations, and</claim-text><claim-text>a slow calibration procedure to generate third stereo calibration parameters to calibrate the first and second camera sensors to compensate for low-frequency perturbations.</claim-text></claim-text></claim><claim id="CLM-00047" num="00047"><claim-text><b>47</b>. The stereo vision system of <claim-ref idref="CLM-00046">claim 46</claim-ref>, wherein:<claim-text>the fast calibration procedure is based on tracking image points corresponding to features in the rectified stereo images,</claim-text><claim-text>the medium calibration procedure is based on optimizing a number of valid pixels for dense stereo block matching as a function of a relative camera pitch and a relative camera roll, and</claim-text><claim-text>the slow calibration procedure is based on optimizing a number of valid pixels for dense stereo block matching as a function of any one or any combination of: a relative camera pitch, a relative camera roll, a relative camera yaw, and a relative camera position.</claim-text></claim-text></claim><claim id="CLM-00048" num="00048"><claim-text><b>48</b>. The stereo vision system of <claim-ref idref="CLM-00046">claim 46</claim-ref>, wherein the automatic system calibration calibrates, for each of the first and second camera sensors, any one or any combination of: a focal length, a principal point, at least one radial lens distortion coefficient, and at least one tangential lens distortion coefficient.</claim-text></claim><claim id="CLM-00049" num="00049"><claim-text><b>49</b>. The stereo vision system of <claim-ref idref="CLM-00041">claim 41</claim-ref>, further comprising:<claim-text>a close-range sensor system configured to sense reflected energy of a second scene and to provide at least one video signal to the at least one computer processor, the second scene including a range that is closer than a minimum depth-range of the first and second camera sensors,</claim-text><claim-text>wherein the at least one computer processor is configured to combine the 3D data produced from the first and second video signals with 3D data produced from the at least one video signal provided by the close-range sensor system.</claim-text></claim-text></claim><claim id="CLM-00050" num="00050"><claim-text><b>50</b>. The stereo vision system of <claim-ref idref="CLM-00049">claim 49</claim-ref>, wherein the close-range sensor system includes a pair of camera sensors having a wider field-of-view and a shorter baseline distance than the first and second camera sensors.</claim-text></claim><claim id="CLM-00051" num="00051"><claim-text><b>51</b>. The stereo vision system of <claim-ref idref="CLM-00049">claim 49</claim-ref>, wherein the close-range sensor system includes a third camera sensor that forms a trinocular stereo system with the first and second camera sensors, such that the first and third camera sensors have a shorter baseline length than the first and second camera sensors.</claim-text></claim><claim id="CLM-00052" num="00052"><claim-text><b>52</b>. A computer-implemented calibration method performed by one or more computer processors to calibrate a stereo vision system that includes at least two camera sensors, the method comprising:<claim-text>receiving, in real time or nearly real time, video signals from the at least two camera sensors, each of the at least two camera sensors being configured to sense reflected energy of a scene and to generate a video signal based on the reflected energy; and</claim-text><claim-text>processing the video signals to:<claim-text>produce three-dimensional (3D) data corresponding to the scene, and</claim-text><claim-text>calibrate the at least two camera sensors by automatically updating camera parameters for the at least two camera sensors on a frame-by-frame basis for frames of the video signals from the at least two camera sensors.</claim-text></claim-text></claim-text></claim><claim id="CLM-00053" num="00053"><claim-text><b>53</b>. The method of <claim-ref idref="CLM-00052">claim 52</claim-ref>,<claim-text>wherein the at least two camera sensors include a first camera sensor configured to sense first reflected energy of the scene and to generate a first video signal based on the first reflected energy and a second camera sensor configured to sense second reflected energy of the scene and to generate a second video signal based on the second reflected energy, and</claim-text><claim-text>wherein the method further comprises:<claim-text>generating rectified stereo images from the first and second video signals utilizing stereo calibration parameters;</claim-text><claim-text>performing a stereo matching on the rectified stereo images;</claim-text><claim-text>producing a disparity map based on the rectified stereo images; and</claim-text><claim-text>calibrating the first and second camera sensors by performing an automatic system calibration based on optimizing a cost function, wherein the performing of the automatic system calibration includes performing an absolute range calibration procedure to generate a stereo calibration parameter for calibrating a relative camera yaw of the first and second camera sensors.</claim-text></claim-text></claim-text></claim><claim id="CLM-00054" num="00054"><claim-text><b>54</b>. The method of <claim-ref idref="CLM-00053">claim 53</claim-ref>, wherein the performing of the absolute range calibration procedure generates the stereo calibration parameter based on a vehicle odometry of the vehicle.</claim-text></claim><claim id="CLM-00055" num="00055"><claim-text><b>55</b>. The method of <claim-ref idref="CLM-00053">claim 53</claim-ref>, wherein the performing of the absolute range calibration procedure generates the stereo calibration parameter based on a non-negative disparity constraint.</claim-text></claim><claim id="CLM-00056" num="00056"><claim-text><b>56</b>. The method of <claim-ref idref="CLM-00053">claim 53</claim-ref>, wherein the performing of the absolute range calibration procedure generates the stereo calibration parameter based on dimensions of an object in the rectified stereo images and a focal length of the first and second camera sensors.</claim-text></claim><claim id="CLM-00057" num="00057"><claim-text><b>57</b>. The method of <claim-ref idref="CLM-00053">claim 53</claim-ref>, wherein the cost function:<claim-text>is based on a number of valid pixels in the disparity map, and</claim-text><claim-text>is optimized based on intrinsic camera parameters and extrinsic camera parameters of the first and second camera sensors.</claim-text></claim-text></claim><claim id="CLM-00058" num="00058"><claim-text><b>58</b>. The method of <claim-ref idref="CLM-00053">claim 53</claim-ref>, wherein the performing of the automatic system calibration includes:<claim-text>performing a fast calibration procedure to generate first stereo calibration parameters to calibrate the first and second camera sensors to compensate for high-frequency perturbations,</claim-text><claim-text>performing a medium calibration procedure to generate second stereo calibration parameters to calibrate the first and second camera sensors to compensate for medium-frequency perturbations, and</claim-text><claim-text>performing a slow calibration procedure to generate third stereo calibration parameters to calibrate the first and second camera sensors to compensate for low-frequency perturbations.</claim-text></claim-text></claim><claim id="CLM-00059" num="00059"><claim-text><b>59</b>. The method of <claim-ref idref="CLM-00058">claim 58</claim-ref>, wherein:<claim-text>the fast calibration procedure is based on tracking image points corresponding to features in the rectified stereo images,</claim-text><claim-text>the medium calibration procedure is based on optimizing a number of valid pixels for dense stereo block matching as a function of a relative camera pitch and a relative camera roll, and</claim-text><claim-text>the slow calibration procedure is based on optimizing a number of valid pixels for dense stereo block matching as a function of any one or any combination of: a relative camera pitch, a relative camera roll, a relative camera yaw, and a relative camera position.</claim-text></claim-text></claim><claim id="CLM-00060" num="00060"><claim-text><b>60</b>. The method of <claim-ref idref="CLM-00058">claim 58</claim-ref>, wherein the performing of the automatic system calibration includes calibrating, for each of the first and second camera sensors, any one or any combination of: a focal length, a principal point, at least one radial lens distortion coefficient, and at least one tangential lens distortion coefficient.</claim-text></claim><claim id="CLM-00061" num="00061"><claim-text><b>61</b>. A non-transitory computer-readable medium storing computer-executable code that, when executed by one or more processors, causes the one or more processors to calibrate a stereo vision system that includes at least two camera sensors, the method comprising:<claim-text>receiving, in real time or nearly real time, video signals from the at least two camera sensors, each of the at least two camera sensors being configured to sense reflected energy of a scene and to generate a video signal based on the reflected energy; and</claim-text><claim-text>processing the video signals to:<claim-text>produce three-dimensional (3D) data corresponding to the scene, and</claim-text><claim-text>calibrate the at least two camera sensors by automatically updating camera parameters for the at least two camera sensors on a frame-by-frame basis for frames of the video signals from the at least two camera sensors.</claim-text></claim-text></claim-text></claim><claim id="CLM-00062" num="00062"><claim-text><b>62</b>. The computer-readable medium of <claim-ref idref="CLM-00061">claim 61</claim-ref>,<claim-text>wherein the at least two camera sensors include a first camera sensor configured to sense first reflected energy of the scene and to generate a first video signal based on the first reflected energy and a second camera sensor configured to sense second reflected energy of the scene and to generate a second video signal based on the second reflected energy, and</claim-text><claim-text>wherein the method further comprises:<claim-text>generating rectified stereo images from the first and second video signals utilizing stereo calibration parameters;</claim-text><claim-text>performing a stereo matching on the rectified stereo images;</claim-text><claim-text>producing a disparity map based on the rectified stereo images; and</claim-text><claim-text>calibrating the first and second camera sensors by performing an automatic system calibration based on optimizing a cost function, wherein the performing of the automatic system calibration includes performing an absolute range calibration procedure to generate a stereo calibration parameter for calibrating a relative camera yaw of the first and second camera sensors.</claim-text></claim-text></claim-text></claim><claim id="CLM-00063" num="00063"><claim-text><b>63</b>. The computer-readable medium of <claim-ref idref="CLM-00062">claim 62</claim-ref>, wherein the performing of the absolute range calibration procedure generates the stereo calibration parameter based on a vehicle odometry of the vehicle.</claim-text></claim><claim id="CLM-00064" num="00064"><claim-text><b>64</b>. The computer-readable medium of <claim-ref idref="CLM-00062">claim 62</claim-ref>, wherein the performing of the absolute range calibration procedure generates the stereo calibration parameter based on a non-negative disparity constraint.</claim-text></claim><claim id="CLM-00065" num="00065"><claim-text><b>65</b>. The computer-readable medium of <claim-ref idref="CLM-00062">claim 62</claim-ref>, wherein the performing of the absolute range calibration procedure generates the stereo calibration parameter based on dimensions of an object in the rectified stereo images and a focal length of the first and second camera sensors.</claim-text></claim><claim id="CLM-00066" num="00066"><claim-text><b>66</b>. The computer-readable medium of <claim-ref idref="CLM-00062">claim 62</claim-ref>, wherein the cost function:<claim-text>is based on a number of valid pixels in the disparity map, and</claim-text><claim-text>is optimized based on intrinsic camera parameters and extrinsic camera parameters of the first and second camera sensors.</claim-text></claim-text></claim><claim id="CLM-00067" num="00067"><claim-text><b>67</b>. The computer-readable medium of <claim-ref idref="CLM-00062">claim 62</claim-ref>, wherein the performing of the automatic system calibration includes:<claim-text>performing a fast calibration procedure to generate first stereo calibration parameters to calibrate the first and second camera sensors to compensate for high-frequency perturbations,</claim-text><claim-text>performing a medium calibration procedure to generate second stereo calibration parameters to calibrate the first and second camera sensors to compensate for medium-frequency perturbations, and</claim-text><claim-text>performing a slow calibration procedure to generate third stereo calibration parameters to calibrate the first and second camera sensors to compensate for low-frequency perturbations.</claim-text></claim-text></claim><claim id="CLM-00068" num="00068"><claim-text><b>68</b>. The computer-readable medium of <claim-ref idref="CLM-00067">claim 67</claim-ref>, wherein:<claim-text>the fast calibration procedure is based on tracking image points corresponding to features in the rectified stereo images,</claim-text><claim-text>the medium calibration procedure is based on optimizing a number of valid pixels for dense stereo block matching as a function of a relative camera pitch and a relative camera roll, and</claim-text><claim-text>the slow calibration procedure is based on optimizing a number of valid pixels for dense stereo block matching as a function of any one or any combination of: a relative camera pitch, a relative camera roll, a relative camera yaw, and a relative camera position.</claim-text></claim-text></claim><claim id="CLM-00069" num="00069"><claim-text><b>69</b>. The computer-readable medium of <claim-ref idref="CLM-00067">claim 67</claim-ref>, wherein the performing of the automatic system calibration includes calibrating, for each of the first and second camera sensors, any one or any combination of: a focal length, a principal point, at least one radial lens distortion coefficient, and at least one tangential lens distortion coefficient.</claim-text></claim></claims></us-patent-application>